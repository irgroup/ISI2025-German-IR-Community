<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.29,99.57,409.45,14.93">OVERVIEW OF THE TREC 2021 DEEP LEARNING TRACK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.52,162.69,57.27,8.64"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<email>nickcr@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,202.76,162.69,57.27,8.64"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
							<email>bmitra@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,269.00,162.69,57.28,8.64"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
							<email>emine.yilmaz@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.26,162.69,61.71,8.64"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
							<email>dcampos3@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Neural Magic Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,428.04,162.69,43.46,8.64"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@uwaterloo.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.26,183.56,37.60,8.64"><surname>Microsoft</surname></persName>
						</author>
						<title level="a" type="main" coord="1,101.29,99.57,409.45,14.93">OVERVIEW OF THE TREC 2021 DEEP LEARNING TRACK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E7980EF9FC5677DEBAC26B2B17CEFD05</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the third year of the TREC Deep Learning track. As in previous years, we leverage the MS MARCO datasets that made hundreds of thousands of human annotated training labels available for both passage and document ranking tasks. In addition, this year we refreshed both the document and the passage collections which also led to a nearly four times increase in the document collection size and nearly 16 times increase in the size of the passage collection. Deep neural ranking models that employ large scale pretraininig continued to outperform traditional retrieval methods this year. We also found that single stage retrieval can achieve good performance on both tasks although they still do not perform at par with multistage retrieval pipelines. Finally, the increase in the collection size and the general data refresh raised some questions about completeness of NIST judgments and the quality of the training labels that were mapped to the new collections from the old ones which we discuss in this report.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At TREC 2021, we hosted the third TREC Deep Learning Track continuing our focus on benchmarking ad hoc retrieval methods in the large-data regime. As in previous years <ref type="bibr" coords="1,295.84,465.99,84.48,8.64" target="#b2">[Craswell et al., 2020</ref><ref type="bibr" coords="1,387.62,465.99,25.85,8.64">[Craswell et al., , 2021a]]</ref>, we leverage the MS MARCO datasets <ref type="bibr" coords="1,106.78,476.90,76.68,8.64" target="#b0">[Bajaj et al., 2016]</ref> that made hundreds of thousands of human annotated training labels available for both passage and document ranking tasks. In addition, this year we refreshed both the document and the passage collections which also led to a nearly four times increase in the document collection size and nearly 16 times increase in the size of the passage collection. In addition to evaluating retrieval methods on the larger collections, the data refresh also aimed to provide additional metadata-e.g., passage-to-document mappings-that may be useful for ranking as well as incorporate some fixes for known text encoding issues in previous versions of the datasets. This year, in addition to focusing on TREC-style blind evaluation of neural methods against strong traditional baselines, the track also encouraged participating groups to annotate their runs based on whether they employ dense retrieval methods and whether their ranking pipeline is a single stage retrieval process. The goal was to both encourage more explorations of neural methods in first stage retrieval as well as to allow analysis of how these emerging methods compare to previous state-of-the-art.</p><p>Deep neural ranking models that employ large scale pretraininig continued to outperform traditional retrieval methods this year. We also found that single stage retrieval can achieve good performance on both tasks although they still do not perform at par with multistage retrieval pipelines. Finally, the increase in the collection size and the general data refresh raised some questions about completeness of NIST judgments and the quality of the training labels that were mapped to the new collections from the old ones which we discuss later in this report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task description</head><p>Similar to previous years, Deep Learning Track in 2021 has two tasks: Document retrieval and passage retrieval. Participants were allowed to submit up to three runs for each task. When submitting each run, participants indicated what external data, pretrained models and other resources were used, as well as information on what style of model was used.</p><p>Across the two tasks, same set of 477 queries were used by the participants, who were allowed to submit up to three runs per task. From the full set of 477 queries, stratified sampling based on query length was used to select the subset queries for pooling and judging. Queries were split into two strata based on their length, where queries containing more than or equal to 10 words were put into the stratum corresponding to long queries and the rest of the queries were put into the stratum corresponding to short queries. An equal number of queries were sampled from each stratum.</p><p>In the pooling and judging process, NIST chose a subset of these sampled queries for judging, based on budget constraints and with the goal of finding a sufficiently comprehensive set of relevance judgments to make the test collection reusable <ref type="bibr" coords="2,150.00,184.62,92.22,8.64">[Craswell et al., 2021c]</ref>. This led to a judged test set of 57 queries (28 short, 29 long queries) for the document retrieval task and 53 queries (25 short, 28 long queries) for the passage retrieval task.</p><p>Below we provide more detailed information about the document retrieval and passage retrieval tasks, as well as the datasets provided as part of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document retrieval task</head><p>The first task focuses on document retrieval, with two subtasks: (i) Full retrieval and (ii) top-100 reranking.</p><p>The full retrieval subtask models the end-to-end retrieval scenario, documents can be retrieved from the full document collection provided and the runs are expected to rank documents based on their relevance to the query.</p><p>In the reranking subtask, participants were provided with an initial ranking of 100 documents, which was retrieved using Pyserini <ref type="bibr" coords="2,130.70,323.51,69.52,8.64">[Lin et al., 2021b]</ref>. This way the reranking subtask allows all participants to start from the same starting point and to focus on learning an effective relevance estimator, without the need for implementing an end-to-end retrieval system. It also makes the reranking runs more comparable, because they all rerank the same set of 100 candidates.</p><p>For evaluation, judgments were collected on a four-point scale:</p><p>[3] Perfectly relevant: Document is dedicated to the query, it is worthy of being a top result in a search engine.</p><p>[2] Highly relevant: The content of this document provides substantial information on the query.</p><p>[1] Relevant: Document provides some information relevant to the query, which may be minimal.</p><p>[0] Irrelevant: Document does not provide any useful information about the query.</p><p>For metrics that binarize the judgment scale, we map document judgment levels 3,2,1 to relevant and map document judgment level 0 to irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage retrieval task</head><p>Similar to the document retrieval task, the passage retrieval task includes (i) a full retrieval and (ii) a top-100 reranking tasks.</p><p>In the full retrieval subtask, given a query, the participants were expected to retrieve a ranked list of passages from the full collection based on their estimated likelihood of containing an answer to the question. Participants could submit up to 100 passages per query for this end-to-end retrieval task.</p><p>In the top-100 reranking subtask, 100 passages per query were provided to participants, giving all participants the same starting point. Similar to the document retrieval subtask, the 100 passages provided to the participants generated using Pyserini <ref type="bibr" coords="2,107.23,606.57,68.21,8.64">Lin et al. [2021b]</ref>. Participants were expected to rerank the 100 passages based on their estimated likelihood of containing an answer to the query.</p><p>For evaluation, judgments were collected on a four-point scale:</p><p>[3] Perfectly relevant: The passage is dedicated to the query and contains the exact answer.</p><p>[2] Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information.</p><p>[1] Related: The passage seems related to the query but does not answer it.</p><p>[0] Irrelevant: The passage has nothing to do with the query.</p><p>Figure <ref type="figure" coords="3,100.66,315.04,3.88,8.64">1</ref>: Crowd task used to generate the original MS MARCO natural language generation leaderboard. This same crowd data was later adapted to become the MS MARCO ranking tasks.</p><p>For metrics that binarize the judgment scale, different than the document retrieval task, we map passage judgment levels 3,2 to relevant and map document judgment levels 1,0 to irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>This year we introduced the MS MARCO v2 dataset, which was used in both tasks. To understand how the new dataset differs from the old, we will first describe the natural language generation data and v1 ranking data.</p><p>MS MARCO natural language generation dataset. The original MS MARCO dataset was for a natural language generation task, rather than a ranking task. It processed one million queries, using a crowd task as shown in Figure <ref type="figure" coords="3,532.53,466.68,3.74,8.64">1</ref>. The crowd worker would read the query, consider up to ten passages related to the query, decide if the passages could be used to answer the query and if answerable write an answer to the query in their own words. For each answerable question the crowd works provided a non-extractive answer and an annotation of which passages they used to generate their answer. There was substantial quality work with the crowd workers to ensure quality and the crowd workers spent an average of 2.5 minutes on each annotation. The million queries represent a real Bing query workload, and the ten results come from a Bing component designed to handle that workload. The queries were filtered before being annotated to remove any adult or offensive queries and any non-English queries. Moreover, further filtering was performed to ensure that the queries came from the 10 -20% of English queries that were detected as potentially being answerable with a short passage. Although the filter may be imperfect, the intention was to exclude navigational queries (such as [youtube]), queries that require a longer answer (such as [beef wellington recipe]) and queries that aim to complete some transaction (such as [buy xbox live]). We note that about 35% of the queries could not be answered using the ten passages, in which case the crowd worker would indicate No answer, and one part of the original MS MARCO challenge was to predict which queries were answerable.</p><p>MS MARCO ranking v1 datasets. The MS MARCO passage and document ranking v1 datasets are used in the current MS MARCO leaderboards <ref type="bibr" coords="3,212.85,642.58,62.56,8.64" target="#b10">[Lin et al., 2022</ref><ref type="bibr" coords="3,275.42,642.58,92.91,8.64">, Craswell et al., 2021b</ref><ref type="bibr" coords="3,368.32,642.58,76.25,8.64">, Lin et al., 2021a]</ref> and in TREC 2019 and TREC 2020.</p><p>To generate the v1 passage ranking data, we took the union of the top ten passage lists for the one million queries, giving us 8.8 million distinct passages. For queries that were answerable, we used the crowd judge annotation for selected passages as a positive qrel. This gives us highly incomplete qrels, as noted in the original description <ref type="bibr" coords="3,515.65,691.70,24.35,8.64;3,72.00,702.61,46.37,8.64" target="#b0">[Bajaj et al., 2016]</ref>. We should in no way expect the positive qrel to be the "best answer". We found that training and evaluating using these sparse qrels gives us results that are quite correlated with results using much more comprehensive NIST judgments <ref type="bibr" coords="4,140.55,75.48,83.38,8.64" target="#b2">[Craswell et al., 2020</ref><ref type="bibr" coords="4,230.86,75.48,25.85,8.64">[Craswell et al., , 2021a]]</ref>. Further study is needed to understand why this works, but we suspect it's important that the qrel is selected from a Bing ranking that has access to information that's unavailable to TREC participants, such as billions of past queries. This means the selected qrel is not biased towards some existing academic approach such as BM25. For each query that has a qrel, we generated a BM25 top-1000 for use in a reranking task and also allowed fullrank from the 8.8 million passages. We used the same split as in the NLG task: training (80%), dev (10%) and eval (10%).</p><p>To generate the v1 document ranking data, we collected the corresponding urls for which the passages were extracted. Using these 3.5 million URLS, we obtained the associated document title and body corresponding to the ranking qrels. It is worth noting that the original passages were extracted between January 2016 and February 18 while the full documents were extracted in March of 2018 and as a result only 3.2 million URLs were still in existence. From these documents, the body text had the HTML removed and focused on the main content of the page, removing web-page boilerplate such as navigation menus. Since we extracted the document text more than a year later than the passage data and used a completely different document parsing and processing pipeline (which unfortunately had character set processing issues) there was a chance that some pages that had a relevant passage no longer existed, no longer contained the passage, or even had the section of text with the passage accidentally removed as boilerplate. These are all realistic things to happen in a real-world application, where the document corpus is constantly changing, we do not wish to throw away our old relevance labels, and indeed we may not have budget to generate new labels. Doing a better job of generating a clean dataset using old labels is what we have now done in generating the v2 data. Qrels for the document task were assigned by assuming that a relevant passage qrel transfers to the document level as a positive document qrel. We generated top-100 document rankings using Indri, for use in a reranking task and also allowed fullrank from the 3.2 million documents.</p><p>The v1 data had several problems. The corpus was generated based on the queries, such that each passage and each document is in the corpus due to one of our million original queries. For each document in the corpus there may only be one passage in the passage dataset (and on average 2.8 passages per document), but that passage was identified by Bing in relation to one of the MS MARCO queries, possibly a test query. This is unrealistic, since a real system would be able to generate many candidate passages per document, and would not know what the test queries will be ahead of time. Therefore, we had to forbid participants from considering the passage-document mapping. The document dataset had several problems with character sets and missing whitespaces.</p><p>MS MARCO ranking v2 datasets. The MS MARCO passage and document ranking v2 datasets have been used</p><p>for the first time in TREC 2021.</p><p>The v2 data starts by identifying documents. Of the original 3.2 million MS MARCO documents, we were able to still find content for 2.7 million on the Web. We added an additional 9.2 million documents, selected to be the kind of documents that had useful passages of text in past Bing queries, giving a total of 11.9 million documents. For each document we ran a query-independent proprietary algorithm for identifying promising passages, and selected the best non-overlapping passages, giving on average 11.6 passages per document. This gives us our 138 million passages in the v2 passage corpus. We mapped the document qrels at the URL level, for training, dev and eval. The chance that the document is no longer relevant to the query, which also was a concern in v1 data, is now increased since the document content was extracted at a later date. We can consider how big this problem is by analyzing the disagreement rate between MS MARCO qrels and NIST qrels (in v1 and v2), and seeing whether training on MS MARCO qrels yields improved NIST NDCG on the test set. For mapping passage qrels, we required that the passage comes from the same URL as the original passage, and has sufficient text similarity to the positive passage text from v1.</p><p>It is now possible for participants to use the passage-document mapping in participation, for example by considering document information in passage ranking, passage information in document ranking, and so on. Using a larger corpus prevents participants from proposing completely unscalable ranking approaches. The new dataset has fewer character encoding and whitespace issues, and could form the basis for future tasks that include some elements of additional document processing, such as extracting even shorter (phrase) answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and analysis</head><p>Submitted runs A total of 19 groups participated in the TREC 2021 Deep Learning Track. Among them, 11 groups participated in both the document and the passage ranking tasks, and of the remaining four groups participated only in the document ranking task and another four groups on in passage ranking task. We also solicited baseline runs to enrich the judgment pools which are reported under a separate "BASELINES" group as we did in previous years of the track. Across all groups, we received a total of 129 run submissions, including 66 document ranking runs and 63 passage ranking runs. This also includes 37 baseline runs-20 for document ranking and 17 for passage ranking. Unlike previous years, this year some of the baseline runs also employed neural methods. Table <ref type="table" coords="5,468.76,216.24,4.98,8.64" target="#tab_0">1</ref> summarizes the submissions statistics for this year's track.</p><p>This year we had fewer participating groups (19 groups) compared to last year (25 groups) but more than the inaugural year of the track (15 groups) likely due to the delay in releasing the v2 collection. However, we received a larger number of runs this year compared to previous years (75 runs in 2019 and 123 runs in 2020), although a larger number of baseline runs contributed towards that growth.</p><p>This year we asked participants to self-classify each of their runs under the following three categories (same taxonomy as was employed in our previous track overview papers <ref type="bibr" coords="5,294.83,303.56,83.51,8.64" target="#b2">[Craswell et al., 2020</ref><ref type="bibr" coords="5,385.31,303.56,25.31,8.64">[Craswell et al., , 2021a]]</ref>):</p><p>• trad: No neural representation learning-e.g., classical learning to rank, PRF, and BM25</p><p>• nn: Representation learning with text as input, but not using a pre-trained model • nnlm: Using a pre-trained model in any part of the pipeline-e.g., neural document expansion and BERT-style reranking</p><p>The largest category of runs was of type "nnlm" constituting 76% of submissions across both tasks this year. This was a significant increase over previous years-44% in 2019 and 57% in 2020-while the percentage of "trad" runs have remained relatively stable over the years-29% in 2019, 33% in 2020, and 24% in 2021. A significant shift also happened for the "nn" category over the years, decreasing from 27% in 2019 to 10% in 2020 and altogether disappearing as a category this year. This may reflect a convergence in the neural IR community, and the IR community in general, towards large language models, although whether this homogenization of approaches is healthy or premature is yet to be seen.</p><p>Participants were also asked to categorize their runs based on subtasks:</p><p>• Rerank: Reranking the official top-100 candidates • Fullrank: Full ranking from the collection (retrieval)</p><p>We observed an increase in the percentage of "fullrank" runs this year-79% compared to 72% in 2019 and 70% in 2020. The biggest increase in "fullrank" runs this year were for the passage ranking task-81% this year compared to 70% in 2019 and 69% in 2020-which may have been partially influenced by the reduction in size of the official reranking candidate set for the passage ranking task from 1000 (as in previous years) to 100 this year. The growing percentage of "fullrank" runs may also be due to increasing application of neural methods in the full retrieval settingeither using dense retrieval methods <ref type="bibr" coords="5,218.43,575.92,68.67,8.64" target="#b7">[Lee et al., 2019]</ref> or query term independent neural ranking models <ref type="bibr" coords="5,489.97,575.92,50.03,8.64;5,72.00,586.83,21.44,8.64" target="#b12">[Mitra et al., 2019]</ref>. Coincidentally, this year, we also asked participants to tell us (i) if their runs employed dense retrieval methods, and (ii) if the retrieval was performed in a single-stage under full retrieval setting.</p><p>Overall results Table <ref type="table" coords="5,169.63,620.76,4.98,8.64" target="#tab_1">2</ref> and Table <ref type="table" coords="5,219.48,620.76,4.98,8.64" target="#tab_2">3</ref> presents a standard set of relevance quality metrics for document and passage ranking runs, respectively, as we have reported for the track in previous years. We removed couple of runs with very low metric values, which may have been due to some issue with run generation, from our result tables and analysis plots. The reported metrics include Reciprocal Rank (RR) <ref type="bibr" coords="5,301.61,653.49,64.58,8.64" target="#b1">[Craswell, 2009]</ref>, Normalized Discounted Cumulative Gains (NDCG) <ref type="bibr" coords="5,110.01,664.40,127.61,8.64" target="#b5">[Järvelin and Kekäläinen, 2002]</ref>, Normalized Cumulative Gains (NCG) <ref type="bibr" coords="5,401.85,664.40,80.15,8.64" target="#b13">[Rosset et al., 2018]</ref>, and Average Precision (AP) <ref type="bibr" coords="5,133.87,675.31,47.80,8.64" target="#b16">[Zhu, 2004]</ref> computed using the NIST judgments. In addition, we also report RR computed based on the original sparse MS MARCO labels.</p><p>In subsequent discussions, we employ NDCG@10 as our primary evaluation metric to analyze ranking quality produced by different methods. To analyze how different approaches compare beyond just the relevance of top-ranked Figure <ref type="figure" coords="6,101.34,236.74,3.88,8.64">2</ref>: NDCG@10 results by run type. As in the previous two years, "nnlm" runs continue to outperform over "trad" runs for both tasks.</p><p>results, we use NCG@100 which correlates more with how often relevant results are in the top-100 candidate set even if they are not eventually ranked as highly as appropriate. We employ RR mostly for comparison between NIST and MS MARCO labels as the latter consists of binary judgments.</p><p>Neural vs. traditional methods. Figure <ref type="figure" coords="6,245.07,327.35,4.98,8.64">2</ref> summarizes the evaluation results by run type-i.e., comparing "nnlm" vs. "trad" runs. Across both document and passage ranking tasks, "nnlm" runs continue to significantly outperform "trad" runs this year. For the document ranking task, the best performing "nnlm" run improves NDCG@10 over the best performing "trad" run by 15% this year, compared to 29% in 2019 and 23% in 2020. On the other hand, for the passage ranking task, the NDCG@10 gap between the best performing run in 'nnlm" and "trad" categories is 36%, while the same was 38% in 2019 and 42% in 2020. Comparing percentage improvements across different year's tracks or across different tasks in the same year is not very meaningful due to differences in underlying data distributions. However, we still find it interesting that the percentage improvements this year are lower than previous years, and that the gap has been consistently bigger for the passage ranking task compared to the document ranking task for each year of the track.</p><p>Figure <ref type="figure" coords="6,101.21,441.92,4.98,8.64">3</ref> and 4 shows a query-level comparison between the best "nnlm" and "trad" runs for the document and the passage ranking tasks, respectively. The best "nnlm" run outperforms the best "trad run" on 41 out of 57 (72%) queries for the document ranking task-a drop-off from 84% as in the previous two years. For the passage ranking task, the best "nnlm" run wins on 47 out of 53 (89%) queries against the best "trad" run, which is marginally higher than 84% in 2019 and 88% in 2020.</p><p>End-to-end retrieval vs. reranking. This year for the document ranking task, the best "fullrank" run has a 4% NDCG@10 improvement over the best "rerank" run, compared to 2% in in 2019 and 5% in 2020. Similarly, for the passage task this year, the best "fullrank" run has 6% higher NDCG@10 than than the best "rerank" run, which we can compare with a 4% improvement in 2019 and no improvement in 2020. If we compare Figure <ref type="figure" coords="6,471.79,542.25,4.98,8.64">5</ref> (b) and (d), we notice that a stronger correlation between NDCG@10 and NCG@100 metrics. Also, this year the top-4 document ranking tasks and top-3 passage ranking tasks reported employing dense retrieval methods. However, the jury is still out on whether neural methods have demonstrated gains under the full retrieval setting that leads to significant overall improvement in ranking quality.</p><p>This year we also analyze the performance gap between single stage retrieval methods and approaches that involve multiple stages of rank-and-prune <ref type="bibr" coords="6,214.72,613.18,90.84,8.64" target="#b11">[Matveeva et al., 2006</ref><ref type="bibr" coords="6,305.56,613.18,83.73,8.64" target="#b14">, Wang et al., 2011]</ref>. We find that single-stage retrieval methods do surprisingly well but still have a reasonable gap with the top run for both document ranking task (6% worse on NDCG@10) and the passage ranking task (10% worse on NDCG@10).</p><p>Performance on long vs. short queries This year we used stratified sampling based on query length when selecting the queries for judging to analyze whether (1) the relative performance of "nnlm" and "trad" systems could change depending on query length, and 2) longer (hence, likely more difficult) queries are more discriminative in evaluating retrieval performance compared to shorter queries. Our test sets consisted of 57 queries (28 short (number of words &lt; 10), 29 long (number of words &gt;= 10) queries) for the document retrieval task and 53 (25 short, 28 long queries) queries for the passage retrieval task. We analyzed how the ranking of systems using long vs. short queries compare   Queries are sorted by difference in mean performance between "nnlm" and "trad" runs. Queries on which "nnlm" wins with large margin are at the top. 10 We order the runs by their NDCG@10 performance along the x-axis in all four plots. The best run for both tasks correspond to the "fullrank" setting.</p><p>with ranking of systems using all the queries in the test set. Focusing on NDCG@10 as the evaluation metric, in Figure <ref type="figure" coords="11,100.17,489.80,4.98,8.64">6</ref> we show the Kendall's τ correlation between the ranking of systems using (left) short vs. all queries, (middle) long vs. all queries, and (right) long vs. short queries for (top) document retrieval and (bottom) passage retrieval tasks.</p><p>It can be seen that evaluation results obtained solely using long queries tend to be more correlated with results obtained using all the queries, suggesting that longer queries could be more discriminative than shorter queries for system evaluation. This behavior tends to be stronger for "nnlm" systems compared to "trad" systems. The rightmost plots in the figure show that correlations between ranking of systems using long vs. short queries tend to be relatively low, suggesting that relative performance of systems could vary significantly depending on the query length. Note that test sets for both document retrieval and passage retrieval tasks contain a slightly higher number of long queries compared to short queries, which could be affecting the conclusions reached. In the future, we plan to do further analysis controlling for the effect of the different number of queries.</p><p>We further analyzed the number of relevant documents identified on average per query for long vs. short queries.</p><p>Longer queries tend to be more specific; hence, we were expecting to identify fewer relevant documents for longer queries compared to shorter queries. To our surprise, pools constructed for longer queries contained a higher number of relevant documents compared to shorter queries: 147.1 vs. 140.6 for document retrieval task and 67.8 vs. 61.2 for passage retrieval task. This could be another reason for the high correlation between evaluation results with long queries compared to all queries.</p><p>When absolute performance of "trad" systems across long vs. short queries are compared, it can be seen that "trad" systems tend to consistently perform worse on longer queries: the best performing trad run achieves 0.08 higher NDCG@10 score on shorter queries compared to longer queries on the document retrieval task (NDCG@10 score of 0.69 on shorter queries vs. 0.61 on longer queries), and 0.09 higher NDCG@10 on shorter queries for the passage  retrieval task (NDCG@10 score of 0.60 on shorter queries vs. 0.51 on longer queries). On the other hand, the absolute performance of "nnlm" systems do not seem to be much affected from the query length.</p><p>NIST labels vs. Sparse MS MARCO labels. The agreement between evaluation using sparse label RR (MS) and using full label NDCG@10 can be measured by comparing system ordering, for example calculating Kendall's Tau. Agreement is decreasing over the years of the track. In 2019, 2020 and 2021, the tau agreement on document ranking was 0.69, 0.46, and 0.43. On passage ranking 0.68, 0.69, and 0.51. This year's agreement plots can be seen in Figure <ref type="figure" coords="12,532.53,587.49,3.74,8.64" target="#fig_4">7</ref>.  One reason for reduced agreement could have been that the dataset creation process for the v2 MS MARCO data applied positive labels to irrelevant results. Unlike the document ranking dataset where we were able to transfer the v1 labels to the v2 collection using the document URLs as unique identifiers, for the passage dataset we had to transfer the labels based on matching the text content between v1 and v2 passages. The latter raises potential for increased label noise that could negatively impact model training on the v2 dataset. <ref type="bibr" coords="13,372.61,347.91,88.82,8.64" target="#b6">Lassance et al. [2021]</ref> reported that when they performed a manual reassessment for a small sample of training queries, they find evidence of significant false positives and negatives in the v2 labels. However, as a comparable analysis wasn't available for the v1 dataset, it is difficult to conclude how much of this may be explained by inter-annotator disagreement and general judgment noise.</p><p>Understanding the properties of the MS MARCO v2 passage qrels could allow us to develop better training procedures, as measured using the official test data generated by NIST. Another way of analyzing the qrels, since the 2021 passage qrels are the ones in question, is to compare the qrels to NIST qrels, as in Table <ref type="table" coords="13,389.02,418.84,3.74,8.64" target="#tab_3">4</ref>. The table compares all cases where an evaluation query-result pair had both an MS MARCO label and a NIST label. The analysis indicates that less than 20% of this year's sparse labels were assigned label 0 by NIST, which is in line with last year's numbers, we see no evidence from this that the positive labels have significantly decreased in quality in v2 data. For the document task, where the URL could be used to directly map the qrels, if anything the v2 data (2021) has better agreement between the two types of qrel. This could be because of the greatly improved quality of the document processing in the v2 data, giving documents with fewer character set issues, better extraction of main content and better layout. For the passage qrels, the 2021 data looks about the same or slightly worse, which could be understandable since the 2020 data is derived precisely from our human labeling task whereas 2021 data maps that label to a passage in the updated corpus.</p><p>Oldness Another reason for reduced Tau agreement could be that some systems are learning something specific about the sparse MS MARCO data that's not true of NIST labels. We do find some evidence of this. For this analysis, we divide the document corpus into old and new URLs. The old URLs are the ones that are included in both v1 and v2 data. The new URLs are those that are present in v2 only. Note that both old and new URLs have document content from the present day, so if models can tell the difference, it's not because the old URLs have old content. Our hope was that old and new URLs would be indistinguishable, since they all use a snapshot of content from the present day, and that's important because in our MS MARCO sparse labels, only old URLs can have a positive label.</p><p>There are 2.7 million old URLs out of a total 12 million URLs in the v2 data. We define the metric oldness@10, such that a random ranker has an expected oldness@10 of 2.7/12 = 0.225. However, because the v1 corpus was constructed based on the MS MARCO queries, and the new URLs were not selected based on those queries, we expect most runs to have higher oldness than a random run would. Because runs are paying attention to the query, and the old URLs have a higher density of relevant content. The question is whether some rankers learned, from training data, where all positive examples are old, to prefer old URLs. If so, their oldness@10 could be much higher than 0.225 Those systems with high oldness would also perform well on the MRR test data, since the sparse MRR test data also has the property that only old URLs can be positive. On the sparse MRR evaluation, returning a non-old document only has downside, since such documents can never be rewarded. Figure <ref type="figure" coords="14,100.51,282.37,4.98,8.64" target="#fig_5">8</ref> shows the metric disagreement plot for the document task. It is like the plot in Figure <ref type="figure" coords="14,450.23,282.37,3.74,8.64" target="#fig_4">7</ref>, but we colored each run according to its oldness. We can observe that most runs indeed have higher oldness than a random run. Some runs have very high oldness, of around 0.72. This suggests that the runs indeed learned to favor old documents. Those runs also perform particularly well on MRR, which only rewards old results, and this makes sense. These are top runs on MRR but not the best runs on NDCG@10. This is an artifact of the evaluation using sparse labels. We think that the runs with high NDCG@10 are the best runs.</p><p>To further dig into this, we create a version of every run that only uses old documents, and a version of the NIST qrels that only includes old documents. Of course, the MRR results are already old-only. We maintain the same coloring, to show which runs had high oldness before filtering. We call the after-filtering results the v1 universe, since it contains only the old documents and their associated qrels. The Tau agreement increases from 0.43 to 0.56, and the runs that had high oldness@10 are now in line with other runs. This suggests multiple options for future use of the v2 data. One is to have some evaluation in the v1 universe. Another is that when running the dev set MRR, this should be done on the v1 universe. Another is that during training, the v1 universe should be used. This would mean that the positive training examples are unchanged, they are still all old. But when sampling negative documents during training, we no longer have an old-new pair, teaching the model to have high oldness. Instead, all the training pairs are now old-old, and the model purely learns about relevance ranking.</p><p>Completeness of judgments An artifact of the passage collection being more than 15 times larger and the document collection being more than three times larger in the v2 datasets is that the number of relevant passages and documents for the test queries also grew correspondingly. As the NIST assessor budget has largely remained unchanged between the 2020 and the 2021 editions of the track, the increasing size of the relevance set lead to lower completeness of the NIST judgments. This prompted the track organizers to issue a warning to the participants about high proportions of unjudged below the shallow pooling cutoff and potential implications for reuse of this dataset for benchmarking outside of the TREC settings. One of the participating groups [Kamps and Rau, 2021] reported similar concerns as they noticed that the ratio of relevant-to-judged documents decreases much more gradually as we go down the ranks in the 2021 edition of the track compared to previous year.</p><p>Passage vs. document level judgments During the construction of the passage ranking and document ranking test collections, the two collections were judged completely independently. Previous work showed that it may be possible to infer document level labels from passage level labels <ref type="bibr" coords="14,301.65,620.76,66.99,8.64" target="#b15">Wu et al. [2019]</ref> since passages that are relevant are likely to occur on relevant documents. If such a phenomenon holds for the Deep Learning Track test collections, it could be possible to mainly focus annotation effort on obtaining passage level labels and use them to infer document level labels. This could result in having a more complete test collection for the passage retrieval task, which could also result in a more completely judged document collection.</p><p>In order to analyze how reliable this process would be, we used the passage level labels to infer document level labels. Some documents contain several judged passages, and we used the maximum relevance score across all judged passages in a document to infer the final label for the document. During this process some documents did not have any inferred labels assigned to them as 1) the test collection for the passage retrieval task contains 7% fewer queries Figure <ref type="figure" coords="15,100.81,311.86,3.88,8.64">9</ref>: NDCG@10 values when systems are evaluated using actual labels vs. inferred labels when (left) all documents with labels are considered for evaluation for the common queries and (right) only documents that are common across the two judgments sets are used for evaluation.</p><p>than the document retrieval task, and 2) not all documents judged for the document retrieval task contain a passage that was judged for the passage retrieval task.</p><p>Table <ref type="table" coords="15,96.38,389.79,4.98,8.64" target="#tab_4">5</ref> shows how the document labels inferred from passage labels compare with the actual document labels, where each entry in the table shows number of documents with a specific inferred (columns) vs. actual (rows) relevance label. Figure <ref type="figure" coords="15,125.15,411.61,4.98,8.64">6</ref> shows the percentage of documents with a particular inferred relevance label, conditioned on the actual relevance label, and Figure <ref type="figure" coords="15,182.18,422.52,4.98,8.64" target="#fig_4">7</ref> shows the percentage of documents with a particular actual relevance label, conditioned on the inferred relevance label. It can be seen that while there is a reasonable agreement between the inferred labels and actual labels, there are also some differences between the two sets of labels. The inter-annotator agreement between the two sets of labels is 0.468 according to Cohen's Kappa, suggesting a moderate agreement between the two sets of labels.</p><p>We next compared how the evaluation results using inferred labels compare with evaluations using actual labels.</p><p>Figure <ref type="figure" coords="15,100.94,493.45,4.98,8.64">9</ref> shows the NDCG@10 values of systems submitted to the document retrieval task, with x axis showing the metric value computed using the actual document labels compared to the metric value computed using the relevance labels inferred from passage level labels in the y axis. The left plot in the figure shows the evaluation results when all documents judged for the document retrieval task are used for computing the NDCG@10 value using actual judgments, ignoring the queries that were not included in the passage retrieval task. The right plot in the figure shows the results when the two evaluation results are computed on the intersection of the two judgments sets, ignoring the documents that do not have a corresponding inferred label. It can be seen that when the same documents are considered in evaluation, the two sets of judgments result in much higher agreement in terms of ranking of systems. This result suggests that while the document labels inferred from passage labels have the potential to be useful for evaluation, to compensate for the cases where no inferred label is available for a document (that should otherwise be annotated), we may need to obtain additional document level judgments. In the next year of the track, we might consider using such a hybrid evaluation dataset that contains a combination of inferred labels with actual labels in order to build a test collection that is more complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This is the third year of the TREC Deep Learning track. This year we refreshed both the document and the passage collections consequently also growing both collections significantly in size. We also continued to observe healthy participation in the track although the number of participating groups reduced slightly this year due to the delay in releasing the v2 collections. Deep learning models with large scale pretraining continued to outperform traditional retrieval methods, and single stage retrieval with deep models seems to gain some more ground this year. This report summarizes our analysis of submitted runs and the impact of the v2 collections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,72.00,707.41,468.00,8.64;10,72.00,718.32,468.00,8.64;10,72.00,729.23,148.50,8.64;10,301.02,743.40,9.96,8.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Comparison of the best "nnlm" and "trad" runs on individual test queries for the passage retrieval task. Queries are sorted by difference in mean performance between "nnlm" and "trad" runs. Queries on which "nnlm" wins with large margin are at the top. 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,72.00,397.36,468.00,8.64;11,72.00,408.27,468.00,8.64;11,72.00,418.86,467.99,8.96;11,72.00,430.09,166.02,8.64"><head></head><label></label><figDesc>Figure 5: Comparing "fullrank" and "rerank" runs on ranking quality. Figure (a) and (b) plots the NDCG@10 for different runs on the document and passage ranking tasks, respectively, and Figure (c) and (d) plot the NCG@100 for the same. We order the runs by their NDCG@10 performance along the x-axis in all four plots. The best run for both tasks correspond to the "fullrank" setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,72.00,291.40,468.00,8.96;12,72.00,302.63,425.14,8.64"><head></head><label></label><figDesc>Figure 6: Kendall's τ correlation between ranking of systems using (left) short vs. all queries, (middle) long vs. all queries, and (right) long vs. short queries for (top) document retrieval and (bottom) passage retrieval tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,105.20,477.57,401.61,8.64"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Agreement on evaluation outcomes when using sparse MS MARCO labels vs NIST labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="13,205.68,267.32,200.65,8.64"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Metric disagreement based on 'oldness'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,130.50,72.00,350.99,234.92"><head></head><label></label><figDesc></figDesc><graphic coords="3,130.50,72.00,350.99,234.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,143.34,80.12,322.84,109.61"><head>Table 1 :</head><label>1</label><figDesc>TREC 2021 Deep Learning Track run submission statistics.</figDesc><table coords="5,143.34,93.04,322.84,96.70"><row><cell></cell><cell cols="2">Document ranking Passage ranking</cell></row><row><cell>Number of groups</cell><cell>15</cell><cell>15</cell></row><row><cell>Number of total runs</cell><cell>66</cell><cell>63</cell></row><row><cell>Number of baseline runs</cell><cell>20</cell><cell>17</cell></row><row><cell>Number of runs w/ category: nnlm</cell><cell>48</cell><cell>50</cell></row><row><cell>Number of runs w/ category: nn</cell><cell>0</cell><cell>0</cell></row><row><cell>Number of runs w/ category: trad</cell><cell>18</cell><cell>13</cell></row><row><cell>Number of runs w/ category: rerank</cell><cell>15</cell><cell>12</cell></row><row><cell>Number of runs w/ category: fullrank</cell><cell>51</cell><cell>51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,83.31,126.62,443.63,543.49"><head>Table 2 :</head><label>2</label><figDesc>Summary of results for document ranking runs.</figDesc><table coords="7,83.31,141.00,443.63,529.11"><row><cell></cell><cell>group</cell><cell>subtask</cell><cell>neural</cell><cell>stage</cell><cell>dense ret.</cell><cell>RR (MS)</cell><cell>RR</cell><cell>NDCG@10</cell><cell>NCG@100</cell><cell>AP</cell></row><row><cell>pash_doc_f1</cell><cell>PASH</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.3027</cell><cell>0.9795</cell><cell>0.7437</cell><cell>0.5037</cell><cell>0.3111</cell></row><row><cell>pash_doc_f4</cell><cell>PASH</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2999</cell><cell>0.9795</cell><cell>0.7404</cell><cell>0.5983</cell><cell>0.3498</cell></row><row><cell>pash_doc_f5</cell><cell>PASH</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.3018</cell><cell>0.9795</cell><cell>0.7368</cell><cell>0.5992</cell><cell>0.3521</cell></row><row><cell>d_f10_mdt53b</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.4297</cell><cell>0.9883</cell><cell>0.7256</cell><cell>0.5162</cell><cell>0.2837</cell></row><row><cell>NLE_D_v1</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2536</cell><cell>1.0000</cell><cell>0.7215</cell><cell>0.5564</cell><cell>0.3133</cell></row><row><cell>uogTrDDQt5</cell><cell>uogTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3044</cell><cell>0.9737</cell><cell>0.7201</cell><cell>0.4849</cell><cell>0.2963</cell></row><row><cell>pash_doc_r3</cell><cell>PASH</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3022</cell><cell>0.9772</cell><cell>0.7164</cell><cell>0.4376</cell><cell>0.2672</cell></row><row><cell>pash_doc_r1</cell><cell>PASH</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3362</cell><cell>0.9772</cell><cell>0.7150</cell><cell>0.4376</cell><cell>0.2665</cell></row><row><cell>pash_doc_r2</cell><cell>PASH</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3198</cell><cell>0.9772</cell><cell>0.7076</cell><cell>0.4376</cell><cell>0.2640</cell></row><row><cell>d_f10_mt53b</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.4009</cell><cell>0.9605</cell><cell>0.7024</cell><cell>0.5162</cell><cell>0.2767</cell></row><row><cell>d_fusion00</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.2451</cell><cell>0.9630</cell><cell>0.7003</cell><cell>0.5226</cell><cell>0.2887</cell></row><row><cell>uogTrBaseDDQC</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2663</cell><cell>0.9649</cell><cell>0.6966</cell><cell>0.4849</cell><cell>0.2869</cell></row><row><cell>NLE_D_V1andV2</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2697</cell><cell>0.9503</cell><cell>0.6871</cell><cell>0.5372</cell><cell>0.2969</cell></row><row><cell>d_fusion10</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.2951</cell><cell>0.9423</cell><cell>0.6831</cell><cell>0.4752</cell><cell>0.2518</cell></row><row><cell>bcai_bertm1_ens</cell><cell>bcai</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.4793</cell><cell>0.9444</cell><cell>0.6812</cell><cell>0.4974</cell><cell>0.2559</cell></row><row><cell>CIP_run2</cell><cell>CIP</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3429</cell><cell>0.9373</cell><cell>0.6783</cell><cell>0.4376</cell><cell>0.2478</cell></row><row><cell>bl_bcai_wloo_d</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.4681</cell><cell>0.9230</cell><cell>0.6762</cell><cell>0.4752</cell><cell>0.2534</cell></row><row><cell>CIP_run1</cell><cell>CIP</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3668</cell><cell>0.9505</cell><cell>0.6755</cell><cell>0.4376</cell><cell>0.2445</cell></row><row><cell>max-firstp-pass</cell><cell>CFDA_CLIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.3015</cell><cell>0.9363</cell><cell>0.6727</cell><cell>0.4383</cell><cell>0.2321</cell></row><row><cell>maxp-firstp</cell><cell>CFDA_CLIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.2642</cell><cell>0.9436</cell><cell>0.6709</cell><cell>0.4308</cell><cell>0.2278</cell></row><row><cell>parade_bm25</cell><cell>mpii</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3395</cell><cell>0.9592</cell><cell>0.6707</cell><cell>0.4376</cell><cell>0.2490</cell></row><row><cell>maxp</cell><cell>CFDA_CLIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.2535</cell><cell>0.9538</cell><cell>0.6672</cell><cell>0.4306</cell><cell>0.2308</cell></row><row><cell>bl_bcai_nn_rtr</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.2865</cell><cell>0.9630</cell><cell>0.6671</cell><cell>0.5044</cell><cell>0.2749</cell></row><row><cell>CIP_run3</cell><cell>CIP</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3350</cell><cell>0.9567</cell><cell>0.6668</cell><cell>0.4376</cell><cell>0.2457</cell></row><row><cell>d_f10_mdt5base</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.4198</cell><cell>0.9401</cell><cell>0.6606</cell><cell>0.4758</cell><cell>0.2376</cell></row><row><cell>d_tct0</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.2203</cell><cell>0.9455</cell><cell>0.6537</cell><cell>0.4562</cell><cell>0.2418</cell></row><row><cell>ielab-roberta1d</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2137</cell><cell>0.9591</cell><cell>0.6522</cell><cell>0.3484</cell><cell>0.2039</cell></row><row><cell>TUW_IDCM_S4</cell><cell>TU_Vienna</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3325</cell><cell>0.9115</cell><cell>0.6494</cell><cell>0.4376</cell><cell>0.2460</cell></row><row><cell>watdrf</cell><cell>Waterloo_Cormack</cell><cell>rerank</cell><cell>trad</cell><cell>multi</cell><cell>yes</cell><cell>0.1736</cell><cell>0.9693</cell><cell>0.6467</cell><cell>0.4218</cell><cell>0.2533</cell></row><row><cell>TUW_IDCM_ALL</cell><cell>TU_Vienna</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3461</cell><cell>0.9092</cell><cell>0.6455</cell><cell>0.4376</cell><cell>0.2460</cell></row><row><cell>ielab-roberta2d</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2624</cell><cell>0.9470</cell><cell>0.6431</cell><cell>0.3484</cell><cell>0.2040</cell></row><row><cell>ielab-AD-uni-d</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.2377</cell><cell>0.9684</cell><cell>0.6424</cell><cell>0.4754</cell><cell>0.2492</cell></row><row><cell>ielab-uniCOIL-d</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.2377</cell><cell>0.9684</cell><cell>0.6424</cell><cell>0.4754</cell><cell>0.2492</cell></row><row><cell>doc_full_100</cell><cell>ALIBABA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.4022</cell><cell>0.9308</cell><cell>0.6414</cell><cell>0.5021</cell><cell>0.2465</cell></row><row><cell>doc_full_100e</cell><cell>ALIBABA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.4022</cell><cell>0.9308</cell><cell>0.6414</cell><cell>0.5021</cell><cell>0.2465</cell></row><row><cell>uogTrDot5pmp</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2152</cell><cell>0.9386</cell><cell>0.6411</cell><cell>0.3540</cell><cell>0.2077</cell></row><row><cell>watdrd</cell><cell>Waterloo_Cormack</cell><cell>rerank</cell><cell>trad</cell><cell>multi</cell><cell>yes</cell><cell>0.1737</cell><cell>0.9520</cell><cell>0.6411</cell><cell>0.4376</cell><cell>0.2529</cell></row><row><cell>Fast_Forward_2</cell><cell>L3S</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1959</cell><cell>0.9247</cell><cell>0.6338</cell><cell>0.5279</cell><cell>0.2773</cell></row><row><cell>d_unicoil0</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.1967</cell><cell>0.9088</cell><cell>0.6325</cell><cell>0.4784</cell><cell>0.2495</cell></row><row><cell>watdrp</cell><cell>Waterloo_Cormack</cell><cell>rerank</cell><cell>trad</cell><cell>multi</cell><cell>yes</cell><cell>0.1818</cell><cell>0.9386</cell><cell>0.6307</cell><cell>0.4218</cell><cell>0.2419</cell></row><row><cell>parade_h3</cell><cell>mpii</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3064</cell><cell>0.9488</cell><cell>0.6295</cell><cell>0.5058</cell><cell>0.2590</cell></row><row><cell>Fast_Forward_5</cell><cell>L3S</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2165</cell><cell>0.9373</cell><cell>0.6282</cell><cell>0.5143</cell><cell>0.2730</cell></row><row><cell>d_tct1</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.2754</cell><cell>0.9190</cell><cell>0.6269</cell><cell>0.3568</cell><cell>0.1794</cell></row><row><cell>dseg_bm25rm3</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.1582</cell><cell>0.9018</cell><cell>0.6185</cell><cell>0.5232</cell><cell>0.2933</cell></row><row><cell>doc_rank_100</cell><cell>ALIBABA</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.3835</cell><cell>0.9392</cell><cell>0.6175</cell><cell>0.4376</cell><cell>0.2168</cell></row><row><cell>bl_bcai_trad</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>0.2083</cell><cell>0.9276</cell><cell>0.6136</cell><cell>0.4735</cell><cell>0.2494</cell></row><row><cell>watdff</cell><cell>Waterloo_Cormack</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>yes</cell><cell>0.1164</cell><cell>0.9211</cell><cell>0.6060</cell><cell>0.5319</cell><cell>0.2961</cell></row><row><cell>ielab-TILDEv2d</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1935</cell><cell>0.8829</cell><cell>0.6022</cell><cell>0.4335</cell><cell>0.2262</cell></row><row><cell>maxp_h3</cell><cell>mpii</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2772</cell><cell>0.9313</cell><cell>0.6017</cell><cell>0.4596</cell><cell>0.2198</cell></row><row><cell>NLE_D_quick</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.1816</cell><cell>0.9024</cell><cell>0.6015</cell><cell>0.4344</cell><cell>0.2198</cell></row><row><cell>bigrams_cont_qe</cell><cell>CERTH_ITI_M4D</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.1488</cell><cell>0.8751</cell><cell>0.5941</cell><cell>0.5178</cell><cell>0.2957</cell></row><row><cell>bigram_qe_cedr</cell><cell>CERTH_ITI_M4D</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.1730</cell><cell>0.8872</cell><cell>0.5920</cell><cell>0.5273</cell><cell>0.2802</cell></row><row><cell>webis-dl-3</cell><cell>Webis</cell><cell>rerank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>0.3205</cell><cell>0.9488</cell><cell>0.5918</cell><cell>0.4376</cell><cell>0.2305</cell></row><row><cell>Fast_Forward_7</cell><cell>L3S</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2180</cell><cell>0.8952</cell><cell>0.5905</cell><cell>0.4880</cell><cell>0.2525</cell></row><row><cell>webis-dl-1</cell><cell>Webis</cell><cell>rerank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>0.3409</cell><cell>0.9356</cell><cell>0.5831</cell><cell>0.4376</cell><cell>0.2254</cell></row><row><cell>dseg_bm25</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.2066</cell><cell>0.8937</cell><cell>0.5776</cell><cell>0.4709</cell><cell>0.2436</cell></row><row><cell>webis-dl-2</cell><cell>Webis</cell><cell>rerank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>0.2693</cell><cell>0.9396</cell><cell>0.5747</cell><cell>0.4376</cell><cell>0.2224</cell></row><row><cell>uogTrBaseDD</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.2019</cell><cell>0.8297</cell><cell>0.5704</cell><cell>0.4849</cell><cell>0.2487</cell></row><row><cell>uogTrBaseDDQ</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.2019</cell><cell>0.8297</cell><cell>0.5704</cell><cell>0.4849</cell><cell>0.2487</cell></row><row><cell>watdfd</cell><cell>Waterloo_Cormack</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>yes</cell><cell>0.1189</cell><cell>0.8895</cell><cell>0.5615</cell><cell>0.4833</cell><cell>0.2504</cell></row><row><cell>watdfp</cell><cell>Waterloo_Cormack</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>yes</cell><cell>0.1263</cell><cell>0.8874</cell><cell>0.5580</cell><cell>0.4561</cell><cell>0.2351</cell></row><row><cell>d_bm25rm3</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.1240</cell><cell>0.7994</cell><cell>0.5339</cell><cell>0.4614</cell><cell>0.2453</cell></row><row><cell>d_bm25</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.1647</cell><cell>0.8367</cell><cell>0.5116</cell><cell>0.4376</cell><cell>0.2126</cell></row><row><cell>uogTrBaseDDQpmp</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.1795</cell><cell>0.8316</cell><cell>0.5105</cell><cell>0.3972</cell><cell>0.2029</cell></row><row><cell>uogTrBaseDDpmp</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.1788</cell><cell>0.8563</cell><cell>0.5070</cell><cell>0.3594</cell><cell>0.1769</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,83.52,138.57,443.23,519.58"><head>Table 3 :</head><label>3</label><figDesc>Summary of results for passage ranking runs.</figDesc><table coords="8,83.52,152.96,443.23,505.20"><row><cell></cell><cell>group</cell><cell>subtask</cell><cell>neural</cell><cell>stage</cell><cell>dense ret.</cell><cell>RR (MS)</cell><cell>RR</cell><cell>NDCG@10</cell><cell>NCG@100</cell><cell>AP</cell></row><row><cell>pash_f1</cell><cell>PASH</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2058</cell><cell>0.8732</cell><cell>0.7494</cell><cell>0.5249</cell><cell>0.3193</cell></row><row><cell>pash_f2</cell><cell>PASH</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2068</cell><cell>0.8737</cell><cell>0.7494</cell><cell>0.5721</cell><cell>0.3318</cell></row><row><cell>pash_f3</cell><cell>PASH</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2068</cell><cell>0.8737</cell><cell>0.7494</cell><cell>0.5882</cell><cell>0.3378</cell></row><row><cell>NLE_P_v1</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2079</cell><cell>0.8697</cell><cell>0.7347</cell><cell>0.6172</cell><cell>0.3923</cell></row><row><cell>pash_r2</cell><cell>PASH</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1939</cell><cell>0.8678</cell><cell>0.7076</cell><cell>0.3862</cell><cell>0.2389</cell></row><row><cell>pash_r3</cell><cell>PASH</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.1939</cell><cell>0.8675</cell><cell>0.7072</cell><cell>0.3862</cell><cell>0.2385</cell></row><row><cell>yorku21_a</cell><cell>yorku</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.1903</cell><cell>0.8629</cell><cell>0.6965</cell><cell>0.5456</cell><cell>0.3309</cell></row><row><cell>pash_r1</cell><cell>PASH</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2359</cell><cell>0.8663</cell><cell>0.6951</cell><cell>0.3862</cell><cell>0.2362</cell></row><row><cell>yorku21_c</cell><cell>yorku</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.1794</cell><cell>0.8393</cell><cell>0.6930</cell><cell>0.5413</cell><cell>0.3323</cell></row><row><cell>mono_electra_h3</cell><cell>mpii</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2538</cell><cell>0.7887</cell><cell>0.6753</cell><cell>0.5286</cell><cell>0.2880</cell></row><row><cell>NLE_P_V1andV2</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2127</cell><cell>0.7756</cell><cell>0.6724</cell><cell>0.5834</cell><cell>0.3381</cell></row><row><cell>ielab-AD-uni</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.1618</cell><cell>0.8045</cell><cell>0.6714</cell><cell>0.5239</cell><cell>0.2842</cell></row><row><cell>p_f10_mdt53b</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.3728</cell><cell>0.7871</cell><cell>0.6617</cell><cell>0.3961</cell><cell>0.2089</cell></row><row><cell>ielab-robertav1</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1851</cell><cell>0.8144</cell><cell>0.6551</cell><cell>0.3862</cell><cell>0.2246</cell></row><row><cell>uogTrPot5</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.1931</cell><cell>0.8160</cell><cell>0.6517</cell><cell>0.3980</cell><cell>0.2499</cell></row><row><cell>bcai_p_vbert</cell><cell>bcai</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2846</cell><cell>0.7957</cell><cell>0.6502</cell><cell>0.4296</cell><cell>0.2323</cell></row><row><cell>ihsm_colbert64</cell><cell>IHSM</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1498</cell><cell>0.8463</cell><cell>0.6453</cell><cell>0.3862</cell><cell>0.2148</cell></row><row><cell>pass_full_1000</cell><cell>ALIBABA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2830</cell><cell>0.7695</cell><cell>0.6434</cell><cell>0.4238</cell><cell>0.2413</cell></row><row><cell>ielab-uniCOIL</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.1873</cell><cell>0.7975</cell><cell>0.6420</cell><cell>0.5207</cell><cell>0.2745</cell></row><row><cell>p_f10_mt53b</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.3125</cell><cell>0.7700</cell><cell>0.6394</cell><cell>0.3961</cell><cell>0.1992</cell></row><row><cell>ihsm_bicolbert</cell><cell>IHSM</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1868</cell><cell>0.7962</cell><cell>0.6393</cell><cell>0.3862</cell><cell>0.2111</cell></row><row><cell>bcai_p_mbert</cell><cell>bcai</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2902</cell><cell>0.7765</cell><cell>0.6363</cell><cell>0.4600</cell><cell>0.2487</cell></row><row><cell>ihsm_poly8q</cell><cell>IHSM</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1468</cell><cell>0.8233</cell><cell>0.6342</cell><cell>0.3862</cell><cell>0.2059</cell></row><row><cell>ielab-robertav2</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2231</cell><cell>0.7645</cell><cell>0.6226</cell><cell>0.3862</cell><cell>0.2000</cell></row><row><cell>p_f10_mdt5base</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.3360</cell><cell>0.7486</cell><cell>0.6193</cell><cell>0.3711</cell><cell>0.1819</cell></row><row><cell>mono_h3</cell><cell>mpii</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2455</cell><cell>0.7267</cell><cell>0.6115</cell><cell>0.4964</cell><cell>0.2548</cell></row><row><cell>NLE_P_quick</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.1141</cell><cell>0.7342</cell><cell>0.6087</cell><cell>0.4884</cell><cell>0.2443</cell></row><row><cell>pass_full_1000e</cell><cell>ALIBABA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2772</cell><cell>0.7339</cell><cell>0.6071</cell><cell>0.4676</cell><cell>0.2425</cell></row><row><cell>mono_d3</cell><cell>mpii</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2493</cell><cell>0.7341</cell><cell>0.6037</cell><cell>0.4051</cell><cell>0.2150</cell></row><row><cell>bl_bcai_wloo_p</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2108</cell><cell>0.7691</cell><cell>0.6029</cell><cell>0.4098</cell><cell>0.2077</cell></row><row><cell>p_fusion10</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.1776</cell><cell>0.7854</cell><cell>0.5857</cell><cell>0.4098</cell><cell>0.1855</cell></row><row><cell>ielab-TILDEv2</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1489</cell><cell>0.6926</cell><cell>0.5825</cell><cell>0.4511</cell><cell>0.2112</cell></row><row><cell>p_unicoil0</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.1333</cell><cell>0.6788</cell><cell>0.5785</cell><cell>0.4408</cell><cell>0.2165</cell></row><row><cell>p_fusion00</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.1706</cell><cell>0.7783</cell><cell>0.5713</cell><cell>0.4675</cell><cell>0.2005</cell></row><row><cell>yorku21_b</cell><cell>yorku</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.1099</cell><cell>0.7180</cell><cell>0.5694</cell><cell>0.4112</cell><cell>0.2032</cell></row><row><cell>TUW_TAS-B_768</cell><cell>TU_Vienna</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.0723</cell><cell>0.7333</cell><cell>0.5619</cell><cell>0.4666</cell><cell>0.2093</cell></row><row><cell>Fast_ForwardP_2</cell><cell>L3S</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1754</cell><cell>0.6486</cell><cell>0.5521</cell><cell>0.4579</cell><cell>0.1998</cell></row><row><cell>watprp</cell><cell>Waterloo_Cormack</cell><cell>rerank</cell><cell>trad</cell><cell>multi</cell><cell>yes</cell><cell>0.1127</cell><cell>0.6827</cell><cell>0.5493</cell><cell>0.3862</cell><cell>0.1728</cell></row><row><cell>Fast_Forward_3</cell><cell>L3S</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1644</cell><cell>0.6401</cell><cell>0.5451</cell><cell>0.4547</cell><cell>0.1927</cell></row><row><cell>TUW_TAS-B_ANN</cell><cell>TU_Vienna</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.0842</cell><cell>0.7015</cell><cell>0.5426</cell><cell>0.4595</cell><cell>0.1932</cell></row><row><cell>pass_rank_100</cell><cell>ALIBABA</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.2161</cell><cell>0.6588</cell><cell>0.5389</cell><cell>0.3862</cell><cell>0.1781</cell></row><row><cell>bl_bcai_p_nn_rt</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.1844</cell><cell>0.6852</cell><cell>0.5245</cell><cell>0.3926</cell><cell>0.1691</cell></row><row><cell>watprf</cell><cell>Waterloo_Cormack</cell><cell>rerank</cell><cell>trad</cell><cell>multi</cell><cell>yes</cell><cell>0.1051</cell><cell>0.6050</cell><cell>0.5184</cell><cell>0.3862</cell><cell>0.1631</cell></row><row><cell>Fast_ForwardP_5</cell><cell>L3S</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.1482</cell><cell>0.6034</cell><cell>0.5132</cell><cell>0.4294</cell><cell>0.1728</cell></row><row><cell>top1000</cell><cell>UAmsterdam</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.0411</cell><cell>0.6566</cell><cell>0.5104</cell><cell>0.2387</cell><cell>0.1217</cell></row><row><cell>p_tct0</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.1780</cell><cell>0.6574</cell><cell>0.5001</cell><cell>0.3552</cell><cell>0.1332</cell></row><row><cell>TUW_DR_Base</cell><cell>TU_Vienna</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.0956</cell><cell>0.6768</cell><cell>0.4991</cell><cell>0.3675</cell><cell>0.1540</cell></row><row><cell>watpfp</cell><cell>Waterloo_Cormack</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>yes</cell><cell>0.0934</cell><cell>0.5985</cell><cell>0.4950</cell><cell>0.4044</cell><cell>0.1738</cell></row><row><cell>uogTrBasePDQ</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.1602</cell><cell>0.5611</cell><cell>0.4747</cell><cell>0.3980</cell><cell>0.1724</cell></row><row><cell>watprd</cell><cell>Waterloo_Cormack</cell><cell>rerank</cell><cell>trad</cell><cell>multi</cell><cell>yes</cell><cell>0.1163</cell><cell>0.6240</cell><cell>0.4698</cell><cell>0.3862</cell><cell>0.1442</cell></row><row><cell>uogTrBasePD</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.1617</cell><cell>0.5610</cell><cell>0.4619</cell><cell>0.3642</cell><cell>0.1439</cell></row><row><cell>uogTrPC</cell><cell>uogTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.0733</cell><cell>0.6119</cell><cell>0.4611</cell><cell>0.1295</cell><cell>0.0855</cell></row><row><cell>p_tct1</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.1550</cell><cell>0.5703</cell><cell>0.4499</cell><cell>0.3118</cell><cell>0.1159</cell></row><row><cell>p_bm25rm3</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.1371</cell><cell>0.4925</cell><cell>0.4480</cell><cell>0.3989</cell><cell>0.1632</cell></row><row><cell>p_bm25</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.1277</cell><cell>0.5060</cell><cell>0.4458</cell><cell>0.3862</cell><cell>0.1357</cell></row><row><cell>WLUPassage</cell><cell>WLU</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.0909</cell><cell>0.5682</cell><cell>0.4432</cell><cell>0.3862</cell><cell>0.1348</cell></row><row><cell>watpff</cell><cell>Waterloo_Cormack</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>yes</cell><cell>0.0722</cell><cell>0.5104</cell><cell>0.4408</cell><cell>0.3915</cell><cell>0.1346</cell></row><row><cell>bl_bcai_p_trad</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>0.1493</cell><cell>0.5086</cell><cell>0.4261</cell><cell>0.3435</cell><cell>0.1133</cell></row><row><cell>WLUPassage1</cell><cell>WLU</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.0453</cell><cell>0.4886</cell><cell>0.4093</cell><cell>0.3862</cell><cell>0.1170</cell></row><row><cell>paug_bm25</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.0848</cell><cell>0.5303</cell><cell>0.3977</cell><cell>0.3116</cell><cell>0.0977</cell></row><row><cell>paug_bm25rm3</cell><cell>BASELINES</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>0.0676</cell><cell>0.4906</cell><cell>0.3906</cell><cell>0.3152</cell><cell>0.1050</cell></row><row><cell>watpfd</cell><cell>Waterloo_Cormack</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>yes</cell><cell>0.0690</cell><cell>0.4833</cell><cell>0.3672</cell><cell>0.2535</cell><cell>0.0688</cell></row></table><note coords="9,72.00,707.41,468.00,8.64;9,72.00,718.32,468.00,8.64;9,72.00,729.23,148.50,8.64;9,303.51,743.40,4.98,8.64"><p><p><p>Figure</p>3</p>: Comparison of the best "nnlm" and "trad" runs on individual test queries for the document retrieval task. Queries are sorted by difference in mean performance between "nnlm" and "trad" runs. Queries on which "nnlm" wins with large margin are at the top. 9</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,180.15,628.88,249.21,87.48"><head>Table 4 :</head><label>4</label><figDesc>NIST labels on positive MS MARCO qrels.</figDesc><table coords="12,180.15,643.38,249.21,72.99"><row><cell></cell><cell cols="2">NIST label</cell><cell></cell><cell></cell><cell cols="2">% NIST label</cell><cell></cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell cols="3">2020 docs 11 12 7</cell><cell cols="5">16 24% 26% 15% 35%</cell></row><row><cell>2020 pass 7</cell><cell cols="7">12 15 23 12% 21% 26% 40%</cell></row><row><cell>2021 docs 4</cell><cell cols="4">11 11 35 7%</cell><cell cols="3">18% 18% 57%</cell></row><row><cell>2021 pass 7</cell><cell cols="7">12 13 14 15% 26% 28% 30%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,151.32,80.12,309.37,74.49"><head>Table 5 :</head><label>5</label><figDesc>Number of documents that have a particular inferred vs. actual label.</figDesc><table coords="14,236.50,90.64,136.51,63.97"><row><cell></cell><cell cols="2">inferred rel</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>actual rel</cell><cell cols="4">0 1096 186 49 1 194 561 277 58 12 2 100 298 531 157 3 49 99 190 425</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,72.00,179.59,468.00,74.49"><head>Table 6 :</head><label>6</label><figDesc>Percentage of documents with a particular inferred relevance grade, conditioned on the actual relevance label.</figDesc><table coords="14,216.85,190.11,175.81,63.97"><row><cell></cell><cell></cell><cell>inferred rel</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>actual rel</cell><cell cols="4">0 81.6% 13.8% 3.6% 1 17.8% 51.5% 25.4% 5.3% 0.9% 2 9.2% 27.4% 48.9% 14.5% 3 6.4% 13.0% 24.9% 55.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,72.00,80.12,468.00,74.49"><head>Table 7 :</head><label>7</label><figDesc>Percentage of documents with a particular actual relevance grade, conditioned on the inferred relevance label.</figDesc><table coords="15,216.85,90.64,175.81,63.97"><row><cell></cell><cell></cell><cell>inferred rel</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>actual rel</cell><cell cols="4">0 76.2% 16.3% 4.7% 1 13.5% 49.0% 26.5% 8.9% 1.8% 2 6.9% 26.0% 50.7% 24.1% 3 3.4% 8.7% 18.1% 65.2%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="16,72.00,132.41,468.00,8.64;16,81.96,143.32,458.04,8.64;16,81.96,154.05,193.32,8.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="16,276.80,143.32,263.20,8.64;16,81.96,154.23,26.39,8.64">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,72.00,168.95,445.87,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,135.35,169.13,83.65,8.64">Mean reciprocal rank</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,237.42,168.95,137.41,8.59">Encyclopedia of Database Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1703" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,184.02,468.00,8.64;16,81.96,194.93,83.55,8.64" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="16,413.04,184.02,126.96,8.64;16,81.96,194.93,53.81,8.64">Overview of the trec 2019 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,209.82,468.00,8.64;16,81.96,220.73,26.84,8.64;16,72.00,235.63,468.00,8.64;16,81.96,246.36,298.87,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,350.40,209.82,185.86,8.64;16,400.12,235.63,139.88,8.64;16,81.96,246.54,124.04,8.64">Ms marco: Benchmarking ranking models in the large-data regime</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,224.67,246.36,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1566" to="1576" />
		</imprint>
	</monogr>
	<note>Overview of the trec 2020 deep learning track</note>
</biblStruct>

<biblStruct coords="16,72.00,261.43,468.00,8.64;16,81.96,272.16,428.12,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,499.43,261.43,40.58,8.64;16,81.96,272.34,253.85,8.64">Trec deep learning track: Reusable test collections in the large data regime</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,354.48,272.16,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2369" to="2375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,287.05,465.90,8.82;16,72.00,301.95,417.78,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,194.99,287.23,199.56,8.64;16,188.55,302.13,231.36,8.64">University of amsterdam at trec 2021: Deep learning track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,72.00,302.13,108.33,8.64;16,438.02,301.95,21.48,8.59">Jaap Kamps and David Rau</title>
		<imprint>
			<date type="published" when="2002">2002. 2021</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
		</imprint>
	</monogr>
	<note>TREC</note>
</biblStruct>

<biblStruct coords="16,72.00,317.02,468.00,8.64;16,81.96,327.75,208.65,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,466.96,317.02,73.05,8.64;16,81.96,327.93,138.04,8.64">Naver labs europe (splade) @ trec deep learning 2021</title>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thibault</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnaud</forename><surname>Sors</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,238.86,327.75,21.48,8.59">TREC</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,342.82,468.00,8.64;16,81.96,353.55,458.04,8.82;16,81.96,364.64,72.23,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,297.66,342.82,242.34,8.64;16,81.96,353.73,39.09,8.64">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,143.65,353.55,366.17,8.59">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,379.54,468.00,8.64;16,81.96,390.27,458.04,8.82;16,81.96,401.35,26.84,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,401.11,379.54,138.89,8.64;16,81.96,390.45,309.58,8.64">Significant improvements over the state of the art? a case study of the ms marco document ranking leaderboard</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,412.19,390.27,47.23,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2283" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,416.25,468.00,8.64;16,81.96,426.98,458.04,8.82;16,81.96,437.89,458.03,8.82;16,81.96,448.98,458.04,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,494.14,416.25,45.87,8.64;16,81.96,427.16,391.29,8.64">Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,490.65,426.98,49.35,8.59;16,81.96,437.89,458.03,8.82;16,81.96,448.98,11.83,8.64">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;21</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2356" to="2362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,463.87,468.00,8.64;16,81.96,474.60,365.64,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,392.51,463.87,147.50,8.64;16,81.96,474.78,270.97,8.64">Fostering coopetition while plugging leaks: The design and implementation of the ms marco leaderboards</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,371.41,474.60,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,489.67,468.00,8.64;16,81.96,500.40,458.04,8.82;16,81.96,511.31,213.28,8.82" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,390.09,489.67,149.92,8.64;16,81.96,500.58,50.79,8.64">High accuracy retrieval with multiple nested ranker</title>
		<author>
			<persName coords=""><forename type="first">Irina</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Burkard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Laucius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leon</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,149.31,500.40,390.69,8.59;16,81.96,511.31,92.44,8.59">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="437" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,526.39,468.00,8.64;16,81.96,537.12,458.04,8.82;16,81.96,548.03,100.17,8.82" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="16,485.78,526.39,54.23,8.64;16,81.96,537.30,393.00,8.64">Incorporating query term independence assumption for efficient retrieval and ranking using deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Corby</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03693</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,72.00,563.10,468.00,8.64;16,81.96,573.83,347.47,8.82" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,396.79,563.10,143.21,8.64;16,81.96,574.01,150.65,8.64">Optimizing query evaluations using reinforcement learning for web search</title>
		<author>
			<persName coords=""><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damien</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,250.98,573.83,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1193" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,588.72,468.00,8.82;16,81.96,599.63,458.04,8.82;16,81.96,610.72,89.54,8.64" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,259.25,588.90,213.55,8.64">A cascade ranking model for efficient ranked retrieval</title>
		<author>
			<persName coords=""><forename type="first">Lidan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,490.65,588.72,49.35,8.59;16,81.96,599.63,427.57,8.59">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,625.61,468.00,8.64;16,81.96,636.52,374.42,8.64" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,345.34,625.61,194.66,8.64;16,81.96,636.52,150.14,8.64">Investigating passage-level relevance and its role in document-level relevance judgment</title>
		<author>
			<persName coords=""><forename type="first">Zhijing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,240.15,636.52,37.14,8.64">SIGIR&apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,651.24,468.00,8.82;16,81.96,662.15,127.40,8.82" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="16,115.84,651.42,159.98,8.64">Recall, precision and average precision</title>
		<author>
			<persName coords=""><forename type="first">Mu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">30</biblScope>
			<pubPlace>Waterloo, 2</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics and Actuarial Science, University of Waterloo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
