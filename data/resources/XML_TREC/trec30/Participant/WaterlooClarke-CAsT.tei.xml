<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,185.05,164.85,241.13,15.12;1,198.26,186.77,214.73,15.12">WaterlooClarke at the TREC 2021 Conversational Assistant Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-06">November 6, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.13,219.25,49.09,10.48"><forename type="first">Xinyi</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName coords="1,233.98,219.25,103.95,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
						</author>
						<author>
							<persName coords="1,347.03,219.25,89.08,10.48"><forename type="first">Negar</forename><surname>Arabzadeh</surname></persName>
						</author>
						<title level="a" type="main" coord="1,185.05,164.85,241.13,15.12;1,198.26,186.77,214.73,15.12">WaterlooClarke at the TREC 2021 Conversational Assistant Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-06">November 6, 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">C0EE334414C1AA14EE981D6EA23F7325</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For TREC 2021, the WaterlooClarke group submitted three runs to TREC Conversational Assistance Track (CAsT):</p><formula xml:id="formula_0" coords="1,148.71,340.00,70.59,49.81">• clarke-auto • clarke-cc • clarke-manual</formula><p>The three runs were based on raw utterances, canonical response, and manually rewritten utterances respectively. This report describes the generation and the results of each of these runs.</p><p>The overall approach consists of three steps: 1) query reformulation, 2) passage retrieval, and 3) passage reranking. We did not apply the query reformulation step for the clarke-manual run as manually rewritten utterances were used. In order to improve our performance, this year we focused our effort on maximizing the total system recall at the first stage by employing both dense and sparse retrievers. Research has shown that sparse retrievers and dense retrievers can retrieve complementary information <ref type="bibr" coords="1,349.60,508.04,9.96,8.74" target="#b2">[3]</ref>. We merged the retrieved passages into a single pool, then reranked this pool using a two-stage reranking pipeline with monoT5 and duoT5 <ref type="bibr" coords="1,285.93,531.95,9.96,8.74" target="#b3">[4]</ref>. In the next section, we will explain the details of our methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Reformulation</head><p>Similar to last year, the year 3 conversations are multi-turn, contextually dependent and informally expressed. To incorporate essential information from previous context and resolve issues such as co-reference and word omissions, we trained a T5 model on the QReCC dataset <ref type="bibr" coords="1,319.22,654.92,12.64,8.74" target="#b0">[1]</ref> to reformulate raw queries.</p><p>In general, query reformulation task is defined as follows. Given a conversational context c = [u 0 , p 0 , u 1 , p 1 , ..., u i-1 , p i-1 ], and current query u i , the goal is to use the context to rewrite u i as ûi where ûi is context-independent and would be consumed directly by retrievers.</p><p>Based on experiments on the TREC CAsT 2019 and 2020 datasets, we chose to use previous rewritten utterances [ û0 , û1 , ..., ûi-1 ] and the last system response p i-1 as context to rewrite u i :</p><formula xml:id="formula_1" coords="2,150.64,212.33,127.86,7.86">ûi = T 5( û0, û1, ..., ûi-1, pi-1, ui)</formula><p>The clarke-auto run and the clarke-cc run both used query reformulation technique. The difference is the choice of the system response passage p i-1 . In clarke-auto, we used top five passages retrieved purely by our system as context to rewrite next queries, whereas in clarke-cc the provided canonical results were used. While the T5 re-writer works surprisingly well on the CAsT 2019 and 2020 datasets, we do observe that the model more often fails to find the omitted information or detect topic shift in CAsT 2021, which suggests CAsT 2021 tasks are harder than before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval</head><p>This year we utilized the BERT-based dense retriever ANCE <ref type="bibr" coords="2,409.13,353.57,10.52,8.74" target="#b4">[5]</ref> and a tuned BM25 with pseudo-relevance feedback to retrieve passages for a set of rewritten queries at very first stage. BM25 parameters were tuned to maximize recall@1000 over the 2019 and 2020 tasks using the manually formulated questions from those years. The pseudo-relevance feedback step executed queries over both the target corpus and the much larger C4 corpus developed to train T5<ref type="foot" coords="2,447.55,411.77,3.97,6.12" target="#foot_0">1</ref> .Then we merged the results into a single pool that would be reranked in the next stage. Our experiments on the CAsT 2019 and 2020 datasets have shown that broadening the pool of retrieved passages can lead to a significant improvement in the performance <ref type="bibr" coords="2,219.62,461.16,9.96,8.74" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Passage Reranking</head><p>On all the three runs, we applied a pointwise reranker monoT5, followed by a pairwise reranker duoT5 <ref type="bibr" coords="2,244.81,519.40,9.96,8.74" target="#b3">[4]</ref>. According to recent research and experiments in passage ranking tasks such as MSMARCO, neural rerankers can significantly improve the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Table <ref type="table" coords="2,161.34,598.07,4.98,8.74" target="#tab_0">1</ref> compare our three runs based on ndcg@3, ndcg@5, recall@500 and recall@R, where the former three are the main measures 2 used by the 2021 CAsT track, and R is the number of relevant documents for that query. Not surprisingly, the use of manual rewrites led to enhanced performance, which suggested that there is room left for improvements in the query reformulation stage. In addition, we investigated how much the use of different first stage retrievers would contribute to the final scores. Table <ref type="table" coords="3,324.62,151.87,4.98,8.74" target="#tab_1">2</ref> shows the results when manually resolved queries were used. We repeated the same analysis for canonical runs and raw runs, which were demonstrated in Table <ref type="table" coords="3,357.75,175.78,4.98,8.74" target="#tab_2">3</ref> and<ref type="table" coords="3,387.21,175.78,32.17,8.74" target="#tab_3">Table 4</ref>. In general, adding pseudo-relevance feedback and combining different first stage retrievers improved the system performance and further implied that dense retrievers and sparse retrievers could be combined together to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>ndcg@3 ndcg@5 recall@500 recall@R clarke-auto 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In TREC CAsT 2021, our team experimented with a tuned BM25 along with the dense retriever ANCE to expand the pool of retrieved documents at very first stage using a set of rewritten queries. The results have shown that our query reformulation methods can be further improved, possible future directions including detecting topic shifts in the conversations and better resolving coreferences. In addition, we are interested in studying how to make use of hybrid retrievers more smartly, one example is to select between different retrievers First-stage Retrievers ndcg@3 ndcg@5 recall@500 recall@R Baseline BM25</p><p>N based on the query alone <ref type="bibr" coords="4,245.22,246.32,9.96,8.74" target="#b2">[3]</ref>. We look forward to participating in TREC CAsT 2022.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,133.77,246.26,300.57,143.03"><head>Table 1 :</head><label>1</label><figDesc>Comparing the results of our three submitted runs</figDesc><table coords="3,139.75,246.26,294.59,143.03"><row><cell></cell><cell>3753</cell><cell>0.3685</cell><cell>0.6760</cell><cell>0.2576</cell></row><row><cell>clarke-cc</cell><cell>0.5137</cell><cell>0.5107</cell><cell>0.8303</cell><cell>0.3549</cell></row><row><cell>clarke-manual</cell><cell>0.6440</cell><cell>0.6382</cell><cell>0.8894</cell><cell>0.4537</cell></row><row><cell cols="6">First-stage Retrievers ndcg@3 ndcg@5 recall@500 recall@R</cell></row><row><cell>Baseline BM25</cell><cell></cell><cell>0.5952</cell><cell>0.5882</cell><cell>0.7459</cell><cell>0.4210</cell></row><row><cell>BM25</cell><cell></cell><cell>0.6464</cell><cell>0.6381</cell><cell>0.8083</cell><cell>0.4530</cell></row><row><cell>BM25 + C4</cell><cell></cell><cell>0.6409</cell><cell>0.6375</cell><cell>0.8378</cell><cell>0.4545</cell></row><row><cell>ANCE</cell><cell></cell><cell>0.6284</cell><cell>0.6157</cell><cell>0.7843</cell><cell>0.4400</cell></row><row><cell cols="2">BM25 + C4 + ANCE</cell><cell cols="2">0.6440 0.6382</cell><cell>0.8894</cell><cell>0.4537</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,133.77,392.91,343.71,117.93"><head>Table 2 :</head><label>2</label><figDesc>Comparing the results of manual runs when different first-stage re-</figDesc><table coords="3,133.77,404.87,300.57,105.97"><row><cell>trievers were used</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">First-stage Retrievers ndcg@3 ndcg@5 recall@500 recall@R</cell></row><row><cell>Baseline BM25</cell><cell>0.4357</cell><cell>0.4265</cell><cell>0.6168</cell><cell>0.3067</cell></row><row><cell>BM25</cell><cell cols="2">0.5166 0.5073</cell><cell>0.7933</cell><cell>0.3482</cell></row><row><cell>BM25 + C4</cell><cell>0.5076</cell><cell>0.5007</cell><cell>0.8015</cell><cell>0.3481</cell></row><row><cell>ANCE</cell><cell>0.5105</cell><cell>0.5031</cell><cell>0.7650</cell><cell>0.3461</cell></row><row><cell>BM25 + C4 + ANCE</cell><cell>0.5083</cell><cell>0.4996</cell><cell>0.8254</cell><cell>0.3496</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,133.77,514.46,343.71,20.69"><head>Table 3 :</head><label>3</label><figDesc>Comparing the results of canonical runs when different first-stage retrievers were used</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,133.77,139.12,343.71,82.46"><head>Table 4 :</head><label>4</label><figDesc>Comparing the results of raw runs when different first-stage retrievers were used</figDesc><table coords="4,139.75,139.12,294.59,58.15"><row><cell></cell><cell>/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>BM25</cell><cell>0.3708</cell><cell>0.3578</cell><cell>0.5798</cell><cell>0.2516</cell></row><row><cell>BM25 + C4</cell><cell>0.3553</cell><cell>0.3471</cell><cell>0.5739</cell><cell>0.2377</cell></row><row><cell>ANCE</cell><cell>0.3724</cell><cell>0.3674</cell><cell>0.5975</cell><cell>0.2490</cell></row><row><cell cols="3">BM25 + C4 + ANCE 0.3753 0.3685</cell><cell>0.6760</cell><cell>0.2576</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,149.01,653.44,177.83,6.64"><p>https://huggingface.co/datasets/allenai/c4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,149.01,662.37,126.11,6.99"><p>Evaluation is based on documents</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,154.25,313.04,323.23,8.74;4,154.25,324.99,295.32,9.02" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,261.68,313.04,215.80,8.74;4,154.25,324.99,136.06,8.74">Open-Domain Question Answering Goes Conversational via Question Rewriting</title>
		<author>
			<persName coords=""><forename type="first">Raviteja</forename><surname>Anantha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04898[cs.IR</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,154.25,340.93,323.23,8.74;4,154.25,352.89,189.20,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,356.35,340.93,121.13,8.74;4,154.25,352.89,158.67,8.74">WaterlooClarke at the Trec 2020 Conversational Assistant Track</title>
		<author>
			<persName coords=""><forename type="first">Negar</forename><surname>Arabzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,154.25,368.83,323.23,8.74;4,154.25,380.78,323.23,8.74;4,154.25,392.74,196.02,9.02" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,415.37,368.83,62.11,8.74;4,154.25,380.78,323.23,8.74;4,154.25,392.74,37.09,8.74">Predicting Efficiency/Effectiveness Trade-offs for Dense vs. Sparse Retrieval Strategy Selection</title>
		<author>
			<persName coords=""><forename type="first">Negar</forename><surname>Arabzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinyi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10739[cs.IR</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,154.25,408.68,323.23,8.74;4,154.25,420.63,323.23,8.74;4,154.25,432.59,187.39,9.02" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="4,387.83,408.68,89.65,8.74;4,154.25,420.63,323.23,8.74;4,154.25,432.59,27.92,8.74">The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models</title>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05667[cs.IR</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,154.25,448.53,323.23,8.74;4,154.25,460.49,281.37,9.02" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,223.69,448.53,253.79,8.74;4,154.25,460.49,122.41,8.74">Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00808</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cs.IR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
