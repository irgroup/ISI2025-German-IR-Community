<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.43,115.96,290.49,12.62">CIP at TREC 2021 Deep Learning Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.20,153.79,65.03,8.74"><forename type="first">Xuanang</forename><surname>Chen</surname></persName>
							<email>chenxuanang19@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.12,153.79,32.25,8.74"><forename type="first">Ben</forename><surname>He</surname></persName>
							<email>benhe@ucas.ac.cn</email>
						</author>
						<author>
							<persName coords="1,315.26,153.79,30.59,8.74"><surname>Le Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.77,153.79,50.92,8.74"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
							<email>yfsun@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.43,115.96,290.49,12.62">CIP at TREC 2021 Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6162573CCF5958FD1EC4BFBED8483469</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the CIP participation in the TREC 2021 Deep Learning Track. Akin to our previous participation, we adopt the passage-level BERT re-ranker in the re-ranking subtask of the document ranking task. Besides, we utilize the MS MARCO v1 passage dataset and both the MS MARCO v2 passage and document datasets to generate hopefully sufficient training data, and BERT re-ranker is finetuned on these three kinds of training data one by one. Meanwhile, we adopt pairwise hinge loss rather than pointwise cross-entropy loss this year for model training, to boost the ranking effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CIP participation in the TREC 2021 Deep Learning (DL) track focuses on the re-ranking subtask of the document ranking task. In the TREC 2021 DL track, a new, larger and cleaner corpus MS MARCO v2 is released, and it unifies the passage and document datasets. Based on our participation <ref type="bibr" coords="1,448.95,443.31,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,460.93,443.31,7.75,8.74" target="#b1">2]</ref> in the TREC 2019 and 2020 DL tracks, we remain to use the (passage-level) BERT re-ranker <ref type="bibr" coords="1,177.07,467.22,10.52,8.74" target="#b2">[3]</ref> as our main neural ranking model, but we adopt the pairwise hinge loss <ref type="bibr" coords="1,152.70,479.17,10.52,8.74" target="#b3">[4]</ref> rather than the cross-entropy loss for model training this year. Besides, we still use the MS MARCO v1 passage dataset as an important part of our training data as it provides large-scale ready-made training triples (a query, a positive passage, and a negative passage). Before being fine-tuned on the MS MARCO v2 document dataset, the BERT re-ranker has been trained on the passage ranking datasets (MS MARCO v1 and v2) as in <ref type="bibr" coords="1,313.27,538.95,9.96,8.74" target="#b1">[2]</ref>. And, K-Max-AvgP <ref type="bibr" coords="1,417.27,538.95,10.52,8.74" target="#b1">[2]</ref> still acts as an effective option for the document score aggregation under the MS MARCO v2 corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The passage-level BERT re-ranker has been widely-used in the document ranking task <ref type="bibr" coords="1,171.83,632.21,7.75,8.74" target="#b4">[5]</ref><ref type="bibr" coords="1,179.58,632.21,3.87,8.74" target="#b5">[6]</ref><ref type="bibr" coords="1,183.45,632.21,7.75,8.74" target="#b6">[7]</ref>, and it produces a relevance score for every passage in a document and then use these scores of passages to aggregate the relevance score of this document, e.g. MaxP <ref type="bibr" coords="1,233.94,656.12,9.96,8.74" target="#b4">[5]</ref>, K-Max-AvgP <ref type="bibr" coords="1,314.13,656.12,9.96,8.74" target="#b1">[2]</ref>. The input format of passage-level</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT re-ranker is [CLS] [query] [SEP] [passage]</head><p>[SEP], and the final pooled hidden vector of the [CLS] token is fed into a single layer feed-forward network to obtain the probability (score) of the passage being relevant. We use the pairwise hinge loss to fine-tune the BERT re-ranker, because it can produce better re-ranking effectiveness than the cross-entropy loss in our experiments. Akin to <ref type="bibr" coords="2,146.64,178.77,9.96,8.74" target="#b1">[2]</ref>, we adopt a passage filter step after splitting documents and preserve topranked passages in a relevant document to construct the training data in the MS MARCO v2 document dataset. But we preserve all passages for the validation and test set, to completely judge the relevance of query-document pairs. overlapping words), which are all retained without any filter step. As for the test set, the official top-100 candidates are re-ranked and are processed in the same way as the validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>As for BERT re-ranker, we adopt the pre-trained BERT-Large model (bertlarge-uncased) <ref type="bibr" coords="3,201.92,500.70,9.96,8.74" target="#b7">[8]</ref>, and it is fine-tuned on above three kinds of training data as described in Section 3.1 with the training order of Passage v1, Passage v2 and Document v2. The query has up to 32 tokens, and the concatenation of query, passage, separator tokens has the maximum length of 256 tokens. For the document ranking task, we use MaxP or K-Max-AvgP aggregation method, and K is set as 4 in our submitted runs. However, for the passage ranking during model validation using Passage v1 and v2, there is no score aggregation.</p><p>According to the used training data and the final aggregation way for test set in TREC 2021 DL track, we summarize our three submitted runs in Table <ref type="table" coords="3,451.38,596.34,3.87,8.74" target="#tab_0">1</ref>.</p><p>We carry out our experiments on three TITAN RTX 24G GPUs with Mixed Precision Training <ref type="bibr" coords="3,217.22,620.25,9.96,8.74" target="#b8">[9]</ref>. We use Adam optimizer with a weight decay of 0.01, and the learning rate is set as 3e-6 for the whole fine-tuning procedure with batch size of 32. Besides, the BERT re-ranker is trained for 1 epoch using Passage v1 and Passage v2, and 2 epochs using Document v2. The margin in hinge ranking loss is set as 1 in all training steps. We save a checkpoint per 5,000 training steps, and select the checkpoint according to the best NDCG@10 in Passage v1 and Document v2, and the best MRR@10 in Passage v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The evaluation results of our submitted runs for document re-ranking subtask are shown in Table <ref type="table" coords="4,223.91,204.19,3.87,8.74" target="#tab_1">2</ref>. For a more comprehensive comparison, we also present the validation results on test queries from both TREC 2019 and 2020 DL test sets. From the above results, we find that CIP run2 outperforms other two runs on both Validation 2 and TREC 2021 DL test sets in terms of NDCG@10, and meanwhile CIP run1 behaves better than other two runs on Validation 1 set in terms of NDCG@10. But CIP run3 behaves better than other two runs on TREC 2021 DL test set in terms of P@10 and MRR. Thus, the submitted three runs seem comparable to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we describe the system based on BERT model for the document re-ranking subtask in TREC 2021 Deep Learning track. Our experiments demonstrate again that the BERT re-ranker fine-tuned on passage dataset can be transferred to the document ranking task effectively. Meanwhile, K-Max-AvgP aggregation method behaves better than MaxP when using the passage-level BERT re-ranker for document ranking task. In future work, we plan to investigate the uses of passage-document mapping released in the MS MARCO v2 corpus, like how to use the mapped passage information for document ranking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,271.70,50.48,8.77;2,134.77,292.28,345.82,8.77;2,134.77,304.26,345.82,8.74;2,134.77,316.22,217.52,8.74;2,140.99,336.63,339.60,8.77;2,151.70,348.62,328.89,8.74;2,140.99,360.67,339.60,8.77;2,151.70,372.66,328.89,8.74;2,151.70,384.61,328.89,8.74;2,151.70,396.57,224.27,8.74;2,140.99,408.62,339.60,8.77;2,151.70,420.61,328.89,8.74;2,151.70,432.56,328.89,8.74;2,151.70,444.52,328.89,8.74;2,151.70,456.47,328.89,8.74;2,151.70,468.43,328.89,8.74;2,151.70,480.38,328.89,8.74;2,151.70,492.34,328.89,8.74;2,151.70,504.29,328.89,8.74;2,151.70,516.25,328.89,8.74;2,151.70,528.21,328.89,8.74;2,151.70,540.16,141.25,8.74;2,134.77,560.45,345.82,8.77;2,134.77,572.43,345.83,8.74;2,134.77,584.39,345.83,8.74;2,134.77,596.34,345.83,8.74;2,134.77,608.30,345.82,8.74;2,134.77,620.25,345.82,8.74;2,134.77,632.21,345.82,8.74;2,134.77,644.16,345.83,8.74;2,134.77,656.12,345.82,8.74"><head>3. 1</head><label>1</label><figDesc>Data Training data. Although the new MS MARCO v2 corpus is released, we still use the passage dataset in the MS MARCO v1 corpus. Thus, in our experiments, totally three datasets are used for model training: -Passage v1: In the MS MARCO v1 passage dataset, we sample about 4.2 M training triples from the provided training triple ids as our training data. -Passage v2: In the MS MARCO v2 passage dataset, as a lack of ready-made training triples, we sample negative passages from BM25 top list (namely, the top-500 to top-1000 ranked passages) for positive passages in the qrels file, and finally we get about 1.4 M training triples. -Document v2: In the MS MARCO v2 document dataset, we use the official provided top-100 file of train queries to generate the training data. Specifically, for a train query, the judged positive documents and the negative documents in top-100 list are split into up to 32 overlapping passages (100 whole words and 50 overlapping words), and the title (up to 16 words) and headings (up to 32 words) are added to the beginning of every passage if they are available. Then, for every passage in the positive documents, a negative passage is sampled from the passage pool of negative documents. Finally, a BERT re-ranker trained on Passage v1 training data is used to score all passages in the positive documents, and only five top-ranked passages are retained along with their sampled negative passages. Thus, here we finally get about 1.6 M training triples. Validation data. When BERT re-ranker is trained with Passage v1, we use 43 and 54 test queries in TREC 2019 and 2020 DL track, and re-rank BM25 top-1000 candidates. When BERT re-ranker is trained with Passage v2, we use official Dev set (3,903 queries) and official top-100 list for model validation. When BERT re-ranker is trained with Document v2, we use the provided two official validation sets (namely, 43 and 45 test queries from TREC 2019 and 2020 DL track) and the BM25 top-100 documents for model selection. If two kinds validation sets are used, we use the average metric of them. Besides, all top documents are split into overlapping passages (100 whole words and 50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,174.32,345.83,96.11"><head>Table 1 .</head><label>1</label><figDesc>The summary of submitted runs. Aggregation refers the way to get the score of a document according to the scores of its split passages during inference, MaxP means the max score of passages and 4-Max-AvgP means the average score of the top-4 ranked passages.</figDesc><table coords="3,173.43,229.27,268.73,41.16"><row><cell cols="2">Run ID Passage v1 Passage v2 Document v2 Aggregation</cell></row><row><cell>CIP run1</cell><cell>4-Max-AvgP</cell></row><row><cell>CIP run2</cell><cell>4-Max-AvgP</cell></row><row><cell>CIP run3</cell><cell>MaxP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.77,317.66,345.83,105.77"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results on TREC 2021 DL test queries in the document re-ranking subtask. Validation 1 (2) consists the 43 (45) test queries from TREC 2019 (2020) DL track, but under the MS MARCO v2 document corpus. The TREC 2021 DL test set consists of 57 queries. The best values are highlighted in boldface.</figDesc><table coords="3,154.11,371.31,307.14,52.12"><row><cell>Run ID</cell><cell cols="5">Validation 1 Validation 2 NDCG@10 NDCG@10 MAP NDCG@10 P@10 MRR TREC 2021 DL Test</cell></row><row><cell>CIP run1</cell><cell>0.4003</cell><cell>0.3675</cell><cell>0.2445</cell><cell>0.6755</cell><cell>0.8158 0.9505</cell></row><row><cell>CIP run2</cell><cell>0.3914</cell><cell>0.3732</cell><cell>0.2478</cell><cell>0.6783</cell><cell>0.8140 0.9373</cell></row><row><cell>CIP run3</cell><cell>0.3925</cell><cell>0.3718</cell><cell>0.2457</cell><cell>0.6668</cell><cell>0.8175 0.9567</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.35,481.81,342.24,7.86;4,146.91,492.77,333.68,7.86;4,146.91,503.73,106.24,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,291.08,481.81,170.82,7.86">UCAS at TREC-2019 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,146.91,492.77,135.54,7.86">TREC. NIST Special Publication</title>
		<imprint>
			<biblScope unit="volume">1250</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,514.63,342.25,7.86;4,146.91,525.59,333.68,7.86;4,146.91,536.55,106.24,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,297.15,514.63,164.76,7.86">ICIP at TREC-2020 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,146.91,525.59,135.54,7.86">TREC. NIST Special Publication</title>
		<imprint>
			<biblScope unit="volume">1266</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,547.42,342.25,7.89;4,146.91,558.41,25.60,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,245.34,547.45,125.27,7.86">Passage re-ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR abs/1901.04085</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,569.31,342.24,7.86;4,146.91,580.24,311.58,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,344.96,569.31,135.64,7.86;4,146.91,580.27,69.55,7.86">Large margin rank boundaries for ordinal regression</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,223.37,580.27,143.12,7.86">Advances in large margin classifiers</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="132" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,591.16,342.24,7.86;4,146.91,602.12,218.03,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,227.71,591.16,252.88,7.86;4,146.91,602.12,60.93,7.86">Deeper text understanding for IR with contextual neural language modeling</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,229.34,602.12,23.62,7.86">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,613.02,342.25,7.86;4,146.91,623.96,166.51,7.89" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="4,272.25,613.02,208.35,7.86;4,146.91,623.98,32.07,7.86">Simple applications of BERT for ad hoc document retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/1903.10972</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,634.88,342.24,7.86;4,146.91,645.84,333.68,7.86;4,146.91,656.80,197.16,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,262.98,634.88,217.61,7.86;4,146.91,645.84,153.06,7.86">Comparing score aggregation approaches for document retrieval with pretrained transformers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,322.73,645.84,23.04,7.86">ECIR</title>
		<imprint>
			<biblScope unit="volume">12657</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="163" />
			<date type="published" when="2021">2021</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,119.67,342.24,7.86;5,146.91,130.63,333.68,7.86;5,146.91,141.59,222.80,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,344.98,119.67,135.61,7.86;5,146.91,130.63,199.13,7.86">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,367.17,130.63,68.18,7.86">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,152.55,342.24,7.86;5,146.91,163.51,333.68,7.86;5,146.91,174.47,190.79,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,392.23,163.51,88.36,7.86;5,146.91,174.47,11.14,7.86">Mixed precision training</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,178.91,174.47,130.13,7.86">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
