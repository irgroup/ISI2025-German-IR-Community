<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.79,124.97,270.18,12.58">UNTIIA Lab at TREC 2021 -Clinical Trial</title>
				<funder ref="#_RKRHcB8">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_bbYC4GF">
					<orgName type="full">National Security Agency</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.03,157.85,64.21,8.74"><forename type="first">Huyen</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Information Access Laboratory</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.44,157.85,57.01,8.74"><forename type="first">Haihua</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Information Access Laboratory</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.66,157.85,61.80,8.74"><forename type="first">Bhanu</forename><surname>Prasad</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Information Access Laboratory</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.67,157.85,69.75,8.74"><forename type="first">Huanhuan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Bioinformatics</orgName>
								<orgName type="institution">University of Texas at El Paso</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,192.24,169.80,56.17,8.74"><forename type="first">Junhua</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Information Access Laboratory</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,265.63,169.80,68.21,8.74"><forename type="first">Jiangping</forename><surname>Chen</surname></persName>
							<email>jiangping.chen@unt.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.18,169.80,63.93,8.74"><forename type="first">Ana</forename><surname>Cleveland</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.79,124.97,270.18,12.58">UNTIIA Lab at TREC 2021 -Clinical Trial</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55825EA6F13F03791A99EB3209E3243B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Clinical Trial</term>
					<term>Information Retrieval</term>
					<term>ElasticSearch</term>
					<term>BM25</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports our participation in 2021 TREC Clinical Trial (CT) track based on the ElasticSearch information retrieval platform. We studied different query extraction and query expansion methods with both manual and automatic strategies for query construction. Our experiments on the 2016 Clinical Decision Support collection proved the effectiveness of the knowledge base mapping method for both query extraction and expansion. We proposed a novel query construction strategy to balance precision and accuracy: we retrieve clinical trials documents with a complete list of query terms first and then decrease the number of query terms used for searching additional documents to improve recall. We also investigated two transformer-based models for reranking: unsupervised and supervised learning. Pairs of query and candidate documents were encoded with the sentence-BERT in the unsupervised reranking model, and then their semantic similarity was compared using a Cross-encoder model. We also took advantage of BERT transformers for the supervised reranking model by finetuning the model on the ground truth of the 2016 Clinical Decision Support collection and then feeding it into the TFR-BERT model for reranking. Our experiment indicates that the unsupervised transformer reranking model outperformed the supervised learning model and achieved the highest performance</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The TREC Biomedical Tracks, aiming to improve the speed at which treatments are developed and disseminated into clinical practice <ref type="foot" coords="2,391.85,226.96,3.97,6.12" target="#foot_0">1</ref>  Compared to the previous tracks, the CT track is more challenging. The topic text, which is a patient case description that simulates an admission statement in an EHR, is much longer (5-10 sentences) with non-standardized English texts. In other words, the topics provide more information but contain noise that may significantly reduce the accuracy of the retrieval results. There are more topics (75 topics) than in previous years, which will need more computational resources for getting the results.</p><p>Our team participated in TREC medical Tracks in the past <ref type="bibr" coords="2,420.54,551.32,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,435.18,551.32,7.01,8.74" target="#b1">2]</ref>. One of the shared purposes of our participation has been to train graduates in information retrieval and retrieval systems. We have seen the growing demands from industries on computational professionals who are able to develop or implement information retrieval systems. Other purposes of our participation in this year's TREC include: (1) to explore and implement a new information retrieval (IR) platform for the Lab; (2) to investigate effective query extraction and construction to handle lengthy and noisy topics. For this purpose, we consider both manual and automatic topic processing methods and compare their retrieval performance; and (3) to explore the performance of various reranking models, especially deep learning-based reranking models for information retrieval.</p><p>We chose to build a high-performance IR system using ElasticSearch, an efficient search platform supporting large-scale distributed search and a userfriendly interface for indexing and retrieval. ElasticSearch has been applied by other TREC participants and is becoming more and more popular among the TREC community. As for the topics, we investigated different automatic query extraction and query expansion methods to handle lengthy topics: knowledgebase mapping, keyword extraction, and named entity extractions were used for automatic query extraction, while knowledge-base mapping and pseudo-relevant feedback were for query expansion. We conducted a group of experiments to select the best parameters, such as text encoding models, number of expanded terms, type of entities. We proposed an iterative search strategy that could balance the precision and recall for the ranking. We implemented two BERTbased algorithms for reranking: one is an unsupervised learning model based on sentence-BERT <ref type="bibr" coords="3,217.86,468.68,9.96,8.74" target="#b2">[3]</ref>, the other is a supervised learning model based on TFR-BERT <ref type="bibr" coords="3,165.65,486.62,9.96,8.74" target="#b3">[4]</ref>. The experiments show that the knowledge-base mapping method for query extraction and expansion generated the most effective queries. Our proposed search strategy produced a more inclusive candidate document list for reranking. Our experiments also show that unsupervised BERT Cross-encoder performs better than the TFR-BERT model for the reranking.</p><p>The rest of the paper is organized as follows: Section 2 presents the experimental design and the methodologies implemented for the 2021 TREC CT track. Section 3 describes the runs submitted. Section 4 presents the evaluation results and discussion. The paper concludes with a summary and an outlook of future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>Like our previous TREC participation <ref type="bibr" coords="4,317.31,154.86,10.52,8.74" target="#b0">[1]</ref> [2], the whole process of matching potential patients (queries) to clinical trials (the documents) includes four stages in this study: Collection Indexing, Query Processing, Document Retrieval, and Reranking. However, we applied different strategies from our previous participation at each stage. The four stages are depicted in Figure <ref type="figure" coords="4,417.19,226.59,4.98,8.74" target="#fig_1">1</ref> and will be explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Collection Indexing</head><p>The TREC 2021 clinical trial track document collection comprises 3.7 million clinical trial documents in XML format. Besides, we made use of the TREC 2016 clinical decision support track test collection to develop our retrieval system.</p><p>The two collections were separately indexed using ElasticSearch<ref type="foot" coords="4,418.09,347.56,3.97,6.12" target="#foot_1">2</ref> . As for the 2021 test document collection, all fields are indexed. However, we concatenated all content fields together for our retrieval convenience, including brief title, official title, brief summary text block, detailed description text block, primary outcome description, and secondary outcome description. We hope that the recall could be maximized by including all content fields. A similar indexing process was applied to the 2016 document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Topic processing</head><p>TREC 2021 clinical trial track consists of 75 topics in XML format. Their format is very different from the topics of previous precision medicine tracks.</p><p>TREC precision medicine 2019 and 2020 are structured into predetermined information types, such as disease, genetic variants, demographic information (in topics 2019), and disease, genetic variants, the proposed treatment in topics 2020. In contrast, topics of this year are in the format of admission notes which are long and narrative; they also contain many acronyms and abbreviations. Therefore, query processing is necessary to extract the important information from topics for retrieval.</p><p>The topics include a good deal of demographic information such as age, gender, and physical exams. They may also contain patients' past medical history, family history, and labs and diagnostics studies. By studying the topics, we select terms representing disease and symptom as keywords for retrieval. These terms are mentioned in the physical exams and diagnostics studies. Generally, our topic processing is for the following purposes: (1) to extract the most important clinical trial terms for searching (i.e., disease and symptom information is predetermined), ( <ref type="formula" coords="6,222.18,289.35,4.24,8.74">2</ref>) to expand the selected terms with their synonyms and broader terms to increase recall. Figure <ref type="figure" coords="6,310.95,307.29,4.98,8.74" target="#fig_2">2</ref> presents our topic processing strategies and methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Manual topic processing</head><p>Lexigram<ref type="foot" coords="7,189.56,145.31,3.97,6.12" target="#foot_2">3</ref> was used to extract candidate query lists that include disease and symptom. Lexigram is an online tool built on a knowledge graph, covering clinical entities such as drugs, diseases, and symptoms. Then, by referring to other available resources such as UMLS, Wikipedia, etc., and using our knowledge, we refined the candidate lists and extracted more terms from topics. The automatic KB extraction method identifies terms contained in knowledge bases such as UMLS and DrugBank. Inspired by <ref type="bibr" coords="7,348.61,385.00,9.96,8.74" target="#b4">[5]</ref>, <ref type="bibr" coords="7,365.06,385.00,9.96,8.74" target="#b5">[6]</ref>, <ref type="bibr" coords="7,381.51,385.00,10.52,8.74" target="#b6">[7]</ref> that achieved good performance in TREC 2019, 2016, and 2015, respectively, we used Metamap <ref type="bibr" coords="7,466.96,402.93,10.52,8.74" target="#b7">[8]</ref> for extracting terms related to disease and symptom. We specified two semantic types: Sign or Symptom, and Disease or Syndrome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Automatic topic processing</head><p>As for automatic keyword extraction, we used Yake! <ref type="bibr" coords="7,388.71,456.73,9.96,8.74" target="#b8">[9]</ref>, an unsupervised model based on text statistic features to select the most important words from the text. We experimented with different numbers of terms (k= 15, 14, 13, 12, 11, 10) and found that k=12 generated the highest-quality query lists. BERN <ref type="bibr" coords="7,180.94,528.46,14.61,8.74" target="#b9">[10]</ref>, a named entity recognition (NER) model, was implemented to identify the initial candidate entities. BERN is based on BioBERT <ref type="bibr" coords="7,430.20,546.39,14.61,8.74" target="#b10">[11]</ref>, a pretrained biomedical language representation model for biomedical text mining, to initially identify biomedical entities. The BERN model then removed the overlapping entities between entity types by applying multiple pre-defined rules.</p><p>We used BERN to extract disease and symptom entities from the topics.</p><p>We conducted a series of experiments with these methods on the 2016 test collection and, we found that the automatic KB method achieved the highest recall. Therefore, we selected the automatic KB method for implementation on 2021 Clinical Trial topics.</p><p>Query expansion. To enrich our queries with their variants, we performed two query expansions: Metamap automatic KB, and Pseudo Relevance Feedback (PRF). When extracting UMLS terms, Metamap also returns their synonyms and broad terms if they exist in UMLS. Queries extracted by this method are used for the first run. We also implemented PRF on top of ClinicalBERT <ref type="bibr" coords="8,461.98,281.38,15.50,8.74" target="#b11">[12]</ref> for query expansion. As shown in figure <ref type="figure" coords="8,319.12,299.32,3.87,8.74" target="#fig_8">3</ref>, using queries extracted manually (no query expansion), we first retrieved a set of documents and then selected titles of the top 20 documents for implementing PRF. All the three automatic information extraction methods mentioned previously were tested using the 2016 test collection for finding the best methods. We extracted all possible terms using automatic KB and NER methods, and retained a maximum of 10 terms from each document title by using automatic keyword extraction. Extracted terms from titles and queries were encoded with ClinicalBERT word embeddings <ref type="bibr" coords="8,133.77,442.78,14.61,8.74" target="#b12">[13]</ref>. ClinicalBERT was trained on top of the BERT model but further finetuned on MIMIC clinical notes; it yields a better performance on some Clinical NLP tasks <ref type="bibr" coords="8,159.56,478.65,14.61,8.74" target="#b12">[13]</ref>. Finally, we measured the similarity of extracted terms and queries using cosine similarity, in which we calculated the distance between document and query vector representations. We selected the top two terms from each document title with the highest similarity scores as expanded queries. Expanded queries obtained with this method were used for the fourth run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Document Retrieval</head><p>Accumulative Retrieval. We developed an optimal retrieval method, called accumulative retrieval, to balance precision and recall. We assumed that the more topic terms found in the document, the more relevant is the document to the query. Our ranking starts with a complete list of query terms to guarantee Algorithm 1 The Accumulative Retrieval.</p><p>1: Input: term list (m terms) of the initial query T 1 , term list (n terms) of the query expansion T 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>Step 1: retrieve documents using T 1 with Boolean "AND" strategy Retrieve the documents with the query 6:</p><p>Append the retrieved documents to the document list 7: end for 8:</p><p>Step 2: retrieve documents using T 1 + T 2 with Boolean "AND" strategy Retrieve the documents with the query 12:</p><p>Append the retrieved documents to the document list </p><formula xml:id="formula_0" coords="9,134.90,552.99,299.82,26.28">number of terms = m + n, query = t 1 OR t 2 OR . . . t ( m + n) 23:</formula><p>Retrieve the documents with the query 24:</p><p>Append the retrieved documents to the document list  Retrieval Model. In each step in Algorithm 1, Boolean model was combined with our accumulative retrieval method to retrieve documents which were then ranked with the BM25 model. As we observed, the BM25 model has been used</p><p>as a baseline by most TREC teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Reranking</head><p>Our study leveraged the transformer -a deep learning model-to develop two reranking models: unsupervised learning and supervised learning. The transformer adopts the attention mechanism for allowing to input sequential data in parallel. Recently, the transformer models have proven their incomparable performance in many NLP tasks. In the unsupervised learning model, we used cross-encoder/ms-marco-MiniLM-L-6-v2, a pre-trained sentence embedding model built on top of BERT transformer <ref type="bibr" coords="10,363.59,590.38,10.52,8.74" target="#b2">[3]</ref> to encode queries and documents, and further trained on MS MARCO, a large-scale information retrieval dataset. Each pair of the query and the document was encoded into 384-dimension dense vector space and then simultaneously passed to a transformer network, called Cross-Encoder. The model returned a score between 0 and 1, indicating the similarity between the query and document. We used this reranking model for the fourth run to rerank the initially retrieved documents returned by the second run. We used the relevance judgment of 2016 Clinical Decision Support to train our supervised reranking model. Like topics of this year, topics of the 2016 collection are curated EHR admission notes; therefore, the model trained on that dataset is likely applicable to this year's task. We implemented TFR-BERT <ref type="bibr" coords="11,133.77,253.49,10.52,8.74" target="#b3">[4]</ref> for this reranking model. BERT embedding model was used to represent query-document pairs in this model. Query-document pairs were concatenated to input into the BERT model as sentence pairs with the format:</p><formula xml:id="formula_1" coords="11,133.77,289.35,343.71,26.67">[CLS] query text [SEP] document text [SEP].</formula><p>The hidden units of the CLS token were fed into the TF-ranking model to finetuning using pointwise, pairwise, and listwise losses <ref type="bibr" coords="11,161.06,343.15,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Run Description</head><p>We submitted results from 5 experiments or runs. Our final runs are summarized as follows:</p><p>-UNTIIARUN1 (Baseline): Automatic knowledge-base extraction on topics and expansion with BM25. We used Metamap to extract the terms from the topics and obtained their synonyms and broader terms from UMLS<ref type="foot" coords="11,434.14,497.00,3.97,6.12" target="#foot_3">4</ref> . Finally, the retrieved documents were ranked with BM25 scores.</p><p>-UNTIIARUN2: Manual knowledge-base extraction and expansion with BM25. We extracted and expanded queries based on external knowledge bases such as UMLS, Lexigram for searching. Similarly, BM25 was used to rank our retrieved documents initially.</p><p>-UNTIIARUN3: Combining manual query extraction and ClinicalBERT-PRF query expansion. Queries extracted manually with knowledge bases were used as input of our ClinicalBERT-PRF query expansion model. Finally, we retrieved documents for this run using manually-extracted terms and PRFexpanded terms. Again, BM25 was used to rank our retrieved documents.</p><p>-UNTIIARUN4: Unsupervised learning reranking model. Our BERT Crossencoder reranking model reranked documents retrieved from UNTIIARUN2.</p><p>-UNTIIARUN5: Supervised learning reranking model. Our TFR-BERT reranking was trained on the relevance judgment of the 2016 Clinical Decision Support track. Similar to UNTIIARUN4, we used documents that were achieved from UNTIIARUN2 for the reranking task in this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Result</head><p>Our results are summarized in this section. Table <ref type="table" coords="12,375.81,319.24,4.98,8.74" target="#tab_0">1</ref> and Figure <ref type="figure" coords="12,437.10,319.24,4.98,8.74" target="#fig_9">4</ref> present results of our five submissions in terms of reciprocal rank, P@10, and NDCG@10, in comparison to the averages of the TREC medians. The result indicates that UNTIIARUN4 outperforms other runs, and other runs perform quite poorly.</p><p>Note that we applied an unsupervised reranking model for UNTIIARUN4.</p><p>However, compared to the TREC official median (manual runs) in each topic, our system performs well in a number of topics. Figure <ref type="figure" coords="12,375.88,426.84,4.98,8.74" target="#fig_10">5</ref> and figure <ref type="figure" coords="12,430.55,426.84,4.98,8.74" target="#fig_11">6</ref> show our performance on NDCG@10 compared to the TREC official median and best, respectively. For example, our UNTIIARUN4 performs better than the TREC official median of NDCG@10, reciprocal rank, and P@10 in a number of topics (14 topics, 28 topics, and 22 topics, respectively) (figure <ref type="figure" coords="12,388.80,498.57,3.87,8.74" target="#fig_10">5</ref>). Our results also indicate that we achieve the highest NDCG@10, reciprocal rank, and P@10 scores for certain topics; for example, our UNTIIARUN4 yields the highest reciprocal rank score for 19 topics, the highest NDCG@10 for 4 topics, and the highest P@10 for 3 topics (figure <ref type="figure" coords="12,279.92,570.30,3.87,8.74" target="#fig_11">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Our outperformed RUN4 indicates that our unsupervised learning reranking model is more effective than the supervised one. It significantly improves the initial retrieved document rank by 0.26, 0.16, and 0.27 on Reciprocal rank, P@10, and NDCG@10, respectively. In contrast, the supervised reranking model failed in this reranking task and even worsened the initial results (UNTIIARUN2).</p><p>The model was trained on the relevance judgments of the 2016 Clinical Decision Support track. Even though the topics of this track are in a similar format to topics of this year, documents are entirely different, i.e., scientific articles rather than clinical trials. The distinctions between training data and testing data could be one possible reason for the declined performance. In the future, we   are comparable with this year's documents.</p><p>The results also prove that our initial ranking model (i.e., BM25) is very limited. We used this ranking model for three first runs, which performed poorly, even for the UNTIIARUN2, in which queries were manually processed. However, the performance improved substantially by using the top 2000 documents from the UNTIIARUN2 for reranking with the UNTIIARUN4, meaning that the BM25 model failed to include and/or rank the most relevant documents in the top 1000 retrieved documents.</p><p>We did not achieve the performance that we expected. The IR system failed in topics 1, 9, 12, 21, 42, 46, and 47 with very low recalls. We would like to investigate the causes by observing processed queries of these topics in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents the methods and results of our participation in the TREC 2021 Clinical Trials track. First, we successfully implemented a new platform for our IR system; it proves suitable for use next year. Further, the results indicate that one of our reranking models performed effectively. This run exceeded the official TREC medians (automatic runs) and contributed the highest performance in many topics. However, the results showed that our topic processing methods are not effective as we expected. We will need to conduct more analysis to understand issues during the experiments. In the future, we would like to investigate other topic processing methods that can handle long topics better. We also want to improve our supervised reranking model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,396.32,228.53,80.88,8.74;2,133.77,246.46,343.49,8.74;2,133.77,264.40,343.42,8.74;2,133.77,282.33,343.49,8.74;2,133.77,300.26,343.44,8.74;2,133.77,318.20,343.38,8.74;2,133.77,336.13,343.37,8.74;2,133.77,354.06,343.44,8.74;2,133.77,371.99,343.37,8.74;2,133.77,389.93,343.37,8.74;2,133.77,407.86,58.71,8.74"><head></head><label></label><figDesc>, has been running for 19 years at the Text REtrieval Conference. From 2003-2007, the TREC Genomics track focused on genomics researchers seeking relevant biomedical literature; from 2011-2012, the TREC Medical Records track focused on retrieving cohorts of patients from electronic health records (EHRs); from 2014-2016, the Clinical Decision Support track focused on clinicians looking for evidencebased full-text literature to support diagnosis, treatment, and testing decisions; from 2017-2020, the Precision Medicine track focused on oncologists looking for evidence-based treatment literature and clinical trials. This year, the newly introduced Clinical Trials (CT) track focuses on matching patients to relevant clinical trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,204.09,619.65,203.07,6.99;5,135.55,124.80,340.18,476.13"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General Architecture of UNT IIA IR System</figDesc><graphic coords="5,135.55,124.80,340.18,476.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,238.21,600.99,134.83,6.99;6,135.55,345.92,340.17,236.35"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Query processing methods</figDesc><graphic coords="6,135.55,345.92,340.17,236.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,148.71,267.44,328.77,8.74;7,133.77,285.37,45.72,8.74;7,133.77,313.27,343.71,8.74;7,133.77,331.20,343.71,8.74;7,133.77,349.13,343.71,8.74"><head></head><label></label><figDesc>Automatic query processing includes two steps: query extraction and query expansion. Query extraction. Three different automatic information extraction methods, including automatic knowledge base (KB) extraction, keyword extraction, and named entity recognition (NER), were applied to compare the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,139.14,212.24,165.27,8.77;9,139.14,231.56,6.59,6.99;9,165.65,230.20,238.54,9.65;9,139.14,249.49,6.59,6.99"><head>3 : 4 :</head><label>34</label><figDesc>for iteration = m, m -1, . . . , 2 do number of terms = m, query = t 1 AND t 2 AND . . . t m 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,139.14,319.83,203.25,8.77;9,134.90,339.16,10.82,6.99;9,165.65,337.80,284.01,9.96;9,134.90,357.09,10.82,6.99"><head>9 :</head><label>9</label><figDesc>for iteration = m + n, m + n -1, . . . , 2 do 10: number of terms = m + n, query = t 1 AND t 2 AND . . . t ( m + n) 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,134.90,391.56,51.05,8.77;9,134.90,409.50,305.60,9.68;9,134.90,427.43,166.19,8.77;9,134.90,446.75,10.82,6.99;9,165.65,445.39,223.15,9.65;9,134.90,464.69,10.82,6.99;9,165.65,463.32,170.72,8.74;9,134.90,482.62,10.82,6.99;9,165.65,481.26,234.84,8.74;9,134.90,499.16,51.05,8.77;9,134.90,517.09,328.06,9.68;9,134.90,535.03,205.82,8.77;9,134.90,554.35,10.82,6.99"><head>13: end for 14 :</head><label>14</label><figDesc>Step 3: retrieve documents using T 1 with Boolean "OR" strategy 15: for iteration =m, m -1, . . . , 2 do 16: number of terms = m, query = t 1 OR t 2 OR . . . t m 17: Retrieve the documents with the query 18: Append the retrieved documents to the document list 19: end for 20: Step 4: retrieve documents using T 1 + T 2 with Boolean "OR" strategy 21: for iteration = m + n, m + n -1, . . . , 2 do 22:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,134.90,606.76,51.05,8.77;9,134.90,624.72,234.68,8.74;9,134.90,642.62,206.23,8.77"><head>25: end for 26 :</head><label>26</label><figDesc>Remove duplication of the retrieved document list 27: Output: top 1000 document for reranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,151.87,275.93,307.51,6.99;10,135.55,124.80,340.16,132.41"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pseudo Relevance Feedback (PRF) with ClinicalBERT word embeddings</figDesc><graphic coords="10,135.55,124.80,340.16,132.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="13,133.77,496.28,343.71,6.99;13,133.77,510.47,218.77,6.99;13,77.08,124.80,453.55,352.76"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: All submissions compared to the official median NDCG@10, reciprocal rank, and P@10 of manual runs (upper), and automatic runs (lower).</figDesc><graphic coords="13,77.08,124.80,453.55,352.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="14,133.77,588.35,343.71,6.99;14,133.77,602.55,48.28,6.99;14,91.25,303.84,425.21,265.79"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: NDCG@10 of all the submitted runs compared to the official median NDCG@10 of manual runs.</figDesc><graphic coords="14,91.25,303.84,425.21,265.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="15,133.77,351.82,343.71,6.99;15,133.77,366.02,62.86,6.99;15,91.25,124.80,425.19,208.31"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: NDCG@10 of our best run compared to the official median, and best NDCG@10 of the manual runs.</figDesc><graphic coords="15,91.25,124.80,425.19,208.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="14,162.03,134.86,287.19,153.19"><head>Table 1 :</head><label>1</label><figDesc>Our runs' results in comparison to the TREC average of the median</figDesc><table coords="14,174.84,148.99,261.56,139.05"><row><cell>Submission</cell><cell>Reciprocal</cell><cell>P@10</cell><cell>NDCG@10</cell></row><row><cell>UNTIIARUN1</cell><cell>0.0736</cell><cell>0.0253</cell><cell>0.0551</cell></row><row><cell>UNTIIARUN2</cell><cell>0.1690</cell><cell>0.0893</cell><cell>0.1389</cell></row><row><cell>UNTIIARUN3</cell><cell>0.1399</cell><cell>0.0667</cell><cell>0.1060</cell></row><row><cell>UNTIIARUN4</cell><cell>0.4243</cell><cell>0.2520</cell><cell>0.4043</cell></row><row><cell>UNTIIARUN5</cell><cell>0.1176</cell><cell>0.0360</cell><cell>0.0739</cell></row><row><cell>AVG Median (manual)</cell><cell>0.7213</cell><cell>0.4573</cell><cell>0.6212</cell></row><row><cell>AVG Median (auto)</cell><cell>0.2942</cell><cell>0.1613</cell><cell>0.3040</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,149.01,657.73,102.12,6.64"><p>http://www.trec-cds.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,149.01,649.58,97.39,6.64"><p>https://www.elastic.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="7,149.01,652.57,97.39,6.64"><p>https://www.lexigram.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="11,149.01,658.55,156.67,6.64"><p>https://uts.nlm.nih.gov/uts/umls/home</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">Payam Kelich, Ph.D.</rs> student at the <rs type="institution">University of Texas at El Paso</rs>, for assisting us in configuring the ElasticSearch platform and the experiment environment. The authors would also like to thank <rs type="person">Dustin Ho</rs>, <rs type="person">Jesus Guillen</rs>, and <rs type="person">Zachary Tian</rs> for analyzing the topics.</p></div>
			</div>
			<div type="funding">
<div><p>This project was supported in part by the <rs type="funder">National Science Foundation</rs> under Grant <rs type="grantNumber">1852249</rs>, and in part by the <rs type="funder">National Security Agency</rs> under Grant <rs type="grantNumber">H98230-20-1-0417</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RKRHcB8">
					<idno type="grant-number">1852249</idno>
				</org>
				<org type="funding" xml:id="_bbYC4GF">
					<idno type="grant-number">H98230-20-1-0417</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="16,154.25,495.58,323.24,8.74;16,154.25,513.51,226.76,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="16,423.54,495.58,53.95,8.74;16,154.25,513.51,142.17,8.74">Unt medical information retrieval at trec 2016</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">K</forename><surname>Viswavarapu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>TREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,154.25,539.42,323.23,8.74;16,154.25,557.35,268.27,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="16,417.69,539.42,59.79,8.74;16,154.25,557.35,183.68,8.74">Unt precision medicine information retrieval at trec 2017</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">K</forename><surname>Viswavarapu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>TREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,154.25,583.25,323.23,8.74;16,154.25,601.19,245.80,8.74" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="16,280.47,583.25,197.01,8.74;16,154.25,601.19,94.27,8.74">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,154.25,627.09,323.23,8.74;16,154.25,645.02,193.03,8.74" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08476</idno>
		<title level="m" coord="16,349.33,627.09,128.15,8.74;16,154.25,645.02,41.79,8.74">Learning-to-rank with bert in tf-ranking</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,154.25,127.96,323.23,8.74;17,154.25,145.89,242.64,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="17,338.81,127.96,138.67,8.74;17,154.25,145.89,157.80,8.74">Exploring how to combine query reformulations for precision medicine</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marchesin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Agosti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>TREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,154.25,171.80,323.23,8.74;17,154.25,189.73,323.23,8.74;17,154.25,207.66,57.28,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="17,453.13,171.80,24.35,8.74;17,154.25,189.73,298.93,8.74">Semisupervised information retrieval system for clinical decision support</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schepers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Megaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>TREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,154.25,233.56,323.23,8.74;17,154.25,251.50,295.20,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="17,263.65,233.56,209.29,8.74">Tuw@ trec clinical decision support track 2015</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Vienna University of Technology Vienna Austria</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="17,154.25,277.40,323.23,8.74;17,154.25,295.33,323.23,8.74;17,154.25,313.27,151.21,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,280.54,277.40,196.94,8.74;17,154.25,295.33,109.55,8.74">An overview of metamap: historical perspective and recent advances</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F.-M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,273.56,295.33,203.91,8.74;17,154.25,313.27,49.93,8.74">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,154.25,339.17,323.23,8.74;17,154.25,357.10,323.23,8.74;17,154.25,375.03,208.28,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,154.25,357.10,323.23,8.74;17,154.25,375.03,20.36,8.74">Yake! keyword extraction from single documents using multiple local features</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mangaravite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pasquali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,182.00,375.03,90.31,8.74">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">509</biblScope>
			<biblScope unit="page" from="257" to="289" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,154.25,400.94,323.24,8.74;17,154.25,418.87,323.24,8.74;17,154.25,436.80,298.05,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,195.14,418.87,282.34,8.74;17,154.25,436.80,134.01,8.74">A neural named entity recognition and multi-type normalization tool for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,296.24,436.80,55.88,8.74">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="73729" to="73740" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,154.25,462.70,323.23,8.74;17,154.25,480.64,323.23,8.74;17,154.25,498.57,210.55,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,431.90,462.70,45.57,8.74;17,154.25,480.64,323.23,8.74;17,154.25,498.57,27.99,8.74">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,190.22,498.57,63.32,8.74">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,154.25,524.47,323.23,8.74;17,154.25,542.41,323.24,8.74;17,154.25,560.34,77.52,8.74" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="17,411.64,524.47,65.83,8.74;17,154.25,542.41,253.29,8.74">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,154.25,586.24,323.24,8.74;17,154.25,604.17,323.23,8.74;17,154.25,622.11,77.52,8.74" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323</idno>
		<title level="m" coord="17,225.20,604.17,182.08,8.74">Publicly available clinical bert embeddings</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
