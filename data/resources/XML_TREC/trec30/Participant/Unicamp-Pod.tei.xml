<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.50,55.57,417.00,20.73;1,243.58,83.47,124.84,20.73">Multilingual Podcast Summarization using Longformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,121.74,123.28,61.33,9.50"><forename type="first">Edgar</forename><surname>Tanaka</surname></persName>
							<email>edgart@spotify.com</email>
						</author>
						<author>
							<persName coords="1,272.10,123.28,53.76,9.50"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
							<email>aclifton@spotify.com</email>
						</author>
						<author>
							<persName coords="1,405.21,123.28,94.74,9.50"><forename type="first">Md</forename><forename type="middle">Iftekhar</forename><surname>Tanveer</surname></persName>
						</author>
						<title level="a" type="main" coord="1,97.50,55.57,417.00,20.73;1,243.58,83.47,124.84,20.73">Multilingual Podcast Summarization using Longformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2096F3DDEB89438C53E94D53038B7727</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>podcasts</term>
					<term>summarization</term>
					<term>multilingual</term>
					<term>longformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most literature on automated summarization, including podcast summarization, has been restricted to the English language. At the same time, podcasts are now an important form of media in many countries and in many languages and therefore, it is crucial that we expand the problem of podcast summarization to a wider range of languages. In this work, we explore the application of multilingual models to the task of summarizing podcasts in English and Portuguese. We compare various training scenarios including adapting a Longformer encoder, cross-lingual and cross-task transfer learning and we demonstrate that a unified model fine-tuned to multilingual data can perform on par with dedicated models that are fine-tuned monolingually. As a result, our models significantly outperform the TREC baseline based on the first minute of each episode.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Text summarization has long been researched within the Natural Language Processing (NLP) community. However, most works have focused on summarizing news articles while the summarization of podcasts still remains fairly unexplored. Summarizing podcasts is a challenging task due to a number of reasons. Firstly, there are many podcast formats such as interviews, debates, monologues. Secondly, podcast transcripts are noisy as the audio often contains fillers and overlapping speakers, and the resulting transcripts contain errors with ASR and inferred punctuation. Lastly, these transcripts are often very long, whereas state-of-the-art models are generally trained on short texts and can ingest only a limited number of tokens.</p><p>The Podcast Summarization Track in TREC 2020 has encouraged the research in this area but it has been restricted to the English language. At the same time, previous works have demonstrated that single models trained on multiple languages provide competitive performance when compared against monolingual models <ref type="bibr" coords="1,163.58,614.32,16.60,8.64" target="#b15">[16]</ref> [15] <ref type="bibr" coords="1,201.64,614.32,10.58,8.64" target="#b5">[6]</ref>. In this work, we aim to explore the problem of multilingual podcast summarization by training a model to summarize podcasts in Portuguese and English; we have selected these two languages for proof of concept, but we note that nothing about our approach restricts it to just two languages. We have trained two models: (1) one finetuned on podcasts data (English and Portuguese) and <ref type="bibr" coords="1,288.41,686.05,11.62,8.64" target="#b1">(2)</ref> another finetuned first on news articles and then on podcasts data (English and Portuguese).</p><p>Previous works have used BART <ref type="bibr" coords="1,464.13,195.89,16.60,8.64" target="#b9">[10]</ref> to summarize podcasts in English: as a baseline <ref type="bibr" coords="1,446.45,207.84,11.62,8.64" target="#b2">[3]</ref> and in a summarization model using Longformer attention <ref type="bibr" coords="1,460.09,219.80,10.58,8.64" target="#b7">[8]</ref>. We have decided to use mBART-50 <ref type="foot" coords="1,376.10,230.08,3.49,6.05" target="#foot_0">1</ref> which is a multilingual version of BART pre-trained in 50 languages including English and Portuguese. Starting with a model pre-trained in these two languages was a requirement for our work as training a new language from scratch requires massive computing power and data. mBART-50 was also a good fit because, even though it was evaluated as a machine translator, it is still a sequence-to-sequence model trained in many languages.</p><p>Another model we would like to use as a starting point is XL-SUM <ref type="bibr" coords="1,365.65,342.38,10.58,8.64" target="#b5">[6]</ref>. It is a massive multilingual summarization model trained in 44 languages, including English and Portuguese. Due to time restrictions, we have only used XL-SUM's dataset with BBC news articles and summaries in this work. In the future, we would like to finetune the XL-SUM model with podcasts data.</p><p>In addition to exploring the multilingual perspective on summarization, we also wanted to investigate the benefits of using the Longformer attention mechanism. Transformerbased models are unable to process long documents due to their self-attention mechanism, which scales quadratically with the sequence length <ref type="bibr" coords="1,396.68,476.92,10.58,8.64" target="#b1">[2]</ref>, and thus most large-scale pretrained models can only accept inputs much shorter than the average transcript length. To address this problem, Iz Beltagy et al proposed Longformer <ref type="bibr" coords="1,403.37,512.78,10.58,8.64" target="#b1">[2]</ref>, a transformer-based model with an attention mechanism that scales linearly with sequence length, making it possible to process documents with thousands of tokens or more. While BERT-based pretrained models typically have a 512 or 1024 token limit, Longformer can process up to 16K tokens.</p><p>Figure <ref type="figure" coords="1,351.44,587.55,4.98,8.64" target="#fig_0">1</ref> illustrates the different attention mechanisms proposed by Iz Beltagy et al <ref type="bibr" coords="1,419.23,599.50,10.58,8.64" target="#b1">[2]</ref>. This paper's major contribution is in providing a multilingual alternative to the podcast summarization problem which can be compared to other monolingual models submitted to TREC 2021. In addition, we also investigate how this stacks with increasing the text input size with Longformer <ref type="bibr" coords="1,311.98,674.26,11.62,8.64" target="#b1">[2]</ref> attention mechanism. Source: Longformer: The Long-Document Transformer <ref type="bibr" coords="2,283.47,135.54,11.62,8.64" target="#b1">[2]</ref> II. DATASETS Since we are experimenting in a multilingual space, we have decided to work with two languages: Portuguese and English. We have also decided to work with two distinct types of data: news articles provided by the XL-SUM dataset and podcast transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. English Podcasts dataset</head><p>This dataset consists of over 100,000 podcast episodes in English. This includes nearly 60,000 hours of audio and accompanying transcripts. It also includes metadata such as creator-provided descriptions <ref type="bibr" coords="2,170.51,314.09,11.62,8.64" target="#b2">[3]</ref> and was published last year to the research community<ref type="foot" coords="2,159.25,324.38,3.49,6.05" target="#foot_1">2</ref> in the 2020 Text Retrieval Conference (TREC) Podcasts Track.</p><p>To train the summarization model, we have reused the same dataset from the TREC 2020 submission described in <ref type="bibr" coords="2,288.41,362.56,11.62,8.64" target="#b7">[8]</ref> which is a subset of the original podcasts dataset <ref type="bibr" coords="2,267.87,374.52,10.58,8.64" target="#b2">[3]</ref>. We have also reused the same splits (train, dev, test). To build this dataset, the following filters were used:</p><p>• Removed episodes where the creator descriptions are either too long or too short with the boundary conditions set to between 10 and 1300 characters. • Applied a TF-IDF vectorization of the descriptions which were compared to each other using the cosine distance. Any data points with too similar descriptions (threshold 0.95) were filtered out. • Removed boilerplate sentences from the creator descriptions using a sentence classifier based on BERT <ref type="bibr" coords="2,265.25,509.38,10.58,8.64" target="#b3">[4]</ref>. This classifier was trained using a small set of 1000 manually labeled episodes <ref type="bibr" coords="2,139.04,533.29,15.27,8.64" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Portuguese Podcasts dataset</head><p>This dataset composed of more than 100,000 podcast episodes in Portuguese (both PT-PT and PT-BR) was built internally at Spotify.</p><p>We have followed the same process used by the English podcasts dataset <ref type="bibr" coords="2,115.92,623.29,11.62,8.64" target="#b2">[3]</ref> but filtering for Portuguese content instead of English content. The following filters were used:</p><p>• The language of the show specified in the metadata must be Portuguese (PT-BR or PT-PT) • The episodes are all Spotify owned-and-operated, for copyright reasons.</p><p>• Since the metadata language tags are noisy, the episode descriptions must also be identified as Portuguese by the langid<ref type="foot" coords="2,356.91,74.67,3.49,6.05" target="#foot_2">3</ref> python package. • The episode must have more than 50% of speech over its duration; a proprietary speech detection algorithm was used to determine this. This filters out podcasts that are more music, white noise, or meditation than speech. Once the episodes were selected, they were sent to transcription using the Speech-to-Text service provided by the Azure platform.</p><p>The Portuguese Podcasts dataset is not yet available to the general public but we do plan to release it in 2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. XL-SUM dataset</head><p>We have used the BBC news articles and their summaries provided by the XL-SUM dataset <ref type="foot" coords="2,459.66,231.14,3.49,6.05" target="#foot_3">4</ref> . This dataset contains article-summary pairs in 44 languages but we only used the articles in Portuguese and English.</p><p>In order to keep data in both languages balanced, we have downsampled the English articles so that they matched the number of articles in Portuguese. No other filters were applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Removing extraneous content</head><p>To remove extraneous content such as ads or boilerplate from episode descriptions, we manually annotated sentences from 1000 episode descriptions as either "extraneous" or "not extraneous". We then used these labeled data to train a binary classifier to detect extraneous content in the manner described in <ref type="bibr" coords="2,323.44,387.12,16.60,8.64" target="#b13">[14]</ref> and used it to clean the creator-provided descriptions both in Portuguese and English podcasts.</p><p>Here are some examples of extraneous content found in episode descriptions:</p><p>• "Send in voice message http://anchor.com/foobar" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>Our methodology was comprised of the following steps: 1) Prepare the data as described in section "Datasets".</p><p>2) Convert the mBART-50 model into a Longformer <ref type="bibr" coords="3,288.41,52.42,11.62,8.64" target="#b1">[2]</ref> version. 3) Finetune model to the summarization task using news article-summary pairs. (for the double finetuned variant only) 4) Finetune model to the podcast summarization using podcast transcriptions and descriptions. 5) Evaluate model. We have chosen the mBART-50 <ref type="bibr" coords="3,193.87,150.45,16.60,8.64" target="#b11">[12]</ref> model because it has been already pre-trained in 50 languages including Portuguese and English. Then, we changed the source code to adapt it to a Longformer version which allowed the expansion of the input size from 1024 to 4096 tokens. The notebook available in <ref type="bibr" coords="3,288.41,198.27,11.62,8.64" target="#b0">[1]</ref> was used as a reference for this code change. Our hypothesis here is that the model will generate more complete summaries by being exposed to more of the full input text. This extension seems particularly crucial for podcasts given that transcripts are much longer than a news article.</p><p>The next step was to finetune the model to the task of podcast summarization. We developed two distinct models with different finetuning strategies:</p><p>• The first one called "Unicamp1" was finetuned only using podcasts data. Although mBART is a sequenceto-sequence model, it is initially trained to the machine translation task so we wanted to verify if finetuning it directly into the podcast summarization task would be successful. The training was configured for early stopping once the loss function had not improved after 3 validation checkpoints. • The second one called "Unicamp2" was finetuned initially on news articles from the XLSUM dataset <ref type="bibr" coords="3,249.12,416.04,11.62,8.64" target="#b5">[6]</ref> and then subsequently finetuned using podcasts data. The intuition here is that in the first round of finetuning, the model should learn how to summarize using high-quality news article-summary pairs. In other words, we would expect mBART to transition from a neural machine translation model to a summarization model. With the second round of finetuning, the model would then learn how to summarize podcast transcripts specifically. The training was set to early stop once the ROUGE score didn't improve after 3 validation checks. In both cases, Portuguese and English data was intermingled in order to avoid catastrophic forgetting <ref type="bibr" coords="3,215.40,561.89,10.58,8.64" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Human evaluation</head><p>From the 1000 podcast episodes in the test set, 193 were selected at random to be evaluated by NIST assessors. There were two types of evaluation: (1) summary quality based on a 4-point scale: bad, fair, good and excellent, and (2) eight yes/no questions were answered evaluating the summaries submitted. These results were all compared against a 1stminute baseline where the summary is simply the 1st minute of the episode transcript.</p><p>The summary quality was defined in these terms:</p><p>• Excellent: the summary accurately conveys all the most important attributes of the episode, which could include topical content, genre, and participants. In addition to giving an accurate representation of the content, it contains almost no redundant material which is not needed when deciding whether to listen. It is also coherent, comprehensible, and has no grammatical errors. • Good: the summary conveys most of the most important attributes and gives the reader a reasonable sense of what the episode contains with little redundant material which is not needed when deciding whether to listen. Occasional grammatical or coherence errors are acceptable. • Fair: the summary conveys some attributes of the content but gives the reader an imperfect or incomplete sense of what the episode contains. It may contain redundant material which is not needed when deciding whether to listen and may contain repetitions or broken sentences. • Bad: the summary does not convey any of the most important content items of the episode or gives the reader an incorrect or incomprehensible sense of what the episode contains. It may contain a large amount of redundant information that is not needed when deciding whether to listen to the episode. In figure IV-A, we have the absolute count of summaries for each grade. Our models (both Unicamp1 and Unicamp 2) produced 63% more Good/Excellent summaries than the 1st Minute Baseline while producing 13% less Bad/Fair summaries. At the same time, we did not notice any significant differences between runs Unicamp1 and Unicamp2. In fact, when we bucket together Bad and Fair summaries, both models produce the exact same count of summaries. The same happens when we bucket together Good and Excellent summaries.</p><p>In the second part of the evaluation, NIST assessors answered the following yes/no questions to evaluate the content in the summary:</p><p>• Q1: Does the summary include names of the main people (hosts, guests, characters) involved or mentioned in the podcast?</p><p>• Q2: Does the summary give any additional information about the people mentioned (such as their job titles, biographies, personal background, etc)? • Q3: Does the summary include the main topic(s) of the podcast? • Q4: Does the summary tell you anything about the format of the podcast; e.g. whether it's an interview, whether it's a chat between friends, a monologue, etc • Q5: Does the summary give you more context on the title of the podcast? • Q6: Does the summary contain redundant information? • Q7: Is the summary written in good English? • Q8: Are the start and end of the summary good sentence and paragraph start and end points? In figure IV-A, we see the percentage of summaries where the answer was "yes".</p><p>For questions 1 and 2, our runs generated summaries that were equal to or worse than the 1st-minute baseline. This is an indication that our models have not learned to include the names of the main people in the episode or provide additional information about them. In this context, we consider that any results matching the 1st-minute baseline indicate poor performance given its simplicity. This requires further investigation. However, we find this result surprising given the fact that the annotators judged our systems' summaries to be overall Good or Excellent more often than the 1st-minute baseline, indicating that these qualities in isolation may not predicate the reader's experience of quality.</p><p>Questions 3 and 5 are very important as they evaluate if the model was capable of surfacing the main topic and context around it. For (3) questions, our models provided summaries that were roughly on par with the 1st-minute baseline; for (5) they were slightly worse.</p><p>In question 4, the results show that our model performs similarly to the 1st-minute baseline. A hypothesis is that a lot of the episode descriptions do not provide their format in the description.</p><p>Our models outperform the 1st-minute baseline in questions 6, 7 and 8. SOTA (state-of-the-art) transformer-based models Overall, we conclude that the 1st-minute baseline was strong for informational adequacy and that our models' advantage was in coherence and fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Automated evaluation: ROUGE scores</head><p>In this section, we evaluate our models using the ROUGE metric <ref type="bibr" coords="4,341.54,373.72,15.27,8.64" target="#b10">[11]</ref>. This method does not require any human intervention and relies on a gold reference summary to measure the performance of each model. Since gold reference summaries do not exist for this dataset, we use the filtered creator descriptions (without extraneous content as defined in subsection II-D) as proxies for the reference summaries. We note that the creator descriptions are noisier than gold reference summaries, which must be taken into account when interpreting the ROUGE scores. However, in <ref type="bibr" coords="4,438.61,469.36,10.58,8.64" target="#b6">[7]</ref>, the authors note that the ranking over systems induced by ROUGE scores correlated with human judgments of quality. The First Minute baseline extracts the initial segment of the transcript up until the first minute of the episode and considers that as the summary. This is the same baseline used by the human evaluation.</p><p>TextRank Top 2 sentences and TextRank Top 5 sentences use TextRank <ref type="bibr" coords="4,371.47,626.05,16.60,8.64" target="#b12">[13]</ref> which is an unsupervised extractive summarization model. TextRank Top 2 sentences uses the top 2 sentences to compose the summary. TextRank Top 5 sentences uses the top 5 sentences.</p><p>XLSUM vanilla <ref type="foot" coords="4,388.92,672.84,3.49,6.05" target="#foot_4">5</ref> is the mT5 model finetuned to the summarization task with the XLSUM dataset. This is a multilingual LongMBART is the same as Unicamp1 as explained in section III. However, we have experimented with the same variants as done with the MBART-50: finetuned only to Portuguese podcasts, only to English podcasts and finally to both intermingled.</p><p>LongMBART + finetuned XL-SUM + finetuned PT/EN podcasts follows the training protocol of Unicamp2 as explained in section III. We used English and Portuguese data in an intermingled fashion for both finetuning rounds.</p><p>When interpreting the results from Table <ref type="table" coords="5,243.93,666.36,3.32,8.64">I</ref> and II, we notice that the MBART models finetuned monolingually have produced the highest ROUGE scores and outperformed the 6 https://huggingface.co/facebook/mbart-large-50 First Minute Baseline. Generally speaking, the best results came from the MBART and LongMBART models finetuned to the same language (either monolingually or bilingually) as the test set's language. It is also worth noting that the difference between finetuning monolingually and bilingually is marginal which leads us to conclude that learning to summarize in an additional language does not come at the cost of worse performance in a first language.</p><p>When we interpret the results from Table <ref type="table" coords="5,489.79,148.07,9.33,8.64" target="#tab_3">III</ref>, we can clearly see the importance of finetuning on both languages. Any of the monolingually-finetuned models have performed poorly here.</p><p>It came as a surprise that LongMBART model did not lead to higher ROUGE scores when compared to the MBART model. By converting the MBART model into a Longformer version, we increased the input text size limit from 512 tokens to 4096 tokens. Our initial hypothesis was that passing more information (i.e. more text from the transcript) to the model would lead to a better summary. However, the LongMBART model ended up producing summaries with ROUGE scores on par with the MBART model finetuned. As future work, we may want to evaluate if the text after the initial 512 tokens is irrelevant to composing a better summary, i.e. a summary that resembles the gold reference.</p><p>Relatedly, as already noted during this year's TREC Overview presentation, the First Minute baseline is a competitive baseline. This finding can be explained by the layout bias <ref type="bibr" coords="5,311.98,363.26,11.62,8.64" target="#b8">[9]</ref> present in podcasts. Similar to news articles, episodes tend to start with a brief summary of the overall content of each episode. The hosts usually present the guests (if any), they mention the topics to be covered and provide some background context for the listeners. The very same information is usually provided in the episode description which is used as our gold reference. This layout bias could also account for the fact that the Longformer-based model did not outperform the MBART model.</p><p>Another unexpected outcome was the fact that the XL-SUM vanilla model performed worse than MBART vanilla. Given that XL-SUM is a model finetuned on the summarization task, our expectation was that it would lead to higher ROUGE scores when compared to MBART which is a neural machine translation model. While observing the examples in Tables VI and IV, we noticed that MBART is mostly copying the beginning of a transcript and therefore producing a summary similar to the First Minute baseline. MBART in this case is simply acting as a translation model where the source and target language are the same. Thus, we surmise that the competitive performance of the vanilla MBART model is again an artifact of the layout bias previously discussed, and would not necessarily generalize to other datasets.</p><p>When we analyze the models finetuned monolingually, unsurprisingly, ROUGE scores were the highest when the language of the test set and the training set were the same. On the other hand, when they were different, ROUGE scores were the lowest. These low scores are due to the fact these models tended to produce text only in the language seen during its finetuning process. For example, a model finetuned only on Portuguese podcasts, tended to produce text in Portuguese even when the input was in English. And vice-versa. We can observe this phenomenon for models "LongMBART + finetuned PT podcasts" and "MBART + finetuned PT podcasts" in Table VII and model "LongMBART + finetuned EN podcasts" in Table V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented the results of summarizing podcasts in Portuguese and English with multilingual summarization models. We have experimented with a number of summarization techniques, models and finetuning strategies. We conclude that there are no significant ROUGE score improvements when extending the text input size of MBART or when adding an extra finetuning round on XL-SUM data. Nevertheless, our models outperformed the first-minute baseline both according to human evaluation and ROUGE scores. Our best model produced 63% more summaries with Good/Excellent grades according to NIST assessors when compared against the firstminute baseline. Using ROUGE-L F-score as our metric, MBART finetuned on PT/EN podcasts led to a gain of 1.5 points over the first-minute baseline. This paper's major contribution is to serve as a first step in the study of multilingual podcast summarization and share important findings based on the various experiments conducted. O consenso né entre os economistas é que é esse tipo de imposto ele é regressivo é no sentido de que é por exemplo pobre faz muita ele consome bastante para parte da sua renda ele faz muito a transação é financeira EE ele acabaria pagando é mais do que o rico não é é por conta disso apesar de ser em valores bem pequenininhos não é é tem essa questão da agressividade do lado bom da do imposto como de transações financeiras é. É essa tendência mundial já vem de muito tempo né ela é muito estudada tem a ver com é questões é democrata demográficas não é desculpa é as pessoas é tão vivendo mais aumenta expectativa de vida então as pessoas tem que aumentar a poupança ao longo da vida é então é é um fenômeno longo e está muito difícil ver isso se revertendo rapidamente né se a gente olhar para para as inflações nos países desenvolvidos. TextRank Top 5 sentences Ou uma eleição com mais sal não é com um cara mais esquerda como Sanders ou é Elizabeth Warren isso isso é coisa rápida é entre fevereiro e meados de março que a gente vai saber é isso vai determinar como é que se comporta o resto do ano né é mas certamente vai ser um tema que vai vai vai acompanhar os mercados aí esse ano não é difícil não ser diferente agora falando em economia doméstica é o crescimento do PIB brasileiro tu achas que ele vai conseguir se descolar? É sumido não é isso é o Chile especificamente é em relação a todos né por exemplo na Colômbia agora dia 21 a gente tem de 21 de janeiro é tem marcada e uma já 11 manifestação grande tudo mais que a gente tem que acompanhar de perto como é que vai ser é mas o fato é que acalmaram bem né e principalmente no Chile em que que isso ganhou proporções muito grandes né é para para para as nossas teses de investimento né o histórico do nosso fun? É está é atacando uma parte desses gastos obrigatórios a outra parte é que é muito importante atacar é os gastos com pessoal é funcionalismo público que é exatamente o que essas essa reforma administrativa essa PEC emergencial fazem né então é é eu acho que assim para efeito de mercado essas reformas podem ter um impacto parecido com o que teve o teto de gastos lá atrás né que foi bem bastante positivo? O consenso né entre os economistas é que é esse tipo de imposto ele é regressivo é no sentido de que é por exemplo pobre faz muita ele consome bastante para parte da sua renda ele faz muito a transação é financeira EE ele acabaria pagando é mais do que o rico não é é por conta disso apesar de ser em valores bem pequenininhos não é é tem essa questão da agressividade do lado bom da do imposto como de transações financeiras é. É essa tendência mundial já vem de muito tempo né ela é muito estudada tem a ver com é questões é democrata demográficas não é desculpa é as pessoas é tão vivendo mais aumenta expectativa de vida então as pessoas tem que aumentar a poupança ao longo da vida é então é é um fenômeno longo e está muito difícil ver isso se revertendo rapidamente né se a gente olhar para para as inflações nos países desenvolvidos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MBART vanilla</head><p>Hoje Hoje eu converso com Andreas pacote economista chefe da explore TAS uma gestora de recursos independentes com foco em renda fixa e variável no Brasil e na América Latina a gente vai falar sobre as expectativas do mercado doméstico e Internacional para 2020 os desafios fiscais no Brasil e na Argentina o cenário para o crescimento global e as eleições americanas e por aí vai se inscreva no nosso canal e fique conosco. Andrei recentemente você se tornou sócio da Explorer TAS com você também entrou o Edson sarti que é um gestor e um trader bastante experiente né é foi um ano bastante turbulento para as portas no ano passado principalmente por conta dos choques com Argentina conta conta para mim o que que que muda com a entrada de vocês. Bom obrigado pela oportunidade de estar aqui é. Compartilhando com vocês a nossa visão. XLSUM vanilla O ano passado foi um ano bastante turbulento para as portas.  We'll talk a little bit more about myths the tools the benefits and of course the science behind this amazing therapy before we get into talking more about sound healing or sound therapy. So a sound healing was just kind of a natural progression from my music and I started to experiment a little with sound frequencies in songs and things of that nature and I just got really excited about the power of sound and we all know that a song can certainly touch Us in such a deep way and it's the same thing for for for sound healing as well. In fact, we were trying not to he ate music and not trying to organize sound but to distill sound down to notes and use those very intentionally and specifically and what I thought was really interesting was the fact that sometimes what didn't sound necessarily musical had some of the highest healing properties right such as gongs and things like that that just reverberates so deeply, but it was very difficult for me because I immediately when I hear Sam Sound of any kind, I immediately tried to give it Melody and Harmony and I create around that in a musical way. We looked at a variety of training organizations from organizations in California to New York City and we kept coming back to a smaller training organization in New York state and in particular in the area that we spend a lot of time up in the Catskills and we Sound Stage Academy of sound healing and Tom and I that's kind of our happy place going up to the Catskills. I think it's the only practice that I can think of that is fully reciprocal right like while we are practicing and Tom and I do sessions one-on-one with clients or we'll do a session where Tom and I worked together on a client which is, you know, really a like a full bonus kind of session and then we also work in group sound therapy or what we call sound baths where a large group of clients will come together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MBART vanilla</head><p>We've been having so much fun making these podcasts. If you're thinking about making a podcast you should think about anchor anchor is the easiest way to make a podcast. Let me explain a little bit about this creation tool. XLSUM vanilla This is a full transcript of BBC Radio 4's Welcome to the Sound of Music series. Check in these shows we're going to be discussing in depth all aspects of vibrational sound in healing and try to help you gain an understanding of how to achieve personally a body in balance and overall sense of wellbeing. MBART + finetuned PT/EN podcasts Welcome to the first episode of Good Vibration Sound Healing with Dr. Tom Gleason and Dr. Lisa Gleason. In this episode, we introduce the concept of sound and vibration ultherapy, and talk about the various applications over time. We talk a little bit more about myths, tools, benefits, and of course the science behind this amazing therapy. Before we get into talking more about sound healing or sound therapy, we talk a little bit about how we came into this practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,48.96,56.73,251.06,6.91;2,48.96,65.70,162.49,6.91;2,49.96,72.61,249.07,57.45"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparing (a) the full self-attention pattern and (b)(c)(d) the configuration of attention patterns in Longformer.</figDesc><graphic coords="2,49.96,72.61,249.07,57.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,311.98,503.35,251.06,6.91;3,311.98,512.32,155.10,6.91;3,311.98,336.60,249.07,154.01"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall quality scores. '1st Minute Baseline' refers to the TRECprovided baseline of the first minute of speech.</figDesc><graphic coords="3,311.98,336.60,249.07,154.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,48.96,396.61,251.06,6.91;4,48.96,405.57,155.10,6.91;4,48.96,229.85,249.07,154.01"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Averages per question. '1st Minute Baseline' refers to the TRECprovided baseline of the first minute of speech.</figDesc><graphic coords="4,48.96,229.85,249.07,154.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,311.98,493.27,251.06,68.41"><head></head><label></label><figDesc>Table I presents the ROUGE scores when evaluating only on a test set of 4511 English podcasts. Table II presents the ROUGE scores when evaluating only on a test set of 5073 Portuguese podcasts. Lastly, Table III presents the ROUGE scores for the combination of both test sets in English and Portuguese.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,48.96,53.72,251.06,501.95"><head>TABLE II ROUGE</head><label>II</label><figDesc>SCORES FOR INTERNAL TEST SET OF 5073 PORTUGUESE There was no finetuning with podcasts data. MBART vanilla is the MBART-50 6 model without any finetuning. Note that this is machine translation model and not a summarization model. MBART + finetuned PT podcasts is MBART-50 finetuned only to Portuguese podcasts. MBART + finetuned EN podcasts is MBART-50 finetuned only to English podcasts. MBART + finetuned PT/EN podcasts is MBART-50 finetuned to both English and Portuguese podcasts intermingled.</figDesc><table coords="5,48.96,72.69,251.06,387.41"><row><cell cols="2">PODCAST EPISODES.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>R1-F</cell><cell>R2-F</cell><cell>RL-F</cell></row><row><cell>First Minute baseline</cell><cell cols="3">0.1674 0.0327 0.1397</cell></row><row><cell>TextRank Top 5 sentences</cell><cell cols="3">0.1169 0.0143 0.0959</cell></row><row><cell>TextRank Top 2 sentences</cell><cell cols="3">0.1335 0.0139 0.1058</cell></row><row><cell>XLSUM vanilla</cell><cell cols="3">0.1120 0.0159 0.0951</cell></row><row><cell>MBART vanilla</cell><cell cols="3">0.1586 0.0277 0.1342</cell></row><row><cell>MBART + finetuned PT podcasts</cell><cell cols="3">0.1886 0.0516 0.1634</cell></row><row><cell>MBART + finetuned EN podcasts</cell><cell cols="3">0.0393 0.0067 0.0369</cell></row><row><cell>MBART + finetuned PT/EN podcasts</cell><cell>0.1835</cell><cell>0.0501</cell><cell>0.1598</cell></row><row><cell>LongMBART vanilla</cell><cell cols="3">0.1136 0.0119 0.1021</cell></row><row><cell>LongMBART + finetuned PT podcasts</cell><cell cols="2">0.1826 0.0501</cell><cell>0.1598</cell></row><row><cell>LongMBART + finetuned EN podcasts</cell><cell cols="3">0.0280 0.0046 0.0266</cell></row><row><cell cols="4">LongMBART + finetuned PT/EN podcasts 0.1761 0.0491 0.1536</cell></row><row><cell>LongMBART + finetuned XL-SUM + finetuned PT/EN podcasts</cell><cell cols="3">0.1764 0.0481 0.1535</cell></row><row><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ROUGE SCORES FOR INTERNAL TEST SET OF 5073 PORTUGUESE</cell></row><row><cell cols="4">PODCAST EPISODES COMBINED WITH INTERNAL SET OF 4511 ENGLISH</cell></row><row><cell cols="2">PODCAST EPISODES.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>R1-F</cell><cell>R2-F</cell><cell>RL-F</cell></row><row><cell>First Minute baseline</cell><cell cols="3">0.1697 0.0316 0.1466</cell></row><row><cell>TextRank Top 5 sentences</cell><cell cols="3">0.1278 0.0152 0.1064</cell></row><row><cell>TextRank Top 2 sentences</cell><cell cols="3">0.1369 0.0142 0.1099</cell></row><row><cell>XLSUM vanilla</cell><cell cols="3">0.1146 0.0157 0.0991</cell></row><row><cell>MBART vanilla</cell><cell cols="3">0.1583 0.0275 0.1369</cell></row><row><cell>MBART + finetuned PT podcasts</cell><cell cols="3">0.1191 0.0295 0.1047</cell></row><row><cell>MBART + finetuned EN podcasts</cell><cell cols="3">0.1084 0.0300 0.0978</cell></row><row><cell>MBART + finetuned PT/EN podcasts</cell><cell cols="3">0.1846 0.0500 0.1625</cell></row><row><cell>LongMBART vanilla</cell><cell cols="3">0.1364 0.0195 0.1218</cell></row><row><cell>LongMBART + finetuned PT podcasts</cell><cell cols="3">0.1128 0.0286 0.1001</cell></row><row><cell>LongMBART + finetuned EN podcasts</cell><cell cols="3">0.1018 0.0270 0.0911</cell></row><row><cell cols="4">LongMBART + finetuned PT/EN podcasts 0.1785 0.0487 0.1576</cell></row><row><cell>LongMBART + finetuned XL-SUM + finetuned PT/EN podcasts</cell><cell cols="3">0.1802 0.0515 0.1589</cell></row><row><cell cols="4">summarization trained in 45 languages including Portuguese</cell></row><row><cell>and English.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,54.94,153.06,500.72,149.25"><head>TABLE IV SUMMARIES</head><label>IV</label><figDesc>GENERATED BY EACH NON-FINETUNED MODEL FOR A GIVEN EPISODE IN PORTUGUESE. e economista chefe da Exploritas, gestora de recursos independentes com foco em renda variável e fixa no Brasil, falou sobre as expectativas do mercado doméstico e internacional para 2020, os desafios fiscais no Brasil e na Argentina, o cenário para o crescimento global, eleições americanas e também os principais riscos para o investidor se atentar neste ano. First Minute baseline hoje eu converso com andreas pacote economista chefe da explore tas uma gestora de recursos independentes com foco em renda fixa e variável no brasil e na américa latina a gente vai falar sobre as expectativas do mercado doméstico e internacional para dois mil e vinte os desafios fiscais no brasil e na argentina o cenário para o crescimento global e as eleições americanas e por aí vai se inscreva no nosso canal e fique conosco andrei recentemente você se tornou sócio da explorer tas com você também entrou o edson sarti que é um gestor e um trader bastante experiente né é foi um ano bastante turbulento para as portas no ano passado principalmente por conta dos choques com argentina conta para mim o que que muda com a entrada de vocês bom obrigado pela oportunidade de estar aqui é compartilhando com vocês a nossa visão é o explore todas desde o início ele ele é um fundo que tem TextRank Top 2 sentences</figDesc><table coords="7,54.94,178.53,183.82,16.19"><row><cell>Model</cell><cell>Predicted summary</cell></row><row><cell>Episode description by creator</cell><cell>Andrei Spacov, sócio</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,54.94,242.73,500.72,274.78"><head>TABLE V SUMMARIES</head><label>V</label><figDesc>GENERATED BY EACH FINETUNED MODEL FOR A GIVEN EPISODE IN PORTUGUESE. Episode description by creator Andrei Spacov, sócio e economista chefe da Exploritas, gestora de recursos independentes com foco em renda variável e fixa no Brasil, falou sobre as expectativas do mercado doméstico e internacional para 2020, os desafios fiscais no Brasil e na Argentina, o cenário para o crescimento global, eleições americanas e também os principais riscos para o investidor se atentar neste ano. LongMBART + finetuned PT podcasts No episódio de hoje, eu converso com o Andreas Paquito, Economista-chefe da Exploritas, uma gestora de recursos independentes com foco em renda fixa e variável no Brasil e na América Latina. Falamos sobre as expectativas do mercado doméstico e internacional para 2020, os desafios fiscais no Brasil e na Argentina, o cenário para o crescimento global e as eleições americanas. LongMBART + finetuned EN podcasts In this episode, I spoke with Andreas Pak economist, chefe da Explore TAS, about the expectations of mercado doméstico e Internacional para 2020, os desafios fiscais no Brasil e na Argentina, o cenário para o crecimiento global e as eleições americanas. LongMBART + finetuned PT/EN podcasts Andreas Paco, Economista-Chefe da Explorer TAS, fala sobre as expectativas do mercado doméstico e internacional para 2020, os desafios fiscais no Brasil e na Argentina, cenário para o crescimento global e eleições americanas. MBART + finetuned PT podcasts O economista-chefe da Exploritas, Andrea Pacote, conversou com o economista-chefe da Exploritas, Edson Sarti, sobre as expectativas do mercado doméstico e internacional para 2020, os desafios fiscais no Brasil e na Argentina, o cenário para o crescimento global e as eleições americanas. MBART + finetuned EN podcasts Andreas pacote economista and Andreas pacote economista chefe Andreas pacote economista Chefe da Explo Explo Explo Explo Explo Explo Exploration TAS, uma gestora de recursos independentes with a Andreas pacote economista Chefe da Explo Explo Explo Explo Explo Explo Explo Exploration TAS, is a co co co co co co co co Gabriel Andreas pacote economista and Andreas pacote economista Chefe da Explo Explo Explo Explo Explo Explo Explo Explo Exploration TAS, a gestora of ER Andreas pacote economista and Andreas pacote economista Chefe da Explo Explo Explo Explo Exploration TAS, a gestora of explore explore explore TAS, a gestora de recursos independents, uma gestora de recursos independientes, with a geo MBART + finetuned PT/EN podcasts Neste episódio, o economista-chefe da Explorer TAS, Andreas Pacote, fala sobre as expectativas do mercado doméstico e internacional para 2020, os desafios fiscais no Brasil e na Argentina, o cenário para o crescimento global e as eleições americanas.</figDesc><table coords="8,54.94,268.19,208.79,7.17"><row><cell>Model</cell><cell>Predicted summary</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,54.94,193.41,500.72,212.01"><head>TABLE VI SUMMARIES</head><label>VI</label><figDesc>GENERATED BY NON-FINETUNED MODELS FOR A GIVEN EPISODE IN ENGLISH. Dr. Lisa and Tom Gleason introduce listeners to the fundamentals of sound healing, including the origins and benefits of this therapeutic practice. -This episode is sponsored by • Anchor: The easiest way to make a podcast. https://anchor.fm/app First Minute baseline We've been having so much fun making these podcasts. If you're thinking about making a podcast you should think about anchor anchor is the easiest way to make a podcast. Let me explain a little bit about this creation tool. It's free these tools allow you to record and edit your podcast right from your phone or your computer and then anchor distributes your podcast for you, so it can be heard on Spotify Apple podcast and all other major podcasting platforms. And here's the best part you can make money from your Podcast with no minimum listenership. It's everything you need to podcast in one place. Just go ahead and download the free anchor app or go to Anchor dot f m-to get started. Hello everyone, and thank you for tuning in to Good Vibration sound healing the Art and Science of vibro acoustic sound therapy. I really appreciate TextRank Top 2 sentencesSo a sound healing was just kind of a natural progression from my music and I started to experiment a little with sound frequencies in songs and things of that nature and I just got really excited about the power of sound and we all know that a song can certainly touch Us in such a deep way and it's the same thing for for for sound healing as well. In fact, we were trying not to he ate music and not trying to organize sound but to distill sound down to notes and use those very intentionally and specifically and what I thought was really interesting was the fact that sometimes what didn't sound necessarily musical had some of the highest healing properties right such as gongs and things like that that just reverberates so deeply, but it was very difficult for me because I immediately when I hear Sam Sound of any kind, I immediately tried to give it Melody and Harmony and I create around that in a musical way. TextRank Top 5 sentences</figDesc><table coords="9,54.94,218.88,183.09,16.19"><row><cell>Model</cell><cell>Predicted summary</cell></row><row><cell>Episode description by creator</cell><cell>In this episode,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,54.94,242.73,500.72,203.05"><head>TABLE VII SUMMARIES</head><label>VII</label><figDesc>GENERATED BY FINETUNED MODELS FOR A GIVEN EPISODE IN ENGLISH. Episode description by creatorIn this episode, Dr. Lisa and Tom Gleason introduce listeners to the fundamentals of sound healing, including the origins and benefits of this therapeutic practice. -This episode is sponsored by • Anchor: The easiest way to make a podcast. https://anchor.fm/app LongMBART + finetuned PT podcasts O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O que é Sound Healing? O LongMBART + finetuned EN podcasts Welcome to Good Vibration, Sound Healing, the Art and Science of Vibro Acoustic Sound Therapy. In this episode, Dr. Lisa Gleason and Dr. Tom Gleason discuss all aspects of vibrational sound healing and try to help you gain an understanding of how to achieve personally a body in balance and an overall sense of well-being. LongMBART + finetuned PT/EN podcasts In our first episode, Tom and Dr. Lisa discuss the science behind Sound Healing and the use of vibrational sound healing. MBART + finetuned PT podcasts Neste episódio, Tom Gleason e Lisa Gleason falam sobre a importância do vibracional sound para a saúde e bem-being. MBART + finetuned EN podcasts Welcome to Good Vibration Sound Healing, the Art and Science of Vibrational Sound Therapy. I really appreciate you spending some time with us today. My name is Tom Gleason, and I'm here with my wife Dr. Lisa Gleason, and we are both certified sound practitioners and extremely passionate about this subject.</figDesc><table coords="10,54.94,268.19,208.79,7.17"><row><cell>Model</cell><cell>Predicted summary</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,323.44,711.26,152.20,6.91"><p>https://huggingface.co/facebook/mbart-large-50</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,60.42,711.26,121.25,6.91"><p>https://podcastsdataset.byspotify.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,323.44,701.39,97.42,6.91"><p>https://pypi.org/project/langid/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,323.44,711.26,118.42,6.91"><p>https://github.com/csebuetnlp/xl-sum</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,323.44,711.26,196.58,6.91"><p>https://huggingface.co/csebuetnlp/mT5_multilingual_XLSum</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,67.22,370.53,232.80,6.91;6,67.22,379.50,232.80,6.91;6,67.22,388.47,198.75,6.91" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,119.05,370.53,180.97,6.91;6,67.22,379.50,58.33,6.91">Roberta to longformer: build a &quot;long&quot; version of pretrained models</title>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<ptr target="https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb" />
		<imprint>
			<biblScope unit="page" from="2021" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.22,397.43,232.80,6.91;6,67.22,406.40,110.62,6.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,243.37,397.43,56.65,6.91;6,67.22,406.40,86.66,6.91">Longformer: The long-document transformer</title>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.22,415.37,232.80,6.91;6,67.22,424.33,232.80,6.91;6,67.22,433.30,232.80,6.91;6,67.22,442.12,232.80,7.05;6,67.22,451.09,232.80,7.05;6,67.22,460.20,232.80,6.91;6,67.22,469.16,37.42,6.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,194.41,433.30,105.61,6.91;6,67.22,442.26,55.32,6.91">000 podcasts: A spoken English document corpus</title>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Bonab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,143.90,442.12,156.13,6.87;6,67.22,451.09,111.56,6.87">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="5903" to="5917" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct coords="6,67.22,478.13,232.80,6.91;6,67.22,487.10,232.80,6.91;6,67.22,495.92,172.90,7.05" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="6,86.65,487.10,213.38,6.91;6,67.22,496.06,35.92,6.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,67.22,505.03,232.80,6.91;6,67.22,513.85,165.22,7.05" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,134.80,505.03,161.76,6.91">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,67.22,513.85,93.06,6.87">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.22,522.96,232.80,6.91;6,67.22,531.93,232.80,6.91;6,67.22,540.89,232.80,6.91;6,67.22,549.86,54.59,6.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,82.56,540.89,217.46,6.91;6,67.22,549.86,30.48,6.91">Xl-sum: Large-scale multilingual abstractive summarization for 44 languages</title>
		<author>
			<persName coords=""><forename type="first">Tahmid</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhik</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Md</forename><surname>Saiful Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kazi</forename><surname>Samin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yong-Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sohel Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rifat</forename><surname>Shahriyar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.22,558.83,232.80,6.91;6,67.22,567.79,232.80,6.91;6,67.22,576.76,136.22,6.91" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Trec 2020 podcasts track overview</note>
</biblStruct>

<biblStruct coords="6,67.22,585.73,232.80,6.91;6,67.22,594.55,155.97,7.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,176.52,585.73,123.50,6.91;6,67.22,594.69,97.69,6.91">Abstract podcast summarization using bart with longformer attention</title>
		<author>
			<persName coords=""><forename type="first">Hannes</forename><surname>Karlbom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,181.00,594.55,17.18,6.87">TREC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.22,603.66,232.80,6.91;6,67.22,612.63,232.80,6.91;6,67.22,621.59,55.52,6.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,171.49,612.63,128.53,6.91;6,67.22,621.59,31.63,6.91">Neural text summarization: A critical evaluation</title>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.22,630.56,232.80,6.91;6,67.22,639.53,232.80,6.91;6,67.22,648.49,232.80,6.91;6,67.22,657.46,162.57,6.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,85.89,648.49,214.13,6.91;6,67.22,657.46,32.39,6.91">Denoising sequence-to-sequence pre-training for natural language generation</title>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note>translation, and comprehension</note>
</biblStruct>

<biblStruct coords="6,67.22,666.42,232.80,6.91;6,67.22,675.25,232.80,7.05;6,67.22,684.36,199.83,6.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,120.54,666.42,179.49,6.91;6,67.22,675.39,19.92,6.91">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,103.74,675.25,109.92,6.87">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.22,693.32,232.80,6.91;6,67.22,702.29,232.80,6.91;6,67.22,711.26,195.15,6.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="6,260.16,702.29,39.86,6.91;6,67.22,711.26,171.46,6.91">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,330.24,53.72,232.80,6.91;6,330.24,62.55,232.80,6.87;6,330.24,71.51,232.80,7.05;6,330.24,80.62,140.10,6.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,436.90,53.72,112.93,6.91">TextRank: Bringing order into text</title>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,330.24,62.55,232.80,6.87;6,330.24,71.51,69.74,6.87">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,330.24,89.59,232.80,6.91;6,330.24,98.55,232.80,6.91;6,330.24,107.52,17.93,6.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,426.00,98.55,133.77,6.91">Detecting extraneous content in podcasts</title>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aswin</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,330.24,116.49,232.80,6.91;6,330.24,125.45,232.80,6.91;6,330.24,134.42,177.06,6.91" xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>mt5: A massively multilingual pre-trained text-to-text transformer</note>
</biblStruct>

<biblStruct coords="6,330.24,143.39,232.80,6.91;6,330.24,152.35,232.80,6.91;6,330.24,161.18,232.80,7.05;6,330.24,170.14,232.80,7.05;6,330.24,179.25,140.10,6.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,529.51,143.39,33.53,6.91;6,330.24,152.35,232.80,6.91;6,330.24,161.32,17.27,6.91">Improving massively multilingual neural machine translation and zero-shot translation</title>
		<author>
			<persName coords=""><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,365.33,161.18,197.71,6.87;6,330.24,170.14,98.96,6.87">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="1628" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
