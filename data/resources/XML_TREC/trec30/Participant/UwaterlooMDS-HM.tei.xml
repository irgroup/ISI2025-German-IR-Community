<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,80.70,105.06,411.84,12.85">UWaterlooMDS at the TREC 2021 Health Misinformation Track</title>
				<funder ref="#_pSeuqyZ #_7K9Uvgw">
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
				</funder>
				<funder>
					<orgName type="full">Compute Canada</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Waterloo</orgName>
				</funder>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,80.70,133.30,122.36,10.69"><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.28,133.30,117.11,10.69"><forename type="first">Irene</forename><forename type="middle">Xiangyi</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.61,133.30,94.16,10.69"><forename type="first">Kamyar</forename><surname>Ghajar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,445.00,133.30,86.30,10.69;1,80.70,147.25,31.37,10.69"><forename type="first">Linh</forename><surname>Nhi Phan Minh</surname></persName>
						</author>
						<author>
							<persName coords="1,122.26,147.25,103.69,10.69"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.14,147.25,112.91,10.69"><forename type="first">Amir</forename><surname>Vakili Tahami</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.24,147.25,73.39,10.69"><forename type="first">Dake</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,80.70,105.06,411.84,12.85">UWaterlooMDS at the TREC 2021 Health Misinformation Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">42CBA40875CD8526B5262862979F822C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this report, we discuss the experiments we conducted for the TREC 2021 Health Misinformation Track. For our manual runs, we used an improved version of our high-recall retrieval system [2] to manually search and judge documents. The system is built to efficiently retrieve the most-likely relevant documents based on a Continuous Active Learning (CAL) model and allows a speedy document assessment phase. Using the judged documents, we built CAL models to score documents that are part of our filtered collections. We also experimented with neural reranking methods based on question answering and stance detection methods to modify our CAL-based runs and a traditional BM25 run. For our automatic runs, we filtered the collection by running PageRank with a seed set of reliable domains and then using a text classifier and further refined the collection by including only medical web pages. We then ran traditional BM25 on this smaller and more reliable collection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The task of the 2021 Health Misinformation Track is an ad-hoc retrieval task where researchers design and build retrieval technologies to retrieve credible and correct information while avoiding non-credible and incorrect information in order to help search users make correct decisions about their health concerns.</p><p>We submitted both automatic and manual runs. Most of these runs were constructed using one of three filtered collections that are subsets of the track's collection. The motivation behind the filtering was to produce a collection with high-quality health-related documents so that our retrieval methods would be able to find correct and credible documents and avoid retrieving incorrect and low-credibility documents. This effectively reduced the size of the given collection, which also allowed for faster data processing and retrieval. Different techniques were used to filter the collections resulting in collections of different sizes.</p><p>We used several methods to construct our manual runs. The first method utilized our high-recall retrieval system <ref type="bibr" coords="1,112.18,517.39,10.57,8.83" target="#b1">[2]</ref>, which is based on Continuous Active Learning (CAL), to score documents with assessors making manual judgements for the track's topics. The second method implemented a combination of CAL, and the RoBERTa language model <ref type="bibr" coords="1,187.00,541.30,10.35,8.83" target="#b5">[6]</ref>, where we scored paragraphs using CAL trained on assessors' manual judgements and then reranked based on RoBERTa to match each topic has given stance field. The last method was to fine-tune T5-Large <ref type="bibr" coords="1,121.14,565.21,16.50,8.83" target="#b9">[10]</ref> to acquire a binary classification model to predict the stance of each document. We built our automatic runs using Anserini's BM25 on our different filtered collections.</p><p>Results show that in terms of the compatibility measure, using our filtered collections produced runs with better performances than just using the entire collection (as was done to create the baseline run). Based on the nDCG measure, several of our runs achieved higher scores than the baseline run. Precision at 10 (P@10) scores show that the use of our filtered collections produced runs with better credibility. Overall, creating filtered collections allowed for a boost in performance. This collection expands M by adding domains that were linked them. We select the top 10,000 domains and filter out all non-medical documents with a medical text classifier, thus ending up with a smaller collection than M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T 3k BM25 144,367</head><p>This collection contains the top 3k BM25 results per topic from the complete collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">COLLECTION 2.1 Original collection (A)</head><p>This year, the track used the c4/en.noclean version of the c4 dataset <ref type="foot" coords="2,354.74,358.98,3.38,6.44" target="#foot_0">1</ref> . The collection is comprised of text extracts from the April 2019 snapshot of Common Crawl web corpus and contains about 1 billion English documents.</p><p>The compressed size of the collection is 2.2 terabytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Filtered collections</head><p>We created different subsets of the collection that focus on filtering out non-medical and unreliable documents. These subsets are much smaller than the original collection and allow easier and faster data processing and retrieval. Most of our runs were constructed using one of these filtered collections. Table <ref type="table" coords="2,432.65,446.38,4.54,8.83" target="#tab_0">1</ref> shows a brief summary of each collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Reliable Medical Collection ( M).</head><p>In this collection, we focused on filtering out non-credible and nonhealth-related websites. Our filtering method is based on detecting domains with HONCode certification <ref type="foot" coords="2,506.94,487.23,3.38,6.44" target="#foot_1">2</ref> . The HONcode certification is maintained by a non-profit and non-governmental organization named The Health On the Net Foundation (HON) and is created to promote access to useful and reliable health information online. The certification assessment is carried out by medical experts and is only given to websites offering health information that is deemed reliable. Such websites include the World Health Organization<ref type="foot" coords="2,393.41,535.05,3.38,6.44" target="#foot_2">3</ref> and Mayo Clinic<ref type="foot" coords="2,466.52,535.05,3.38,6.44" target="#foot_3">4</ref> , among others. According to the foundation's website, there are currently more than 8,000 websites that have been certified.</p><p>To the best of our knowledge, the list of HONCode certified domains are not publicly available. In our case, we used the HON foundation's browser plugin <ref type="foot" coords="2,256.56,570.91,3.38,6.44" target="#foot_4">5</ref> that was designed for people to easily identify HONCode certified websites while browsing and searching the internet. The extension works by matching the MD5 hash of a website domain to a public list of HONCode certified MD5 domain hashes <ref type="foot" coords="3,342.66,106.18,3.38,6.44" target="#foot_5">6</ref> that the plugin uses. To extract the HONCode domains in our collection, we iterated and calculated MD5 hashes for every unique domain in the collection and recorded domains that are part of the list. In total, we found 2094 domains. We excluded domains that are part of wordpress.com or blogpost.com. In addition to these domains, we added 13 health-related websites that we believe are reliable but are not HONCode certified (e.g., kidshealth.org, aarp.org, etc.). Some of these websites were manually selected from Alexa Global Traffic Rank. With this new set of domains, we constructed the collection by including only documents from domains that are part of our set, yielding a total of 3,568,939 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Reliable Medical</head><p>Websites + 10k BM25 Collection ( C). While the previous collection (M) focuses on reliable health-related websites, our HONCode filtering method filters out documents that are potentially very useful and relevant to the track's topics. Such documents can be from websites that are deemed credible but are not HONCode certified, non-health-related websites that include correct health information, or websites that their credibility is unknown but provide correct health information. In this collection, we expand on our previous collection by including the top 10k BM25 search results for each topic retrieved from the original collection. We used Anserini's<ref type="foot" coords="3,143.46,285.75,3.38,6.44" target="#foot_6">7</ref> BM25 with its default parameters to retrieve the top 10k results. The additional 10k results are used to try to include as many relevant documents as possible for each topic, regardless of their credibility or domain focus (e.g., news websites). In total, this collection has 4,040,012 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Expanded Reliable Medical Collection ( E).</head><p>The purpose of this collection is to explore an alternative query independent method of expanding M while keeping the document count low. Using the hosts in M as a base, we expanded this list using the common crawl host graph <ref type="foot" coords="3,300.17,357.71,3.38,6.44" target="#foot_7">8</ref> . The hosts graph contains roughly 4 million nodes and 4 billion edges. This expanded collection contains domains for reputable journals and organizations not included in M (e.g. bmj.com ) but also various irrelevant and/or non-reputable domains (e.g. blog posts). The number of documents for these hosts also greatly increased. We used two steps to filter this expanded collection.</p><p>In our first step, we aim to expand the list of reliable medical websites in M. We do this by calculating PageRank scores in a subset of the common crawl host-level graph. The subset is created as follows: For the nodes, we take the domains in M and all the domains they link to. For the edges, we take all the edges with a M domain as its source. We calculate PageRank but only jump randomly to domains in M. This approach is similar to Topic Sensitive PageRank <ref type="bibr" coords="3,191.00,455.20,10.58,8.83" target="#b4">[5]</ref>. After calculating each domain's score, we take the top 10,000 domains. With this approach, we end up with roughly 30 million documents. Many of these documents are non-medical and irrelevant to the task at hand. Thus, in the next step, we aim to filter these out from the collection.</p><p>In our next step, we filter out these non-medical pages with a rudimentary medical text classifier. For the positive samples in our training data, we use the top 100 documents retrieved from collection A by Anserini's BM25 using the 2019 track topics as queries. The idea is that since these queries are medical in nature and the top 100 documents are pseudo-relevant, they would represent a good mix of medical pages present in the collection. For our negative samples, we sample randomly from ùê¥ -ùëÄ. We train a model using a linear support vector machine and validate with 5-fold cross-validation where we train on 40 topics and test with 11 in each fold. With this classifier, we filter out documents whose text is classified as non-medical and are left with 1,829,111 documents.</p><p>Table <ref type="table" coords="4,238.90,106.39,3.07,8.02">2</ref>. List of submitted runs and their details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run tag</head><p>Collection Manual Assessment Fields Ranking method(s) query desc. stance narr. evidence</p><formula xml:id="formula_0" coords="4,113.76,180.48,384.62,210.84">Automatic baselineBM25 A ‚Ä¢ WatSAM-BM25 M ‚Ä¢ WatSAE-BM25 E ‚Ä¢ BM25 (Anserini) WatSAE-BM25RM3 E ‚Ä¢ WatSAE-BM25-RR E ‚Ä¢ Manual WatSMM-CAL M ‚úì ‚Ä¢ ‚Ä¢ CAL WatSMM-CALHC M ‚úì ‚Ä¢ ‚Ä¢ CAL WatSMM-CALPR M ‚úì ‚Ä¢ ‚Ä¢ CAL WatSMM-Fused M ‚úì ‚Ä¢ ‚Ä¢ CAL WatSMC-CAL C ‚úì ‚Ä¢ ‚Ä¢ CAL WatSMM-CALQA100 M ‚úì ‚Ä¢ ‚Ä¢ ‚Ä¢ CAL + RoBERTa WatSMM-CALQAAll M ‚úì ‚Ä¢ ‚Ä¢ ‚Ä¢ CAL + RoBERTa WatSMC-CALQA100 C ‚úì ‚Ä¢ ‚Ä¢ ‚Ä¢ CAL + RoBERTa WatSMC-CALQAAll C ‚úì ‚Ä¢ ‚Ä¢ ‚Ä¢ CAL + RoBERTa WatSMC-CALQAHC1 C ‚úì ‚Ä¢ ‚Ä¢ ‚Ä¢ CAL + RoBERTa WatSMC-CALQAHC2 C ‚úì ‚Ä¢ ‚Ä¢ ‚Ä¢ CAL + RoBERTa WatSMT-SD-S1 T ‚Ä¢ ‚Ä¢ BM25 (Anserini) + T5 WatSMT-SD-S2 T ‚Ä¢ ‚Ä¢ BM25 (Anserini) + T5 WatSMC-Correct C ‚úì ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ CAL</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SUBMITTED RUNS</head><p>Table <ref type="table" coords="4,105.46,441.82,4.73,8.83">2</ref> shows the list of runs we submitted to the track. In total, we submitted 19 runs -5 automatic and 14 manual runs. We describe the details of each run below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Runs</head><p>3.1.1 baselineBM25. As a baseline for the other runs, we have retrieved the top 1000 documents per topic using Anserini's implementation of BM25 scoring with their default parameters of ùëò 1 = 0.9 and ùëè = 0.4. We built the index for this run from our A collection (original collection) mentioned in section 2.1. For indexing, we used Anserini's IndexCollection program with its default English analyzer that uses Apache Lucene's(v8.0) implementations of the standard tokenizer, Porter stemmer and some typical text cleansing techniques such as stopwords filtering and lowercase conversions for English documents' text analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">WatSAM-BM25.</head><p>In this run, we used Anserini's BM25 on our M collection that contains reliable medical websites only. We used Anserini's default English analyzer for indexing and retrieval with the same process as the baselineBM25 run, but using our M collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">WatSAE-BM25.</head><p>In this run we use Anserini's BM25 implementation on the E collection explained in section 2.2.3. We used Anserini's default English analyzer for indexing and default parameters for retrieval in this run as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">WatSAE-BM25RM3.</head><p>In this run we use Anserini's BM25 + RM3 implementation on the E collection explained in section 2.2.3. We used Anserini's default parameters.</p><p>3.1.5 WatSAE-BM25-RR. In this run, we rerank the top 1000 results returned by WatSAE-BM25 with a series of features. These features are as follows:</p><p>‚Ä¢ Presence of specific words in the URL such as "https", "buy", "shop" and "product".</p><p>‚Ä¢ Ratio of medical to total documents on the domain ‚Ä¢ PageRank score of the domain in the hosts' web graph ‚Ä¢ PageRank score of domain in graph subset discussed in section 2.2.3</p><p>‚Ä¢ whether the domain is part of the Reliable Medical Collection</p><p>The features were normalized using Z-score normalization. We use the sigmoid function to map the features into the [0,1] range. We give each feature a weight that we fine-tuned using a small number of manual judgements for the 2019 track topics. We calculate the new scores as follows:</p><formula xml:id="formula_1" coords="5,207.96,277.83,195.58,26.63">Score(ùëû, ùëë) = BM25(ùëû, ùëë) + ùëõ ‚àëÔ∏Å ùëñ ùúÜ ùëñ sigmoid (ùêπ ùëñ (ùëë))</formula><p>where ùúÜ ùëñ ) is the weight hyper-parameter for the ùëñth feature, ùêπ ùëñ (ùëë) is the normalized score of document ùëë for the ùëñth feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Manual Runs</head><p>Generally, our manual runs can be classified into three main categories: CAL-based active learning, stance detection model-based reranking, and CAL-assisted human assessments.  In this run, we used our assessment system on the M collection to manually find useful documents for each topic. We primarily focused on finding useful documents and disregarded the correctness and credibility of the information.</p><p>For each topic, we initialize a CAL model with the topic's query as the seed query. One assessor spent a maximum of 10 minutes per topic using both "Search" and "Discovery" components to find documents. On average, we found 32.22 useful documents per topic (min=5, max=121). We used all the judgments made during a topic's session to build a classifier to score all documents in the collection. To produce the run, we rank documents based on the CAL models' scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.3</head><p>WatSMM-CALHC. This run reranks the top 50 results from WatSMM-CAL based on the harmonic centrality <ref type="bibr" coords="7,519.71,414.51,11.59,8.83" target="#b6">[7]</ref> score of the hostnames, which are obtained from CommonCrawl <ref type="foot" coords="7,352.49,424.61,6.76,6.44" target="#foot_9">10</ref> . Harmonic centrality originates from the social networks field and can be used to determine influencing nodes in a graph. Our goal for this run is to utilize harmonic centrality scores to push the most influential documents to the top of the list. We filtered the list of hosts only to contain the hostnames found in the M collection and normalized the scores to be between 0 and 1. Table <ref type="table" coords="7,114.93,474.29,4.73,8.83" target="#tab_2">4</ref> shows the top 10 hosts based on the normalized score. The top 50 documents from WatSMM-CAL are reranked based on the following: CAL score √ó (1 + normalized harmonic centrality score) Table <ref type="table" coords="7,115.16,525.44,4.63,8.83">6</ref> shows an example of the ordering of documents before and after our reranking method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.4</head><p>WatSMM-CALPR. This run follows the same procedure as WatSMM-CALHC, except it uses PageRank scores of hostnames, which were also obtained from CommonCrawl. Table <ref type="table" coords="7,370.38,555.62,4.73,8.83">8</ref> shows the top 10 hosts based on the normalized score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.5</head><p>WatSMM-Fused. This run uses reciprocal rank fusion <ref type="bibr" coords="7,315.50,585.81,11.58,8.83" target="#b3">[4]</ref> on runs WatSMM-CAL, WatSMM-CALHC, and WatSMM-CALPR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.6</head><p>WatSMC-CAL. This run is similar to WatSMM-CAL, except it is using the C collection that contains more documents. For each topic, we initialize a CAL model with the judgments from WatSMM-CAL as seed judgments. We primed the model for the new collection with an additional round of judgments with a maximum of 5 minutes Table <ref type="table" coords="8,101.78,106.39,3.01,8.02">6</ref>. Example ordering based on harmonic centrality in WatSMM-CALHC for Topic #105 (Should I apply ice to burn?). While all results appear to contain useful information, the documents after the reordering allow more well-known websites (e.g. Mayo Clinic, WebMD, Healthline) to be ranked higher in the list. Table <ref type="table" coords="8,185.84,151.22,3.07,8.02">7</ref>. Before.</p><p>Rank URL per topic. Topics were split between two assessors. Like WatSMM-CAL, we only focus on finding useful documents. On average, we found 60 useful documents per topic (min=14, max=236). Documents are ranked in a similar fashion as WatSMM-CAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.7</head><p>WatSMM-CALQA100. For the six following runs, we experimented with using the RoBERTa language model <ref type="bibr" coords="8,108.48,382.38,11.78,8.83" target="#b5">[6]</ref> to rerank the results we obtained from our CAL models. RoBERTa, as with many transformer-based models, enforces a hard limit of 512 tokens as input. As such, using the whole document is often unfeasible. To work around this limitation, we select the most likely-relevant paragraph excerpt of the document to be used as part of the input to the language model. As shown in Table <ref type="table" coords="8,318.88,418.25,3.35,8.83" target="#tab_4">9</ref>, these excerpts are often much shorter than the cap.</p><p>To find the paragraphs excerpts, we first split each document in the collection into a set of paragraphs using newlines as a delimiter. Excerpts are constructed such that they contain a minimum of 100 words while ignoring lines with five or fewer words to avoid including boilerplate content. For each topic, we created a new CAL model using our previous judgments as seeds to train the model, and instead of scoring documents like in the previous runs, we scored all generated paragraph excerpts and selected the topmost scoring paragraph for each document. Examples of these paragraphs are shown in Table <ref type="table" coords="8,286.74,489.98,3.45,8.83" target="#tab_4">9</ref>. We use the topmost likely relevant paragraph excerpts to construct the top 1000 documents, with each document now having an associated most-likely relevant excerpt.</p><p>For our language model, we used roberta-large <ref type="bibr" coords="8,295.85,513.89,10.48,8.83" target="#b5">[6]</ref>, and fine-tune it on the BoolQ (for Boolean Questions) dataset <ref type="bibr" coords="8,112.57,525.84,10.58,8.83" target="#b2">[3]</ref>. The BoolQ dataset contains natural language questions in the form of yes/no, with each question paired with a paragraph from Wikipedia containing the answer. We choose this dataset as it aligns with the track's goal of finding the correct information for different topics that are already written in the form of a yes/no question in the description field (e.g., "Does duct tape work for wart removal?"). Our goal was to use the topics' description field with the paragraph excerpts to determine if the answer matches with the stance field of the topic, i.e. the document provides the correct information. If a document provides the correct information, we change its position such that it is placed higher than documents with incorrect answers while still maintaining the original order. For this particular run, we only reranked the top 100 scoring documents from the CAL model trained on the M collection.</p><p>We used a batch size of 8, a learning rate of 1e-5, and three training epochs for fine-tuning the model. A softmax layer is applied to get the yes/no answer probabilities. We assigned the final answer as "yes" if the probability </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic: 104</head><p>Topic description: Does duct tape work for wart removal? (Stance: unhelpful) Q: Does duct tape work on common warts? A: Occasionally recommended as a home remedy for warts, duct tape has not been confirmed as an effective treatment. Research is conflicting, but some people believe that doing the following may help to get rid of a common wart: Covering the wart with a small piece of duct tape Removing the duct tape every three to six days and gently using an emery board or pumice stone on the wart Covering the wart with a fresh piece of duct tape about 10 to 12 hours later Results may only be seen after a number of weeks, if at all. Duct tape can cause skin irritation, bleeding and pain when removed. It should never be used in sensitive areas, such as the underarms or face.</p><p>[28] (docno: en.noclean.c4-train.07150-of-07168.4483. See more of this document in Figure <ref type="figure" coords="9,508.64,240.16,9.33,8.85" target="#fig_0">1b</ref>)</p><formula xml:id="formula_2" coords="9,85.68,255.66,8.85,43.91">Topic: 105</formula><p>Topic description: Should I apply ice to a burn? (Stance: unhelpful) With a burn caused by a chemical, make sure the chemical or any clothing or jewelry in contact with the chemical is removed. If possible, use gloves so that you don't get burned elsewhere or so that someone helping you doesn't get burned. Put the burn under cool running water long enough to reduce pain, which may take about 10 to 15 minutes. If running water isn't available, you can immerse the burn in cool water or apply a cool, wet compress. Don't put ice directly on the burn. Dry the area with a clean cloth and apply a sterile, lightly wrapped bandage. Don't apply ointments or butter to a burn, as these can hold heat in the skin causing further damage in addition to increasing the risk of infection. (docno: en.noclean.c4-train.03543-of-07168.43352) Topic: 106 Topic description: Can vitamin b12 and sun exposure together help treat vitiligo? (Stance: helpful) Most people with vitiligo generally use vitamins and supplements in combination with other treatments. Some studies have shown that folic acid, B12, and sun exposure, when used together, can aid in repigmenting the skin. Consult your doctor for the appropriate dosages. A few supplement combinations can be dangerous when combined or when taken out of balance with one another. Common vitamin deficiencies in people with vitiligo include folic acid, B12, copper, and zinc. As a result, doctors may prescribe vitamin supplements to boost your immune system. Vitamin B12 with Folic Acid Studies focusing on vitamin B12 deficiencies and vitiligo show a high incidence of vitiligo among individuals with pernicious anemia, a condition that hinders B12 absorption. Nevertheless, no recent studies indicate that supplementing vitamin B12, or B12 with folic acid, will help skin pigmentation. (docno: en.noclean.c4train.04820-of-07168.113071)</p><p>of a yes answer is &gt; 0.5, and as "no" otherwise. We matched the final answer with the topic's stance field to determine whether or not an answer was correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.8</head><p>WatSMM-CALQAAll. This run is similar to WatSMM-CALQA100, except we do not enforce a reranking cutoff and instead rerank the entire list of documents. While this approach may introduce more irrelevant documents to be ranked higher in the list, it should effectively lower the rank of incorrect documents that can potentially be harmful and should not be shown to the user.</p><p>3.2.9 WatSMC-CALQA100. This run is similar to WatSMM-CALQA100. The main difference is that we are using the C collection, and for training the CAL models, we used the two rounds of judgments from WatSMM-CAL and WatSMC-CAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.10</head><p>WatSMC-CALQAAll. This run is similar to WatSMC-CALQA100, except, like WatSMM-CALQAAll, we do not enforce a reranking cutoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.11</head><p>WatSMC-CALQAHC1. This run sorts the predicted correct documents in WatSMC-CALQAAll based on their harmonic centrality score. Other documents are kept in their original order. The goal of this run is to introduce more correct and credible documents to be at the top of the page.</p><p>3.2.12 WatSMC-CALQAHC2. This run is similar to the previous run, except we only modify the ranking of predicted correct documents with domains part of the M collection. In other words, we ignore documents that were added outside of the M collection and only focus on sorting those that are part of the M collection. The intuition behind this method is that documents from the M collection are HONCode certified websites, and could be more reliable than the other websites in the C collection that were added using BM25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.13</head><p>WatSMT-SD-S1. The intuition behind the next two runs (WatSMT-SD-S1 and WatSMT-SD-S2) is that misinformation carries different stance from the "truth". Therefore, given the correct stance towards a topic, we can train a model to detect the stance of a document and compare it with the correct stance to determine whether this document is misinformation or not.</p><p>We obtained a binary classification model by fine-tuning T5-Large <ref type="bibr" coords="10,377.71,348.70,16.50,8.83" target="#b9">[10]</ref> on a loosely balanced subset of effectiveness judgments from 2019 qrels (around 400 training examples). To exploit this text-to-text transformer, we constructed the input in a way similar to the approach by <ref type="bibr" coords="10,327.14,372.61,10.43,8.83" target="#b7">[8]</ref>: "stance detection topic: " + query + "document: " + document content.</p><p>Due to the 512 input tokens limit of T5, we summarized the document by selecting sentences that were most relevant to the topic. We scored each sentence based on a list of stance words <ref type="foot" coords="10,389.99,406.62,6.76,6.44" target="#foot_10">11</ref> and the query terms. Specifically, for each word in the sentence, we first stemmed it and checked if it was among the stemmed stance words or the stemmed query terms. Each stance word would have a score of 1. Each query term would have a score relative to their position in the query because we wanted sentences to be more relevant to the treatment instead of the health issue. For example, for the query "yoga asthma", "yoga" would have a score of 2 and "asthma" would have a score of 1. However, in our post-TREC analysis, this query scoring scheme does not yield significantly better model performance than all query term with the equal score of 1. Then for each document, we selected those top-ranked sentences and concatenated them together in their original order in the document to form the input sequence of 512 words. We fine-tuned the model using the AdamW optimizer with a learning rate of 2e-5 and batch size of 16, with Early Stopping based on the F1-macro on the validation set (10% of the training set) with a patience of 3.</p><p>Similar to the work from <ref type="bibr" coords="10,195.60,539.98,10.56,8.83" target="#b8">[9]</ref>, we applied a Softmax function on the logits of the word "favor" and the word "against" at the first generated token to get binary classification probabilities. We further mapped them into a single correct_probability by incorporating the "correct stance" field of each topic. That is, if the given topic was helpful, we took the favor_probability as the correct_probability. Otherwise, we used the against_probability. We applied the stance model trained above to predict the stance of the top 3,000 documents of each topic from this year's baselineBM25 and reranked those documents using two strategies.</p><p>WatSMT-SD-S1 and WatSMT-SD-S2 differ in the strategy to combine the BM25_score and the correct_probability. For this run, we used the following strategy to promote correct documents and suppress incorrect documents: final_score = BM25_score √ó ùëí correct_probability-0.5 3.2.14 WatSMT-SD-S2. This run used a different reranking strategy from the one used in WatSMT-SD-S1. We preferred to order documents as: (1) documents with a clear and correct stance, (2) documents without a clear stance, and (3) documents with a clear but incorrect stance. Specifically, we used the following rule:</p><formula xml:id="formula_3" coords="11,176.18,211.52,258.44,45.64">final_score = Ô£± Ô£¥ Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£¥ Ô£≥ BM25_score √ó 10, if correct_probability &gt; 0.75 -BM25_score,</formula><p>if correct_probability &lt; 0.25 BM25_score, otherwise 3.2.15 WatSMC-Correct. In this run, we manually assessed documents for usefulness and correctness using our high-recall retrieval system's "Search" and "Discovery" components <ref type="bibr" coords="11,361.11,273.87,10.49,8.83" target="#b1">[2]</ref>. The idea behind the WatSMC-Correct run was that training CAL on correct documents would allow us to find other correct documents while avoiding incorrect documents. Having the CAL model be trained on only correct documents would allow it to more easily learn the problem of finding correct documents than to let it learn incorrect documents as well, since the difference between correct and incorrect documents is mostly only the stance which is only a few words. For this run, we trained CAL on useful and correct documents, unlike the other CAL-based runs which ignored stance. Thus, we can compare performances between our different CAL-based approaches. At the same time, we can use our WatSMC-Correct run to better evaluate our other automatic and manual runs.</p><p>For the first judgement round, two assessors were restricted to using the "Search" interface (interactive search and judging) for approximately 10 minutes per topic to judge documents. Here, our definition of usefulness was different from our previous runs. A document was judged as "highly useful" if it contained an answer to the health issue that matched the stance given by the track for the topic, and assessors could mark other documents as "not useful" to keep track that they had viewed the document and decided it either was not useful or it was useful but incorrect. For the second round of judgement, the two assessors used the "Discovery" interface (CAL), with the first round's judgements for each topic used as seeds for training. The two assessors spent approximately 10 minutes each per topic to judge documents in this round. To speed up the assessment phase, the assessors were presented with most likely paragraph excerpt of the document to make their judgements. A document was marked as "highly useful" if its summary contained a correct answer. If the summary represented the document as one that was useful but contained an incorrect answer or lacking an answer, it was marked as "useful". Lastly, if the summary was not useful, then the document was marked as "not useful".</p><p>Figure <ref type="figure" coords="11,119.42,512.97,4.70,8.83">2</ref> shows the total number of correct documents found by assessors for each topic which was used to train the final CAL model and were placed at the top of the run. The figure also shows that for each topic, the total number of correct documents is comprised of documents found using "Search" (interactive search and judging), denoted by the green bar, and additional correct documents found using "Discovery" (CAL), denoted by the blue bar. In total, 1481 correct documents were found by assessors across all topics, with 429 correct documents found using "Search" and 1052 additional correct documents found using "Discovery".</p><p>Both assessors agreed that it was harder to find correct documents for topics with an "unhelpful" stance compared to the topics with a "helpful" stance. Additionally, while using "Discovery" (CAL), many near-duplicate documents were returned to the assessors for judgement.</p><p>To create the run, the documents judged highly useful in both rounds of judging were placed first, followed by documents returned by the final CAL model trained on "highly useful" documents only (the other judgements were ignored for training the CAL model).  <ref type="figure" coords="12,95.87,388.71,3.01,8.02">2</ref>. This plot shows the total number of correct documents found by our assessors for each topic for the WatSMC-Correct run. Additionally, for each topic, the green bar indicates the number of correct documents found using "Search" (interactive search and judging) and the blue bar indicates the number of additional correct documents found using "Discovery" (CAL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULT AND DISCUSSION</head><p>Coverage refers to the percentage of documents that have been assessed. Table <ref type="table" coords="12,422.92,453.57,9.45,8.83" target="#tab_0">10</ref> shows the coverage of assessed documents among top ùëò documents in each run. We can argue that metrics focusing on the top 20 and fewer documents should be fair and objective among our runs, where we don't need to consider the case of useful/credible/correct documents not being assessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Automatic Runs</head><p>Table <ref type="table" coords="12,105.36,526.41,9.45,8.83" target="#tab_0">11</ref> shows our automatic BM25 run on the M collection (WatSAM-BM25) and the E collection (WatSAE-BM25) performing better than the baseline in terms of compatibility (0.055 and 0.042 vs. -0.015). In Figure <ref type="figure" coords="12,510.22,538.36,3.41,8.83" target="#fig_1">3</ref>, we can see the increase in helpful and decrease in harmful results for the filtered collections. It appears that using a refined dataset for retrieval can be an effective approach in reducing harmfulness and increasing helpfulness. It also appears that expanding M using the common crawl domain link graph and filtering out non-medical web pages to construct collection E further improved compatibility over the baseline with collection E giving a boost to helpful at the cost of a slight increase in harmful results.</p><p>In Table <ref type="table" coords="12,126.10,610.10,7.66,8.83" target="#tab_0">11</ref>, we also see a significant jump for precision at rank 10 in WatSAE-BM25 and WatSAM-BM25 for credible documents compared to the baseline (0.557 and 0.586 vs. 0.417). This increase shows that collection filtering can help us retrieve credible documents. These runs also have an increase incorrect results as well (0.288 and 0.288 vs. 0.203). We also see a drop in the number of incorrect documents retrieved by the two methods compared to the baseline (0.194 and 0.209 vs. 0.291). It appears that in addition to returning more correct information, the collections help in reducing the amount of incorrect information returned to the users.</p><p>The goal of creating E was to create a smaller, more reliable and more encompassing collection than M, which could aid in downstream tasks such as acting as a source of truth. While results show that the methods used were effective, further refinements need to be made before using this collection to detect the stance of a query automatically.</p><p>Unfortunately, further modifications to the WatSAE-BM25 run worsened results. Relevance feedback did not improve compatibility (0.031 vs. 0.055). Our reranking also worsened results (0.027 vs. 0.055). This is likely due to limited tuning data. As discussed in 2.2.3, previous years did not have a domain link graph, so using them as training data was not as effective as they were missing certain features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Credibility-based Filtering and Reranking</head><p>Our goal for the M collection was to create a subset of the collection that only contains documents from reliable sources. In this collection, we used HONCode certification as our method of determining reliability, in addition to manually adding a few reliable websites. The certification assessment is done by medical experts that determined such websites to be reliable in providing medical information. As such, in our first five manual runs, we primarily focused on returning documents that were deemed useful and anticipated their correctness in providing health information to match the truth.</p><p>Table <ref type="table" coords="13,113.88,328.21,9.08,8.83" target="#tab_1">13</ref> shows precision scores under different criteria. Our baseline run, baselineBM25 differs from WatSAM-BM25 only in the type of collection used. In WatSAM-BM25, we used the M collection that contains reliable websites. In terms of finding correct documents, our baseline run and WatSAM-BM25 seems to have about the same number of correct documents in the top 10 results, but WatSAM-BM25 performs better in terms of returning credible documents in terms of precision@10. This difference in credibility is statistically significant using a two-tailed paired t-test (ùëù = 0.008).</p><p>In terms of the overall compatibility measure (Table <ref type="table" coords="13,309.50,399.94,7.36,8.83" target="#tab_0">11</ref>), all of our automatic runs perform better than the baseline, with our WatSAE-BM25 run, which uses our smallest collection E, performing the best. Overall, the results indicate that using traditional retrieval methods with the filtering techniques described in Section 2.2 provides better performance than simply using the entire collection.</p><p>In WatSMM-CAL, we used our high-recall system to retrieve as many useful documents as possible from the M collection.</p><p>We also experimented with reranking based on harmonic centrality and PageRank. Both WatSMM-CALHC and WatSMM-CALPR attempt to push more influential to the top of the list. There is a slight increase in credibility over WatSMM-CAL, but the result is not statistically significant. All runs seem to have similar scores in terms of overall compatibility measure (Table <ref type="table" coords="13,201.22,507.54,7.21,8.83" target="#tab_0">11</ref>), with WatSMM-CALPR having slightly lower performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Correctness and Stance detection-based reranking</head><p>We also experimented with reranking the results from the CAL models using the RoBERTa language model. The language model was fine-tuned on the BoolQ dataset, which contains questions in the form of yes/no. The goal of using the language model is to determine whether a relevant excerpt from a document contains an answer that matches with the topic's stance (i.e., whether the document has correct or incorrect information). Both WatSMM-CALQA100 and WatSMM-CALQAAll attempts to rerank the results based on correctness under the M collection. In terms of helpful compatibility, both appear to perform the same, but reranking the complete set of results, as in WatSMM-CALQAAll, appears to lower the harmful compatibility score. The results are also similar for WatSMC-CALQA100 and WatSMC-CALQAAll with the C collection. Under this reranking method, our best run in terms of helpful compatibility is WatSMC-CALQAHC1, where we further rerank the documents marked as correct based on harmonic centrality, in an attempt to push more influential and correct documents to the top of the list. In terms of precision@10, this run has the highest score for documents that are useful &amp; correct &amp; credible. Compared to runs that used CAL scores alone, the runs that used the RoBERTa-based reranking method have better overall compatibility performance.</p><p>For WatSMT-SD-S1 and WatSMT-SD-S2, we applied a stance detection model to promote correct documents and suppress incorrect documents. Among our manual runs, those two are the only ones that don't utilize manual assessments, although they need the correct stance toward each health treatment. From the performance in Tables <ref type="table" coords="14,109.42,191.72,9.45,8.83" target="#tab_0">11</ref> and<ref type="table" coords="14,139.38,191.72,7.79,8.83" target="#tab_1">13</ref>, we can see that WatSMT-SD-S1 and WatSMT-SD-S2 are comparable with those other manual runs in terms of correctness. Specifically, if we focus on the Compatibility measure (helpful -harmful) in Table <ref type="table" coords="14,80.47,215.63,7.79,8.83" target="#tab_0">11</ref>, we can notice that WatSMT-SD-S1 is the most competitive run approaching human level (0.183 v.s. 0.226). This fact also demonstrates the power of pre-trained language models applied to stance detection tasks. In the future, there is still much room for further improving the classification performance of the stance detection model. Meanwhile, we can also add a reranking stage between the BM25 stage and the stance detection stage to improve the relevance and credibility of top results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">WatSMC-Correct Run</head><p>The WatSMC-Correct run was created by having assessors manually judge documents to include only correct documents into the training set for CAL. As such, we expected that the run would perform well. Results show that the run did indeed perform generally well across all metrics. It was our best run as per the Compatibility (helpful -harmful), Compatibility (helpful), nDCG (Useful &amp; Correct), nDCG (Useful &amp; Correct &amp; Credible) and P@10 (Useful &amp; Correct) measures. However, under some other metrics, there are other runs that performed better than the WatSMC-Correct run. This could be because assessors only had a limited time to make assessments and/or for some topics, it was hard for assessors to manually find many correct documents. Another observation we found from the results was that even though assessors were able to find at least two correct documents for every topic, the P@10 score for a few topics was still 0. Lastly, as the motivation of this run was to help us better evaluate our other runs, we can see that some of the other runs did a comparable job to what humans can do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this report, we introduced several methods to tackle the challenge of misinformation in online health searches.</p><p>To retrieve correct and credible results, we constructed curated collections based on the URL domain credibility. Running BM25 on these collections achieved higher compatibility scores compared to the baseline, returning more helpful and fewer harmful documents.</p><p>We also used continuous active learning to find useful documents within these curated collections, relying on the idea that these collections should contain credible information. This approach resulted in a higher compatibility score than the baseline and BM25 runs on those same curated collections.</p><p>Our runs utilizing the correct topic stance showed the value of using pre-trained language models to detect the stance of documents. Runs using this approach showed a sizeable drop in the number of harmful documents returned as well as an increase in the number of helpful documents returned. Table <ref type="table" coords="17,101.93,106.39,6.76,8.02" target="#tab_0">12</ref>. Performance of the runs using the nDCG measure. An asterisk indicates statistical significance computed over the baselineBM25 run at ùëù &lt; 0.05 using a two tailed paired t-test. For each metric we highlight the best result for automatic and manual runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run tag</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,201.96,674.48,208.07,8.02;6,84.09,386.52,450.59,253.90"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Screenshots of our document assessment system.</figDesc><graphic coords="6,84.09,386.52,450.59,253.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="16,80.70,460.45,450.61,8.02;16,80.70,471.40,450.60,8.02;16,80.70,482.36,450.60,8.02;16,80.70,493.32,50.42,8.02"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Helpful-Harmful Compatibility plot. "WatS" has been dropped from run names. Runs in the lower right corner have a better compatibility metric. The cluster of runs in the lower right all make use of the topic stance. Note the dramatic shift in compatibility between WatSMC-CAL and WatSMC-Correct. This shows that training for correctness can greatly improve compatibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,85.68,106.39,447.79,149.18"><head>Table 1 .</head><label>1</label><figDesc>Summary of our filtered collections used to build our runs.</figDesc><table coords="2,85.68,132.90,447.79,122.68"><row><cell cols="2">Tag Name</cell><cell cols="2"># Documents Description</cell></row><row><cell>A</cell><cell>c4/en.noclean</cell><cell cols="2">1,063,805,381 The track's collection. The collection is comprised of text extracts from</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the April 2019 snapshot of Common Crawl web crawl corpus.</cell></row><row><cell cols="2">M Reliable Medical Collection</cell><cell>3,568,939</cell><cell>This collection only includes documents with domains having an HON-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>code certification (see www.hon.ch for more details) or are part of 13</cell></row><row><cell></cell><cell></cell><cell></cell><cell>handpicked health related websites (e.g. kidshealth.org).</cell></row><row><cell>C</cell><cell>M + 10k BM25</cell><cell>4,040,012</cell><cell>This collection expands M by including the top 10k BM25 results per</cell></row><row><cell></cell><cell></cell><cell></cell><cell>topic from the complete collection.</cell></row><row><cell>E</cell><cell>Expanded Reliable Medical</cell><cell>1,829,111</cell><cell></cell></row><row><cell></cell><cell>Collection</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,126.59,106.34,358.57,8.06"><head>Table 3 .</head><label>3</label><figDesc>Top 10 ranked domains by harmonic centrality and PageRank scores in the M collection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,87.63,129.30,211.20,159.93"><head>Table 4 .</head><label>4</label><figDesc>Host names sorted by harmonic centrality score.</figDesc><table coords="7,114.51,155.85,157.67,133.38"><row><cell cols="2">Rank Score Host name</cell></row><row><cell>1</cell><cell>1.000 psychologytoday.com</cell></row><row><cell>2</cell><cell>0.997 webmd.com</cell></row><row><cell>3</cell><cell>0.978 fda.gov</cell></row><row><cell>4</cell><cell>0.955 healthline.com</cell></row><row><cell>5</cell><cell>0.951 columbia.edu</cell></row><row><cell>6</cell><cell>0.948 heart.org</cell></row><row><cell>7</cell><cell>0.946 mayoclinic.org</cell></row><row><cell>8</cell><cell>0.946 cancer.org</cell></row><row><cell>9</cell><cell>0.941 psychcentral.com</cell></row><row><cell>10</cell><cell>0.939 aarp.org</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,80.70,129.30,425.46,192.20"><head>Table 5 .</head><label>5</label><figDesc>Host names sorted by PageRank score.</figDesc><table coords="7,337.33,155.85,162.65,133.38"><row><cell cols="2">Rank Score Host name</cell></row><row><cell>1</cell><cell>1.000 fda.gov</cell></row><row><cell>2</cell><cell>0.763 webmd.com</cell></row><row><cell>3</cell><cell>0.671 psychologytoday.com</cell></row><row><cell>4</cell><cell>0.572 mayoclinic.org</cell></row><row><cell>5</cell><cell>0.418 healthline.com</cell></row><row><cell>6</cell><cell>0.390 e-monsite.com</cell></row><row><cell>7</cell><cell>0.317 cancer.org</cell></row><row><cell>8</cell><cell>0.305 heart.org</cell></row><row><cell>9</cell><cell>0.290 aarp.org</cell></row><row><cell>10</cell><cell>0.264 medicalnewstoday.com</cell></row></table><note coords="7,80.70,312.55,80.28,8.96"><p>3.2.2 WatSMM-CAL.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,210.22,106.39,191.32,8.02"><head>Table 9 .</head><label>9</label><figDesc>Example paragraph excerpts for few topics.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,83.93,607.04,154.24,7.06"><p>https://www.tensorflow.org/datasets/catalog/c4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,84.07,617.01,134.20,7.06"><p>https://www.hon.ch/en/certification.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,84.07,626.97,67.91,7.06"><p>https://www.who.int</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,83.93,636.93,93.90,7.06"><p>https://www.mayoclinic.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,84.07,646.90,181.46,7.06"><p>https://github.com/healthonnet/hon-honcode-extension</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,84.07,624.29,181.18,7.06"><p>https://www.honcode.ch/HONcode/Plugin/listeMD5.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="3,83.93,634.26,119.72,7.06"><p>https://github.com/castorini/anserini</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="3,84.07,644.22,300.93,7.06"><p>https://commoncrawl.org/2020/02/host-and-domain-level-web-graphs-novdecjan-2019-2020/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8" coords="5,80.70,393.70,450.61,8.96;5,80.70,405.70,286.47,8.83;5,367.17,403.85,3.38,6.44;5,371.05,405.70,160.25,8.83;5,80.70,417.66,450.61,8.83;5,80.70,429.61,451.20,8.83;5,80.70,441.57,450.61,8.83;5,80.70,453.52,450.61,8.83;5,80.70,465.48,450.97,8.83;5,80.70,477.43,450.61,8.83;5,80.70,489.39,450.60,8.83;5,80.70,501.35,312.67,8.83;5,90.66,513.30,440.65,8.83;5,80.70,525.26,450.61,8.83;5,80.70,537.21,451.73,8.83;5,80.70,549.17,450.61,8.83;5,80.70,561.12,450.60,8.83;5,80.70,573.08,450.61,8.83;5,80.70,585.03,450.60,8.83;5,80.70,596.99,450.61,8.83;5,80.70,608.94,450.61,8.83;5,80.70,620.90,176.60,8.83;5,80.70,644.87,135.64,8.78"><p>3.2.1 CAL-based Manual Assessments.We used an improved version of our high-recall retrieval system<ref type="bibr" coords="5,519.44,393.75,11.87,8.83" target="#b1">[2]</ref> to manually assess some of the documents in our filtered collections<ref type="bibr" coords="5,367.17,403.85,3.38,6.44" target="#b8">9</ref> . This system was successfully used in previous TREC tracks for building manual runs<ref type="bibr" coords="5,275.84,417.66,10.38,8.83" target="#b0">[1,</ref><ref type="bibr" coords="5,288.71,417.66,11.26,8.83" target="#b10">11]</ref>. The system has two main components: "Search", which allows interactive search and judging, and "Discovery", which is based on Continuous Active Learning (CAL) that prompts the user with the next-likely relevant documents based on a logistic regression model. Given a seed query or seed documents, the system initiates a CAL session and presents the user with documents that are predicted as most-likely relevant to the seed. As the user judges a document, the model is retrained with the new judgment, and the next likely relevant document is then presented to the user. This process keeps going until all documents are judged or when the user reaches their allocated judging budget for the topic. In our case, we allocated fixed time periods of 10 or 5 minutes where the user keeps judging.Screenshots of both components are shown in Figure1. The "Search" interface in Figure1auses Anserini's implementation of BM25 scoring to retrieve documents for user-submitted queries. Any judgment made while in the "Search" interface is used to retrain the CAL model. Figure1bshows an example of the "Discovery" interface, where the next-likely relevant document from the CAL model is presented to the user after having previously judged some documents. As the interface shows, the system allows different judging criteria to be submitted for each document (e.g., usefulness, credibility, and supportiveness). For all our CAL-based runs except WatSMC-Correct, only the usefulness of a document is used for training, where highly useful and useful documents are treated as positive samples and not useful documents as negative samples. To speed up the judging process in our document assessing phase of building the runs, we only focused on judging the usefulness of documents and left other criteria unjudged. 9 https://github.com/UWaterlooIR/gathera</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="7,86.80,647.18,288.10,7.06"><p>https://commoncrawl.org/2019/11/host-and-domain-level-web-graphs-aug-sep-oct-2019/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="10,86.80,635.67,445.38,7.06;10,80.70,645.64,109.66,7.06"><p>Stance words: help, treat, benefit, effective, safe, evidence, improve, harm, hurt, useful, prove, ineffective, limit, poor, lack, insufficient, consider, quality, against, reliable.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada</rs> (<rs type="grantNumber">RGPIN-04665-2020</rs>, <rs type="grantNumber">RGPAS-00080-2020</rs>), in part by <rs type="funder">Google</rs>, in part by <rs type="funder">Compute Canada</rs>, and in part by the <rs type="funder">University of Waterloo</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pSeuqyZ">
					<idno type="grant-number">RGPIN-04665-2020</idno>
				</org>
				<org type="funding" xml:id="_7K9Uvgw">
					<idno type="grant-number">RGPAS-00080-2020</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>P@10 P@50 P@10 P@50 P@10 P@50 P@10 P@50 P@10 P@50 baselineBM25 0. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="16,97.77,537.45,433.53,7.06;16,97.77,547.37,19.61,7.13" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,357.77,537.45,161.30,7.06">UWaterlooMDS at the TREC 2019 Decision Track</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Mustafa Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beylunioƒülu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Duimering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,97.77,547.37,15.69,7.13">TREC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.77,557.33,433.53,7.13;16,97.77,567.29,434.05,7.13;16,97.77,577.30,53.64,7.06" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="16,501.80,557.33,29.50,7.13;16,97.77,567.29,106.76,7.13">A System for Efficient High-Recall Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210176</idno>
		<ptr target="https://doi.org/10.1145/3209978.3210176" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1317" to="1320" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.77,587.26,433.53,7.06;16,97.77,597.22,254.18,7.06" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv:cs.CL/1905.10044</idno>
		<title level="m" coord="16,475.25,587.26,56.05,7.06;16,97.77,597.22,171.90,7.06">BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.77,607.18,433.53,7.06;16,97.77,617.11,433.53,7.13;16,97.77,627.07,428.43,7.13" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,324.33,607.18,206.97,7.06;16,97.77,617.15,77.44,7.06">Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Buettcher</surname></persName>
		</author>
		<idno type="DOI">10.1145/1571941.1572114</idno>
		<ptr target="https://doi.org/10.1145/1571941.1572114" />
	</analytic>
	<monogr>
		<title level="m" coord="16,189.84,617.11,341.47,7.13;16,97.77,627.07,63.35,7.13">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;09)</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.77,637.03,433.54,7.13;16,97.77,646.99,172.95,7.13" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,190.62,637.07,266.12,7.06">Topic-sensitive pagerank: A context-sensitive ranking algorithm for web search</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,464.50,637.03,66.81,7.13;16,97.77,646.99,102.10,7.13">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="784" to="796" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.77,405.49,433.53,7.06;17,97.77,415.46,333.61,7.06" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv:cs.CL/1907.11692</idno>
		<title level="m" coord="17,150.83,415.46,197.95,7.06">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.77,425.38,433.72,7.13;17,97.53,435.38,220.38,7.06" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,238.29,425.42,92.15,7.06">Harmony in the small-world</title>
		<author>
			<persName coords=""><forename type="first">Massimo</forename><surname>Marchiori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vito</forename><surname>Latora</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0378-4371(00)00311-3</idno>
		<ptr target="https://doi.org/10.1016/s0378-4371(00)00311-3" />
	</analytic>
	<monogr>
		<title level="j" coord="17,336.72,425.38,167.39,7.13">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">285</biblScope>
			<biblScope unit="page" from="539" to="546" />
			<date type="published" when="2000-10">2000. Oct 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.77,445.34,433.53,7.06;17,97.77,455.27,434.41,7.13;17,97.59,465.27,198.50,7.06" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,333.19,445.34,198.11,7.06;17,97.77,455.31,18.88,7.06">Document Ranking with a Pretrained Sequence-to-Sequence Model</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.63</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.findings-emnlp.63" />
	</analytic>
	<monogr>
		<title level="m" coord="17,131.24,455.27,227.55,7.13">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.77,475.19,433.53,7.13;17,97.77,485.15,433.53,7.13" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="17,322.66,475.19,208.64,7.13;17,97.77,485.15,73.68,7.13">Vera: Prediction Techniques for Reducing Harmful Misinformation in Consumer Health Search</title>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3463120</idno>
		<ptr target="https://doi.org/10.1145/3404835.3463120" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="2066" to="2070" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.77,495.16,434.76,7.06;17,97.77,505.08,384.57,7.13" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="17,97.77,505.12,254.75,7.06">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,97.77,515.08,434.76,7.06;17,97.77,525.01,434.05,7.13;17,97.77,535.01,77.53,7.06" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,97.77,525.05,178.37,7.06">UWaterlooMDS at the TREC 2017 Common Core Track</title>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angshuman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><surname>Grossman</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec26/papers/UWaterlooMDS-CC.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,290.21,525.01,102.75,7.13">Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
