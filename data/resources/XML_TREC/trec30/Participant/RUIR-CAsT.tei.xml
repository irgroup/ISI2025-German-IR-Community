<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.88,115.96,297.59,12.62">Radboud University at TREC CAsT 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.08,156.75,54.80,8.74"><forename type="first">Hideaki</forename><surname>Joko</surname></persName>
							<email>hideaki.joko@ru.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.76,156.75,76.51,8.74"><forename type="first">Emma</forename><forename type="middle">J</forename><surname>Gerritse</surname></persName>
							<email>emma.gerritse@ru.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.79,156.75,64.79,8.74"><forename type="first">Faegheh</forename><surname>Hasibi</surname></persName>
							<email>f.hasibi@cs.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.58,156.75,74.70,8.74"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
							<email>a.devries@cs.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.88,115.96,297.59,12.62">Radboud University at TREC CAsT 2021</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B7B85FCF11EE8490173576B6A905C8C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes Radboud University's participation in the TREC Conversational Assistance Track (CAsT) 2021 for the manually rewritten utterances. We propose an entity-enriched BERT-based retrieval model, where entity information is injected into the BERT model, and compare it to the regular BERT-based retrieval model. We annotate the manually resolved user utterances with named entities using an entity linker, and inject both text and entity representations into our entity-enriched BERT-based retrieval model. We present our experimental setup, results, and analysis of helped and hurt queries when using entity information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the growing popularity of personal assistants such as Google Assistant and Alexa, conversational systems are becoming more important. In conversational systems, entity information is known to be useful to understand user utterances and generate natural and meaningful responses <ref type="bibr" coords="1,350.31,485.63,11.32,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,361.63,485.63,7.55,8.74" target="#b4">5,</ref><ref type="bibr" coords="1,369.17,485.63,7.55,8.74" target="#b5">6,</ref><ref type="bibr" coords="1,376.72,485.63,11.32,8.74" target="#b9">10]</ref>. Despite the importance of entity information in conversational systems, the research on utilizing entity information in conversational document retrieval has so far been limited.</p><p>In the paper, we report on our participation in the TREC 2021 Conversational Assistance Track (CAsT), which addresses the task of document retrieval in conversational context. Our main objective is to understand the effect of entity information on BERT-based retrieval models for this task. To this end, we utilize entity information in an entity-enhanced BERT-based retrieval model. We map entity embeddings into the same space as a BERT model and use this model to rank documents based on monoBERT re-ranking method <ref type="bibr" coords="1,432.37,596.34,9.96,8.74" target="#b6">[7]</ref>. We use an entity linker to identify named entities mentioned in the conversation utterances and inject entity embedding, along with BERT wordpiece embeddings to our BERT-based retrieval model. Our results suggest that including only named entity information in our retrieval model does not bring added value compared to a regular BERT-based retrieval model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Conversational Assistance Track</head><p>CAsT is defined as a traditional document retrieval task, where queries are issued in a conversational context <ref type="bibr" coords="2,257.56,154.25,9.96,8.74" target="#b1">[2]</ref>. The specific task, referred to as conversational document retrieval task, is as follows. Given the user utterances</p><formula xml:id="formula_0" coords="2,248.94,190.11,231.65,9.65">U n = {u 1 , u 2 , ..., u i , ..., u n },<label>(1)</label></formula><p>the system is required to find a set of relevant documents D i for every user utterance u i from the document collection D. Each user utterance may contain ambiguity, have references to other utterances, or be dependent on the response of previous utterances. CAsT distinguishes two submission groups: raw and manually rewritten utterances. In the latter group, the dependency and ambiguity of user utterances are manually resolved. Each manually rewritten utterance u * i contains all necessary information to obtain relevant documents D i for a turn. This, however, does not necessarily imply that conversation context is not important for retrieval anymore; even if the user utterances are self-contained, conversational context makes it easier to understand the current user utterances. In our participation, we focus on manually rewritten queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our retrieval model is an entity-enhanced BERT-based retrieval model, a hybrid between monoBERT <ref type="bibr" coords="2,251.11,393.66,10.52,8.74" target="#b6">[7]</ref> and E-BERT <ref type="bibr" coords="2,327.43,393.66,9.96,8.74" target="#b8">[9]</ref>. We utilize Wikipedia2Vec <ref type="bibr" coords="2,462.32,393.66,14.61,8.74" target="#b10">[11]</ref>, which jointly embeds words L word and entities L ent using the lookup function E Wikipedia into the vector space R d Wikipedia , where d Wikipedia are the dimensions of the Wikipedia2Vec embeddings. Following E-BERT <ref type="bibr" coords="2,380.64,429.53,9.96,8.74" target="#b8">[9]</ref>, we map the entity embeddings from Wikipedia2Vec into the BERT wordpiece vector space R d BERT using a linear transformation. The linear mapping W ∈ R d BERT ×d Wikipedia is obtained by minimizing:</p><formula xml:id="formula_1" coords="2,203.68,485.91,276.91,22.84">x∈L word ∩L W P ||W • E Wikipedia (x) -E BERT (x)|| 2 2 ,<label>(2)</label></formula><p>where E BERT is the lookup function, mapping BERT wordpiece dictionary L wp to R d BERT , with d BERT being the dimension of the BERT input embeddings. Using the found linear mapping W, we obtain transformed entity embeddings using:</p><formula xml:id="formula_2" coords="2,246.51,566.88,234.08,9.68">E en (x) = W • E Wikipedia (x),<label>(3)</label></formula><p>where x represents an entity. The transformed entity embeddings are then injected to the BERT model. We employ this entity-enriched BERT model in our retrieval model and perform pointwise re-ranking based on monoBERT. The input to our model consists of entity-annotated query-document pairs, with the annotations predicted by REL <ref type="bibr" coords="2,172.68,644.16,9.96,8.74" target="#b3">[4]</ref>. These queries and documents are then tokenized where all entities are embedded using Equation 3 and all other tokens using E BERT .</p><p>The model is fine-tuned on the MS MARCO passage ranking collection, using 600k randomly selected query-document pairs with a batch size of 64, a learning rate of 10 -6 , and 40k warm-up steps. For Wikipedia2Vec, we used the pre-trained embeddings from <ref type="bibr" coords="3,212.62,154.86,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs and Results</head><p>In this section, we describe our runs and present the results on the TREC CAsT 2020 and 2021 datasets, followed by analysis of helped and hurt queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Runs</head><p>We submitted three official runs:</p><p>-RU-turn: Uses the manually resolved form of the current turn u * i as input for our method; see Section 3.</p><p>-RU-turn-finetuning: Uses the current turn u * i as input, and the model is fine-tuned on CAsT 2020 dataset. We split the dataset into train, validation, and test, containing 15, 5, and 5 conversations, respectively. The settings for fine-tuning are the same as Section 3, except that batch size 8 is used.</p><p>-RU-history: For each turn u * i , uses current and previous turns {u * 1 , ..., u * i } as input to model, where turns are concatenated with space.</p><p>In addition to our official runs, we report on the following two runs:</p><p>-monoBERT-turn: The model described in <ref type="bibr" coords="3,350.95,464.23,9.96,8.74" target="#b6">[7]</ref>, and current run as input.</p><p>-baseline: The official baseline model<ref type="foot" coords="3,315.73,475.88,3.97,6.12" target="#foot_0">1</ref> , which is a T5-based retrieval model using BM25 as first stage retriever.</p><p>We use the baseline method as first stage retriever for all five runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on CAsT 2020 dataset</head><p>Similar to this year's CAsT, the 2020 dataset provides manually rewritten queries for conversations. Table <ref type="table" coords="3,241.50,587.59,4.98,8.74" target="#tab_0">1</ref> shows the results of our runs for CAsT 2020 dataset.</p><p>The results indicate that RUIR-turn outperforms the other methods: baseline and monoBERT-turn, suggesting that information from named entities might be beneficial for conversational document retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on CAsT 2021 dataset</head><p>Table <ref type="table" coords="4,164.19,225.15,4.98,8.74" target="#tab_1">2</ref> presents the results on the CAsT 2021 dataset. Median represents the mean of officially provided median values of all submitted runs for manually rewritten submission group. We observe that monoBERT outperforms our entity-enriched models, suggesting that the injected information from named entities does not lead to improvements. Additionally, the RU-turn run outperforms the RU-turn-fine-tuning and RU-hist runs, indicating that using history as input does not improve retrieval performance. Another observation is that fine-tuning on CAsT 2020 hurts the performance. We attribute this to known BERT instabilities in few-sample fine-tuning <ref type="bibr" coords="4,332.11,320.79,14.61,8.74" target="#b11">[12]</ref>. Table <ref type="table" coords="4,176.51,475.53,4.98,8.74" target="#tab_2">3</ref> presents the performance results on the queries from Y3 where entity annotations are available. Out of 158 queries, 48 contain at least one entity annotation (30.3%). We observe that even for the queries where entity annotations are available, monoBERT outperforms our entity-enriched BERT models. We cannot yet explain why news articles behave so differently in our entityfocused models, as it is generally believed that entities are predominant in news. Our best guess it that the finetuning setup is not well suited to adapt to the language use in the newspaper articles, and more beneficial to the style of writing on Wikipedia and the web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed an entity-enriched BERT-based retrieval model, where named entity information is injected into a BERT-based model. Our model was employed on TREC CAsT 2020 and 2021 manually rewritten utterances. We compared our model with the regular BERT-based retrieval model and analyzed the helped and hurt queries when named entity information is used. We found that injecting only named entity information is not sufficient to improve performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,158.19,115.91,300.65,63.53"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results for CAsT 2020 manually written dataset.</figDesc><table coords="4,158.19,136.71,300.65,42.73"><row><cell></cell><cell cols="3">MAP MRR Recall NDCG@3 NDCG@5 NDCG@500</cell></row><row><cell>baseline</cell><cell>0.272 0.772 0.508 0.479</cell><cell>0.461</cell><cell>0.451</cell></row><row><cell cols="2">monoBERT-turn 0.272 0.781 0.508 0.496</cell><cell>0.481</cell><cell>0.451</cell></row><row><cell>RUIR-turn</cell><cell>0.273 0.791 0.508 0.498</cell><cell>0.483</cell><cell>0.453</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,153.45,352.34,310.13,96.41"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results for CAST 2021 manually written dataset.</figDesc><table coords="4,153.45,373.14,310.13,75.61"><row><cell></cell><cell cols="3">MAP MRR Recall NDCG@3 NDCG@5 NDCG@500</cell></row><row><cell>baseline</cell><cell>0.417 0.868 0.746 0.595</cell><cell>0.588</cell><cell>0.649</cell></row><row><cell>median</cell><cell>0.371 NaN NaN 0.555</cell><cell>0.550</cell><cell>0.612</cell></row><row><cell>monoBERT-turn</cell><cell>0.418 0.872 0.746 0.597</cell><cell>0.589</cell><cell>0.650</cell></row><row><cell>RU-turn</cell><cell>0.390 0.829 0.746 0.554</cell><cell>0.560</cell><cell>0.626</cell></row><row><cell cols="2">RU-turn-finetuning 0.378 0.818 0.746 0.554</cell><cell>0.548</cell><cell>0.618</cell></row><row><cell>RU-hist</cell><cell>0.361 0.811 0.746 0.493</cell><cell>0.492</cell><cell>0.593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,163.95,542.39,289.14,96.41"><head>Table 3 .</head><label>3</label><figDesc>Results for queries with at least one annotated entities.</figDesc><table coords="4,163.95,563.19,289.14,75.61"><row><cell></cell><cell cols="3">MAP MRR Recall NDCG@3 NDCG@5 NDCG</cell></row><row><cell>baseline</cell><cell>0.386 0.818 0.759 0.525</cell><cell>0.530</cell><cell>0.620</cell></row><row><cell>median</cell><cell>0.344 NaN NaN 0.488</cell><cell>0.490</cell><cell>0.579</cell></row><row><cell>monoBERT-turn</cell><cell>0.387 0.832 0.759 0.531</cell><cell>0.533</cell><cell>0.621</cell></row><row><cell>RU-turn</cell><cell>0.364 0.801 0.759 0.491</cell><cell>0.503</cell><cell>0.590</cell></row><row><cell cols="2">RU-turn-finetuning 0.331 0.770 0.759 0.501</cell><cell>0.478</cell><cell>0.572</cell></row><row><cell>RU-hist</cell><cell>0.360 0.771 0.759 0.425</cell><cell>0.431</cell><cell>0.561</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,134.77,115.91,345.83,73.64"><head>Table 4 .</head><label>4</label><figDesc>Examples from CAsT 2021 queries where entity annotations are available. RU-turn and monoBERT columns represent NDCG@3 scores for RU-turn and monoBERT-turn. Utterance column shows manually rewritten utterances. Source and #relevant documents column shows in which collections the documents judged as relevant are found and the number of those documents. The top (bottom) five rows are the queries where RU-turn performs better (worse) than monoBERT with the largest NDCG@3 differences.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,646.48,308.53,7.47;3,144.73,656.80,55.65,7.86"><p>https://github.com/daltonj/treccastweb/tree/master/2021/baselines, accessed Aug.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2021" xml:id="foot_1" coords=""><p></p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>To better understand the effect of named entities on our retrieval model, we analyze queries that are most helped and hurt by our model. We calculate the differences of NDCG@3 between RU-turn, which is the best performing of our methods, and monoBERT-turn. The top-5 helped and hurt queries are shown in Table <ref type="table" coords="5,176.36,524.61,3.87,8.74">4</ref>. RU-turn and monoBERT columns represent NDCG@3 scores for RU-turn and monoBERT-turn. The top five rows are the queries where RU-turn performs better than monoBERT. We observe that KILT <ref type="bibr" coords="5,387.55,548.52,9.96,8.74" target="#b7">[8]</ref>, which is based on Wikipedia, has relevant documents for all of the top five queries, while only two KILT documents are observed in the five bottom queries. Additionally, in the queries where RU-turn's NDCG@3 scores are 0 (i.e., qid 117 6, 117 4, 117 1), only WAPO has the relevant documents. Knowing that KILT is based on Wikipedia (containing more entity related documents), and WAPO contains news articles, we observe that our entity-enriched model performs better on entity-related documents than general news articles. This finding can also explain why our model performs better at CAsT 2020 dataset, as the collections used in CAsT 2020 do not contain WAPO but only MS-MARCO and Wikipedia.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,331.17,337.63,7.86;6,151.52,342.13,329.07,7.86;6,151.52,353.09,144.02,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,438.56,331.17,42.03,7.86;6,151.52,342.13,209.75,7.86">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,381.44,342.13,99.15,7.86;6,151.52,353.09,115.35,7.86">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,363.66,337.63,7.86;6,151.52,374.62,258.22,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,260.26,363.66,220.32,7.86;6,151.52,374.62,189.62,7.86">Glasgow representation and information learning lab (GRILL) at the conversational assistance track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gemmell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,385.18,337.63,7.86;6,151.52,396.14,329.07,7.86;6,151.52,407.10,72.19,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,298.02,385.18,179.01,7.86">Graph-embedding empowered entity retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gerritse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,164.61,396.14,311.31,7.86">Proceedings of the 42nd European Conference on Information Retrieval (ECIR)</title>
		<meeting>the 42nd European Conference on Information Retrieval (ECIR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,417.67,337.63,7.86;6,151.52,428.63,329.07,7.86;6,151.52,439.58,329.07,7.86;6,151.52,450.54,132.28,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,423.76,417.67,56.83,7.86;6,151.52,428.63,161.40,7.86">Rel: An entity linker standing on the shoulders of giants</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Van Hulst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dercksen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,332.50,428.63,148.09,7.86;6,151.52,439.58,325.19,7.86">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,461.11,337.64,7.86;6,151.52,472.07,329.07,7.86;6,151.52,483.03,329.07,7.86;6,151.52,493.99,50.43,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,337.06,461.11,143.53,7.86;6,151.52,472.07,106.62,7.86">Conversational entity linking: Problem definition and datasets</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,278.46,472.07,202.13,7.86;6,151.52,483.03,325.48,7.86">Proceedings of the 44rd International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR &apos;21</title>
		<meeting>the 44rd International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR &apos;21</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,504.55,337.63,7.86;6,151.52,515.51,329.07,7.86;6,151.52,526.47,329.07,7.86;6,151.52,537.43,216.49,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,371.10,504.55,109.49,7.86;6,151.52,515.51,166.64,7.86">Knowledge-driven slot constraints for goal-oriented dialogue systems</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lertvittayakumjorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bonadiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,338.76,515.51,141.82,7.86;6,151.52,526.47,127.48,7.86">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3407" to="3419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,547.99,337.64,7.86;6,151.52,558.95,54.14,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<title level="m" coord="6,333.23,547.99,147.36,7.86;6,151.52,558.95,25.47,7.86">Multi-stage document ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,569.52,337.64,7.86;6,151.52,580.48,329.07,7.86;6,151.52,591.44,329.07,7.86;6,151.52,602.40,329.07,7.86;6,151.52,613.36,329.07,7.86;6,151.52,624.31,71.90,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,151.52,591.44,234.93,7.86">KILT: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,406.78,591.44,73.81,7.86;6,151.52,602.40,329.07,7.86;6,151.52,613.36,198.20,7.86">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,634.88,337.64,7.86;6,151.52,645.84,329.07,7.86;6,151.52,656.80,137.59,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,313.02,634.88,167.58,7.86;6,151.52,645.84,74.08,7.86">E-BERT: Efficient-yet-effective entity embeddings for BERT</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Poerner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,247.22,645.84,233.38,7.86;6,151.52,656.80,53.52,7.86">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,119.67,337.98,7.86;7,151.52,130.63,329.07,7.86;7,151.52,141.59,329.07,7.86;7,151.52,152.55,280.43,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,347.64,130.63,132.95,7.86;7,151.52,141.59,52.18,7.86">Entity resolution in open-domain conversations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,224.57,141.59,256.02,7.86;7,151.52,152.55,52.24,7.86">Proceedings of the 2021 Annual Conference of the North American Chapter</title>
		<meeting>the 2021 Annual Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,163.51,337.98,7.86;7,151.52,174.47,329.07,7.86;7,151.52,185.43,329.07,7.86;7,151.52,196.39,292.53,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,165.90,174.47,314.69,7.86;7,151.52,185.43,153.32,7.86">Wikipedia2Vec: An efficient toolkit for learning and visualizing the embeddings of words and entities from Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sakuma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Takefuji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,328.47,185.43,152.13,7.86;7,151.52,196.39,217.48,7.86">Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,207.34,337.98,7.86;7,151.52,218.30,227.93,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05987</idno>
		<title level="m" coord="7,393.46,207.34,87.13,7.86;7,151.52,218.30,61.98,7.86">Revisiting few-sample bert fine-tuning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
