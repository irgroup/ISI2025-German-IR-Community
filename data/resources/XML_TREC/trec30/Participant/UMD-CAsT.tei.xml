<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,113.22,99.96,376.57,12.90;1,112.11,117.89,378.78,12.90">Full-Collection Search with Passage and Document Evidence: Maryland at the TREC 2021 Conversational Assistance Track</title>
				<funder ref="#_REYSzxJ">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,238.88,156.03,36.80,8.64"><forename type="first">Xin</forename><surname>Qian</surname></persName>
							<email>xinq@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Studies</orgName>
								<orgName type="institution">UMIACS University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.04,156.03,69.08,8.64"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Studies</orgName>
								<orgName type="institution">UMIACS University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,113.22,99.96,376.57,12.90;1,112.11,117.89,378.78,12.90">Full-Collection Search with Passage and Document Evidence: Maryland at the TREC 2021 Conversational Assistance Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AA63E45248D9DF517B766DD57DE61E6E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Maryland (UMD) team submitted four runs to the automatic canonical condition of the track, exploring three ideas: (1) indexing both document-scale and passage-scale features, (2) using sharding to scale dense retrieval to large collections, and (3) combining results from sparse and dense methods using re-ranking and result fusion. Compared with the three-stage baseline pipeline of query rewriting using T5-base, document retrieval using BM25, and passage re-ranking using monoT5, UMD Run #1 modifies the second stage to retrieve passages rather than documents. UMD Run #2 retrieves documents in the second stage, but augments each second-stage document with additional document-scale evidence. UMD Run #3, our best run, fuses the final output of three runs: UMD Run #1, UMD Run #2, and the organizer-provided baseline. UMD Run #4, fuses results from TCT-ColBERT (a distilled ColBERT model) passage retrieval with results from BM25 document retrieval as the second stage. The TCT-ColBERT passage retrieval uses sharding to accommodate the large collection size. In each case, the first stage is the organizer-provided baseline query rewriter, and the third stage is a re-implementation of the baseline's third stage, but using monoBERT-large.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Conversational Assistance Track (CAsT) is a shared task to study conversational information seeking, where an information system engages in conversational exchanges with human users to help satisfy information needs <ref type="bibr" coords="1,195.80,454.31,10.58,8.64" target="#b1">[2]</ref>. The user who guides a human-machine conversation in CAsT can freely choose to introduce a topic, continue on that topic, or shift from one topic to another. Interpreting a current question in the context of conversational interaction is natural for humans, but challenging for machines. CAsT operationalizes this interpretation process as query rewriting. The goal is to rewrite a question in a way containing all of the necessary contexts to answer that question. In 2020, the best result for the automatic canonical condition (i.e., for automatic query rewriting based on canonical responses) was 0.493 for NDCG@3, quite close to the best result for the manual condition (i.e., for manually rewritten queries), which was 0.530 for NDCG@3 <ref type="bibr" coords="1,218.61,538.00,10.58,8.64" target="#b2">[3]</ref>.</p><p>This success motivated development by the track organizers of a three-stage baseline pipeline in CAsT 2021 to which participating systems can be compared. In that pipeline (detailed in Section 3.1), the first stage is an automatic question rewriter, implemented using a T5-base model trained on the CANARD question answering dataset, <ref type="foot" coords="1,181.59,583.98,3.69,6.39" target="#foot_0">1</ref> and the third stage is a pointwise passage re-ranker implemented using a monoT5 model trained on MS MARCO <ref type="bibr" coords="1,219.45,597.86,15.27,8.64" target="#b9">[10]</ref>. For our experiments, we adopted the baseline's first-stage question rewriter. We also implemented a simple pointwise re-ranker using monoBERT and consistently used that as the third stage in our four submitted runs, and we experimented with different second-stage full-collection retrievers. In keeping with the track guidelines, we measure the relative effectiveness of our second-stage full-collection retriever using the end-to-end ranking quality of the full three-stage pipeline as an extrinsic measure of retrieval effectiveness for the conversational assistance task (as detailed in Section 4.2).</p><p>In this work, we explore two approaches to improve the second (full-collection retrieval) stage. In one approach (Run #2), we augment the content representation with additional document-level evidence (title terms and tokenized URL terms) and then use BM25 (k1 = 4.46, b = 0.82, consistent with the organizerprovided baseline) for full-collection ranked retrieval. In the other approach (Run #4), we use a TCT-ColBERT <ref type="bibr" coords="1,132.66,717.50,11.62,8.64" target="#b7">[8]</ref> bi-encoder (a distilled ColBERT model) to perform late-interaction ranked retrieval using dense representations in a way that is sufficiently efficient to be performed over the full collection. Given the sizes of the collection searched in CAsT, we use sharding to enable parallel processing. In this run, we augment each passage with the same document-level evidence as in Run #2, adding the same evidence for each passage from the same document. Traditional and neural methods have complementary strengths, so in Run #4 we fuse results from TCT-ColBERT with BM25 (k1 = 4.46, b = 0.82) results using weighted sum fusion before third-stage re-ranking. To encourage diversity, we compute these BM25 results without using the additional document-level evidence.</p><p>We compare each of these approaches to two baseline approaches. The full-collection search in our re-implemented low baseline (Run #1) is passage-level BM25 (k1 = 0.82, b = 0.68). Our results show that Run #2 and Run #4 both statistically significantly outperform Run #1 by Mean Average Precision (MAP), although (because of our use of monoBERT-large rather than monoT5) our Run #1 yields results numerically (but not statistically significantly) below the organizers' baseline by every measure in Table <ref type="table" coords="2,484.28,236.69,3.74,8.64" target="#tab_2">3</ref>. As a high baseline, our Run #3, uses reciprocal rank fusion to combine the ranked lists from Run #1, Run 2 and the organizers' baseline, statistically significantly outperforming all three of those component approaches by normalized Discounted Cumulative Gain in the top 500 documents (nDCG@500), a recall-oriented measure. Run #4 also statistically significantly outperforms both Run #1 and Run #2 by nDCG@500. Finally, we also locally scored three post hoc runs, including one in which we augmented both the secondstage retriever from Run #1 and the third-stage re-ranker with the additional document-level evidence. As shown in Table <ref type="table" coords="2,155.76,320.38,3.74,8.64">4</ref>, that post hoc run statistically significantly outperforms Run #1 by nDCG@500, thus further illustrating the potential benefit of indexing title and URL terms. Our other two post hoc runs serve to illustrate the substantial benefit of the baseline (first-stage) question rewriting that we have used.</p><p>The remainder of this paper describes our techniques in greater detail. We start with the problem settings for TREC CAsT, then provide details on our systems, followed by results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>In the submission condition automatic canonical, specifically, with the raw utterance and canonical responses, the input is a raw utterance U k , the conversational context prior to the current k-th utterance consisting of raw utterances U = {U 1 ,U 2 , . . . ,U k-1 }, and canonical system responses to each of those utterances denoted R = {R 1 , R 2 , . . . , R k-1 }. For each raw utterance U k , the task is to retrieve a list of passages P k = {p k 1 , p k 2 , ...} as relevant responses to the utterance. While an initial retrieval pass, the second stage of our three-stage pipeline, can use traditional IR techniques for efficiency, a competitive system will presumably do a final-pass, often the third-stage of our pipeline, using a more expensive re-ranker, where the system will learn and do inference with a re-ranker model M that scores the probability of a passage p being relevant in the context of the current utterance U k , denoted as M(rel = 1|p,U k ,U , R ). The collection to be retrieved from is in general heterogeneous, constructed from several document collections C = {C 1 ,C 2 , ...}. For CAsT 2021, there are three such collections: an English Wikipedia dump, Version 1 of the MS MARCO document collection, and the Washington Post (WaPo) collection, where WaPo is a new addition this year. Table <ref type="table" coords="2,204.18,587.42,4.98,8.64" target="#tab_0">1</ref> summarizes some statistics for these three parts of the CAsT 2021 collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Design</head><p>As noted above, all of our systems are patterned on the three-stage baseline pipeline in the track guidelines: <ref type="foot" coords="3,111.59,137.07,3.69,6.39" target="#foot_1">2</ref> (1) query rewriting using T5-base; (2) document retrieval using BM25 followed by segmentation to passages; (3) passage re-ranking using monoT5. In this section, we describe the components we developed and the ways we used those components together in our submitted runs.</p><p>3.1 Baseline Architecture: Query rewriting, retrieval, and re-ranking</p><p>The baseline architecture uses a T5-base model <ref type="bibr" coords="3,289.15,212.26,11.62,8.64" target="#b8">[9]</ref> to generate automatic question rewrites. All of our systems do the same-when given a query, along with the conversational context (including canonical responses), we rewrite the query with the same pre-trained T5-base question rewriter, <ref type="foot" coords="3,434.74,234.24,3.69,6.39" target="#foot_2">3</ref> publicly available on Huggingface. 4 This T5 rewriter is trained on the CANARD dataset <ref type="bibr" coords="3,368.53,248.12,10.58,8.64" target="#b3">[4]</ref>, which includes 40K questions to train models for question-in-context rewriting. Each instance in the CANARD dataset contains a question together with prior questions and answers, and a human-created rewritten question.</p><p>As input to the rewriter, we provide the current utterance (user question), all previous utterances, and (at most) three most recent canonical system responses. We concatenate these inputs using the special separator token (|||), and the model then generates a rewritten query.</p><p>With the rewritten query, our simplest systems then perform full-collection retrieval using BM25, returning the top-1000 indexed units. We have two options for the indexed units, corresponding to two granularities:</p><p>-Run #1: Passages are segmented from documents as pre-processing, using the same passage chunker as in the organizers' baseline, then indexed, and retrieved using BM25 with default Anserini parameters (k1 = 0.82, b = 0.68, Figure <ref type="figure" coords="3,223.02,387.31,3.60,8.64">1</ref>). -Run #2: Full Documents (without segmentation) are indexed, and retrieved using BM25 with organizers' baseline parameters (k1 = 4.46, b = 0.82, Figure <ref type="figure" coords="3,331.89,411.16,3.60,8.64">1</ref>). The retrieved documents are then chunked into passages in the same way as in Run #1. Note that because 1000 documents are retrieved, this results in more than 1000 passages.</p><p>Our third stage re-ranks the list of passages generated by the second stage using a pointwise monoBERTlarge model that was pre-trained on MS MARCO. We used monoBERT-large rather than a monoT5 model used in the organizers' baseline because our pilot experiments showed limited improvements from using monoT5-base rather than monoBERT-large. In retrospect, a better choice might have been either (1) the monoT5-large model, which was found to outperform monoBERT-large by the winning team at CAsT 2020 <ref type="bibr" coords="3,112.45,514.53,10.58,8.64" target="#b2">[3]</ref>, or (2) the duoBERT-large two-stage pipeline for leveraging both pointwise and pairwise training that yielded strong results for another high-scoring team at CAsT 2020 <ref type="bibr" coords="3,374.73,526.49,10.58,8.64" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Augmenting with Additional Document-Level Evidence</head><p>In Run #2 and Run #4 we experiment with adding additional document-level features to the second-stage (full-collection) retrieval process. Each collection includes a URL and some form of title for each document. In the Washington Post collection, the title is the headline of the news story. In the Wikipedia dump, the title is the HTML title field for the Wikipedia page. For the MS MARCO collection, we used the title field formatted in the raw collection tsv file. Table <ref type="table" coords="3,298.76,623.65,4.98,8.64" target="#tab_1">2</ref> lists examples. We tokenized both titles and URLs using Lucene's default English analyzer.</p><p>The University of Waterloo (which was called to our attention by this year's TREC Deep Learning Track) reported substantial improvements (around 20 points absolute on recall@100 on dev set queries) from the use of additional fields in the collection tsv file (which in their case included titles, URLs, and headings; we did not use headings in our augmented MS MARCO collection). This report was our inspiration for exploring the use of additional document-level evidence for all three CAsT collections. The Fig. <ref type="figure" coords="4,107.44,280.94,3.88,8.64">1</ref>: Compared with the baseline pipeline of question rewriting (T5-base), document retrieval (BM25), and then passage re-ranking (monoBERT-large), our Run #1 modifies the second-stage indexing unit to be passages rather than documents. Run #2 indexes documents in the second stage, but augments each document with document-level evidence. Run #3, our best run (not shown here), fuses the final output of Runs #1, #2, and the organizer-provided baseline. Run #4 fuses TCT-ColBERT (a distilled ColBERT model) passage retrieval with BM25 document retrieval as the second stage, using sharding to accommodate the large collection size. inspiration also motivated a post hoc experiment in which we also used this additional document-level evidence during (third-stage) re-ranking. This additional evidence might help in two ways: it might add terms not present in the text, or it might serve to reinforce (i.e., up weight) important terms present in both the text and the additional evidence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dense Retrieval and Sharding</head><p>Neural ranking methods using dense representations have been shown to achieve better recall than traditional ranking methods that rely on sparse representations, such as the BM25 model that we used in Run #1 and Run #2 <ref type="bibr" coords="4,152.66,665.27,10.58,8.64" target="#b5">[6]</ref>. Recently, fairly effective neural bi-encoders that are sufficiently efficient for use with moderately large collections have been introduced. Scaling such approaches up to collections of the size of CAsT 2021 still requires some parallelism, however. We therefore combined sharding with an efficient neural bi-encoder. The key idea in this approach is to perform shallow-depth retrieval on shards, using an efficient approximate nearest neighbor on dense representations that are computed separately for each query and each document. The hope is that this would improve recall over that achieved using sparse methods, although achieving that benefit depends on the relevant documents being reasonably well distributed across the shards. Sharding also reduces GPU memory requirements, making it possible to fit each shard into our compute infrastructure's 200GB memory limit.</p><p>A dense encoder based on the Transformer model has length limitations on its input, so we first divide documents into passages as in Run #1. We then augment each passage with the additional document-level evidence for the document from which the passage was extracted. Next, we divide the passage collection into 200 shards of equal size, with 17 shards from the Washington Post collection, 100 shards from MS MARCO, and 82 shards from Wikipedia. When performing this sharding, we respect passage boundaries, but not document boundaries, and we assign passages to shards in order, without randomization.</p><p>For the dense encoder, we used the TCT-ColBERT model, which is publicly available on Huggingface.<ref type="foot" coords="5,508.82,198.06,3.69,6.39" target="#foot_4">5</ref> This is a distilled version of ColBERT that replaces ColBERT's MaxSim operation with a less expensive dot product, improving efficiency while retaining most of the ColBERT model's effectiveness. ANN search is performed with FAISS, <ref type="foot" coords="5,196.65,233.92,3.69,6.39" target="#foot_5">6</ref> a dense vector similarity search library. This process produces a score for each passage. We then calculate each document score using maxP (i.e., the largest of its passage scores). Independently, BM25 (on document terms only, with no augmentation) also produces a score for each document. We combine the two scores using weighted CombSUM, giving 100% of the weight to the Augmented TCT-ColBERT MaxP score and 10% of the weight to the BM25 score, as recommended in the TCT-ColBERT documentation. <ref type="foot" coords="5,214.68,293.70,3.69,6.39" target="#foot_6">7</ref>Unlike our other submitted runs, in Run #4 we perform third-stage re-ranking on a per-shard basis, for all passages from the top-scored documents in each shard. Because we want a final depth-1000 ranking over the full collection, we then essentially take the union of the top-scoring 50 passages from each of the 200 shards. Although the same passage can not appear in two shards, we actually perform this union by doing CombMAX fusion using the polyfuse library. <ref type="foot" coords="5,296.62,353.96,3.69,6.39" target="#foot_7">8</ref> The procedure is illustrated in Figure <ref type="figure" coords="5,453.82,355.89,3.74,8.64" target="#fig_1">2</ref>.</p><p>Stage three re-ranking was performed on shards rather than the union of the collection for reasons of implementation convenience, and we expect the effect of thsi choice to be small in practice. Because we use the same pointwise monoBERT-large re-ranker for all our submitted runs, performing third-stage reranking on a per-shard basis is guaranteed to produce the same final ranking as would have resulted from re-ranking the union of the shards, down to at least rank 50. Below rank 50, quantization effects could result in the omission of some passages that might otherwise have been highly enough ranked to be included.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fusion</head><p>Fusing the results from two or more runs is often an effective technique to improve ranking quality <ref type="bibr" coords="6,483.52,123.84,10.58,8.64" target="#b6">[7]</ref>. We tried two fusion techniques.</p><p>Run #3: To establish a high baseline, we combine the results from Run #1, Run #2, and the organizers' baseline. We use reciprocal rank fusion (RRF) with k = 60, which has been shown to be fairly robust <ref type="bibr" coords="6,494.17,171.77,10.58,8.64" target="#b0">[1]</ref>.</p><p>Run #4: As described in Section 3.3, sharding produces multiple ranked lists, one per shard. We do weighted CombSUM fusion within each shard to combine the results from dense retrieval (using TCT-ColBERT) and sparse retrieval (using BM25). The recombination of results from multiple shards is also a fusion operation <ref type="bibr" coords="6,157.52,231.66,15.27,8.64" target="#b10">[11]</ref>, although a simple one using CombMAX because the shards are logically disjoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary of Submitted Runs</head><p>Submitted runs are four system variants built from combinations of the above techniques, as Figure <ref type="figure" coords="6,508.02,294.51,4.98,8.64">1</ref> illustrates.</p><p>-Run #1: Baseline BM25 implementation with passage index; -Run #2: BM25 with document index, augmented with additional document-level evidence; -Run #3: Reciprocal rank fusion of Run #1, Run #2, and the organizers' baseline; -Run #4: Fused dense and sparse retrieval with sharding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>In this section, we describe the evaluation process, and we present and discuss our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation</head><p>We submitted a ranked list of passages for each run, but inconsistencies in passage definitions made passage-based evaluation impractical this year. The organizers thus chose instead to perform relevance judgments and evaluation at document scale. This was done by mapping ranked lists from passages to documents using MaxP (i.e., by replacing passage identifiers with the document identifier from that passage and then deduplicating the resulting document ranking by removing lower-ranked duplicates). This resulted in shallower judgment pools, and it required some changes to the relevance judgment guidelines to avoid penalizing the presence of extraneous information. We have implemented these changes to evaluate two post hoc runs that we report with (first-stage) question rewriting omitted, and a third post hoc run in which we augmented the passage representation used for (third-stage) re-ranking with additional document-scale evidence. Our other results are reported as they were received from the track organizers.</p><p>The primary evaluation measure is normalized Discounted Cumulative Gain at position 3 (nDCG@3), which focuses only on the top 3 results, and which gives decreasing weight to results in positions 2 or 3. nDCG@5, nDCG@500 and Mean Average Precision at 500 (MAP@500) are also reported to characterize results even lower in the ranked list. All evaluation measures are reported as averages over the computed measure for each query (i.e., for each evaluated conversational turn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table" coords="6,114.27,689.18,4.98,8.64" target="#tab_2">3</ref> summarizes the results for our submitted runs. For reference we also include the per-topic best and median as provided by the organizers, averaged over judged topics, as well as results from the organizers' baseline. '*' indicates a statistically significant improvement over the organizers' baseline using a twosided paired t-test at p &lt; 0.05 with Bonferroni correction. Similarly, ' †' indicates a statistically significant improvement over Run #1, and ' ' over Run #2. Locally scored results of post hoc runs without the query formulation stage are also shown as contrastive conditions.</p><p>Effect of first-stage question rewriting: The improvements from question rewriting shown in Table <ref type="table" coords="7,493.58,337.06,4.98,8.64" target="#tab_2">3</ref> are quite substantial. Comparing Run #1 or #2, with and without (wo) rewriting, illustrates this simple ablation study. Question rewriting brings very substantial absolute improvements of 0.16 or 0.21 in nDCG@3 for Run #1 or Run #2, respectively, with similar benefits observed for other measures.</p><p>Benefit of augmentation with document-level evidence: Table <ref type="table" coords="7,352.93,397.39,4.98,8.64">4</ref> shows a benefit from augmenting passages with terms from the title and URL of the document from which the passage was drawn. In this case, the additional evidence was used in both the second-stage retrieval and the third-stage re-ranking. The 0.029 absolute apparent improvement in nDCG@3 is not statistically significant, but the comparable improvements in nDCG@5 and nDCG@500 are both statistically significant. One caveat to this analysis is that computing evaluation measures on documents rather than passages, as has been done this year, may favor the addition of document-level evidence. Note that we can not tell with this study design whether the benefit results from the effects of augmentation on the second-stage retrieval, the third-stage re-ranking, or both. Comparing to Run #2, which used augmentation only in second-stage retrieval, is confounded by the fact that Run #2 indexed documents rather than passages for stage two. Note also that because the same document-level evidence is indexed repeatedly for each passage from the same document, there is some additional storage overhead for the index (in this case, a relative increase of 9%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefit of fusion:</head><p>Comparing Run #3 to Runs #1, #2, and the organizers' baseline, we see that the fused run (Run #3) statistically significantly outperforms every individual run in the combination by nDCG@500, with an absolute improvement of 0.035 over the best of the three constituent runs. No statistically significant improvement over the best of the three constituent runs was observed for nDCG@3 or nDCG@5, however, indicating that the benefit observed in nDCG@500 occurs later in the ranked list.</p><p>Table <ref type="table" coords="7,114.34,655.26,3.88,8.64">4</ref>: Benefit of retrieval from a non-augmented passage index (Run #1) vs. a passage index augmented with document title and URL terms (scored locally). ' ‡' indicates a statistically significant improvement over Run #1.</p><p>Index size nDCG@3 nDCG@5 nDCG@500 MAP@500 Original index (Run #1) 66G 0.3875 0.3764 0.4625 0.2619 Augmented index + re-ranking 72G 0.4162 0.4100 ‡ 0.4815 ‡ 0.2770 Benefit of combining dense and sparse retrieval: As Table <ref type="table" coords="8,335.79,103.37,4.98,8.64" target="#tab_2">3</ref> shows, Run #4, which combines dense and sparse retrieval, achieves a statistically significant improvement of 0.033 over Run #2, our most closely comparable sparse-only run, by nDCG@500. However, we note no improvement from Run #2 to Run #4 in nDCG@3 or nDCG@5, indicating that the benefits we observe in nDCG@500 are coming later in the ranked list. Note also that as we have shown with Run #3, fusion can be useful even when both runs use sparse retrieval, and our study design is not able to separate that effect from specific benefits that may be accruing from using dense retrieval in the set of runs being fused.</p><p>Efficiency: The end-to-end throughput for all three stages of the Run #1 (passage-as-indexing-unit) and Run #2 (document-as-indexing-unit) pipelines are 47.3 and 304.6 seconds per query, respectively. As Table 2 shows, the average number of passages per topic varies between 4 and 7, and this factor of 6 or so in end-to-end processing time is thus dominated by the fact that the document-as-indexing-unit pipeline results in the generation of more passages that require re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head><p>Table <ref type="table" coords="8,114.71,302.08,4.98,8.64">5</ref> shows some examples of automatic question rewriting and a comparison with manually rewritten questions. Table <ref type="table" coords="8,114.13,647.00,3.88,8.64">5</ref>: Original questions for one example topic, compared with the manually rewritten "silver standard", the automatically rewritten question from the organizers' baseline, and our automatically rewritten question. We note that in seven of the ten turns, our automatic rewrite differs from the automatic rewrite used in the organizers' baseline, despite our effort to closely replicate the usage of a T5-base model from their description. IdC = Invasive ductal carcinoma.</p><p>While the manual rewrites demonstrate desired rewriting effects, the automatic rewrites achieve some useful improvements:</p><p>Utterance Simplification: Question rewriting simplifies colloquial utterances into a single-sentence, interrogative sentence, as in turns 1 and 4, which affects term re-weights.</p><p>Pronoun Resolution: Question rewriting expands pronouns such as replacing it with lobular carcinoma breast cancer, or expanding the abbreviation LCIS. However, this effect only appears from turn 3, but not earlier, possibly due to our context concatenation approach in which inference is on three previous canonical responses.</p><p>Clause Addition: Question rewriting adds clauses based on the context to complement the original utterance, where the meaning of the whole query is clarified. Examples include turns 1 and 8, where we see the most common types of cancer, and the first stage of lobular carcinoma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present our submissions to TREC CAsT 2021. Run #3, our best run, fuses the final output of Run #1 doing indexing on passages, Run #2 doing retrieval on documents and augmenting each document in the index with additional document-level evidence, and the organizers' baseline. That run achieves a comparable nDCG@3 to the organizers' baseline, and it statistically significantly outperforms the organizers' baseline by nDCG@500. Run #4 is also noteworthy because it fuses dense retrieval with sparse retrieval, using sharding to accommodate the large collection size, thus beginning our exploration of an important design space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,119.67,659.77,363.67,8.64"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: An illustration of Run #4, using sharding and combining dense with sparse retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,189.80,644.02,223.41,61.14"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the CAsT 2021 collection.</figDesc><table coords="2,189.80,659.51,223.41,45.65"><row><cell></cell><cell cols="3">Wikipedia MS MARCO WaPo</cell></row><row><cell># Documents</cell><cell>5.0M</cell><cell>3.2M</cell><cell>0.7M</cell></row><row><cell># Passages</cell><cell>20.0M</cell><cell>22.0M</cell><cell>3.8M</cell></row><row><cell>Avg # Passages per Document</cell><cell>4.0</cell><cell>7.0</cell><cell>5.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,92.79,467.29,415.93,98.60"><head>Table 2 :</head><label>2</label><figDesc>Examples of additional document-level evidence from titles and URLs.</figDesc><table coords="4,92.79,467.29,415.93,80.74"><row><cell>Collection</cell><cell>URL</cell><cell>Title/headline</cell></row><row><cell>Wikipedia</cell><cell>https://en.wikipedia.org/w/index.php?title=Origin% 20of%20the%20domestic%20dog&amp;oldid=908221312</cell><cell>Origin of the domestic dog</cell></row><row><cell>MS MARCO</cell><cell>https://www.sciencedaily.com/releases/2014/01/ 140107102634.htm</cell><cell>Cancer Statistics 2014: Death rates continue to drop</cell></row><row><cell>WaPo</cell><cell>https://www.washingtonpost.com/sports/colleges/ danny-coale-jarrett-boykin-are-a-perfect-1-2-punch-\ for-virginia-tech/2011/12/31/gIQAAaW4SP_story.html</cell><cell>Danny Coale, Jarrett Boykin are a perfect 1-2 punch for x Virginia Tech</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,111.74,423.00,159.75"><head>Table 3 :</head><label>3</label><figDesc>Results for submitted runs. '*' indicates significant improvement over the organizers' baseline, ' †' over Run #1, ' ' over Run #2. Results without question rewriting were scored locally.</figDesc><table coords="7,180.76,140.49,241.47,131.01"><row><cell></cell><cell cols="2">nDCG@3 nDCG@5 nDCG@500 MAP@500 Judging priority</cell></row><row><cell>Best</cell><cell>0.8067 0.7716 0.7668 0.5483</cell><cell>-</cell></row><row><cell>Median</cell><cell>0.3820 0.3872 0.4499 0.2431</cell><cell>-</cell></row><row><cell>Baseline</cell><cell>0.4357 0.4265 0.5036 0.3135</cell><cell>-</cell></row><row><cell>Run #1</cell><cell>0.3875 0.3764 0.4625 0.2619</cell><cell>2</cell></row><row><cell cols="2">wo rewriting 0.2216 0.2100 0.2613 0.1435</cell><cell>-</cell></row><row><cell>Run #2</cell><cell>0.3985 0.3904 0.4784 0.2811  †</cell><cell>2</cell></row><row><cell cols="2">wo rewriting 0.1888 0.1878 0.2375 0.1257</cell><cell>-</cell></row><row><cell>Run #3</cell><cell>0.4252  † 0.4275  † 0.5388  *  † 0.3221  †</cell><cell>3</cell></row><row><cell>Run #4</cell><cell>0.3768 0.3744 0.5116  † 0.2841  †</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,99.96,738.00,305.98,7.47"><p>https://github.com/daltonj/treccastweb/tree/master/2021/baselines</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,99.96,715.41,305.98,7.47"><p>https://github.com/daltonj/treccastweb/tree/master/2021/baselines</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,99.96,726.35,106.64,7.77"><p>Note, however, that in section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3" coords="3,208.84,726.35,238.63,7.77;3,93.74,735.69,228.47,9.79"><p>4.3 we show that our rewriter actually generates different rewrites! 4 https://huggingface.co/castorini/t5-base-canard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,99.96,693.25,274.52,7.47"><p>https://huggingface.co/castorini/tct_colbert-v2-hn-msmarco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,99.96,704.55,193.00,7.47"><p>https://github.com/facebookresearch/faiss</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,99.96,715.39,413.04,7.77;5,99.96,726.35,138.95,7.77"><p>No normalization is performed before we apply weighted CombSUM; in future work we plan to more fully explore the design space for fusion techniques.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,99.96,738.00,165.25,7.47"><p>https://github.com/rmit-ir/polyfuse</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research has been supported in part by the <rs type="funder">National Science Foundation</rs> under grant number <rs type="grantNumber">1618695</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the <rs type="funder">National Science Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_REYSzxJ">
					<idno type="grant-number">1618695</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,97.85,519.12,415.15,7.77;9,106.19,530.08,406.81,7.77;9,106.19,541.04,212.37,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,207.64,519.12,305.36,7.77;9,106.19,530.08,59.54,7.77">Bayes optimal metasearch: a probabilistic model for combining the results of multiple retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,186.14,530.08,326.86,7.77;9,106.19,541.04,135.68,7.77">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="379" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,551.90,415.15,7.77;9,106.19,562.86,406.81,7.77;9,106.19,573.82,101.22,7.77" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<title level="m" coord="9,256.74,551.90,256.26,7.77;9,106.19,562.86,223.08,7.77;9,347.48,562.86,69.55,7.77">Research frontiers in information retrieval: Report from the third strategic workshop on information retrieval in Lorne (SWIRL 2018)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="34" to="90" />
		</imprint>
	</monogr>
	<note>ACM SIGIR Forum</note>
</biblStruct>

<biblStruct coords="9,97.85,584.69,415.15,7.77;9,106.19,595.65,91.42,7.77" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13624</idno>
		<title level="m" coord="9,224.13,584.69,231.68,7.77">TREC CAsT 2019: The conversational assistance track overview</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,97.85,606.51,415.15,7.77;9,106.19,617.47,212.17,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,267.72,606.51,228.32,7.77">Can you unpack that? learning to rewrite questions-in-context</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Peskov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,106.19,617.47,186.03,7.77">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,628.34,415.15,7.77;9,106.19,639.29,406.81,7.77;9,106.19,650.25,261.93,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,201.35,628.34,311.64,7.77;9,106.19,639.29,74.22,7.77">Glasgow representation and information learning lab (GRILL) at the conversational assistance track 2020</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gemmell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,198.71,639.29,214.46,7.77">Proceedings of the Twenty-Ninth Text REtrieval Conference</title>
		<meeting>the Twenty-Ninth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1266</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,661.12,415.15,7.77;9,106.19,672.08,406.81,7.77;9,106.19,683.04,217.02,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,426.89,661.12,86.10,7.77;9,106.19,672.08,131.59,7.77">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,257.16,672.08,255.85,7.77;9,106.19,683.04,76.45,7.77">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020-11">Nov 2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,693.90,415.15,7.77;9,106.19,704.86,406.81,7.77;9,106.19,715.82,70.98,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,270.05,693.90,171.81,7.77">Probfuse: a probabilistic approach to data fusion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lillis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Toolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,459.59,693.90,53.41,7.77;9,106.19,704.86,403.35,7.77">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,726.69,415.15,7.77;9,106.19,737.64,122.05,7.77" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11386</idno>
		<title level="m" coord="9,216.78,726.69,269.55,7.77">Distilling dense representations for ranking using tightly-coupled teachers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,97.85,104.02,415.15,7.77;10,106.19,114.98,392.23,7.77" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,359.00,104.02,154.00,7.77;10,106.19,114.98,241.48,7.77">Conversational question reformulation via sequence-to-sequence architectures and pretrained language models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01909</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,97.47,125.94,415.53,7.77;10,106.19,136.90,406.81,7.77;10,106.19,147.86,112.44,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,271.18,125.94,237.70,7.77">Document ranking with a pretrained sequence-to-sequence model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,119.65,136.90,272.74,7.77">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">Nov 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,97.47,158.82,415.53,7.77;10,106.19,169.77,185.57,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,282.77,158.82,106.03,7.77">The collection fusion problem</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Johnson-Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,406.90,158.82,106.10,7.77;10,106.19,169.77,112.54,7.77">Proceedings of the Third Text Retrieval Conference (TREC-3)</title>
		<meeting>the Third Text Retrieval Conference (TREC-3)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
