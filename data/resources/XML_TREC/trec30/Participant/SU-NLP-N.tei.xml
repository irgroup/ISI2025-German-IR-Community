<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,193.60,172.49,223.04,15.20">SU-NLP at TREC NEWS 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.77,206.34,80.45,10.56"><forename type="first">Kenan</forename><surname>Fayoumi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sabancı University</orgName>
								<address>
									<settlement>İstanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,314.88,206.34,87.61,10.56;1,402.48,204.26,1.41,7.97"><forename type="first">Reyyan</forename><surname>Yeniterzi</surname></persName>
							<email>reyyan.yeniterzi@sabanciuniv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sabancı University</orgName>
								<address>
									<settlement>İstanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,193.60,172.49,223.04,15.20">SU-NLP at TREC NEWS 2021</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">25185C39F34D0DCD377DC60F28E23C8E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our work and submissions for the TREC 2021 News Track Wikification task. We approach the problem as an entity linking task initially and after identifying the mentions and their corresponding Wikipedia entities, we rank the mentions within the news article based on their usefulness. For the entity linking part, transformer-based architectures are explored for both detecting the mentions, generating the possible candidates and re-ranking them. Finally for the mention ranking, we use previous years' best performing approach which uses the position of the mention within the text.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Bases (KBs) such as Wikipedia, are comprised of a huge collection of entries/articles related to entities, concepts and artifacts. Entity related tasks such as wikification (entity linking), entity disambiguation, entity representation and others are gaining a lot of attention in the literature as these tasks are useful building blocks for more complex NLP tasks such as question answering, information extraction and text summarization. KBs provide a rich source of knowledge that can be incorporated to these tasks. Furthermore, in the context of news articles, automatically linking mentions of entities to KBs can provide useful background and contextual information to readers for a better reading experience.</p><p>The terms wikification and entity linking are used interchangeably over the literature. Given a text document and a knowledge base which contains a set of predefined entities, the task is to detect mentions of entities within the text and link them to their corresponding entry in the KB. In the wikification case, Wikipedia is used as the knowledge base, and mentions of entities within the text are linked to the corresponding Wikipedia pages. In the TREC News Wikification task, the task is not only about linking entities but also ranking them based on their relevance or importance to the news article. This is the second time wikification is included as a task at the TREC News track. This paper describes SU-NLP's efforts on this year's wikification task. This year, we specifically focus on the impact of utilizing different components for entity linking sub-tasks and analyze the effectiveness of these methods on wikification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>TREC 2021 organizers have provided an updated version of the Washington Post news corpus (Version 4) containing 728,626 articles and blog posts dating from January 2012 up until December 2020. Previous version of the Washington Post corpus which was used in previous year's tasks, contains 671,947 articles. Each news article contains a number of content paragraphs. These could consist of text, images or videos. The news articles contain HTML tags and these tags are kept in place to assure the original mention placements/locations while evaluating the system. This is the second year wikification task is offered at TREC News. Last year's wikification topics consisted of 50 news articles. As for this year's task, 51 news articles/topics are provided. Since the relevance judgments for 2020 topics were provided, we utilize these for evaluating our work. More statistics concerning the topics are displayed in Table <ref type="table" coords="2,381.16,475.81,4.22,10.91" target="#tab_0">1</ref> 3 Methodology</p><p>We approach the TREC News Wikification task as a two steps procedure. The first step is the entity linking part and the second step is the mention (and corresponding entity) ranking part based on its usefulness for the news article.</p><p>In the literature, the task of entity linking is mostly split into a pipeline of 3 sub-tasks: Mention Detection/Recognition (MD), Candidate Generation, Candidate Re-ranking or Entity Disambiguation (ED). In MD, the task is to find spans of texts that mention a specific entity. Detected mentions are given to Candidate Generation step where for each mention a list of candidate entities from the knowledge base are returned. The last step is to re-rank the list of candidate entity-mention pairs in terms of similarity (Entity Disambiguation). This pipeline is depicted in Figure <ref type="figure" coords="3,419.38,305.95,5.42,10.91" target="#fig_0">1</ref> for our case when Wikipedia is used as the knowledge base and Washington Post articles are used as the input text documents. This year we extend on our work from previous year <ref type="bibr" coords="3,403.63,659.24,80.82,10.91" target="#b0">(Ak et al., 2020)</ref> by employing more recent neural network-based architectures in the entity linking part of the task. Namely, we experiment with utilizing two different mention detection models, two different candidate retrieval models and a transformer-based model for candidate ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mention Detection</head><p>In mention detection step, we treat the task as a named-entity recognition task. For that purpose as our first approach we follow the work in last year's approach <ref type="bibr" coords="4,203.35,250.11,78.42,10.91" target="#b0">(Ak et al., 2020)</ref> and utilize Stanford CoreNLP's NER tool <ref type="bibr" coords="4,125.80,263.66,106.89,10.91" target="#b3">(Manning et al., 2014)</ref>. As for our second NER model, we use Flair NLP tool <ref type="bibr" coords="4,147.26,277.21,91.52,10.91" target="#b1">(Akbik et al., 2019)</ref> which utilizes transformer architectures in the NER modeling. These two tools are used to extract the entity mentions which are passed to our candidate retrieval system as the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Candidate Retrieval</head><p>Two different approaches are explored for the candidate retrieval step. Our first method is inspired by <ref type="bibr" coords="4,259.47,367.66,78.22,10.91" target="#b0">Ak et al. (2020)</ref> as we utilize an Elasticsearch search engine built on top of the Wikipedia. We follow <ref type="bibr" coords="4,405.32,381.21,79.13,10.91" target="#b0">Ak et al. (2020)</ref> in both the index and query formulation as we query the extracted entity mentions over an index built by only using the title field of the Wikipedia articles.</p><p>As of our second method, we follow the Bi-Encoder approach of Wu et al. (2020) for dense entity retrieval. In their approach candidate retrieval is handled with vector similarity search. Mentions and entities are represented as vectors using a BERT encoder. To encode the Wikipedia entities, the following input is generated with the addition of the special token [EN T IT Y ] to separate the Wikipedia title and description from each other:</p><formula xml:id="formula_0" coords="4,197.09,528.35,216.07,9.60">[CLS] title [EN T IT Y ] description [SEP ]</formula><p>Similarly, mentions are encoded with BERT using the following input structure which uses both the left and the right context of the mention:</p><formula xml:id="formula_1" coords="4,131.48,590.92,347.29,10.69">[CLS] lef t context [M start ] mention text [M end ] right context [SEP ]</formula><p>In both of these models, [CLS] token representation is extracted and used for representing the Wikipedia entities in the first one and mentions in the second one. Later on these vectors are passed into a single layer neural network that performs dot-product to determine the similarity between each mention and candidate entity pairs. The parameters are optimized by maximizing the dot-product similarity of golden mention-entity pairs. In this Bi-Encoder model, all inputs are cropped or padded into 256 tokens. For Wikipedia the first 256 tokens are used and for mention text, the included left and right context sizes are dependant on the mention length and are limited into 128 tokens.</p><p>A dataset of all linked mentions in the Wikipedia dump was used for training the encoder. After training, all entity vectors are calculated and used for building a FAISS <ref type="bibr" coords="5,257.20,240.95,106.41,10.91" target="#b2">(Johnson et al., 2017)</ref> vector search index. At inference time, each mention is first encoded and then the query is sent to FAISS to retrieve the nearest N neighbors in terms of vector similarity.</p><p>For both of our candidate retrieval methods, namely (1) ElasticSearch and (2) Bi-Encoder, we obtain the top 100 candidates and pass them to final stage in the entity linking pipeline: candidate re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Candidate Re-ranking</head><p>After obtaining the top 100 candidate entities for each detected mention, the next step is to re-rank these candidates and select the most likely prediction for each mention. This step is an optional step as the ranking scores (if available) from the previous retrieval step can be used as well.</p><p>Furthermore we explore the Cross-Encoder approach of Wu et al. ( <ref type="formula" coords="5,459.34,412.70,20.09,10.91">2020</ref>) for additional re-ranking. Cross-Encoder is similar to Bi-Encoder but this time both the Wikipedia entity and the mention are encoded together in the BERT architecture. They are concatenated into an input as shown below:</p><formula xml:id="formula_2" coords="5,135.76,481.54,338.73,26.14">[CLS] lef t context [M start ] mention text [M end ] right context [SEP ] title [EN T IT Y ] description [SEP ]</formula><p>This method is used to further apply deep attention between the mention context and entity text. Following this representation, the embedding of [CLS] token is obtained and sent to a linear layer to produce a final ranking score for the mention-entity pair. Similar to Bi-Encoder, the parameters are optimized by maximizing the scores of golden mention-entity pairs. Unlike Bi-Encoder, this approach does not use the FAISS, instead a score is inferred for a mention and its context together with a candidate Wikipedia entry. For each mention the highest scoring entity out of the 100 retrieved candidates is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mention Ranking</head><p>After obtaining a list of mentions and their linked entities in each news article, final step of the TREC Wikification task is to rank these mentions (and entities) in terms of their importance and relevance to the article. For this purpose we use previous years' top performing method Text Order in this step. The position of the mentions is used for the ranking. Based on previous years' experiments the position of the entities seem to be good indication of their relevance to the news article.</p><p>As for our second method, we utilize the Cross-Encoder model scores for ranking the linked mentions in a document. With this approach we like to see if modeling both the Wikipedia article and news article together can be useful for estimating the relevant entities of a news article as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Discussion</head><p>In order to analyze the effects of different components of the proposed pipeline, 2020 topics are used in the pre-submission experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mention Detection</head><p>For mention detection, in our experiments with Stanford Core-NLP and FLAIR, FLAIR consistently outperforms (≈ 10% boost in final wikification score) Core-NLP in experiments with 2020 topics. Furthermore, we investigate the effectiveness of these tools by calculating recall for the mention detection subtask using 2020 topics which contain 422 gold mentions. As expected, FLAIR with a score of 78% out-performs CoreNLP's 64% score. As a results of these results FLAIR is used for detecting mentions in the submitted runs. It should be noted that both of these tools perform much worse precision-wise with a high rate of false positives mentions detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Candidate Retrieval</head><p>As for candidate retrieval, we report the recall at 1, 10, 50 and 100 candidates for our two retrieval methods on 2020 topics in Table <ref type="table" coords="6,404.58,582.46,4.22,10.91" target="#tab_1">2</ref>. Elastic-Search performs slightly better overall for our retrieval purpose compared to Bi-Encoder. For both methods, we observe a 1-2% boost in recall when retrieving 100 candidates compared to 10 candidates, therefore 100 candidates are retrieved for the submissions. Model R@1 R@10 R@50 R@100 Bi-Encoder 0.67 0.73 0.73 0.74 Elastic-Search 0.72 0.75 0.76 0.77 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Candidate Re-Ranking</head><p>Candidate re-ranking step is an optional step. Normally both Elastic-Search and Bi-Encoder approaches used in the previous retrieval step return a ranked list of results with relevance scores. This ranking can be used directly for linking the entities. With 2020 topics using the Bi-Encoder retrieval list directly without any other re-ranking returned 0.3211 NDCG@5 score. Similarly, Elastic-Search returned 0.3400, which is slightly higher. This year we also experiment with Cross-Encoder as an additional reranking mechanism. Re-ranking the retrieved 100 entities with this method provided significant improvements in both Bi-Encoder and Elastic-Search experiments, NDCG@5 score of 0.4244 and 0.4380 respectively. Modeling the Wikipedia entry and the news article together seems to be useful for identifying the more relevant entities. In order to see whether this holds for 2021 topics as well, all these four settings are submitted as part of the official runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Mention Ranking</head><p>For mention ranking, we continue to use the text order (position of the mention) for most of the runs. As an addition experiment we also use Cross-Encoder model's score directly for mention ranking, as we would like to analyze the correlation between Cross-Encoder's score and mention's relevance to the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">TREC 2021 Submissions</head><p>This year five runs are submitted, and all of our submissions utilize FLAIR for the mention detection. FLAIR model detected a total of 2830 mentions in 1198 paragraph across 51 news article.</p><p>The details of the five submissions are as following:</p><p>• SU-BiEnc: Utilizes Bi-Encoder for retrieval, also uses Bi-Encoder score for candidate ranking and finally text-order for mention ranking.</p><p>System NDCG@5 SU-BiEnc 0.6422 SU-ES 0.6483 SU-BiEnc-CrsEnc 0.7542 SU-ES-CrsEnc 0.7482 SU-ES-CrsEnc-NF 0.7476 Table <ref type="table" coords="8,236.26,222.74,4.22,10.91">3</ref>: NDCG@5 Scores on 2021 Topics</p><p>• SU-BiEnc-CrsEnc: Utilizes Bi-Encoder for retrieval, Cross-Encoder for candidate re-ranking and text-order for mention ranking.</p><p>• SU-ES-CrsEnc: Utilizes Elastic-Search for retrieval, Cross-Encoder for candidate re-ranking and text-order for mention ranking.</p><p>• SU-ES: Utilizes Elastic-Search for retrieval, Elastic-Search scores are used for candidate ranking and text-order is used for mention ranking.</p><p>• SU-ES-CrsEnc-NF: Utilizes Elastic-Search for retrieval and Cross-Encoder for both candidate and mention ranking. NF stands for no text order ranking for mentions.</p><p>The NDCG@5 scores of our submitted runs are presented in Table <ref type="table" coords="8,476.01,415.50,4.22,10.91">3</ref>. We observe that 2021 results are mostly consistent with our experiments on 2020 topics. Using Bi-Encoder or Elastic-Search directly without any candidate re-ranking returned very similar performances. When the Cross-Encoder is introduced as an additional re-ranking mechanism, a significant improvement (around 0.10-0.11) is observed again. Using Bi-Encoder and Cross-Encoder together slightly outperformed the Elastic-Search and Cross-Encoder combination this time.</p><p>As specified by TREC submission rules, only 100 detected entities can be submitted per news article. For 4 of our 5 submissions, text order (appearance in text) is used to create the top 100 ranked mentions. As our last submission (SU-ES-CrsEnc-NF) we use the Cross-Encoder scores to create the final ranking for submission. This one also performed similar to the text order ranking approach.</p><p>Even though the high scores (around 0.75), we believe there is one pitfall in our architecture. Utilizing separate models in the wikification pipeline causes errors to propagate from one step to the other. Our first step, mention detection model detects a large amount of candidate mention spans with a low precision score. This calls for a method to filter out or reject unlikely mention spans instead of trying to link them to entities. This will be analyzed as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present our work for TREC 2021 News Track Wikification task. This year, we explore different methods for the different subtasks in the pipeline. In addition to trying successful methods from previous years' wikification experiments, more recent neural network-based entity linking approaches are also explored. Based on our experiments, we observe high effectiveness of neural-based architecture, especially in mention detection and candidate ranking subtasks. Furthermore, based on our results, search engines are still competent options for high-recall candidate retrieval systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,194.84,536.95,220.57,10.91;3,135.05,355.31,340.16,170.37"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Wikification Entity Linking Pipeline</figDesc><graphic coords="3,135.05,355.31,340.16,170.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,125.80,475.81,358.66,154.42"><head>Table 1 , topics from this year and last year are very similar in terms of length. As the knowledge base, previous year's Wikipedia dump of Jan 2020 is also used this year.</head><label>1</label><figDesc>. According to</figDesc><table coords="2,350.20,540.02,55.35,10.91"><row><cell>2020 2021</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,167.18,182.49,275.90,10.91"><head>Table 2 :</head><label>2</label><figDesc>Recall Scores of Retrieval Models on 2020 Topics</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,125.80,358.39,358.66,10.91;9,136.65,371.94,105.13,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,394.30,358.39,90.15,10.91;9,136.65,371.94,19.77,10.91">Su-nlp at trec news 2020</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E</forename><surname>Ak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Çağhan</forename><surname>Köksal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fayoumi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yeniterzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,179.72,372.60,26.99,9.63">TREC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,125.80,394.45,358.65,10.91;9,136.65,408.00,347.81,10.91;9,136.65,422.22,347.81,9.63;9,136.65,435.10,323.18,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,136.65,408.00,259.41,10.91">Flair: An easy-to-use framework for state-of-the-art nlp</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,417.32,408.67,67.13,9.63;9,136.65,422.22,347.81,9.63;9,136.65,435.77,226.46,9.63">NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,125.80,457.62,358.65,10.91;9,136.65,471.17,215.58,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m" coord="9,317.91,457.62,166.54,10.91;9,136.65,471.17,19.82,10.91">Billion-scale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,125.80,493.68,358.66,10.91;9,136.65,507.23,347.81,10.91;9,136.65,520.78,104.60,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,136.65,507.23,294.45,10.91">The stanford corenlp natural language processing toolkit. 01</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-5010</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,125.80,543.30,358.66,10.91;9,136.65,556.85,283.04,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,439.80,543.30,44.66,10.91;9,136.65,556.85,189.94,10.91">Zero-shot entity linking with dense entity retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,349.15,557.51,35.19,9.63">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
