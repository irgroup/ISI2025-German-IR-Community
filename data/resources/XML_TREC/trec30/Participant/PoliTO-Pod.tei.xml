<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.31,101.17,389.86,15.48">PoliTO at TREC 2021 Podcast Summarization Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.69,158.64,65.05,8.96"><forename type="first">Lorenzo</forename><surname>Vaiani</surname></persName>
							<email>lorenzo.vaiani@polito.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Automatica e Informatica Politecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.44,158.64,47.69,8.96"><forename type="first">La</forename><surname>Moreno</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Automatica e Informatica Politecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.62,158.64,30.99,8.96"><surname>Quatra</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Automatica e Informatica Politecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.76,219.55,60.42,8.96"><forename type="first">Luca</forename><surname>Cagliero</surname></persName>
							<email>luca.cagliero@polito.it</email>
							<affiliation key="aff2">
								<orgName type="department">Dipartimento di Automatica e Informatica Politecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.65,219.55,52.75,8.96"><forename type="first">Paolo</forename><surname>Garza</surname></persName>
							<email>paolo.garza@polito.it</email>
							<affiliation key="aff3">
								<orgName type="department">Dipartimento di Automatica e Informatica Politecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.31,101.17,389.86,15.48">PoliTO at TREC 2021 Podcast Summarization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F620CD713B74AAEBC411BFAF9E4E5698</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the approach proposed by the PoliTO team to accomplish the TREC 2021 podcast summarization task. The purpose is to extract synchronized text/audio segments that convey the most relevant podcast information. The main challenge is to consider is the multimodal nature of the data source, which comprises both textual and acoustic sequences. PoliTO presents a two-stage pipeline that (i) extracts relevant content from multimodal sources and (ii) leverages the extracted content to generate abstractive summaries by using an attention-based Deep Learning architecture. The extractive stage combines the high-dimensional encodings of both textual and audio sources to build a neural network-based regression model. The key idea is to predict the textual similarity between the selected text snippets and the podcast description by also exploiting the underlying information provided by the acoustic features. While audio summaries are obtained by concatenating selected audio samples, summaries in textual form are generated by exploiting the selected information as input of a sequence-to-sequence generative model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Podcasts Track at the Text Retrieval Conference (TREC) is intended to foster research in podcast retrieval and access by researchers from the information retrieval, the NLP, and the speech analysis fields <ref type="bibr" coords="1,132.03,547.57,10.56,8.64" target="#b8">[9]</ref>. The research challenge consists of two main tasks: segment retrieval and podcast episode summarization. The former one is focused on retrieving the two-minute video segments that are most pertinent to a given query. The latter task aims at extracting a concise summary of each podcast episode consisting of a shortlist of speech transcription/audio extracts. The system proposed by the PoliTO team addresses the podcast summarization task.</p><p>The main challenges that have to be faced by the participants to the 2021 Edition of the podcast summarization task are (i) the integration of the acoustic features deeply into the summarization process, and (ii) the ability to process heterogeneous podcast episodes and shows. Challenge (i) entails analyzing multimodal content to gain insights into both textual and acoustic features. The resulting summary is expected to include both audio and textual segments. To address the aforesaid issue, the PoliTO approach relies on multimodal Deep Learning architecture. Challenge (ii) is related to the presence in the source dataset of heterogeneous sets of podcast episodes and shows. The need to summarize podcasts with highly variable content and length calls for new, effective approaches to attend relevant information in the raw data. For example, the automatic recognition of advertisements has shown to be particularly helpful while coping with long-lasting podcasts.</p><p>The PoliTO system first retrieves a selection of speech transcription segments, which can be straightforwardly mapped to the corresponding portions of the original audio track. Then, the output summary in textual form is generated on top of the extracted content by using an abstractive summarization model. For each episode the sentence retrieval step evaluates content relevance according to its similarity with the (creator-provided) episode description.</p><p>We propose an architecture composed of multiple components: a text encoder, an acoustic feature aggregator, and a multi-layered regression network whose aim is to extract the multimodal data pairs that will then constitute the input of the audio selector and the abstractive summarizer. The PoliTo system supports the generation of multiple variants of the textual summaries depending on the end-user preferences on number of input sentences and the minimum summary length. This paper is organized as follows. Section 2 overviews of the data collection and task-related details. Section 3 discusses the methodology adopted for designing the proposed system. Section 4 describes the systems runs submitted for TREC evaluation and discusses the main findings. Finally, Section 5 draws conclusions and enumerates possible future developments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Collection</head><p>A detailed description of the Spotify podcast dataset can be found in <ref type="bibr" coords="2,385.67,300.98,10.66,8.64" target="#b2">[3]</ref>. We briefly recap its main characteristics that were useful for the design of our multimodal summarization system. The dataset consists of a collection of more than 100,000 podcasts grouped into approximately 1,800 shows. For each episode, there are several information divided into three main classes: audio, text and metadata. The acoustic information available in the original data collection includes (i) the original audio file, (ii) OpenSmile <ref type="bibr" coords="2,190.16,355.52,11.75,8.64" target="#b4">[5]</ref> low-level descriptors and (iii) Yamnet<ref type="foot" coords="2,359.33,353.85,3.49,6.05" target="#foot_0">1</ref> embedding vectors. Each episode comes its transcript extracted from the audio track. It is segmented into textual segments and each of them contains timing information for text-audio alignment. Episode metadata contains additional information such as the podcast creator and the his/her manually-written description for the episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Podcast summarization task</head><p>TREC 2021 launched two different research challenges related to the podcast track: segment retrieval and summarization. The PoliTO team proposes a system tailored to the podcast summarization task. Its objective is to generate/extract a short digest that summarizes the content of the episode in both textual and audio formats. While the former can be either extracted from the text or abstractly generated, the latter must necessarily be an extract of the audio track shorter than one minute (according to the task rules). The evaluation is manually performed by the NIST assessors and the quality of each summary is ranked within a scale of 4 possible values: Excellent, Good, Fair, Bad. In addition to the quality evaluation, the assessors also provide answers for nine "yes/no" questions regarding the content and quality of the generated digests:</p><p>• Q1: Does the summary include names of the main people (hosts, guests, characters) involved or mentioned in the podcast?</p><p>• Q2: Does the summary give any additional information about the people mentioned (such as their job titles, biographies, personal background, etc)?</p><p>• Q3: Does the summary include the main topic(s) of the podcast?</p><p>• Q4: Does the summary tell you anything about the format of the podcast; e.g. whether it's an interview, whether it's a chat between friends, a monologue, etc?</p><p>• Q5: Does the summary give you mode context on the title of the podcast?</p><p>• Q6: Does the summary contain redundant information?</p><p>• Q7: Is the summary written in good English?</p><p>• Q8: Are the start and end of the summary good sentence and paragraph start and end points?</p><p>• Q9: Does the audio clip give a sense of what the podcast sounds like, (as far as you can tell from listening to it)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The PoliTO system</head><p>In this section, we present the designed deep learning architecture for multimodal podcast summarization. It is designed to perform simultaneous analyses of audio and text modalities in order to create effective representations of the input data. As first step, for each sentence of the podcast we simultaneously encode its text and aggregate audio information into two separate fixed-size vectors. Then, we combine the two feature vectors to process them with a multimodal regression network. It is composed of a stack of 7 fully connected layers with ReLU activation fuction <ref type="bibr" coords="3,427.37,193.59,10.55,8.64" target="#b5">[6]</ref>. The network is aimed at predicting the relevance of each text-audio snippet to the output summary. We select the top-ranked sentences to (i) provide them as input for a the abstractive summarization component that generate the final abstractive summary and (ii) arrange the audio summary that is instrumental for creating a trailer for the podcast episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Description filtering</head><p>We apply a preliminary filtering step to the description content aimed at removing advertisements as well as commercial contents. To this aim, the description content is split by exploiting the sentence tokenization provided by spaCy library <ref type="bibr" coords="3,270.02,304.48,10.72,8.64" target="#b6">[7]</ref>. Next, similar to <ref type="bibr" coords="3,354.14,304.48,16.73,8.64" target="#b11">[12]</ref> it fine-tunes a pre-trained BERT model <ref type="bibr" coords="3,134.91,315.39,11.48,8.64" target="#b3">[4]</ref> for binary text classification. The model is trained on a small subset of manually annotated description snippets to automatically recognize advertising content. Advertising content is early pruned from the descriptions before running the summarization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text encoding</head><p>Each sentence in the episode transcripts is encoded using of Transformer-based model trained on sentence similarity tasks <ref type="bibr" coords="3,225.17,393.55,15.42,8.64" target="#b10">[11]</ref>. The state-of-the-art approach is used as reference encoder for the podcast speech transcription. In our experiments we use the paraphrase-mpnet-base-v2 based on MPNet model <ref type="bibr" coords="3,207.59,415.37,15.42,8.64" target="#b12">[13]</ref>. It allows a maximum sequence length of 512 tokens and generates 768-dimensional vector for each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Acoustic information aggregation</head><p>The proposed multimodal architecture also leverages acoustic information. Our model rely on the OpenSmile <ref type="bibr" coords="3,156.22,482.62,11.75,8.64" target="#b4">[5]</ref> features provided by dataset authors. The feature vector for each audio sentence is an 88-dimensional vector that represents some acoustic characteristics of the given speech audio segment. Those features are used for sound description and to identify some speaker-related aspects, such as emotion, age, gender, and personality. OpenSmile features extraction occurs with a sampling frequency of one second, so we represent the acoustic information of a sentence aggregating the 88-dimensional extractions that occurred for the corresponding audio snippet. In the aggregation phase we compute means, standard deviations, minimums and maximums values for each descriptor. By concatenating the resulting vectors we get the 352-dimensional vector per audio sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-layered regression network</head><p>The vectors obtained by audio and text encoding steps are then fed to multi-layered fully-connected architecture. It aims at processing the multimodal representation, including a mixture of deep learned and hand-crafted features, obtained by concatenating text-and audio-related encodings. The network consists of a stack of fully-connected (FC) layers and it is trained to predict a score that represents the pertinence of the audio, text pair to the episode description. In our experiments, we set the depth of the fusion network to 7, where the width for the first three layers and the last four is set to 1120 and 768 respectively. The network is trained to predict the relevance of the multimodal text-audio segments to the creator-provided episode description. The relevance score for the training set is computed, for each sentence, as the semantic similarity between the sentence itself and the episode's description. To this aim we exploit the Sentence-BERT model trained on Semantic Text Similarity (STS) task <ref type="bibr" coords="3,152.50,713.51,10.58,8.64" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary generation</head><p>The PoliTO system provides end-users with a multimodal summary of the podcast episode including both audio and text. To produce textual summary it selects the top-scored sentences according to the model prediction. Those sentences are concatenated and fed as input of the abstractive summarizer. The abstractive summarization system relies on a transformer-based encoder-decoder architecture that can process long text sequences, namely Longformer <ref type="bibr" coords="4,392.09,139.09,10.66,8.64" target="#b0">[1]</ref>. The Longformer model is based on transformer architecture <ref type="bibr" coords="4,259.63,150.00,16.73,8.64" target="#b13">[14]</ref> while introducing relevant modification to the original attention mechanism. While the original self-attention scales quadratically with the sequence length, thus hindering the processing of long text sequences, Longformer introduces a windowed attention mechanism that scales linearly with sequence length, enabling efficient long documents processing. LED (Longformer Encoder-Decoder) is the sequence-to-sequence variant of the architecture that support long document-based generative tasks. LED reduces the limitations on the number of sentences that need to be selected to contribute to the generation of the abstractive summary. Indeed, the number of selected sentences is one of the two parameters that characterize our submissions to the competition, in addition to the threshold on the minimum length of the generated summary. The sequence-to-sequence model is fine-tuned for three epochs to generate summaries as close as possible to the author's provided description.</p><p>The generation of audio summaries entails the selection of the K audio samples associated to the topscored multimodal pairs. We choose K as the minimum number of pairs such that the total duration of the audio snippets exceeds the threshold set for the Podcast Summarization Track (i.e., 60 seconds).</p><p>The selected audio snippets are (i) re-ranked according to their ascending order of appearance in the original podcast, (i) concatenated and (iii) trimmed to avoid exceeding the maximum duration (i.e., 60 seconds). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>4.1 Submitted runs TREC organizers allowed participants to submit up to 4 different runs. PoliTO applied the approach described in the previous section to generate 4 different textual outputs. They differ in the minimum length of the generated summaries and the number of selected sentences from the original transcript. Each textual summary is paired with an audio trailer extracted using the same approach for all the runs. Both parameters characterizing each submission are related to the training and generation procedures of the abstractive summarization model.</p><p>Each run submitted by our team is described below:</p><p>• 25_32-128: this run includes abstractive text-based summaries with a length that ranges between 32 and 128 words. The input of the abstractive model consists of the 25 top-ranked sentences.</p><p>• 50_32-128: this run includes abstractive text-based summaries with a length that ranges between 32 and 128 words. The input of the abstractive model consists of the 50 top-ranked sentences.</p><p>• 50_64-128: this run includes abstractive text-based summaries with a length that ranges between 64 and 128 words. The input of the abstractive model consists of the 50 topranked sentences. The choice of increasing the length of the generated summaries is due to the analysis of the summaries generated in the previous runs. In those cases, the output summaries tend to be very short with a token length close to the lower limit.</p><p>• 100_32-128: this run includes abstractive text-based summaries with a length that ranges between 32 and 128 words. The input of the abstractive model consists of the 100 top-ranked sentences. This parametrization of the system aims at reducing the impact of extractive sentence selection. The model acts as a filter only for very long episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human evaluation</head><p>NIST evaluators judged 193 episodes randomly selected from the full test set consisting of approximately 1000 episodes. Task organizers also provides a baseline summary, for each podcast in the test set, based on the content of the first minute of the episode. Table <ref type="table" coords="5,397.76,287.82,5.08,8.64" target="#tab_0">1</ref> shows the results of our submissions comparing them with the proposed baseline. The run identifier is reported in the first column, the average quality (between 0 and 3) in the second one and the percentage of "yes" given by the assessors for each remaining question in the rest of the columns. The best value for each column is written in boldface. Note that, for question 6, the smallest value correspond to best performances since the "yes" answer has a negative meaning.</p><p>Analyzing the results, the human evaluation shows that our summaries outperforms the baseline in the majority of cases. On average the highest quality score (e.g., Quality) is obtained by setting the minimum length to the highest value (e.g., 64 tokens). Our runs get a better rating for the majority of other questions as well. Grouping the questions according to their semantic meaning,</p><p>• Q1 and Q2 are the only two questions for which the baseline gets a better evaluation. Both questions are related to the recognition of names, titles and personal information from the original podcast. The lower scores associated to our submissions are probably due to the use of LED model. Its peculiar attention mechanism is prone to hallucinations <ref type="bibr" coords="5,447.74,447.12,11.75,8.64" target="#b7">[8]</ref> thus could happen that the model fabricate or alter content that is not present in the input data.</p><p>• Q3, Q4 and Q5 are content-related questions and for two of them (Q3 and Q5) the best score is obtained by the submission 50_64-128 (which achieves a good rating also for Q4, slightly lower than the top-scored one). Those scores are mainly related to the amount of information contained in the summaries. Our top-scored submission generate longer summaries compared with others since the parameter for minimum number of words is doubled.</p><p>• Q6 is related to the redundancy of the generated summary. The best results is obtained by the submission that is provided with the longer input data (e.g., 100_32-128). This result demonstrates that allowing the model to process an higher number of input sentences helps reducing the redundant content in the output summaries.</p><p>• Q7 and Q8 are syntactic-and linguistic-related questions. In both cases the best performing submission is 50_32-128. It turned out to be the best model to balance the amount of input and output information to generate fluent summaries.</p><p>• Q9 is the only audio-related question. Our submissions obtain slightly different evaluations even if the output audio snippets are shared among all submission. This can happen when the audio summaries for the same episode are manually evaluated from different assessors. In all cases, however, our model achieves higher rating if compared with the baseline.</p><p>When compared with other system submissions for the Podcast Summarization task, our best run (e.g., 50_64-128) ranked first and second-best according to text and audio quality respectively <ref type="bibr" coords="5,484.43,713.51,15.27,8.64" target="#b9">[10]</ref>. We also talk about Bianca and her scar, and how she feels about the Greek gods.</p><p>50_64-128 3 We're back with another episode! This time we're talking about the Titans of Olympus and the trials of Apollo. We're also talking about Bianca's death and how she feels about her relationship with her mother. We also talk about the Riptides and the Ocarina of Time. We hope you enjoy it! 100_64-128 2 In this episode we talk about the Titans of Olympus and the trials of Apollo. We also talk about Bianca's accent and how she is a monster.</p><p>Table <ref type="table" coords="6,163.09,462.14,3.88,8.64">2</ref>: Qualitative examples for the podcast episode: 6iRBuqS8OxEdShKp85uXQv</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative results</head><p>In Table <ref type="table" coords="6,142.65,526.02,4.90,8.64">2</ref> we report a qualitative comparison between summaries generated for each submission and the original podcast description. For each run we also report the quality score assigned by manual evaluation. All abstractive summaries are written in fluent English and use similar opening sentence, that is commonly used in episode descriptions. All of them discuss the same topic by using different level of details. Considering names and titles of people involved into the discussion, our submitted summaries focus on the same fictional character (e.g., Bianca). However, different submissions focus on different peculiar aspects of the character while mentioning concepts related to similar topics (e.g., Olympus, gods, and mythology). It is worth noting that, most of the people's names mentioned in the description do not emerge in automatically generated summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future work</head><p>In this paper we present the PoliTO system designed for multimodal summarization of podcast episodes. A deep learning architecture is proposed to effectively combine text encoding and acoustic descriptors using a multimodal regression network.</p><p>The summarization performance achieved by our submissions is superior to that of the baseline model in terms of quality and for 7 binary questions out of 9. However, there is no submission that has the highest rating for all characteristics assessed by human evaluation. Different parameters' configurations have different strengths and weaknesses. Analyzing the the manual evaluation, we can conclude that the use of multimodality is beneficial for the identification of key phrases of a podcast. However, our solution has some limitations due to the phenomenon of hallucination that hinders handling specific information (e.g., people's names or titles).</p><p>As future work, we plan to replace the acoustic descriptors exploiting ad-hoc speech embedding models. Furthermore, we aim at proposing an end-to-end architecture including feature extractors models into the training process, for both text and audio.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,218.27,526.40,175.47,8.64;4,108.00,348.75,396.00,166.25"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sketch of the PoliTO architecture.</figDesc><graphic coords="4,108.00,348.75,396.00,166.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,108.00,76.86,402.44,243.24"><head>Table 1 :</head><label>1</label><figDesc>NIST evaluation results. In this episode, we talk about Percy and the hunters, and we also talk about the Bianca and Talia encounter. We also get a glimpse into the mind of the Riptide, and a little bit of the mythology. -50_32-1282 In this episode, we talk about The Titan's Curse and the quest.</figDesc><table coords="6,108.00,76.86,402.44,202.01"><row><cell></cell><cell>Quality</cell><cell>Q1</cell><cell>Q2</cell><cell>Q3</cell><cell>Q4</cell><cell>Q5</cell><cell>Q6</cell><cell>Q7</cell><cell>Q8</cell><cell>Q9</cell></row><row><cell>baseline</cell><cell>0.772</cell><cell cols="9">0.549 0.326 0.606 0.427 0.536 0.451 0.456 0.187 0.957</cell></row><row><cell>25_32-138</cell><cell>0.974</cell><cell cols="9">0.354 0.255 0.645 0.513 0.523 0.183 0.806 0.594 0.978</cell></row><row><cell>50_32-138</cell><cell>0.860</cell><cell cols="9">0.323 0.234 0.615 0.437 0.500 0.204 0.811 0.615 0.989</cell></row><row><cell>50_64-128</cell><cell>1.010</cell><cell cols="9">0.378 0.285 0.682 0.503 0.562 0.292 0.715 0.536 0.983</cell></row><row><cell>100_32-128</cell><cell>0.917</cell><cell cols="9">0.333 0.256 0.606 0.446 0.497 0.182 0.776 0.601 0.994</cell></row><row><cell cols="5">Model Quality score Summary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Creator-provided</cell><cell cols="9">/ Welcome! In this episode of A Dam PJO Podcast Caleigh and</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">Izzy discuss The Titan's Curse, as well as 2006 trends, godly</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">interventions, and strongly worded opinions on Bianca di Angelo.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">Hope you'll join us! Come talk to us! Instagram, run by Caleigh</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">https://www.instagram.com/dampjocast/ Twitter, run by Izzy</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">https://twitter.com/dampjocast E-Mail dampjocast@gmail.com</cell></row><row><cell cols="2">25_32-128</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,124.14,704.20,379.86,7.83;2,108.00,714.16,65.75,7.77"><p>https://github.com/tensorflow/models/tree/master/research/audioset/yamnet. Latest access: November</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2021" xml:id="foot_1" coords=""><p></p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,129.58,203.76,375.66,8.64;7,129.58,214.67,22.42,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,326.13,203.76,175.15,8.64">Longformer: The long-document transformer</title>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,129.58,234.01,376.07,8.64;7,129.58,244.92,374.42,8.64;7,129.27,255.65,374.73,8.82;7,128.83,266.74,47.32,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,467.77,234.01,37.88,8.64;7,129.58,244.92,358.41,8.64">Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,129.27,255.65,282.02,8.59">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>SemEval-2017</note>
</biblStruct>

<biblStruct coords="7,129.58,286.08,375.67,8.64;7,129.58,296.98,376.08,8.64;7,129.58,307.71,374.42,8.82;7,129.58,318.62,376.16,8.82;7,129.58,329.71,221.12,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,470.70,296.98,34.95,8.64;7,129.58,307.89,162.17,8.64">000 podcasts: A spoken English document corpus</title>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Bonab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,310.07,307.71,193.93,8.59;7,129.58,318.62,117.92,8.59">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="5903" to="5917" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct coords="7,129.58,349.05,374.42,8.64;7,129.58,359.78,376.08,8.82;7,129.58,370.69,374.42,8.59;7,129.30,381.60,375.94,8.82;7,129.58,392.69,264.84,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,414.47,349.05,89.53,8.64;7,129.58,359.96,230.07,8.64">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,377.99,359.78,127.67,8.59;7,129.58,370.69,374.42,8.59;7,129.30,381.60,94.22,8.59">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s" coord="7,275.11,381.60,89.48,8.59">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,129.58,412.03,374.42,8.64;7,129.58,422.76,376.07,8.82;7,129.58,433.67,374.59,8.82;7,129.58,444.75,92.33,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,349.38,412.03,154.62,8.64;7,129.58,422.94,154.42,8.64">Opensmile: The munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,301.94,422.76,203.71,8.59;7,129.58,433.67,118.17,8.82">Proceedings of the 18th ACM International Conference on Multimedia, MM &apos;10</title>
		<meeting>the 18th ACM International Conference on Multimedia, MM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,129.58,464.09,374.42,8.64;7,129.27,474.82,375.97,8.59;7,129.58,485.91,277.56,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,341.39,464.09,146.38,8.64">Deep sparse rectifier neural networks</title>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,129.27,474.82,372.55,8.59;7,194.60,485.91,183.36,8.64">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct coords="7,129.58,505.25,374.42,8.64;7,129.58,516.16,344.24,8.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,285.10,505.25,218.90,8.64;7,129.58,516.16,270.36,8.64">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="7,129.58,535.50,374.59,8.64;7,129.58,546.23,374.42,8.82;7,129.25,557.14,375.99,8.59;7,129.58,568.22,324.88,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,416.79,535.50,87.38,8.64;7,129.58,546.41,119.19,8.64">Efficient attentions for long document summarization</title>
		<author>
			<persName coords=""><forename type="first">Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolaus</forename><surname>Parulian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,267.56,546.23,236.43,8.59;7,129.25,557.14,371.72,8.59">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="1419" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,129.58,587.56,374.42,8.64;7,129.00,598.47,376.75,8.64;7,129.22,609.20,376.52,8.82;7,129.02,620.11,374.98,8.82;7,128.83,631.02,376.41,8.82;7,129.58,642.11,22.42,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,307.07,598.47,144.78,8.64">TREC 2020 podcasts track overview</title>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,269.86,609.20,235.89,8.59;7,129.02,620.11,106.92,8.59">Proceedings of the Twenty-Ninth Text REtrieval Conference, TREC 2020, Virtual Event</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Angela</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth Text REtrieval Conference, TREC 2020, Virtual Event<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">November 16-20, 2020. 2020</date>
			<biblScope unit="volume">1266</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,129.58,661.45,375.66,8.64;7,129.58,672.36,308.57,8.64" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tanaka</surname></persName>
		</author>
		<title level="m" coord="7,271.77,672.36,136.12,8.64">Trec 2021 podcasts track overview</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,129.58,691.70,376.07,8.64;7,129.58,702.43,374.42,8.82;7,129.27,713.34,258.30,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,273.67,691.70,231.97,8.64;7,129.58,702.61,34.49,8.64">Sentence-bert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName coords=""><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,182.76,702.43,321.23,8.59;7,129.27,713.34,42.11,8.59">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,129.58,75.48,375.80,8.64;8,129.58,86.21,316.70,8.82" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,410.51,75.48,94.87,8.64;8,129.58,86.39,189.16,8.64">Spotify at TREC 2020: Genre-aware abstractive podcast summarization</title>
		<author>
			<persName coords=""><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<idno>CoRR, abs/2104.03343</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,129.58,105.27,374.42,8.64;8,129.58,116.00,324.19,8.82" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09297</idno>
		<title level="m" coord="8,382.12,105.27,121.88,8.64;8,129.58,116.17,156.81,8.64">Mpnet: Masked and permuted pre-training for language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,129.58,135.05,375.66,8.64;8,129.58,145.78,374.42,8.82;8,129.58,156.69,178.03,8.82" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,272.16,145.96,92.52,8.64">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,381.52,145.78,122.47,8.59;8,129.58,156.69,74.02,8.59">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
