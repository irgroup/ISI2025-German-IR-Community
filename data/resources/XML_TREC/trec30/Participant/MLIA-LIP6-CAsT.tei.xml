<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.04,164.85,283.17,15.12;1,141.12,186.77,329.01,15.12;1,251.04,208.68,109.17,15.12">MLIA-LIP6@TREC-CAST2021: Feature augmentation for query recontextualization and passage ranking</title>
				<funder ref="#_n8zKzmU #_AVnATAX">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.23,241.17,78.75,10.48"><forename type="first">Nawel</forename><surname>Astaouti</surname></persName>
							<email>nawel.astouati@etu.upmc.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CNRS-ISIR</orgName>
								<orgName type="institution" key="instit2">Sorbonne University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,339.94,241.17,79.48,10.48"><forename type="first">Thomas</forename><surname>Gerald</surname></persName>
							<email>thomas.gerald@lip6.fr</email>
						</author>
						<author>
							<persName coords="1,205.12,274.92,70.55,10.48"><forename type="first">Maya</forename><surname>Touzari</surname></persName>
							<email>maya.touzari@etu.upmc.fr</email>
						</author>
						<author>
							<persName coords="1,341.80,274.92,67.46,10.48"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
							<email>nie@iro.umontreal.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.04,308.67,68.44,10.48"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
							<email>soulier@isir.upmc.fr</email>
						</author>
						<title level="a" type="main" coord="1,164.04,164.85,283.17,15.12;1,141.12,186.77,329.01,15.12;1,251.04,208.68,109.17,15.12">MLIA-LIP6@TREC-CAST2021: Feature augmentation for query recontextualization and passage ranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55E8CA32A5788FA87E3816D664C6F558</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we investigate approaches for query recontextualization in the context of conversational search. We use a pipeline setting in which we first reformulate the query and then rank passages according to a backbone model. Our main focus is put on the feature inputs of a T5 query reformulation model and we evaluate different evidence sources such as the history (previous questions and answers) as well as semantic proxy through the doc2query model. We also experiment an end-to-end version of the setting which unfortunately has not been much optimized due to time constraints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A key component in conversational search is the construction of contextualized queries, which should incorporate relevant information from previous utterances in the context in order to retrieve documents. Different approaches can be used, ranging from query reformulation before document ranking or directly ranking documents using the conversation context in an end-to-end fashion.</p><p>In this work, we aim to test approaches to collect relevant pieces of information from previous utterances through a recontextualization (RC) sequence-tosequence model. Namely, we use the T5 model <ref type="bibr" coords="1,334.01,640.15,10.52,8.74" target="#b5">[6]</ref> to reformulate queries. From a given query and a conversation context, the trained RC model produces a new query that must contain self-sufficient information allowing a ranking model (BM25+MonoT5) <ref type="bibr" coords="2,215.07,127.96,10.52,8.74" target="#b3">[4]</ref> to retrieve relevant documents. The main objective of the work is to understand what types of input features are necessary for the RC model to perform well. To this end, we only work on features based on either previous queries or the associated provided answers (namely documents). Particularly, our features are based on the raw data (either documents or queries), reformulated raw queries, or queries generated from relevant documents by using the doc2query model <ref type="bibr" coords="2,227.69,199.69,9.96,8.74" target="#b4">[5]</ref>. We also develop an end-to-end reformulation/ranking approach, hoping that the outcome can be used to optimize the entire process. However, due to the late development of the approach, we were unable to efficiently train the model and index it on the whole released document collection. We have thus formulated relevance proxy for training data that did not allow us to optimize this last setting at its best. All approaches described here have been submitted to the TREC CAsT 2021 challenge.</p><p>In the following sections, we will first briefly present notations. Then, we will describe the different approaches submitted to TREC CAsT 2021, distinguishing pipeline and end-to-end versions. Finally, we present and discuss the results on TREC CAsT 2021 challenge and conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task definition and notation</head><p>Let's consider a conversation C composed of successive utterances i, alternating questions q i and passages returned as answers p i . Its history is viewed as a sequence of queries/passages: C = {q 1 , p 1 , q 2 , p 2 , . . . , q i , p i , . . . }</p><p>The objective of the the TREC-CAST 2021 challenge is to retrieve, for each query q i in the conversation, the relevant passages p i given the previous utterances. To do this task, we develop and submit different types of models: the first group of models relying on a reformulation and ranking pipeline, the second using an end-to-end version for reformulating and ranking conversational queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reformulation pipeline</head><p>This pipeline relies on learning two main modules as presented in Figure <ref type="figure" coords="2,469.73,556.30,3.87,8.74">1</ref>.</p><p>The first one, reformulating the current query given the context using the pretrained T5 model<ref type="foot" coords="2,209.00,578.63,3.97,6.12" target="#foot_0">1</ref>  <ref type="bibr" coords="2,216.34,580.21,9.96,8.74" target="#b5">[6]</ref>. The T5 model is a sequence-to-sequence model encoding and decoding text using transformers. This model has shown great ability for generating text sequences and has already been used for reformulated queries <ref type="bibr" coords="2,133.77,616.07,9.96,8.74" target="#b2">[3]</ref>. The second performs re-ranking on the contextualised query using the pretrained MonoT5 model <ref type="bibr" coords="2,237.85,628.03,9.96,8.74" target="#b3">[4]</ref>.</p><formula xml:id="formula_1" coords="3,221.13,132.38,176.54,18.13">Reformulation (T5) Ranking (BM25+MonoT5)</formula><p>Figure <ref type="figure" coords="3,166.05,172.13,3.87,8.74">1</ref>: The Conversational IR pipeline, containing reformulation using T5 model and ranking using BM25 as primary ranker and Mono-T5 as re-ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpora and Training data</head><p>For fine-tuning the T5 Reformulation model, we considered two datasets:</p><p>• The CANARD corpus 2 is a conversational dataset with reformulated queries. Similarly to the TREC CAsT test set, it is possible to obtain an history for a query q i as well as its reformulated version. The training, development, and test sets includes 31.538, 3.418, and 5.571 contextual and reformulated queries respectively.</p><p>• CASTuR <ref type="bibr" coords="3,186.06,305.29,10.52,8.74" target="#b0">[1]</ref> also includes conversational data but highlights for each query utterance which previous questions could help to express the current information need. For each query, we have manually rewritten the query (by using the context). We thus obtain a new dataset composed of 752 contextual and reformulated queries (by considering only the helpful context). This dataset has been used to augment the CANARD training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submitted models</head><p>We focus on the reformulation module and use a fixed ranking module for all submitted runs. We propose three different models for reformulation, which differ by the type and the data provided to T5 during the inference phase. t5 monot5: Given conversation C, this model takes into consideration the previous queries and passages to reformulate query q i . The underlying intuition of this model is to evaluate the potential of leveraging the whole conversation for reformulating queries. The input to T 5 is the following sequence for query utterance q i :</p><formula xml:id="formula_2" coords="3,228.57,520.60,153.61,9.65">q 1 |||p 1 |||q 2 |||p 2 ||| . . . |||q i-1 |||p i-1 |||q i</formula><p>where q 1 , ... q i-1 are previous queries and p 1 , ... p i-1 are the passages returned to the user in conversation C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rewritt5 monot5:</head><p>Similarly to the previous model, the history is given as input to the model, but the raw queries are replaced by the reformulated queries q 1 , ... q i-1 contained in the ground truth of our training dataset. Our objective is to evaluate the potential of online reformulation, this model being the oracle since it uses the ground truth. The input fed to the T5 model is thus:</p><formula xml:id="formula_3" coords="3,228.57,647.39,153.61,10.62">q 1 |||p 1 |||q 2 |||p 2 ||| . . . |||q i-1 |||p i-1 |||q i</formula><p>2 https://sites.google.com/view/qanta/projects/canard where q j is the reformulated j th query in the ground truth.</p><p>t5 doc2query: This model is based on the history of the conversation queries, supplemented by queries generated by a doc2query model (noted d2q()) on previous relevant passages (p i ). The doc2query model <ref type="bibr" coords="4,368.08,177.77,10.52,8.74" target="#b4">[5]</ref> generates queries from a given document which are used to augment the document content for collection indexing. Assuming that passages are longer than queries and might make noise in the semantic understanding, we propose here to replace each passage in the conversation by the top query generated by the doc2query model. Our intuition is that it might enhance the intent context with additional queries. The input to T 5 is a text with a special token delimiting utterance or type of features. For query q i , the associated input sequence is:</p><formula xml:id="formula_4" coords="4,194.49,285.36,221.77,10.62">q 1 |||d2q(p 1 )|||q 2 |||d2q(p 2 )||| . . . |||q i-1 |||d2q(p i-1 )|||q i</formula><p>where d2q(p j ) is the first query generated by the doc2query model for paragraph p j returned in conversation C to the user in response to query q j .</p><p>4 End-to-end reformulation-based ranking</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus and Training data</head><p>We propose here a model learning reformulation and ranking modules in a endto-end fashion. To learn such a model, one way would be to only rely on a ground truth of relevant passages but we believe that the signal within the back-propagation would be weak for guiding the reformulation module. We thus built a dataset with two types of ground truth: relevant passages and query reformulation pairs. To do so, we rely on two datasets: Canard corpus for reformulation MSMarco for ranking. For each CANARD reformulated query (see Section 3.1), we retrieve passages from MSMarco using a re-ranking pipeline based on BM25<ref type="foot" coords="4,239.23,484.47,3.97,6.12" target="#foot_1">3</ref> and MonoT5<ref type="foot" coords="4,303.46,484.47,3.97,6.12" target="#foot_2">4</ref> . We then select the 200 most relevant passages (according to the BM25+MonoT5 re-ranker) for each query. The first 4 passages are assumed to be relevant and the last 4 passages as irrelevant.</p><p>The final training set contains 123 836 negative/positive pairs (30 959 queries) and validation set, 1152 samples. Due to time constraint, we do not index the corpus, but only test on a re-ranking setting based on the monoT5 baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submitted model</head><p>Our end-to-end approach is called t5colbert and aims to reformulate and rank by branching two architectures, as illustrated in Figure <ref type="figure" coords="4,377.46,604.05,7.75,8.74" target="#fig_0">2:</ref> • For the reformulation, we consider the input of the t5 monot5 model presented in Section 3. The network is then trained in a supervised fashion in different steps. We thus have a cross-entropy loss L ref for reformulation and a cross-entropy loss L rank for ranking. The final loss is the sum of both losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Table <ref type="table" coords="5,162.62,430.50,4.98,8.74" target="#tab_0">1</ref> reports the results on the test set submitted to TREC CAsT 2021 using the different approaches. Samples of reformulated queries are presented in Table <ref type="table" coords="5,172.79,454.41,3.87,8.74">2</ref>.</p><p>By first looking at the pipeline models, we can see that the best model is the t5 monot5, which leverages the full conversation with raw queries and raw passages, as provided in the dataset. In contrast, other pipeline models obtain lower results. This suggests that reformulating queries as in t5 doc2query and in Rewritt5 monot5 might induce noise in the conversation. This is surprising since reformulated queries are the ones used in the ground truth. One reason </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,234.54,284.13,142.16,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The T5-Colbert model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,151.96,583.91,307.33,80.97"><head>Table 1 :</head><label>1</label><figDesc>Submitted TREC CAsT 2021 Results (×100)</figDesc><table coords="5,151.96,595.76,307.33,69.11"><row><cell>Model</cell><cell cols="4">NDCG@3 NDCG@5 NDCG@500 MAP@500</cell></row><row><cell>t5 monot5</cell><cell>38.7</cell><cell>39.0</cell><cell>33.6</cell><cell>19.5</cell></row><row><cell>Rewritt5 monot5</cell><cell>36.9</cell><cell>37.2</cell><cell>33.1</cell><cell>18.9</cell></row><row><cell>t5 doc2query</cell><cell>37.7</cell><cell>37.9</cell><cell>33.5</cell><cell>19.7</cell></row><row><cell>E2E (t5colbert)</cell><cell>15.3</cell><cell>15.8</cell><cell>31.4</cell><cell>10.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,149.01,647.51,208.10,6.99"><p>https://huggingface.co/transformers/model doc/t5.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,149.01,644.95,175.22,6.99"><p>based on https://github.com/castorini/pyserini</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,149.01,654.46,207.88,6.99"><p>https://huggingface.co/castorini/monot5-large-msmarco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="5,149.01,545.07,187.69,6.99"><p>https://github.com/stanford-futuredata/ColBERT</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>We would like to thank <rs type="institution">ANR</rs> for supporting this work under the grant <rs type="grantNumber">ANR JCJC</rs> <rs type="projectName">SESAMS</rs> (<rs type="grantNumber">ANR-18-CE23-0001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_n8zKzmU">
					<idno type="grant-number">ANR JCJC</idno>
					<orgName type="project" subtype="full">SESAMS</orgName>
				</org>
				<org type="funding" xml:id="_AVnATAX">
					<idno type="grant-number">ANR-18-CE23-0001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reformulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>I live in a remote area How can I treat it at home t5 monot5</p><p>How can I treat dog swollen ear at home? Rewritt5 monot5</p><p>How can I treat dog ear hematoma at home? t5 doc2query</p><p>How can I treat hematoma in dog? E2E I live in a remote area. How can I treat dog swollen ear ? Query How can I protect them t5 monot5</p><p>How can I protect my dog from the coronavirus? Rewritt5 monot5</p><p>How can I protect Jasmine from the coronavirus? t5 doc2query</p><p>How can I protect dogs from coronavirus? E2E</p><p>How can I protect my dog from the coronavirus? Query That's great What makes it so efficient t5 monot5</p><p>What makes a heat pump so efficient? Rewritt5 monot5</p><p>What makes a heat pump so efficient? t5 doc2query</p><p>That's great What makes a geothermal heat pump so efficient? E2E</p><p>What makes geothermal heat pumps so efficient?</p><p>Table <ref type="table" coords="6,199.18,268.56,3.87,8.74">2</ref>: Examples of reformulations given the original query.</p><p>might be the redundancy of information between the conversation history and the reformulated queries, which might overload the search intent. Moreover, the t5 doc2query is slightly better than the Rewritt5 monot5. The difference of these models lies in the consideration of relevant passages: raw passages for Rewritt5 monot5 and the generated query issued from the doc2query model for t5 doc2query. This result is surprising since the T5 model has been learned on the full conversation history (sequence of queries and passages) and the t5 doc2query use features of a different distribution at the inferences (sequence of queries). This suggests that although based on a different distribution, passages might be somehow noisy for formulating queries. Combined with the previous statement on query reformulation and the results obtained in Table <ref type="table" coords="6,469.73,419.94,3.87,8.74">1</ref>, it seems that replacing passages with queries is not sufficient for balancing the overload of online query reformulation. One additional baseline for future work will be to evaluate the model with raw queries and queries issued from relevant passages (combination of t5 monot5 and t5-doc2query models).</p><p>Concerning the end-to-end t5colbert model, it drastically fails on the test set. This can be due to different reasons such as the complexity of training (finding the best hyperparameters) or designing a better training dataset. As explained earlier, we lack time for indexing the whole collection and performing effective tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this report, we describe our work in the TREC CAsT challenge. We proposed different approaches based on feature selection or generation for the conversational search task. To this end, we adopted the reformulation and ranking pipeline which first re-contextualizes the query and then retrieves relevant passages from a collection. We tested different types of input such as generated queries (with a doc2query model), previous relevant documents (passage), previous queries, or previous reformulated queries.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,149.27,216.50,328.22,8.74;7,149.27,228.46,328.21,8.74;7,149.27,240.41,328.21,8.74;7,149.27,252.37,148.96,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,439.98,216.50,37.51,8.74;7,149.27,228.46,308.49,8.74">Harnessing evolution of multi-turn conversations for effective answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Ríssola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,149.27,240.41,328.21,8.74;7,149.27,252.37,119.07,8.74">Proceedings of 2020 Conference on Human Information Interaction and Retrieval (CHIIR), CHIIR &apos;20</title>
		<meeting>2020 Conference on Human Information Interaction and Retrieval (CHIIR), CHIIR &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,149.27,272.29,328.21,8.74;7,149.27,284.25,328.22,8.74;7,149.27,296.20,328.21,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,277.61,272.29,199.87,8.74;7,149.27,284.25,197.94,8.74">Colbert: Efficient and effective passage search via contextualized late interaction over BERT</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,370.68,284.25,106.80,8.74;7,149.27,296.20,210.83,8.74">SIGIR conference on research and development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,149.27,316.13,328.22,8.74;7,149.27,328.08,328.21,8.74;7,149.27,340.04,128.65,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,423.28,316.13,54.21,8.74;7,149.27,328.08,323.97,8.74">Query reformulation using query history for passage retrieval in conversational search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/2005.02230</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,149.27,359.97,328.21,8.74;7,149.27,371.92,328.22,8.74;7,149.27,383.88,184.86,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,363.75,359.97,113.72,8.74;7,149.27,371.92,166.62,8.74">Document ranking with a pretrained sequence-to-sequence model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,336.88,371.92,32.39,8.74">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,149.27,403.80,328.21,8.74;7,149.27,415.76,179.59,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,344.43,403.80,133.05,8.74;7,149.27,415.76,42.29,8.74">Document expansion by query prediction</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR, abs/1904.08375</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,149.27,435.68,328.21,8.74;7,149.27,447.64,328.21,8.74;7,149.27,459.59,306.57,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,242.75,447.64,234.73,8.74;7,149.27,459.59,102.77,8.74">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,260.92,459.59,90.49,8.74">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
