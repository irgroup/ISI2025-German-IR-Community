<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,79.85,84.23,452.30,15.44;1,225.33,104.15,161.34,15.44">bigIR at TREC 2021: Adopting Transfer Learning for News Background Linking</title>
				<funder>
					<orgName type="full">Qatar Foundation</orgName>
				</funder>
				<funder ref="#_weSFvPh">
					<orgName type="full">Qatar National Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.46,134.31,68.40,10.59"><forename type="first">Marwa</forename><surname>Essam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department Qatar University Doha</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,316.82,134.31,71.72,10.59"><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
							<email>telsayed@qu.edu.qa</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department Qatar University Doha</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,79.85,84.23,452.30,15.44;1,225.33,104.15,161.34,15.44">bigIR at TREC 2021: Adopting Transfer Learning for News Background Linking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">94C98A6CDA5EB5EF2548BAB5D411760C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the participation of the bigIR team at Qatar University in the TREC 2021 news track. We participated in the background linking task. The task mainly aims to retrieve news articles that provide context and background knowledge to the reader of a specific query article. We submitted five runs for this task. In the first two, we adopted an ad-hoc retrieval approach, where the query articles were analyzed to generate search queries that were issued against the news articles collection to retrieve the required links. In the remaining runs, we adopted a transfer learning approach to rerank the articles retrieved given their usefulness to address specific subtopics related to the query articles. These subtopics were given by the track organizers as a new challenge this year. The results show that one of our runs outperformed TREC median submission, while others achieved comparable results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Motivated to help readers of online news articles to better understand its content and gain more background knowledge on its context, the news background linking task was proposed by the Text REtrieval Conference (TREC) in 2018, with follow-ups in 2019, 2020 and 2021. The task mainly requires researchers to analyze an input query news article to provide links to other news articles that the reader can read to gain more context and background knowledge on the content of the input article.</p><p>Many teams participated in this news background linking task, providing different solutions to retrieve the required background articles <ref type="bibr" coords="1,83.34,504.23,9.43,7.94" target="#b0">[1,</ref><ref type="bibr" coords="1,95.38,504.23,6.18,7.94" target="#b4">5,</ref><ref type="bibr" coords="1,104.16,504.23,6.29,7.94" target="#b6">7]</ref>. Surprisingly though, the most effective method reported to date uses the whole content of the query article as a search query in an ad-hoc setting to retrieve the background links. Aside from the fact that this method is considered inefficient, as it generates very long search queries, it is still far from being optimal; it achieved nDCG@5 of 0.46, 0.63 and 0.59 in TREC 2018, 2019, and 2020 respectively <ref type="bibr" coords="1,118.99,569.99,7.37,7.94" target="#b7">[8]</ref><ref type="bibr" coords="1,126.36,569.99,3.68,7.94" target="#b8">[9]</ref><ref type="bibr" coords="1,130.04,569.99,11.05,7.94" target="#b9">[10]</ref>, which leaves room for improvement.</p><p>In this paper, we demonstrate the participation of our bigIR team at Qatar University in the background linking task in TREC 2021. We submitted five runs for the task. One of our runs outperformed TREC median submission, while others achieved comparable results. In two runs, we adopted an ad-hoc retrieval approach, where we analyzed the query article to extract a search query that we issued against the collection of the news articles to retrieve the background links. To analyze the query articles, we adopted two keyword extraction techniques: k-Truss <ref type="bibr" coords="1,207.83,668.62,13.49,7.94" target="#b10">[11]</ref>, which is a graphbased keyword extraction algorithm that we experimented with before and showed promising results <ref type="bibr" coords="1,198.12,690.53,9.52,7.94" target="#b2">[3]</ref>, and YAKE! <ref type="bibr" coords="1,258.66,690.53,9.52,7.94" target="#b1">[2]</ref>, a statistical keyword extraction technique that was recently released and achieved effective performance in keyword extraction that outperformed many well-known baselines.</p><p>For the other three runs, we adopted a transfer learning approach to rerank the articles, retrieved using the ad-hoc based approach, given their relevance to the subtopics of the query article. Our approach mainly relied on splitting the retrieved articles into passages, rerank those passages, using pre-trained transformer models, given their relevance to the subtopics description, then use the passage scores to assign article scores to rerank the news articles. We experimented with two pre-trained BERT-based reranker models to rerank the passages: monoBERT <ref type="bibr" coords="1,433.82,313.40,9.27,7.94" target="#b5">[6]</ref>, a relevance classifier model for query-passage pairs, and DistilBERT-TAS-B <ref type="bibr" coords="1,480.63,324.36,9.52,7.94" target="#b3">[4]</ref>, a sentence transformer model that is optimized for the task of semantic search. Our results show that using the monoBERT model is better for the background linking task.</p><p>The rest of this paper is organized as follows. We provide details on the dataset released for this task and how we processed it in Section 2. Details on how we generated the main news background linking runs are discussed in Section 3. Details on how we applied transfer learning to generate the subtopic runs are given in Section 4. Results are presented in Section 5. Finally, our conclusion and future work are presented in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DATASET AND PREPROCESSING</head><p>We used version 4 of the Washington Post news collection released for the news track by TREC<ref type="foot" coords="1,414.18,478.48,3.38,6.44" target="#foot_0">1</ref> in 2021. The collection contains 728,626 news articles, published by the Washington Post, that span nine years from 2012 to 2020. Within the dataset file, the news articles are written as JSON objects. For each article, we extracted and stored the metadata (title, author, publishing date, and URL), along with the type of the article (e.g., opinions, sports, climate, politics, etc.). For the article's main content, we extracted from the JSON object only the content that is marked by a "sanitized_html" type, and ignored other descriptions of media or other embedded objects. We then used JSOUP library<ref type="foot" coords="1,408.42,577.11,3.38,6.44" target="#foot_1">2</ref> to extract the raw text from the HTML text. We stored that text of the article, split into paragraphs (as marked in the JSON object) into a mySQL database. We also used Lucene<ref type="foot" coords="1,344.69,609.98,3.38,6.44" target="#foot_2">3</ref> ver. 8.0 to create an inverted index of the collection for ad-hoc retrieval purposes. To index the articles, we concatenated all paragraphs, lower-cased the text, and removed stop words, all non-alphabetical characters, and all single character terms. The final preprocessed text was indexed as a text field in Lucene along with the article's metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AD-HOC BASED RETRIEVAL OF BACKGROUND LINKS</head><p>We used mainly an ad-hoc based retrieval approach to generate our first two submitted runs: QU_LeadPar and QU_YakeTruss. Our approach mainly relies on extracting keywords from the article that best indicate its relevant subtopics, weigh these keywords according to its influence in the article, and then generate a weighted search query out of these keywords. The query is then issued against an index of the news article collection to retrieve the required background links. More precisely, let ğ‘ be the set of extracted terms from an input query article, and assume that each term ğ‘¡ ğ‘– âˆˆ ğ‘ has a weight ğ‘¤ ğ‘¡ ğ‘– , that indicates its importance in the query article. We choose ğ‘˜ âŠ‚ ğ‘ terms with the highest weights to construct the search query ğ‘„ as follows:</p><formula xml:id="formula_0" coords="2,107.77,249.52,186.28,10.05">ğ‘„ = {(ğ‘¡ 1 , ğ‘¤ ğ‘¡ 1 ), (ğ‘¡ 2 , ğ‘¤ ğ‘¡ 2 ), ..., (ğ‘¡ ğ‘˜ , ğ‘¤ ğ‘¡ ğ‘˜ )}<label>(1)</label></formula><p>To generate our first run QU_LeadPar, we simply extracted the unique terms that appeared in the title of the query article plus its first 16 leading paragraphs, and used these as search terms, with their weight assigned as the term frequencies. We opted to extract the terms for this run from only the 16 leading paragraphs, as our experiments on the queries from last years showed that using 16 paragraphs on average is sufficient to achieve high effectiveness.</p><p>To generate the second run QU_YakeTruss, we adopted two keyword extraction methods to extract the keywords from the query article; namely k-Truss <ref type="bibr" coords="2,136.34,365.69,13.22,7.94" target="#b10">[11]</ref>, a graph-based algorithm, and YAKE! <ref type="bibr" coords="2,279.03,365.69,12.00,7.94" target="#b1">[2]</ref>, a recent statistical algorithm. We then used linear interpolation to aggregate their assigned weights to the extracted keywords to generate the final search query as follows:</p><formula xml:id="formula_1" coords="2,97.37,420.22,196.68,9.45">ğ‘Š ğ‘¡ ğ‘– = ğ›¼ * ğ‘Šğ‘‡ğ‘Ÿğ‘¢ğ‘ ğ‘  ğ‘¡ ğ‘– + (1 -ğ›¼) * ğ‘Š ğ‘Œğ´ğ¾ğ¸ ğ‘¡ ğ‘–<label>(2)</label></formula><p>where ğ‘Š ğ‘‡ ğ‘Ÿğ‘¢ğ‘ ğ‘  ğ‘¡ ğ‘– and ğ‘Š ğ‘Œğ´ğ¾ğ¸ ğ‘¡ ğ‘– are the weights assigned by the k-Truss and YAKE! methods respectively to the term ğ‘¡ ğ‘– . In our run, we set the interpolation parameter ğ‘ğ‘™ğ‘â„ğ‘ to 0.5. After interpolating the term weights from k-Truss and YAKE!, we selected the top 100 terms to generate our search query. Both methods are briefly described below along with how we used them to assign the weights to the different terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">k-Truss</head><p>k-Truss <ref type="bibr" coords="2,83.14,537.11,14.63,7.94" target="#b10">[11]</ref> is a graph decomposition technique that mainly relies on converting the text into a graph of words and analyzing this graph to reveal the most influential words/nodes. To build this graph, the unique terms in the text are added as nodes, then a sliding window goes over the text from left to right, creating edges between co-occurring terms within the window. The edges are weighted with the co-occurring frequency of the terms. k-Truss analyzes the created graph by decomposing it into a set of nested subgraphs through iteratively pruning weak edges. Precisely, k-Truss prunes an edge from the ğ‘˜-1 subgraph if it is not supported by at least ğ‘˜-2 other edges that form triangles with that edge. A node with no more connecting edges is pruned from the graph as well. Finally, each node is assigned a truss number equal to the maximum subgraph number in which it resides. A truss number can then be used as a node's weight. However, this will result in many nodes having the same weight. Therefore, in this work, we adopted the work in <ref type="bibr" coords="2,360.85,87.79,13.22,7.94" target="#b10">[11]</ref>, and assigned a weight to the node (the term) equal to the sum of truss numbers of its neighbors in the original graph of text. Figure <ref type="figure" coords="2,360.33,109.71,4.09,7.94" target="#fig_0">1</ref> shows an example of applying k-Truss decomposition.</p><p>In our run, we set the width of the sliding window to 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">YAKE!</head><p>YAKE! is a statistical keyword extraction method that was proposed recently, and outperformed many well-known keyword extraction methods <ref type="bibr" coords="2,351.22,327.36,9.33,7.94" target="#b1">[2]</ref>. It mainly aggregates the following statistical features of a term in the text to calculate its weight:</p><p>â€¢ The term frequency.</p><p>â€¢ How frequent the term is mentioned starting with a capital case letter excluding the beginning of a sentence, or marked as an acronym. â€¢ The position of the term in the document, favoring more terms that occur at the beginning of the document. â€¢ The percentage of different sentences in which the term appears, on an assumption that terms that occur in different sentences are more influential. â€¢ The term context, by capturing the number of different terms that co-occur with the term on both its left and right sides, on the assumption that the higher this number is, the less significant the term will be.</p><p>After aggregating YAKE! features, and as reported by its authors according to the aggregation equation <ref type="bibr" coords="2,458.13,507.85,11.35,7.94" target="#b1">[2]</ref>, the smaller the YAKE! value of a term is, the more significant it is. Since in our work we assign weights to terms that reflect their influence, we convert this score to a boost score by taking its reciprocal value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Background Links Retrieval</head><p>For retrieving the background links, we used Lucene's default ranking model. We used the "type" field from the retrieved articles to filter out the ones that are "Opinions", "Letters to the Editor", or "The Post's View", as they were declared by TREC to be nonrelevant. We also filtered out all articles that were published after the query article. Finally, we submitted the first 100 background articles retrieved (after removing duplicates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRANSFER LEARNING FOR NEWS BACKGROUND LINKING</head><p>The news background linking task this year involved the new submission of "subtopic" runs. In these runs, the researchers are asked Figure <ref type="figure" coords="3,107.61,286.82,3.45,7.70">2</ref>: A high level overview of the background links retrieval system used for generating the "subtopic" runs to retrieve news articles that address a specific subtopic that is either: 1) mentioned in the query article; thus the background article is supposed to give more details on the subtopic, or 2) not mentioned in the query article, but somehow related to it; thus reading about it will allow the query article's reader to gain more knowledge on its context. The description of the subtopics for each query article was given by the organizers of the track within the description of the query article topics, and participants were only allowed to use this description to generate "subtopic" runs. In our participation for the background linking task, we generated three "subtopic" runs.</p><p>In this section, we first describe our general approach to generate these runs, then we discuss each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Approach Overview</head><p>To generate the "subtopic" runs, we first used a search query extracted from the query article to retrieve an initial set of 100 candidate background links from the news articles collection. As with our first two runs, we filtered out irrelevant articles and articles that were published after the query article. We then adopted a transfer learning approach to rerank this candidate set, given the description of each subtopic. More precisely, we adopted two pretrained BERT-based reranker models to rerank the candidate background links given the subtopics description. The models we used are: monoBERT <ref type="bibr" coords="3,112.15,570.06,9.41,7.94" target="#b5">[6]</ref>, which is a relevance classifier model that was trained on the MS MARCO dataset, <ref type="foot" coords="3,184.92,578.87,3.38,6.44" target="#foot_3">4</ref> ranking document passages given how they answer an input question, and DistilBERT-TAS-B <ref type="bibr" coords="3,282.67,591.98,9.27,7.94" target="#b3">[4]</ref>, whic is also trained on MS MARCO for passage reranking, but with balanced topic-aware sampling.</p><p>Since both models were trained on reranking passages, which is of small size compared to the long news articles, we split each candidate background article into passages, and feed those passages along with the subtopic description as a query to the reranker model. This is to avoid important text truncation if the articles are fed directly along with the subtopics to the reranker model.</p><p>For simplicity, we consider each paragraph in the candidate article as a single passage, and any paragraph that is less than 20 terms in length is concatenated with its predecessor in a single passage. After reranking the passages, we assign each candidate article a score equal to the maximum relevance score assigned to any of its passages. Finally, we rerank the candidate articles given their scores. Figure <ref type="figure" coords="3,369.18,380.55,4.09,7.94">2</ref> shows an outline of the adopted retrieval approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submitted Subtopic Runs</head><p>Our submitted runs are described as follows:</p><p>â€¢ QU_SP_MBERT: In this run, we obtained initially the set of candidate background links using the 16 leading paragraph of the query article along with its title as a search query, and monoBERT was used as a reranker model. â€¢ QU_SP_MSM: In this run, we obtained initially the set of candidate background links using the 16 leading paragraph of the query article along with its title as a search query, and DistilBERT-TAS-B was used as a reranker model. â€¢ QU_SS_MSM: In this run, we obtained initially the set of candidate background links using a concatenation of all subtopics of the query article along with its title as a search query, and DistilBERT-TAS-B was used as a reranker model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OFFICIAL TREC RESULTS</head><p>Table <ref type="table" coords="3,340.35,591.90,4.25,7.94">1</ref> shows the results as provided to us by the track organizers for the main background linking task. In this task, nDCG@10 was used as the official evaluation metric this year. As can be seen, QU_leadPar outperformed the TREC median for this task. This result, as in previous years of the track, shows again the effectiveness of using almost the full article as a search query to retrieve the background links. QU_YakeTruss did not achieve similar results, eliminating the need for the extensive keyword extraction process, at least for effectiveness purposes. Table <ref type="table" coords="3,350.19,690.53,4.22,7.94" target="#tab_0">2</ref> shows the results for the subtopic runs. For these runs, the organizers reported also the Precision@10 evaluation metric. We clearly notice that the results are generally much lower than the main background linking task, indicating the challenging nature for this specific type of news background linking. It can be also noticed that using monoBERT for reranking the passages in this task is generally better than using DistilBERT-TAS-B, as its performance is comparable to the TREC median in both evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we present our bigIR group's participation in the background linking task in TREC 2021. This year, we adopted two approached to tackle the problem of news background linking. The first is ad-hoc retrieval based, which depends on the extraction of a search query from the input article to retrieve the required links. The results showed that using the most frequent items extracted from the 16 leading paragraphs of the query article achieved the best results among our submitted runs. The second approach generated subtopics runs, which mainly depends on using BERT-based reranker models to rerank the retrieved articles given their relevance to the described subtopic. Our results for those runs showed that using monoBERT as a reranker model achieved comparable results to the TREC median for this task. The results also showed that the effectiveness for those runs is generally much lower than the main background linking task, which leaves a big room for improvement in the future.</p><p>Our future work will initially focus on investigating the effect of the different parameters that we adopted in the design of our approach on the results achieved by our runs. For instance, we used the date filter to filter out articles that were published after the query article, and we believe that we might have missed relevant articles this way, specially if the query article was published early in the news collection. We will also work on addressing the subtopic-based news linking problem as it is challenging. We aim to investigate other models that best represent the candidate news articles along with the subtopics for reranking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,317.96,244.00,240.25,7.70;2,317.96,254.95,133.35,7.70;2,317.96,142.64,240.25,82.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Decomposing a graph into subgraphs. (a) original graph (b) 3-Truss decomposition.</figDesc><graphic coords="2,317.96,142.64,240.25,82.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,153.55,83.70,302.65,194.80"><head></head><label></label><figDesc></figDesc><graphic coords="3,153.55,83.70,302.65,194.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,53.50,86.13,240.55,172.70"><head>Table 2 :</head><label>2</label><figDesc>Effectiveness scores for the submitted runs for the subtopic runs of the background linking task compared to TREC'21 median</figDesc><table coords="4,53.50,86.13,240.55,138.45"><row><cell>Run</cell><cell></cell><cell>nDCG@10</cell></row><row><cell cols="3">TREC'21-Median 0.3581</cell></row><row><cell cols="2">ğ‘„ğ‘ˆ _ğ‘™ğ‘’ğ‘ğ‘‘ğ‘ƒğ‘ğ‘Ÿ</cell><cell>0.3774</cell></row><row><cell cols="2">ğ‘„ğ‘ˆ _ğ‘Œğ‘ğ‘˜ğ‘’ğ‘‡ğ‘Ÿğ‘¢ğ‘ ğ‘ </cell><cell>0.3418</cell></row><row><cell cols="3">Table 1: nDCG@10 scores for the submitted runs for the main</cell></row><row><cell cols="3">background linking task compared to TREC'21 median</cell></row><row><cell>Run</cell><cell cols="2">nDCG@10 Precision@10</cell></row><row><cell cols="2">TREC'21-Median 0.2177</cell><cell>0.1974</cell></row><row><cell>ğ‘„ğ‘ˆ _ğ‘†ğ‘ƒ_ğ‘€ğµğ¸ğ‘…ğ‘‡</cell><cell>0.2176</cell><cell>0.1956</cell></row><row><cell>ğ‘„ğ‘ˆ _ğ‘†ğ‘ƒ_ğ‘€ğ‘†ğ‘€</cell><cell>0.1983</cell><cell>0.1674</cell></row><row><cell>ğ‘„ğ‘ˆ _ğ‘†ğ‘†_ğ‘€ğ‘†ğ‘€</cell><cell>0.2019</cell><cell>0.1815</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,320.88,685.97,94.99,6.18"><p>https://trec.nist.gov/data/wapost/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,321.00,694.38,49.42,6.18"><p>https://jsoup.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,321.00,702.79,71.10,6.18"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,56.72,702.79,104.96,6.18"><p>https://microsoft.github.io/msmarco/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was made possible by NPRP grant# <rs type="grantNumber">NPRP 11S-1204-170060</rs> from the <rs type="funder">Qatar National Research Fund</rs> (a member of <rs type="funder">Qatar Foundation</rs>). The statements made herein are solely the responsibility of the authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_weSFvPh">
					<idno type="grant-number">NPRP 11S-1204-170060</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,333.39,282.47,224.81,6.18;4,333.39,290.44,225.88,6.18;4,333.39,298.36,224.81,6.23;4,333.39,306.33,94.01,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,447.72,290.44,111.55,6.18;4,333.39,298.41,117.12,6.18">Nima Saken Shaft, and Klaus Berberich. 2018. htw saar @ TREC 2018 News Track</title>
		<author>
			<persName coords=""><forename type="first">Agra</forename><surname>Bimantara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michelle</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Engelhardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Gerwert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Gottschalk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Lukosz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shenna</forename><surname>Piri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,462.84,298.36,95.36,6.23;4,333.39,306.33,90.94,6.23">Proceedings of the Twenty-Seventh Text REtrieval Conference (TREC)</title>
		<meeting>the Twenty-Seventh Text REtrieval Conference (TREC)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,314.35,225.58,6.18;4,333.39,322.32,224.81,6.18;4,333.39,330.24,182.95,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,401.11,322.32,157.09,6.18;4,333.39,330.29,62.21,6.18">YAKE! Keyword extraction from single documents using multiple local features</title>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">VÃ­tor</forename><surname>Mangaravite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arian</forename><surname>Pasquali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alipio</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">CÃ©lia</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,400.81,330.24,56.93,6.23">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">509</biblScope>
			<biblScope unit="page" from="257" to="289" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,338.26,224.81,6.18;4,333.15,346.18,225.05,6.23;4,333.39,354.15,80.71,6.23" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,458.34,338.26,99.86,6.18;4,333.15,346.23,107.30,6.18">bigIR at TREC 2019: Graph-based Analysis for News Background Linking</title>
		<author>
			<persName coords=""><forename type="first">Marwa</forename><surname>Essam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,455.00,346.18,103.20,6.23;4,333.39,354.15,77.64,6.23">Proceedings of the Twenty-Eighth Text REtrieval Conference (TREC)</title>
		<meeting>the Twenty-Eighth Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,362.17,224.81,6.18;4,333.39,370.14,224.81,6.18;4,333.18,378.06,114.45,6.23" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,378.37,370.14,179.83,6.18;4,333.18,378.11,62.76,6.18">Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>HofstÃ¤tter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,408.42,378.06,36.37,6.23">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,386.08,224.81,6.18;4,333.39,394.00,224.81,6.23;4,333.39,401.97,53.59,6.23" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,424.81,386.08,133.39,6.18;4,333.39,394.05,77.41,6.18">Leveraging Entities in Background Document Retrieval for News Articles</title>
		<author>
			<persName coords=""><forename type="first">Kuang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,422.99,394.00,135.21,6.23;4,333.39,401.97,50.52,6.23">Proceedings of the Twenty-Eighth Text REtrieval Conference (TREC)</title>
		<meeting>the Twenty-Eighth Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,409.99,225.88,6.18;4,333.39,417.91,108.33,6.23" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="4,467.87,409.99,87.86,6.18">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,425.93,224.99,6.18;4,333.39,433.85,224.81,6.23;4,333.39,441.82,53.59,6.23" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,487.22,425.93,71.16,6.18;4,333.39,433.90,74.63,6.18">TREC 2020 NEWS Track Background Linking Task</title>
		<author>
			<persName coords=""><forename type="first">Dwaipayan</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rahul</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,421.93,433.85,136.27,6.23;4,333.39,441.82,50.52,6.23">Proceedings of the Twenty-Ninth Text REtrieval Conference (TREC)</title>
		<meeting>the Twenty-Ninth Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,449.84,224.99,6.18;4,333.39,457.76,223.26,6.23" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,493.08,449.84,65.30,6.18;4,333.39,457.81,25.21,6.18">TREC 2018 News Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,370.60,457.76,183.04,6.23">Proceedings of the Twenty-Seventh Text REtrieval Conference (TREC)</title>
		<meeting>the Twenty-Seventh Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,465.78,224.99,6.18;4,333.39,473.70,223.05,6.23" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,493.08,465.78,65.30,6.18;4,333.39,473.75,25.34,6.18">TREC 2019 News Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,371.15,473.70,182.27,6.23">Proceedings of the Twenty-Eighth Text REtrieval Conference (TREC)</title>
		<meeting>the Twenty-Eighth Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,481.72,224.99,6.18;4,333.39,489.65,223.03,6.23" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="4,493.08,481.72,65.30,6.18;4,333.39,489.69,25.63,6.18">TREC 2020 News Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,371.54,489.65,181.83,6.23">Proceedings of the Twenty-Ninth Text REtrieval Conference (TREC)</title>
		<meeting>the Twenty-Ninth Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,497.66,224.81,6.18;4,333.39,505.59,224.81,6.23;4,333.39,513.56,216.46,6.23" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="4,534.90,497.66,23.30,6.18;4,333.39,505.63,146.41,6.18">A graph degeneracy-based approach to keyword extraction</title>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fragkiskos</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,492.58,505.59,65.63,6.23;4,333.39,513.56,180.70,6.23">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1860" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
