<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.17,84.23,389.65,15.44;1,171.46,104.15,269.06,15.44">University of Glasgow Terrier Team (uogTr) at the TREC 2021 Incident Streams Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.04,130.33,106.27,10.59"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Hepburn</surname></persName>
							<email>a.hepburn.1@research.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.95,130.33,93.32,10.59"><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
							<email>richard.mccreadie@glasgow.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.17,84.23,389.65,15.44;1,171.46,104.15,269.06,15.44">University of Glasgow Terrier Team (uogTr) at the TREC 2021 Incident Streams Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8EDD5D54A4ECC0A99492EA6F29CF2D7C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we detail our approach as part of the runs submitted on behalf of the University of Glasgow Terrier Team (uogTr) for the 2021-A/B edition of the Incident Streams track. Our approach employs the use of transfer learning between component labels of the dataset; more specifically, we decompose the traditional multilabel approach and investigate the relationship between each label as a binary classification task. We submit a total of three official runs to the 2021-A/B edition of the track, namely: uogTr-01-pw, uogTr-02-pwcoocc, and uogTr-04-coocc. Our results show that there exists potential for performance increase through transfer learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>On behalf of the Terrier Team (uogTr) at the University of Glasgow, we have submitted a total of three official runs to the TREC 2021 Incident Streams track. We experimented with a variety of different approaches as part of ongoing work into task relatedness in transfer learning. We approached the problem by decomposing the multilabel approach into that of separate binary classification tasks, and investigated the degree of synergy between these tasks through inductive transfer. As our base model, we utilise the pretrained, case-sensitive BERT BASE model provided by HuggingFace 1 library.</p><p>As mentioned previously, our submissions to this track were as a result of ongoing work into the investigation of the quality of task relatedness, using prior editions of the Incident Streams dataset. We performed an exhaustive search over task and hyperparameter combinations in order to determine the best-performing combinations for transfer. However, as the number of tasks and parameters increase, the number of possible combinations quickly becomes computationally infeasible to train. In order for this to be achievable for our work, we limited our scope to the Task 2 formulation from previous editions of TREC-IS, which restricted the number of tasks to 12, listed in Fig. <ref type="figure" coords="1,197.93,547.30,3.07,7.94" target="#fig_0">1</ref>.</p><p>Beginning with the BERT model, we then fine-tuned each pair of tasks in succession (i.e. BERT‚ÜíSource Task‚ÜíTarget Task). To estimate the priority of each document, we used a simple logistic regression layer in place of our classification head.</p><p>The remainder of this paper is structured as follows: Section 2 will discuss the process involved in the transformation of the multilabel ontology to separate binary tasks, the details of the dataset included, the parameters used, and our preprocessing steps; Section 3 discusses our approach as part of our work into transfer learning and task relatedness; Section 4 outlined our submitted runs for the 2021-A/B edition of the track and the differences between them in approach; Section 5 details our results compared 1 https://huggingface.co/bert-base-cased  with other participants in the track; Section 6 includes a brief, reflective discussion of our results; and finally, Section 7 includes our concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EXPERIMENTAL SETUP</head><p>Dataset. In order to compute the best pairs from the prior Task 2 formulations of the track, we initially investigated this using the 2019-A/B (17.2k documents) and 2020-A (6.6k documents) editions of the track for training and testing, respectively. For submission to the 2021 edition of the track, we add the 2020-A and 2020-B labelled datasets to our training set, giving us a total of 70.4k documents for training. We further split our training set into training (90%) and validation (10%) sets (63.4k v. 7k documents).</p><p>Finally, we test on the unlabelled 2021-A collection provided by the track, totalling 1.5M documents.</p><p>Preprocessing. We use only the tweet text from each document, capped at 280 characters as features. As we make use of the BERT BASE model, we follow much the same preprocessing steps as the original authors, wrapping each input sequence in special tokens [CLS] and [SEP] which denote the special classification and separator (end-of-input signifier) tokens, respectively. Afterwards, we perform a few additional normalisation steps; we remove URLs, Mentions (@ symbol followed by username of mentioned user), and the "RT:" prefix signifying a retweet.</p><p>Model. Using the aforementioned BERT BASE model as a starting point, we add a linear layer on top of the network for binary classification. We also use a learning rate scheduler, an AdamW optimiser <ref type="bibr" coords="1,353.74,679.57,9.27,7.94" target="#b1">[2]</ref>, and add a dropout with a rate of 30%. For the purposes of estimating the priority of each document, our classification layer is replaced by a regression head. </p><formula xml:id="formula_0" coords="2,113.04,425.91,181.01,34.14">ùë§ 0 = |ùë¶ ùë¥ | |ùë¶ 0 | , ùë§ 1 = |ùë¶ ùë¥ | |ùë¶ 1 | where |ùë¶ ùë¥ | = ùëöùëñùëõ(|ùë¶ 0 |, |ùë¶ 1 |) (1)</formula><p>Hyperparameters. We began with the pool of parameters described as optimal for BERT in the paper by Devlin et al. <ref type="bibr" coords="2,266.95,476.69,9.52,7.94" target="#b0">[1]</ref>. We found, generally, the most stable configuration for all tasks was to set the learning rate to 2e-5, the number of iterations to 2, and the batch size to 32.</p><p>However, when experimenting with a transfer learning approach, we discovered that our standard parameters yielded poor results when training in succession, which we hypothesised may be as a result of phenomena such as catastrophic forgetting. As such, we further restricted our pool of parameters to the following<ref type="foot" coords="2,263.28,562.21,3.38,6.44" target="#foot_0">2</ref> :</p><p>(1) Learning rate (Adam): 1e-5, 2e-5 (2) Number of epochs: 1, 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Our experimentation on the 12 tasks from the Task 2 formulation of the 2020-A edition consisted of two separate stages. Firstly, we experimented with hyperparameter tuning and loss function weighting for each task in isolation and secondly, we trained each possible combination of tasks and parameters by fine-tuning in succession (i.e. BERT‚ÜíSource Task‚ÜíTarget Task).</p><p>After discovering that utilising the optimal parameters identified in the first stage of our experiments resulted in poor performance, we further reduced our pool of parameters in order to mitigate the effects of excessive training, where potentially useful information may be overwritten.</p><p>Our transfer learning experiments yielded some interesting results, shown in Table <ref type="table" coords="2,397.28,437.06,3.07,7.94" target="#tab_1">1</ref>. The categories InformationWanted, SearchAndRescue, GoodsServices, and Volunteer showed no observable change in performance through inductive transfer regardless of source information type used. For Emerging Threats and Multimedia Share, we observe an increase in performance of around 8% and 6% respectively, which is a modest increase as these types were already some of the better performing. However, we also observe a staggering performance increase of 191% for First Party Observation. This shows that, for some information types, there is clear scope for improving performance via transfer learning techniques. Indeed, we see an improvement in Information Type Actionable F1 in 75% of the tasks in the table. Overall, we observed a 12.4% increase across 8 tasks using transfer learning.</p><p>We then decided to evaluate our approach for the 2021 edition of the track, adding the 2020-A/B sets to our training set. Since we did not have the available information of best-performing task pairs for the remaining 13 tasks, we determined the suitability of task pairs under the assumption that, for each target task, its topranked co-occurring label (in our training set) must exhibit some degree of semantic similarity and may, by extension, be synergistic in training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC-IS 2021-A Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SUBMITTED RUNS</head><p>We submitted three separate runs, evaluating all of our individual classifiers as a cohesive system. For each of these runs, we estimated the priority of each document using a logistic regression head in place of our classification layer.</p><p>(1) uogTr-01-pw: For our first run, we use the parameters which exhibited the highest performance when the 12 tasks were tuned in isolation. For the remaining 13 tasks, we set the learning rate to 2e-5, used 2 training iterations, and a batch size of 32, which generally exhibited the most stable performance across all tasks. (2) uogTr-02-pwcoocc: For our second run, we use the bestperforming, combined models from Table <ref type="table" coords="3,233.93,456.11,4.22,7.94" target="#tab_1">1</ref> for each of the eight target tasks (omitting the four which showed no performance increase from transfer). For the remaining 17 tasks, we used the single-task models with the "standard" hyperparameters of 2e-5, 2, 32 for the learning rate, number of epochs, and batch size, respectively. (3) uogTr-04-coocc: For our third run, we use transfer learning on all tasks, using the models from Table <ref type="table" coords="3,235.82,532.82,4.25,7.94" target="#tab_1">1</ref> and using the top-ranked co-occurrences for the remaining 13. Taking into account the potential impact on performance from excessive training, we reduce our parameters to 1e-5, 2, 32 for the learning rate, number of training epochs, and batch size, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Table <ref type="table" coords="3,75.63,624.78,4.18,7.94" target="#tab_2">2</ref> reports the performance of our submitted runs in comparison to the TREC Best and Median systems. We abbreviate each track metric as follows: "ITAct" means Information Type Positive F1-score (Actionable), "ITAll" means Information Type Positive F1score (All), "ITAcc" means Information Type Accuracy, "PF1Act" means Priority F1-score (Actionable), "PF1Act" means Priority F1score (All), "PRAct" means Priority R (Actionable), and "PRAll" means Priority R (All).</p><p>When comparing the Information Type F1 Actionable scores between uogTr-01-pw and both uogTr-02-pwcoocc and uogTr-04-coocc, that is to say, the comparison between our system which contained no element of transfer learning and systems that did, we can see a jump of 68% and 83% in performance when using the transfer learning models for the actionable tasks. When we employed transfer learning across all tasks, we see no observable, significant change in the Information Type F1 All metric, indicating the co-occurrence may not necessarily be a good indicator of task relatedness, at least in the context of semantic similarity.</p><p>For priority-centric metrics, we can see that our NDCG@100 had roughly 30% worse performance when compared with the median of participants' results. For both the sets of Priority F1 and Priority R scores, we performed worse still. This was somewhat expected as we did not optimise for priority in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Comparison with other systems. Unfortunately, the performance gains from our ongoing work were not as strongly reflected in our notebook submission. However, our submitted systems achieved around the median for Information Type F1 All and Information Type Accuracy but fell short in obtaining adequate results for the Information Type F1 Actionable. It is clear that, despite the potential gains from using transfer learning techniques, the necessary conditions to reproduce these effects are difficult to predict prior to training. Our hypothesis that co-occurrent labels are likely to be somewhat synergistic in training did not prove to be correct. Reflections on method. It is evident that what constitutes task synergy (for transfer) for the 2021 edition are more complex than initially expected. We believe that this method can show promising results for low-resource labels but more investigation must be carried out to better estimate the suitability of pairs of tasks and parameters prior to training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we proposed a method of classifying crisis and disaster documents by decomposing the multi-label problem into separate binary tasks and carrying out transfer learning between them. Further investigation into the quality of task relatedness and how to exploit the shared information between related tasks is something we have left for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,317.96,187.98,241.85,7.70;1,317.96,198.94,39.68,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TREC-IS Information Type Ontology, Task 2 Formulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,53.47,85.75,506.27,326.37"><head>Table 1 :</head><label>1</label><figDesc>After observing poor performance on tasks which had a significantly lower number training examples, we weighted our cross-entropy loss function, optimising for the minority class as follows:</figDesc><table coords="2,273.38,85.75,276.58,7.42"><row><cell>Inductive Transfer (Source)</cell><cell>Target Parameters</cell><cell>Evaluation Scores</cell></row></table><note coords="2,88.46,324.01,469.74,7.70;2,53.80,334.97,505.94,7.70;2,54.07,345.22,301.58,8.58;2,63.76,371.53,100.15,7.70"><p>Information type categorisation performance with and without inductive transfer from a source task. Metrics are micro-averaged across events and range from 0 to 1, higher is better. Statistical significance is measured with McNemar's test. ‚ñ≤ and ‚ñº denote significant performance increases and decreases at ùëù ‚â§ 0.05 Loss function weighting.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,123.84,97.26,364.33,188.25"><head>Table 2 :</head><label>2</label><figDesc>TREC-IS Performance on the 2021-A/B events</figDesc><table coords="3,123.84,97.26,364.33,175.91"><row><cell></cell><cell>Ranking</cell><cell>Type Categorization</cell><cell>Priority</cell></row><row><cell>ID</cell><cell cols="3">NDCG@100 ITAct ITAll ITAcc PF1Act PF1All PRAct PRAll</cell></row><row><cell>TREC Best</cell><cell>0.6115</cell><cell cols="2">0.2815 0.3211 0.8902 0.3060 0.3211 0.4349 0.3585</cell></row><row><cell>TREC Median</cell><cell>0.5695</cell><cell cols="2">0.2060 0.2823 0.8827 0.2113 0.2175 0.1728 0.2099</cell></row><row><cell>uogTr-01-pw</cell><cell>0.3965</cell><cell cols="2">0.0983 0.2062 0.8813 0.0301 0.0810 0.0879 0.0654</cell></row><row><cell>uogTr-02-pwcoocc</cell><cell>0.3967</cell><cell cols="2">0.1657 0.2924 0.8827 0.0301 0.0810 0.0879 0.0654</cell></row><row><cell>uogTr-04-coocc</cell><cell>0.3953</cell><cell cols="2">0.1657 0.2889 0.8842 0.0301 0.0810 0.0879 0.0654</cell></row><row><cell></cell><cell></cell><cell>TREC-IS 2021-B Results</cell><cell></cell></row><row><cell>Run</cell><cell>Ranking</cell><cell>Type Categorization</cell><cell>Priority</cell></row><row><cell>ID</cell><cell cols="3">NDCG@100 ITAct ITAll ITAcc PF1Act PF1All PRAct PRAll</cell></row><row><cell>TREC Best</cell><cell>0.4791</cell><cell cols="2">0.2510 0.2623 0.9067 0.2798 0.2756 0.2302 0.2952</cell></row><row><cell>TREC Median</cell><cell>0.4272</cell><cell cols="2">0.1842 0.2330 0.8947 0.2107 0.2031 0.1495 0.1993</cell></row><row><cell>uogTr-01-pw</cell><cell>0.2928</cell><cell cols="2">0.0731 0.1532 0.8916 0.0727 0.1223 0.1375 0.1096</cell></row><row><cell>uogTr-02-pwcoocc</cell><cell>0.2945</cell><cell cols="2">0.1340 0.2280 0.8982 0.0727 0.1223 0.1375 0.1096</cell></row><row><cell>uogTr-04-coocc</cell><cell>0.2934</cell><cell cols="2">0.1340 0.2284 0.8977 0.0727 0.1223 0.1375 0.1096</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,60.33,694.57,233.71,6.18;2,53.80,702.79,18.10,6.18"><p>After observing negligible effects on changing the batch size, we maintained a batch size of</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_1" coords="2,81.87,702.79,40.44,6.18"><p>for all models.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We would like to thank the organises of the <rs type="institution">TREC Incident Streams track</rs> for providing the datasets, evaluation metrics, and fostering a community for information retrieval in crises.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="4,330.15,156.59,228.87,6.18;4,330.15,164.56,228.05,6.18;4,330.15,172.48,23.16,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,541.38,156.59,17.64,6.18;4,330.15,164.56,217.40,6.18">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,330.15,172.48,19.30,6.23">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,330.15,180.50,229.13,6.18;4,330.15,188.42,22.87,6.23" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,443.54,180.50,112.90,6.18">Decoupled Weight Decay Regularization</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
