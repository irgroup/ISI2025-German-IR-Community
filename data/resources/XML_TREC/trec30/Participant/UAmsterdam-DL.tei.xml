<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.59,114.89,416.82,15.15">Recall Aspects of Transformers for Text Ranking</title>
				<funder ref="#_Q2vHehC">
					<orgName type="full">Facebook Research (Computationally Efficient NLP</orgName>
				</funder>
				<funder ref="#_Qpv498T">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO STW</orgName>
				</funder>
				<funder ref="#_wT5YvKd">
					<orgName type="full">Innovation Exchange Amsterdam</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.29,148.84,55.27,10.48"><forename type="first">David</forename><surname>Rau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.75,148.84,63.95,10.48"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.59,114.89,416.82,15.15">Recall Aspects of Transformers for Text Ranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5DA3487E4CEB54E5321888E104641967</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper documents the University of Amsterdam's participation in the TREC 2021 Deep Learning Track. In addition to providing labeled training data at scale, the other major contribution of the TREC DL track is to avoid the pool-bias exhibited in all the earlier adhoc search test collections created through the pooling only runs from traditional sparse retrieval systems.</p><p>However, even in the TREC deep learning track, we have shallow pools, and runs with varying but high fractions of unjudged documents. This prompts a deeper analysis of pool coverage over ranks for both a representative traditional approach (i.e., BM25) and a representative neural approach (i.e., the BERT cross-encoder for the passage retrieval task). Our main conclusions are the following. First, we submitted a neural run that specifically looks beyond those documents easily found by traditional models, highlighting the potential of neural models to address recall aspects in addition to the precision aspects prioritized in the TREC Deep Learning Track up to now. Second, we observe high fractions of unjudged documents after the initial ranks for both the 2020 and 2021 data, which may hinder the evaluation of recall-oriented aspects and reusability of the judgments for runs not contributing to the pooling. Third, we observe a gradual decline of the fraction of relevant over judged documents for 2020, which is a positive sign against pooling bias, but almost no decrease for 2021. Our general conclusion is that coverage below the guaranteed pooling horizon is far from complete and that analysis of recall aspects must be done with care, but that there is great potential to study these in future editions of the track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper documents our participation in the TREC 2021 Deep Learning Track <ref type="bibr" coords="1,465.18,506.39,74.82,9.57;1,72.00,519.94,23.23,9.57" target="#b2">[Craswell et al., 2022]</ref>. The Deep Learning Track started at TREC 2019 <ref type="bibr" coords="1,353.25,519.94,100.96,9.57" target="#b0">[Craswell et al., 2020</ref><ref type="bibr" coords="1,463.84,519.94,23.23,9.57" target="#b1">[Craswell et al., , 2021]]</ref>, and is in it's third year. The TREC Deep Learning Track is providing very large training data for its task, which is a necessity for training deep neural networks. The track consists of four tasks in total: two tasks (Passage retrieval or Document retrieval) for two collections (Re-Rank or Full-Rank). The Re-Ranking task focuses on ranking only the top 100 candidates from the collection for each query. We focused on the collection of passages and performed experiments for both tasks. TREC provides training, evaluation and test queries, the collection of passages and training triplets that consist of a query, a couple of candidates and an indicator that defines the most relevant passage among the two candidates.</p><p>This paper is structured in the following way. Our simple experiment is described in Section 2 and the results of these experiments in Section 3. Finally, we end in Section 4 with a discussion of our main findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Design</head><p>In this section we detail our simple pooling experiments.</p><p>In addition to providing labeled training data at scale, the other major contribution of the TREC DL track is to avoid the pool-bias exhibited in all the earlier adhoc search test collections created through the pooling only runs from traditional sparse retrieval systems. However, even in the TREC deep learning track, we have shallow pools and runs with varying but high fractions of unjudged documents.</p><p>Although all official submissions to the TREC deep learning track contribute to the pool, only the top 10 of the run is guaranteed to be judged by the assessors, and the remaining pool is determined incrementally by an active learning approach based on the already judged passages or documents up to this point. This leads to varying, and sometimes high, fractions of unjudged documents over official submissions, and even high fractions of unjudged documents in the top results of post-submission experiments with runs not contributing to the pool. So far this is based on anecdotal evidence from the analysis of recall aspects of various models [e.g., our own earlier work, <ref type="bibr" coords="2,101.73,276.00,77.31,9.57" target="#b3">Rau et al., 2020]</ref>.</p><p>This prompts a deeper analysis of pool coverage over ranks for both a representative traditional approach (i.e., BM25) and a representative neural approach (i.e., the BERT cross-encoder for the passage retrieval task).</p><p>BM25 BM25 is the prototypical traditional system. As in 2021 the reranking task only provides a top 100 rerank set based on BM25, we reconstruct the BM25 run using the Anserini setup. Specifically, we use no stemming, standard stopword removal, and don't optimize the BM25 parameters. This results in marginally lower scores than an optimized setup, and marginally lower scores than a feedback variant with RM3 [e.g., <ref type="bibr" coords="2,353.58,393.36,98.17,9.57" target="#b1">Craswell et al., 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CE</head><p>The BERT cross-encoder is a prototypical application of large pre-trained transformers, in particular BERT, to the passage ranking task by directly encoding a query+passage and training class label that informs the ranking task. Specifically, we use the pretrained BERT and train the CLS token in an additional layer using the MS-Marco training data. This model has shown significant and clear improvements over traditional systems in the track <ref type="bibr" coords="2,496.57,470.07,43.42,9.57;2,99.27,483.62,50.79,9.57" target="#b0">[Craswell et al., 2020</ref><ref type="bibr" coords="2,158.67,483.62,23.23,9.57" target="#b1">[Craswell et al., , 2021]]</ref>.</p><p>As our main interest is to do analysis that may generalize to other settings, we strictly prefer to work with a setup that is as clean and simple as possible, without optimizing the settings to specific setup and conditions of the TREC track.</p><p>Recall that only the top 10 is guaranteed to be pooled, so in order see beyond this shallow pooling horizon, we are particularly interested in what happens at a lower rank (say, beyond the top 100). We specifically manipulate a submission to contain only results that are outside the top 100 of the traditional BM25 run (corresponding to the rerank task in 2021). In this way, we hope to increase pool diversity, and also obtain a comparable sample of judged documents from lower in the ranking: what is the quality of the neural ranker beyond the initial precision?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>This section contains the main results of our experiments, focusing on the analysis of pool coverage over standard sparse retrieval systems and standard neural rankers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effectiveness</head><p>As our main goal is the analysis of pool coverage, we opt to study two runs in detail. First, a standard BM25 runs as used as an oracle for the rerank sets in the passage retrieval task in 2020 (top 1,000 passages) and 2021 (top 100 passages). As the 2021 rerank set contains fewer documents, we use similar top 1,000 runs in order to compare both years. Second, a standard BERT / crossencoder model, that is a clean and simple model that incorporates the effect of using large language models (in particular pretrained BERT) in a passage retrieval setting. Tables <ref type="table" coords="3,124.62,391.00,5.45,9.57" target="#tab_0">1</ref> and<ref type="table" coords="3,157.69,391.00,5.45,9.57" target="#tab_1">2</ref> show the effectiveness of these two models. In this task setting, the crossencoder clearly outperforms the traditional BM25 system on the main measure NDCG@10, as was observed throughout the years <ref type="bibr" coords="3,221.87,418.10,98.72,9.57" target="#b0">[Craswell et al., 2020</ref><ref type="bibr" coords="3,329.49,418.10,23.23,9.57" target="#b1">[Craswell et al., , 2021]]</ref>. While the BM25 run has the highest initial precision (with an MRR of 83-85% versus 75% for the neural model), the neural model clearly outperforms on NDCG@10, also reflecting the superiority of dense models to correctly rank passages with the highest "Perfect" assessment. It is worthy to note that this years cross-encoder scores are significantly lower than last year. This can potentially be explained by the significant increase of the collection containing orders of magnitude more documents this year, and the use of a smaller "rerank" set of 100 passages leading to more diversity in the submissions.</p><p>Our only official submission was a cross-encoder run which left out any document already in the top 100 rerank set of the passage retrieval task. As expected removing the top-100 documents (see Table <ref type="table" coords="3,125.55,540.82,5.73,9.09" target="#tab_0">1</ref> w/o official top-100) re-rank set decreases the AP scores significantly, as we're focusing exclusive on the harder to find relevant documents (ignoring all that BM25 can pick up with ease). Looking at the top of the manipulated run, we see only a small decrease in initial precision (NDCG@10 decreases from 60% to 51% and P@10 from 50% to 43%).</p><p>This manipulated run intended to shed some new light on what's happening in the neural ranking beyond the shallow pooling depth used in the TREC Deep Learning track, motivated by earlier observations that relatively high fractions of unjudged documents remain, even in the rerank setup of the passage retrieval task. While this filtered run leaving out the "easy" relevant documents takes an obvious hit in performance, and isn't a contender for the top of the table in terms of performance, it still obtains very reasonable precision scores. This highlights the potential of neural models to address recall-aspects in addition to the precision aspects prioritized in the TREC Deep Learning Track setup up to now. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis</head><p>In this section, we analyse recall-oriented aspects: what is happening beyond the pooling depth?</p><p>We focus our analysis on two prototypical runs: BM25 to represent a traditional sparse retrieval model, and the BERT cross-encoder to represent an effective neural dense retrieval model relying on large language models such as BERT.</p><p>To provide context, we first give some details about the judgments in 2020 and 2021. The distribution of judgments is shown in Table <ref type="table" coords="4,283.64,461.19,4.24,9.57" target="#tab_2">3</ref>. We see that in 2020, 68% of the judged passages is non-relevant, and hence 32% of the judged passages has some relevance ('Related' or higher), and 15% of the passages is assessed as 'Highly Relevant' or 'Perfect'. For 2021, the fraction of nonrelevant passages is much lower with 40%, and hence 60% of the judged passages has some relevance, whereas 32% of the judgements is 'Highly Relevant' or 'Perfect'. The high fraction of relevance in the pool may be a result of incompleteness of the recall base, as only relatively shallow guaranteed pooling depth was used, which would lead to underestimation of the performance of systems at higher ranks. So what is happening beyond the top of the ranking for a typical traditional and a typical neural ranker?</p><p>Table <ref type="table" coords="4,118.64,583.13,5.45,9.57">4</ref> shows a breakdown of judged passages over the ranks for the 2020 qrels. Here, relevant means any degree of relevance (including 'Related'), whereas the evaluation scores use only the 'Highly Relevant' and 'Perfect' judgments. In 2020, the rerank task was based on the BM25 top 1,000 passages provided, with full-rank systems obtaining very similar recall as rerank submissions <ref type="bibr" coords="4,72.00,637.33,100.57,9.57" target="#b1">[Craswell et al., 2021]</ref>. In terms of pooling: the organizers pooled the top 10 of all submitted runs, and used active learning (HiCal BMI) for incrementally expanding the pool based on the relevant and non-relevant passages up to this point. A total of 54 topics with at least three known relevant and a ratio of relevant over judged below 40 percent form the 2020 test collection.</p><p>BM25 and cross-encoder runs were submitted by some teams in 2020, as runs p bm25 and nlm-bert-rr respectively in <ref type="bibr" coords="4,212.82,705.07,103.16,9.57" target="#b1">[Craswell et al., 2021]</ref>, hence contributed to the pool but with only their top 10's being guaranteed to be included. Considering BM25 we see a very low percentage of unjudged documents within the first 10 ranks (3.3%@10). However, when looking at lower ranks the percentage of unjudged increases dramatically from 46.32% @50 up to 90.01% @1,000. For CE we observe a similar trend: Up to rank 10 very few documents are unjudged (2.78%), whereas later in the 25.58% @50 up to 80.67% @1,000 are unjudged. Those numbers show that the judgments of the neural model exhibit higher coverage compared to the traditional model in 2020.</p><p>With so many unjudged documents, are we seriously underestimating the performance? We also look at the fraction of Relevant to Judged documents for both types of runs, and observe that fraction goes down over ranks suggesting that the pool is not unfavorably biased against the runs. We also observe that the neural model is superior compared to the traditional model, although at the end of the ranking both sets converge due to both reranking the same top 1,000 of passages.</p><p>Table <ref type="table" coords="5,119.06,419.08,5.45,9.57" target="#tab_3">5</ref> replicates this for the 2021 judgments. Recall that in 2021, the rerank task was based on only the top 100 based on a BM-25 run, likely leading to far greater run diversity. To do a comparable analysis, we reconstructed a top 1,000 rerank set (see Table 2 BM25 (own)). In terms of pooling, again the organizers automatically pooled the top 10 of all submitted runs, and use of active learning (CAL) for incrementally expanding the pool based on the relevant and non-relevant passages up to this point. A total of 53 topics with at least five known relevant documents, combined with sufficient judgements and low enough generality, form the 2021 test collection. Unlike 2020, the organizers already issue a warning in 2021 that even some official submissions, with guaranteed top 10 pools, have high fractions of unjudged documents below this shallow pooling cut-off.</p><p>Judging by the pool coverage, BM25 and cross-encoder runs were submitted by some teams in 2021 <ref type="bibr" coords="5,97.44,554.57,101.14,9.57" target="#b2">[Craswell et al., 2022]</ref>. We only submitted a manipulated version of the CE run that left out documents from the official top 100 BM25 rerank set, in order to increase pool diversity and to aid analysis of the effectiveness of the neural ranker beyond the pooling horizon.</p><p>Considering BM25 we see, similarly to 2020, a very low percentage of unjudged documents within the first 10 ranks (9.06% @10), with the percentage of unjudged increasing from 40.98% @50 to 90.49% @1,000. For CE we observe a similar trend: Up to rank 10 the percentage of unjudged is zero as the top-10 contributed to the pool. However, later in the ranking 53.58% @50 up to 86.48% @1,000 are unjudged. So similar to 2020, we observed very high fractions of unjudged documents deeper in the rankings. We also look again at the fraction of relevant over judged documents for both runs. Unlike 2020, we observe in 2021 an almost non-decreasing fraction of relevant documents over the ranks, for both BM25 and the neural model. The BM25 fractions marginally drop from 77% to 69%, and the neural model from 75% to 72%. This could be a signal that the pools are just too shallow to cover a representative (and unbiased) sample of the relevant documents.</p><p>We also look at the pool coverage of our manipulated neural run looking beyond the rerank set's BM25 top 100 (not displayed in the Table ). When comparing the runs of 2020 and 2021, we observe that our manipulated run contains twofold more unjudged documents than the regular neural model in Table <ref type="table" coords="6,180.51,143.25,4.24,9.57" target="#tab_3">5</ref>, suggesting that documents that are brought up from lower ranks are not covered well by the current form of pooling.</p><p>The pool incompleteness is a necessary consequence of finite judging budgets, and mostly a call to caution when interpreting the results of the evaluation. Our analysis signals that pool coverage below the guaranteed pooling horizon is far from complete, and that analysis of recall aspects and runs not directly contributing to the pools, must be done with care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper documented our participation in the TREC 2021 Deep Learning Track. We conducted a simple experiment in quantifying pool coverage. Our main conclusions are the following. First, we submitted a neural run that specifically looks beyond those documents easily found by traditional models, highlighting the potential of neural models to address recall aspects in addition to the precision aspects prioritized in the TREC Deep Learning Track up to now. Second, we observe high fractions of unjudged documents after the initial ranks for both the 2020 and 2021 data, which may hinder the evaluation of recall-oriented aspects and reusability of the judgments for runs not contributing to the pooling. Third, we observe a gradual decline of the fraction of relevant over judged documents for 2020, which is a positive sign against pooling bias, but almost no decrease for 2021. Our general conclusion is that coverage below the guaranteed pooling horizon is far from complete and that analysis of recall aspects must be done with care, but that there is great potential to study these in future editions of the track. At the time of writing, the systems contributing to the pools have not been released, and more advanced reusability analysis will be part of future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,77.98,84.00,433.20,60.61"><head>Table 1 :</head><label>1</label><figDesc>Performance on the NIST judgments of the TREC Deep Learning Task 2020</figDesc><table coords="3,77.98,102.47,423.76,42.14"><row><cell>Ranker</cell><cell>NDCG@10</cell><cell>MAP</cell><cell>MRR</cell><cell>P@10</cell></row><row><cell>BM25</cell><cell>49.59</cell><cell>27.47</cell><cell>67.06</cell><cell>37.96</cell></row><row><cell>Cross-encoder</cell><cell>68.95</cell><cell>45.96</cell><cell>79.88</cell><cell>50.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,77.98,173.26,433.20,87.71"><head>Table 2 :</head><label>2</label><figDesc>Performance on the NIST judgments of the TREC Deep Learning Task 2021</figDesc><table coords="3,77.98,191.73,423.76,69.24"><row><cell>Ranker</cell><cell>NDCG@10</cell><cell>MAP</cell><cell>MRR</cell><cell>P@10</cell></row><row><cell>BM25 (official provided)</cell><cell>31.60</cell><cell>21.22</cell><cell>84.53</cell><cell>35.47</cell></row><row><cell>BM25 (own)</cell><cell>30.58</cell><cell>18.43</cell><cell>82.99</cell><cell>33.96</cell></row><row><cell>Cross-encoder (official top-100)</cell><cell>59.98</cell><cell>18.83</cell><cell>75.23</cell><cell>50.19</cell></row><row><cell cols="2">Cross-encoder (w/o official top-100) 51.04</cell><cell>12.17</cell><cell>65.66</cell><cell>43.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,77.98,84.00,445.29,260.72"><head>Table 3 :</head><label>3</label><figDesc>Distribution of passage judgments(TREC Deep Learning Track 2020 and<ref type="bibr" coords="4,492.75,84.00,26.06,9.57" target="#b1">2021)</ref> </figDesc><table coords="4,77.98,102.47,445.29,242.25"><row><cell></cell><cell></cell><cell></cell><cell>Fraction</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CCDF</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell>≥ 0</cell><cell>≥ 1</cell><cell>≥ 2</cell><cell>≥ 3</cell></row><row><cell>2020</cell><cell>68.33</cell><cell>17.04</cell><cell>8.96</cell><cell>5.67</cell><cell></cell><cell>100.00</cell><cell>31.67</cell><cell>14.63</cell><cell>5.67</cell></row><row><cell>2021</cell><cell>40.06</cell><cell>28.29</cell><cell>21.62</cell><cell cols="2">10.03</cell><cell>100.00</cell><cell>59.94</cell><cell>31.65</cell><cell>10.03</cell></row><row><cell></cell><cell cols="8">Table 4: Judged passages across ranks (TREC Deep Learning Track 2020)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>500</cell><cell>1,000</cell></row><row><cell cols="2">BM25 Relevant (%)</cell><cell></cell><cell>77.78</cell><cell>65.19</cell><cell>56.67</cell><cell>31.26</cell><cell>21.52</cell><cell>6.91</cell><cell>3.92</cell></row><row><cell></cell><cell cols="3">Non-relevant (%) 22.22</cell><cell>34.44</cell><cell>40.00</cell><cell>36.22</cell><cell>27.06</cell><cell>10.13</cell><cell>6.07</cell></row><row><cell></cell><cell cols="2">Unjudged (%)</cell><cell>0.00</cell><cell>0.37</cell><cell>3.33</cell><cell>32.52</cell><cell>51.43</cell><cell>82.96</cell><cell>90.01</cell></row><row><cell></cell><cell cols="3">Rel./Judged (%) 77.78</cell><cell>65.43</cell><cell>58.62</cell><cell>46.32</cell><cell>44.30</cell><cell>40.58</cell><cell>39.23</cell></row><row><cell>CE</cell><cell>Relevant (%)</cell><cell></cell><cell>87.04</cell><cell>80.37</cell><cell>71.67</cell><cell>42.93</cell><cell>29.04</cell><cell>7.63</cell><cell>3.90</cell></row><row><cell></cell><cell cols="3">Non-relevant (%) 12.96</cell><cell>18.89</cell><cell>25.56</cell><cell>30.41</cell><cell>25.65</cell><cell>10.80</cell><cell>6.45</cell></row><row><cell></cell><cell cols="2">Unjudged (%)</cell><cell>0.00</cell><cell>0.74</cell><cell>2.78</cell><cell>25.85</cell><cell>43.98</cell><cell>74.73</cell><cell>80.67</cell></row><row><cell></cell><cell cols="3">Rel./Judged (%) 87.04</cell><cell>80.97</cell><cell>73.71</cell><cell>58.54</cell><cell>53.10</cell><cell>41.41</cell><cell>37.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,77.98,84.00,445.29,157.90"><head>Table 5 :</head><label>5</label><figDesc>Judged passages across ranks(TREC Deep Learning Track 2021)    </figDesc><table coords="5,77.98,102.47,445.29,139.43"><row><cell></cell><cell></cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>500</cell><cell>1,000</cell></row><row><cell cols="2">BM25 Relevant (%)</cell><cell>75.47</cell><cell>69.81</cell><cell>66.04</cell><cell>40.34</cell><cell>29.81</cell><cell>10.94</cell><cell>6.57</cell></row><row><cell></cell><cell cols="2">Non-relevant (%) 22.64</cell><cell>26.04</cell><cell>24.91</cell><cell>18.68</cell><cell>13.94</cell><cell>4.90</cell><cell>2.95</cell></row><row><cell></cell><cell>Unjudged (%)</cell><cell>1.89</cell><cell>4.15</cell><cell>9.06</cell><cell>40.98</cell><cell>56.25</cell><cell>84.16</cell><cell>90.49</cell></row><row><cell></cell><cell cols="2">Rel./Judged (%) 76.92</cell><cell>72.83</cell><cell>72.61</cell><cell>68.35</cell><cell>68.13</cell><cell>69.08</cell><cell>69.03</cell></row><row><cell>CE</cell><cell>Relevant (%)</cell><cell>75.47</cell><cell>73.58</cell><cell>70.57</cell><cell>34.49</cell><cell>22.57</cell><cell>6.61</cell><cell>3.43</cell></row><row><cell></cell><cell cols="2">Non-relevant (%) 24.53</cell><cell>26.42</cell><cell>29.43</cell><cell>11.92</cell><cell>7.66</cell><cell>2.41</cell><cell>1.33</cell></row><row><cell></cell><cell>Unjudged (%)</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>53.58</cell><cell>69.77</cell><cell>90.98</cell><cell>86.48</cell></row><row><cell></cell><cell cols="2">Rel./Judged (%) 75.47</cell><cell>73.58</cell><cell>70.57</cell><cell>74.31</cell><cell>74.66</cell><cell>73.26</cell><cell>72.10</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments We thank the track organizers for their amazing service and effort in making realistic benchmarks for neural ranking available. This research is funded in part by the <rs type="funder">Netherlands Organization for Scientific Research (NWO STW</rs> # <rs type="grantNumber">17752</rs>; <rs type="grantNumber">NWO CI # CISC.CC.016</rs>), <rs type="funder">Facebook Research (Computationally Efficient NLP</rs> grant), and the <rs type="funder">Innovation Exchange Amsterdam</rs> (<rs type="grantName">POC grant</rs>). Views expressed in this paper are not necessarily shared or endorsed by those funding the research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Qpv498T">
					<idno type="grant-number">17752</idno>
				</org>
				<org type="funding" xml:id="_Q2vHehC">
					<idno type="grant-number">NWO CI # CISC.CC.016</idno>
				</org>
				<org type="funding" xml:id="_wT5YvKd">
					<orgName type="grant-name">POC grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,72.00,577.36,468.00,8.74;6,82.91,589.32,457.09,8.74;6,82.91,601.27,136.02,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,387.06,577.36,152.94,8.74;6,82.91,589.32,60.64,8.74">Overview of the TREC 2019 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,171.09,589.32,335.54,8.74">TREC 2019: Proceedings of the Twenty-Eighth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,613.23,468.00,8.74;6,82.91,625.18,457.09,8.74;6,82.91,637.14,22.69,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,305.60,613.23,214.65,8.74">Overview of the TREC 2020 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,82.91,625.18,314.42,8.74">TREC 2020: Proceedings of the Twenty-Ninth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1266</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,649.09,468.00,8.74;6,82.91,661.05,457.09,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,327.33,649.09,208.52,8.74">Overview of the TREC 2021 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,95.32,661.05,297.00,8.74">TREC 2021: Proceedings of the Thirtieth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,673.01,468.00,8.74;6,82.91,684.96,457.09,8.74;6,82.91,696.92,96.30,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,248.41,673.01,272.62,8.74">University of Amsterdam at TREC 2020: Deep learning track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kondylidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,82.91,684.96,302.36,8.74">The Twenty-Ninth Text REtrieval Conference Notebook (TREC 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
