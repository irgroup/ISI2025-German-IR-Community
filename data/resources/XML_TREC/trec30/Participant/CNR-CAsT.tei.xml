<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,181.14,116.95,253.07,12.62;1,172.25,134.89,270.85,12.62;1,182.47,154.84,250.41,10.86">Finding Context through Utterance Dependencies in Search Conversations Participation of the CNR Team in CAsT 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.48,190.98,38.19,8.74"><forename type="first">Ida</forename><surname>Mele</surname></persName>
							<email>ida.mele@iasi.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="institution">IASI-CNR</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.22,190.98,104.42,8.74"><forename type="first">Cristina</forename><forename type="middle">Ioana</forename><surname>Muntean</surname></persName>
							<email>cristina.muntean@isti.cnr.it</email>
							<affiliation key="aff1">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.49,202.94,94.87,8.74"><forename type="first">Franco</forename><forename type="middle">Maria</forename><surname>Nardini</surname></persName>
							<email>francomaria.nardini@isti.cnr.it</email>
							<affiliation key="aff1">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.91,202.94,67.28,8.74"><forename type="first">Raffaele</forename><surname>Perego</surname></persName>
							<email>raffaele.perego@isti.cnr.it</email>
							<affiliation key="aff1">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.13,202.94,75.27,8.74"><forename type="first">Nicola</forename><surname>Tonellotto</surname></persName>
							<email>nicola.tonellotto@unipi.it</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,181.14,116.95,253.07,12.62;1,172.25,134.89,270.85,12.62;1,182.47,154.84,250.41,10.86">Finding Context through Utterance Dependencies in Search Conversations Participation of the CNR Team in CAsT 2021</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0B10CF57ED6F7D39FF740299D7E11099</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To help research on Conversational Information Seeking, TREC has organized a competition on conversational assistant systems, called Conversational Assistant Track (CAsT). It provides test collections for open-domain conversational search systems. For our participation in CAsT 2021, we implemented a three-step architecture consisting of: (i) automatic utterance rewriting, (ii) firststage retrieval of candidate passages, and (iii) neural re-ranking of candidate passages.</p><p>Each run is based on a different utterance rewriting technique for enriching the raw utterance with context extracted from the previous utterances and/or replies in the conversation. Two of our approaches use only raw utterances and other two use utterances plus the canonical responses of the automatically rewritten utterances provided by CAsT 2021. Our approaches also rely on utterances manually classified by human assessors using a taxonomy defined ad hoc for this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The increasing popularity of conversational assistant systems as well as the advances of automatic-speech recognition and understanding tools have brought novel attention to Conversational Information Seeking (CIS).</p><p>A conversational assistant system helps the user in different activities such as checking the weather forecast, searching for information, or performing ecommerce transactions. Such systems are used in wearable devices, smartphones (e.g., Apple Siri, Google Assistant, Microsoft Cortana), and smart home devices (e.g., Google Home, Amazon Alexa).</p><p>The ability of conversational assistant systems to support conversational information seeking is still limited due to the complexity of the search task. Indeed, information seeking often evolves as a multi-turn dialogue between the user and the system, so the search goes on as natural-language questions (i.e., utterances) and answers. The retrieval of documents relevant to an utterance is difficult due to ambiguity of natural language as well as the lacking of context (a subject may be mentioned before in the conversation). The operation of adding context to ambiguous/incomplete utterances is challenging due to the complexity of understanding the semantic meaning of previous utterances and their answers.</p><p>Thanks to TREC CAsT, the researchers can experiment with their methodologies that aim to improve the automatic understanding of the users' requests and to find the relevant responses using contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The TREC Conversational Assistant Track<ref type="foot" coords="2,326.88,253.85,3.97,6.12" target="#foot_0">4</ref> (CAsT) 2021 provided a dataset including search conversations and document collections. Compared to previous years, CAsT 2021 is based on three collections: (1) English Wikipedia (KILT Wikipedia dump from 2019/08/01) consisting of 5M articles, (2) MS MARCO Web documents (first version) consisting of 3.2M documents from Bing search, and (3) TREC Washington Post collection (V4 2020) consisting of 728,626 news articles from 2012 to 2020.<ref type="foot" coords="2,250.47,325.58,3.97,6.12" target="#foot_1">5</ref> Documents are split into passages, and the passage segmentation is performed using tools (available in the TREC CAsT tools) using SpaCy sentence detection with a fixed non-overlapping passage size.</p><p>CAsT 2021 dataset is made of 26 conversations, each having from 6 to 13 utterances for a total of 239 utterances. The dataset also provides canonical system responses for the utterances.</p><p>An example of conversation is as follows: (1) "I'd like to learn more about frogs. What's the biggest one?", (2) "What's been done to protect them?", (3) "Has that been effective?", (4) "How can I help?", and (5) "Okay, that's the biggest. What is the smallest?" . While the first utterance is relatively easy to process by an Information Retrieval (IR) system, the follow-up utterances have references to previous subjects mentioned in the conversation or in the answers. For example, "What's been done to protect them?", "them" refers to the biggest frogs. Hence, the second utterance lacks context and adding the missing keywords is mandatory for a better retrieval of the relevant documents. Third utterance also lacks context and it depends on the answer to the previous utterance since "that" refers to the strategy to protect the frogs. Also, we can observe a topic shift in the fifth utterance as the user is interested in knowing more about smallest frogs. Even in this case, the utterance needs to be rewritten and enriched with the keyword "frogs" to be successfully answered by an automatic IR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search Conversations</head><p>By carefully inspecting the utterances in the CAsT 2021 dataset, we noticed some common patterns in the conversations: (a) Some utterances do not lack context, and we define them as self-explanatory utterances. As an example, the first utterance of each conversation is always self-explanatory, but there could be other self-explanatory utterances in the middle of the conversation. Very often these utterances introduce a subtopic exploration or even a topic shift. (b) In some cases, the topic of the first utterance dominates the conversation.</p><p>Follow-up utterances are not self-explanatory and refer to the topic introduced at the beginning of the conversation. These utterances depend on the first topic of the conversation. (c) In other cases, utterances are not self-explanatory and refer to some topics mentioned in a previous utterance (different from the first utterance). Hence, these utterances need to be enriched with some context extracted from the previous utterances in the conversation. (d) Similarly to (c), the utterances are not self-explanatory and refer to some topics mentioned in previous utterances and/or in their answers. These utterances are even more tricky as they must be enriched with context extracted from the previous utterances and also from their responses. We will refer to these utterances as depending on previous responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Utterance Labeling</head><p>Given our previous observations, we asked human assessors to manually check the raw utterances along with the manually rewritten utterances with the purpose of labeling raw utterances based on their dependencies. In particular, assessors familiar with the challenges in conversational search evaluated the 239 utterances from 26 conversations using the following labels:</p><p>• Self-Explanatory (SE): the utterance is self-explanatory, so the context is fully provided; • First Topic (F T ): the utterance misses context which depends on the first utterance; • Previous Topic (P T ): the utterance misses context which depends on the previous utterance; • Previous Response (*-P R): the utterance misses context which depends on the previous canonical response. Where * defines whether the utterance depends on the first or the previous utterance, e.g., F T or P T .</p><p>An example of manually labeled conversation is reported in Table <ref type="table" coords="3,441.45,555.16,3.87,8.74" target="#tab_0">1</ref>. Notice that in some cases, the human assessors use the labels F T -P R (i.e., 125 2) or P T -P R (e.g., 125 4, 125 5) to specify that the current utterance depends on the topics from the first/previous utterance and from the response, too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodologies</head><p>Our framework consists of three steps: (1) utterance rewriting, (2) candidate passage retrieval, and (3) neural re-ranking. All our methods employ a Python NLP toolkit for extracting various linguistic features from the utterances<ref type="foot" coords="4,289.77,271.16,3.97,6.12" target="#foot_2">6</ref> and perform utterance rewriting to enrich the raw utterance with the missing context. After utterance rewriting, in the first-stage retrieval, we use the rewritten utterances to retrieve the candidate passages and narrow down the search space. Then, neural re-ranking exploits a contextualized language model based on BERT for passage re-ranking <ref type="bibr" coords="4,443.45,320.56,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Utterance Rewriting</head><p>We assume that a user has an information need that intends to fulfill by issuing utterances to a conversational IR system. A raw utterance, u i , represents the natural language question issued by the user to the system. This is the input of our automatic utterance rewriting module whose output is an enriched utterance, ûi , used to retrieve candidate passages from the document collections. The purpose of the utterance rewriting module is adding missing context to the raw utterance so that the user can get a good answer to her request.</p><p>Runs with Unsupervised Utterance Rewriting. These runs are inspired by our work on topic propagation in multi-turn conversational searches <ref type="bibr" coords="4,442.22,473.62,9.96,8.74" target="#b0">[1]</ref>. They use the raw utterances only.</p><p>-CNR-run1. The approach automatically rewrites the utterance by adding the topics extracted from the first utterance and the previous utterance. The idea behind this approach is that the first utterance has the general topic of the conversation, while the previous one represents the most recent context. This approach has a drawback since it always propagates the context of the very first-turn utterance. This can lead to noisy results, especially for those cases where the focus of interest may change during the conversation (e.g., topic shift, subtopic exploration). -CNR-run3. This run tries to address the weakness of the previous run, avoiding the dependency with the first utterance. The run adds the topics extracted from the previous automatically rewritten utterances provided by CAsT 2021.</p><p>Runs with Utterance Rewriting based on Classification. These runs are inspired by our work on adaptive topic propagation in conversational utterances <ref type="bibr" coords="5,161.89,143.90,9.96,8.74" target="#b1">[2]</ref>. These runs perform the automatic rewriting of raw utterances using the utterance classification explained in Section 2.2. The labels represent the dependencies between the current utterance and the previous utterances as well as their canonical responses. The classification is used to determine the best enrichment for the current utterance. In particular:</p><p>• If the raw utterance is labeled as SE, no rewriting is applied.</p><p>• If the raw utterance is labeled as F T , it is enriched with the topic extracted from the first utterance of the conversation. • When the utterance label is P T , the rewriting is performed using the topic extracted from the previous enriched utterance. • When the label is F T -P R, the utterance is rewritten using the topic extracted from the first utterance. Plus, the context (e.g., topics or keywords) from the canonical response of the previous automatically rewritten utterance is added at the end of the enriched utterance. • When the label is P T -P R, the utterance is rewritten using the topic extracted from the previous enriched utterance. Plus, the context (e.g., topics or keywords) from the canonical response of the previous automatically rewritten utterance is added at the end of the enriched utterance.</p><p>The runs perform utterance rewriting as follows:</p><p>-CNR-run2. This run uses raw utterances plus canonical responses (when needed). For each utterance, the approach checks the label and enriches the current utterance with the topics extracted from the utterance and the response of the previous turns for which there is a dependency. -CNR-run4. As for run2, this run uses raw utterances plus canonical responses (when needed) by checking the utterance label. Differently from the previous run, the topics are extracted from previous automatically rewritten utterances provided by CAsT 2021 for which there is a dependency.</p><p>In all our runs, the topics are extracted from utterances using Spacy noun chunks (objects or subjects). In those cases where the utterance also depends on a previous response, the approach adds the named entities extracted from the candidate response by TagMe<ref type="foot" coords="5,281.23,558.05,3.97,6.12" target="#foot_3">7</ref> with threshold set to 0.1. Using only named entities has the advantage to clean a noisy context, although, in some cases, the set of recognized named entities can be empty which may lead to poor context enrichment.</p><p>Compared to CNR-run1&amp;3, CNR-run2&amp;4 are both based on manual labels, they use context from canonical responses (when needed), and they extract topics from the utterances of the previous turns (enriched or not).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental settings</head><p>Metrics. The effectiveness of the rewriting techniques is evaluated with traditional TREC metrics. In particular, the Average Precision for cutoff at 500 (AP@500) and the normalized Discounted Cumulative Gain (nDCG) for cutoffs at 3, 5, and 500. The use of small cutoffs, such as 3 and 5, is common for the conversational search task since the user expects to receive one crisp answer rather than a long list of potentially relevant results.</p><p>First-stage retrieval. In all our runs, we used Anserini BM25 with RM3 query expansion. In particular, for the first-stage retrieval, we used BM25 with parameters b = 0.9 and k1 = 2.0, chosen after a fine-tuning on MSMARCO-docs collection for the retrieval task with 5, 192 queries from the DEV set. The query expansion is done with 10 keywords taken from the top-10 results with the original query weight set to 0.5.</p><p>Neural re-ranking. We used the model by Nogueira and Cho <ref type="bibr" coords="6,421.69,295.46,10.52,8.74" target="#b2">[3]</ref> to re-rank the results from the previous stage. The model fine-tunes the BERT base pretrained model for re-ranking on the MSMARCO passage retrieval dataset. For each query, Anserini retrieves 1K results which are the input for the re-ranking step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In Table <ref type="table" coords="6,177.39,392.60,3.87,8.74" target="#tab_1">2</ref>, we report the values of the following metrics nDCG@k (with k = 3, 5, and 500) and AP@500 for our four runs. As we can see, the worst results are achieved by CNR-run1 as it does not use any utterance classification and any context from the canonical responses of the previous utterances. On the other hand, CNR-run3 performs pretty well as it uses context from the previous automatic rewritten utterance provided by CAsT 2021.</p><p>Better performances are achieved by CNR-run2 and CNR-run4 as they enrich the raw utterances leveraging the utterance classification and adding the context extracted from the previous canonical responses when needed. Still, they cannot outperform CNR-run3.</p><p>CAsT 2021 also provided for each query/utterance the worst, median, and best performance for 10 raw runs, 27 canonical runs, and 13 manual runs. We computed the average over all the queries, and the results are shown in Table <ref type="table" coords="6,472.84,536.07,3.87,8.74" target="#tab_2">3</ref>.</p><p>As expected, the performances of the two unsupervised runs (CNR-run1 and CNR-run3) using raw utterances are close to the raw median values reported in Table <ref type="table" coords="6,163.15,571.93,3.87,8.74" target="#tab_2">3</ref>. While the performances of CNR-run2 and CNR-run4 are close to the canonical median values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this report, we have presented the methodologies implemented for our participation in CAsT 2021. Our approaches aim to enrich the raw utterances using topical keywords extracted from the previous utterances and their responses. As future work, we plan to improve the utterance classification in order to better capture the dependencies between the current utterance and the utterances of the previous turns as well as their canonical responses. Also, we plan to use neural expansion methods with the purpose of improving our automatic rewriting techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,135.78,116.91,343.81,114.30"><head>Table 1 .</head><label>1</label><figDesc>Example of manually labeled conversation</figDesc><table coords="4,135.78,139.49,343.81,91.72"><row><cell>ID</cell><cell>Manual</cell><cell>Raw</cell><cell>Label</cell></row><row><cell>125 1</cell><cell>I'd like to learn more about frogs. What's the biggest one?</cell><cell>I'd like to learn more about frogs. What's the biggest one?</cell><cell>SE</cell></row><row><cell>125 2</cell><cell>What has been done to protect Goliath frogs?</cell><cell>What's been done to protect them?</cell><cell>FT-PR</cell></row><row><cell>125 3</cell><cell>Has the Equatorial Guinean government's conservation measures to protect Goliath frogs been effective?</cell><cell>Has that been effective?</cell><cell>PT-PR</cell></row><row><cell>125 4</cell><cell>How can I help with protecting Goliath frogs?</cell><cell>How can I help?</cell><cell>PT-PR</cell></row><row><cell>125 5</cell><cell>What is the smallest frog?</cell><cell cols="2">Okay, that's the biggest. What is the smallest? FT</cell></row><row><cell>125 6</cell><cell>Why would leaf litter affect the size of a frog?</cell><cell>Why would leaf litter affect its size?</cell><cell>FT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,158.53,116.91,286.00,79.19"><head>Table 2 .</head><label>2</label><figDesc>Performance of our runs at CAsT 2021</figDesc><table coords="7,158.53,139.12,286.00,56.98"><row><cell>Run</cell><cell>nDCG@3</cell><cell>nDCG@5</cell><cell>nDCG@500</cell><cell>AP@500</cell></row><row><cell>Run1 (raw)</cell><cell>0.2983</cell><cell>0.2897</cell><cell>0.1956</cell><cell>0.1122</cell></row><row><cell>Run2 (canonical)</cell><cell>0.3035</cell><cell>0.2936</cell><cell>0.2018</cell><cell>0.1218</cell></row><row><cell>Run3 (raw)</cell><cell>0.3490</cell><cell>0.3395</cell><cell>0.2218</cell><cell>0.1256</cell></row><row><cell>Run4 (canonical)</cell><cell>0.3327</cell><cell>0.3281</cell><cell>0.2201</cell><cell>0.1279</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,149.86,211.94,306.43,145.48"><head>Table 3 .</head><label>3</label><figDesc>Performance of CAsT 2021 runs: averaged over all queries</figDesc><table coords="7,149.86,235.90,306.43,121.53"><row><cell>Run</cell><cell></cell><cell>nDCG@3</cell><cell>nDCG@5</cell><cell>nDCG@500</cell><cell>AP@500</cell></row><row><cell></cell><cell>worst</cell><cell>0.057</cell><cell>0.066</cell><cell>0.078</cell><cell>0.030</cell></row><row><cell>Raw</cell><cell>median</cell><cell>0.338</cell><cell>0.336</cell><cell>0.334</cell><cell>0.176</cell></row><row><cell></cell><cell>best</cell><cell>0.675</cell><cell>0.635</cell><cell>0.656</cell><cell>0.433</cell></row><row><cell></cell><cell>worst</cell><cell>0.017</cell><cell>0.024</cell><cell>0.063</cell><cell>0.017</cell></row><row><cell cols="2">Canonical median</cell><cell>0.380</cell><cell>0.384</cell><cell>0.454</cell><cell>0.244</cell></row><row><cell></cell><cell>best</cell><cell>0.809</cell><cell>0.770</cell><cell>0.766</cell><cell>0.545</cell></row><row><cell></cell><cell>worst</cell><cell>0.088</cell><cell>0.111</cell><cell>0.118</cell><cell>0.041</cell></row><row><cell>Manual</cell><cell>median</cell><cell>0.555</cell><cell>0.550</cell><cell>0.612</cell><cell>0.371</cell></row><row><cell></cell><cell>best</cell><cell>0.780</cell><cell>0.761</cell><cell>0.765</cell><cell>0.535</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,646.84,105.03,7.86"><p>http://www.trecCAsT.ai/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,657.79,231.70,7.86"><p>This data requires a signed license agreement with NIST.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="4,144.73,657.79,307.77,8.37"><p>spacy library available at https://spacy.io/usage/linguistic-features.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="5,144.73,658.44,174.17,7.47"><p>https://pypi.org/project/tagme/0.1.2/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,490.62,342.24,7.86;7,146.91,501.58,333.68,7.86;7,146.91,512.54,48.38,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,146.91,501.58,181.52,7.86">Topic Propagation in Conversational Search</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">I</forename><surname>Muntean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,353.60,501.58,47.75,7.86">SIGIR 2020</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2057" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,523.50,342.24,7.86;7,146.91,534.46,333.68,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,146.91,534.46,214.23,7.86">Adaptive utterance rewriting for conversational search</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">I</forename><surname>Muntean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,378.69,534.46,37.75,7.86">IPM 2021</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,545.41,342.24,7.86;7,146.91,556.37,97.10,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="7,270.20,545.41,132.91,7.86">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
