<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,197.81,97.71,208.45,10.76">THE THISL SDR SYSTEM AT TREC-9</title>
				<funder ref="#_SPwYStd">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_H52c6Gj">
					<orgName type="full">ESPRIT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,221.79,126.60,60.58,10.76"><forename type="first">Steve</forename><surname>Renals</surname></persName>
							<email>s.renals@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<postCode>S1 4DP</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.28,126.60,71.01,10.76;1,377.29,124.30,1.49,8.37"><forename type="first">Dave</forename><surname>Abberley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<postCode>S1 4DP</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,197.81,97.71,208.45,10.76">THE THISL SDR SYSTEM AT TREC-9</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3BC35CF48521858DE2AC71C420C71D06</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the TREC-9 Spoken Document Retrieval (SDR) track. The THISL SDR system consists of a realtime version of a hybrid connectionist/HMM large vocabulary speech recognition system and a probabilistic text retrieval system. This paper describes the configuration of the speech recognition and text retrieval systems, including segmentation and query expansion. We report our results for development tests using the TREC-8 queries, and for the TREC-9 evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The TREC-9 Spoken Document Retrieval (SDR) track followed on from the TREC-8 track, using the same audio collection: 902 shows (502 hours) of US broadcast news material covering the period February-June 1998. The collection contained 21 754 individual news items, totalling 389 hours of news material. The basic task was to retrieve the set of stories relevant to each of 50 topics.</p><p>There were three principal dimensions of variation to be investigated in this year's evaluation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Story Boundaries</head><p>The main task assumed unknown story boundaries. Each episode was treated as a continuous audio stream and it was the task of the SDR system to find the location (time) of the relevant news stories.</p><p>The known story boundary condition, in which stories are segmented manually and irrelevant material such as adverts are removed, was used as a contrast.</p><p>Query Length Previous SDR tracks used short (sentence length) queries. In TREC-9, a terse query was also provided for each topic, which typically contained 2-3 words, to reflect queries submitted to web search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Recognizer Effects</head><p>In addition to the baseline recognizer and reference (subtitle) transcripts, we also used the transcripts produced by other evaluation participants (Cambridge University and LIMSI). This il- * Now at: SoftSound, Cambridge CB4 0WS, UK luminates the effect of speech recognizer word error rate on SDR system performance.</p><p>Much of the paper describes experiments on the development test set, using the TREC-8 SDR queries. Since that evaluation included short queries only, we generated terse queries ourselves: our TREC-8 terse queries are thus not comparable with similar queries that have been generated by other groups. In our development experiments we took average precision on the short queries as our primary metric.</p><p>The paper is structured as follows. Section 2 describes the speech recognition component of the system, which is based on the ABBOT hybrid connectionist/HMM large vocabulary speech recognizer, running in real-time mode. Section 3 outlines the text retrieval system that we have used, together with a discussion of the algorithms employed for query expansion and segmentation. Section 4 presents the results we obtained on the TREC-9 SDR track and further discussion of some of the issues raised ends the paper, along with some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SPEECH RECOGNITION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Abbot</head><p>ABBOT <ref type="bibr" coords="1,348.26,513.57,97.02,8.97" target="#b15">(Robinson et al., 1996)</ref> is a connectionist/HMM system <ref type="bibr" coords="1,343.94,525.52,118.86,8.97" target="#b1">(Bourlard and Morgan, 1994)</ref> which estimates posterior phone probabilities given the acoustic data at each frame. This discriminative approach differs from that used by most recognizers in that it does not include a generative model of the data. That is, the joint probability of the acoustics and word sequence is not estimated; instead an estimate of the posterior probability of the word sequence given the acoustic data is provided. This may be interpreted as a probabilistic finite state acceptor model <ref type="bibr" coords="1,453.46,621.16,92.31,8.97" target="#b4">(Hennebert et al., 1997)</ref>.</p><p>A recurrent network (RNN) trained as a phone classifier <ref type="bibr" coords="1,328.69,646.05,69.29,8.97" target="#b12">(Robinson, 1994)</ref> is used as the principal posterior probability estimator. This approach is attractive since fewer parameters are required for the connectionist model (the posterior distribution is typically less complex than the likelihood) and connectionist architectures make very few assumptions on the form of the distribution. Additionally, this approach enables the use of posterior probability based pruning in decoding <ref type="bibr" coords="2,136.74,87.11,115.33,8.97" target="#b10">(Renals and Hochberg, 1999)</ref>.</p><p>We produced two sets of transcriptions of the audio data for the TREC-9 evaluation, referred to as S1 and S2. Both systems used the same language model (LM) and search components. The S1 system was configured to run in realtime, while the S2 system used a richer acoustic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">S1 Acoustic Model</head><p>The S1 acoustic model comprised two RNNs each of which estimated 54 context-independent posterior phone probabilities for each frame of acoustic data. Both networks were trained using a sequence of 12th order perceptual linear prediction features <ref type="bibr" coords="2,118.77,241.05,72.96,8.97" target="#b5">(Hermansky, 1990</ref>) (plus log energy). One network estimated the phone probabilities for the current frame conditioned on the past sequence of acoustic features. The second network was trained using a frame sequence that was reversed in time, and thus estimated the phone probabilities conditioned on the future. The two estimated probability streams were averaged in the log domain to produce a final set of probability estimates. The models were trained using the 104 hours of Broadcast News training data released in 1997 (the first half of the complete broadcast news training set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">S2 Acoustic Model</head><p>The acoustic model for the S2 system was obtained by log domain merging of the probability estimates produced by the RNNs used in the S1 system with those produced by an acoustic model using modulation-filtered spectrogram features. This is essentially the system used by the SPRACH group in the 1998 broadcast news evaluation <ref type="bibr" coords="2,239.77,466.58,50.92,8.97;2,54.00,478.54,22.70,8.97" target="#b3">(Cook et al., 1999;</ref><ref type="bibr" coords="2,79.19,478.54,86.48,8.97" target="#b13">Robinson et al., 2001)</ref>.</p><p>The modulation-filtered spectrogram (MSG) was developed by <ref type="bibr" coords="2,91.65,502.59,97.78,8.97" target="#b7">Kingsbury et al. (1998)</ref> as a feature representation that is robust to the signal variations caused by reverberation and noise. The robustness is obtained by emphasising modulation in the speech spectral structure occurring at rates of 16Hz or less (as measured with a criticalband-like resolution) and adapting to slowly-varying components of the speech signal (a form of automatic gain control). MSG feature processing involves first calculating an auditory-like spectrum, then filtering the amplitude in each frequency band by two parallel banks of filters, one lowpass below 16Hz, and the second bandpass between 2Hz and 16Hz. Each channel is then passed, in series, through two feedback Automatic Gain Control units with time constants of 160ms and 640ms. The resulting spectra are used as features; orthogonalization (e.g. via the discrete cosine transform) provides no benefit for these features in our experience with connectionist models. However, we do increase the robustness of the system to environmental condi-tions by normalizing the statistics of every feature channel to zero mean and unit variance over each segment, or over entire recordings if no segmentation is performed.</p><p>The MSG acoustic model used an MLP containing 8000 hidden units trained on the full 200 hours broadcast news training set, with the training data downsampled to 4 kHz bandwidth. Experiment has previously indicated that although the word error rate of the bandlimited MSG-based system is higher than that of the PLP-based S1 system, the errors are different and the overall performance may be improved by merging the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Language Modelling and Search</head><p>The same backed-off trigram LM was used by both the S1 and S2 systems <ref type="bibr" coords="2,382.38,251.89,94.89,8.97" target="#b13">(Robinson et al., 2001)</ref>. Approximately 450 million words of text data were used to generate the model, comprising: the Broadcast News acoustic training transcripts (1.6M words); the 1996 Broadcast News LM text data (150M words); and the 1998 North American News text data (LA Times/Washington Post (12M words), Associated Press World Service (100M words), NY Times (190M words)). The models were trained using version 2 of the CMU-Cambridge SLM Toolkit <ref type="bibr" coords="2,445.17,347.54,104.89,8.97;2,313.37,359.49,23.24,8.97" target="#b2">(Clarkson and Rosenfeld, 1997)</ref> using Witten-Bell discounting. We used a lexicon containing 65 432 words, including every word in the broadcast news training data. The dictionary was constructed using phone decision tree smoothed acoustic alignments. The LM and lexicon were constructed from material pre-dating the acoustic data and were fixed throughout the evaluation.</p><p>For both systems we used a large vocabulary stack decoder CHRONOS <ref type="bibr" coords="2,383.02,443.18,118.86,8.97" target="#b14">(Robinson and Christie, 1998)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Results</head><p>Table <ref type="table" coords="2,338.52,488.41,4.98,8.97">1</ref> gives the word error rate estimates obtained using the S1 and S2 systems. These estimates were obtained using a 10 hour sample of the test corpus defined by NIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TEXT RETRIEVAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Text Retrieval System</head><p>We used a standard probabilistic system using a short stop list of 132 words (with an additional stop list of 78 words System Sub. Del. Ins. WER S1 22.0 6.1 3.9 32.0 S2 20.0 5.4 3.8 29.2</p><p>Table 1: Word error rates (WER) for the S1 and S2 speech recognition systems, estimated using a 10 hour subset of the corpus.</p><p>when processing a query), the Porter stemming algorithm and term weighting similar to that used in the Okapi system. Specifically, following <ref type="bibr" coords="3,174.00,99.07,116.69,8.97;3,54.00,111.02,24.90,8.97" target="#b11">Robertson and Spärck Jones (1997)</ref>, we used the following function CW (t, d) to compute the combined relevance weight between a term t and a document d: <ref type="bibr" coords="3,238.12,167.43,23.04,8.97">(t, d)</ref> .</p><formula xml:id="formula_0" coords="3,67.47,153.86,178.21,22.87">CW (t, d) = CFW (t) * T F(t, d) * (K + 1) K((1 -b) + b * NDL(d)) + T F</formula><p>(1)</p><formula xml:id="formula_1" coords="3,54.00,188.73,236.70,8.97">T F(t, d) is the frequency of term t in document d, NDL(d)</formula><p>is the normalized document length of d:</p><formula xml:id="formula_2" coords="3,134.07,221.65,156.62,23.07">NDL(d) = DL(d) DL ,<label>(2)</label></formula><p>where DL(d) is the length of document d (ie the number of unstopped terms in d). CFW (t) is the collection frequency weight of term t and is defined as:</p><formula xml:id="formula_3" coords="3,122.68,299.67,164.15,22.55">CFW (t) = log N N(t) , (<label>3</label></formula><formula xml:id="formula_4" coords="3,286.82,306.41,3.87,8.97">)</formula><p>where N is the number of documents in the collection and N(t) is the number of documents containing term t. The parameters b and K in (1) control the effect of document length and term frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Segmentation</head><p>Since the core task of the SDR track involves the situation where story boundaries are unknown, segmentation of the audio stream assumes some importance. Unlike some other broadcast news speech recognition systems (eg, <ref type="bibr" coords="3,246.63,452.33,44.06,8.97;3,54.00,464.29,24.28,8.97" target="#b8">Odell et al. (1999)</ref>), we do not perform any acoustic segmentation in the recognition phase (the audio stream is decoded directly); anyway, there is no good correlation between segments obtained purely from low-level audio features and story segments required for information retrieval. Although other approaches, such as those investigated in the TDT programme, are of some interest, we have no evidence of their suitability for spoken document retrieval. Thus we have retained the simple approach used last year, based on overlapping rectangular windows of the audio stream 1 . At query time, those relevant segments which overlap are merged. Previously for this type of automatic segmentation, we have used (1) with b = 0, since each segment is the same length. However with short segments (30s) this can result in a large number of identical scores, with no good way of breaking the tie. Since the segments do not contain identical numbers of terms -and since we need a tie-breaker -we have used a small non-zero value for b (typically 0.1).</p><p>1 Also used successfully with an SDR system for a 3 000 hour archive of BBC news broadcasts.</p><p>The procedure for merging was as follows. The ranked list of (presumed) relevant segments was processed in best first order. Segments that could be potentially merged with the current segment must: (1) come from the same episode;</p><p>(2) overlap in time; and (3) be within a rank ∆ r of the current segment. If these conditions are met then the two segments are merged. If the scores of the two segments are within a factor m of each other, and the ranks are within ∆ f ≤ ∆ r then we assume an equal merge; otherwise the higher ranked segment dominates the other. In an equal merge, the score of the merged segment is set to be the maximum score of the two segments increased by a factor s, and the reference time is set to be the mid-point of the segment. For a dominating merge, the score and reference time of the merged segment are set to be the same as for the highest scoring component segment. The merging process is iterated until convergence, with parameters ∆ r and ∆ f halved on each iteration.</p><p>The overall segmentation procedure is summarized as follows:</p><p>1. Entire news episode decoded into a stream of text 2. For indexing, the text stream is split into documents using a fixed length rectangular window with a frame length of t and a frame shift of t s 3. At retrieval time a list of 5R segments are retrieved, and the above merging process is carried out. We conducted a number of development experiments to obtain values for the segment merging parameters: ∆ r = 1600, ∆ f = 200, m = 0.95 and s = 1.005 4. The top R merged segments are then returned.</p><p>Previously we have used t = 30s and t s = 15s. We conducted a variety of experiments looking at the effect of varying the frame shift, with a constant frame length (t = 30s) -our hypothesis was that the possible cost of redundant segments of decreasing the frame shift might be offset by the segment merging algorithm. The results (table 2) indicated a frame shift of t s = 9s to be a good tradeoff between average precision and index size.</p><p>This merging scheme was developed using the TREC-8 development set. Given the several heuristically set parameters, there is a distinct possibility of over-tuning. An alternative approach <ref type="bibr" coords="3,379.09,634.09,86.48,8.97" target="#b6">(Johnson et al., 2000)</ref> merged all segments originating within 4 or 5 minutes of each other from a single episode. While this approach may well prove to be robust in actual usage, we believed it may be counter-productive for the SDR track since different relevant documents are sometimes located within less than 4 minutes of each other (owing to adverts, etc.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Query Expansion</head><p>Following experiments on TREC-7 and TREC-8 data <ref type="bibr" coords="4,271.88,253.63,14.11,8.97;4,54.00,265.58,76.71,8.97" target="#b0">(Abberley et al., 1999;</ref><ref type="bibr" coords="4,134.01,265.58,79.64,8.97" target="#b9">Renals et al., 2000)</ref> we have applied a query expansion approach whereby the relevance of potential expansion terms to original query terms is obtained by a product of term frequencies weighted by collection frequency weights. Specifically, the query expansion weight QEW (Q, e) for a potential expansion term e and a query Q, across a set of nr (pseudo) relevant documents is defined as:</p><formula xml:id="formula_5" coords="4,54.28,366.60,236.15,27.57">QEW (Q, e) = CFW (e) ∑ t∈Q CFW (t) nr ∑ i=1 T F(e, d i ) • T F(t, d i ).</formula><p>(4) QEW (Q, e) is used to rank the expansion terms, and the top nt are chosen to expand Q. nr is chosen such that only those documents with a relevance score of greater than r f • W (r f &lt; 1) are used. The expanded query terms are weighted by (nt -rank +1)/nt, with terms in the existing query given an additional weight of 1.</p><p>In TREC-7 and TREC-8 we obtained significant benefits from query expansion using a parallel corpus largely consisting of newspaper and newswire text from the same period as the target broadcast news corpus. In TREC-9 we constructed a parallel corpus from the following sources:</p><p>• TREC-7 SDR reference transcripts (North American broadcast news, covering parts of June 1997 -January 1998): c.0.7M words  <ref type="table" coords="4,337.76,180.18,3.88,8.97">3</ref>: Query expansion using target and parallel corpora, with TREC-8 queries. Self+Parallel indicates that query expansion occurs on a corpus made up of the union of the target and parallel corpora; Self then Parallel indicates that QE is first performed on the target corpus, to produce an expanded query which is then expanded a second time using the parallel corpus. Parallel then Self uses the parallel corpus first, then the target corpus.</p><p>experiments on TREC-8, we found that 50% of the documents used for QE were from the AP newswire, 36% from the LA Times/Washington Post corpus, 12% from the New York Times and 2% from the TREC-7 reference transcripts.</p><p>In addition to the parallel corpus query expansion, we also experimented with query expansion using blind feedback on the main (target) corpus (also using ( <ref type="formula" coords="4,500.55,378.48,3.49,8.97">4</ref>)). Table <ref type="table" coords="4,545.08,378.48,4.98,8.97">3</ref> shows the results of query expansion purely using the target corpus, purely using the parallel corpus and various configurations using both (parallel then self, self then parallel, self and parallel simultaneously). Using our primary metric of average precision with short queries, it appears that expanding the query first on the parallel corpus, then on the target corpus is best. However, this result does not hold for terse queries. So far we have not investigated this effect further. Using a parallel corpus augmented with a copy of the target corpus produced similar results to the parallel corpus alone, as virtually all the documents used for query expansion came from the parallel corpus -probably a side-effect of mixing short 30s segments with whole stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>In the TREC-9 SDR track we performed experiments on the main unknown story boundary (SU) condition and the contrast known story boundary (SK) condition. The same transcriptions were used in each case. Although different text retrieval parameters were used for the SU and SK conditions, the parameters were not dependent on the form of the queries (short or terse). In all cases a query expansion approach of first expanding on the parallel corpus, then on the target corpus was adopted. is the length parameter and K the discounting parameter in the weighting function. The additional query expansion parameters are nt (number of terms to add), nrmax (maximum number of pseudo-relevant documents) and r f (multiple of best relevance score that a document must be greater than to be used in QE). The segment merging parameters (described in section 3.2) control the ranking distance threshold below which merging may occur (∆ r ), the ranking difference (∆ f ) and score multiple (m) to determine whether a merge is equal or dominating, and the factor to increase the score by in the case of an equal merge (s).</p><p>ers (S1 and S2), we also used the following transcriptions: Results for the SU case in the TREC-9 SDR track are presented in table 5. The average precisions are, in all cases, 20-25% lower (relative) than for TREC-8. This suggests that the TREC-9 queries may have been more difficult in some way, or that the system was over-tuned to the TREC-8 queries. Secondly, we see that the performance on the terse queries is similar to that on short queries. Note that we optimised our system using short queries on TREC-8. Finally, following the trend of previous evaluations, the link between word error rate and text retrieval accuracy is very weak. Indeed, out of all the speech recognition transcriptions, the highest average precision on short queries is achieved using S2 (with a WER of 29%).</p><p>For contrast, results for the SK case in the TREC-9 SDR track are presented in table 6. These results follow the same form as the SU results, indicating that the low average precisions (compared with TREC-8) are not due to the segmentation/merging procedure. The relative gap between SK and SU average precision is 10-20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Terse and Short Queries</head><p>The topics for which we get a substantially better performance from short queries compared with terse queries fall into two basic types: those where the terse query is little more than an abbreviation -eg, "I L O" (130), "N B A" (165) -and those where the terse query is expressed using different words or incompletely compared with the short query <ref type="bibr" coords="5,338.65,681.91,21.22,8.97">(136,</ref><ref type="bibr" coords="5,359.86,681.91,16.97,8.97">155,</ref><ref type="bibr" coords="5,376.83,681.91,16.97,8.97">156,</ref><ref type="bibr" coords="5,393.81,681.91,16.97,8.97">171)</ref>. The first case might be improved by better acronym processing, the second ought to be dealt with by query expansion. More analysis is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Query Expansion</head><p>Previously we used parallel corpus QE only, having found that query expansion on the target corpus did not give a reliable improvement. From our experiments on TREC-8, it seems that first expanding on the parallel corpus, with the resultant expanded query being expanded again using the target corpus gives a reliable improvement. An interesting factor to be investigated is that query expansion seems to have different behaviour on terse and short queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Non-lexical Information</head><p>Although we were able to compute various types of nonlexical information (eg, named entities, speaker changes, sentence boundaries) we chose not to use such information in this evaluation. In the case of named entities, this was because we did not have a principled way of using them. In the case of richer boundary information, we did not feel that this would be rewarded under the evaluation metrics in use. For example, in discussions with broadcast archive users of our system, it has been apparent that returning clips that begin and end at natural boundaries would enhance their appreciation of the system; the single reference time method of denoting segments does not give any credit for accurate begin/end points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Standard QE Corpus</head><p>A great deal of effort has gone into standardizing the acoustic model and LM training data for speech recognition, to enable better evaluation of the underlying models and algorithms. It would be of interest to increase this standardization, by specifying a baseline query expansion corpus, to be used in a contrast run (at least).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our major conclusions are as follows:</head><p>• There is only a weak link between speech recognition accuracy and spoken document retrieval precision and recall;</p><p>• Query expansion using both a parallel text corpus and the target corpus is reliable and extremely effective;</p><p>• Simple fixed segmentation, followed by query-time segment merging is reliable, causing a degradation of 10-20% compared with the hand-segmented case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,54.00,73.96,236.69,127.14"><head>Table 2 :</head><label>2</label><figDesc>Varying segmentation frame shift (t s ), affect on average precision and R-precision, for development test on TREC-8 queries, with t = 30s.</figDesc><table coords="4,89.60,73.96,165.50,81.49"><row><cell></cell><cell cols="3">Short Queries Terse Queries</cell></row><row><cell>Shift/s</cell><cell>AveP</cell><cell>R-P AveP</cell><cell>R-P</cell></row><row><cell cols="4">6 0.526 0.524 0.486 0.490</cell></row><row><cell cols="4">9 0.526 0.518 0.477 0.485</cell></row><row><cell cols="4">12 0.518 0.508 0.487 0.476</cell></row><row><cell cols="4">15 0.510 0.507 0.470 0.477</cell></row><row><cell cols="4">20 0.498 0.492 0.459 0.467</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,313.37,681.62,236.69,33.17"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="4,471.93,681.62,78.13,8.97"><row><cell>summarizes the pa-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,54.00,348.25,236.69,8.97"><head>Table 4 :</head><label>4</label><figDesc>Parameters used for TREC-9 Evaluation Runs. b</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,313.37,73.96,236.69,267.63"><head>Table 6 :</head><label>6</label><figDesc>TREC-9 SDR track evaluation results for story boundary known (SK) condition.</figDesc><table coords="5,313.37,73.96,236.69,233.94"><row><cell cols="2">Transcriptions</cell><cell cols="3">Short Queries Terse Queries</cell></row><row><cell>ID</cell><cell>WER</cell><cell>AveP</cell><cell>R-P AveP</cell><cell>R-P</cell></row><row><cell>R1</cell><cell cols="4">10.3 0.409 0.419 0.418 0.425</cell></row><row><cell>S1</cell><cell cols="4">32.0 0.392 0.399 0.392 0.396</cell></row><row><cell>S2</cell><cell cols="4">29.2 0.399 0.410 0.393 0.401</cell></row><row><cell>B1</cell><cell cols="4">26.7 0.387 0.401 0.384 0.398</cell></row><row><cell>CUHTK</cell><cell cols="4">20.5 0.373 0.388 0.373 0.387</cell></row><row><cell>LIMSI1</cell><cell cols="4">21.5 0.377 0.405 0.386 0.391</cell></row><row><cell>LIMSI2</cell><cell cols="4">21.2 0.395 0.407 0.397 0.421</cell></row><row><cell cols="5">Table 5: TREC-9 SDR track evaluation results for story</cell></row><row><cell cols="4">boundary unknown (SU) condition.</cell><cell></cell></row><row><cell cols="5">Transcriptions Short Queries Terse Queries</cell></row><row><cell>ID</cell><cell>WER</cell><cell>AveP</cell><cell>R-P AveP</cell><cell>R-P</cell></row><row><cell>R1</cell><cell cols="4">10.3 0.509 0.489 0.492 0.477</cell></row><row><cell>S1</cell><cell cols="4">32.0 0.464 0.441 0.475 0.463</cell></row><row><cell>S2</cell><cell cols="4">29.2 0.465 0.435 0.478 0.463</cell></row><row><cell>B1</cell><cell cols="4">26.7 0.462 0.447 0.469 0.451</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p><rs type="person">Dan Ellis</rs> and <rs type="person">Tony Robinson</rs> worked on the system for TREC-8 SDR and the work described here uses the fruits of their labour. We thank <rs type="person">Tony Robinson</rs> and <rs type="institution">SoftSound</rs> for use of the CHRONOS decoder. This work was supported by <rs type="funder">ESPRIT</rs> project <rs type="projectName">THISL</rs> (<rs type="grantNumber">EP23495</rs>) and <rs type="funder">EPSRC</rs> project <rs type="projectName">SToBS</rs> (<rs type="grantNumber">GR/M36717</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_H52c6Gj">
					<idno type="grant-number">EP23495</idno>
					<orgName type="project" subtype="full">THISL</orgName>
				</org>
				<org type="funded-project" xml:id="_SPwYStd">
					<idno type="grant-number">GR/M36717</idno>
					<orgName type="project" subtype="full">SToBS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,313.37,215.81,236.69,8.97;6,323.33,227.77,226.73,8.97;6,323.33,239.72,226.73,8.97;6,323.33,251.68,98.24,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,323.33,227.77,226.73,8.97;6,323.33,239.72,25.86,8.97">Retrieval of broadcast news documents with the THISL system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Abberley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,379.15,239.72,170.91,8.97">Proc. Seventh Text Retrieval Conference</title>
		<meeting>Seventh Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.37,272.91,236.70,8.97;6,323.33,284.87,216.22,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,463.87,272.91,86.20,8.97;6,323.33,284.87,132.81,8.97">Connectionist Speech Recognition-A Hybrid Approach</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Kluwer Academic</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.37,306.11,236.69,8.97;6,323.33,318.06,226.73,8.97;6,323.33,330.02,113.69,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,471.82,306.11,78.24,8.97;6,323.33,318.06,181.86,8.97">Statistical language modeling using the CMU-Cambridge toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,528.65,318.06,21.41,8.97;6,323.33,330.02,44.48,8.97">Proc. Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="2707" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.37,351.25,236.69,8.97;6,323.33,363.21,226.73,8.97;6,323.33,375.16,226.73,8.97;6,323.33,387.12,226.73,8.97;6,323.33,399.07,163.70,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,447.44,375.16,102.62,8.97;6,323.33,387.12,144.30,8.97">The SPRACH system for the transcription of broadcast news</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Al-Ghoneim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gotoh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,494.07,387.12,55.99,8.97;6,323.33,399.07,104.24,8.97">Proc. DARPA Broadcast News Workshop</title>
		<meeting>DARPA Broadcast News Workshop</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="161" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.37,420.31,236.69,8.97;6,323.33,432.27,226.73,8.97;6,323.33,444.22,226.73,8.97;6,323.33,456.18,172.46,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,372.80,432.27,177.26,8.97;6,323.33,444.22,204.65,8.97">Estimation of global posteriors and forwardbackward training of hybrid HMM/ANN systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hennebert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,323.33,456.18,68.38,8.97">Proc. Eurospeech</title>
		<meeting>Eurospeech<address><addrLine>Rhodes</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1951" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.37,477.41,236.69,8.97;6,323.33,489.37,226.73,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,412.37,477.41,137.69,8.97;6,323.33,489.37,70.71,8.97">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,401.81,489.37,83.53,8.97">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.37,510.61,236.70,8.97;6,323.33,522.56,226.73,8.97;6,323.33,534.52,226.73,8.97;6,323.33,546.47,226.73,8.97;6,323.33,558.43,47.32,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,451.51,522.56,98.55,8.97;6,323.33,534.52,162.38,8.97">Audio indexing and retrieval of complete broadcast news shows</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jourlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Spärck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,504.05,534.52,46.01,8.97;6,323.33,546.47,207.86,8.97">Proc. RIAO 2000, Content Based Mulitmedia Information Access</title>
		<meeting>RIAO 2000, Content Based Mulitmedia Information Access</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1163" to="1177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.37,579.66,236.69,8.97;6,323.33,591.62,226.73,8.97;6,323.33,603.57,175.55,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,323.33,591.62,226.73,8.97;6,323.33,603.57,18.37,8.97">Robust speech recognition using the modulation spectrogram</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">E D</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,349.88,603.57,94.21,8.97">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="117" to="132" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.37,624.81,236.69,8.97;6,323.33,636.77,226.73,8.97;6,323.33,648.72,226.73,8.97;6,323.33,660.68,37.36,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,534.57,624.81,15.49,8.97;6,323.33,636.77,226.73,8.97;6,323.33,648.72,25.86,8.97">The CUHTK-Entropic 10xRT broadcast news transcription system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,368.61,648.72,161.62,8.97">Proc. DARPA Broadcast News Workshop</title>
		<meeting>DARPA Broadcast News Workshop</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="271" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.37,681.91,236.69,8.97;6,323.33,693.87,226.73,8.97;6,323.33,705.82,84.68,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,323.33,693.87,162.96,8.97">Indexing and retrieval of broadcast news</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Abberley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,496.67,693.87,53.39,8.97;6,323.33,705.82,44.83,8.97">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,75.16,236.69,8.97;7,63.96,87.11,226.73,8.97;7,63.96,99.07,226.73,8.97;7,63.96,111.02,17.44,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,218.19,75.16,72.50,8.97;7,63.96,87.11,226.73,8.97;7,63.96,99.07,14.39,8.97">Start-synchronous search for large vocabulary continuous speech recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,86.54,99.07,171.07,8.97">IEEE Trans. Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="542" to="553" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,130.95,236.70,8.97;7,63.96,142.90,226.73,8.97;7,63.96,154.86,179.36,8.97" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,233.14,130.95,57.56,8.97;7,63.96,142.90,110.99,8.97">Simple proven approaches to text retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<idno>TR356</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Cambridge University Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="7,54.00,174.78,236.69,8.97;7,63.96,186.74,226.73,8.97;7,63.96,198.69,87.45,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,156.91,174.78,133.78,8.97;7,63.96,186.74,124.69,8.97">The application of recurrent nets to phone probability estimation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,197.72,186.74,92.97,8.97;7,63.96,198.69,37.64,8.97">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="298" to="305" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,218.62,236.69,8.97;7,63.96,230.57,226.73,8.97;7,63.96,242.53,226.73,8.97;7,63.96,254.48,103.48,8.97" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="7,270.76,230.57,19.93,8.97;7,63.96,242.53,189.68,8.97">Connectionist speech recognition of broadcast news</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A G</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Speech Communication. In press</note>
</biblStruct>

<biblStruct coords="7,54.00,274.41,236.69,8.97;7,63.96,286.36,226.73,8.97;7,63.96,298.32,226.73,8.97;7,63.96,310.28,17.44,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,207.38,274.41,83.31,8.97;7,63.96,286.36,143.62,8.97">Time-first search for large vocabulary speech recognition</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Christie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,228.13,286.36,62.57,8.97;7,63.96,298.32,185.23,8.97">Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="829" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,330.20,236.69,8.97;7,63.96,342.16,226.73,8.97;7,63.96,354.11,226.73,8.97;7,63.96,366.07,226.73,8.97;7,63.96,378.02,190.09,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,259.28,330.20,31.41,8.97;7,63.96,342.16,222.74,8.97">The use of recurrent networks in continuous speech recognition</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,268.75,354.11,21.94,8.97;7,63.96,366.07,226.73,8.97;7,63.96,378.02,10.17,8.97">Automatic Speech and Speaker Recognition -Advanced Topics</title>
		<editor>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="233" to="258" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
