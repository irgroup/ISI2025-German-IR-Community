<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,109.96,76.39,392.17,16.20">Hummingbird&apos;s Fulcrum SearchServer at TREC-9</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2001-02-06">February 6, 2001</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stephen</orgName>
								<address>
									<addrLine>Tomlinson 1, Tom Blackwell Hummingbird Ottawa</addrLine>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,109.96,76.39,392.17,16.20">Hummingbird&apos;s Fulcrum SearchServer at TREC-9</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2001-02-06">February 6, 2001</date>
						</imprint>
					</monogr>
					<idno type="MD5">CEB4D74B9EDEADA4DB84FF4B2FB3A6BF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hummingbird submitted ranked result sets for the Main Web Task (10GB of web data) and Large Web Task (100GB) of the TREC-9 Web Track, and for Stage 2 of the TREC-9 Query Track (43 variations of 50 queries). SearchServer's Intuitive Searching produced the highest Precision@5 score (averaged over 50 web queries) of all Title-only runs submitted to the Main Web Task. SearchServer's approximate text searching and linguistic expansion each increased average precision for web queries by 5%. Enabling SearchServer's document length normalization increased average precision for web queries by 10-30% and for long queries by 100%. Squaring the importance of the inverse document frequency (relevance method 'V2:4') increased average precision in the query track by 5%. Blind query expansion decreased average precision of highly relevants for web queries by almost 15%; the same method was neutral when counting all relevants the same.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hummingbird's Fulcrum SearchServer kernel is an indexing, search and retrieval engine which runs on Windows and UNIX platforms. SearchServer, originally a product of Fulcrum Technologies, was acquired by Hummingbird in 1999. The SearchServer kernel is embedded in <ref type="bibr" coords="1,366.28,408.76,4.98,8.96" target="#b5">5</ref> Hummingbird products, including SearchServer, an application toolkit used for knowledge-intensive applications that require fast access to unstructured information.</p><p>The SearchServer kernel supports a variation of the Structured Query Language (SQL), called SearchSQL, which has extensions for text retrieval. Almost 200 document formats are supported, such as Word, WordPerfect, PDF and HTML. Many character sets and languages are supported, including the major European languages, Japanese, Korean, Greek and Arabic. SearchServer's Intuitive Searching algorithms were updated for version 4.0 which shipped in Fall 1999, and in subsequent releases of other products. The next major kernel release works in Unicode internally and supports many more languages <ref type="bibr" coords="1,451.35,512.32,10.67,8.96" target="#b4">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>All experiments were conducted on a single-cpu desktop system, OTWEBTREC, with a 600MHz Pentium III cpu, 512MB RAM, 186GB of external disk space on one e: partition, and running Windows NT 4.0 Service Pack 6.</p><p>For most official TREC runs, an experimental version of SearchServer 5.0 was used (a different experimental version was used for the Query Track runs in September than the Web Track runs in July and 1 Core Technology, Research and Development, stephen.tomlinson@hummingbird.com August). Commercial release SearchServer 4.0 was used for one Main Web Task run and one Query Track run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Setup</head><p>We describe how SearchServer was used to handle the Main Web Task (10GB of web data) and Large Web Task (100GB) of the TREC-9 Web Track, and Stage 2 of the TREC-9 Query Track (1GB of news and government documents).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>The WT10g collection of the Main Web Task was distributed on 5 CDs. We copied the contents of each CD onto the OTWEBTREC e: drive (e:\data\wt10g\cd1 -e:\data\wt10g\cd5). The cd5\info subdirectory, containing supporting information not considered part of WT10g, was removed to ensure it wasn't indexed. The 5157 .gz files comprising WT10g were uncompressed. No further pre-processing was done on the data. Uncompressed, the 5157 files consist of 11,032,691,403 bytes (10.3GB), about 2MB each. Each file contains on average 328 "documents", for a total of 1,692,096 documents.</p><p>The WT100g collection of the Large Web Task was distributed on 2 DLT-4000 tapes. We copied the contents onto a "compressed NTFS" area of OTWEBTREC's e: drive (e:\data\compressed\wt100g). The BASE10 and BASE1 subsets are not considered part of WT100g and were stored elsewhere. We uncompressed the 50,023 files comprising WT100g from .gz format (and Windows NT internally recompressed them on the compressed NTFS drive), which took 5 hours. No further pre-processing was done on the data. Uncompressed, the 50,023 files consist of 107,828,665,842 bytes (100.4GB). Based on the change in bytes free on the drive, we estimate the files occupied about 58.8 billion bytes (54.8GB) on the compressed NTFS drive. Hence, NTFS compression saved about 46GB of space, poor compared to gzip compression (which saved 71GB), but still worthwhile. Each file contains on average 371 "documents", for a total of 18,571,671 documents. (For more information on the WT100g collection, see <ref type="bibr" coords="2,90.04,414.52,10.55,8.96" target="#b2">[3]</ref>.)</p><p>Text Research Collection Volume 1, Revised March 1994, more commonly known as "TREC Disk 1", was used in Stage 2 of the Query Track, and consists of a single CD. We copied its contents to e:\data\TREC\Vol1. The various README files and the DTD directory were removed because they are not considered part of the collection. The 1265 .Z files comprising the collection were uncompressed. No further pre-processing was done on the data. Uncompressed, the 1265 files consist of 1,265,137,373 bytes (1.2GB), about 1MB each. Each file contains on average 404 "documents", for a total of 510,637 documents. (For more information on the TREC Disk 1 collection, see <ref type="bibr" coords="2,377.26,506.55,15.05,8.96" target="#b11">[10]</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Reader</head><p>To index and retrieve data, SearchServer requires the data to be in Fulcrum Technologies Document Format (FTDF). SearchServer includes "text readers" for converting most popular formats (e.g. Word, WordPerfect, HTML, PDF, Excel, PowerPoint, etc.) to FTDF. A special class of text readers, "expansion" text readers, can insert a row into a SearchServer table for each logical "document" inside a container, such as directory or library file. Users can also write their own text readers in C for expanding proprietary container formats and converting proprietary data formats to FTDF.</p><p>The library files of WT10g and WT100g consisted of several logical documents, each starting with a &lt;DOC&gt; tag and ending with a &lt;/DOC&gt; tag. After the &lt;DOC&gt; tag, the unique id of the document, e.g. WTX104-B01-1, was included inside &lt;DOCNO&gt;..&lt;/DOCNO&gt; tags. Other HTTP header information, such as the URL of the document, appeared inside &lt;DOCHDR&gt;..&lt;/DOCHDR&gt; tags. The content of the web document started after the &lt;/DOCHDR&gt; tag and ended at the already-mentioned &lt;/DOC&gt; tag. Most document's content were HTML format because only documents with mime type "text/html" were included in the collections, but on the web, some servers mislabel binaries and other file types as text/html. We made no attempt to screen out such mislabeled documents.</p><p>We wrote a custom text reader called cTREC to handle expansion of the library files of the WT10g and WT100g collections and to make a few conversions to the HTML format.</p><p>In expansion mode (/E switch), cTREC scans the library file and for each logical document determines its start offset in the file (i.e. offset of &lt;DOC&gt; tag), its length in bytes (i.e., distance to &lt;/DOC&gt; tag), and extracts its document id (from inside &lt;DOCNO&gt;..&lt;/DOCNO&gt; tags). SearchServer is instructed to insert a row for each logical document. The filename column (FT_SFNAME) stores the library filename. The text reader column (FT_FLIST) includes the start offset and length for the logical document (e.g. cTREC/w/100000/30000). The document id column (controllable with the /d switch), contains the document id.</p><p>In web track format translation mode (/w switch), cTREC would insert control sequences around the header to turn off indexing (i.e. from &lt;DOC&gt; down to the &lt;/DOCHDR&gt; tag was not indexed). Indexing was also turned off around HTML tags, except for the content of META NAME/HTTP-EQUIV="DESCRIPTION/KEYWORDS/SUBJECT/TITLE" tags. Some entities were converted: the ones listed in the DTDs for the TREC disks 1-5, e.g. &amp;eacute; to e, and numeric entities, e.g. &amp;#233; to e. Because we knew the queries were all English, we didn't try to take advantage of SearchServer's rich character support capabilities, such as accent-indexing and recognition of semantically equivalent forms of Unicode.</p><p>The library files of TREC Disk 1 also consisted of several logical documents delineated by &lt;DOC&gt;..&lt;/DOC&gt; tags and identified by a &lt;DOCNO&gt;, so the cTREC /E switch also handled expansion of these files. When invoked without the /w or /E switch, cTREC assumes it is reading a document from TREC disks 1-5 and by default inserts control sequences to turn off indexing around all tags listed in the TREC disk 1-5 DTDs, and converts all entities listed in the DTDs. By default, cTREC also turns off indexing for data delineated by tags indicating keyword fields (namely IN, CO, G, GV, RE, MS, NS, DESCRIPT or SUBJECT tags) because the original TREC guidelines did not permit using those fields (a /k option exists for overriding this guideline). Some other tagged data is not indexed by default (nor with the /k option) because its content isn't considered helpful (for TREC Disk 1, data delineated by DOCNO, FIRST, SECOND, FILEID, NOTE, UNK, BYLINE, C, CODE-213, DOCID, NOTE, T2, T4, AUTHOR, DATE, SO, ADDRESS, AUTHOR and JOURNAL tags is not indexed by default; a longer list exists to cover the other disks). cTREC currently doesn't differentiate its tag handling by collection type; for example, the &lt;G&gt; tag is a keyword field in the Wall Street Journal documents, but not in the Federal Register documents, but cTREC treats it as a keyword field in both, a minor limitation. cTREC looks ahead at most 8000 bytes for an end tag when it encounters a tag indicating indexing should be turned off; if the end tag is not found, indexing is not turned off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Indexing</head><p>WT10g was indexed in one table in most runs, created with the following SearchSQL statement: The APPROX_ZONES '32' parameter specifies that an approximate search index should be built on the external text column (32). The STOPFILE parameter specified a file containing a list of 101 stopwords to not index, including all letters and single-digit numbers. The PERIODIC parameter prevents immediate indexing of rows at insertion time. The BASEPATH parameter specified the directory from which relative filenames of insert statements would be applied. The DOCNO column was assigned number 128 and a maximum length of 256 characters.</p><formula xml:id="formula_0" coords="3,126.04,605.62,161.97,7.83">CREATE SCHEMA WT10GW CREATE</formula><p>After creating the table, we added the string "IND:x16384;b4096;" to the wt10gw.cfg file to ensure an obscure internal dictionary limit wouldn't be encountered at index-time. This step is not necessary as of SearchServer 5.0 Beta2. The VALIDATE TABLE option of the VALIDATE INDEX statement causes SearchServer to review whether the contents of container rows, such as directory rows and library files, are correctly reflected in the table. In this particular case, SearchServer initially validated the directory row by inserting each of its sub-directories and files into the table. Then SearchServer validated each of those directory and library file rows in turn, etc. Validating library file rows invoked the cTREC text reader in expansion mode to insert a row for each logical document in the library file, including its document id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Into this</head><p>After validating the table, SearchServer indexed the table, in this case using up to 256MB of memory for sorting (as per the BUFFER parameter) and using temporary sort files of up to 2GB (as per the TEMP_FILE_SIZE parameter), to produce a dictionary of the unique words and reference file with the locations of the word occurrences (mostly unused in our experiments because we did no proximity searches nor search term highlighting). By default, SearchServer stores the original words, not just the stems.</p><p>For one of our Main Web Task runs (hum9td4), we used commercial release SearchServer 4.0, which is limited to 2GB reference files. Hence for that run we indexed WT10g in 2 tables. The first table contained CD1, CD2, and the first 4 directories of CD5 (WTX097-WTX100). The second table contained CD3, CD4 and the last 4 directories of CD5 (WTX101-WTX104).</p><p>Because of various internal limits in the experimental SearchServer version used for WT100g, we indexed WT100g in 12 tables (more than proved to be necessary). No approximate search index was built; however, some Large Web Task runs re-used the approximate search index of WT10g as a spell-corrector.</p><p>For the Query Track, we indexed TREC Disk 1 three times, once filtering keywords as per the traditional TREC guidelines, a second time with keyword fields included in the index, and a third time using commercial release SearchServer 4.0 (including keywords).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Search Techniques</head><p>For the Main Web Task, the 50 "topics" were in a file called "topics.451-500". The topics were numbered from 451-500, and each contained a Title (which was an actual web query taken from a search engine log), a Description (NIST's interpretation of the query, with spelling and grammar errors fixed), and a Narrative (a more detailed set of guidelines for what a relevant document should or should not contain).</p><p>For the Large Web Task, the 10000 web queries were in a file called "queries_10000". Queries were numbered from 20001-30000. There was no separate Title, Description or Narrative, just the original web queries, one per line.</p><p>For Stage 2 of the Query Track, there were 43 separate files of 50 queries each (variants of TREC topics 51-100).</p><p>We modified the example stsample.c program included with SearchServer to parse the TREC topics files, construct and execute corresponding SearchSQL queries, fetch the top 1000 or top 20 rows, and write out the rows in the results format requested by NIST. (The modified stsample.c was called QueryToRankings.c.) SELECT statements were issued with the SQLExecDirect api call. Fetches were done with SQLFetch (typically 1000 SQLFetch calls per query in the Main Web Task and Query Track, and 20 SQLFetch calls per query in the Large Web Task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Intuitive Searching</head><p>All queries used SearchServer's Intuitive Searching, i.e. the IS_ABOUT predicate of SearchSQL, which accepts unstructured text. For example, for topic 451 of the Main Web Task, the Title was "What is a Bengals cat?". A corresponding SearchSQL query would be:</p><formula xml:id="formula_1" coords="5,126.04,329.62,281.97,41.79">SELECT RELEVANCE('V2:3') AS REL, DOCNO FROM WT10GW WHERE FT_TEXT IS_ABOUT 'What is a Bengals cat?' ORDER BY REL DESC;</formula><p>This query would create a working table with the 2 columns named in the SELECT clause, a REL column containing the relevance value of the row for the query, and a DOCNO column containing the document's identifier. The ORDER BY clause specifies that the most relevant rows should be listed first. Typically a statement such as "SET MAX_SEARCH_ROWS 1000" was previously executed so that the working table would contain at most 1000 rows. In cases where the data was indexed in more than one table, the FROM clause would specify a UNION of the tables, e.g. SearchServer 4.0 queries contained "FROM WT10GW1 UNION WT10GW2".</p><p>Our QueryToRankings program removed a short list of words from the given topics before presenting them to SearchServer: "documents", "document", "items", "item", "relevant". This was originally done for internal TREC-5 experiments based on the TREC-5 topics frequently containing these words (e.g. "A relevant item will mention..."). It was found to make almost no difference to the scores whether these words were excluded or not, so we didn't bother to expand the list further for the TREC-9 Main Web Task. For the Large Web Task, in which most queries were known to be phrased as questions, we additionally removed the words "do", "does", "find", "how", "me", "show", "tell", "what" and "why".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Secondary Term Selection</head><p>The IS_ABOUT predicate by default expands each word to a "superterm" comprising all the linguistic variants of the term, e.g. "run" is added for "ran" (linguistic expansion can be disabled with the VECTOR_GENERATOR parameter). Some of these superterms may be subsequently discarded when searching the table (secondary term selection). For example, the RELEVANCE_METHOD setting has an optional document frequency parameter for discarding all terms which occur in more than a specified percentage of the rows (based on the most frequently occurring variant of the term). Secondary term selection improves performance and prevents highlighting of unimportant terms.</p><p>In the SearchServer 5.0 web track runs, we experimented with a different term selection approach based on an estimate of how many rows the terms would bring into the search and which involved a different formula for term importance which incorporated the vector length. In the SearchServer 4.0 runs, in which the experimental approach wasn't available, a document frequency cutoff of 15% was normally used, because that cutoff in past experiments didn't hurt quality, but significantly improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Statistical Relevance Ranking</head><p>To calculate a relevance value for a row of a table with respect to the vector of terms (actually, superterms) resulting from secondary term selection, the inverse document frequency of the term and the number of occurrences of the term in the row (term frequency) are determined from the index. The length of the row (based on the number of indexed characters in all columns of the row, which is typically dominated by the external document), is optionally incorporated, and the number of occurrences of the term in the vector is also used. The full details of synthesizing this information into a relevance value are proprietary, but draws from <ref type="bibr" coords="6,111.88,247.84,11.67,8.96">[7]</ref> (particularly the Okapi approach to term frequency dampening) and <ref type="bibr" coords="6,399.58,247.84,10.64,8.96" target="#b9">[9]</ref>. SearchServer's relevance values are always an integer in the range 0 to 1000.</p><p>SearchServer's RELEVANCE_METHOD setting can be used to optionally square the importance of the inverse document frequency (by choosing a RELEVANCE_METHOD of 'V2:4' instead of 'V2:3'). Experiments on past TREC ad hoc topics found that V2:4 often worked better than V2:3, but past TREC ad hoc topics didn't contain spelling errors, which could be over-emphasized when squaring the idfs.</p><p>SearchServer's RELEVANCE_DLEN_IMP parameter controls the importance of document length (scale of 0 to 1000) to the ranking. We found 200 worked best on TREC-8 small web experiments and used 200 for most submitted TREC-9 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approximate Text Searching</head><p>The Title-only queries of the Main Web Task and the queries of the Large Web Task were unedited web queries from search engine logs which appeared to sometimes contain spelling errors; for example, a query containing "vanila ice creem" probably meant to say "vanilla ice cream".</p><p>SearchServer's approximate text searching is based on edit distance, also known as the Levenshtein distance, which is the minimum number of insertions, deletions and/or replacements needed to transform one pattern into another. SearchServer's approximate text searching is fast for error ratios up to at least one-third, i.e. when the allowed edit distance is one-third the length of the search term. An excellent overview of approximate text searching techniques may be found in <ref type="bibr" coords="6,364.49,505.36,10.66,8.96" target="#b6">[6]</ref>.</p><p>We experimented with using SearchServer's approximate text searching to fix some spelling errors. The first step was to look up the closest matches of each term in the web query by increasing edit distance and decreasing number of rows containing the term. For example, the SearchSQL to find the closest matches to "vanila" is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SELECT TERM, FT_DISTANCE(TERM) AS NUMERRORS, MAX(ROW_COUNT) AS NUMROWS FROM SEARCH_TERMS WHERE TABLE_NAME CONTAINS 'WT10GW' AND COLUMN_NAME CONTAINS 'FT_TEXT' AND TERM CONTAINS 'vanila' BEST_MATCHES 1000 GROUP BY TERM ORDER BY NUMERRORS, NUMROWS DESC;</head><p>The results in the case of "vanila" were ('VANILA', 0, 8) ('VANILLA', 1, 2427) ('MANILA', 1, 1763) ('VANIA', 1, 47) ('VANITA', 1, 21) ('VAXILA', 1, 11) ('DANILA', 1, 10) ('VANINA', 1, 9) ('VANNILA', 1, 5) ... We used the default error ratio of 34% for all approximate searches, e.g. 2 errors were allowed in the 6letter vanila query. The WT10G collection contained 5,792,772 distinct words. SearchServer used an inmemory approximate search index to substantially reduce the time needed to find the close matches.</p><p>If the original term appeared in fewer than 10 rows, then the closest matches were added to the query until the sum of the row counts was 10 or more, with the following adjustments:</p><p>• On the first pass through the list of close matches, only matches with the same Soundex code <ref type="bibr" coords="7,483.34,314.44,11.68,8.96" target="#b5">[5]</ref> were considered. For example, "vanilla" has the same Soundex code as "vanila", but "manila" does not. If the Soundex matches didn't sum to 10 or more rows, then the closest non-Soundex matches were added until the sum was 10 or more rows. (Note: Soundex was implemented in the QueryToRankings program, not SearchServer.)</p><p>• If the row sum was still less than 10 after adding all close matches, the last character of the term was dropped, and the process repeated. For example, after "vanila", the next search would be for "vanil", then "vani", etc.</p><p>In the case of "vanila", the term occurred in just 8 rows, fewer than 10 , so the next term, "vanilla" was considered. It was a Soundex match, so it was added to the query, and adding its row count of 2427 exceeded the sum requirement, so the search for more close matches ended.</p><p>In the case of "creem", the heuristic didn't work because "creem" appeared in 43 rows, more than our arbitrary parameter of 10. In retrospect, we probably should have added the first Soundex match which occurred in more rows than the original term (if any), making an arbitrary parameter unnecessary. The closest match to "creem" is "creek" (17,521 rows), which Soundex would filter out. The next closest match is "cream" (10,692 rows), which is the term we wanted. Also, it may be better to just sort close matches by decreasing number of rows and not differentiate by edit distance. This change would have properly handled the case of "australai" (4 rows); our heuristic generated "australi" (99 rows, 1 difference), but probably the best match was "australia" (75,297 rows, 2 differences).</p><p>The truncation heuristic was an attempt to deal with words stuck together, e.g. "londonengland", but it wasn't very successful. A better solution may be to include phrases in the word list.</p><p>We kept the original terms in the query, e.g. the query "vanila ice cream" was changed to "vanila ice cream vanilla". A downside of this approach is that the ranking algorithm treats vanila and vanilla as separate terms, hence over-weighting documents which happen to contain both terms. If we integrated this approach into SearchServer, we could treat the terms as variants of one term, like we do for linguistic variations of the term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Row Expansion</head><p>In past TRECs, "query expansion" was considered necessary to produce top results <ref type="bibr" coords="8,424.50,99.52,15.37,8.96" target="#b13">[11]</ref>. We experimented with using row expansion to indirectly expand the query in 2 of our Main Web Task submissions. The approach was built on top of SearchServer. An optimized version may be implemented inside SearchServer in a future release.</p><p>After running the initial query (possibly already expanded by approximate text searching in the Title-only case), we retrieved the top 1000 rows (unless SearchServer returned fewer, which sometimes happened for Title-only queries). For each of the top 5 rows, we asked SearchServer's Intuitive Searching to "find more rows like this" (we call these row expansion queries), retrieving the top 1000 rows. We then combined the relevance values from each of the 6 result sets, giving a weight of 5 to the original query results, and weights of 1 to each of the 5 row expansion results. We did not include any negative information.</p><p>Mathematically, this approach works out to be similar to Rocchio expansion. (A detailed description of a good Rocchio feedback technique is in <ref type="bibr" coords="8,247.73,237.52,10.54,8.96" target="#b0">[1]</ref>.)</p><p>We always used the same parameters in row expansion queries as were used in the initial query, e.g. the same document length importance. We used the top 5 rows because in experiments on the TREC-8 small web we found somewhere from 3 to 8 rows usually gave best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The evaluation measures are explained in an appendix of the conference proceedings. Briefly: Precision is the percentage of retrieved documents which are relevant. Precision@n is the precision after n documents have been retrieved. Average precision for a topic is the average of the precision after each relevant document is retrieved (using zero as the precision for relevant documents which are not retrieved). Recall is the percentage of relevant documents which have been retrieved. Interpolated precision at a particular recall level for a topic is the maximum precision achieved for the topic at that or any higher recall level. For a set of topics, the measure is the average of the measure for each topic (i.e. all topics are weighted equally).</p><p>Below we present an analysis of our results, including results of several unofficial "diagnostic" runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Web Task</head><p>The Main Web Task was to run 50 queries against 10GB of web data and submit a list of the top-1000 ranked documents to NIST for judging.</p><p>Topics were broken into 3 categories: automatic runs which used only the original Excite web query (called Title-only runs), automatic runs which used some other part of the topic statement (called Full Topic runs), and manual runs. We did not submit any manual runs, just automatic runs.</p><p>NIST produced a "qrels" file: a list of documents judged to be highly relevant, relevant or not relevant for each topic. From these, the scores were calculated with Chris Buckley's trec_eval program, which counts all relevants the same, including highly relevants. To produce scores which just counted highly relevants as relevant, we ran trec_eval a 2 nd time on a modified version of the qrels file which had the ordinary relevants filtered out, then multiplied by 50/46 (in 4 of the 50 topics, there were no highly relevants).</p><p>Hence the scores focused on highly relevants are averaged over just 46 topics.</p><p>The medians were derived from the statistics provided in the draft conference proceedings at the conference, which counted all relevants the same. For the Title-only category, the medians are the 9 thhighest score of the 18 groups, just counting the highest score from each group in each measure. For the Full Topic category, the median is the 10 th -highest score of the 19 groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Title-only runs</head><p>Table <ref type="table" coords="9,115.38,367.36,4.98,8.96" target="#tab_4">2</ref> shows Title-only runs produced with the experimental SearchServer 5.0 in July 2000. The only differences between these runs were the relevance method (V2:3 or V2:4) and whether or not row expansion post-processing was applied. Run 2b was the official "hum9te" run, which had the highest Precision@5 score and highest interpolated precision at the 10%, 20%, 30% and 40% recall levels of the 40 submitted Title-only runs from 18 groups:</p><p>SearchServer run AvgP P@5 P@10 P@20 Rec0 Rec30 AvgH H@5 H0 2a: V2:3 AvgP: Average Precision (defined above) P@5, P@10, P@20: Precision after 5, 10 and 20 documents retrieved, respectively Rec0, Rec30: Interpolated Precision at 0% and 30% Recall, respectively AvgH: Average Precision just counting Highly Relevants as relevant H@5: P@5 just counting Highly Relevants as relevant H0: Rec0 just counting Highly Relevants as relevant Impact of Relevance Method (compare 2a to 2c, or 2b to 2d): V2:3 was modestly better at finding relevant documents (columns AvgP through Rec30), but V2:4 was modestly better at finding highly relevant documents (columns AvgH through H0, except in 1 H0 case).</p><p>Impact of Row Expansion (compare 2a to 2b, or 2c to 2d): Our experimental row expansion postprocessing made little difference for relevants, but hurt the scores when focusing on highly relevants. Perhaps the finding of past TRECs that query expansion is usually necessary for top results is not valid when just focusing on highly relevants. However, the results of many groups will have to be considered, not just ours.</p><p>To measure the impact of approximate text searching and linguistic expansion, Table <ref type="table" coords="10,433.26,189.16,4.98,8.96" target="#tab_5">3</ref> shows runs which were done in January 2001 with a more recent SearchServer build (5.0.500.14). This version contained a new linguistic package and did not use the experimental secondary term selection (instead, terms in more than 15% of documents were discarded (relevance method V2:3:15)). No row expansion was done, and document length importance was increased to 750: SearchServer Run AvgP P@5 P@10 P@20 Rec0 Impact of Approximate Text Searching (compare 3a to 3b, or 3d to 3c): The spell-correction heuristics increased most precision scores by just 1-2 points. Almost all of the improvement was in 2 topics: 487 ("angioplast7", for which "angioplasty" was added) and 463 ("tartin", for which "tartan" was added). 2 topics became a little worse, 481 and 495, because "1920" was unnecessarily added for "1920's", apparently over-weighting that term.</p><p>Impact of Linguistic Expansion (compare 3c to 3b, or 3d to 3a): Linguistic expansion improved average precision, but slightly lowered Precision@10. In average precision, topic 469 was helped ("steinbach nutcracker") as was 490 ("motorcycle safety helmets"), but topic 492 was hurt ("us savings bonds") as was 458 ("fasting"). Note that all topics were English. SearchServer's linguistic expansion is likely to be more useful for languages with more noun forms, such as German and Finnish.</p><p>Table <ref type="table" coords="10,115.35,494.92,4.98,8.96" target="#tab_6">4</ref> shows additional runs just varying in the setting of document length importance (RELEVANCE_DLEN_IMP parameter). These runs used build 5.0.500.14, approximate text searching and linguistic expansion were both on, and the relevance method was 'V2:4:15': DLen Importance AvgP P@5 P@10 P@20 Rec0 Impact of Document Length Normalization: Ignoring document length (row 4a) hurt all scores; average precision was 10-30% higher in the other rows. The impact on highly relevants was even larger. Generally, we find that settings of 200 or higher all work pretty well for average precision, but higher settings appear to be better for finding highly relevants. The setting of 750 was probably the best overall in Table <ref type="table" coords="11,115.37,120.16,3.76,8.96" target="#tab_6">4</ref>. See Table <ref type="table" coords="11,170.12,120.16,4.98,8.96" target="#tab_9">6</ref> for another document length experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Full Topic runs</head><p>The other category for automatic runs were runs which included any part of the topic besides the Title field. The median precision scores were higher for this category, as were SearchServer's scores, which makes sense because the queries had the more detailed Description and/or Narrative fields included.</p><p>Table <ref type="table" coords="11,115.37,224.80,4.98,8.96" target="#tab_7">5</ref> shows Full Topic runs produced in July 2000. The differences between these runs were the relevance method (V2:3 or V2:4), whether or not row expansion post-processing was applied, whether or not commercial release SearchServer 4.0 was used, and whether or not the Narrative was included. Runs 5b, 5c and 5h were submitted (official runs "hum9tde", "hum9td4" and "hum9tdn" respectively). All runs were above the median in average precision:</p><p>SearchServer run AvgP P@5 P@10 P@20 Rec0 Impact of Row Expansion (compare 5a to 5b, or 5e to 5f): Row expansion post-processing hurt all scores, especially for highly relevants, as in the Title-only case.</p><p>Impact of Relevance Method (compare 5a to 5e, 5b to 5f, 5c to 5g, or 5d to 5h): More often than not, V2:4 was a little better, including for highly relevants, but it made little difference.</p><p>Impact of including the Narrative (compare 5a to 5d, or 5e to 5h): Including the Narrative hurt average precision scores. It increased relevants early in the result list, but not highly relevants.</p><p>Difference from SearchServer 4.0 (compare 5a to 5c, or 5e to 5g): SearchServer 4.0, which split the data into 2 tables and used the simpler secondary term selection, produced scores which were a little lower than SearchServer 5.0's with V2:3, and about the same with V2:4.</p><p>Table <ref type="table" coords="11,115.37,619.36,4.98,8.96" target="#tab_9">6</ref> shows a dramatic result when re-doing the runs of Impact of Document Length Normalization: Ignoring the document length (row 6a) significantly hurt all scores; average precision was about 100% higher in the other rows. Many irrelevant long documents (e.g. 500KB or more) were brought into the search result by the numerous, relatively unimportant terms in the long queries when there was no document length adjustment. It appears that even a low setting, such as 250, was enough to overcome the issue. As in the Title-only case, the impact was even larger for highly relevants, and higher settings were better. Again, probably 750 was the best setting overall in Table <ref type="table" coords="12,492.81,254.20,3.76,8.96" target="#tab_9">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Large Web Task</head><p>Results of our Large Web Task runs are summarized in The use of approximate text searching for handling misspelled terms (used in hum9w1 but not hum9w2, the only difference) improved most precision scores by 1 point. The benefit primarily came from query 28616 ("where can i find a good deal on a mothermoard") for which the term "motherboard" was helpfully added.</p><p>Turning off document length normalization and linguistic expansion (as done in hum9w3, the only differences from hum9w1) lowered precision scores by just 2-3 points. This result is in line with what one would expect from the Main Web Title-only findings for Precision@5 and Precision@10, which suggest that removing the document length adjustment hurt 3-4 points, but disabling linguistics helped 0-1 points.</p><p>Unfortunately, the pool of documents submitted per topic is too small for this task for us to run meaningful experiments on isolated factors after the fact like we could for the Main Web, e.g. perhaps the individual impact of document length and linguistics is actually higher in this task.</p><p>We divided WT100g into 12 tables, each with their own set of inverse document frequencies. This need not lower the scores: AT&amp;T found that Precision@10 scores were actually a little higher when they split WT100g into 20 tables in TREC-8 (see <ref type="bibr" coords="12,249.70,607.48,10.55,8.96">[8]</ref>). However, our preliminary experiments with global idfs suggest that the table-splitting may have cost us several points of precision; e.g. with global idfs, we get 30% in precision@10 with 30% unjudged, and many of the unjudged appear to be satisfactory. Our experimental secondary term selection was set more aggressively for this task than in the Main Web, and there would have been some inconsistencies in the terms discarded for different tables in our official runs.</p><p>For query 27028 ("s3 patches"), we regret that 's' and '3' were stop words. Perhaps we should enable SearchServer's option of parsing numbers as if they were letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Query Track</head><p>The Query Track is for evaluating not just retrieval systems, but the effect of query variations on such systems. For background on this track, see <ref type="bibr" coords="13,264.90,157.00,10.67,8.96" target="#b1">[2]</ref>.</p><p>In Stage 1 of the Query Track, participants created variations of old TREC topics 51-100, including very short queries (2-3 words), natural language sentence queries, and queries based on reading system results without consulting the original topics. In all, 43 sets of 50 queries were produced by 6 different groups.</p><p>We did not submit any queries for Stage 1.</p><p>In Stage 2, all groups, including those which did not submit queries, were asked to run all the query sets on their systems. The more systems, the more reliable the conclusions about varying queries. We contributed 7 runs. The overall average precision scores for each of these runs (averaged over all 43*50 queries) are in These results suggest that excluding keyword fields makes little difference. Decreasing the document length importance was modestly helpful. SearchServer's relevance method 'V2:4', which squared the importance of the inverse document frequency, produced modestly better results. The attempt at handling spelling errors was of only minor benefit, though probably relatively few queries contained spelling errors (compared to the web queries). The experimental SearchServer 5.0 scores were slightly higher than those of SearchServer 4.0, despite some known glitches in the new linguistic package, such as expanding "in" to "Indiana" (since ironed out).</p><p>Perhaps the most interesting result is that excluding terms which occur in more than 15% of the documents, which helps search-time performance, didn't decrease average precision significantly. This result is consistent with other experiments we have done, but differs from the finding reported in Managing Gigabytes that discarding frequent terms "greatly reduces retrieval effectiveness" <ref type="bibr" coords="13,417.57,625.48,16.67,8.96" target="#b15">[12]</ref> (page 427).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,126.04,605.62,353.97,30.39"><head>TABLE WT10GW</head><label>WT10GW</label><figDesc></figDesc><table coords="3,126.04,616.89,353.97,19.11"><row><cell>(DOCNO VARCHAR(256) 128) PERIODIC</cell></row><row><cell>BASEPATH 'E:\DATA' STOPFILE 'MYTREC.STP' APPROX_ZONES '32';</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.04,166.12,425.43,99.45"><head></head><label></label><figDesc>table, we just inserted one row, specifying the top directory of WT10g, with this Insert statement:</figDesc><table coords="4,90.04,189.33,329.97,76.23"><row><cell>INSERT INTO WT10GW ( FT_SFNAME, FT_FLIST )</cell></row><row><cell>VALUES ( 'WT10G', 'cTREC/E/d=128:s!cTREC/w/@:s');</cell></row><row><cell>To index the table, we just executed this Validate Index statement:</cell></row><row><cell>VALIDATE INDEX WT10GW VALIDATE TABLE</cell></row><row><cell>TEMP_FILE_SIZE 2000000000 BUFFER 256000000;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,90.04,449.08,429.98,168.45"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="8,90.04,485.44,429.62,132.09"><row><cell>Run</cell><cell cols="7">hum9te hum9tde hum9td4 hum9tdn hum9w1 hum9w2 hum9w3</cell></row><row><cell>Task</cell><cell>Main</cell><cell>Main</cell><cell>Main</cell><cell>Main</cell><cell>Large</cell><cell>Large</cell><cell>Large</cell></row><row><cell>Topic Fields</cell><cell>T-only</cell><cell>T+D</cell><cell>T+D</cell><cell>T+D+N</cell><cell>web</cell><cell>web</cell><cell>web</cell></row><row><cell>SearchServer Ver.</cell><cell>5.0Tr1 2</cell><cell>5.0Tr1</cell><cell>4.0</cell><cell>5.0Tr1</cell><cell>5.0Tr1</cell><cell>5.0Tr1</cell><cell>5.0Tr1</cell></row><row><cell>Approx. Search</cell><cell>Y</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>Y</cell><cell>N</cell><cell>Y</cell></row><row><cell>Row Expansion</cell><cell>Y</cell><cell>Y</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell></row><row><cell>Linguistic Exp.</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>N</cell></row><row><cell>REL...METHOD</cell><cell>V2:3</cell><cell>V2:3</cell><cell>V2:3:15</cell><cell>V2:4</cell><cell>V2:3</cell><cell>V2:3</cell><cell>V2:3</cell></row><row><cell>REL...DLEN_IMP</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>0</cell></row><row><cell>REL...AVG_DLEN</cell><cell>10000</cell><cell>10000</cell><cell>10000</cell><cell>10000</cell><cell>10000</cell><cell>10000</cell><cell>n/a</cell></row><row><cell cols="2">Exp'l Sec. Term Sel. Y</cell><cell>Y</cell><cell>N</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell></row></table><note coords="8,97.57,460.60,333.71,8.96"><p>summarizes the "official" web track runs we submitted for judging in August 2000:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,90.04,633.04,356.99,50.60"><head>Table 1 : Summary of Runs submitted for the TREC-9 Web Track</head><label>1</label><figDesc></figDesc><table /><note coords="8,90.04,672.49,3.24,5.83;8,95.80,674.68,209.06,8.96"><p><p>2 </p>Build 5.0.0.61 with experimental changes for TREC</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,90.04,448.93,429.84,109.67"><head>Table 2 : Precision of Title-only runs Glossary:</head><label>2</label><figDesc></figDesc><table coords="9,90.04,448.93,429.84,73.91"><row><cell>3</cell><cell cols="3">0.1949 32.0% 26.2% 22.0% 0.5223 0.2696 0.1948 15.7% 0.3264</cell></row><row><cell>2b: V2:3 + exp</cell><cell cols="3">0.1970 32.4% 25.4% 21.5% 0.4802 0.2808 0.1703 13.5% 0.2732</cell></row><row><cell>2c: V2:4</cell><cell cols="3">0.1931 29.6% 25.0% 21.5% 0.5170 0.2674 0.2078 15.7% 0.3441</cell></row><row><cell>2d: V2:4 + exp</cell><cell cols="3">0.1909 29.6% 23.8% 21.1% 0.4756 0.2774 0.1778 14.8% 0.2618</cell></row><row><cell cols="2">Median (18 grps) 0.1464 21.6% 21.2% 17.4% 0.4015 0.1993 n/a</cell><cell>n/a</cell><cell>n/a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,90.04,260.08,429.83,83.01"><head>Table 3 : Impact of Approximate Text Searching and Linguistic Expansion (Title-only runs)</head><label>3</label><figDesc></figDesc><table coords="10,371.99,260.08,133.12,8.96"><row><cell>Rec30 AvgH</cell><cell>H@5</cell><cell>H0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,90.04,542.80,429.83,95.25"><head>Table 4 : Impact of Document Length Normalization (Title-only runs)</head><label>4</label><figDesc></figDesc><table coords="10,371.99,542.80,133.12,8.96"><row><cell>Rec30 AvgH</cell><cell>H@5</cell><cell>H0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,90.04,295.72,429.81,159.20"><head>Table 5 : Precision of Full Topic runs</head><label>5</label><figDesc></figDesc><table coords="11,371.99,295.72,133.12,8.96"><row><cell>Rec30 AvgH</cell><cell>H@5</cell><cell>H0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,90.04,619.36,385.08,20.48"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="11,358.10,619.36,117.02,8.96"><row><cell>(RELEVANCE_DLEN_IMP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="12,162.88,173.92,286.42,8.96"><head>Table 6 : Impact of Document Length Normalization (T+D+N runs)</head><label>6</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="12,90.04,313.96,400.22,94.28"><head>Table 7 .</head><label>7</label><figDesc>Only 84 of the 10000 queries were judged, and only the top 10 documents submitted for each query were judged:</figDesc><table coords="12,90.04,350.32,376.93,57.92"><row><cell>SearchServer</cell><cell>Reciprocal Rank of</cell><cell cols="3">Precision@1 Precision@5 Precision@10</cell></row><row><cell>Run</cell><cell>First Satisfactory</cell><cell></cell><cell></cell><cell></cell></row><row><cell>hum9w1</cell><cell>0.4381</cell><cell>30.95%</cell><cell>32.62%</cell><cell>32.50%</cell></row><row><cell>hum9w2</cell><cell>0.4262</cell><cell>30.95%</cell><cell>31.43%</cell><cell>31.67%</cell></row><row><cell>hum9w3</cell><cell>0.4174</cell><cell>28.57%</cell><cell>30.24%</cell><cell>29.40%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="12,214.00,423.64,184.11,8.96"><head>Table 7 : Precision of Large Web Task runs</head><label>7</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="13,90.04,271.96,424.53,188.84"><head>Table 8 :</head><label>8</label><figDesc>RunAvgP Experiment (i.e. what was different from baseline) humB* 0.1732 baseline humK* 0.1713 keyword fields were not indexed (/k option of cTREC text reader was not used, see section 3.2) humD* 0.1771 4 document length importance was set low (RELEVANCE_DLEN_IMP was set to 200 (baseline was 750)) humV* 0.1648 inverse document frequency not squared (RELEVANCE_METHOD was 'V2:3:15' (baseline was 'V2:4:15')) humA* 0.1741 approximate text searching added fixes for spelling errors (algorithm of section 4.2 except the table used to index TREC Disk 1 with keywords was used)</figDesc><table coords="13,90.04,416.56,417.65,44.24"><row><cell>hum4*</cell><cell>0.1713</cell><cell>SearchServer 4.0 was used (baseline used experimental SS 5.0 which contained a</cell></row><row><cell></cell><cell></cell><cell>new linguistic expansion package which was known to still have a few glitches)</cell></row><row><cell>humI*</cell><cell>0.1736</cell><cell>terms in more than 15% of rows not discarded</cell></row><row><cell></cell><cell></cell><cell>(RELEVANCE_METHOD was 'V2:4:100' (baseline was 'V2:4:15'))</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="13,202.84,476.32,206.33,8.96"><head>Table 8 : Average Precision of Query Track runs</head><label>8</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="9,95.80,663.16,408.39,8.96;9,90.04,674.68,389.35,8.96"><p>Scores for diagnostic runs may differ slightly from those given in the notebook paper because, for this paper, ties in relevance values were broken according to SearchServer's ordering in the result list.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="13,95.80,674.68,370.61,8.96"><p>We received corrections to the humD and humV results after submitting the notebook paper.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,104.25,101.80,416.62,8.96;14,90.04,113.32,426.71,8.96;14,90.04,124.72,392.52,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,302.93,101.80,217.94,8.96;14,90.04,113.32,31.83,8.96">Using Query Zoning and Correlation Within SMART: TREC 5</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec5/t5_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,310.60,113.32,206.15,8.96;14,90.04,124.72,37.20,8.96">Proceedings of the Fifth Text REtrieval Conference (TREC-5)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Fifth Text REtrieval Conference (TREC-5)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,104.25,147.76,388.98,8.96;14,90.04,159.28,418.03,8.96;14,90.04,170.80,222.40,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,232.74,147.76,103.46,8.96">The TREC-8 Query Track</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janet</forename><surname>Walz</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec8/t8_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,122.32,159.28,252.87,8.96">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,104.25,193.72,416.74,8.96;14,90.04,205.24,25.91,8.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="14,327.05,193.72,193.95,8.96;14,90.04,205.24,21.59,8.96">Overview of the TREC-7 Very Large Collection Track</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thistlewaite</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,121.02,205.24,400.07,8.96;14,90.04,216.76,392.52,8.96" xml:id="b3">
	<analytic>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>In</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Harman</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec7/t7_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,303.28,205.24,217.81,8.96;14,90.04,216.76,37.20,8.96">Proceedings of the Seventh Text REtrieval Conference (TREC-7)</title>
		<meeting>the Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,104.29,239.80,391.96,8.96;14,90.04,251.32,261.71,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,182.30,239.80,201.41,8.96">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,404.20,239.80,92.05,8.96;14,90.04,251.32,80.63,8.96">Sixteenth International Unicode Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,104.26,274.24,385.52,8.96;14,90.04,285.76,207.87,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,357.24,274.24,132.55,8.96;14,90.04,285.76,30.31,8.96">Sorting &amp; Searching, 2nd edition Revised</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,179.62,274.24,139.92,8.96">The Art of Computer Programming</title>
		<imprint>
			<publisher>Addison Wesley Longman</publisher>
			<date type="published" when="1998-01">January 1998</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,104.26,308.80,416.02,8.96;14,90.04,320.20,92.85,8.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="14,180.91,308.80,114.08,8.96">Approximate Text Searching</title>
		<author>
			<persName coords=""><forename type="first">Gonzalo</forename><surname>Navarro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-12">December 1998</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Chile</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct coords="14,104.25,343.24,408.13,8.96;14,90.04,354.76,415.64,8.96;14,90.04,366.28,320.72,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,487.92,343.24,24.46,8.96;14,90.04,354.76,40.90,8.96">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec3/t3_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,237.04,354.76,236.77,8.96">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="500" to="226" />
		</imprint>
		<respStmt>
			<orgName>City University.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,104.23,389.20,412.63,8.96;14,90.04,400.72,425.60,8.96;14,90.04,412.24,236.13,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,90.04,400.72,70.67,8.96">AT&amp;T at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Abney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,350.80,400.72,164.84,8.96;14,90.04,412.24,85.80,8.96">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,104.31,435.28,402.05,8.96;14,90.04,446.80,422.86,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,419.71,435.28,70.54,8.96">AT&amp;T at TREC-7</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,261.64,446.80,251.26,8.96">Proceedings of the Seventh Text REtrieval Conference (TREC</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Seventh Text REtrieval Conference (TREC</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.22,458.20,354.08,8.96" xml:id="b10">
	<analytic>
		<title/>
		<ptr target="http://trec.nist.gov/pubs/trec7/t7_proceedings.html" />
	</analytic>
	<monogr>
		<title level="j" coord="14,105.87,458.20,102.38,8.96">NIST Special Publication</title>
		<imprint>
			<biblScope unit="page" from="500" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,109.36,481.24,403.10,8.96" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<title level="m" coord="14,273.30,481.24,239.16,8.96">Overview of the Eighth Text REtrieval Conference (TREC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.27,492.76,401.87,8.96;14,90.04,504.28,392.52,8.96" xml:id="b12">
	<analytic>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>In</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Harman</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec8/t8_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,285.76,492.76,213.38,8.96;14,90.04,504.28,37.20,8.96">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,109.26,527.20,408.69,8.96" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<title level="m" coord="14,273.20,527.20,244.75,8.96">Overview of the Seventh Text REtrieval Conference (TREC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.27,538.72,408.79,8.96;14,90.04,550.24,392.52,8.96" xml:id="b14">
	<analytic>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>In</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Harman</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec7/t7_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,288.28,538.72,217.78,8.96;14,90.04,550.24,37.20,8.96">Proceedings of the Seventh Text REtrieval Conference (TREC-7)</title>
		<meeting>the Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,109.21,573.28,409.97,8.96;14,90.04,584.68,106.50,8.96;14,196.60,582.49,6.48,5.83;14,205.48,584.68,183.17,8.96;14,90.04,596.20,140.10,8.96" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="14,321.40,573.28,197.78,8.96;14,90.04,584.68,106.50,8.96;14,196.60,582.49,6.48,5.83;14,205.48,584.68,26.50,8.96">Managing Gigabytes: Compressing and Indexing Documents and Images. 2 nd edition</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>_</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.04,619.24,424.30,8.96;14,90.04,630.76,403.76,8.96" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="14,248.90,619.24,265.44,8.96;14,90.04,630.76,399.29,8.96">SearchSQL and Intuitive Searching are the intellectual property of Hummingbird Ltd. All other company and product names are trademarks of their respective owners</title>
		<author>
			<persName coords=""><forename type="first">Fulcrum</forename><surname>Hummingbird</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Searchserver</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
