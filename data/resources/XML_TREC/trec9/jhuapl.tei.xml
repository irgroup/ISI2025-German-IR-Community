<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,201.88,87.38,208.26,12.64">The HAIRCUT System at TREC-9</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,197.32,116.44,58.89,8.96"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>paul.mcnamee@jhuapl.edu</email>
						</author>
						<author>
							<persName coords="1,264.28,116.44,61.73,8.96"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<email>james.mayfield@jhuapl.edu</email>
						</author>
						<author>
							<persName coords="1,349.96,116.44,64.62,8.96"><forename type="first">Christine</forename><surname>Piatko</surname></persName>
							<email>christine.piatko@jhuapl.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road Laurel</addrLine>
									<postCode>20723-6099</postCode>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">The Hopkins Automated Information Retriever for Combing Unstructured Text</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University Applied Physics Laboratory (JHU/APL)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,201.88,87.38,208.26,12.64">The HAIRCUT System at TREC-9</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B623D12C8EFB945997D3A1D766234DC6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>HAIRCUT benefits from a basic design decision to support flexibility throughout the system. One specific example of this is the way we represent documents and queries; words, stemmed words, character n-grams, multiword phrases are all supported as indexing terms. This year we concentrated our efforts on two of the tasks in TREC-9, the main web task and cross-language retrieval in Chinese and English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small Web Task</head><p>For this task we indexed documents using two types of indexing terms, unstemmed words and character ngrams using n=6. Summary information of the two indices is shown in Table <ref type="table" coords="1,186.40,457.72,3.76,8.96">1</ref>. The difference in the number of documents is likely attributable to a few documents that contain a single short word from which no six character sequence can be formed. Note that the use of 6-grams greatly increased both the size of the dictionary and the size of the index files. No attempt was made compress our data structures and reduce the amount of disk space required although such techniques have been successful with both words <ref type="bibr" coords="1,183.28,558.16,16.76,8.96" target="#b9">[12]</ref> and n-grams <ref type="bibr" coords="1,255.04,558.16,15.43,8.96" target="#b7">[10]</ref>.</p><p>Each document was processed in the following fashion. First, we ignored HTML tags and used them only to delimit portions of text. Thus no special treatment was given for sectional tags such as &lt;TITLE&gt; or &lt;H1&gt; and both tags and their attribute values were eliminated from the token stream. The text was lowercased, punctuation was removed, and diacritical marks were retained. Tokens containing digits were preserved; however only the first two of a sequence of digits were retained (e.g., 1920 became 19##). The result is a stream of blank-separated words.</p><p>When using n-grams we construct indexing terms from the same sequence of words. These n-grams may span word boundaries; an attempt is made to discover sentence boundaries so that n-grams spanning sentence boundaries are not recorded. Thus n-grams with leading, central, or trailing spaces are formed at word boundaries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Queries were parsed in the same fashion as were documents with two exceptions. On some of our title only runs we attempted to correct the spelling of words that did not occur in our dictionary. Also, we tried to remove stop structure from the description and narrative sections of the queries using a list of about 1000 phrases constructed from past TREC topic statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># docs</head><p># terms index size words 1,588,374 3,019,547 2.96 GB 6-grams 1,588,169 19,209,934 36.0 GB Table <ref type="table" coords="1,349.36,453.16,3.76,8.96">1</ref>. Index statistics for the wt10g collection In all our experiments we used a linguistically motivated probabilistic model. This model, described in a report by Hiemstra and de Vries <ref type="bibr" coords="1,516.40,497.80,10.69,8.96" target="#b1">[2]</ref>, is essentially the same model that was used by BBN in TREC-7 <ref type="bibr" coords="1,364.84,520.12,10.69,8.96" target="#b6">[9]</ref>. The similarity calculation that is performed is:</p><formula xml:id="formula_0" coords="1,324.04,539.51,212.56,37.63">( ) ) , ( ) ( ) 1 ( ) , ( ) , ( q t f terms t t df d t f d q Sim ∏ = ⋅ - + ⋅ = α α Equation 1. Similarity calculation.</formula><p>where f(t,d) is the frequency of term t in document d and df(t) denotes the document frequency of t.</p><p>After the query is parsed each term is weighted by the query term frequency and an initial retrieval is performed followed by a single round of relevance feedback.</p><p>To perform relevance feedback we first retrieve the top 1000 documents. We use the top 20 documents for positive feedback and the bottom 75 documents for negative feedback; however duplicate or nearduplicate documents are removed from these sets. We then select terms for the expanded query. After retrieval using this expanded and reweighted query, we have found a slight improvement by penalizing document scores for documents missing many highly ranked query terms. We multiply document scores by a penalty factor: Equation 2. Penalty function for missing terms.</p><p>As can be seen in Several of our official runs were formed by merging baseline ranked lists of documents, for example, merging a word-based query and a 6-gram based query. We merged separate ranked lists by first normalizing document scores and then linearly combining values from different runs, an approach that was successful for us in TREC-8 <ref type="bibr" coords="2,223.48,376.96,10.69,8.96" target="#b4">[7]</ref>.</p><p>We conducted our work on a 4-node Sun Microsystems Ultra Enterprise 450 server. The workstation had 2.5 GB of physical memory and access to 100 GB of dedicated hard disk space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Official Results</head><p>For the most part we ignored the web-nature of the documents and relied on textual content to rank documents. We did however, try two techniques to boost our content-based runs. Both techniques were motivated by the track guidelines.</p><p>First, we attempted to exploit hyperlink structure and submitted two runs that used backlink frequency to rerank content-based runs. Secondly, we attempted to correct misspellings in title-only queries.</p><p>We submitted six official submissions in the small web track, four of the runs were solely based on document content and the other two were an attempt to utilize backlink frequency information to improve a content-based run. Three of our four content-based runs differ only in the selection of which parts of the topic statements were used. Thus apl9t, apl9td, and apl9tdn used the title, title and description, and title, description, and narrative sections, respectively. The fourth run, apl9all was a combination of the three other runs. A summary of each run's performance on the task is shown in Table <ref type="table" coords="2,388.36,53.68,3.77,8.96" target="#tab_2">3</ref> We were surprised by lower than expected results in the web task. During brief post-hoc analysis of our constituent runs we observed that relevance feedback had an adverse effect on our runs; rather than the 25-30% increase in average precision that we typically find, average precision decreased by roughly 10%. It will require further analysis to discover the cause for this phenomenon. We observe that the mean number of relevant documents per query, 52.3, is lower than past ad hoc TREC tracks and it is possible that this would reduce the benefit normally associated with automated relevance feedback. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naïve Use of Backlink Frequency</head><p>We made a simple attempt to incorporate link frequencies in our results. This was done in a very simple way -we multiplied a document's score in a content-based retrieval by a multiplicative factor derived from backlink frequency and resorted the retrieved documents. Comparing the results in Table <ref type="table" coords="3,206.20,42.64,4.98,8.96" target="#tab_2">3</ref> and Table <ref type="table" coords="3,260.44,42.64,3.77,8.96">4</ref>, it is clear that such a simple attempt to exploit backlink counts is insufficient.</p><p>avg prec change # best # ≥ median apl9lt 0.1062 -0.0210 0 25 apl9ltdn 0.1494 -0.0454 0 26 Table <ref type="table" coords="3,97.36,123.76,3.76,8.96">4</ref>. Link-influenced runs corresponding to apl9t and apl9tdn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use of Spelling Correction</head><p>If three or fewer documents in the TREC-8 collection contained a topic term, we attempted spelling correction on that term. First, we looked for words occurring in at least five documents that were one insertion, deletion, substitution, or transposition away from the misspelled word. If such a word was found, we used it in lieu of the misspelled word; if more than one such word was found, we selected the one that occurred most frequently (this led us to correct 'tartin' to 'martin' rather than 'tartan'). If no correction was found, we then tried to split the word into two pieces of three characters or more, each of which appeared in at least five TREC-8 documents. If no such pair was found, we left the word uncorrected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Language Task</head><p>The TREC-9 CLIR task consisted of bilingual retrieval of Chinese newspaper articles from English queries. A monolingual Chinese-Chinese run was also permitted. This was JHU/APL's first experience with Chinese document retrieval and we learned quite a lot from the experience. Undaunted by our inability to read Chinese, we attempted the task with only an English/Chinese parallel corpus and a minimal knowledge of the Big-5 encoding. Our CLIR experiments focused on two questions, namely, "How do 2-and 3-grams compare as indexing terms in unsegmented Chinese text?" and "Does query translation with parallel corpora perform on par with an available machine translation system?"</p><p>Philosophically, we desire to maximize crosslanguage performance using few language-specific resources. Although segmenters and dictionaries are available for a high-density language such as Chinese, many languages lack these tools. Additionally such resources are rarely in a standard format and the quality of the resource depends greatly on the source.</p><p>Though we did perform an experiment indexing only the raw bytes of the collection, on the whole it seemed better to process the Big-5 encoded documents on a character basis. The CJKV text by Ken Lunde was an invaluable aid in our software development <ref type="bibr" coords="3,380.08,198.76,10.69,8.96" target="#b3">[6]</ref>. We did not segment the text, and instead elected to index the documents using both 2and 3-grams. Nie and Ren have previously reported that 2-grams perform comparably with words on the TREC 5/6 Chinese collection and that a combination of both is best <ref type="bibr" coords="3,383.44,254.56,15.43,8.96" target="#b8">[11]</ref>. We wanted to assess the use of 3grams in a straight-up comparison with 2-grams.</p><p>We tried translating the topic statements in three different ways, two using a parallel corpus and one using an online machine translation tool. In our monolingual Chinese run we attempted to remove stop structure using translations of our English stop phrases. We used the same linguistically motivated probabilistic model that was used for our English web retrieval. Most of our official runs were produced by combining individual runs using both 2and 3-grams, an approach that as it turns out, depressed our results.</p><p># docs # terms index size 2-grams 127938 1974077 673 MB 3-grams 127938 15185076 959 MB Table <ref type="table" coords="3,349.36,457.48,3.76,8.96">6</ref>. Index statistics for the TREC-9 Chinese collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translation Using Hong Kong Parallel Corpora</head><p>About one month before the CLIR results were due at NIST we observed that we had no in-house method for translating English to Chinese. We quickly obtained two parallel English/Chinese collections from the Linguistic Data Consortium (LDC), the Hong Kong Laws Parallel Text collection [4] and the Hong Kong News Parallel Text collection <ref type="bibr" coords="4,242.80,136.12,10.69,8.96">[5]</ref>.</p><p>The Laws collection contains roughly 310,000 aligned sentences. The News collection contains roughly 18,000 aligned documents. Both collections are encoded in Big-5 which matches the encoding in the TREC-9 Chinese collection.</p><p>We built a hybrid collection from the Laws collection and from aligned sections of the News documents. We indexed the collection twice, both with 2-grams and 3-grams. Summary information about these two indices is shown in the following </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Official results</head><p>We submitted four official runs for the CLIR task, apl9xmon, apl9xtop, apl9xwrd, and apl9xcmb, that are described below. Each run is produced by combining multiple base runs. All of the base runs made use of relevance feedback. The number of expansion terms varied depending on the indexing terms; 100 expansion terms were used with the 2gram index and 400 terms were used with 3-grams.</p><p>Our only monolingual submission was apl9xmon. This run was produced by combining six base runs, title-only, title + description, and title + description + narrative, using both 2-and 3-grams.</p><p>Our first method for query translation followed the approach we used successfully in the CLEF-2000 evaluation <ref type="bibr" coords="4,121.00,577.96,10.69,8.96" target="#b5">[8]</ref>, namely, pre-translation expansion using highly ranked documents from a document collection in the same language as the source query followed by individual term translation using our parallel collection. Using this approach, the run, apl9xtop, was built from two base runs that were produced from 2-and 3-grams. The base runs used queries produced by expanding full topics from documents in the TREC-8 collection.</p><p>We were concerned that using the TREC-8 collection as an expansion collection might not be a good idea since it is not contemporaneous with the Chinese collection. We therefore tried a word-by-word translation of the topic statements, also using the parallel collection. The run apl9xwrd was produced by combining six base runs (2-, 3-grams; T, TD, TDN queries).</p><p>The final run, apl9xcmb, was simply a combination of all base runs used in apl9xtop, apl9xwrd, and the unofficial machine translation run, apl9xibm. We wanted to compare translation using our parallel collection to available machine translation. We were not in possession of Chinese MT software in-house so we relied on a web-based translation. The first operational web-based translation service we found was the IBM AlphaWorks server <ref type="bibr" coords="4,472.00,314.56,10.69,8.96" target="#b2">[3]</ref>. We had no previous experience with this service or knowledge of its methods or quality; we decided to use it solely based on convenience. The unofficial run, apl9xibm was produced from six base runs (2-, 3-grams; T, TD, TDN queries).</p><formula xml:id="formula_1" coords="4,380.92,165.76,14.46,8.96">avg</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing 2-grams and 3-grams</head><p>Our decision to submit combined runs using both 2and 3-grams was based on experience that shows benefit from a combination of multiple, reasonable quality results. As it turns out, our runs using 3-grams performed appreciably worse than those using 2grams.</p><p>Average precision and recall for the monolingual base runs used in apl9xmon are shown in Table <ref type="table" coords="4,359.68,486.16,3.77,8.96">9</ref>.</p><p>It seems clear that 2-grams are preferable to 3-grams, at least on a collection of this size. This trend seems to hold both in monolingual retrieval with natural language queries and in bilingual retrieval using word-based 'translations'. We created a post-hoc monolingual run using only the 2-grams and saw average precision increase from 0.3085 in apl9xmon to 0.3339, an 8.2% increase. A previous study by Chen et. al. <ref type="bibr" coords="5,214.60,42.64,10.69,8.96" target="#b0">[1]</ref>, examined the relative merits of 1-, 2-, and 3-grams (as well as several other methods of indexing) using the TREC-5 Chinese collection. Though the data, character encoding, and retrieval model differ from this present study, the relative performance between 2-grams and 3-grams is quite similar for several metrics. On automatic long queries they report average precision of 0.3677 for 2-grams and 0.2405 for 3-grams, a performance ratio of 1.529; from values in Table <ref type="table" coords="5,283.00,142.96,4.98,8.96">9</ref> we compute a comparable ratio of 1.408. Looking at relevant documents retrieved we report a ratio of 1.123 to their 1.162.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of Different Translation Schemes</head><p>Another thing we wanted to examine was the effect of using different query translation methods. Our three methods achieved similar performance. Rather than compare the combined runs, we instead look at the constituent base runs. The following tables reveal the performance achieved by each run and its relative performance to apl9xmon. For each strategy the best performance was observed when 2-grams were used on full-length topic statements. The performance achieved by each of the translation methods was very similar. The precision-recall graph in Figure <ref type="figure" coords="5,115.84,682.24,4.98,8.96" target="#fig_1">2</ref> shows the performance of each query translation scheme using 2-gram indexing and full topic statements. The graph shows that while the average precision using each method is nearly the same, the AlphaWorks translator performs slightly better at the high-precision part of the curve.</p><p>None of the bilingual runs achieves comparable performance to the monolingual run and our best official bilingual submission, aplxcmb only achieves performance of 49.4% of our official monolingual run, apl9xmon. This is significantly lower percentage than the 70-80% we obtained in our experiments with the CLEF-2000 workshop that was devoted to European languages <ref type="bibr" coords="5,407.68,165.28,10.69,8.96" target="#b5">[8]</ref>.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIR Results Using 2-gram Indexing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic CH73</head><formula xml:id="formula_2" coords="6,72.20,459.85,358.40,51.20">A P Y A ¡ ¢ £ ¤ ¥ ¦ § A¨© ª « ¬ Y ® ē°± ² ³ ´µ • ¶ e¹ º » ¼ ½ ¾ ¿ À Á Â Ã Ä xÅ AE Ç È DÉ Ê eË YÌ Í Î</formula><formula xml:id="formula_3" coords="6,355.30,199.95,151.20,155.00">¦ ¦ W E E @ 0 ` " ( ` ¨ E E X ` " 0 @¡ ¢ ¦£ (¤ ¥ ¦ § ©Ëª « @¬ 0 "® `¯°± ¨¬ ² ¨³ 0´µ ¶ • ¸¹ º 8» "¼ `½ "¾ (¼ `½ ¿ £À Á ©Â DÃ Ä "Å "AE ¦Ç XÈ É Ê Ë Ì EË Í Î Ï XÐ @Ñ 0Ð @Ò Ó Ô Õ Ö ©× EØ Ù ¨Ú ¦Û Ü "Ý (Þ ß à "á â ã ä Eã å $ae ¦ç Xè é (ê ¦ê ë "ì ¦ë Wç ¦é (í ë @î "ï ï î "ï ae ï î "ï ae ð 8ë ¨å Wñ Xê ò ó ô £ó õ ö X÷ $ø "ù ¨ú û ü ý Dþ ÿ ¡ £¢ ¥¤ §¦ ©¢ ý Dþ © ¥ §¦ £ ¦ ©!¢ " # $ %# '&amp; ( ¡) ©0 1 2 3 4 65 7 8 9 @ @ ¥A ©@ B C D %C E F HG PI Q PR S ¥T</formula><formula xml:id="formula_4" coords="6,355.10,415.65,136.70,220.30">} ~ ¡ H X 6 P © ¥ ¥ © % ¡ H c ! H § ¡ ©¢ ¥£ ¥¤ £¥ c¤ £¦ § ¨© %¨ª « ©¬ ® ©¯ °± %² ³ %µ ¶ §• !¸¹ %º ¹ %º '» ¼ ©½ ¡¾ ¿ ¾ ¼ £À Á ¿ ¥Â HÃ À ¥Â c¼ Ã Ä Å AE Ç È É Ê Ë !Ì HÍ Ì HÎ ¥Ì cÏ Ð Ñ Ò Ó Ô Ò Õ !Ö c× ¥Ø Ù Ú Û 6Ü ÞÝ ß à Há â ©ã cà £ä ¡â Xá å ae ç è ¡é ê ë §ì ¥í Pî ï £ð é %ê ñ óò ¥ô õ £ö ©÷ ø §ù ú !û ü ý þ 6ÿ ¡ ¢ ¤£ ¦¥ ¨ § © ¨ ¤£ ¡ ¦ ¦ "! $# % "&amp; ' ( ) ¡0 ¡1 2 %3 $4 65 87 9 @ A ¡B C D 6E ¦F %G $H ¦I ¤P Q RP RD S T U V W X `Y a 6Y $b dc ¦e gf h i p rq si t du $v "w gx yw r ¤ d d g ¤ ¤ ¦ ¨ 8d ye f g h i j k $l ¤m $n o ¡p q r s t `u v ¤w Rx ¤y $v ¨z { ¨| ¦{ ¤} { ¨| ¦{ ¨}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>This year we participated in two tracks that each presented new challenges.</p><p>In the small web task, we focused on content-based methods and tried two techniques to 'accommodate' the web-nature of the task. The first technique was a rudimentary use of backlink counts that proved too simplistic to be beneficial. The second technique, spell correcting misspelled short queries was generally beneficial, however it backfired in certain instances. We found automated relevance feedback to have a deleterious effect on our performance, a finding that warrants further investigation.</p><p>Though our team is experienced in cross-language retrieval, we had no experience in Asian language retrieval. We started the Chinese task with no ability to read Chinese and no language resources such as segmenters or dictionaries to draw on. Due to time constraints we were unable to make use of the TREC-5/6 training data and thus we entered the task relatively unprepared. We relied on our general experience using n-grams as indexing terms, a quickly acquired knowledge of the Big-5 encoding, and an English/Chinese parallel collection.</p><p>From our experience in the CLIR track we draw the following lessons. First, 2-grams are preferable to 3grams for indexing Chinese. We remain open to the possibility that other techniques may be better stillfor example, using both 2-grams and 3-grams, or 2grams and segmented words. Our second observation is that corpus-based translation is a viable alternative to extant machine translation software. However, our present results in English to Chinese, bilingual retrieval seem to fall well short of Chinese monolingual retrieval. Now that we have some experience in Chinese text retrieval and a training collection to draw from, we will endeavor to refine our methods to narrow this gap.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,324.04,522.40,221.85,8.96"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Adverse effects of blind relevance feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,324.04,409.60,36.21,8.96;5,378.04,409.60,146.91,8.96;5,324.04,431.92,215.94,8.96;5,324.04,443.08,83.61,8.96"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.Precision-recall curve for CLIR runs See the following page for an example of the query translations we used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,72.04,338.08,129.62,8.96;6,72.10,358.45,30.30,12.20;6,72.10,384.35,5.30,12.20;6,92.90,384.35,184.40,12.20;6,72.90,397.45,213.70,12.20;6,72.20,407.85,110.80,12.20;6,226.70,407.85,40.30,12.20;6,72.40,420.85,29.70,12.20;6,132.20,420.85,75.10,12.20;6,72.10,449.35,154.80,12.20;6,247.10,449.35,10.00,12.20;6,92.20,459.85,9.90,12.20"><head></head><label></label><figDesc>G H PI Q R PS T U V W YX `a b Pc ed Yf g h i p q r s t &amp;u ev Pu xw y e Y &amp; D Y e A e d e f g h ji lk nm om qp r s t u v ew yx ez Px x{ Y| D} Yẽ Y A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,38.68,658.72,533.91,8.96;6,38.68,669.88,533.81,8.96;6,38.68,681.04,533.86,8.96;6,38.68,692.20,533.84,8.96;6,38.68,703.36,533.86,8.96;6,38.68,715.36,97.89,8.96"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Two query translation methods are compared. The original English version of topic CH73 is shown along with the results of the IBM AlphaWorks translator.In the table on the right the query used in apl9xtop is partially displayed. The first column contains the best sixty terms produced by searching the TREC-8 ad hoc English documents using the official English version of topic CH73. The second column contains the top-ranked 2-gram extracted from our parallel collection; the third column contains the topranked 3-gram. During retrieval the top three 2-grams and the top 10 3-grams were used; however, only the top term is shown here due to space constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.04,196.36,216.03,77.36"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="2,72.04,196.36,216.03,77.36"><row><cell></cell><cell cols="2">, we use only about one-</cell></row><row><cell cols="3">fifth of the terms of the expanded query for this</cell></row><row><cell>penalty function</cell><cell></cell><cell></cell></row><row><cell cols="2"># Expansion Terms</cell><cell># Penalty terms</cell></row><row><cell>words</cell><cell>60</cell><cell>12</cell></row><row><cell>6-grams</cell><cell>400</cell><cell>75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,72.04,276.52,215.94,20.12"><head>Table 2 .</head><label>2</label><figDesc>Number of expansion terms and penalty terms by indexing scheme.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,324.04,53.68,219.93,91.28"><head>Table 3 .</head><label>3</label><figDesc>. Content-based runs for the Small Web task.</figDesc><table coords="2,332.92,74.42,198.18,58.78"><row><cell></cell><cell cols="4">avg prec recall # best # ≥ median</cell></row><row><cell>apl9t</cell><cell>0.1272</cell><cell>1276</cell><cell>0</cell><cell>28</cell></row><row><cell>apl9td</cell><cell>0.1917</cell><cell>1535</cell><cell>2</cell><cell>33</cell></row><row><cell cols="2">apl9tdn 0.1785</cell><cell>1584</cell><cell>1</cell><cell>32</cell></row><row><cell>apl9all</cell><cell>0.1948</cell><cell>1609</cell><cell>0</cell><cell>37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="2,86.92,142.58,488.55,558.47"><head>Effects of Automated Relevance Feedback</head><label></label><figDesc>The exact computation was:</figDesc><table coords="2,86.92,142.58,488.55,558.47"><row><cell>PF</cell><cell>=</cell><cell>0 . 1</cell><cell>-</cell><cell>   </cell><cell>total</cell><cell>in terms terms missing of number of #</cell><cell>query</cell><cell>1    </cell><cell>.</cell><cell>25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>0.3 0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall Level</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Word -RF (0.1648)</cell><cell>Word -No RF (0.1848)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Six -RF (0.1681)</cell><cell>Six -No RF (0.1904)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BLFactor</cell><cell>(</cell><cell>d</cell><cell>)</cell><cell>=</cell><cell>. 0</cell><cell>1</cell><cell>+</cell><cell>. 0</cell><cell>9</cell><cell>kCount d unt MaxBacklin backlinkco ) (</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Equation 3. MaxBacklinkCount is the number of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">documents that link to the most linked-to document.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,72.04,342.28,239.82,178.04"><head>Table 5 .</head><label>5</label><figDesc>The results of our attempts at spelling correction are shown in the following table: Impact of spelling correction.</figDesc><table coords="3,72.40,376.03,239.46,88.02"><row><cell>Topic Original</cell><cell>avg prec Correction</cell><cell>avg prec Change</cell></row><row><cell>463 tartin</cell><cell>0.0000 martin</cell><cell>0.0000 0.0000</cell></row><row><cell cols="3">464 nativityscenes 0.0000 nativity scenes 0.0000 0.0000</cell></row><row><cell>474 bennefits</cell><cell>0.0003 benefits</cell><cell>0.0002 -0.0001</cell></row><row><cell>476 aniston</cell><cell>0.1517 anniston</cell><cell>0.0062 -0.14.55</cell></row><row><cell>483 rosebowl</cell><cell>0.0108 rose bowl</cell><cell>0.3198 +0.3090</cell></row><row><cell>487 angioplast7</cell><cell>0.0000 angioplasty7</cell><cell>0.1553 +0.1553</cell></row></table><note coords="3,72.04,489.04,215.94,8.96;3,72.04,500.20,216.03,8.96;3,72.04,511.36,193.41,8.96"><p>These results reflect word-based title-only runs with relevance feedback. Spelling correction helped us dramatically on two queries, and hurt us on one.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,73.72,270.04,212.62,66.68"><head>table :</head><label>:</label><figDesc></figDesc><table coords="4,73.72,292.84,212.62,43.88"><row><cell></cell><cell># docs</cell><cell># terms</cell><cell>index size</cell></row><row><cell>English words</cell><cell>344,299</cell><cell cols="2">46,951 105 MB</cell></row><row><cell cols="2">Chinese 2-grams 343,714</cell><cell cols="2">553,358 195 MB</cell></row><row><cell cols="4">Chinese 3-grams 333,007 2,908,676 270 MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,72.04,339.40,216.01,20.12"><head>Table 7 .</head><label>7</label><figDesc>Statistics for APL's hybrid parallel collection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="4,324.04,163.70,215.90,81.71"><head>Table 8 .</head><label>8</label><figDesc>Official results for CLIR task</figDesc><table coords="4,324.04,163.70,215.90,69.95"><row><cell></cell><cell>recall</cell><cell>#</cell><cell># ≥</cell><cell>%</cell></row><row><cell></cell><cell>prec</cell><cell>best</cell><cell>median</cell><cell>mono</cell></row><row><cell cols="2">apl9xmon 0.3085 621</cell><cell>5</cell><cell>20</cell><cell>100 %</cell></row><row><cell>apl9xtop</cell><cell>0.0763 360</cell><cell>0</cell><cell>7</cell><cell>24.7%</cell></row><row><cell cols="2">apl9xwrd 0.1076 416</cell><cell>0</cell><cell>8</cell><cell>34.9%</cell></row><row><cell cols="2">apl9xcmb 0.1523 535</cell><cell>0</cell><cell>11</cell><cell>49.4%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,90.04,562.36,197.97,8.96;7,72.04,573.52,215.97,8.96;7,72.04,584.68,150.66,8.96;7,222.76,582.99,3.96,4.54;7,236.80,584.68,51.25,8.96;7,72.04,595.84,215.94,8.96;7,72.04,607.00,215.94,8.96;7,72.04,618.16,22.65,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,76.48,573.52,203.42,8.96">Chinese Text Retrieval Without Using a Dictionary</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Meggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,90.52,584.68,132.18,8.96;7,222.76,582.99,3.96,4.54;7,236.80,584.68,51.25,8.96;7,72.04,595.84,215.94,8.96;7,72.04,607.00,139.90,8.96">the Proceedings of the 20 th International Conference on Research and Development in Information Retrieval (SIGIR-97)</title>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.04,640.48,197.99,8.96;7,72.04,651.64,215.98,8.96;7,72.04,662.80,216.01,8.96;7,72.04,673.96,113.73,8.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,219.21,640.48,68.82,8.96;7,72.04,651.64,215.98,8.96;7,72.04,662.80,107.79,8.96">Relating the new language models of information retrieval to the traditional retrieval models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
		<idno>TR-CTIT-00-09</idno>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">CTIT Technical Report</note>
</biblStruct>

<biblStruct coords="7,90.04,696.28,165.33,8.96;7,72.04,707.44,195.73,8.96;7,324.04,53.68,211.02,8.96;7,324.04,64.84,128.65,8.96;7,324.04,76.00,218.17,8.96;7,324.04,98.32,211.02,8.96;7,324.04,109.48,129.73,8.96;7,324.04,120.64,201.13,8.96" xml:id="b2">
	<monogr>
		<ptr target="http://www.ldc.upenn/Catalog/LDC2000T46.html" />
		<title level="m" coord="7,90.04,696.28,161.27,8.96;7,342.04,53.68,193.02,8.96;7,324.04,64.84,73.80,8.96;7,342.04,98.32,193.02,8.96;7,324.04,109.48,74.88,8.96">Linguistic Data Consortium (LDC), Hong Kong Laws Parallel Text</title>
		<imprint/>
	</monogr>
	<note>Linguistic Data Consortium (LDC), Hong Kong News Parallel Text</note>
</biblStruct>

<biblStruct coords="7,342.04,142.96,197.97,9.08;7,324.04,154.12,151.77,8.96" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ken</forename><surname>Lunde</surname></persName>
		</author>
		<title level="m" coord="7,404.20,143.08,135.81,8.96;7,324.04,154.12,88.75,8.96">CJKV Information Processing, O&apos;Reilly &amp; Associates</title>
		<imprint>
			<date type="published" when="1999-01">January 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,342.04,176.44,197.98,8.96;7,324.04,187.60,215.97,8.96;7,324.04,198.76,215.98,9.08;7,324.04,209.92,215.94,9.08;7,324.04,221.08,29.25,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,525.81,176.44,14.21,8.96;7,324.04,187.60,166.08,8.96">The JHU/APL HAIRCUT System at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,464.92,198.88,75.10,8.96;7,324.04,210.04,193.31,8.96">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="7,342.04,243.40,197.99,8.96;7,324.04,254.56,216.01,8.96;7,324.04,265.72,215.98,9.08;7,324.04,276.88,215.96,9.08;7,324.04,288.04,22.65,8.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,534.75,243.40,5.28,8.96;7,324.04,254.56,216.01,8.96;7,324.04,265.72,215.98,9.08;7,324.04,277.00,89.99,8.96">Draft version in the Working Notes of the CLEF-2000 Workshop</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-09">September 2000</date>
			<pubPlace>Lisbon, Portugal</pubPlace>
		</imprint>
	</monogr>
	<note>A Language-Independent Approach to European Text Retrieval</note>
</biblStruct>

<biblStruct coords="7,342.04,310.36,197.99,8.96;7,324.04,321.52,216.01,8.96;7,324.04,332.68,156.30,8.96;7,480.40,330.99,5.04,4.54;7,488.80,332.68,51.25,8.96;7,324.04,343.84,215.94,8.96;7,324.04,355.00,215.97,8.96;7,324.04,366.16,54.09,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,534.75,310.36,5.28,8.96;7,324.04,321.52,216.01,8.96;7,324.04,332.68,26.43,8.96">A Hidden Markov Model Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,374.68,332.68,105.66,8.96;7,480.40,330.99,5.04,4.54;7,488.80,332.68,51.25,8.96;7,324.04,343.84,215.94,8.96;7,324.04,355.00,142.54,8.96">the Proceedings of the 22 nd International Conference on Research and Development in Information Retrieval (SIGIR-99)</title>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,342.04,388.48,197.97,8.96;7,324.04,399.64,215.96,8.96;7,324.04,410.80,215.98,8.96;7,324.04,421.96,206.01,9.08" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,328.59,399.64,211.41,8.96;7,324.04,410.80,176.43,8.96">Performance and Scalability of a Large-Scale Ngram Based Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nicholas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,324.04,422.08,121.05,8.96">Journal of Digital Information</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2000-01">January 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,342.04,444.28,197.94,8.96;7,324.04,455.44,215.94,9.08;7,324.04,466.60,171.45,9.08" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,454.48,444.28,85.50,8.96;7,324.04,455.44,146.20,8.96">Chinese Information Retrieval: using characters or words?</title>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,481.36,455.44,58.62,9.08;7,324.04,466.72,114.39,8.96">In Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,342.04,488.92,197.94,8.96;7,324.04,500.08,216.03,8.96;7,324.04,511.24,53.46,8.96;7,377.56,509.55,5.04,4.54;7,385.12,511.24,124.65,8.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,499.36,488.92,40.62,8.96;7,324.04,500.08,216.03,8.96;7,324.04,511.24,53.46,8.96;7,377.56,509.55,5.04,4.54;7,385.12,511.24,26.54,8.96">Managing Gigabytes: Compressing and Indexing Documents and Images 2 nd edition</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
