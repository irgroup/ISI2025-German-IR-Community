<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.24,114.41,309.29,12.31">A semantic approach to Question Answering systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,241.89,138.09,42.57,9.61"><forename type="first">Jose</forename><surname>Luis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos. Universidad de Alicante</orgName>
								<address>
									<addrLine>Campus de San Vicente del Raspeig Apartado 99</addrLine>
									<postCode>03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,298.67,138.09,87.15,9.61"><forename type="first">Antonio</forename><surname>Ferrández</surname></persName>
							<email>antonio@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos. Universidad de Alicante</orgName>
								<address>
									<addrLine>Campus de San Vicente del Raspeig Apartado 99</addrLine>
									<postCode>03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.24,114.41,309.29,12.31">A semantic approach to Question Answering systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">981F2C298E8FB210CE4444E68EAF1370</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the architecture, operation and results obtained with the Question Answering prototype developed in the Department of Language Processing and Information Systems at the University of Alicante. Our approach accomplishes question representation by combining keywords with a semantic representation of expected answer characteristics. Answer string ranking is performed by computing similarity between this representation and document sentences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The prototype presented in this paper tries to face up question answering task from a new point of view. Question analysis obtains a mixed representation of queries based on keywords and a semantic representation of main information characteristics required by the question. Sentence ranking algorithm combines both representations to rank and select the best five answers. In the following section, our system is described. Afterwards, we analyse results obtained in TREC-9 Question Answering task. Initial conclusions are extracted and finally, directions for future work are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>Our system is structured into two main modules: Question analysis module and Answer selection module. First module processes questions expressed in open-domain natural language in order to obtain a representation of the information requested. This analysis is accomplished by obtaining question type and classifying terms into keywords and definition terms. Keywords help the system to locate sentences where answers can probably be found. A term in a query is considered a definition term if it defines characteristics of the expected answer. Question type and definition terms define the main information required by each question. A WordNet-based tool process questions type and definition terms in order to obtain a semantic representation of expected answer characteristics. This representation defines what we call semantic context of the target answer. The answer selection module uses keywords and semantic context to locate the sentence containing the answer and extract the part of the sentence that contains it. Figure <ref type="figure" coords="1,368.02,643.76,5.34,9.61">1</ref> shows system architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Selection</head><p>We only processed the first fifty ranked documents supplied by TREC Organisation. Nevertheless, all QA track collection was analysed to obtain term idf weights <ref type="bibr" coords="2,359.86,504.91,59.73,9.61" target="#b5">(Salton, 1989)</ref>. Term normalisation was performed using a version of Porter´s stemmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question and Document pre-processing</head><p>Several Natural Language Processing techniques have been applied to both questions and documents. These tools compose the Slot Unification Parser for Anaphora Resolution (SUPAR) described in <ref type="bibr" coords="2,139.53,593.11,69.33,9.61" target="#b2">Ferrández (1999</ref><ref type="bibr" coords="2,216.93,593.11,18.41,9.61" target="#b1">Ferrández ( , 1998</ref>). SUPAR's architecture consists of three independent modules that interact with one other. These modules are lexical analysis, syntactic analysis, and a resolution module for NLP problems such as anaphora resolution. Queries and documents are pre-processed before entering question analysis and answer selection modules. Queries pre-processing consists on part-of-speech-tagging terms and parsing. Documents are pre-processed by detecting sentence boundaries, part-of-speech-tagging terms, parsing and solving pronominal anaphora. Managing with pronominal anaphora consists on substituting non-pleonastic third-person pronouns for their antecedents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question Analysis</head><p>Question processing module accomplishes different tasks. This module extracts main keywords, expands keyword terms, determines question type and builds the semantic context representation of the expected answer.</p><p>Question type is detected by analysing Wh-terms. This process maps Wh-terms into one or several of the following categories:</p><p>Each of these categories is related to WordNet top concepts <ref type="bibr" coords="3,347.42,273.79,59.58,9.61" target="#b4">(Miller, 1995)</ref>. When no category can be detected by Wh-term analysis, NONE is used (e.g. "What" questions). This analysis gives the system three kinds of information: (1) lexical restrictions that expected answer should validate, (2) how to detect definition terms (if they exist), and (3) top WordNet concepts relevant to the expected answer.</p><p>Definition terms do not help the system to locate the correct answer but instead, they usually describe the kind of information requested. Depending on question type, different approaches are used to detect definition terms. For "What", "Which", "How", and similar questions these terms are detected by selecting noun phrases appearing next to the Wh-term. When questions such as "Find the number of…" or "Name a flying …" are analysed, noun phrases following the verb are considered definition terms.</p><p>Once question type and definition terms are analysed, the system generates the semantic context of the expected answer. A WordNet-based tool processes each definition term in order to build its semantic context representation. This context is represented as a weighted term vector that is computed as follows: for each definition, synonyms, one-level search hyponyms and all hyperonyms (until a top concept is achieved) are obtained. The weight assigned to these new terms is the idf of the analysed definition term in the collection divided by the distance in the WordNet hierarchy from this term to each new obtained one. When question type has been successfully mapped to a top concept, only terms related to this concept will be added to the term context representation. This way we obtain the terms that made up the context of a unique definition term. The semantic context representation of the answer (the joined representation of all definition term contexts) is computed by adding the context vector of each definition term in the question.</p><p>The semantic context of the answer helps the system in different ways: First, it approximates the type of the expected answer when the Wh-term analysis has been unable to obtain it. Second, as top concepts are too broad, it allows sub-classifying them for each particular question. And third, it helps the system to decide between different possible answers by comparing expected answer and probable answer semantic contexts.</p><p>To finish with question analysis, remaining question terms are considered keywords. When there are no remaining terms left (e.g. for the question "Name a flying mammal"), definition terms are used as keywords too. Non proper noun keywords are expanded using WordNet by adding to the question, keyword synonyms, one-level search hyponyms and one-level search hyperonyms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Selection</head><p>The input to this module is a small number of pre-processed candidate documents and the results of question analysis module. As first step, sentences are ranked accordingly to the following score:</p><p>In both cases, the idf of a term that occur twice or more times in a sentence is added only once.</p><p>When this initial sentence ranking has finished, the first 100 ranked sentences that include probable answers are selected as the best candidates to contain the correct one. A term is considered a probable answer if it verifies lexical restrictions obtained by Wh-term analysis.</p><p>The final step is to analyse sentences to extract and rank the windows of the desired length that probably contain the correct answer. The system selects a window for each probable answer by taking as centre the term considered a probable answer. Each window is assigned a window-score that is computed as follows:</p><p>Finally, windows are ranked on window-score and the system returns the first five ranked windows as final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>TREC-9 Question Answering Track allowed five answers for each question. Besides, depending on answer-string length, two different run types were defined: up to 50 or 250 bytes long. We participated with two runs for each different answer length. Figure <ref type="figure" coords="4,372.06,657.31,5.34,9.61">2</ref> shows results obtained. ALI9C runs have been produced applying the whole system described above. ALIC9A files contain results obtained applying the same strategy but without solving pronominal anaphora in relevant documents. These results were computed after getting rid of eleven questions whose answer did not appear in the document collection. Therefore, only 682 questions were evaluated.</p><p>Sentence-score = Keyword_idf_sum + (0.65 * Expanded_keyword_idf_sum)</p><p>where :</p><p>Keyword_idf_sum: is the sum of the idf weights for query keywords that appear in sentences.</p><p>Expanded_keyword_idf_sum: is the sum of the idf weights for terms obtained when expanding query keywords that also appear in sentences.</p><p>Window-score = Sentence-score * (1+cos(Question_SC,Window_SC))</p><p>where :</p><p>cos: Cosine Question_SC: vector representing the semantic context of the expected answer.</p><p>Window_SC: vector representing the semantic context of terms contained into the selected window (excluding keywords and expanded keyword terms).</p><p>Although a detailed results analysis is a very complex task, several conclusions can be extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieving relevant documents.</head><p>Correct answer was not included into the top fifty ranked documents supplied by TREC for 95 questions. As this fact relies on document retrieval strategy, we can not measure how our approach managed with these queries. Figure <ref type="figure" coords="5,242.34,324.19,5.34,9.61">3</ref> analyses the percentage of questions that could be correctly answered depending on the number of top documents selected for searching the answer. Even if first 1000 documents were analysed, it would have been impossible to obtain the correct answer for 25 questions. It seems that document retrieval techniques do not fit QA retrieval needs. In fact, systems applying paragraph-indexing techniques <ref type="bibr" coords="5,297.45,373.39,79.33,9.61">(Harabagiu, 2000)</ref>  <ref type="bibr" coords="5,380.19,373.39,60.41,9.61" target="#b0">(Clarke 2000)</ref> have obtained a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context based answer detection.</head><p>Our main objective was to inspect how Wh and definition terms could be used to build a useful semantic representation of expected answer and if this representation could improve correct answer detection. Results analysis shows several circumstances. This approach increases system performance by comparing expected answer with probable answer contexts. Very good results are obtained when possible answers context gives some indication about the nature of these answers. In this case context analysis allows the system to find the correct answer, even to successfully decide between similar but different possible answers. However, when possible answer context does not include characteristics that define the possible answer, the system does not take profit of expected answer context definition. It seems clear that semantic context representation can not substitute the use of a Name-Entity tagger (not applied in our prototype). We think that combining both tools will contribute to improve system performance in two important aspects: (1) increasing the amount and quality of the information obtained from the question and (2) improving possible answers detection.</p><p>Another circumstance to take into account is the way of selecting terms that define the context of the possible answers. Nowadays, the system builds the semantic context of a possible answer from all terms included into the window (250 or 50 bytes) surrounding each probable answer. As results show, results have become poorer as answer length decreased. This fact relies on the number and type of terms selected for building possible answer semantic context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pronominal anaphora resolution</head><p>Application of pronominal anaphora resolution has produced only a small benefit (around a 1%). Analysing this fact is very difficult but it relies on two main reasons. First, we have noticed that the number of relevant sentences involving pronouns is very low. And second, there are a lot of documents related to the same information, and sentences in a document that contain the right answer referenced by a pronoun, can also appear in another document without pronominal anaphora. Anyway, although the benefit is low, it can be considered a blind evaluation of how automatic pronominal anaphora resolution always helps QA systems performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future Work</head><p>Several areas of future work have appeared while analysing results. First, IR system used for retrieving relevant documents has to be adapted for QA tasks. The IR used by TREC Organisation retrieved the document containing the correct answer into the first fifty relevant documents only for a 86% of the evaluated questions. Second, question analysis has to be improved by increasing the number of question types analysed (i.e. definition or list questions). Third, unless context based answer detection has revealed to help system performance it needs a finer tuning on defining the number and type of terms used for semantic context building and exploring the possibilities of a Name-Entity tagger. This strategy needs to be investigated and tested.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,234.80,417.07,126.02,9.61"><head></head><label></label><figDesc>Figure 1. System architecture</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,83.12,410.71,429.30,9.61;6,91.88,422.95,373.99,9.61" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,365.36,410.71,147.06,9.61;6,91.88,422.95,36.96,9.61">Question Answering by passage selection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kisman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,147.08,422.95,225.92,9.61">Proceedings of the Nineth Text Retrieval Conference</title>
		<meeting>the Nineth Text Retrieval Conference<address><addrLine>Washington (USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,83.12,444.07,429.37,9.61;6,91.88,456.31,420.54,9.61;6,91.88,468.55,420.81,9.61;6,91.88,480.91,84.05,9.61" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,304.52,444.07,207.97,9.61;6,91.88,456.31,63.38,9.61">Anaphora resolution in unrestricted texts with partial parsing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,174.33,456.31,338.09,9.61;6,91.88,468.55,414.83,9.61">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, COLING -ACL</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, COLING -ACL<address><addrLine>Montreal (Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,83.12,501.91,429.29,9.61;6,91.88,514.27,197.80,9.61" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,311.12,501.91,201.29,9.61;6,91.88,514.27,42.14,9.61">An empirical approach to Spanish anaphora resolution</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moreno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>To appear in Machine Translation</note>
</biblStruct>

<biblStruct coords="6,83.12,535.15,429.42,9.61;6,91.88,547.39,76.45,9.61;6,203.60,547.39,308.93,9.61;6,91.88,559.75,252.23,9.61" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,110.60,547.39,57.73,9.61;6,203.60,547.39,224.68,9.61">MorUHVFX 3 FALCON: Boosting Knowledge for Answer Engines</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasça</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gîrju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,448.29,547.39,64.24,9.61;6,91.88,559.75,159.41,9.61">Proceedings of the Nineth Text Retrieval Conference</title>
		<meeting>the Nineth Text Retrieval Conference<address><addrLine>Washington (USA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,83.12,580.75,429.30,9.61;6,91.88,593.11,40.96,9.61" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,162.54,580.75,179.24,9.61">Wordnet: A Lexical Database for English</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,353.83,580.75,127.36,9.61">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,83.12,614.11,428.37,9.61;6,91.88,626.35,286.71,9.61" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,163.44,614.11,348.06,9.61;6,91.88,626.35,106.98,9.61">Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison Wesley Publishing</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,83.12,647.47,429.52,9.61;6,91.88,659.71,41.79,9.61" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><surname>Trec-9</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/cfp.html" />
		<title level="m" coord="6,151.58,647.47,274.32,9.61">Call for participation Text Retrieval Conference 2000 (TREC-9)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
