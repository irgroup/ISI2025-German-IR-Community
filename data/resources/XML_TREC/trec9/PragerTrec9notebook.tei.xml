<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.64,86.02,292.70,11.10">One Search Engine or Two for Question-Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,255.64,102.83,46.85,9.07"><forename type="first">John</forename><surname>Prager</surname></persName>
							<email>jprager@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,309.39,102.83,46.58,9.07"><forename type="first">Eric</forename><surname>Brown</surname></persName>
						</author>
						<author>
							<persName coords="1,236.44,114.35,69.91,9.07"><forename type="first">Ibm</forename><forename type="middle">T J</forename><surname>Watson</surname></persName>
						</author>
						<author>
							<persName coords="1,266.44,160.48,79.44,9.07"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
							<email>radev@umich.edu</email>
						</author>
						<author>
							<persName coords="1,270.52,217.84,67.35,9.07"><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
							<email>kczuba@cs.cmu.edu</email>
						</author>
						<author>
							<persName coords="1,270.14,300.64,66.28,9.07"><forename type="first">Research</forename><surname>Center</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center Yorktown Heights</orgName>
								<address>
									<postCode>10598</postCode>
									<region>N.Y</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie-Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.64,86.02,292.70,11.10">One Search Engine or Two for Question-Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DBA5AF9322227AA24EE889433533BAB4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present here a preliminary analysis of the results of our runs in the Question Answering track of TREC9. We have developed a complete system, including our own indexer and search engine, GuruQA, which provides document result lists that our Answer Selection module processes to identify answer fragments. Some TREC participants use a standard set of result lists provided by AT&amp;T's running of the SMART search engine. We wondered how our results would be affected by using the AT&amp;T result sets. For a variety of reasons we could not replace GuruQA's results with SMART's, but we could use document co-occurrence counts to influence our hit-lists. We submitted two runs to NIST for both the 50-and 250-byte cases, one with and one without consideration of the AT&amp;T document result sets. The AT&amp;T document set was only used for a subset of about a third of the questions. This subset exhibited an increase in Mean Reciprocal Answer Rank score of 13% and 8% for the two tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Question Answering is a computer-based activity that involves searching large quantities of text and understanding both questions and textual passages to the degree necessary to recommend a text fragment as an answer to a question. The TREC Question-Answering track is an attempt to bring together the Information Retrieval (IR) and Natural Language Processing <ref type="bibr" coords="1,271.75,703.36,25.43,9.07">(NLP)</ref> or Information Extraction (IE) communities. The strengths of IR lie in the search engines, while those of NLP lie in the ability to parse and analyze text. Indeed, some NLP groups have no expertise or interest in developing their own search engines, yet still wish to participate in the Question-Answering track. To enable these groups to readily participate, AT&amp;T have made available the results of running their version of the SMART search engine <ref type="bibr" coords="1,414.96,414.40,65.48,9.07" target="#b3">(Buckley, 1985;</ref><ref type="bibr" coords="1,484.36,414.40,55.89,9.07" target="#b13">Salton, 1971)</ref> on the questions. These datasets consist of the top 50 documents retrieved for each of the questions in the track. These document sets were used by, amongst others, the best-performing entry in TREC8 QA <ref type="bibr" coords="1,508.90,460.48,31.60,9.07;1,315.16,472.00,53.38,9.07" target="#b16">(Srihari and Li, 2000)</ref>.</p><p>The principal advantage of using these hit lists is that a group can concentrate on the information extraction task of finding the answers from a relatively limited quantity of text. (The entire collection contains close to 1 million documents.) A secondary benefit is that all groups who operate this way are on a common footing regarding the IE activity. The principal disadvantage of this approach, of course, is that no custom question (or collection) pre-processing is possible. As a consequence it can happen that no document in the top 50 available contains the answer to a particular question.</p><p>Our group has its own search engine, GuruQA, based on Guru <ref type="bibr" coords="1,352.36,644.57,106.19,9.07" target="#b2">(Brown and Chong, 1998)</ref>, and is thus able to control the entire processing operation, from question processing and text indexing to answer selection. The technique we use, called Predictive Annotation, involves indexing anticipated semantic types, identifying the semantic type of the answer sought by the question, and extracting the best matching entity in answer passages <ref type="bibr" coords="2,96.77,85.55,78.68,9.07">(Prager et al. 2000b</ref>). Here we explore the question of whether we are at an advantage or disadvantage by not making use of a respected search engine such as SMART as used by AT&amp;T, and we look at a particular way of gaining the best of both worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>There is much evidence in the field of text processing that combining the results of a single system acting upon different problem formulations, or of different systems acting on the same queries, provides superior performance over individual systems. <ref type="bibr" coords="2,242.48,220.19,54.62,9.07;2,72.04,231.71,27.13,9.07" target="#b0">Belkin et al. (1993)</ref> discuss this in some depth for information retrieval. Amongst others, they cite <ref type="bibr" coords="2,210.81,243.23,86.18,9.07;2,72.04,254.51,26.87,9.07">Saracevic and Cantor (1987)</ref> and <ref type="bibr" coords="2,120.99,254.51,97.92,9.07" target="#b17">Turtle and Croft (1991)</ref> who demonstrated that combining the results from processing with a single search engine, but with different query formulations of a common information need, would produce increased retrieval performance. Foltz and Dumais (1992) used the same query formulation with multiple search engines, and again found increased retrieval performance.</p><p>A similar effect has been shown in other areas of the text-processing field, notably classification/part-ofspeech tagging <ref type="bibr" coords="2,137.82,369.71,85.38,9.07" target="#b1">(Brill and Wu, 1998)</ref> Given this history, we suspected that we could gain some improvement in the TREC Question-Answering task by combining the hit lists that our search engine produced with those produced by AT&amp;T, and made available to the track participants. The search engine used by AT&amp;T is the SMART system from Cornell; their internally modified version is described by Singhal (1998).</p><p>Unlike our system, described in the next section, the AT&amp;T version of SMART used to generate the document sets for Question-Answering was not tailored to the task. It was the same system they used for participation in the TREC7 "Ad-hoc" task. It uses a standard series of IR techniques, such as stop-word removal, tokenization, rule-based and statistics-based phrase formation, tfxidf style term weighting and relevance feedback using Rocchio weights <ref type="bibr" coords="2,215.37,588.12,65.87,9.07" target="#b11">(Rocchio, 1971)</ref>. It returned a ranked list of documents with no indication of relevant passages within the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our System</head><p>Our Question-Answering system employs the technique of Predictive Annotation, introduced and described in <ref type="bibr" coords="2,72.04,690.59,80.66,9.07">(Prager et al. 2000a</ref>). The technique revolves around the concept of semantic class labels which we call QA-Tokens, corresponding loosely to some of the Basic Categories of <ref type="bibr" coords="2,373.25,85.55,78.00,9.07" target="#b12">Rosch et al. (1976)</ref>. These are used not only as Named Entity descriptors, but are actual tokens processed by the indexer. The basic operation of our system is as follows.</p><p>The question is analysed and the desired answer type is determined. The "wh-words" are replaced by the corresponding QA-Token or set of QA-Tokens (thus "how hot" is replaced by TEMPERATURE$, "when" is replaced by @syn(DATE$, TIME$, YEAR$)). The QA-Tokens identified in the documents are indexed as if they were regular terms. A set of about 400 patterns is used for this conversion. We found in TREC8 that questions that failed to match this way were usually of the form "What X" where X was a relatively rare noun (e.g. "What debts did the Qintex group leave?"). For these cases we used WordNet <ref type="bibr" coords="2,442.35,269.63,58.64,9.07" target="#b6">(Miller, 1995)</ref> to find a hypernym synset of X that corresponded to one of our QA-Tokens. In the case of X= "debts", the synset for "monetary-value" corresponds to MONEY$.</p><p>WordNet was also used to generate synonym lists of head nouns in the questions. Word-sense disambiguation was performed by calculating co-occurrence counts, as described by <ref type="bibr" coords="2,409.97,361.56,125.99,9.07" target="#b7">Moldovan and Mihalcea (2000)</ref>, but using the TREC collection instead of the Web.</p><p>Stop-words are removed, inflected terms are reduced to their lemma form, morphological variants that go beyond simple inflection are added as synonyms (thus "moved" -&gt; "move" -&gt; "@syn(move, motion)"). Weights are associated with terms, according to the scheme "QA-Tokens &gt; proper names &gt; common words". This in effect is a simple implementation of idf weighting, but applied to the lexical classes of the terms being indexed.</p><p>A set of text patterns is associated with each of the approximately 50 QA-Tokens. Before the text collection is indexed, it is processed by Textract (Byrd and Ravin, 1999; Wacholder, Ravin and Choi, 1997) which applies these text patterns; when a match is found the text is annotated with the corresponding QA-Token. The indexer indexes not only the base terms but all annotations too. Thus when the indexer encounters "France", for example, it will also index the tokens PLACE$ and COUNTRY$ at the same location.</p><p>Search is not document-oriented but passage-oriented, where a passage is one, two or three sentences. Scoring does not use tf but a kind of combination match, where each query term found in the passage contributes its weight to the passage's score, but only once for any number of occurrences. Only one (the "best") passage is returned per document, thus inducing a document ranking.</p><p>The top n (normally n=10) passages returned by the search engine are then processed by the Answer Selection module, Ansel <ref type="bibr" coords="3,155.58,131.63,81.70,9.07" target="#b10">(Radev et al., 2000)</ref>. Textract is used again to identify all of the named entities, including simple noun phrases, in those passages. The named entities are typed by QA-Token (simple noun phrases being THING$s), and seven features, such as searchengine ranking, distance from beginning of sentence, and presence of QA-Token in query, are calculated for each entity. A linear evaluation function, using weights discovered by a machine-learning algorithm, is used to associate a final score with each named entity. Finally, text fragments of the desired size (50 or 250 bytes for TREC9) centered on the best named entities are generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The experiment</head><p>The QA track permits participants to submit up to two runs in each of the 50-and 250-byte sub-tracks. Last year we had developed two different answer-selection modules, neither of which was clearly better than the other, so we submitted one run using each module. Since then we have combined the best of each module to give us a single answer-selection component, and we were looking for significant experimental variations we could develop to take advantage of the two-run opportunity. In particular we wanted to do more than just submit runs with different parameter settings. Consequently we decided to submit one run ("R", for Regular) using our system alone, and another ("A", for AT&amp;T) with reference to the AT&amp;T document set.</p><p>We will call this latter set SET-A. Our approach was simply to use these documents to increase the score of documents on our own hit lists if the documents also occurred in SET-A (per question).</p><p>We had arbitrarily set an internal hit-list size of 10; that is, the search engine would return the top 10 documents (passages) which would then be forwarded to the Answer-Selection module (Ansel). The scores from the search engine were in the range of 0-2000 (approximately). For the "A" run, we increased the internal hitlist size to 50. For every document that was also on the SET-A hit-list we increased its score on our hit-list by 10,000, and then sorted. This had the effect of putting all of the documents that occurred on both hit-lists ahead of those on ours alone, but keeping the relative order within the two groups. The top 10 documents were then forwarded to Ansel. Since search-engine score is a factor in Ansel processing, we subtracted off any 10,000s. This meant that the passage scores that Ansel saw were exactly the scores that our search engine had given. The effect of considering SET-A was therefore solely in determining whether a passage would appear in the input to Ansel. It is an open question, that we need to answer experimentally, whether this is the best way to combine the hit-lists, and whether we should give documents that occur on both lists a permanently increased score.</p><p>Note that we did not add to our hit-list any documents that occurred in SET-A but were not originally in our list. This was primarily because our search engine returns not only a document list, but for every document on the list the offset and length of the best-matching passage -for use in Answer Selection. This information was not available in SET-A. The secondary reason was that it was unclear (without any theory or extensive experimentation) how to assign scores to such additional documents.</p><p>The overall effect of considering SET-A was to give the "A" run a slightly improved score over the base "R" run. Before we look at the numbers in detail, we need to address the question of whether any improved performance might be due to the AT&amp;T SMART search engine being intrinsically "better" than ours, at least as the two search engines were deployed for this exercise.</p><p>To that end, we compared the search engines' performances in the following way.</p><p>In this comparison we do not ask whether the system extracted the right answer from the documents being considered, but solely whether the documents contained the right answer (a necessary but not sufficient condition for ultimate system success). We call such a document a "correct" document. We had available lists of which documents out of the 50 per question in SET-A were correct, in the sense just defined. These lists were posted to the QA track mailing list by Ken Litkowski (ken@clres.com). For each question there was a list of 0 to 50 numbers in the range of 1 to 50, corresponding to the positions in the hit-list of documents that contained a correct answer. Whether a document contained a correct answer or not was determined by the document's association with a correct response in the qa-judgments file, made available on the TREC web site (http://trec.nist.gov) after the TREC9 submissions deadline. Note that this document-judging scheme admits of two possible sources of error: 1) errors by human judges in developing the judgments file, and 2) documents in the set which contained valid answers but were never chosen by any participating entry, so were never judged. We generated a comparable set of correct-document lists for our own search engine; we'll call this set SET-R. We are now able to compare the two search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search engine comparison</head><p>The first measure we calculated was Mean Reciprocal Document Rank (MRDR) of the first correct response, in analogy to the way the answer fragments are judged in the QA-track. For each question, the Reciprocal Document Rank is 1 point if the top document in the hit-list contains a correct answer, ½ if the first doesn't but the second does, all the way down to 1/50 if only the 50 th document does, or 0 if no document on the list does. The RDR scores are averaged over all of the 682 questions. (There were originally 693 questions but 11 were discarded by NIST for reasons of ill-definition.)</p><p>For both systems, the MRDR value was calculated to be 0.49 -in other words, on average both systems produced a document containing the right answer in the second position. The fact that both systems had identical MRDR scores indicates that any improvement of our overall score is due to the complementary nature of the two search engines, not to the intrinsic superiority of one or the other.</p><p>Before we leave the subject of search engine comparison, we look at two more measures. Choosing the "best" size for a hit list is a heuristic for which we have no firm data. Using a small hit list is desirable if there is frequently a correct document in a high position, since the subsequent processing will then have relatively little noise to contend with, and precision will increase. Long hits lists, on the other hand, offer greater recall. Therefore we looked at subsets of the 50-document hit lists (always starting at document #1). We ask for a hit list of size N (1&lt;=N&lt;=50) whether there was a correct document on the list. The results were very similar, but not identical, for the two document sets, as shown in Figure <ref type="figure" coords="4,195.02,551.16,3.78,9.07" target="#fig_0">1</ref>. The SET-R curve is the solid one, being higher for the first few documents but lower thereafter.</p><p>We can make a couple of observations from the data in Figure <ref type="figure" coords="4,345.65,317.63,3.78,9.07" target="#fig_0">1</ref>. First, the two search engines seem to have almost equivalent performance. Any significant change in our system's behaviour (good or bad) due to consideration of the AT&amp;T documents will be due to the process of considering multiple result sets, not due to the inherent superiority of one or other set. Second, the curves suggest absolute values for hit list size. To the degree to which the answer-selection processes that operate on these document sets are imperfect, that is, suffer in precision due to the presence of incorrect documents in the set, the hit-lists should be cut back. An obvious "knee" in both curves occurs at around 6-10 documents. On the other hand, if the answer processing is sufficiently sophisticated to be able to easily reject incorrect documents based on their internal semantics, then the hit lists could be quite large. It would be useful then to know at what point the curves, if extended to the right, would asymptote to 100% (=682 questions for TREC9). The only data point we have in this regard is for GuruQA. 542 questions had at least one correct document on a hit-list of size 50; with hitlists of size 200 we have a correct document for 576 questions. This suggests that the asymptote is far away, and that the correct place to concentrate effort on answering these residual questions is in question preprocessing, and possibly collection processing prior to indexing.</p><p>We also looked briefly at the overlap between the document sets. For the entire 50-deep hit lists, we asked which ones contained a correct document somewhere. The totals across 682 documents are presented in Table <ref type="table" coords="4,351.45,685.56,3.78,9.07">1</ref>. Table <ref type="table" coords="5,97.51,133.55,3.78,9.07">1</ref>. This shows how the two search engines overlapped in answering a question. A document set scores a "Yes" for a question if at least one of the documents in the set contains a strictly correct answer to the question.</p><p>It is difficult to make too many quantitative predictions about the potential advantages of using document result sets from multiple systems, since there is more processing to come after the result sets are established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actual Performance Increase</head><p>We did not attempt to use the AT&amp;T documents for every question, since we did not have a chance to test the idea (using the previous year's sets) before the current year's submission. Instead, we just used them on those question types for which we previously had experienced inferior performance. These were questions, generally of the form "What X …", for which none of our QA-tokens was instantiated (save for THING$, matching a generic noun phrase, which was created just for such situations).</p><p>Of the 682 questions, there were 214 questions which we labelled type THING$. We calculated the Mean Reciprocal Answer Rank (MRAR) score for the THING$, non-THING$ and total sets of questions, both with ("A") and without ("R") the AT&amp;T documents. The MRAR scores reported here are for the actual answers, not the correct documents as MRDR measures in the previous section.</p><p>The results are summarized for the 50-byte run in Table <ref type="table" coords="5,72.04,543.00,3.78,9.07">2</ref>, and for the 250-byte run in Table <ref type="table" coords="5,227.32,543.00,3.78,9.07">3</ref>. Two styles of judging were provided in TREC9: strict, in which the answer was present in the returned fragment and justified in the surrounding context, and lenient, where the correct answer was present but not necessarily in a context that addressed the question. All of the data reported here were for the strict interpretation. . It was expected that there would be a difference between the runs for THING$-type questions, but it can be seen that the Non-THING$ scores also differ between the "A" and "R" runs, in the 50 byte task. This occurred for two reasons, which curiously only affected the Non-THING$ questions. Firstly, there was a wordalignment error in the 50-byte fragment selection code that was present in the "A" system but not in the "R" system. This caused in some cases critical answer words to be truncated and hence disallowed. This affected 6 questions (actually, 2 questions plus 4 paraphrases of one of them), to the tune of a loss of .009 to the MRAR score. The remaining .009 of the discrepancy was due to inconsistent judging (the same answer submitted by both runs was judged differently). Eight questions were negatively affected by these judging problems: in seven cases the "A" run was affected, and in one the "R" run. Unfortunately, the deleterious effects of the inconsistent judging and our alignment bug swamped the positive effect of using the second document set, when the overall scores are calculated (for the 50-byte runs).</p><formula xml:id="formula_0" coords="5,72.04,646.92,10.08,9.07">50</formula><p>From Tables <ref type="table" coords="5,370.13,501.48,5.04,9.07">2</ref> and<ref type="table" coords="5,396.29,501.48,3.78,9.07">3</ref>, we see that for the 214 THING$ questions, in the 50-byte sub-track MRAR improved by 13%, and in the 250-byte sub-track by 8%. An experiment that we need to do now is to try a run using the SET-A documents for the Non-THING$ questions too. Due to the labor-intensive nature of the document judging, we will await a set of answer patterns per question from NIST to enable us to judge such future runs automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>Indexing QA-Tokens improves the precision of our IR system, since it gives it more semantics and provides a means of better matching questions to answers. The technique is not so useful in conditions when no semantic type is identifiable, such as "What X" type ques-tions. The experiments reported here demonstrate that considering results sets from a second search engine can improve QA results, at least for those "What X" questions. This was achieved using search engines working under very different operational conditions and with a very basic method of combining the hit-lists. These results extend existing demonstrations of the benefit of using multiple systems in "ad-hoc"-style search and classification.</p><p>An incidental discovery was that our use of GuruQA with Predictive Annotation and passage ranking produced result sets with identical MRDR to AT&amp;T's version of SMART, for 214 "What X" type questions. This finding may be related to the existence of theoretical and practical limits to the results achievable with statistical information retrieval.</p><p>As mentioned earlier, we plan to see what kind of improvement will be afforded if the approach is extended to all question types. It is also completely open what is the best way to incorporate the information from other result sets. We took the simplest possible approach, which was to move documents that occurred in both hit lists up towards the top of ours. We did not experiment with increasing the score, which we expect will positively effect the results, since Answer-Selection uses passage score as a feature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,315.16,213.95,214.60,9.07;4,315.16,225.47,222.69,9.07;4,315.16,236.99,218.28,9.07"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Comparison of number of questions with a "correct" document in the hit list against hit-list depth, for our search engine (SET-R) and AT&amp;T's (SET-A).</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,90.15,427.07,206.95,9.07;6,90.04,438.59,206.93,9.07;6,90.04,450.11,207.11,9.12;6,90.04,461.63,180.42,9.12" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,95.12,438.59,201.85,9.07;6,90.04,450.11,175.97,9.07">The Effect of Multiple Query Representations on Information Retrieval System Performance</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,278.92,450.16,18.23,9.07;6,90.04,461.68,85.31,9.07">Proceedings of SIGIR&apos;93</title>
		<meeting>SIGIR&apos;93<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.21,476.99,206.73,9.07;6,90.04,488.51,207.19,9.12;6,90.04,500.03,97.08,9.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,186.70,476.99,110.24,9.07;6,90.04,488.51,133.05,9.07">Classifier combination for improved lexical disambiguation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,246.76,488.56,50.47,9.07;6,90.04,500.08,66.09,9.07">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.03,515.63,207.09,9.07;6,90.04,527.15,207.12,9.12;6,90.04,538.67,67.80,9.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,225.94,515.63,71.18,9.07;6,90.04,527.15,44.10,9.07">The Guru System in TREC-6</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">A</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,154.12,527.20,94.82,9.07">Proceedings of TREC</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="1998">1998</date>
			<pubPlace>Gaithersburg, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.18,554.03,206.98,9.07;6,90.04,565.55,207.04,9.07;6,90.04,577.07,207.18,9.07;6,90.04,588.59,186.37,9.07" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,151.65,554.03,145.51,9.07;6,90.04,565.55,112.49,9.07">Implementation of the SMART information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<idno>TR85-686</idno>
		<imprint>
			<date type="published" when="1985">14853. May 1985</date>
			<pubPlace>Ithaca, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="6,90.03,604.19,207.14,9.07;6,90.04,615.47,207.14,9.12;6,90.04,626.99,92.53,9.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,189.87,604.19,107.30,9.07;6,90.04,615.47,69.39,9.07">Identifying and Extracting Relations in Text</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ravin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,170.92,615.47,126.26,9.12;6,90.04,626.99,27.84,9.07">Proceedings of NLDB 99, Klagenfurt</title>
		<meeting>NLDB 99, Klagenfurt<address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.21,642.59,207.04,9.07;6,90.04,654.11,207.15,9.07;6,90.04,665.63,207.00,9.12;6,90.04,677.15,82.44,9.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,219.73,642.59,77.52,9.07;6,90.04,654.11,207.15,9.07;6,90.04,665.63,68.31,9.07">Personalized information delivery: An analysis of informationfiltering methods</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,174.28,665.68,116.79,9.07">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.33,74.03,207.01,9.07;6,333.16,85.55,207.10,9.12;6,333.16,97.07,35.52,9.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,381.89,74.03,158.45,9.07;6,333.16,85.55,14.67,9.07">WordNet: A Lexical Database for English</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,358.84,85.60,120.96,9.07">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.15,112.43,207.07,9.07;6,333.16,123.95,207.04,9.07;6,333.16,135.47,206.99,9.12;6,333.16,146.99,57.00,9.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,476.61,112.43,63.61,9.07;6,333.16,123.95,207.04,9.07;6,333.16,135.47,34.28,9.07">Using WordNet and Lexical Operators to Improve Internet Searches</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,380.92,135.52,106.50,9.07">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="page" from="34" to="43" />
			<date type="published" when="2000-02">Jan-Feb 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.15,162.59,207.00,9.07;6,333.16,174.11,206.92,9.07;6,333.16,185.64,207.03,9.12;6,333.16,196.91,139.98,9.12" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,373.29,174.11,166.79,9.07;6,333.16,185.64,128.82,9.07">The Use of Predictive Annotation for Question-Answering in TREC8</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Coden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,477.40,185.69,62.79,9.07;6,333.16,196.97,26.90,9.07">Proceedings of TREC</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2000">2000</date>
			<pubPlace>Gaithersburg, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.16,212.51,207.04,9.07;6,333.16,224.04,207.14,9.07;6,333.16,235.56,207.01,9.12;6,333.16,247.08,90.12,9.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,369.48,224.04,170.83,9.07;6,333.16,235.56,22.45,9.07">Question-Answering by Predictive Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,369.40,235.61,110.98,9.07">Proceedings of SIGIR 2000</title>
		<meeting>SIGIR 2000<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.16,262.44,207.13,9.07;6,333.16,273.96,207.22,9.07;6,333.16,285.48,207.04,9.12;6,333.16,297.00,120.38,9.12" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,506.27,262.44,34.03,9.07;6,333.16,273.96,207.22,9.07;6,333.16,285.48,123.63,9.07">Ranking Suspected Answers to Natural Language Questions using Predictive Annotation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Samn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,473.80,285.53,66.40,9.07;6,333.16,297.05,35.38,9.07">Proceedings of ANLP&apos;00</title>
		<meeting>ANLP&apos;00<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.34,312.60,206.77,9.07;6,333.16,324.12,207.09,9.12;6,333.16,335.40,207.00,9.12;6,333.16,346.92,207.07,9.07;6,333.16,358.44,22.68,9.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,397.19,312.60,142.92,9.07;6,333.16,324.12,34.49,9.07">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,391.00,324.17,149.25,9.07;6,333.16,335.45,185.72,9.07">The SMART Retrieval System -Experiments in Automatic Document Retrieval</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall, Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.16,374.04,207.14,9.07;6,333.16,385.56,187.80,9.12" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,403.36,374.04,136.94,9.07;6,333.16,385.56,14.48,9.07">Basic Objects in Natural Categories</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,357.64,385.61,88.71,9.07">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="382" to="439" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.22,400.92,207.06,9.12;6,333.16,412.49,207.00,9.07;6,333.16,423.96,194.73,9.07" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,405.88,400.97,134.41,9.07;6,333.16,412.49,203.00,9.07">The SMART Retrieval System -Experiments in Automatic Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall, Inc</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.16,439.56,207.14,9.07;6,333.16,451.08,207.02,9.07;6,333.16,462.60,207.12,9.12;6,333.16,474.12,43.08,9.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,460.41,439.56,79.90,9.07;6,333.16,451.08,207.02,9.07;6,333.16,462.60,68.25,9.07">A study of information seeking and retrieving. III. Searchers, searches, overlap</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Saracevic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kantor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,415.00,462.65,76.80,9.07">Journal of the ASIS</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="197" to="216" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.16,489.48,207.02,9.07;6,333.16,501.00,207.04,9.12;6,333.16,512.52,137.83,9.12" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<idno>AT&amp;T at TREC7</idno>
		<title level="m" coord="6,474.04,501.05,66.16,9.07;6,333.16,512.57,26.90,9.07">Proceedings of TREC7</title>
		<meeting>TREC7<address><addrLine>Gaithersburg, Md</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.16,528.12,207.12,9.07;6,333.16,539.40,207.03,9.12;6,333.16,550.92,137.83,9.12" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="6,434.12,528.12,106.17,9.07;6,333.16,539.40,134.28,9.07">Question Answering Supported by Information Extraction</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,478.84,539.45,61.35,9.07;6,333.16,550.97,26.90,9.07">Proceedings of TREC8</title>
		<meeting>TREC8<address><addrLine>Gaithersburg, Md</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.31,566.52,207.00,9.07;6,333.16,578.04,207.12,9.12;6,333.16,589.56,207.00,9.12;6,333.16,601.08,22.68,9.07" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="6,448.33,566.52,91.98,9.07;6,333.16,578.04,141.95,9.07">Evaluation of an inference network-based retrieval model</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,489.64,578.09,50.64,9.07;6,333.16,589.61,137.90,9.07">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="187" to="222" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.16,616.44,207.12,9.07;6,333.16,627.96,207.19,9.12;6,333.16,639.48,174.38,9.12" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="6,511.07,616.44,29.21,9.07;6,333.16,627.96,144.49,9.07">Disambiguation of Proper Names in Text</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Wacholder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ravin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,489.88,628.01,50.47,9.07;6,333.16,639.53,45.95,9.07">Proceedings of ANLP&apos;97</title>
		<meeting>ANLP&apos;97<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-04">April 1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
