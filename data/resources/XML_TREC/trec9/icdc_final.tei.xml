<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.38,49.35,419.20,12.93;1,194.12,65.79,223.82,12.93">Training Context-Sensitive Neural Networks With Few Relevant Examples for the TREC-9 Routing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,84.20,93.61,81.16,10.80"><forename type="first">Mathieu</forename><surname>Stricker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatique-CDC -Groupe Caisse des Dépôts Direction des Techniques Avancées</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.70,93.61,66.40,10.80"><forename type="first">Frantz</forename><surname>Vichot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatique-CDC -Groupe Caisse des Dépôts Direction des Techniques Avancées</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.54,93.61,74.99,10.80"><forename type="first">Gérard</forename><surname>Dreyfus</surname></persName>
						</author>
						<author>
							<persName coords="1,434.06,93.61,82.36,10.80"><forename type="first">Francis</forename><surname>Wolinski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatique-CDC -Groupe Caisse des Dépôts Direction des Techniques Avancées</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postCode>94114</postCode>
									<settlement>Berthollet, Arcueil cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire d&apos;Electronique</orgName>
								<address>
									<addrLine>10 rue Vauquelin</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.38,49.35,419.20,12.93;1,194.12,65.79,223.82,12.93">Training Context-Sensitive Neural Networks With Few Relevant Examples for the TREC-9 Routing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B09F58737FE3EC8A5D010CBB44AED00D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The present paper describes our second participation to the routing task; it features improvements over our previous approach <ref type="bibr" coords="1,156.08,255.22,84.88,9.02" target="#b3">[Stricker et al., 2000]</ref>. Our former model used a "bag of words" for text representation with a feature selection, and a neural network without hidden neuron (i.e. a logistic regression), to estimate the probability of relevance of each document. This approach was close to the ones proposed by <ref type="bibr" coords="1,470.24,278.62,62.56,9.02;1,79.22,290.32,23.50,9.02" target="#b1">[Schütze et al., 1995]</ref> or <ref type="bibr" coords="1,120.56,290.32,90.70,9.02" target="#b4">[Wiener et al., 1995]</ref> but its original feature was the use of very few relevant features for text representation (25 features per topic on the average for the TREC-8 Routing). In this paper, two main improvements are proposed:</p><p>• The feature selection defines target words for which vectors of local contexts are subsequently defined.</p><p>These vectors help disambiguate the target words and are defined by an analysis of both the relevant and the irrelevant documents of the training set. • This new representation requires large neural networks, which are therefore prone to overfitting. A regularization technique is applied during training to favor smoother network mappings, thereby avoiding overfitting. This was achieved by adding a weight decay term to the usual cost function. This approach led to good results on the MeSH Sample topics (S2RNsamp) and on the OHSUMED topics (S2RNr1 and S2RNr2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem and data description</head><p>The corpus for TREC-9 is the OHSUMED collection, which is a subset of the MEDLINE database. This corpus features documents from medical journals of years 1987 to 1991, which usually have titles and abstracts; some of them, however, only have a title. The documents were manually indexed using subject categories (Medical Subject Headings or MeSH). All documents contain the assigned MeSH headings, which are manual annotations (called .M field in the documents). There are three topic sets for the TREC-9 routing:</p><p>1. 63 topics from the original OHSUMED query set.</p><p>2. 4904 topics based on MeSH categories. 3. 500 topics chosen amongst the 4904 previous ones called MeSH sample topics.</p><p>The manual annotations could be used with the OHSUMED queries (as long as it was mentioned) but NOT with the MeSH queries (nor of course with the MeSH sample queries).</p><p>We submitted 2 runs for the OHSUMED queries (one without manual annotation and one with manual annotations) and one for the MeSH sample topics. We may observe that the number of relevant documents available per topic is small, especially for the OHSUMED queries since the median is 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Selection</head><p>Each document of the collection is first tokenized into single words, case being ignored. In the following, each word is considered as a single unit called feature. No stemming is performed. The goal of feature selection is to define, for each topic, a vector of features that are neither too frequent nor too rare, and are typical of the relevant documents. The choice of these features must be done very carefully since the quality of the filter relies heavily on this choice, irrespective of the model. These features must be chosen so as to allow a classifier to discriminate between relevant and irrelevant documents. Their number results from a tradeoff between two requirements: the larger the number of features, the larger the number of examples required to have a significant estimate of the classifier parameters; however, discarding features leads to information loss.</p><p>For each topic, a ranked list of features is computed, in which the top features are specific to the relevant documents. The method has already been used for the TREC-8 routing <ref type="bibr" coords="2,367.63,317.86,86.74,9.02" target="#b3">[Stricker et al., 2000]</ref> and is discussed in detail in <ref type="bibr" coords="2,114.25,329.56,61.78,9.02" target="#b3">[Stricker, 2000]</ref>. With this technique, rare and frequent words are discarded automatically; it is useful to discard rare words because it is not possible to compute reliable statistics from them, and to discard frequent words because they carry no information. This method is fully automatic and relies only on the computation of corpus frequency for each feature. There is no need, for example, to define a list of stop words that will depend on the language. Contrary to last year, a Gram-Schmidt orthogonalisation with a stopping criterion was not used, because the number of relevant documents per topic was too small. The twenty-five first features were selected, and defined the target words whose specific local context will be considered in the next section.</p><p>Figure <ref type="figure" coords="2,108.25,469.96,5.01,9.02">2</ref> shows two lists of the top 10 features obtained at the end of this step for the topics OHSU7 "young wf with lactase deficiency". The left column is the result when the manual annotations are ignored and the right one is the result when the manual annotations are taken into account. We may observe in the right column the presence of the words intolerance and galactosidases, which arise from the manual annotations.</p><p>Words are naturally prone to ambiguity, and can be used in many contexts. For example, the presence of the word intolerance, which has been selected for the topic OHSU7 (cf. Figure <ref type="figure" coords="3,383.77,79.66,3.64,9.02">2</ref>), does not imply that a text is about "young wf with lactase deficiency". In the past, the use of dictionaries to help disambiguate words has not proved efficient for information retrieval <ref type="bibr" coords="3,79.21,114.76,69.88,9.02" target="#b4">[Voorhees, 1993]</ref>. But disambiguation with corpus-based methods has been used successfully in <ref type="bibr" coords="3,484.81,114.76,47.97,9.02;3,79.21,126.46,56.74,9.02" target="#b0">[Cohen and Singer, 1996]</ref> and more recently in <ref type="bibr" coords="3,233.65,126.46,128.14,9.02" target="#b0">[Jing and Tzoukermann, 1999]</ref>. The basic idea of these methods is to disambiguate words through their local context. Therefore, in our approach, a local context of ten words (five words on either side of the word) is considered to define which words have the highest rate of co-occurrence near a target word in the training set. Actually, two context vectors are computed for each target word: one is computed from the set of relevant documents, and the other one from the irrelevant documents. Of course, the words with the highest rates of co-occurrence near a given word are stop words, which are useless for disambiguation. Consequently, the same procedure as used to achieve feature selection is applied to give a weight to each potential context. This method has the additional benefit of discarding automatically frequent words and rare words from the context vectors.</p><p>To compute these vectors, all relevant documents available were used, and five thousands irrelevant documents were chosen randomly. The context vectors defined by the relevant documents are called positive context vectors and those defined by irrelevant documents are called negative context vectors.</p><p>Figure <ref type="figure" coords="3,111.31,301.96,5.01,9.02">3</ref> shows examples of positive and negative context vectors for the topic OHSU7 with manual annotations. The left column shows context vectors computed from the relevant documents for five target words, and the right one shows context vectors for the same target words computed from the irrelevant documents. The contexts are taken into account only if they appear on more than two documents. It is worth noting that the presence of the word intolerance with lactose in its local context must be in favor of relevancy for the topic OHSU7. But, if the local context of intolerance contains the word glucose instead of lactose the importance of the presence of intolerance must decrease. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive context vectors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Choosing the size of the context vector</head><p>For each target word, the local context is chosen according to several criteria: the five first positive contexts are selected if they appear in more than two documents, and the five first negative contexts are chosen if they appear on more than ten documents. The number of documents required to take into account a context is larger for the irrelevant documents than for the relevant documents, due to the fact that irrelevant documents are much more numerous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Neural Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Definition of the architecture</head><p>The neural network architecture must reflect the representation defined above: the influence of a target word must decrease or increase according to its local context. Therefore, instead of having a single input per target word, the local context is included as indicated in the left side of Figure <ref type="figure" coords="4,372.67,205.06,3.91,9.02" target="#fig_1">4</ref>; the right side shows the entire neural network.</p><p>Each hidden neuron is a sigmoid function and the output of the network is a logistic function in order to keep the output in the range [0,1]. Thus, this architecture contains one hidden neuron for each target words i.e. twenty-five in our case as explained in section 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Choice of irrelevant documents</head><p>Previous experiments <ref type="bibr" coords="4,173.60,493.60,66.28,9.02" target="#b3">[Stricker, 2000]</ref> have shown that it was desirable to exclude from the training set irrelevant documents for which all the target words are absent. Therefore, amongst the five thousand irrelevant documents chosen randomly, only those that share words with the relevant documents are kept.</p><p>The components of the vector are coded using the Lnu scheme defined by <ref type="bibr" coords="4,378.26,540.40,60.70,9.02" target="#b2">[Singhal, 1996]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training with regularization</head><p>Our text representation increases the number of weights of the neural network; in addition, few relevant documents are available for training; therefore, the risk of overfitting increases, which makes the use of a regularization scheme mandatory. A penalty term is added to the usual cost function, which favors smoother functions. In our case, we use a weight decay regularization which has proven to be efficient <ref type="bibr" coords="4,458.65,614.80,46.47,9.02" target="#b0">[Krogh and</ref><ref type="bibr" coords="4,508.03,614.80,24.77,9.02;4,79.21,626.50,79.41,9.02" target="#b0">Hertz, 1992][Gallinari and</ref><ref type="bibr" coords="4,161.17,626.50,51.34,9.02" target="#b0">Cibas, 1999]</ref> and is very simple to implement.</p><p>To summarize, training is performed by minimizing a cost function G defined as:</p><formula xml:id="formula_0" coords="4,265.99,669.12,78.16,26.40">G = J + α 2 w i 2 Σ i = 1 p</formula><p>J is an of cross-entropy term appropriate for classification problems <ref type="bibr" coords="4,371.96,707.68,60.57,9.02" target="#b0">[Bishop, 1995]</ref>; the sum runs over all weights. α is a hyperparameter that defines the tradeoff between the two terms: if α is too small, the penalty term is negligible and overfitting tends to occur, whereas, if α is too large, weights will decay rapidly to zero and no training will occur.</p><p>The computation of the gradient of the new cost function is very simple since:</p><formula xml:id="formula_1" coords="5,268.28,102.82,74.15,15.57">∇G = ∇J + αw</formula><p>Where the quantity is computed by the well-known backpropagation algorithm.</p><p>In practice, all weights do not have the same dynamics, so that it is desirable not to use the same hyperparameter for all weights <ref type="bibr" coords="5,144.19,167.86,66.77,9.02">[MacKay, 1992b</ref><ref type="bibr" coords="5,210.96,167.86,66.05,9.02" target="#b0">][Bishop, 1995]</ref>. Therefore, three hyperparameters are used according to the following relation:</p><formula xml:id="formula_2" coords="5,195.07,194.27,220.06,29.13">G = J + α 1 2 w i 2 + α 2 2 w i 2 + α 3 2 w i 2 Σ ω ∈ W 2 Σ ω ∈ W 1 Σ ω ∈ W 0</formula><p>W 0 denotes the bias of the first layer, W 1 denotes the weights of the first layer of weights except for the bias, and W 3 denotes the weights of the second layer plus the bias of the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Values of the hyperparameters</head><p>The values of (α 1 , α 2 , α 3 ) must be chosen appropriately in order to achieve a satisfactory training. A solution would be to test several values and to pick up the best ones by cross-validation. Unfortunately, this method is intractable since there are three different parameters.</p><p>A theoretical approach based on Bayesian inference has been proposed, in order to determine automatically the values of these hyperparameters during training <ref type="bibr" coords="5,289.21,334.84,71.60,9.02">[MacKay, 1992a]</ref>. The results of the theory rely on the estimation of integrals that cannot be computed easily. MacKay <ref type="bibr" coords="5,363.49,346.54,74.32,9.02">[MacKay, 1992b]</ref> has proposed several approximations in a theoretical framework known as the evidence framework to make the computation feasible.</p><p>Unfortunately, these results did not provide good results on previous experiments.</p><p>Consequently, the values of the hyperparameters were chosen according to experience that we gathered previously on other corpuses (TREC-8 and Reuters21578): α 1 = 0.001 α 2 = 0.1 α 3 = 5.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We proposed three runs for the TREC-9 routing: 1. S2RNr1: 63 OHSUMED queries without using the manual annotations. 2. S2RNr2: 63 OHSUMED queries using the manual annotations. 3. S2RNsamp: 500 MeSH sample queries (without manual annotations).</p><p>The score is computed thanks to the uninterpolated average precision as described in <ref type="bibr" coords="5,445.03,507.34,87.77,9.02;5,79.21,519.04,21.64,9.02">[Hull and Robertson, 2000]</ref>.</p><p>Figure <ref type="figure" coords="5,107.95,530.74,5.01,9.02" target="#fig_2">5</ref> shows the comparisons of our runs with the other officials runs submitted for the TREC-9 Routing. For each subtopics (OHSU and MeSH sample), our method achieved the top scores. It is worth noting that the run S2RNr2 has better results than S2RNr1. It shows that our model can take advantage of the manual annotations without changing anything to our approach, since the difference relies only in the reading of the ".M field" which is considered as part of the text for S2RNr2. In the case of the MeSH sample, the gap between our run and the second one is bigger than in the case of the OSHU topics ; it seems that our method has taken advantage of the greatest number of relevant documents available for training on the MeSH sample.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,225.43,529.24,68.97,9.02;2,219.13,540.94,81.52,9.02;2,323.95,529.24,56.07,9.02;2,311.23,540.94,81.52,9.02;2,217.40,553.42,27.79,9.02;2,217.40,565.12,27.19,9.02;2,217.40,576.82,18.21,9.02;2,217.40,588.52,53.41,9.02;2,217.40,600.22,57.27,9.02;2,217.40,611.92,25.05,9.02;2,217.40,623.62,37.95,9.02;2,217.40,635.32,36.75,9.02;2,217.40,647.02,35.07,9.02;2,217.40,658.72,26.25,9.02;2,309.50,553.42,27.79,9.02;2,309.50,565.12,44.53,9.02;2,309.50,576.82,27.19,9.02;2,309.50,588.52,53.41,9.02;2,309.50,600.22,18.21,9.02;2,309.50,611.92,57.30,9.02;2,309.50,623.62,25.05,9.02;2,309.50,635.32,37.95,9.02;2,309.50,647.02,57.27,9.02;2,309.50,658.72,36.75,9.02;2,223.76,671.20,164.45,9.02"><head></head><label></label><figDesc>Examples of 10 top ranked list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,228.92,454.30,154.13,9.02"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Neural network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,175.52,710.86,260.93,9.02"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparisons of officials runs for the TREC-9 Routing.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,83.50,177.22,416.00,9.02;6,79.21,200.62,453.58,9.02;6,79.21,212.32,97.35,9.02;6,176.59,210.24,5.04,5.83;6,187.33,212.32,345.45,9.02;6,79.21,224.02,152.02,9.02;6,79.21,247.42,453.63,9.02;6,79.21,259.12,115.96,9.02;6,79.21,282.52,453.63,9.02;6,79.21,294.22,392.68,9.02;6,79.21,305.92,453.57,9.02;6,79.21,317.62,153.87,9.02;6,233.11,315.54,6.48,5.83;6,244.81,317.62,287.97,9.02;6,79.21,329.32,192.46,9.02;6,79.21,352.72,453.60,9.02;6,79.21,364.42,453.57,9.02;6,79.21,376.12,220.96,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,319.27,200.62,209.57,9.02;6,83.11,247.42,76.17,9.02;6,283.69,247.42,216.49,9.02;6,298.09,282.52,167.09,9.02;6,83.23,305.92,89.70,9.02;6,314.77,305.92,218.01,9.02;6,79.21,317.62,47.93,9.02;6,272.35,352.72,215.07,9.02">Information Retrieval Based on Context Distance and Morphology</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">W W</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">P</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">H</forename><surname>Cibas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">A</forename><surname>Tzoukermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,201.13,177.22,164.22,9.02;6,79.21,212.32,97.35,9.02;6,176.59,210.24,5.04,5.83;6,187.33,212.32,345.45,9.02;6,79.21,224.02,84.58,9.02;6,472.15,282.52,60.69,9.02;6,79.21,294.22,190.63,9.02;6,137.17,317.62,95.91,9.02;6,233.11,315.54,6.48,5.83;6,244.81,317.62,287.97,9.02;6,79.21,329.32,135.10,9.02">Proceedings of the 19 th Annual International Conference on Research and Development in Information Retrieval (SIGIR &apos;96)</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</editor>
		<meeting>the 19 th Annual International Conference on Research and Development in Information Retrieval (SIGIR &apos;96)<address><addrLine>Oxford; San Mateo CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kauffmann Publishers</publisher>
			<date type="published" when="1992">1995. 1995. 1996. 1996. 1999. 1999. 2000. 2000. 1999. 1999. 1992. 1992</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
	<note>NIST Special Publication</note>
</biblStruct>

<biblStruct coords="6,84.25,399.52,407.09,9.02;6,79.21,422.92,453.63,9.02;6,79.21,434.62,140.62,9.02;6,79.21,458.02,453.63,9.02;6,79.21,469.72,259.11,9.02;6,338.35,467.64,5.04,5.83;6,346.15,469.72,186.63,9.02;6,79.21,481.42,283.42,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,224.89,399.52,89.09,9.02;6,232.33,422.92,264.35,9.02;6,348.13,458.02,184.71,9.02;6,79.21,469.72,163.25,9.02">A Comparison of Classifiers and Document Representations for the Routing Problem</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mackay ; Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,249.79,469.72,88.53,9.02;6,338.35,467.64,5.04,5.83;6,346.15,469.72,186.63,9.02;6,79.21,481.42,215.36,9.02">Proceedings of the 18 th Annual International Conference on Research and Development in Information Retrieval (SIGIR&apos;95)</title>
		<meeting>the 18 th Annual International Conference on Research and Development in Information Retrieval (SIGIR&apos;95)</meeting>
		<imprint>
			<date type="published" when="1992">1992. 1992. 1992. 1995. 1995</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
	<note>A Practical Bayesian Framework for Backpropagation Networks</note>
</biblStruct>

<biblStruct coords="6,79.21,504.82,453.63,9.02;6,79.21,516.52,374.50,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,202.51,504.82,125.28,9.02">Pivoted Length Normalization</title>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,337.21,504.82,195.63,9.02;6,79.21,516.52,316.52,9.02">Proceedings of the 19th Annual International Conference on Research and Development in Information Retrieval (SIGIR&apos;96)</title>
		<meeting>the 19th Annual International Conference on Research and Development in Information Retrieval (SIGIR&apos;96)</meeting>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.97,539.92,449.86,9.02;6,79.21,551.62,293.26,9.02;6,79.21,575.02,453.61,9.02;6,79.21,586.72,453.57,9.02;6,79.21,598.42,99.94,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,199.87,539.92,332.97,9.02;6,79.21,551.62,140.75,9.02;6,253.03,551.62,89.87,9.02">Réseaux de neurones pour le traitement automatique du langage : conception et réalisation de filtres d&apos;informations</title>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">M</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">M</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Vichot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Dreyfus</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wolinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,384.85,575.02,147.97,9.02;6,79.21,586.72,339.79,9.02">Two Step Feature Selection for the TREC-8 Routing. Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<imprint>
			<date type="published" when="2000">2000. 2000. 2000</date>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
	<note type="report_type">Thèse</note>
	<note>de l&apos;université Paris VI. Stricker et al., 2000</note>
</biblStruct>

<biblStruct coords="6,83.65,621.82,449.16,9.02;6,79.21,633.52,35.55,9.02;6,114.79,631.44,5.04,5.83;6,122.59,633.52,410.21,9.02;6,79.21,645.22,61.30,9.02;6,79.21,668.62,453.61,9.02;6,79.21,680.32,453.63,9.02;6,79.21,692.02,113.08,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,221.95,621.82,254.28,9.02;6,369.55,668.62,163.27,9.02;6,79.21,680.32,32.47,9.02">Using WordNet to disambiguate words senses for text retrieval</title>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">E M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,482.65,621.82,50.16,9.02;6,79.21,633.52,35.55,9.02;6,114.79,631.44,5.04,5.83;6,122.59,633.52,405.98,9.02;6,120.67,680.32,412.17,9.02;6,79.21,692.02,44.77,9.02">Proceedings of the 16 th Annual International Conference on Research and Development in Information Retrieval (SIGIR&apos;93)</title>
		<meeting>the 16 th Annual International Conference on Research and Development in Information Retrieval (SIGIR&apos;93)</meeting>
		<imprint>
			<date type="published" when="1993">1993. 1993. 1995. 1995</date>
			<biblScope unit="page" from="317" to="332" />
		</imprint>
	</monogr>
	<note>Proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval (SDAIR&apos;95)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
