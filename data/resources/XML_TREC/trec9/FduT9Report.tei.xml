<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.50,113.34,325.58,14.58">FDU at TREC-9: CLIR, Filtering and QA tasks</title>
				<funder ref="#_bN2By4B">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_eDnaBQ9 #_htsf5SA">
					<orgName type="full">NSF of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,168.60,150.02,32.60,9.72"><forename type="first">Lide</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yuejie Zhang Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.80,150.02,70.28,9.72"><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yuejie Zhang Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.40,150.02,41.27,9.72"><forename type="first">Yikun</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yuejie Zhang Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.60,150.02,49.16,9.72"><forename type="first">Bingwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yuejie Zhang Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.50,113.34,325.58,14.58">FDU at TREC-9: CLIR, Filtering and QA tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">23D62BD3C57FA287F2C7CACB3D698A1D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year Fudan University takes part in the TREC-9 conference for the first time. We have participated in three tracks of CLIR, Filtering and QA.</p><p>We have submitted four runs for CLIR track. Bilingual knowledge source and statistical-based search engine are integrated in our CLIR system. We varied our strategy somewhat between runs: long query (both title and description field of the queries involved) with pseudo relevance feedback (FDUT9XL1), long query with no feedback (FDUT9XL2), median query (just description field of queries involved) with feedback (FDUT9XL3) and, the last, mono long query with feedback (FDUT9XL4).</p><p>For filtering, we participate in the sub-task of adaptive filtering and batch filtering. Vector representation and computation are heavily applied in filtering procedure. 11 runs of various combination of topic and evaluation measure have been submitted: 4 OHSU runs, 1 MeSH run and 2 MSH-SAMPLE runs for adaptive filtering, and 2 OHSU runs, 1 MeSH run and 1 MSH-SAMPLE run for batch filtering.</p><p>Our QA system consists of three components: Question Analyzer, Candidate Window Searcher and Answer Extractor. We submitted two runs in the 50-byte category and two runs in the 250-byte category. The runs of "FDUT9QL1" and "FDUT9QS1" are extracted from the top 100 candidate windows. The other two runs of "FDUT9QL2" and "FDUT9QS1" are extracted from the top 24 candidate windows.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Cross-Language IR</head><p>We focused our attention on Chinese document indexing and query translation. All query processing was fully automatic and both long and short query translation are covered.</p><p>Our overall strategy to CLIR task is to translate English query into Chinese word list, since we feel it is not feasible to build a document translation system in such a short period, while it is much more reasonable to disambiguate word sense in context of long query by statistical approach, such as POS and knowledge. Once queries have been translated, we use IR techniques, which is a variant of MIT's approach <ref type="bibr" coords="1,117.16,519.02,13.34,9.72" target="#b0">[1]</ref> and probabilistic methods to obtain relevant document list. The whole corpus has been indexed with Chinese NLP techniques developed by our group in recent years <ref type="bibr" coords="1,427.80,532.25,9.68,8.10" target="#b1">[2]</ref>. Finally, we also explore the weight of words in both of the title and description fields.</p><p>The system infrastructure is illustrated in figure <ref type="figure" coords="1,302.47,555.02,3.94,9.72" target="#fig_0">1</ref>.1. Description of each part is followed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Indexer</head><p>We explored two different indexing methods for our CLIR task, one is word-based Chinese indexing module, and the other is n-gram based indexing module. Because Chinese is different from English in that there are no extra spaces between Chinese words, we must first segment Chinese character sequence into words or n-grams in order to index documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Chinese word segmentation module</head><p>Figure <ref type="figure" coords="2,132.09,202.82,4.38,9.72" target="#fig_0">1</ref>.2 illustrates the architecture of our word segmentation sub-system. Given a document, it is first divided into a sequence of sentences (sub-sentences) by punctuation such as full stop, comma, etc. Each sentence then passes through the sentence segmentor and is segmented into a sequence of words. Finally, a text-level post-processor will act on the word sequence and generate the final segmentation result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Dictionary</head><p>Two kinds of dictionaries are used during the segmentation process. One is the static dictionary, which records Chinese lexical words and is unchangeable. The other is the OOD (out-of-dictionary words) cache, which records the newly found OOD and changes dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Sentence segmentor</head><p>The input sentence is first segmented by both static and dynamic dictionary. Ambiguous strings are handled at the same time. We use a pattern-matching module to recognize those OODs with fixed structure pattern, such as money, date, time, percentage and digit. The recognition module of person's name, place, organization and transliteration is more complex. Contextual and structural information both play important roles in identifying these kinds of OODs. The former can provide external evidence for deciding word boundary and predicting the category of OOD. The latter can provide internal evidence for suggesting and validating the appearance of certain OOD. For example, in Chinese, family surnames are stereotypical. We've made lots of statistical analysis on various categories of OOD and built correspondent identification modules for each. Each module works independently. A named entity arbitrator will take effect when two or more kinds of name entities conflict with each other and select the most probable one.</p><p>Beside these OOD types mentioned above, there are still many other kinds of OODs. We can also recognize some OODs according to its string frequency and internal characters' mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Document post-processor</head><p>During the sentence-by-sentence segmentation, some OODs will not be recognized until they occur several times. Therefore, the OOD cache changes continuously until the whole document passes through the sentence segmentor. After that, a document post-processor based on the final content of the cache is necessary to detect missed or mistakenly segmented words before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">n-Gram based Tokenization</head><p>We also implement n-Gram based tokenization process, which does not need sophisticated segmentation method. The document is simply cut into sequence of bigrams. We want to know whether the effectiveness of IR based on n-gram is comparable, inferior or superior to that based on word segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3">Word Indexing</head><p>Every document in the corpus is cut into no more than 64K segment to make indexing procedure more robust and normalize the document length. After being segmented, text id, term frequency, document frequency and term position are stored for the task. No stop word is removed from the invert file, since the corpus is rather small.</p><p>In order to optimize the disk space and I/O in retrieval time, we have also implemented invert file compression. The file was then decrease to about one half of its original size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Query Translation</head><p>The essence of cross-language information retrieval is to use queries in one language to retrieval documents from a pool of documents written in other languages. This may be achieved by using query translation, document translation, or by using both query and document translation.</p><p>Here, we adopt query translation as the dominant strategy, use English query to be translated object, and utilize English-Chinese bilingual dictionary as the important knowledge resource to acquire correct translations. So by using our Chinese Information Retrieval system, the complete English-Chinese CLIR process can be implemented successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1">Knowledge Source Construction</head><p>The knowledge source used in English-Chinese-oriented CLIR system mainly includes dictionary knowledge and Chinese Synonym Dictionary. In addition, stopword list and word morphological resumption list are also utilized in our system. In fact, dictionary is a carrier of knowledge expression and storage, which involves almost all information about vocabulary, namely static information.</p><p>(1) English-Chinese Bilingual Dictionary This dictionary is mainly used in translation processing in word level and phrase level. And it consists of three kinds of dictionary component as follows:</p><p>l Basic Dictionary--A basic knowledge source independent of particular field, which records basic linguistic vocabulary; l Technical Terminology Dictionary -Recording terminology knowledge in a particular technical field, which is mainly referred to Hong Kong commercial terminology knowledge and incorporated in the basic dictionary; l Idiom Dictionary--Recording familiar fixed matching phenomena, such as idiom and phrase. l The whole bilingual dictionary involves almost 50,000 lexical entries. And each entry is established as the following data structure: (2) Chinese Synonym Dictionary ( ) Actually, this dictionary is a thesaurus, which involves nearly 70,000 entries. All entries are arranged according to specified semantic relations. It is mainly used in expanding translation that has passed through translation processing, namely query expansion.</p><p>While the stopword list is used in tagging the stopwords in English query, and the English morphological resumption list which describes all irregular varieties about vocabulary is used in morphological resumption of words with irregular variety forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2">Translation algorithm</head><p>The basic framework of English-Chinese-oriented translation algorithm is mainly divided into three parts, as shown in Figure <ref type="figure" coords="4,189.72,336.62,3.92,9.72" target="#fig_0">1</ref> • Preprocessing --including sentence segmentation, punctuation tagging and capital-to-lower letter conversation for English query; • Pre-analysis --including stop words tagging, word morphological resumption and POS tagging processes;</p><p>Considering that translation processing is related with some stopwords, the stopwords must be tagged by stopword list. Because there are some words with variety forms in English query, translation knowledge cannot be induced correctly. So by using English-Chinese bilingual dictionary, morphological resumption lists for irregular variety and heuristics for regular variety, we get words' original form from the process called "morphological resumption". To analyze word part-of-speech, we develop a HMM-based (Hidden Markov Mode) Part-of-Speech Tagger. l Translation processing --including translation processes in two levels: word level and phrase level. Word level translation: By using the basic vocabulary part of English-Chinese bilingual dictionary, this process mostly implements translation word by word. For word disambiguation, a word may correspond with several kinds of different sense. Word sense is related with particular word, and cannot be given without particular linguistics environment. The condition of linguistics environment may be syntactic and semantic parameters. When selecting a particular word, the difference mark of word should be chosen. This difference mark represents a certain syntactic and semantic feature, and identifies the sense of word uniquely, namely Concept Code. The concept code together with lexical entry can decide a certain word sense to accomplish word sense disambiguation. For machine translation, word disambiguation should be a very important problem. But in our CLIR system, in some degree, word disambiguation has not taken some obvious affect to retrieval efficiency. At the same time, in order to provide more query information to retrieval system, by using "Chinese Synonym Dictionary", expansion operation is done for translation knowledge through translation processing. According various synonymous relations described in the dictionary above, all synonyms corresponding with translation knowledge is listed, namely completing query expansion process. Thus, more affluent query information can be provided to retrieval system. So the retrieval efficiency is increased greatly, and the retrieval performance is improved.</p><p>Phrase level translation: This process is implemented based on the idiom dictionary part of English-Chinese bilingual dictionary. The recognition of near distance phrase and far distance phrase is an important problem. Here, by adopting Greedy Algorithm, the recognition and translation processing of near distance phrase is mainly completed, shown as the following: l Acquiring phrase set which have some query word as the head word of the idiom from English-Chinese bilingual dictionary; l Identifying the phrases which have the same word as head word and the same number of word as the phrase in the above set; l Comparing each one of the identified phrases and every member in the correspondent phrase set and finding out the matched phrase with the maximum length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Experiment</head><p>Our search engine scores document by maximum likelihood ratio, put forward by Spoken Language Systems Group in MIT <ref type="bibr" coords="5,190.20,423.65,9.22,8.10" target="#b0">[1]</ref>. In our retrieval experiment, we use the TREC-5 Chinese task as the "training" data set for tuning and optimizing our retrieval model. Finally, our best run has achieved the mAP (mean average precision) of 0.3869, which is about the same as the best result at that time.</p><p>After that, we submit four runs for CLIR official evaluation this year. Figure <ref type="figure" coords="5,440.47,458.42,4.36,9.72" target="#fig_0">1</ref>.4 is the official precision and recall curve and the mAP score of our 4 CLIR runs. The first three of them are automatic query translation run, using our word segmentation approach for indexing, while the monolingual run we submit uses n-gram based segmentation. Although the results are not as good that of training results, the run of "fdut9xl2" still can achieve the mAP of near 0.30. We have outstanding performance for automatic query translation run: most of the queries outperform the average in the run "fdut9xl1". However, the monolingual run is not as good as we expected. We speculate that it may be due to our sophisticated segmentation method, which could correctly segment the names of people, place and organization etc. In other word, indexer based on word segmentation performs much better than indexer based on n-gram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Filtering</head><p>For filtering, we participate in the sub-task of adaptive filtering and batch filtering. Our research focuses on how to create the initial filtering profile and threshold and then modify them adaptively.</p><p>Our batch runs share the same adaptation module with adaptive runs. Therefore, our batch runs are actually batch adaptive runs.</p><p>There is only a slight difference in the initialization (in other word, training) module of our batch and adaptive runs. Full relevance judgement is provided in batch filtering, while only a small proportion of relevance judgement is provided in adaptive filtering. As a result, for batch run, we can obtain a set of "Negative" documents, which are those irrelevant documents with high similarity to the filtering profile, and then make use of such documents. For adaptive run, we try to discover more pseudo-relevant documents based on the topic and limited relevance judgement in order to optimize the initial profile.</p><p>Following is the detailed introduction to our training and adaptation module of our adaptive and batch task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training of adaptive filtering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Topic processing</head><p>Topic is being processed as such: Firstly, every word in the topic is labeled with one of four attributes: title words; description words; negative words (words which behinds the word "without"); domain dependent stopwords such as "document" and "describe". Each kind of attribute is assigned with a coefficient. If one word occurs several times in the topic and different attributes are labeled, the maximum coefficient is chosen for it. Different coefficients are chosen for OHSU and MeSH topics.</p><p>As we know, for OHSU topic, title is the description of patient, and description is the information request. Both are important. However, for MeSH topic, title is MeSH concept name and description is the definition of the concept. We have found the description part is not as important as the title. For example, the description of the concept of "abdomen" is that "the portion of the body that lies between the thorax and the pelvis". If we expand the initial query with such word as "thorax" or "pelvis". the performance will even be hurt.</p><p>In our experiment, the coefficients of OHSU topics are set to 1, 1, -1 and 0 respectively, while those of MeSH topics are 1, 0, 0 and 0 respectively.</p><p>The weight of each topic word is set to be the product of its coefficient multiplied with Smart's ltc weight: ltc=log(N/n) <ref type="bibr" coords="7,164.48,159.62,12.32,9.72" target="#b2">[3]</ref>, where N is the total number of documents and n is the number of documents in which the word occurs. We adopt ltc weight because the simplicity in computing. We have also tried other weighting formulas with relevant information and do find they can lead to better performance when only topic information is utilized in profile creation. However, once topic vector is combined with feature vectors from the training documents to form the initial profile, more complicated weight algorithm no longer ensures better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Feature selection</head><p>Since the total number of all words is very large and then it cost more time in similarity computation, we decide to select some important words from them. First, we use Porter's stemmer to get the root form of every word. Then we remove the stopwords and low frequent words (occur no more than 6 times in the training document sets). Then we compute the logarithm Mutual Information between remaining words and topics:</p><formula xml:id="formula_0" coords="7,191.40,311.43,278.34,37.54">( ) ( )       = i j i j i w P T w P T w MI | log ) , ( log (2.1)</formula><p>Where, w i is the ith word and T j is the jth topic. Higher logarithm Mutual Information means w i and T j are more relevant. P(w i ) is estimated by maximal likelihood method. Since the total number of relevant documents is very small, ( )</p><formula xml:id="formula_1" coords="7,227.40,376.22,34.40,11.94">j i T w P |</formula><p>is estimated by Turing-Good method.</p><p>For each topic, we select those words with logarithm Mutual Information higher than 3.0 and occurs more than once in the relevant documents. Thus the average feature number of each topic is around 100. Logarithm Mutual Information is not only used as the selection criterion, but also as the weight of feature words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Creating initial profile</head><p>Each topic profile is represented by a vector which is the weighted sum of topic vector, feature vector from positive (relevant) documents and feature vector from pseudo relevant documents with the coefficient of A, B and C.</p><p>We add a procedure of pseudo feedback to acquire more relevant documents for training. Those documents that have highest similarity and don't occur in the positive documents are regard to be relevant.</p><p>The initial profile is created by two-phase method. First we get the pseudo relevant documents; then we get the initial profile and re-compute the optimal initial threshold.</p><p>During the first phase, A, B and C are set to be 0.4:1.0:0. We set C to 0 because we have no similarity score at this time. Then some documents are selected as pseudo positive documents. As for how many documents should be appended, we adopt two methods. The first method choose the N highest similar documents which don't occur in the positive documents; while the second method choose those documents whose similarity is higher than a fixed scale ( α ) of the highest similar positive document. These two method lead to similar results. Here we set α = 0.45, or N=10.</p><p>During the second phase, A, B and C are set to be 0.25:1.0:0.25. The parameter of A becomes smaller because now we have so much positive document that topic vector becomes relatively less important. Therefore, we have got the initial profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Similarity Computation</head><p>The similarity between the profile and training documents is computed by the cosine formula:</p><formula xml:id="formula_2" coords="8,155.40,94.22,314.34,53.34">) )( ( * ) , ( 2 2 ∑ ∑ ∑ = = k jk k ik k jk ik j i p d p d Cos p d Sim θ . (2.2)</formula><p>Where, p j is the profile vector of the jth topic and d i is the vector representation of the ith document. d ik , the weight of the kth word in d i , is computed as such:</p><formula xml:id="formula_3" coords="8,346.80,160.89,65.00,14.87">ik ik tf d log 1 + =</formula><p>, where tf ik is the term frequency of the kth word in the ith document.</p><p>Documents are first removed of the redundant tag information and then stemmed. Only the document identifier, tile and abstract are reserved, while MeSH headings and other fields are all removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Setting initial threshold</head><p>Once the threshold is set, those documents with similarity greater than the threshold are regarded to be relevant and those documents with similarity smaller than the threshold are regarded to be irrelevant. Then we can compute the evaluation criteria such as T9U and T9P under different threshold. Thus the initial threshold are set to be the threshold which can result in the largest T9U or T9P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training of batch filtering</head><p>Training of batch filtering is quite similar to adaptive filtering. The only difference is that feature vectors are now extracted from positive and negative (irrelevant but with high similarity) document samples. Each topic profile is represented by a vector which is the weighted sum of topic vector, feature vector from positive documents and feature vector from negative documents with the coefficient of A, B and C. Initial threshold is also set in a two-phase method. At the first phase, A, B and C are set to be 0.25:1.0:0. For each vector profile, we calculate its similarity with every training document and then set the temporary similarity threshold. After that, negative documents are selected to be those irrelevant documents with similarity higher than the temporary threshold and could lead to wrong judgement. During the second phase, A, B and C are set to be 0.25:1.0:-0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adaptation</head><p>For adaptive and batch filtering we adopt the same adaptation procedure. Figure <ref type="figure" coords="8,454.52,473.42,4.38,9.72" target="#fig_3">2</ref>.2 shows the architecture for the adaptation. For each document in the stream, its similarity with the specific topic profile is computed. If the similarity is greater than the threshold, it is assumed to be relevant. Then we search the "qrel" file to see whether it is really relevant and do some adaptation accordingly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Adaptation of threshold-T9P</head><p>Thresholds are adjusted after β documents have been processed (for this experiment, β =8000). For different evaluation measure of T9P and T9U, the adaptation is also different.</p><p>In order for the optimization of T9P, the purpose of threshold adaptation is to make sure that about 50 documents are retrieved during 4 years. Therefore M documents should be retrieved in the βdocument interval. For each topic, we define:</p><p>Cor: # of documents correctly retrieved in the interval Rtv: # of documents retrieved in the interval Cor1: # of documents correctly retrieved heretofore Rtv1: # of documents retrieved heretofore M1: # of documents should be retrieved heretofore T: Similarity threshold Algorithm:</p><p>If Cor&lt;Rtv*0.20 &amp;&amp; Rtv&gt;max(M,4) , then T*=1.2 (If the precision is too slow, the threshold should be increased quickly) If Rtv&gt;M &amp;&amp; Rtv1&gt;M1, then T*=1.1 (If documents are retrieved more than required, the threshold should be increased) If Rtv&lt;M &amp;&amp; Rtv1&lt;M1, then T*=0.9 (If documents are retrieved less than required, the threshold should be lowered) We have supposed that we can retrieve fewer documents at first and then retrieved more documents after profiles are updated. However, such experiment cannot lead to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Adaptation of threshold-T9U</head><p>In order for the optimization of T9U, the purpose of threshold adaptation is to make sure that documents should be retrieved with high accuracy. But if the precision is too high, thresholds should also be decreased to retrieval more documents and then get larger T9U.</p><p>Algorithm: If Cor&lt;Rtv*0.10 &amp;&amp; Rtv&gt;max(M,4) , then T*=1.2 (If the precision is too slow, the threshold should be increased quickly) If Rtv-1&gt;M &amp;&amp; Cor+1&gt;Rtv*0.33, then T*=1.1 (If enough documents have been retrieved and precision is too low, the threshold should be increased)</p><p>If Rtv&lt;M &amp;&amp; Cor-1&gt;Rtv*0.25 or Cor-1&gt;Rtv*0.33 or Cor=0, then T*=0.9 (If documents are retrieved less than required with moderate precision, or precision is too high, or no document is retrieved, the threshold should be lowered)</p><p>In addition, if two irrelevant documents are retrieved continuously, T*=1.1.</p><p>Here, M represents the number of documents should be retrieved in the β -document interval. However, adaptive filtering systems cannot take into account the percentage that are relevant over the entire test set for a particular query in building their retrieval rules. Under such condition, M is estimated from the training corpus while relevant and pseudo relevant documents are taken into account. Although M is actually variable among different topics, we just use the average value for the convenience for computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Adaptation of topic profile</head><p>Once a retrieved document has been judged to be relevant, it is added to the positive document set for further adaptation, otherwise it is added to the negative document set. During profile adaptation, feature vectors are extract from positive documents and negative documents. The new topic profile is the weighted sum of topic vector, feature vector from positive documents and feature vector from negative documents. Thus not only the weight of features but also the feature words can be adjusted. The coefficient of A, B and C are still 0.25, 1.0, and -0.25.</p><p>Since relevant document is too scarce, we adjust the topic profile only after β *4 documents have been processed. In fact, after processing β document, adaptation is triggered. Among 4 successive adaptation, the first 3 are threshold adaptation and the last one is profile adaptation. We don't adjust threshold and profile simultaneously because the threshold is optimized for the original profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation results</head><p>This year Fudan University has submitted 11 runs for adaptive filtering and batch filtering. We submit no routing runs. Table <ref type="table" coords="10,208.96,184.22,4.55,9.72" target="#tab_2">2</ref>.1~2.3 summarize our adaptive and batch filtering runs.</p><p>Table <ref type="table" coords="10,129.87,196.22,4.38,9.72" target="#tab_2">2</ref>.1 shows the results of OHSU topics. The "score" column is the score of each run under different evaluation measure. Micro recall and precision are calculated globally for all the topics, while macro recall and precision are averaged across all the topics <ref type="bibr" coords="10,337.26,220.22,11.43,9.72" target="#b3">[4]</ref>. The last columns give the number of topics in which our runs perform better, equal and worse than median ones. And the numbers inside the parentheses shows the number of topics in which our runs perform best. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Question Answering</head><p>Question Answering is an interesting challenge for NLP researchers because it requires a combination of many traditional NLP techniques, such as tokenization, parsing, named entity identification and retrieval.</p><p>The next section introduces Fudan TREC-9 question answering system. It is followed by the detailed discussion of three main components. Followed are the evaluation results. Finally we will discuss the future prospects of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Fudan Question Answering System</head><p>Similar to other systems <ref type="bibr" coords="10,216.41,696.02,12.52,9.72" target="#b4">[5]</ref>, our system consists of three components: Question analyzer, Candidate Window Searcher and Answer extractor. The architecture is illustrated by Figure <ref type="figure" coords="10,475.93,708.02,3.91,9.72">3</ref>  Initially, with a question parser and semantic mapping chart, we process the given questions and extract useful information: answer type, question focus and the syntax pattern of question. Furthermore, the Question Analyzer generates a set of query terms.</p><p>Each document retrieved by our ranked Boolean search engine is divided into segments of about 4k-byte. Each candidate segment is assigned with a score according to its similarity to the query generated by question analyzer.</p><p>The top-ranked segments are then passed to the Answer Extractor. A named entity finder based on HMM model <ref type="bibr" coords="11,142.31,364.22,13.39,9.72" target="#b5">[6]</ref> and a syntax parser based on chart algorithm are involved in this procedure. The extraction and ranking of the final answer are based on some empirical feature matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Question Analyzer</head><p>The Question Analyzer attempts to excavate all available information inside the given question and generate a query for search engine.</p><p>In order to extract the real answer from the tremendous collection of documents, it's very important to know what the question is asking for. Fortunately, quite a few questions request certain type of answer. For example, for the question "Who invented the paper clip?", a person name is needed. We can either judge the question's answer type directly by its interrogative (who, where, when), or by semantic mapping of other words in question (e.g. how much, what city, which year, etc.). The semantic class of the answer type is listed in table <ref type="table" coords="11,306.44,506.42,3.92,9.72">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Entity Finder</head><p>Inverted List Not all questions can provide obvious clue of their goals. Some questions, which start from what and which, are ambiguous and scarcely say anything about specific answer type. We solve it by defining a concept named question focus.</p><p>Question focus can be interpreted as the most important part of question, which distinguishes the question from others at the most. It may be a word or a sequence of words. Low frequency word and proper noun/phrase is commonly chosen as question focus. For example, in the question "What culture developed the idea of potlatch?", the question focus is potlatch. Question focus makes it easier to filter the irrelevant document and locate the exact answer.</p><p>The syntax pattern of question is generated by question parser. The purpose of parsing is to predict the possible syntax structure of answer sentence. Take the question "What is a caldera?" for an example, the possible answer sentence may be like "Caldera is ... ", "Caldera, ... " or "... known as Caldera.".</p><p>Finally, the question analyzer produces a set of query term and sends them to the search engine. Each term of the query comprises three fields: a. Query Term, word or phrase extracted from the question. b. Term Rank, calculated by the term's syntax role in question and the word frequency. c. Search Mode, the suitable searching method for this query term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Candidate Window Searcher</head><p>Among the large document collection, we try to find some segments of information that may be relevant to the question in order to restrict the scope for further processing. In this phase, we search the entire corpus for the query and generate N best candidate windows, from which we will extract answer to each question.</p><p>Our search engine makes use of the Boolean retrieval model, which is modified to suit for the QA task. Firstly, we define four kinds of search modes, named "Single Word Search", "Common NP Search", "Proper NP Search" and "Quoted Part Search" respectively.</p><p>Single Word Search is used to search the query term, which has only one word. It aims at finding all the occurrences of the word in the corpus.</p><p>Common NP Search is just like operator "OR" in Boolean Information Retrieval in that the words being searched need not co-occur with each other. It is often used to search people's name, which often occurs partially in the corpus.</p><p>Proper NP Search is somewhat like operator "OR" in Boolean Information Retrieval, except it discards sentences that only contain familiar query words that can be found in dictionary. Therefore, the remaining sentences just contain OOD query words, such as named entity. For example, for the query term "Star Trek", the search engine will only retrieve sentences that contain the word "Trek". And the sentence contains both of the words "Star Trek" will be ranked higher than that contains "Trek" only. It makes intuitive sense that the word "Star" occurs too often in the corpus to depict the information need.</p><p>Quoted Part Search performs just like operator "EXACT MATCH" in Boolean Information Retrieval. It is used to search quoted name, such as name of films or books. It not only requires query words to co-occur, but the order of query words to be matched exactly in the corpus as well.</p><p>Then, we create N-best window ranked by their window scores.</p><p>For every matched sentence among the corpus, we scan forward and backward within the same articles to get a candidate window with the size of no more than 4k bytes. That is, we try to locate all 4k-byte windows containing one or more matched sentences. However, these matched sentences are included in only one of those candidate windows, no overlap is allowed.</p><p>The windows are scored by the formula given below:</p><formula xml:id="formula_4" coords="12,226.80,665.07,242.94,29.21">∑ = + = k 1 t t qs i i t t i w * ) sc msc c mc * 2 ( WS (3.1)</formula><p>Where, t mc is the number of terms in each query that locate in the window, while t c is the total number of terms in each query. i msc is the number of total matched sentences in the window, while i sc is the total number of sentences containing in the window. The former factor indicates the coverage of the query terms for the candidate window, whereas the latter favors candidate window with more occurrences of query terms. k is the number of query term for the question. i qs w is the weight of the ith query term. We assign weights to each query term according to its search mode and rank. Sort all the windows by its score and select top N best window for further processing. In our experiments, we use 2k windows for every question at most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Extractor</head><p>The Answer Extractor identifies and extracts answers from the candidate windows. Each candidate window first passes through a named entity finder, which identifies names of person, location and organization, monetary units, dates, time, etc. By use of the answer type and question focus, all possible answers are located within the candidate window. For each possible answer, a 250-byte-long section in the candidate window named answer-window is then created. We evaluate each answer-window using the following four scores: 1) Matched_queries-score: Compute a match-score for each query term and sum them all. The match-score of query term is determined by its search mode, the match degree and the distance to answer-window of each match situation in the candidate window. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation</head><p>In this section, we describe the performance of our system. The system is evaluated by the Mean Reciprocal Answer Rank (MRAR):</p><formula xml:id="formula_5" coords="13,189.60,606.08,280.14,28.63">∑ = = n i i rank n MRAR 1 ) / 1 ( 1 .<label>(3.2)</label></formula><p>We submitted two runs in the 50-byte category and two runs in the 250-byte category. The first two runs are generated by using the top 100 candidate windows. The next two runs are by processing only top 24 candidate windows. The strict evaluation results are presented in Table <ref type="table" coords="13,417.69,661.22,3.92,9.72">3</ref>.3.</p><p>The accuracy (measured by the percentage of questions correct) of our system fluctuates on various answer type. It is pleasant on questions demanding for PERSON (58%) and LOCATION (55%), but disappointing on DATE (35%) and NUMBER (25%). It is mainly because we concentrated on training the statistical model and worked little on rule-based identification, which is relatively simple but more useful on number-relevant named entity. On the training corpus of TREC-8, our system did best while using top 24 candidate windows. But in TREC-9, the 250-byte run using top 100 candidate windows (FDUT9QL1) does better than that of top 24 one (FDUT9QL2). We presume it is caused by the variation on question style. The question of TREC-9 is shorter than that of TREC-8 on average. And some new question structure is too unfamiliar for us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion and Future Work</head><p>It is our first time to take part in TREC. Attending TREC-9 provides us further understanding of NLP technology. We have accumulated such knowledge resources as bilingual dictionary and Chinese synonym dictionary. We have also designed several NLP tools during this period, such as named entity finder, query translator, parser and search engine.</p><p>Although moderate performance has been achieved in our three systems, we still have a lot of things to do in the future. First, we need to enrich our knowledge resources, especially in English. We need to acquire knowledge from different domains and employ a comprehensive machine dictionary (e.g. WORDNET or HowNet) for semantic analysis. Currently, our three systems are developed almost independently. Next time, we will try to implement techniques developed for one system to another. For example, feature selection of filtering system can also play important role in the search engine. And relevance feedback in search engine is quite similar to adaptation in filtering.</p><p>Finally, we hope to apply the ideas and notions learned from TREC to corresponding tasks of our native language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,225.00,572.45,145.23,8.10"><head>Figure 1</head><label>1</label><figDesc>Figure 1.1 System architecture for CLIR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,186.60,366.65,222.61,8.10"><head>Figure 1 . 2</head><label>12</label><figDesc>Figure 1.2 Architecture of our word segmentation sub-system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,126.60,532.25,291.63,8.10"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1.4 official Prec and Recall Curve and mAP score on TREC-9 CLIR Task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,102.60,357.02,411.19,9.72;6,81.60,369.02,431.57,9.72;6,81.60,381.02,432.74,9.72;6,81.60,393.02,432.76,9.72;6,81.60,405.62,50.48,9.72"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.1 shows the architecture of the training in adaptive filtering. At first, topics are changed into topic vectors, while feature vectors are extracted from positive and pseudo-positive document samples.The initial profile is the weighted sum of topic and feature vectors. Then we compute the similarity between the initial profile and all training documents to find the optimal initial threshold for every topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,192.60,417.05,210.05,8.10"><head>Figure 2 . 1</head><label>21</label><figDesc>Figure 2.1 Architecture of the training in adaptive filtering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,222.60,524.45,150.02,8.10;8,354.00,540.65,28.30,8.10;8,354.00,550.85,38.47,8.10"><head>Figure 2</head><label>2</label><figDesc>Figure 2.2 Architecture for the adaptation Positive documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,479.84,708.02,11.74,9.72"><head></head><label></label><figDesc>.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="11,166.80,100.25,180.66,8.10"><head>Figure 3 . 1 :</head><label>31</label><figDesc>Figure 3.1: Architecture of the Fudan QA Systems</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,93.60,336.62,411.71,222.33"><head></head><label></label><figDesc>.3.</figDesc><table coords="4,93.60,354.05,411.71,204.90"><row><cell></cell><cell cols="4">Figure 1.3 Basic framework of English-Chinese-oriented query translation algorithm</cell></row><row><cell></cell><cell cols="2">English-Chinese</cell><cell>Chinese</cell><cell></cell></row><row><cell></cell><cell cols="2">Bilingual Dictionary</cell><cell cols="2">Synonym Dictionary</cell></row><row><cell></cell><cell></cell><cell cols="2">Knowledge Source</cell><cell></cell></row><row><cell></cell><cell>Segmentation</cell><cell cols="2">Stopword Tagging</cell><cell>Word Level</cell></row><row><cell>English</cell><cell>Punctuation</cell><cell>Morphological</cell><cell></cell><cell>Query</cell><cell>Chinese</cell></row><row><cell>Query</cell><cell>Processing</cell><cell>Resumption</cell><cell></cell><cell>Expansion</cell><cell>Query</cell></row><row><cell></cell><cell>Capital-to-Lower</cell><cell cols="2">Part-of-Speech</cell><cell></cell></row><row><cell></cell><cell>Conversation</cell><cell>Tagging</cell><cell></cell><cell>Phrase Level</cell></row><row><cell></cell><cell>Preprocessing</cell><cell>Pre-analysis</cell><cell></cell><cell>Translation Processing</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,82.80,262.25,426.58,96.30"><head>Table 2 .</head><label>2</label><figDesc>1 Adaptive and batch filtering for OHSU topics</figDesc><table coords="10,82.80,262.25,426.58,86.10"><row><cell>Task</cell><cell cols="2">Measure Run</cell><cell>Score</cell><cell>Recall</cell><cell></cell><cell>Precision</cell><cell></cell><cell cols="3">Comparison with median</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Micro Macro Micro</cell><cell>Macro</cell><cell>&gt;(Best)</cell><cell>=</cell><cell>&lt;</cell></row><row><cell>Adaptive</cell><cell>T9U</cell><cell>FDUT9AF2</cell><cell>9.6</cell><cell>0.212</cell><cell>0.181</cell><cell>0.473</cell><cell cols="2">0.319 51(7)</cell><cell>6</cell><cell>6</cell></row><row><cell></cell><cell>T9P</cell><cell>FDUT9AF1</cell><cell>0.264</cell><cell>0.277</cell><cell>0.300</cell><cell>0.283</cell><cell cols="2">0.271 37(6)</cell><cell>9</cell><cell>17</cell></row><row><cell></cell><cell></cell><cell>FDUT9AF3</cell><cell>0.265</cell><cell>0.276</cell><cell>0.301</cell><cell>0.286</cell><cell cols="2">0.273 39(5)</cell><cell>8</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell>FDUT9AF4</cell><cell>0.249</cell><cell>0.263</cell><cell>0.285</cell><cell>0.278</cell><cell cols="2">0.259 34(9)</cell><cell>2</cell><cell>27</cell></row><row><cell>Batch</cell><cell>T9U</cell><cell>FDUT9BF1</cell><cell>13.6</cell><cell>0.276</cell><cell>0.245</cell><cell>0.492</cell><cell cols="2">0.390 37(20)</cell><cell>11</cell><cell>15</cell></row><row><cell></cell><cell>T9P</cell><cell>FDUT9BF2</cell><cell>0.317</cell><cell>0.331</cell><cell>0.379</cell><cell>0.326</cell><cell cols="2">0.322 45(21)</cell><cell>7</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,81.60,367.82,432.21,92.73"><head>Table 2 .</head><label>2</label><figDesc>2 and 2.3 show the results of MeSH and MeSH sample topics. Our MeSH and sample runs don't perform as good as OHSU runs.</figDesc><table coords="10,147.60,397.85,300.30,62.70"><row><cell>Task</cell><cell>Measure</cell><cell>Run</cell><cell>Score</cell><cell cols="3">Comparison with median</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>&gt;(Best)</cell><cell>=</cell><cell>&lt;</cell></row><row><cell>Adaptive</cell><cell>T9P</cell><cell>FDUT9AF6</cell><cell>0.356</cell><cell>134(134)</cell><cell>148</cell><cell>218</cell></row><row><cell></cell><cell>T9U</cell><cell>FDUT9AF7</cell><cell>29.3</cell><cell>120(85)</cell><cell>72</cell><cell>308</cell></row><row><cell>Batch</cell><cell>T9P</cell><cell>FDUT9BF3</cell><cell>0.430</cell><cell>169(61)</cell><cell>151</cell><cell>180</cell></row><row><cell></cell><cell>T9P</cell><cell>FDUT9BF4</cell><cell>0.440</cell><cell>215(101)</cell><cell>138</cell><cell>147</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,168.60,463.25,258.60,57.90"><head>Table 2 .</head><label>2</label><figDesc>2 Adaptive and batch filtering for MeSH sample topics</figDesc><table coords="10,168.60,480.05,258.60,41.10"><row><cell>Task</cell><cell>Run</cell><cell>T9P</cell><cell cols="2">Comparison with median</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>&gt;(Best)</cell><cell>=</cell><cell>&lt;</cell></row><row><cell>Adaptive</cell><cell cols="2">FDUT9AF5 0.351</cell><cell>1297(1297)</cell><cell>1072</cell><cell>2535</cell></row><row><cell>Batch</cell><cell>FDUT9BF3</cell><cell>0.418</cell><cell>2297(2297)</cell><cell>0</cell><cell>2607</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,198.00,523.85,199.24,8.10"><head>Table 2 .</head><label>2</label><figDesc>3 Adaptive and batch filtering for MeSH topics</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,88.80,114.05,433.77,629.10"><head></head><label></label><figDesc>.1. Cooperating with the named entity finder, it does much help to locate and score answer in our QA system.</figDesc><table coords="11,100.80,114.05,421.77,629.10"><row><cell>Question</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Preprocessor</cell><cell cols="2">Parser</cell><cell>Question Analyzer</cell><cell>answer type</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>question focus</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>syntax pattern</cell></row><row><cell>Document</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>query</cell><cell></cell><cell>Answer(s)</cell></row><row><cell cols="2">Candidate Searcher Window</cell><cell>ranked candidate windows</cell><cell></cell><cell>Extractor Answer</cell></row><row><cell>Answer Type</cell><cell cols="2">Question Type</cell><cell></cell><cell>Example</cell></row><row><cell>PERSON</cell><cell cols="2">who/what-who/ which-who</cell><cell></cell><cell>Hugo Young</cell></row><row><cell>LOCATION</cell><cell cols="3">where/what-where/ which-where</cell><cell>China</cell></row><row><cell>ORG</cell><cell cols="2">who/what-who/ which-who</cell><cell></cell><cell>Phoenix Suns</cell></row><row><cell>MONEY</cell><cell cols="2">how much/ how many money</cell><cell></cell><cell>Pounds 12m</cell></row><row><cell>PERCENTAGE</cell><cell cols="2">how much/ what-percentage</cell><cell></cell><cell>0.10%</cell></row><row><cell>DATE</cell><cell cols="2">when/what-date/ which-date</cell><cell></cell><cell>10 Feb 1994</cell></row><row><cell>TIME</cell><cell>what time</cell><cell></cell><cell></cell><cell>6:33 a.m.</cell></row><row><cell>DURATION</cell><cell>how long</cell><cell></cell><cell></cell><cell>9 1/2-month</cell></row><row><cell>LENGTH</cell><cell>how long</cell><cell></cell><cell></cell><cell>147 feet</cell></row><row><cell>SIZE</cell><cell>how large</cell><cell></cell><cell></cell><cell>1.5 million acres</cell></row><row><cell>NUMBER</cell><cell>how many</cell><cell></cell><cell></cell><cell>562</cell></row><row><cell></cell><cell cols="3">Table 3.1: Answer types of question</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,81.60,309.62,432.82,236.13"><head></head><label></label><figDesc>2) Query_coverage-score: Assign a coefficient to each matched query term in the answer window and cumulate them. 3) Syntax_pattern-score: If certain sentence in answer-window satisfies any predictive syntax pattern of the question, a correspondent score will be assigned to it. 4) Consistent_question_part-score: If certain part of question is found consistent in the answerwindow, a score determined by the number of words in that part will be computed. It's a useful feature especially for back-formulation questions or coincident back-formulation situation. The final score for a given answer-window is computed as: final-score = k 1 * matched_queries-score + k 2 * query_coverage-score + k 3 * syntax_pattern-score + k 4 * consistent_question_part-score where, the weight vector (k 1 ,k 2 ,k 3 ,k 4 ) depends on the question feature, table 3.2 shows our empirical weight vector values:</figDesc><table coords="13,193.20,486.05,208.27,59.70"><row><cell>Question feature</cell><cell>(k 1 ,k 2 ,k 3 ,k 4 )</cell></row><row><cell>Num of query term=1</cell><cell>(8,8,16,4)</cell></row><row><cell>Num of low-freq word=0</cell><cell>(8,4,16,12)</cell></row><row><cell>Otherwise</cell><cell>(8,8,8,8)</cell></row><row><cell cols="2">Table3.2. Weight vector for final-score of answer-window</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was partly supported by <rs type="funder">NSF of China</rs> under contracts of <rs type="grantNumber">69873011</rs> and <rs type="grantNumber">69935010</rs>, and 863 <rs type="projectName">High Technology Project of China</rs> under contract of <rs type="grantNumber">863-306-ZD-02-02-4</rs>. We are thankful to <rs type="person">Yaqian Zhou</rs>, <rs type="person">Kaijiang Chen</rs>, <rs type="person">Li Lian</rs> and <rs type="person">Wei Qian</rs> for their help in the implementation of corpus and topic processing, syntactic parser and HMM based English named entity finder.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eDnaBQ9">
					<idno type="grant-number">69873011</idno>
				</org>
				<org type="funded-project" xml:id="_htsf5SA">
					<idno type="grant-number">69935010</idno>
					<orgName type="project" subtype="full">High Technology Project of China</orgName>
				</org>
				<org type="funding" xml:id="_bN2By4B">
					<idno type="grant-number">863-306-ZD-02-02-4</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,84.79,619.85,371.23,8.10;14,454.20,618.71,4.50,4.86;14,462.60,619.85,52.27,8.10;14,102.60,630.05,93.65,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,149.54,619.85,210.30,8.10">A maximum likelihood ratio information retrieval model</title>
		<author>
			<persName coords=""><forename type="first">Kenney</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,378.57,619.85,77.45,8.10;14,454.20,618.71,4.50,4.86;14,462.60,619.85,52.27,8.10;14,102.60,630.05,70.80,8.10">Proceedings of the 8 th Text Retrieval Conference TREC-8</title>
		<meeting>the 8 th Text Retrieval Conference TREC-8</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,84.79,640.25,322.06,8.10" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Wu</forename><surname>Li-De</surname></persName>
		</author>
		<title level="m" coord="14,162.96,640.25,131.94,8.10">Large Scale Chinese Text Processing</title>
		<imprint>
			<publisher>Fudan University Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,84.79,651.05,427.67,8.10;14,102.60,661.25,4.50,8.10;14,106.80,660.11,3.90,4.86;14,113.40,661.25,285.04,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,216.92,651.05,221.75,8.10">Automatic Retrieval With Locality Information Using SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,446.35,651.05,66.11,8.10;14,102.60,661.25,4.50,8.10;14,106.80,660.11,3.90,4.86;14,113.40,661.25,132.26,8.10">Proceedings of the 1 st Text REtrieval Conference (TREC-1)</title>
		<meeting>the 1 st Text REtrieval Conference (TREC-1)</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="500" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,84.79,671.45,429.61,8.10;14,102.60,681.65,300.03,8.10" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<idno>B4-31</idno>
		<title level="m" coord="14,178.80,671.45,197.08,8.10">Machine Learning in Automated Text Categorization</title>
		<meeting><address><addrLine>Consiglio Nazionale delle Ricerche, Pisa, IT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Istituto di Elaborazione dell&apos;Informazione</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="14,84.79,692.45,428.25,8.10;14,102.60,702.65,116.47,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,248.45,692.45,156.68,8.10">LASSO: A Tool for Surfing the Answer Net</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,420.72,692.45,92.32,8.10;14,102.60,702.65,92.45,8.10">Proceedings of the Eighth Text Retrieval Conference</title>
		<meeting>the Eighth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,84.79,712.85,427.68,8.10;14,102.60,723.05,412.81,8.10;14,102.60,733.25,109.89,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,314.40,712.85,194.15,8.10">Nymble: a High-Performance Learning Name-finder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,102.60,723.05,291.85,8.10">Proceedings of the Fifth Conference on Applied Natural Language Processing</title>
		<meeting>the Fifth Conference on Applied Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
