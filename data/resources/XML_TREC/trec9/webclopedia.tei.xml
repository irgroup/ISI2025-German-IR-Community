<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,197.52,81.20,217.01,12.53">Question Answering in Webclopedia</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.32,114.53,55.25,9.72"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.62,114.53,58.90,9.72"><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
							<email>gerber@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.89,114.53,63.26,9.72"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.98,114.53,56.58,9.72"><forename type="first">Michael</forename><surname>Junk</surname></persName>
							<email>junk@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.88,114.53,62.70,9.72"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,197.52,81.20,217.01,12.53">Question Answering in Webclopedia</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">463E7E8D0D67F79ED714748467214147</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction: Question Answering</head><p>IR techniques have proven quite successful at locating within large collections of documents those relevant to a user's query. Often, however, the user wants not whole documents but brief answers to specific questions: How old is the President? Who was the second person on the moon? When was the storming of the Bastille? Recently, a number of research projects have investigated the computational techniques needed for effective performance at this level of granularity, focusing just on questions that can be answered in a few words taken as a passage directly from a single text (leaving aside, for the moment, the answering of longer, more complex answers, such as stories about events, descriptions of objects, compare&amp;contrast discussions, arguments of opinion, etc.).</p><p>The systems being built in these projects exhibit a fairly standard structure: all create a query from the user's question, perform IR with the query to locate (segments of) documents likely to contain an answer, and then pinpoint the most likely answer passage within the candidate documents. The most common difference of approach lies in the pinpointing. A 'pure IR' approach would segment each document in the collection into a series of mini-documents, retrieve the segments that best match the query, and return them as answer. The challenge here would be to make segments so small as to be just answer-sized but still large enough to be indexable. A 'pure NLP' approach would be to match the parse and/or semantic interpretation of the question against the parse and/or semantic interpretation of each sentence in the candidate answer-containing documents, and return the best match(es). The challenge here would be to perform parsing, interpretation, and matching fast enough to be practical, given the large volumes of text to be handled.</p><p>Answering short questions thus becomes a problem of finding the best combination of word-level (IR) and syntactic/semantic-level (NLP) techniques, the former to produce as short a set of likely candidate segments as possible and the latter to pinpoint the answer(s) as accurately as possible.</p><p>Because language allows paraphrasing and inference, however, working out the details is not entirely straightforward. In this paper we describe the Webclopedia, a system that uses a classification of QA types to facilitate coverage, uses a robust syntactic-semantic parser to perform the analysis, and contains a matcher that combines word-and parse-tree-level information to identify answer passages. Section 2 outlines the Webclopedia approach and architecture; Section 3 describes document retrieval and processing, Section 4 describes the QA Typology, Section 5 the parsing, and Section 6 the matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Webclopedia</head><p>Webclopedia's architecture, shown in Figure <ref type="figure" coords="2,286.77,99.65,4.05,9.72" target="#fig_0">1</ref>, follows the pattern outlined in Section 1:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parsing of question:</head><p>The CONTEX parser (see Section 5) is used to parse and analyze the question, assisted by BBN's IdentiFinder <ref type="bibr" coords="2,268.99,131.09,79.20,9.72" target="#b1">(Bikel et al., 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question analysis:</head><p>To form a query, single-and multi-word units (content words) are extracted from the parsed query. WordNet synsets are used for query expansion. See Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IR:</head><p>The IR engine MG <ref type="bibr" coords="2,192.29,181.01,79.55,9.72" target="#b11">(Witten et al. 1994</ref>) is used to return and rank the top 1000 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation:</head><p>To decrease the amount of text to be processed, the documents are broken into semantically coherent segments. Two text segmenters were tried-TexTiling <ref type="bibr" coords="2,443.84,212.45,35.15,9.72">(Hearst,</ref><ref type="bibr" coords="2,482.82,212.45,13.04,9.72">94)</ref>, C99 <ref type="bibr" coords="2,90.00,225.17,27.30,9.72">(Choi,</ref><ref type="bibr" coords="2,120.01,225.17,13.05,9.72">00)</ref>; the first is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking of segments:</head><p>For each segment, each sentence is scored using a formula that rewards word and phrase overlap with the question and expanded query words. The segments are ranked.</p><p>Parsing of segments: CONTEX also parses each sentence of the top-ranked 100 segments.</p><p>Pinpointing: For each sentence, three steps of matching are performed (see Section 6); two compare the parses of the question and the sentence; the third moves a fixed-length window over each sentence and computes a goodness score based on the words and phrases contained in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking of answers:</head><p>The candidate answers' scores are compared and the winning answer(s) are output.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Information Retrieval and Document Ranking</head><p>Analyzing the Question to Create a Query</p><p>We parse input questions using CONTEX (Section 5) to obtain a semantic representation of the questions. For example, we determine that the question "Who is Johnny Mathis' high school track coach?" is asking for the name of person. The question analysis module identifies noun phrases, nouns, verb phrases, verbs, adjective phrases, and adjectives embedded in the question. These phrases/words are assigned significance scores according to the frequency of their type in our question corpus (a collection of 27,000+ questions and answers), secondarily by their length, and finally by their significance scores, derived from word frequencies in the question corpus.</p><p>We remain indebted to BBN for the use of IdentiFinder <ref type="bibr" coords="3,338.95,223.49,81.36,9.72" target="#b1">(Bikel et al., 1999)</ref>, which isolates proper names in a text and classifies them as person, organization, or location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expanding Queries</head><p>In order to boost recall we use WordNet 1.6 <ref type="bibr" coords="3,288.16,283.73,73.30,9.72" target="#b3">(Fellbaum 1998)</ref> to expand query terms and place all the expanded terms into a Boolean expression. For example, "high school" is expanded to:</p><formula xml:id="formula_0" coords="3,128.70,314.93,305.91,9.72">"(high&amp;school)|(senior&amp;high&amp;school)|(senior&amp;high)|high|highschool"</formula><p>It is obvious that such brute force expansion has undesirable effects. The expanded "high school" query contains "high". This will make "high school" relatively less significant, since "high" is a very common word. We did not try to fix this problem in this year's TREC evaluation, but are planning to improve the expansion procedure next year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieving Documents</head><p>We use MG <ref type="bibr" coords="3,149.26,419.33,89.94,9.72" target="#b11">(Witten et al. 1994)</ref> as our search engine. Although MG is capable of performing ranked query, we only use its Boolean query capability. For the entire TREC9 test corpus, the size of the inverse index file is about 200 MB and the size of the compressed text database is about 884 MB. The stemming option is turned on. Queries are sent to the MG database, and the retrieved documents are ranked according to their ranking from query analysis. For example:</p><formula xml:id="formula_1" coords="3,126.00,488.93,381.05,9.72">Johnny&amp;mathis&amp;((high&amp;school)|(senior&amp;high&amp;school)|(senior&amp;high)|high|highschool)</formula><p>will be sent to the database first. If the number of documents returned is less than a pre-specified threshold then we retain this set of documents as the basis for further processing. The threshold is set to 5,000 in our TREC9 evaluation. If nothing is returned then we relax the query by taking the next query term in our query rank list. In this case, it is "high school track coach". If more than 5,000 documents are returned we drop the query expansion and use the original query terms instead. For this example, the query will be "Johnny&amp;mathis&amp;high&amp;school&amp;track&amp;coach".</p><p>In some cases, it is impossible to get the number of returned documents down to 5,000. For example, the question "What is the meaning of life?" will return an enormous amount of documents since all the words in the query are very common. We plan to address this problem by adding proximity and order constraints to the query process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking Documents</head><p>If the total numbers of documents returned by MG is N, we would like to rank the documents to maximize answer recall and precision in the topmost K &lt;&lt; N, in order to minimize the parsing and subsequent processing. In this phase we set K=1,000. Our document ranker uses the following scoring method:</p><p>• Each question word gets a score of 2</p><p>• Each synonym gets a score of 1</p><p>• Other words get a score of 0</p><p>Normally common words are ignored unless they are part of a phrase in question word order, in which case they get a score of 2 along with other words in the phrase. Based on these scores, the total score for a document is:</p><p>Document score = sum of word scores / number of different words</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmenting Documents</head><p>Splitting each document into topical segments to be input to the matcher is based on the assumption that important contextual information for pinpointing answers tends to occur within a local context. This is mostly true for the setup of TREC9 Q&amp;A. Furthermore, CONTEX does not use information outside sentence boundaries. This step helps the system focus on smaller regions of text where answers are most likely to be found.</p><p>We tried two text segmenters, TextTiling <ref type="bibr" coords="4,278.86,305.09,56.31,9.72" target="#b5">(Hearst 1994</ref>) and C99 <ref type="bibr" coords="4,385.13,305.09,52.14,9.72" target="#b2">(Choi 2000)</ref>. They perform at almost the same level, though TextTiling is faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking Segments</head><p>The resulting segments are re-ranked using the same ranker described earlier. This time, only the topmost 100 segments are passed to the parser (and then to the matcher for answer pinpointing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval Results</head><p>We evaluated our IR front end in 6 separate experiments using the 238 training questions obtained from NIST. The resulting answer distributions within the top 1,000 segments are shown in Table <ref type="table" coords="4,128.40,445.48,4.05,9.46">1</ref>.</p><p>N &lt; = 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 5 0 0 1 0 0 0 Overall 1 3 8 1 6 5 2 0 2 2 1 6 2 2 6 2 3 5 2 3 9 2 4 1 2 4 2 2 4 6 2 4 7 2 8 9 2 9 1 % 44% 52% 64% 69% 72% 75% 76% 77% 77% 78% 78% 92% 92%</p><p>Table <ref type="table" coords="4,133.13,673.72,4.05,9.46">1</ref>. Percentage of topmost N segments containing an answer after retrieval and ranking.</p><p>It is interesting to see that the system gets about 52% of answer segments within the top 10 and reaches only 78% within the top 100. And even in the top 1000 segments, 8% of the answers are missing. This indicates that further improvement of the IR front end is critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The QA Typology</head><p>There are many ways to ask the same thing. Likewise, there are many ways of delivering the same answer. Such variations form a sort of semantic equivalence class of both questions and answers; speaking approximately, any form of the question can be answered by any form of the answer. Since the user may employ any version of his or her question, and the source documents may contain any version(s) of the answer, an efficient system should group together equivalent question types and answer types. Any specific question can then be indexed into its type, from which all equivalent forms of the answer can be ascertained. These QA equivalence types can help with both query expansion (for IR) and answer pinpointing (for NLP).</p><p>However, the equivalence is fuzzy; even slight variations introduce exceptions: who invented the gas laser? can be answered by both Ali Javan and a scientist at MIT, while what is the name of the person who invented the gas laser? requires the former only. This inexactness suggests that the QA types be organized in an inheritance hierarchy, allowing the answer requirements satisfying more general questions to be overridden by more specific ones 'lower down'.</p><p>Previous work in automated question answering has often categorized questions by question word alone or by a mixture of question word and the semantic class of the answer <ref type="bibr" coords="5,451.33,352.13,71.01,9.72;5,90.00,364.85,25.28,9.72" target="#b10">(Srihari and Li, 2000;</ref><ref type="bibr" coords="5,119.99,364.85,105.35,9.72" target="#b9">Moldovan et al., 2000)</ref>. To ensure full coverage of all forms of simple question and answer, we have been developing a QA Typology as a taxonomy of QA types, becoming increasingly specific as one moves from root downward. Instead of focusing on question word or semantic type of the answer, our classes attempt to represent the user's intention, including for example the classes Why-Famous (for Who was Christopher Columbus? but not Who discovered America?, which is a Proper-Person QA type) and Abbreviation-Expansion (for What does NAACL stand for?). To create the QA Typology, we analyzed 17,384 questions and their answers (downloaded from answers.com); see <ref type="bibr" coords="6,174.04,87.17,63.55,9.72" target="#b4">(Gerber, 2001)</ref>. The Typology contains 94 nodes, of which 47 are leaf nodes; a section of it appears in Figure <ref type="figure" coords="6,228.89,99.89,4.05,9.72" target="#fig_1">2</ref>.</p><p>Each Typology node has been annotated with examples and typical patterns of expression of both Question and Answer, as indicated in Figure <ref type="figure" coords="6,285.00,131.33,5.40,9.72" target="#fig_2">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Parsing</head><p>Some answers returned by a youthful Webclopedia showed the need to ensure that the answer found is of the right kind semantically:</p><p>Q: Where are zebras most likely found? -A: in the dictionary Q: Where do lobsters like to live? -A: on the table / at the same rate as regular lobsters and in the right range numerically: Q: How many people live in Chile? -A: nine</p><p>We use CONTEX, a parser that is trained on a corpus to return both syntactic and semantic information, to help.</p><p>CONTEX is a deterministic machine-learning based grammar learner/parser that was originally built for MT <ref type="bibr" coords="6,153.59,595.01,86.35,9.72" target="#b6">(Hermjakob, 1997;</ref><ref type="bibr" coords="6,244.56,595.01,143.22,9.72">Hermjakob and Mooney, 1997)</ref>, where a smaller version of CONTEX (lexically restricted English) reached a labeled precision rate of 89.8% when trained on 256 sentences. Over the past few years it has been extended over the past years to handle deployment on new languages, including Japanese and Korean <ref type="bibr" coords="6,372.38,632.93,82.44,9.72" target="#b8">(Hermjakob, 2000)</ref>. The Japanese version of the parser, trained on 4096 sentences and tested on lexically unrestricted sentences, achieves 91.4% labeled precision and 91.1% labeled recall for parse trees with a word level granularity, and a bunsetsu level dependency accuracy rate of 84.5%. For English, CONTEX parses of unseen sentences measured 87.6% labeled precision and 88.4% labeled recall, after being trained on 2048 sentences from the Penn Treebank in March 2000. The robustness and the fact that the parser produces a complete parse tree for every test sentence, makes it very useful for Webclopedia.</p><p>CONTEX works as follows. As with statistical systems, the grammar learning system also induces its rules from training data; however, it makes better use of linguistic knowledge and other knowledge resources. When presented with a set of parse trees, the learning system automatically derives the sequence of Shift-Reduce parsing operations required to produce each tree. To determine which specific action to take at any point, it considers features of the left and right contexts of the current word. These features include words, parts of speech, lexical and semantic features, etc. In cases of ambiguity, it asks the trainer to identify which feature(s) to pay attention to. Viewing ambiguity as a decision making problem, the system builds a variant of a decision tree to handle the ambiguity in future, using the feature(s) within the context as well as background knowledge in the form of lexicons, ontologies, and any results from topic detection, etc. The appropriate features are indicated manually, by the trainer, if he or she decides they are needed. The decision structure however differs from a traditional grammar in two ways: (1) it is more, in the sense that it does not only provide a space of possible analyses, but in fact selects what it believes is the best analysis, and ( <ref type="formula" coords="7,278.47,271.01,4.25,9.72">2</ref>) it has a very operational character in that it directly drives the shift-reduce parser. The grammar as represented by the decision structure therefore has a somewhat different character from the traditional static grammar resource.</p><p>Manual guidance allows CONTEX to require far fewer treebanked sentences for training. CONTEX derives much of its strength from the integration of different types of background knowledge, even if those knowledge resources are incomplete. In this way it is a good example of the hybridization of statistical and symbolic techniques. Machine learning algorithms automatically select the most relevant features that best support specific run time parse decisions. This approach employs human and machine each to best advantage: linguists are good at parsing individual sentences, but less good at keeping all the complexity and generalization of a full grammar under control, while machines are excellent at managing and generalizing large sets of individual data points. The result is a rapid traversal of the learning space toward a robust, widecoverage grammar and parser.</p><p>Webclopedia required four extensions to CONTEX.</p><p>First, the grammar had to be extended to handle questions. This was achieved by adding approx. 250 manually parsed questions to the Penn Treebank, on which the system's English grammar has been trained. Of these questions, 100 were obtained from NIST's TREC-8 QA corpus and 150 from elsewhere.</p><p>Second, the semantic type ontology in CONTEX was extended, both to include QA types and to include many more Objects from our ontology SENSUS. It now contains about 10,000 nodes.</p><p>Third, the results of BBN's IndentiFinder locating proper names had to be taken into account.</p><p>Fourth, the parse tree output had to be augmented to carry question-related information. The semantic type of the desired answer, as determined by CONTEX, we call the Qtarget. CONTEX returns a ranked list of Qtargets, in order of specificity, drawn from its ontology. For example, the expression (((c-date) (c-temp-loc-with-year)) ((eq c-temp-loc)))</p><p>indicates that the system should try to match a specific date or specific year (both first choice) over a more general temporal expression like "after the war".)</p><p>Beside the Qtargets that refer to concepts in CONTEX's concept ontology (see first example in Figure <ref type="figure" coords="7,121.97,693.41,3.94,9.72">4</ref>), Qtargets can also refer to part of speech labels (see first example), to constituent roles or slots of parse trees (see second and third examples), and to more abstract nodes in the QA Typology (see later examples). For questions with Qtargets Q-WHY-FAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and others, the parse tree also provides a slot Qargs that contains additional information helpful for matching (see final examples).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,228.48,682.85,155.12,9.72"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Webclopedia architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,200.64,696.77,210.59,9.72"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Portion of Webclopedia QA Typology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,152.40,383.81,306.90,9.72"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Portion of QA Typology node annotations for Proper-Person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,121.44,131.33,382.32,243.75"><head></head><label></label><figDesc>for Proper-Person. President Saparmurad Niyazov… &lt;entity&gt;'s &lt;role&gt; &lt;person&gt; ...in Tchaikovsky's Eugene Onegin... &lt;person&gt;'s &lt;entity&gt; Mr. Jack Welch, GE chairman... &lt;role-title&gt; &lt;person&gt; ... &lt;entity&gt; &lt;role&gt; ...Chairman John Welch said ...GE's &lt;subject&gt;|&lt;psv object&gt; of related role-verb</figDesc><table coords="6,121.44,168.70,348.17,165.82"><row><cell>Question examples</cell><cell>Question templates</cell></row><row><cell cols="2">Who was Johnny Mathis' high school track coach? who be &lt;entity&gt;'s &lt;role&gt;</cell></row><row><cell>Who was Lincoln's Secretary of State?</cell><cell></cell></row><row><cell>Who was President of Turkmenistan in 1994?</cell><cell>who be &lt;role&gt; of &lt;entity&gt;</cell></row><row><cell>Who is the composer of Eugene Onegin?</cell><cell></cell></row><row><cell>Who is the CEO of General Electric?</cell><cell></cell></row><row><cell>Actual answers</cell><cell>Answer templates</cell></row><row><cell cols="2">Lou Vasquez, track coach of…and Johnny Mathis &lt;person&gt;, &lt;role&gt; of &lt;entity&gt;</cell></row><row><cell>Signed Saparmurad Turkmenbachy [Niyazov],</cell><cell>&lt;person&gt; &lt;role-title*&gt; of &lt;entity&gt;</cell></row><row><cell>president of Turkmenistan</cell><cell></cell></row><row><cell>...Turkmenistan's</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic ontology types (I-EN-CITY) and part of speech labels (S-PROPER-NAME):</p><p>What is the capital of Uganda? QTARGET: (((I-EN-CITY S-PROPER-NAME)) ((EQ I-EN-PROPER-PLACE))) Parse tree roles:</p><p>Why can't ostriches fly? QTARGET: (((ROLE REASON))) Name a film in which Jude Law acted.</p><p>QTARGET: (((SLOT TITLE-P TRUE))) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qargs for additional information:</head><p>Who was Betsy Ross? QTARGET: (((Q-WHY-FAMOUS-PERSON))) QARGS: (("Betsy Ross")) How is "Pacific Bell" abbreviated? QTARGET: (((Q-ABBREVIATION))) QARGS: (("Pacific Bell")) What are geckos? QTARGET: (((Q-DEFINITION))) QARGS: (("geckos" "gecko") ("animal"))</p><p>Figure <ref type="figure" coords="8,173.70,409.25,4.05,9.72">4</ref>. QA-related information, returned in the parse tree of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Answer Matching</head><p>Given the instantiated QA patterns, the Qtargets and Qargs lists, and the potential answer-bearing text segments (also parsed by CONTEX), the Matcher module performs three attempts to pinpoint the answer:</p><p>• match QA patterns,</p><p>• match Qtargets and Qargs,</p><p>• (if all else fails) move a word-level window across the (unparsed) text, scoring each position.</p><p>The window scoring function is as follows:</p><p>Factors:</p><p>• w: window width (modulated by gaps of various lengths: "white house" ≠"white car and house") • r: rank of Qtarget in list returned by CONTEX • I: window word information content (inverse log freq) • q: # different question words, and specific rewards (bonus q=3.0) • e: penalty for question word expansion using WordNet synsets (e=0.8) • b: boosting for main verb match, target words, proper names, etc. (b=2.0) • u: (value 0 or 1) indicates whether a word has been "subsumed" by the Qtarget model and should not contribute (again) to the score. For example, "In what year did Columbus discover America?" the subsumed-words are {what, year}.</p><p>Unless required, we will not try to a more sophisticated scoring function, preferring to focus on the modules that employ information 'deeper' than the word level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments and Results</head><p>We entered the TREC-9 short form QA track, and received an overall Mean Reciprocal Rank score of 0.318, which put Webclopedia in essentially tied second place with two others. (The winning system far outperformed all the others.)</p><p>A sample analysis of the relative performance of the three modules appears in Table <ref type="table" coords="9,465.97,181.01,4.07,9.72">2</ref>. It is clear that the QA patterns made only a small contribution, and that the Qtarget made by far the largest contribution. Interestingly, the word-level window match lay somewhere in between. We are pleased with the performance of Qtargets. They indicate the value of trying to locate the desired semantic type from the meaning of the question. Together with the parse structure, they also help with pinpointing the answer closely: our average answer window length was approx. 25 bytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Date</head><p>We are not however satisfied with the manually built QA patterns. First, it is too difficult and takes too long to build them by hand (the 500 we have were assembled by simply combining approx. 25 question patterns with 25 answer patterns). Second, the patterns are not robust in the face of small variations of phrasing. We aim instead to build the QA patterns automatically.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,95.22,440.24,77.72,12.53" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,464.93,432.27,9.72;9,104.40,477.65,258.66,9.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,320.45,464.93,196.08,9.72">An Algorithm that Learns What&apos;s in a Name</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,104.40,477.65,216.15,9.72">Machine Learning-Special Issue on NL Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,496.37,427.60,9.72;9,517.69,494.76,4.53,5.83;9,104.40,509.09,417.68,9.72;9,104.40,521.81,125.71,9.72" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,180.26,496.37,222.55,9.72">Advances in independent linear text segmentation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Y Y</forename><surname>Choi</surname></persName>
		</author>
		<idno>ANLP-NAACL-00</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,424.22,496.37,93.38,9.72;9,517.69,494.76,4.53,5.83;9,104.40,509.09,187.01,9.72">Proceedings of the 1 st Meeting of the North American Chapter</title>
		<meeting>the 1 st Meeting of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,540.53,404.46,9.72" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,204.00,540.53,181.19,9.72">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">Ch</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,559.25,261.82,9.72" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,167.08,559.25,143.49,9.72">A QA Typology for Webclopedia</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gerber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>In prep</note>
</biblStruct>

<biblStruct coords="9,90.00,577.97,432.28,9.72;9,104.40,590.69,298.87,9.72" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,179.13,577.97,214.01,9.72">Multi-paragraph segmentation of expository text</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,403.68,577.97,118.60,9.72;9,104.40,590.69,293.77,9.72">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-94)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL-94)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,609.41,432.18,9.72;9,104.40,622.13,417.67,9.72;9,104.40,634.85,210.87,9.72" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,194.22,609.41,327.96,9.72;9,104.40,622.13,31.76,9.72">Learning Parse and Translation Decisions from Examples with Rich Context</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<ptr target="file://ftp.cs.utexas.edu/pub/mooney/papers/hermjakob-dissertation-97.ps.gz" />
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct coords="9,90.00,653.57,432.05,9.72;9,104.40,666.05,417.67,9.72;9,104.40,678.77,417.52,9.72;9,104.40,691.49,58.48,9.72" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,265.79,653.57,256.26,9.72;9,104.40,666.05,88.70,9.72">Learning Parse and Translation Decisions from Examples with Rich Context</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<ptr target="file://ftp.cs.utexas.edu/pub/mooney/papers/contex-acl-97.ps.gz" />
	</analytic>
	<monogr>
		<title level="m" coord="9,220.29,666.05,301.78,9.72;9,104.40,678.77,146.77,9.72">35th Proceedings of the Conference of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="482" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,74.45,432.08,9.72;10,104.40,87.17,54.77,9.72;10,174.91,87.17,347.26,9.72;10,104.40,99.89,291.71,9.72" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,190.73,74.45,313.91,9.72">Rapid Parser Development: A Machine Learning Approach for Korean</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/~ulf/papers/kor_naacl00.ps.gz" />
	</analytic>
	<monogr>
		<title level="m" coord="10,104.40,87.17,54.77,9.72;10,174.91,87.17,347.26,9.72;10,104.40,99.89,66.72,9.72">Proceedings the North American chapter of the Association for Computational Linguistics (NAACL-2000)</title>
		<meeting>the North American chapter of the Association for Computational Linguistics (NAACL-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,118.61,432.05,9.72;10,104.40,131.33,417.67,9.72;10,104.40,144.05,417.69,9.72;10,104.40,156.53,40.50,9.72" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,104.40,131.33,396.90,9.72">The Structure and Performance of an Open-Domain Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,104.40,144.05,412.48,9.72">Proceedings of the Conference of the Association for Computational Linguistics (ACL-2000)</title>
		<meeting>the Conference of the Association for Computational Linguistics (ACL-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="563" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,175.25,432.25,9.72;10,104.40,187.97,111.66,9.72;10,216.26,186.36,4.56,5.83;10,225.81,187.97,296.23,9.72;10,104.40,200.69,254.15,9.72" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,217.36,175.25,300.52,9.72">A Question Answering System Supported by Information Extraction</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno>ANLP-NAACL-00</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,117.38,187.97,98.68,9.72;10,216.26,186.36,4.56,5.83;10,225.81,187.97,189.09,9.72">Proceedings of the 1 st Meeting of the North American Chapter</title>
		<meeting>the 1 st Meeting of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,219.41,432.26,9.72;10,104.40,232.13,260.08,9.72" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,300.81,219.41,221.45,9.72;10,104.40,232.13,96.34,9.72">Managing Gigabytes: Compressing and indexing documents and images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Van Nostrand Reinhold</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
