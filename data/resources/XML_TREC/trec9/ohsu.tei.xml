<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,86.25,75.49,439.54,14.17;1,150.00,94.24,312.93,14.17">Further Analysis of Whether Batch and User Evaluations Give the Same Results With a Question-Answering Task</title>
				<funder ref="#_mhdrjtU">
					<orgName type="full">U.S. National Library of Medicine</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.00,123.57,62.24,10.13"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Medical Informatics &amp; Outcomes Research</orgName>
								<orgName type="institution">Oregon Health Sciences University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.79,123.57,65.53,10.13"><forename type="first">Andrew</forename><surname>Turpin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Medical Informatics &amp; Outcomes Research</orgName>
								<orgName type="institution">Oregon Health Sciences University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.70,123.57,73.52,10.13"><forename type="first">Lynetta</forename><surname>Sacherek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Medical Informatics &amp; Outcomes Research</orgName>
								<orgName type="institution">Oregon Health Sciences University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.72,123.57,57.75,10.13"><forename type="first">Daniel</forename><surname>Olson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Medical Informatics &amp; Outcomes Research</orgName>
								<orgName type="institution">Oregon Health Sciences University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.50,136.32,50.32,10.13"><forename type="first">Susan</forename><surname>Price</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Medical Informatics &amp; Outcomes Research</orgName>
								<orgName type="institution">Oregon Health Sciences University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.91,136.32,66.57,10.13"><forename type="first">Benjamin</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Medical Informatics &amp; Outcomes Research</orgName>
								<orgName type="institution">Oregon Health Sciences University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.56,136.32,62.32,10.13"><forename type="first">Dale</forename><surname>Kraemer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Medical Informatics &amp; Outcomes Research</orgName>
								<orgName type="institution">Oregon Health Sciences University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,86.25,75.49,439.54,14.17;1,150.00,94.24,312.93,14.17">Further Analysis of Whether Batch and User Evaluations Give the Same Results With a Question-Answering Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">67DB64811CD5D6069AAEBC0D8BBFCE78</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the TREC-8 Interactive Track, our results indicated that the better performance obtained in batch searching evaluation do not translate into better performance by users in an instance recall task. This year we pursued this investigation further by performing the same experiments using the new questionanswering task adopted in the TREC-9 Interactive Track. Our results once again show that better performance in batch searching evaluation does not translate into gains for real users.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A continuing unanswered question in information retrieval (IR) research is whether batch and user searching evaluations give the same results. We explored this question in the TREC-8 Interactive Track, where we found that the better results obtained in batch studies using the Okapi weighting scheme over the standard TFIDF approach did not accrue to real users for an instance recall task. <ref type="bibr" coords="1,444.55,313.32,12.57,10.13" target="#b0">[1]</ref> This work was limited by the small number of queries as well as the use of a single retrieval task, the recall of specific instances for a topic. Since the TREC-9 Interactive Track would be using a different task -questionanswering -we decided to use the same research question again with this changed task. Although we would still have a small number of queries, it would provide another IR task to assess this research question.</p><p>As with the TREC-8 Interactive Track we performed three experiments. The first experiment was to identify an IR approach that achieved the best possible performance in the batch environment. In the second experiment, we used that best weighting measure as the "experimental" system to be compared with the "control" system using baseline TFIDF weighting. In the final experiment, we verified that the better batch performance of the experimental system held up with the new TREC-9 Interactive Track data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1 -Identifying the "best" weighting scheme</head><p>In TREC-8, the best weighting scheme was chosen by turning Interactive Track data from TREC-6 and TREC-7, which also used an instance recall task, into a test collection. All documents which had one or more instances were deemed relevant, and many runs using variants of TFIDF, Okapi, and pivoted normalization were used. The collection was that used by the instance recall task, the Financial Times 1991-1994 (FT91-94) from Disk 4 of the TREC CD-ROMs. The queries used were derived from the Description field of the topic. The Okapi weighting gave the best mean average precision (MAP), which was 83% over the standard TFIDF baseline.</p><p>All of our batch and user experiments used the MG retrieval system. <ref type="bibr" coords="1,376.50,617.07,12.37,10.13" target="#b1">[2]</ref> MG allows queries to be entered in either Boolean or ranked mode. If ranking is chosen, the ranking scheme can be varied according to the Q-expression notation introduced by Zobel and Moffat. <ref type="bibr" coords="1,333.00,642.57,12.37,10.13" target="#b2">[3]</ref> A Q-expression consists of eight letters written in three groups, each group separated by hyphens. For example, BB-ACB-BCA, is a valid Qexpression. The two triples describe how terms should contribute to the weight of a document and the weight of a query respectively. The first two letters of each triple define how a single term contributes to the document/query weight. The final letter of each triple describes the document/query length normalization scheme. The second character of the Q-expression details how term frequency should be treated in both the document and query weight, e.g., as inverse document/query frequencies. Finally, the first character determines how the four quantities (document term weight, query term weight, document normalization, and query normalization) are combined to give a similarity measure between any given document and query. To determine the exact meaning of each character, the five tables appearing in the Zobel and Moffat paper must be consulted. <ref type="bibr" coords="2,264.75,123.57,12.37,10.13" target="#b2">[3]</ref> Each character provides an index into the appropriate table for the character in that position.</p><p>Although the Q-expressions permit thousands of possible permutations to be expressed, several generalizations can be made. Q-expressions starting with a B use the cosine measure for combining weights, while those starting with an A do not divide the similarity measure by document or query normalization factors. A B in the second position indicates that the natural logarithm of one plus the number of documents divided by term frequency is used as a term's weight, while a D in this position indicates that the natural logarithm of one plus the maximum term frequency divided by term frequency is used. A C in the fourth position indicates a cosine-measure-based term frequency treatment, while an F in this position indicates Okapi-style usage. <ref type="bibr" coords="2,262.50,250.32,12.37,10.13" target="#b3">[4]</ref> Varying the fifth character alters the document length normalization scheme. Letters greater than H use pivoted normalization. <ref type="bibr" coords="2,393.75,263.07,12.37,10.13" target="#b4">[5]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>For the question-answering task of the TREC-9 Interactive Track, we had no prior Interactive Track data to use. Instead, we used almost all queries from the ad hoc collection dating back to TREC-2 (051-450) along with 20 prior instance recall queries (from the past three years of the Interactive Track) and the 200 queries from the TREC-8 question-answering track. For the latter, we deemed any document which had an answer string as relevant. Mean average precision was calculated using the trec_eval program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>While the version of Okapi used in TREC-8 (AB-BFD-BAA) did better on instance recall queries from past Interactive Track experiments (using the FT91-94 collection), it did not perform as well on the other query-collection sets. The weighting scheme giving the best results over all of the query sets was a version of Okapi that employed pivoted normalization (AE-BFM-ABA) as shown in Table <ref type="table" coords="2,469.64,452.82,4.14,10.13" target="#tab_0">1</ref>.</p><p>The new best Okapi weighting calculates the similarity between a document and query as </p><formula xml:id="formula_0" coords="2,192.75,491.87,224.18,60.34">∑ ∈         + ×         - × d q T t d d t d t d t t t q W av W f f f f N f , )<label>( ln ,</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slope</head><p>Mean Average Precision 0.550 0.0740 0.575 0.0781 0.600 0.0782 0.650 0.0780 0.675 0.0776 and the sum is over all terms that occur both in the query and document.</p><p>As this new Okapi approach uses pivoted normalization, we needed to determine the best slope. As shown in Table <ref type="table" coords="3,142.48,469.32,4.12,10.13" target="#tab_1">2</ref>, a slope of 0.6 was determined to be best.</p><p>The baseline TFIDF Q-expression was the same as for TREC-8 (BB-ACB-BAA), which calculates similarity between a document and the query as </p><formula xml:id="formula_1" coords="3,239.25,521.13,131.36,64.35">( ) ( ) ∑ ∑ ∈ ∈ +         + × + d q T t doc t t d t t d f f N f , 2 , ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2 -Interactive retrieval experiments</head><p>Based on the results from Experiment 1, the goal of our interactive experiment was to assess whether the AE-BFM-ABA weighting scheme provided benefits to real users in the TREC interactive setting. The OHSU TREC-9 Interactive Track experiments were carried out according to the consensus protocol developed for the track. We used all of the instructions, worksheets, and questionnaires developed by consensus, augmented with some additional instruments, such as tests of cognitive abilities and a validated user interface questionnaire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>As noted above, the TREC-9 Interactive Track used a question-answering task. A set of eight questions was developed (see Table <ref type="table" coords="4,189.06,224.82,3.99,10.13" target="#tab_2">3</ref>). Questions were of two types. The first type required users to find a small number of instances for a topic, e.g., the number of parks in the United States containing redwood trees. The second type required users to select the correct answer from two given, e.g., which country had a larger population, Denmark or Norway. Searchers from all sites were asked to answer the questions by searching, recording the answer, and recording all documents that contributed to the answer. Assessors at NIST scored each answer as being completely correct, partially correct, or not correct, with the documents saved by the user being judged as completely answering the question, partially answering the question, or not answering the question. For our analysis, a question was deemed correct if the assessor found the answer completely correct and the answer was supported by all documents saved by the user.</p><p>The collection used for these experiments was the same as that used by the question-answering track, consisting of Disks 4 and 5 (minus the Federal Register) of the TREC CD-ROM collection.</p><p>Both the baseline and the Okapi plus pivoted normalization systems used the same Web-based, natural language interface shown in Figure <ref type="figure" coords="4,228.43,401.82,4.12,10.13" target="#fig_1">1</ref>. MG was run on a Sun Enterprise 250 with 1 gigabyte of RAM running the Solaris 2.7 operating system. The user interface accessed MG via CGI scripts which contained JavaScript code for designating the appropriate weighting scheme and logging search strategies, documents viewed (title displayed to user), and documents seen (all of document displayed by user). Searchers accessed each system with either a Windows 95 PC or an Apple PowerMac, running Internet Explorer or Netscape Navigator.  Subjects were recruited by advertising over several librarian-oriented listservs in the Pacific Northwest. The advertisement explicitly stated that we sought information professionals with a library degree and that they would be paid a modest honorarium for their participation. We also recruited graduate students from the Master of Science in Medical Informatics Program at Oregon Health Sciences University (OHSU). They had a variety of backgrounds, from being a physician or other health care professionals to having completed only undergraduate studies.</p><p>The experiments took place in a computer lab. Each session took two hours, broken into three parts, separated by short breaks: personal data and attributes collection, searching with one system, and searching with the other system. The entire process included the following steps: 1. Orientation to experiment (10 minutes) 2. Administration of Pre-Search Questionnaire (10 minutes) 3. Orientation to searching session and retrieval system (10 minutes) 4. Practice search (10 minutes) 5. Short Break (5 minutes) 6. Searching on first 4 topics with assigned system (30 minutes) 7. Short break (10 minutes) 8. Searching on second 4 topics with assigned system (30 minutes) 9. Administration of Exit Questionnaire (5 minutes) Each participant was assigned to search four questions in a block with one system followed by four questions with the other system. A pseudo-random approach was used to insure that all topic and system order effects were nullified. (A series of random orders of topics with subject by treatment blocks were generated (for balance) and used to assign topics.) Per the consensus protocol, each participant was allowed five minutes per question. Participants were instructed to write their answer on the searcher worksheet and save all documents that supported their answers (either by using the "save" function of the system or writing its document identifier down on the searcher worksheet). The results of several participants had to be discarded for failing to follow these instructions.</p><p>The exit questionnaire was augmented from the consensus protocol to include the Questionnaire for User Interface Satisfaction (QUIS) 5.0 instrument <ref type="bibr" coords="6,271.50,224.82,11.67,10.13" target="#b5">[6]</ref>. QUIS provides a score from 0 (poor) to 9 (excellent) on a variety of user factors, with the overall score determined by averaging responses to each item. QUIS was given only at the end as a measure of overall user interface satisfaction since the interfaces for the two systems were identical.</p><p>For statistical analysis, we fit a series of mixed-model analysis of variance models and covariance models to the data. Mixed models allow both fixed effects (system and questions) and random effects (subjects) to be fit in one model. Given the binary outcome (correct or not correct), we fit a logistic model using a generalized linear model approach. We fit the model using SAS® Version 8.0 MACRO GLIMMIX, which uses an iteratively reweighted likelihood approach to fit these models. <ref type="bibr" coords="6,408.00,339.57,12.37,10.13" target="#b6">[7]</ref> Our base model included systems (TFIDF and Okapi plus pivoted normalization) and questions. In addition to system and questions, since each subject answered all questions, we included subject in the model as a random intercept term. We also allowed a separate variance structure for each subject using the mixed model approach. In additional analyses we also added one of 11 covariates to the analysis of variance model (one covariate per analysis) to determine if the covariate made a significant contribution to the model with systems and questions. The covariates represented the factors measured in the various questionnaires and were each based on a Likert scale with values of one to five used as scale variables. The covariates and the variables they represent are listed in Table <ref type="table" coords="6,363.84,453.57,4.15,10.13" target="#tab_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>A total of 25 individuals followed instructions well enough for their data to be included in the analysis. Although a "pure" statistical analysis would only include the 16 subjects who have been balanced for query and system order, we have included the results from all 25 searchers in this initial analysis. The make-up of the participants was 18 librarians and seven others who were graduate students or research assistants. The average age of the librarians was 38.6 years. All but three were female. The average age of the remaining subjects was 34.4 years, with four males and three females.</p><p>The Pre-Search Questionnaire showed this was a group with a great deal of searching experience. Most had been searching for over half of their adult life. Virtually all reported high experience with point-andclick user interfaces, on-line library card catalogs, on-line searching, and Web searching. All indicated they frequently conducted searches and enjoyed doing it. Because of the heterogeneity of this data, no further analysis of this per-user data was performed. We instead focused our analysis on attributes measured on a per-question basis as described below.</p><p>The rate of correctness varied widely across the questions. Table <ref type="table" coords="7,365.53,275.82,5.63,10.13" target="#tab_4">5</ref> shows the results for each question based on the correctness criteria defined above, with results shown for all participating groups and OHSU searchers only. For the statistical analysis, we deleted two of the eight questions (numbers 3 and 8) because all searchers gave the same answer. Including a question in the analysis for which all subjects have the same answer, either correct or incorrect, causes problems for the iterative statistical algorithm.</p><p>No subject answered either of the two deleted questions correctly. No question was answered correctly by all subjects. For OHSU searchers, the differences across questions was statistically significant using a Chi-square test (p &lt; .0001). The rate of correctness did not vary, however, across systems. As shown in Table <ref type="table" coords="7,99.23,377.07,4.08,10.13" target="#tab_5">6</ref>, it was virtually identical for the two retrieval systems. There was no statistically significant difference between systems. The results of the analyses of covariance are shown in Table <ref type="table" coords="8,344.30,284.07,4.17,10.13" target="#tab_6">7</ref>. None of the variables assessed were statistically significant by system and all were statistically significant by question. The latter, of course, represented the large variation in rate of correctness per question. There was a significant association with the following covariates: certainty, easy to do, satisfied, time adequate, seen, and saved. For satisfied and time adequate, the inclusion of the covariate resulted in a change in the p-value for questions. While this p-value was still significant at a 5% level, the p-values were much closer to the 5% than without the covariate. This suggests that the covariate was explaining some of the variation formerly explained by questions alone. There did not appear to a meaningful association between the other six covariates and the likelihood of being correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3 -Verifying "best" weighting scheme</head><p>The final experiment was to determine whether the question-answering data for the TREC-9 Interactive Track gave better results in batch searching evaluation. This would allow us to determine whether the user evaluation in Experiment 2 gave the same or different results than batch searching experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>For this experiment, we developed a test collection consisting of the collection used for Experiment 2, queries derived from the question statement, and relevant judgments derived by designating those determined to "support" the answer by NIST assessors. In their judgment of the results, the assessors selected the correct answers as well as listing which documents provided "supporting" evidence for those answers. We assumed these documents were relevant and used them in our batch experiments accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>As shown in Table <ref type="table" coords="9,157.56,212.82,4.12,10.13" target="#tab_7">8</ref>, the Okapi (AE-BFM-ABA) weighting provided improved MAP over TFIDF for all but on query and by an overall average of 31.5%. This was similar to our TREC-8 Interactive Track experiments, where batch results showed improved performance for the better weighting scheme that did not occur with user experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Our TREC-9 Interactive Track results paralleled our TREC-8 results, i.e., performance enhancement that occurred in batch evaluation studies was not associated with performance of real users. As with our TREC-8 Interactive Track study, this one had limitations as well. Like past experiments, the number of queries and users was small. Recent research suggests that evaluation measures are unstable when less than 25 or 50 queries are used in an evaluation, at least in the batch setting. <ref type="bibr" coords="9,404.25,351.57,12.37,10.13" target="#b7">[8]</ref> Nonetheless, there is some significance to the fact that comparable results have been obtained with two different retrieval tasks even with the small number of queries and users.</p><p>A number of factors were assessed to determine their effect on the rate of correctness, but the large variation in question correctness overwhelmed any differences in effects of the factors.</p><p>The next step in our research will be an investigation to determine why gains in batch evaluation performance do not occur in real user studies. There are really two possibilities: either real users do not get the kind of improved recall and precision seen in batch studies with the queries that they enter or they do get better recall and precision in their searches but it does not translate into better user performance with the specific task. We will assess this by calculating recall and precision on the actual queries entered by users to determine whether systems using Okapi weighting provide benefit to them. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,318.00,563.30,8.42,10.80;3,301.50,563.30,6.00,10.80;3,335.25,533.30,6.00,10.80;3,321.00,533.30,8.42,10.80;3,279.00,533.30,8.42,10.80;3,262.50,533.30,6.00,10.80;3,72.00,587.82,27.70,10.13;3,202.50,600.80,8.00,10.80;3,249.75,600.57,122.46,10.13;3,249.75,613.32,41.56,10.13;3,208.50,635.14,1.88,6.08;3,204.00,628.55,3.34,10.80;3,249.75,626.07,142.18,10.13;3,249.75,638.82,26.38,10.13;3,213.75,660.64,1.88,6.08;3,208.50,660.64,3.38,6.08;3,204.00,654.05,3.34,10.80;3,212.25,660.64,1.69,6.08;3,249.75,651.57,151.62,10.13;3,214.50,679.39,1.88,6.08;3,208.50,679.39,3.38,6.08;3,204.00,672.80,3.34,10.80;3,213.00,679.39,1.69,6.08;3,249.75,670.32,123.14,10.13;3,249.75,683.07,42.95,10.13;3,213.75,704.89,3.38,6.08;3,207.75,704.89,3.38,6.08;3,201.00,698.30,6.67,10.80;3,212.25,704.89,1.69,6.08;3,220.50,698.82,6.35,10.13;3,249.75,695.82,122.63,10.13"><head></head><label></label><figDesc>of the term in the document d q T , = Set of terms both in q and d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,146.25,412.32,319.66,10.13;5,72.00,84.75,432.00,313.50"><head>Figure 1 -</head><label>1</label><figDesc>Figure 1 -Searching interface for baseline and Okapi weighting systems.</figDesc><graphic coords="5,72.00,84.75,432.00,313.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.00,502.63,329.37,211.34"><head>Table 1 -</head><label>1</label><figDesc>Batch results for ad hoc, instance recall, and question-answering tasks using cosine TFIDF, Okapi weighting, and Okapi + pivoted normalization weighting.</figDesc><table coords="2,375.00,502.63,1.88,6.75"><row><cell>,</cell></row></table><note coords="2,212.25,707.89,1.69,6.08;2,220.50,701.82,6.35,10.13;2,249.75,698.82,122.63,10.13"><p>, = Set of terms both in q and d</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,114.00,292.32,383.42,10.13"><head>Table 2 -</head><label>2</label><figDesc>Mean average precision for various slopes, with 0.6 obtaining the best results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.00,541.32,451.88,136.88"><head>Table 3 -</head><label>3</label><figDesc>Questions for interactive question-answering task.1. What are the names of three US national parks where one can find redwoods? 2. Identify a site of Roman ruins in present day France? 3. Name four films in which Orson Welles actually appeared. 4. Name 3 countries that imported Cuban sugar during the period of time covered by the collection.5. Which children's TV program was on the air longer, the original Mickey Mouse Club or the originalHowdy Doody Show? 6. Which painting did Edvard Munch complete first, "Vampire" or "Puberty"? 7. Which was the last dynasty of China: Qing or Ming? 8. Is Denmark larger or smaller in population than Norway ?</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.00,516.57,357.93,181.13"><head>Table 4 -</head><label>4</label><figDesc>Covariates and the variables they represented.</figDesc><table coords="6,72.00,542.82,357.93,154.88"><row><cell>Covariate</cell><cell>Definition</cell></row><row><cell>Familiar</cell><cell>User familiar with topic of question</cell></row><row><cell>Certainty</cell><cell>User certainty of answer</cell></row><row><cell>Easy Start</cell><cell>Easy to get started on question</cell></row><row><cell>Easy To Do</cell><cell>Question easy to answer</cell></row><row><cell>Satisfied</cell><cell>User satisfied system helped answer question</cell></row><row><cell>Time Adequate</cell><cell>Time was adequate to answer question</cell></row><row><cell>Terms</cell><cell>Number of unique terms used in all searchers for question</cell></row><row><cell>Cycles</cell><cell>Number of search cycles for question</cell></row><row><cell>Viewed</cell><cell>Number of document surrogates viewed for question</cell></row><row><cell>Seen</cell><cell>Number of documents for which full-text viewed for question</cell></row><row><cell>Saved</cell><cell>Number of documents saved as answering questions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.00,478.32,383.11,172.13"><head>Table 5 -</head><label>5</label><figDesc>Results for each question for all participants and OHSU participants only.</figDesc><table coords="7,78.75,503.82,376.36,146.63"><row><cell></cell><cell></cell><cell>All Groups</cell><cell></cell><cell></cell><cell>OHSU only</cell><cell></cell></row><row><cell cols="2">Question Incorrect</cell><cell cols="3">Correct % Correct Incorrect</cell><cell cols="2">Correct % Correct</cell></row><row><cell>1</cell><cell>99</cell><cell>8</cell><cell>7.5%</cell><cell>21</cell><cell>4</cell><cell>16.0%</cell></row><row><cell>2</cell><cell>80</cell><cell>18</cell><cell>18.4%</cell><cell>20</cell><cell>5</cell><cell>20.0%</cell></row><row><cell>3</cell><cell>103</cell><cell>3</cell><cell>2.8%</cell><cell>25</cell><cell>0</cell><cell>0.0%</cell></row><row><cell>4</cell><cell>77</cell><cell>29</cell><cell>27.4%</cell><cell>10</cell><cell>15</cell><cell>60.0%</cell></row><row><cell>5</cell><cell>41</cell><cell>65</cell><cell>61.3%</cell><cell>5</cell><cell>20</cell><cell>80.0%</cell></row><row><cell>6</cell><cell>59</cell><cell>41</cell><cell>41.0%</cell><cell>6</cell><cell>19</cell><cell>76.0%</cell></row><row><cell>7</cell><cell>28</cell><cell>77</cell><cell>73.3%</cell><cell>4</cell><cell>21</cell><cell>84.0%</cell></row><row><cell>8</cell><cell>92</cell><cell>9</cell><cell>8.9%</cell><cell>25</cell><cell>0</cell><cell>0.0%</cell></row><row><cell>Total</cell><cell>579</cell><cell>250</cell><cell>30.2%</cell><cell>116</cell><cell>84</cell><cell>42.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,72.00,73.32,386.11,169.87"><head>Table 6 -</head><label>6</label><figDesc>Results for each question per system.</figDesc><table coords="8,72.00,99.57,386.11,143.62"><row><cell></cell><cell>TFIDF</cell><cell></cell><cell></cell><cell cols="3">Okapi + Pivoted Normalization</cell></row><row><cell>Question</cell><cell cols="6">Searches #Correct %Correct Searches #Correct %Correct</cell></row><row><cell>1</cell><cell>13</cell><cell>3</cell><cell>23.1%</cell><cell>12</cell><cell>1</cell><cell>8.3%</cell></row><row><cell>2</cell><cell>11</cell><cell>0</cell><cell>0.0%</cell><cell>14</cell><cell>5</cell><cell>35.7%</cell></row><row><cell>3</cell><cell>13</cell><cell>0</cell><cell>0.0%</cell><cell>12</cell><cell>0</cell><cell>0.0%</cell></row><row><cell>4</cell><cell>12</cell><cell>7</cell><cell>58.3%</cell><cell>13</cell><cell>8</cell><cell>61.5%</cell></row><row><cell>5</cell><cell>12</cell><cell>9</cell><cell>75.0%</cell><cell>13</cell><cell>11</cell><cell>84.6%</cell></row><row><cell>6</cell><cell>15</cell><cell>13</cell><cell>86.7%</cell><cell>10</cell><cell>6</cell><cell>60.0%</cell></row><row><cell>7</cell><cell>13</cell><cell>11</cell><cell>84.6%</cell><cell>12</cell><cell>10</cell><cell>83.3%</cell></row><row><cell>8</cell><cell>11</cell><cell>0</cell><cell>0.0%</cell><cell>14</cell><cell>0</cell><cell>0.0%</cell></row><row><cell>Total</cell><cell>100</cell><cell>43</cell><cell>43.0%</cell><cell>100</cell><cell>41</cell><cell>41.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,72.00,511.32,427.75,206.63"><head>Table 7 -</head><label>7</label><figDesc>Summary of p-values for base analysis of variance model and model with each potential covariate added to model individually.</figDesc><table coords="8,72.00,550.32,307.69,167.63"><row><cell>Covariate</cell><cell>System</cell><cell>Questions</cell><cell>Covariate</cell></row><row><cell>None</cell><cell>0.73</cell><cell>&lt;0.0001</cell><cell>N/A</cell></row><row><cell>Familiar</cell><cell>0.76</cell><cell>&lt;0.0001</cell><cell>0.70</cell></row><row><cell>Certainty</cell><cell>0.92</cell><cell>&lt;0.0001</cell><cell>&lt;0.0001</cell></row><row><cell>Easy Start</cell><cell>0.82</cell><cell>&lt;0.0001</cell><cell>0.18</cell></row><row><cell>Easy To Do</cell><cell>0.82</cell><cell>0.0021</cell><cell>&lt;0.0001</cell></row><row><cell>Satisfied</cell><cell>0.76</cell><cell>0.034</cell><cell>&lt;0.0001</cell></row><row><cell>Time Adequate</cell><cell>0.88</cell><cell>0.030</cell><cell>&lt;0.0001</cell></row><row><cell>Terms</cell><cell>0.98</cell><cell>&lt;0.0001</cell><cell>0.096</cell></row><row><cell>Cycles</cell><cell>0.94</cell><cell>&lt;0.0001</cell><cell>0.44</cell></row><row><cell>Viewed</cell><cell>0.86</cell><cell>&lt;0.0001</cell><cell>0.13</cell></row><row><cell>Seen</cell><cell>0.59</cell><cell>&lt;0.0001</cell><cell>0.0417</cell></row><row><cell>Saved</cell><cell>0.23</cell><cell>&lt;0.0001</cell><cell>&lt;0.0001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,72.00,541.32,350.47,166.88"><head>Table 8 -</head><label>8</label><figDesc>Batch searching results.</figDesc><table coords="9,72.00,567.57,350.47,140.63"><row><cell>Question</cell><cell>TFIDF</cell><cell>Okapi + Pivoted</cell><cell>% improvement</cell></row><row><cell></cell><cell></cell><cell>Normalization</cell><cell></cell></row><row><cell>1</cell><cell>0.1352</cell><cell>0.0635</cell><cell>-53.0%</cell></row><row><cell>2</cell><cell>0.0508</cell><cell>0.0605</cell><cell>19.1%</cell></row><row><cell>3</cell><cell>0.1557</cell><cell>0.3000</cell><cell>92.7%</cell></row><row><cell>4</cell><cell>0.1515</cell><cell>0.1778</cell><cell>17.4%</cell></row><row><cell>5</cell><cell>0.5167</cell><cell>0.6823</cell><cell>32.0%</cell></row><row><cell>6</cell><cell>0.7576</cell><cell>1.0000</cell><cell>32.0%</cell></row><row><cell>7</cell><cell>0.3860</cell><cell>0.5425</cell><cell>40.5%</cell></row><row><cell>8</cell><cell>0.0034</cell><cell>0.0088</cell><cell>158.8%</cell></row><row><cell>Mean</cell><cell>0.2696</cell><cell>0.3544</cell><cell>31.5%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This study was supported in part by Grant <rs type="grantNumber">LM06311</rs> of the <rs type="funder">U.S. National Library of Medicine</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mhdrjtU">
					<idno type="grant-number">LM06311</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,108.00,149.07,432.20,10.13;10,108.00,161.82,416.66,10.13;10,108.00,174.57,123.44,10.13" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,175.50,149.07,242.87,10.13">Do batch and user evaluations give the same results?</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,434.25,149.07,105.95,10.13;10,108.00,161.82,348.58,10.13">Proceedings of the 23rd Annual International ACM Special Interest Group in Information Retrieval</title>
		<meeting>the 23rd Annual International ACM Special Interest Group in Information Retrieval<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,187.32,428.82,10.13;10,108.00,200.07,244.71,10.13" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,247.50,187.32,289.32,10.13;10,108.00,200.07,51.61,10.13">Managing Gigabytes -Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Van Nostrand Reinhold</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,212.82,382.19,10.13" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,209.25,212.82,137.55,10.13">Exploring the similarity space</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,354.75,212.82,56.83,10.13">SIGIR Forum</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="18" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,224.82,425.08,10.13;10,108.00,237.57,432.02,10.13;10,108.00,250.32,359.69,10.13" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,231.00,224.82,302.08,10.13;10,108.00,237.57,144.09,10.13">Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,267.75,237.57,272.27,10.13;10,108.00,250.32,182.83,10.13">Proceedings of the 17th Annual International ACM Special Interest Group in Information Retrieval</title>
		<meeting>the 17th Annual International ACM Special Interest Group in Information Retrieval<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,263.07,431.37,10.13;10,108.00,275.82,421.69,10.13;10,108.00,288.57,170.72,10.13" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,269.25,263.07,180.97,10.13">Pivoted document length normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,468.00,263.07,71.37,10.13;10,108.00,275.82,389.83,10.13">Proceedings of the 19th Annual International ACM Special Interest Group in Information Retrieval</title>
		<meeting>the 19th Annual International ACM Special Interest Group in Information Retrieval<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,301.32,431.97,10.13;10,108.00,313.32,431.81,10.13;10,108.00,326.07,165.47,10.13" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,247.50,301.32,292.47,10.13;10,108.00,313.32,121.37,10.13">Development of an instrument measuring user satisfaction of the human-computer interface</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Norman</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,245.25,313.32,289.59,10.13">Proceedings of CHI &apos;88 -Human Factors in Computing Systems</title>
		<meeting>CHI &apos;88 -Human Factors in Computing Systems<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="213" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,338.82,431.97,10.13;10,108.00,351.57,294.44,10.13" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,241.50,338.82,293.34,10.13">Generalized linear mixed models: a pseudo-likelihood approach</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wolfinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,108.00,351.57,211.14,10.13">Journal of Statistical Computation and Simulation</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="233" to="243" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,364.32,432.31,10.13;10,108.00,377.07,416.66,10.13;10,108.00,389.82,95.69,10.13" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,231.00,364.32,181.82,10.13">Evaluating evaluation measure stability</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,428.25,364.32,112.06,10.13;10,108.00,377.07,348.58,10.13">Proceedings of the 23rd Annual International ACM Special Interest Group in Information Retrieval</title>
		<meeting>the 23rd Annual International ACM Special Interest Group in Information Retrieval<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
