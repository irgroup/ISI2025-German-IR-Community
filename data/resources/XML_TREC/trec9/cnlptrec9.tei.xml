<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,228.00,75.46,154.90,16.20;1,129.00,96.46,354.10,16.20">Question Answering : CNLP at the TREC-9 Question Answering Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,117.00,115.55,65.62,10.80"><forename type="first">Anne</forename><surname>Diekema</surname></persName>
							<email>diekemar@syr.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.68,115.55,61.13,10.80"><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.05,115.55,67.63,10.80"><forename type="first">Jiangping</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.66,115.55,63.44,10.80"><forename type="first">Hudong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.73,115.55,82.35,10.80"><forename type="first">Nancy</forename><surname>Mccracken</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.75,129.80,68.99,10.80"><forename type="first">Ozgur</forename><surname>Yilmazel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.96,129.80,84.51,10.80"><forename type="first">Elizabeth</forename><forename type="middle">D</forename><surname>Liddy</surname></persName>
							<email>liddy@syr.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies</orgName>
								<orgName type="department" key="dep3">Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<addrLine>4-206</addrLine>
									<postCode>1324-4100</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,228.00,75.46,154.90,16.20;1,129.00,96.46,354.10,16.20">Question Answering : CNLP at the TREC-9 Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B8724DEB7E355B52F902764E53ECDBC4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a question answering system that automatically finds answers to questions in a large collection of documents. The prototype CNLP question answering system was developed for participation in the TREC-9 question answering track. The system uses a two-stage retrieval approach to answer finding based on keyword and named entity matching. Results indicate that the system ranks correct answers high (mostly rank 1), provided that an answer to the question was found. Performance figures and further analyses are included.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Question answering is not typically found in traditional information retrieval systems. In information retrieval, the system presents the user with a list of relevant documents in response to the query. The user then reviews these documents in search of the information that prompted the original search. It is not surprising therefore that, especially for short questions, people tend to ask their peers or forego the answer rather than expending time and effort with an information retrieval system. <ref type="bibr" coords="1,163.83,408.57,12.65,10.13" target="#b1">[3]</ref> Ideally, question answering helps users in their information finding task by providing exact answers rather than a ranked list of documents that may contain the answer.</p><p>The TREC question-answering track fosters question-answering research. Question-answering systems are not as well developed as information retrieval systems, especially for domain independent questions. As first-time participants, the Center for Natural Language Processing (CNLP) developed a question-answering system to deal with domain independent questions.</p><p>The CNLP question answering system uses a two-stage retrieval approach to answer-finding based on keyword, entity, and template matching (see figure <ref type="figure" coords="1,357.30,522.57,3.94,10.13">1</ref>). In answering a question, the system first creates a logical query representation of the question that is used for the initial information retrieval step. Additional modules take the retrieved documents for further processing and answer finding. Answer finding uses two different approaches after which answer triangulation takes place to select the most likely answer. The first approach to answer finding is based on keyword and entity matching and the second on template matching. Currently only the keyword and entity matching answer-finding approach has been implemented. A detailed system overview can be found in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem description</head><p>Participants in the question-answering track were provided with 693 questions that originated from search engine logs. The initial question set of 693 questions was reduced to 682 questions after 11 questions were discarded by the National Institute for Standards and Technology (NIST). The remaining questions were mostly fact-based and required short answers only (see figure <ref type="figure" coords="1,499.67,687.57,3.98,10.13">2</ref>). The base set of questions consisted of 500 questions. For 54 questions, slight variations were created resulting in an additional 193 questions. Answers to all 693 questions had to be retrieved automatically from approximately 3 gigabytes of data. Sources of the data were: AP newswire 1988-1990 (728 Mb), Wall Street Journal 1987-1992 (509 Mb), San Jose Mercury News 1991 (287 Mb), Financial Times 1991-1994 (564 Mb), Los Angeles Times 1989, 1990 (475 Mb), Foreign Broadcast Information Service 1996 (470 Mb).</p><p>Figure <ref type="figure" coords="2,125.52,467.07,4.27,10.13">1</ref>. CNLP question answering system (shaded areas not part of TREC-9 system).</p><p>For each question, up to five ranked answer submissions were permitted, with the system producing the most likely answer ranked first. The maximum length of the answer string for a retrieval run was either 50 bytes or 250 bytes. An response to a question consisted of the question number, the document ID of the document containing the answer, rank, run name, and the answer string itself. The submitted answer strings were evaluated by NIST's human assessors for correctness. <ref type="bibr" coords="2,145.99,555.57,13.05,10.13" target="#b4">[6]</ref> TREC-9 question answering questions Base question 419: Who was Jane Goodall? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System overview</head><p>The prototype of the CNLP question-answering system consists of four different processes: question processing, document processing, paragraph finding, and keyword and entity based answer finding. Each of the processes is described in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question processing</head><p>During question processing, the system converts the question into a logical query representation used for first stage information retrieval and the system determines the focus of each question used for answer finding. Question processing takes place in our Language-to-Logic or L2L module. The L2L process for the question-answering track is optimized for retrieval using the AltaVista search engine (see section 3.2), and includes a focus recognizer. For example, the question "What was the monetary value of the Nobel Peace Prize in 1989?" results in the following output: AltaVista query:</p><p>"monetary value*" +"Nobel Peace Prize*" 1989* Question focus: money|numb</p><p>The L2L module converts a natural language query or question into a generic logical representation, which can be interpreted by different search engines. The conversion from language to logic takes place based on an internal query sublanguage grammar, which has been developed by CNLP. Prior to conversion, query processing such as stemming, stopword removal, and phrase and Named Entity recognition take place. We experimented with query expansion for first stage retrieval but experienced a slight drop in the results. Based on these results query expansion was left out of the TREC-9 question-answering system.</p><p>Question focus recognition aims to determine the expected answer by analyzing the question. For example, consider the question: "What is the monetary value of the Nobel Peace Prize in 1989?" The questioner is obviously looking for a monetary value and that is the focus of the question. Determining the question focus (also referred to as question type, answer type or asking point) helps to narrow the possible answers for which the system will look.</p><p>The system uses two strategies to determine the question focus: the question, and, if that strategy fails, the CNLP Named Entity hierarchy. The first strategy tries to find the focus of the question based on clues found directly in the question itself. If the beginning of a question resembles any of a set of clues it is clear what focus is intended. For example, if a question contains the words "which capital city" then the focus is "city". However, it is impossible to predict all possible questions and to have a program that deals with any question. If the system cannot assign a focus to a question using example question phrases, the system then moves to the Named Entity hierarchy clues. The system incorporates one or more clue words for each of the hierarchy classes. For example, the words hurricane or storm in a question might indicate that the questioner is looking for a weather event. The "why" focus is an exceptional case since it does not indicate a particular topic but rather a place in the sentence where an answer might be found (e.g. after the word "because"). The performance of the focus recognition capability is analyzed in section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document processing for first stage retrieval</head><p>We used two different retrie val approaches for first stage retrieval: Boolean and probabilistic. The entire TREC-9 question answering document collection has been indexed using AltaVista Search Engine 3.0, which is a modified version of the software that runs the search engine at http://www.altavista.com. <ref type="bibr" coords="3,205.42,692.82,12.80,10.13" target="#b0">[2]</ref> AltaVista 3.0 indexes all words and does not use stemming. The document collection consisted of 978,952 documents with the average number of words per document being less than 500. Indexing this collection took approximately 6 days using a Dual Pentium III, 550Mhz, with 512MB ram, running Windows 2000 server. AltaVista also provides the Search Developer's Kit (SDK). The SDK's Interoperability API allows programs to read data from indexes created by the search engine. A batch process takes the L2L query representations and the index directory of document collection as input. For each question, the program returns up to 32,000 documents.</p><p>For our probabilistic runs we used the SMART retrieval runs as provided by NIST. The SMART information retrieval system, originally developed by Salton, uses the vector-space model of information retrieval that represents query and documents as term vectors. <ref type="bibr" coords="4,420.50,187.32,12.86,10.13" target="#b3">[5]</ref> All vectors have t components where t is the number of unique terms (or stems) in the collection. A comparison of the Boolean and probabilistic first stage retrieval approaches can be found in section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Paragraph finding</head><p>The system uses paragraphs rather than documents for its second stage retrieval. Based on the TREC question-answering guidelines and last year's questions, we assumed that the desired answers were going to be short and factual (less than 50 bytes long). Also, the answer context, which identifies an answer as belonging to a certain question, is usually a small part of the original document. <ref type="bibr" coords="4,175.47,300.57,12.81,10.13" target="#b2">[4]</ref>   In the paragraph finding stage, we aimed to select the most relevant paragraphs from the retrieved documents from the first stage retrieval step. Paragraph selection was based on keyword occurrences in the paragraphs. The top 300 most relevant paragraphs were selected for each question. After selection, the paragraphs were part of speech tagged and categorized by &lt;!metaMarker&gt; TM using CNLP's categorization rules (see figure <ref type="figure" coords="4,378.84,497.07,4.09,10.13" target="#fig_0">3</ref>).</p><p>[1] The quality of selected paragraphs and the system's categorization capabilities directly impact later processing such as co-reference resolution (currently not implemented), and answer finding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Keyword and entity based answer finding</head><p>The keyword and entity based answer finding process took the tagged paragraphs from the paragraph finding stage and identified different paragraph windows within each paragraph. A weighting scheme was used to identify the most promising paragraph window for each paragraph. These paragraph windows were then used to find answer candidates based on the question focus.</p><p>All answer candidates were weighted and the top 5 were selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">1. Paragraph-window identification and selection</head><p>Paragraph windows were selected by examining each occurrence of a question keyword in a paragraph. Each occurrence of a keyword in relation to the other question keywords was considered to be a paragraph window. A keyword that occurred multiple times thus resulted in multiple paragraph windows, one for each occurrence. A weight for each window was determined by the position of the keywords in the window and the distance between them. An alternative weighting formula was used for single -word questions. The window with the highest score was selected to represent that paragraph. The process was repeated for all 300 paragraphs resulting in an ordered list of paragraph windows -all potentially containing the answer to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Answer candidate identification</head><p>Answer candidates were identified in each paragraph window based on the question focus. Each paragraph window can have multiple answer candidates. If the question focus matched any of the categorized named entities, complex nominals, or numeric concepts in the window, they were considered to be answer candidates. If none of the categorized entities matched the question focus, the system translated the focus into a more general tag. For example, if the question focus called for a city and the paragraph did not have a city tag, the system then looked for a named entity in that paragraph. Naturally these matches received lower weights than entities that directly matched the question tag. If there was no question focus assigned to the question, the system reverted to an alternative strategy and picked the sentence with the largest number of question keywords and looked for named entities. In identifying the different answer candidates, the required window sizes of 50 or 250 bytes were also generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Answer-candidate scoring and answer selection</head><p>The system used a weighting scheme to assign a weight to each answer candidate. The weight was based on the keywords (presence, order, and distance), whether the answer candidate matched the question focus, and punctuation near the answer candidate. This resulted in a pool of at least 300 candidates for each query. The 5 highest scoring answer candidates were selected as the final answers for each question. The answer strings were formatted according to NIST specifications of either 50 bytes or 250 bytes depending on the run. This process was repeated for all 693 questions resulting in an answer file of 4815 (693x5) lines that were submitted to NIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Our submission for the question-answering track consisted of four different runs. The SUT9bn3c runs use our L2L module (see section 3.1) with the AltaVista retrieval system for the first-stage retrieval, whereas the SUT9p2c3c runs used the SMART (provided by NIST). Each of these runs had a 50 byte as well as a 250 byte answer string submission. A system bug caused our 250 byte answers to be about 50 bytes shorter (see table <ref type="table" coords="5,298.42,465.57,3.97,10.13" target="#tab_2">1</ref>), which caused a slight drop in results. The program only extended the number of answer bytes on the right-hand side of the answer string but failed to do so on the left-hand side. The measure used for evaluation in the question-answering track is the mean reciprocal answer rank. For each question, a reciprocal answer rank is determined by evaluating the top five ranked answers starting with one. The reciprocal answer rank is the reciprocal of the rank of the first correct answer. If there is no correct answer among the top five, the reciprocal rank is zero. Since there are only five possible ranks, the mean reciprocal answer ranks can be 1, 0.5, 0.33, 0.25, 0.2, or 0. The mean reciprocal answer ranks for all the questions are summed together and divided by the total number of questions to get the mean reciprocal rank for each system run.</p><p>As is to be expected, the 50 byte runs have a much larger number of questions without an answer than the 250 byte runs. In all four runs, for most questions the system performance equaled the median reciprocal rank of all runs. The majority of the remaining questions were placed above the median. The strength of our system lies in answer ranking. Consistently across all four runs, the majority of the correct answers were ranked first. Unfortunately, in all four runs we had trouble locating the answers to the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>This section examines retrieval performance of first stage retrieval, the Language-to-Logic module, and question focus assignment as well as exact answer finding and the effect of question variants on system performance. Overall analysis based on the probabilistic 50 byte run (SUT9p2c3c050) shows that the system retrieves at least one relevant document for each of 625 questions. In the paragraph finding stage we extract paragraphs from 609 of these documents. Out of these 609 paragraphs, 578 paragraphs contain a possible correct answer. However, for only 243 questions we find that correct answer in these paragraphs. Thus, it appears that the answer scoring mechanism and entity tagging, need further refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">First stage retrieval</head><p>The analysis of the first stage retrieval was based on the list of relevant documents provided by NIST. We used two different first stage retrieval approaches, a Boolean approach using our L2L module with AltaVista, and a probabilistic approach using the SMART runs (see section 3.1).</p><p>Analysis shows that the retrieval performance of both systems is very similar except for the retrieved number of relevant documents, which is larger for SMART (see table <ref type="table" coords="6,440.75,623.07,3.97,10.13">3</ref>). This difference is probably caused by a number of AltaVista query representations that had a large number of mandatory terms and failed to retrieve a single document.</p><p>Although the SMART retrieval system retrieves more relevant documents, the performance of the two first-stage retrieval models in question answering is very similar. SMART performed slightly better in the 250 byte runs (see table <ref type="table" coords="6,253.70,698.82,3.97,10.13" target="#tab_2">1</ref>). Table <ref type="table" coords="7,121.02,206.82,4.27,10.13">3</ref>. First stage retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Question representation</head><p>Logical question representations are one of the things created in the question processing stage (see section 3.1). The question representation analysis is based on the probabilistic 50 byte run (SUT9p2c3c050). A close examination of the question representations created by our Languageto-Logic module showed that for 539 (78.89%) questions, the representation was correct, although 64 (9.38%) representations could stand to be improved. 144 (21.11%) question representations had one or more problems. The most frequently occurring problems were: part-ofspeech tagging errors; difficulties with query length (single word questions and very long questions), and; keyword selection problems (see figure <ref type="figure" coords="7,337.09,332.82,3.94,10.13" target="#fig_1">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem count</head><p>Problems with description 76 49 6 10 part-of-speech errors: wrong tags lead to bad phrases and non-content words being added to query query length: single word queries provide little information for answer finding, long queries with many mandatory terms hinder retrieval misplaced wildcards: wildcards placed on final terms of multi-word terms only, or in the wrong place of single terms creating bad stems keyword selection problems: content words such as numbers erroneously filtered out It is clear that the part-of speech tagger had trouble dealing with the unusual phrase structure presented by questions. Other problems, such as the single word queries, are a direct result of the phrasing of the original question. Question expansion for second-stage retrieval might be a solution for this problem. Keyword selection is an L2L problem that needs to be adjusted to keep numbers, and possibly adjectives, that specify the answer (i.e. Who was the first Russian astronaut to walk in space?).</p><p>The query representation problems were expected to have a negative impact on answer finding but further analysis showed that this was not the case (see table <ref type="table" coords="7,372.69,624.57,3.97,10.13" target="#tab_5">4</ref>). Even with a problematic question representation, the system was still able to find answers for 77 questions while for 276 questions that did have correct query representations, no correct answers were found. This means that query representation alone only accounts for part of the error. Out of all the questions that ranked the correct answer first, 97 questions (75.78%) had a correct question focus. It appears that a correct focus aids in answer ranking. When looking at the questions with an incorrect query focus (86) we see that most of these questions (73, or 84.88%) failed to retrieve an answer at all. We can conclude that it pays to have a determinable focus as long as this focus is correct. However, finding the correct query focus is not a guarantee for finding the answer since 194 questions (55.75%) with a correct focus did not retrieve a correct answer.</p><p>A closer examination of questions with an incorrect question focus shows that 40 of these questions are erroneously assigned a "person" focus. 17 of the erroneous person focus questions are of the "who is Colin Powell" type. Unlike questions such as "who created the Muppets?" the answer to "who is &lt;person name&gt;" questions is not a person's name but rather a description of that person. Additional problems with the person focus were questions looking for groups of people (i.e. cultures, sports teams) rather than individual persons, or other entities than persons (i.e. companies, cartoon characters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Question variants</head><p>As described in section 2, NIST included 193 question variants which are re-wordings of a set of 54 questions (see figure <ref type="figure" coords="8,196.47,621.57,3.93,10.13">2</ref>). The question variants analysis is based on the probabilistic 50 byte run (SUT9p2c3c050). These question variants allowed us to study the effect of question formulation on system performance. For 25 out of the 54 question sets, the query variation caused no difference in performance. The majority of these questions did not retrieve correct answers no matter how the questions were posed to the system. 29 question sets did show differences in retrieval performance. For 12 sets, the performance differences originated entirely in additional question terms being either present or missing. For 7 sets, the differences in performance were partially due to divergence of question terms. Some question terms would guarantee a correct answer, whereas others would throw the results off. The majority of the questions are rather short, so each question term has a relatively large influence on finding the answer. The query variant results indicate that query expansion could have a large impact on system performance. Although we experimented with query expansion for first stage retrieval, we did not have enough time to explore it in the answer-finding stage.</p><p>For 14 sets, some of the differences in system performance appear to be caused by a different or missing question focus. In eight question sets, some of the differences in performance were caused by the question focus being incorrect. Additional question words mislead the system in choosing the wrong question focus. In sets where the question focus is either missing, different, or incorrect, the well-performing counterpart questions did have the correct or more exact focus, and the variant questions, without the exact clue, experienced a drop in rank or a failed attempt to find the answer. These findings indicate that having a correct question focus is of importance, which supports findings of the question focus analysis (section 5.3).</p><p>In seven question sets, some of the differences were caused by inconsistencies in the answer judgments. Certain answers would be judged to be correct for some questions, whereas for others the same answer would be judged to be incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Exact answer finding</head><p>Although plans for an "exact answer" run were abandoned by NIST, we examined the system's exact answer-finding capabilities for the probabilistic 50 byte run (SUT9p2c3c050). The majority of the exact answers that our system produced were judged correct (197 or 81.07%), and only 46 (18.93%) of the answers were produced by the context of the answer window (see table <ref type="table" coords="9,482.98,401.82,4.00,10.13">6</ref>). This indicates that our system had quite a high answer-finding accuracy when a correct answer was contained in the retrieved document. Table <ref type="table" coords="9,121.08,558.57,4.28,10.13">6</ref>. Rank distribution of correctly answered questions and our system performance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future research</head><p>The performance of the CNLP question answering system is highly encouraging. The majority of the correct answers are ranked first and the majority of question representations and assigned question foci were accurate. The prototype system also does well at exact answer finding. However, for a large number of questions no correct answers are found. It appears that the current system does not capitalize on the large number of relevant documents found in the first retrieval stage.</p><p>Further research is needed to refine the weighting in the paragraph selection and answer finding stages, and to improve the query sublanguage grammar to increase question focus assignment robustness. In addition, a new morphological analyzer needs to be implemented and the part-ofspeech tagger needs to be trained on question phrase structure, to improve question representations. A more detailed study of the categorization performance and coverage is also in order. Time also needs to be spent on researching and implementing a second approach to answer finding based on template matching.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,90.00,421.32,396.88,10.13"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Example of tagged paragraph (AP900429-0033) with answer "Mount Everest."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,90.00,497.82,197.40,10.13"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Question representation problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,300.57,424.68,118.55"><head></head><label></label><figDesc>Paragraphs, which are much shorter than documents, have the added benefit of cutting down costly processing time. Paragraph detection is based on text indentations.</figDesc><table /><note coords="4,95.25,340.59,190.39,8.77;4,95.25,352.59,127.48,8.77;4,97.50,364.59,395.30,8.77;4,95.25,375.84,394.58,8.77;4,95.25,387.84,396.11,8.77;4,95.25,399.09,394.61,8.77;4,95.25,410.34,366.12,8.77"><p>889: What is the highest mountain in the world? Question focus: mnt (mountain) &lt;NC cat=numb&gt; two|CD &lt;/NC&gt; &lt;CN&gt; three-person|JJ team|NNS &lt;/CN&gt; of|IN &lt;NP cat=geoadj id=3&gt; American|NP &lt;/NP&gt; ,|, &lt;NP cat=geoadj id=0&gt; Soviet|NP &lt;/NP&gt; and|CC &lt;CN&gt; &lt;NP cat=geoadj id=1&gt; Chinese|NP &lt;/NP&gt; climber|NNS &lt;/CN&gt; will|MD attempt|VB to|TO reach|VB the|DT top|NN of|IN &lt;NC cat=dist&gt; 29,028-foot|JJ &lt;/NC&gt; &lt;NP cat=mnt id=2&gt; Mount|NP Everest|NP &lt;/NP&gt; ,|, the|DT world|NN 's|POS &lt;CN&gt; highest|JJS mountain|NN &lt;/CN&gt; ,|, on|IN &lt;NC cat=time&gt; May|NP 6|CD &lt;/NC&gt; .|.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,90.00,517.59,428.54,116.35"><head>Table 1 .</head><label>1</label><figDesc>Question answering results for all four runs.</figDesc><table coords="5,95.25,517.59,423.29,104.77"><row><cell>Averages over 682 questions</cell><cell>SUT9</cell><cell>SUT9</cell><cell>SUT9</cell><cell>SUT9</cell></row><row><cell>(strict evaluation):</cell><cell>bn3c050</cell><cell>p2c3c050</cell><cell>bn3c250</cell><cell>p2c3c250</cell></row><row><cell>Allowed answer length in bytes</cell><cell>50</cell><cell>50</cell><cell>250</cell><cell>250</cell></row><row><cell>Average response length in bytes</cell><cell>49.68</cell><cell>49.65</cell><cell>203.24</cell><cell>198.62</cell></row><row><cell>Mean reciprocal rank (682 questions)</cell><cell>0.247</cell><cell>0.249</cell><cell>0.365</cell><cell>0.385</cell></row><row><cell>Questions with no answer found</cell><cell>436 (63.9%)</cell><cell>439 (64.4%)</cell><cell>334 (49.0%)</cell><cell>319 (46.8%)</cell></row><row><cell>Questions above the median 1</cell><cell>191 (28.0%)</cell><cell>190 (27.86%)</cell><cell>202 (29.62%)</cell><cell>198 (29.03%)</cell></row><row><cell>Questions on the median</cell><cell>427 (62.61%)</cell><cell>450 (65.98%)</cell><cell>351 (51.47%)</cell><cell>358 (55.64%)</cell></row><row><cell>Questions below the median</cell><cell>64 (9.48%)</cell><cell>42 (6.16%)</cell><cell>129 (18.91%)</cell><cell>99 (14.52%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,238.59,426.29,116.35"><head>Table 2 .</head><label>2</label><figDesc>Answer rank distribution of question answering results.</figDesc><table coords="6,95.25,238.59,421.04,104.02"><row><cell>Answer ranks</cell><cell>SUT9</cell><cell>SUT9</cell><cell>SUT9</cell><cell>SUT9</cell></row><row><cell></cell><cell>bn3c050</cell><cell>p2c3c050</cell><cell>bn3c250</cell><cell>p2c3c250</cell></row><row><cell>Correct answer ranked 1</cell><cell>126 (18.48%)</cell><cell>128 (18.77%)</cell><cell>193 (28.30%)</cell><cell>208 (30.50%)</cell></row><row><cell>Correct answer ranked 2</cell><cell>43 (6.30%)</cell><cell>42 (6.16%)</cell><cell>59 (8.65%)</cell><cell>52 (7.62%)</cell></row><row><cell>Correct answer ranked 3</cell><cell>35 (5.13%)</cell><cell>37 (5.43%)</cell><cell>45 (6.60%)</cell><cell>46 (6.74%)</cell></row><row><cell>Correct answer ranked 4</cell><cell>21 (3.08%)</cell><cell>22 (3.23%)</cell><cell>28 (4.11%)</cell><cell>31 (4.55%)</cell></row><row><cell>Correct answer ranked 5</cell><cell>21 (3.08%)</cell><cell>14 (2.05%)</cell><cell>23 (3.37%)</cell><cell>26 (3.81%)</cell></row><row><cell>No correct answer found (rank 0)</cell><cell>436 (63.93%)</cell><cell>439 (64.37%)</cell><cell>334 (48.97%)</cell><cell>319 (46.77%)</cell></row><row><cell>Total</cell><cell>682</cell><cell>682</cell><cell>682</cell><cell>682</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,90.00,87.84,430.89,279.52"><head>Table 4 .</head><label>4</label><figDesc>Question representation correctness and question answering ability.</figDesc><table coords="8,90.00,87.84,430.89,279.52"><row><cell></cell><cell></cell><cell>Correct representation</cell><cell>Problematic representation</cell></row><row><cell>Answer correct</cell><cell></cell><cell>166 (37.56%)</cell><cell>77 (32.08%)</cell></row><row><cell>Answer incorrect</cell><cell></cell><cell>276 (62.44%)</cell><cell>163 (67.92%)</cell></row><row><cell>Total</cell><cell></cell><cell>442</cell><cell>240</cell></row><row><cell cols="2">5.3 Question focus</cell><cell></cell><cell></cell></row><row><cell cols="4">As described in section 3.1, we determined the focus based on the question clues or Named Entity</cell></row><row><cell cols="4">Hierarchy clues. The question focus analysis is based on the probabilistic 50 byte run</cell></row><row><cell cols="4">(SUT9p2c3c050). Out of 682 answerable questions, our system determined a question focus for</cell></row><row><cell cols="4">434 (63.64%) of the questions. Out of these 434 questions, 348 questions (80.18%) had a correct</cell></row><row><cell cols="4">focus, and 86 questions (19.82%) had an incorrect focus. For 248 (36.36%) questions, our system</cell></row><row><cell cols="2">could not determine a focus.</cell><cell></cell><cell></cell></row><row><cell cols="2">Correct question focus</cell><cell>Incorrect question focus</cell><cell>No determinable question</cell></row><row><cell></cell><cell></cell><cell></cell><cell>focus</cell></row><row><cell>Rank 1</cell><cell>97 (27.87%</cell><cell>5 (5.81%</cell><cell>26 (10.48%)</cell></row><row><cell>Rank 2</cell><cell>22 (6.32%</cell><cell>4 (4.65%</cell><cell>16 (6.45%)</cell></row><row><cell>Rank 3</cell><cell>19 (5.46%</cell><cell>2 (2.33%</cell><cell>16 (6.45%)</cell></row><row><cell>Rank 4</cell><cell>9 (2.59%</cell><cell>1 (1.16%</cell><cell>12 (4.84%)</cell></row><row><cell>Rank 5</cell><cell>7 (2.01%</cell><cell>1 (1.16%</cell><cell>6 (2.42%)</cell></row><row><cell>Rank 0</cell><cell>194 (55.75%</cell><cell>73 (84.88%)</cell><cell>172 (69.35%)</cell></row><row><cell>Total</cell><cell>348</cell><cell>86</cell><cell>248</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,90.00,368.82,260.44,10.13"><head>Table 5 .</head><label>5</label><figDesc>Answer rank distribution of question focus status.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,102.00,454.59,395.64,102.53"><head>Question answered at rank â€¦ Number of Q. judged correct Exact correct answer string found Answer produced by context words in the 50-byte window</head><label></label><figDesc></figDesc><table coords="9,112.50,489.09,350.53,68.03"><row><cell>Rank 1</cell><cell>128</cell><cell>112</cell><cell>16</cell></row><row><cell>Rank 2</cell><cell>42</cell><cell>31</cell><cell>11</cell></row><row><cell>Rank 3</cell><cell>37</cell><cell>27</cell><cell>10</cell></row><row><cell>Rank 4</cell><cell>22</cell><cell>15</cell><cell>7</cell></row><row><cell>Rank 5</cell><cell>14</cell><cell>12</cell><cell>2</cell></row><row><cell>Total</cell><cell>243</cell><cell>197 (81.07%)</cell><cell>46 (18.93%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,96.00,687.84,401.86,8.77;5,90.00,699.09,426.56,8.77;5,90.00,711.09,190.78,8.77"><p>The median is the middle score (or the average of the two middle scores in case of an even number of scores) for each question after the answer scores for all participants have been put in rank order. 33 groups submitted a 50 byte runs, 42 groups submitted a</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="250" xml:id="foot_1" coords="5,300.44,711.09,35.42,8.77"><p>byte run.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="7,96.00,687.84,416.78,8.77;7,90.00,699.09,58.52,8.77"><p>Number includes the 11 questions discarded by NIST and 9 questions for which no relevance judgments were available.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="7,96.00,711.09,260.10,8.77"><p>Average precision over all relevant documents, non-interpolated.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Catie Christiaanse</rs>, <rs type="person">Michelle Monsour</rs>, and <rs type="person">Eileen Allen</rs> for creating the necessary categorization rules in a very limited time frame. We would also like to thank <rs type="person">Hassan Bolut</rs>, and <rs type="person">Wen-Yuan Hsiao</rs> for their engineering support. Lastly we would like to thank <rs type="person">Lois Elmore</rs> for keeping us all in line.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,105.53,301.32,376.17,10.13" xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://solutions.altavista.com/downloads/downloads.html" />
	</analytic>
	<monogr>
		<title level="j" coord="10,105.53,301.32,107.68,10.13">AltaVista Search Engine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.54,326.07,394.01,10.13;10,90.00,338.82,97.22,10.13" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,225.19,326.07,217.01,10.13">Theory of Information Usage</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,450.00,326.07,49.55,10.13;10,90.00,338.82,51.39,10.13">Journal of Informatics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2" to="5" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
	<note>The &quot;Why Bother?</note>
</biblStruct>

<biblStruct coords="10,105.59,364.32,367.83,10.13;10,90.00,377.07,410.77,10.13;10,90.00,389.82,398.42,10.13" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,321.54,364.32,151.88,10.13;10,90.00,377.07,46.57,10.13">Question-Answering by Predictive Annotation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">;</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><forename type="middle">;</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anni</forename><surname>Coden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,158.25,377.07,342.52,10.13;10,90.00,389.82,237.07,10.13">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 23rd annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07-24">2000. July 24 -28, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.31,414.57,414.96,10.13;10,90.00,427.32,265.00,10.13" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ed</surname></persName>
		</author>
		<title level="m" coord="10,206.25,414.57,315.02,10.13;10,90.00,427.32,49.60,10.13">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall, Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page">556</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.12,452.82,413.08,10.13;10,90.00,465.57,431.86,10.13;10,90.00,478.32,432.19,10.13;10,90.00,491.07,82.69,10.13" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,330.06,452.82,192.14,10.13;10,90.00,465.57,45.92,10.13">The TREC-8 Question Answering Track Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,344.25,465.57,177.61,10.13;10,90.00,478.32,41.83,10.13">The Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-11-17">1999. 1999. November 17-19</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
