<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.84,62.18,400.36,15.49;1,213.50,85.70,82.24,7.60">TNO/UT at TREC-9: How different are Web documents? ¡</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.32,93.37,64.77,10.76"><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
							<email>kraaij@tpd.tno.nl</email>
						</author>
						<author>
							<persName coords="1,310.40,85.70,17.26,7.60"><forename type="first">¡</forename><surname>¢¡</surname></persName>
						</author>
						<author>
							<persName coords="1,318.00,93.37,80.97,10.76"><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
							<email>westerve@cs.utwente.nl</email>
						</author>
						<author>
							<persName coords="1,323.40,114.60,91.46,7.60"><forename type="first">¡</forename><surname>£¡</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">TNO-TPD</orgName>
								<address>
									<postBox>P.O. Box 155</postBox>
									<postCode>2600 AD</postCode>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CTIT</orgName>
								<orgName type="institution">University of Twente</orgName>
								<address>
									<postBox>P.O. Box 217</postBox>
									<postCode>7500 AE</postCode>
									<settlement>Enschede</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.84,62.18,400.36,15.49;1,213.50,85.70,82.24,7.60">TNO/UT at TREC-9: How different are Web documents? ¡</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8E3AB148406A7519077BF01D907D9783</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although at first sight, the web track might seem a copy of the ad hoc track, we discovered that some small adjustments had to be made to our systems to run the web evaluation. As we expected, the basic language model based IR model worked effectively on this data. Blind feedback methods however, seem less effective on web data. We also experimented with rescoring the documents based on several algorithms that exploit link information. These methods yielded no positive result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The basic idea for the web track run was to modify our ad-hoc system for the main web task and perform some experiments with the link structure information. We did not know to what scale we would have to re-engineer our systems to be able to deal with 10 giga bytes of data which is about 5 times larger than the ad-hoc collection. We applied the same IR model based on an interpolated unigram language model which had proven to be succesful on several data collections and tasks: Ad hoc, CLIR, SDR and filtering. The model will be presented in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Model</head><p>All runs were carried out with an information retrieval system based on a simple unigram language model. The basic idea is that documents can be represented by simple statistical language models. Now, if a query is more probable given a language model based on document ¤ ¦¥ , than given e.g. a language model based on document ¤ ¨ § , then we hypothesise that the document ¤ ¥ is more relevant to the query than document ¤ § . Thus the probability of generating a certain query given a document-based language model can serve as a score to rank documents with respect to relevance. This choice can be motivated by the fact that empirical studies by Singhal <ref type="bibr" coords="2,367.44,25.26,11.60,8.97" target="#b3">[4]</ref> have shown that there is usually a linear relationship between probability of relevance and document length.</p><p>The model was implemented in a vector product form supported by the TNO search engine. Our system was able to index the 10 Gigabyte dataset in roughly 20 hours on a SUN ultrasparc 300 Mhz. No re-engineering was necessary, except for the HTML entity conversion, which broke on several non-conforming documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Content only Experiments</head><p>We experimented with several variants for the estimator of the marginal © 7 ' in formula <ref type="bibr" coords="2,449.64,128.94,10.58,8.97" target="#b0">(1)</ref>. We compared an estimator based on the document frequency:</p><formula xml:id="formula_0" coords="2,274.70,157.10,265.28,14.76">© P 7 ' W3 df 7 YX a`( 3)</formula><p>with an estimator based on the collection frequency:</p><formula xml:id="formula_1" coords="2,246.80,202.30,293.18,32.80">© 7 ' Q3 S b U £8 ¥ tf 7 cU X ed b 7 98 ¥ S b U £8 ¥ tf 7 cU<label>(4)</label></formula><p>and an estimator based on the term frequency averaged over all documents:</p><formula xml:id="formula_2" coords="2,256.50,265.40,283.48,33.00">© 7 ' Q3 S b U £8 ¥ tf 7 fU (X dlen U '<label>(5)</label></formula><p>In these estimators, `is the number of documents and g the indexing vocabulary.</p><p>A second experiment dealt with score normalisation. Score normalisation is not necessary for the web task, but is relevant for other tasks like CLIR and topic tracking. We had found that dividing the RSV by the query length helps to normalize scores across topics. This makes sense because the RSV is composed of a sum of log terms. (cf. <ref type="bibr" coords="2,528.36,346.98,11.60,8.97" target="#b2">[3]</ref> for a description of the vector space implementation of the model, which is based on taking the log of the probability, thereby converting the product into a summation) However, when we choose a model which includes a document prior</p><formula xml:id="formula_3" coords="2,94.30,377.10,24.90,7.60">© P% &amp; ('</formula><p>, the RSV is not a sum of query term related addends anymore, because the document prior is a constant probability, independent of the query length, which is even added when the query has zero length. We assumed that we could correct for this problem by assuming that both the document prior and the query dependent score component (the first term) are independent sources of evidence, in that case we can add a weighting component h , which controls the ratio of the prior evidence component in the final RSV. </p><p>Experiments showed however, that the assumption that both sources of evidence are independent, is not true. The original model where the document prior is seen as an internal component of the model and where the sum component is not normalised separately showed the best performance. This leaves the RSV normalisation problem (which is not relevant for the web task) yet unsolved. We hypothesize that the document priors are especially helpful as an additional probabilistic knowledge source, when the system does not have a lot of information about the topic of interest (e.g. the query is short). For more informative queries, the influence of the a priori knowledge that longer documents tend to be more often significant is small, because this effect is implicitly coerced by the retrieval model. The longer the query, the lower the probability that a short document contains all query terms.</p><p>We tested several blind feedback methods on the TREC8 2 Gigabyte small web task. We did not find a consistent improvement, for title queries the performance was even hurt. We decided to refrain from feedback in the TREC9 web runs. We think the blind feedback was especially troubled by the presence of typos, which are abundant in web documents. These typos receive a high weight in most pseudo feedback strategies, because of there low document frequency. A more detailed analysis is required to study whether this is the only problem.</p><p>Table <ref type="table" coords="2,111.60,641.34,4.98,8.97" target="#tab_0">1</ref> gives the results of the content only runs. We have focussed on title only runs, because we feel these are most real-life and challenging. The first of the official runs (tnout9t2) is a title run based on the third (average tf) estimator with a lambda value of 0.01 to enhance coordination, a prior weight h of 0.5 while dividing the first term by the query length (according to formula (6)), the second official run (tnoutgf1) is a full run based on the first estimator using the standard model of (1) without document priors. In the full run terms from the title receive a triple weight, terms from the description run receive a double weight and terms from the narrative section a single weight. This choice was motivated by some post hoc experiments on prior collections.</p><p>We have done some additional experiments. First we modified our tokenizer to allow query terms with digits to enter the fuzzy matching process. This brought a small but insignificant improvement (only one topic changed).</p><p>We also re-tested the different estimators in combination with standard document priors and different lambda values. It turned out that the choice of a lambda value of 0.80-0.90 was best for all three estimators, with very small performance differences the table shows results for lambda=0.1. The second estimator, based on the collection frequencies scored best, but practically spoken, the three estimators work about as well.</p><p>We have made some additional plots to check whether the assumption that probability of relevance is linearly correlated with document length holds for a number of collections:  for the TREC9 main web collection the qrel file, but we took bins on a log scale. Subsequently, we computed</p><formula xml:id="formula_5" coords="5,168.70,328.90,371.28,28.36">© P% &amp; isRel # dlen &amp; bin &amp; (' Q3 © dlen &amp; bin &amp; # % &amp; isRel ' H © 0% &amp; isRel ' © dlen &amp; bin &amp; '<label>(7)</label></formula><p>The plots for the web collections seem quite comparable and distinct from the adhoc plots, which in turn are also quite comparable. Especially shorter Ad Hoc documents are relatively much more relevant than their web counterparts. This could be explained by the fact that shorter web documents are often just placeholders for links or pictures. We might be able to improve the performance of our runs by taking this fact into account while estimating document priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Exploitation of links</head><p>We used different link-based techniques to recalculate the scores for the top 1000 documents retrieved by a title-only, content-only run (tnout9t2). Although in last year's TREC, adding link information didn't seem to help, we hope that the higher density of links in this year's collection can improve the results. Below, we first discuss the different approaches and then compare the results to the original content-only run.</p><p>We used two different approaches. The first one is the well-known Kleinberg algorithm of hubs and authorities <ref type="bibr" coords="5,72.00,532.02,10.57,8.97" target="#b1">[2]</ref>. We took the top N documents with their in and out links and computed hubs and authoroties on that set. We then normalised the content only scores in the same way the scores are normalised in the kleinberg algorithm (equation 8). The normalised scores and kleinberg scores are then summed.</p><formula xml:id="formula_6" coords="5,274.30,580.60,265.68,28.00">¤ ' W3 v ¤ ' R T a ed v ¤ ' ¢' § (8)</formula><p>The second approach is based on co-citation <ref type="bibr" coords="5,268.08,621.18,11.62,8.97" target="#b4">[5]</ref> and bibliographic coupling <ref type="bibr" coords="5,393.36,621.18,10.57,8.97" target="#b0">[1]</ref>. The assumptions behind the use of these measures to adjust the document scores are the following. If the set of documents that document A refers to is similar to the set of documents that document B refers to, then document A and B are similar. If the set of documents that refer to A is similar to the set of documents that refer to B, then A and B are similar. First we analysed last year's results. We propagated the relevance judgements along the links and computed the following scores:</p><formula xml:id="formula_7" coords="6,215.20,60.90,324.78,30.50">f g Ch ¦ ¢ ¤ ' i3 R 7 7 ! j 7 ! 2&amp; d k 9 ml vn ¦o p Pg )' q g g Ch A ¤ '<label>(9)</label></formula><p>r s ut Pg Ch ¦ £ ¤ ' Q3 R 7 £v vw j 7 ! 2&amp; d k 9 ml n ¦o ¨ ap g )' q s xt g Ch A ¤ '</p><p>(10)</p><formula xml:id="formula_8" coords="6,209.10,157.40,330.94,78.80">y g t v £ ¤ ' Q3 R 7 7 ! j 7 ! 2&amp; d k z ml r s ut Pg Ch ¦ £ g )' q s ut Pg Ch { ¤ ' (11) | }g )~ s ¢ ¤ ' i3 R 7 ¢v vw j 7 ! 2&amp; d k z ml f Pg Ch £ g )' q g g Ch A ¤ '<label>(12)</label></formula><p>In table 2 the average scores are shown for the whole collection, the assessment pool and the relevant set. Relevant documents have higher cocitation and bibliographic coupling scores than an average (judged) document. We used this information to recalculate the scores of a topic only run in the following way. We took the top N retrieved documents and propagated their scores allong their in and outlinks calculating cocitation and bibcoupling scores analogously to the cocitation and bibliographic coupling relevancies in equations 11 and 12. We used the resulting cocitation and bibliographic coupling scores to weigh the original content-only scores. Due to some misunderstanding about the calculation of the scores, in the official content-link runs the content-only scores are reweighed by multiplying the content-only scores and the link scores (i.e. cocitation scores). However, the original content-only scores are composed of a sum of logarithms of different weights. In unofficial runs (with runtags ending in log), the link scores are properly combined with the content only scores. The results for the different runs can be seen in table <ref type="table" coords="6,152.52,459.42,4.98,8.97" target="#tab_2">3</ref> Adding link information decreases or at the best doesn't influence the average precision. When we take a closer look at the different link runs and compare them to the content only title run, we see that most runs hardly differ from it. The only run that differs a lot is tnout9t2lk50. In this run, the authority scores are added to the normalised In all the other runs, link information was only used to reweigh the content information. When we use kleinberg authority scores for reweighing (tnout2.klein50log), this doesn't change the results of the content run . Even though this year's collection is bigger and has more links than last year's collection, the use of link information does not seem to improve the retrieval results. One of the reasons for this might be that TREC topics are not suitable for using link information because they are too specific. This year's TREC topics on average have only 47.4 relevant documents, so the changes of being many links between them are rather small. Another reason is that there's a lot of garbage in the link information. The basic assumption behind these link based methods is that pages are topically related if they are linked to each other. Obviously, this isn't necessarily the case. Many links on the web refer to creators of the page, sponsors, friends or other pages without a topical relation to the source page of the link. Classifying links in advance into meaningful and meaningless links on the basis of for example the anchor text might help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To our surprise, the web task turned out to be more difficult than we expected. Firstly, web documents (even though the collection has been cleaned) contain a lot of trash, often in the form of of incorrect HTML. The HTML parsing component had to be adjusted to be able to handle this kind of material. Secondly, web documents contain a lot of misspellings. These are often very rare terms. When such terms occur in a pseudo feedback document, they will have a bad influence on the pseudo feedback process, because these terms will receive a high weight. Finally, the title only queries posed a problem. Some queries contained typos, involving digits (topic 487). Our engine simply discarded those terms. The tokenizer has to be updated as well to deal with years. Four-digit years were important query concepts in quite a few queries, they were discarded by our term extraction module. The content-only runs were finally run with some small variants of our standard IR model. Both the full and title runs perform well (33 resp 37 topics) above median, confirming the adequacy of the model. The runs which additionally analyzed link information in order to rescore the runs, were not able to improve on average precision. This confirms the general result of the TREC8 web runs. We hope to improve the link analysis in the future by looking at the anchor texts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,202.68,588.42,36.31,8.97;3,242.30,582.60,26.30,7.60;3,274.56,588.42,134.69,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 1:© 0% y# 0'for the TREC7 Ad Hoc collection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,198.00,619.98,36.31,8.97;4,237.70,614.30,26.20,7.60;4,269.88,619.98,144.17,8.97"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3:© P% y# 0' for the TREC8 small web collection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,78.00,24.06,313.96,115.17"><head>Table 1 :</head><label>1</label><figDesc>Content-only results</figDesc><table coords="3,78.00,24.06,313.96,93.45"><row><cell>runtag</cell><cell>official run</cell><cell>description</cell><cell>average precision</cell></row><row><cell>tnout9t2</cell><cell>yes</cell><cell>title run with 0.5 doc priors</cell><cell>0.1801</cell></row><row><cell>tnoutf1</cell><cell>yes</cell><cell>full run without doc priors</cell><cell>0.2178</cell></row><row><cell>df-estimator</cell><cell>no</cell><cell>title with doc priors</cell><cell>0.1871</cell></row><row><cell>df-estimator</cell><cell>no</cell><cell>title without doc priors</cell><cell>0.1465</cell></row><row><cell>cf-estimator</cell><cell>no</cell><cell>title with doc priors</cell><cell>0.1884</cell></row><row><cell>avtf-estimator</cell><cell>no</cell><cell>title with doc priors</cell><cell>0.1871</cell></row><row><cell>df-estimator</cell><cell>no</cell><cell>full with doc priors</cell><cell>0.2240</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,78.00,329.82,397.42,66.93"><head>Table 2 :</head><label>2</label><figDesc>Average indirect relevancy</figDesc><table coords="6,78.00,329.82,397.42,45.21"><row><cell></cell><cell></cell><cell>Inlinkrel</cell><cell>Outlinkrel</cell><cell>Cociterel</cell><cell>Bibcouplrel</cell></row><row><cell cols="5">Collection (WT2g) 0.00921076 0.012829736 0.004616373 0.00728053 0.006673368</cell></row><row><cell>Assessment pool</cell><cell cols="4">0.050987634 0.010024281 0.004668967 0.010140689 0.010697012</cell></row><row><cell>Relevant set</cell><cell>1</cell><cell cols="3">0.064735196 0.026876365 0.126311025 0.137629731</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,78.00,525.78,315.28,139.17"><head>Table 3 :</head><label>3</label><figDesc>Content-Link results content only scores. This means that the link information is regarded equally important as the content information.</figDesc><table coords="6,78.00,525.78,315.28,117.45"><row><cell>runtag</cell><cell>official run</cell><cell>description</cell><cell>average precision</cell></row><row><cell>tnout9t2</cell><cell>yes</cell><cell>title only content run</cell><cell>0.1801</cell></row><row><cell>tnout9t2lk50</cell><cell>yes</cell><cell>kleinberg on top 50</cell><cell>0.0488</cell></row><row><cell>tnout9t2lc10</cell><cell>yes</cell><cell>cocitation on top 10</cell><cell>0.1630</cell></row><row><cell>tnout9t2lc50</cell><cell>yes</cell><cell>cocitation on top 50</cell><cell>0.1337</cell></row><row><cell>tnout9t2.klein50log</cell><cell>no</cell><cell>kleinberg on top 50</cell><cell>0.1803</cell></row><row><cell>tnout9t2.coc10log</cell><cell>no</cell><cell>cocitation on top 10</cell><cell>0.1786</cell></row><row><cell>tnout9t2.coc50log</cell><cell>no</cell><cell>cocitation on top 50</cell><cell>0.1784</cell></row><row><cell>tnout9t2.bib10log</cell><cell>no</cell><cell>bibcoupling on top 10</cell><cell>0.1691</cell></row><row><cell>tnout9t2.bib50log</cell><cell>no</cell><cell>bibcoupling on top 50</cell><cell>0.1642</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,88.56,400.02,433.89,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,148.92,400.02,193.26,8.97">Bibliographic coupling between scientific papers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,349.80,400.02,100.98,8.97">American Documentation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="10" to="25" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.56,419.94,451.46,8.97;7,88.57,431.94,185.01,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,154.08,419.94,196.51,8.97">Authorative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,368.76,419.94,171.26,8.97;7,88.57,431.94,90.99,8.97">Proceedings of 9th ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>9th ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="668" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.57,451.86,451.44,8.97;7,88.57,463.86,451.89,8.97;7,88.57,475.74,22.42,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,266.04,451.86,273.96,8.97;7,88.57,463.86,31.83,8.97">Twenty-one at TREC-8: using language technology for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pohlmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,141.49,463.86,193.31,8.97">The Eighth Text Retrieval Conference (TREC-8)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.57,495.66,451.49,8.97;7,88.57,507.66,451.44,8.97;7,88.57,519.66,52.29,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,289.57,495.66,156.06,8.97">Pivoted document length normalization</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,465.24,495.66,74.81,8.97;7,88.57,507.66,422.55,8.97">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.57,539.58,451.41,8.97;7,88.57,551.46,176.25,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,135.61,539.58,385.37,8.97">Co-citation in the scientific literature: A new measure of the relationship between documents</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,533.29,539.58,6.69,8.97;7,88.57,551.46,95.52,8.97">J. American Soc. Info. Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="265" to="269" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
