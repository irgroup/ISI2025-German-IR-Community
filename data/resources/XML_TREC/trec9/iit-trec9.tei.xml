<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.64,86.88,318.71,12.99">IIT TREC-9 -Entity Based Feedback with Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.04,118.22,67.26,9.94"><forename type="first">A</forename><surname>Chowdhury</surname></persName>
						</author>
						<author>
							<persName coords="1,227.30,118.22,41.44,9.94"><forename type="first">S</forename><surname>Beitzel</surname></persName>
							<email>beitzel@ir.iit.edu</email>
						</author>
						<author>
							<persName coords="1,275.76,118.22,40.01,9.94"><forename type="first">E</forename><surname>Jensen</surname></persName>
							<email>jensen@ir.iit.edu</email>
						</author>
						<author>
							<persName coords="1,323.16,118.22,44.54,9.94"><forename type="first">M</forename><surname>Sai-Lee</surname></persName>
						</author>
						<author>
							<persName coords="1,374.64,118.22,55.58,9.94"><forename type="first">D</forename><surname>Grossman</surname></persName>
							<email>grossman@ir.iit.edu</email>
						</author>
						<author>
							<persName coords="1,438.25,118.22,42.71,9.94"><forename type="first">O</forename><surname>Frieder</surname></persName>
							<email>frieder@ir.iit.edu</email>
						</author>
						<author>
							<persName coords="1,274.56,206.66,62.88,9.94"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mccabe</surname></persName>
						</author>
						<author>
							<persName coords="1,282.12,257.30,47.63,9.94"><forename type="first">D</forename><surname>Holmes</surname></persName>
							<email>david.holmes@washingtondc.ncr.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Illinois Institute of Technology Chicago</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory</orgName>
								<address>
									<postCode>60616</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NCR Corporation Rockville</orgName>
								<address>
									<settlement>Maryland</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.64,86.88,318.71,12.99">IIT TREC-9 -Entity Based Feedback with Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7AC38A2FEEE34D79CDA95A4CD823E846</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For TREC-9, we focused on effectiveness in the web track. The key techniques we employed were information fusion, entity-based relevance feedback, Wordnet-based query parsing and a user interface designed to assist with web-based manual queries. Our initial results are positive. For the manual task, forty of fifty queries are over the median. In the adhoc, title-only task, thirty-four of fifty queries are over the median.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For TREC-9 we focused on the web track, especially on in improving our effectiveness on large volumes of data. The past few TREC's we have focussed on scalability by treating the IR problem as an application of a parallel, relational database [Grossman97]. To focus on effectiveness this year, we built a new Java-based IR system called AIRE (Advanced Information Retrieval Engine). AIRE is designed to be a flexible, modular IR engine capable of state-of-the art retrieval techniques and easily modified to incorporate new proven or experimental techniques.</p><p>The keys to our effectiveness included the following approaches: Ø Fusion using probabilistic (both traditional and models involving self-relevance <ref type="bibr" coords="1,108.00,610.46,63.01,9.94" target="#b9">[Robertson98,</ref><ref type="bibr" coords="1,180.69,610.46,38.86,9.94" target="#b5">Kwok96]</ref>, and vector space model with pivoted document length normalization <ref type="bibr" coords="1,172.07,623.06,50.38,9.94" target="#b10">[Singhal96]</ref>.</p><p>Ø Entity-based relevance feedback <ref type="bibr" coords="1,254.15,648.38,53.35,9.94">[McCabe00]</ref>.</p><p>Ø Improved parsing techniques of both the query and the documents.</p><p>Ø New user interface for manual queries.</p><p>Section 2 describes each of these approaches. Our initial results are positive. For the manual task, forty of fifty queries are over the median. In the adhoc, title-only task, thirty-four queries of fifty are over the median. More details on these results are given in Section 3. Section 4 describes directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach to TREC-9</head><p>Our basic approach was to focus solely on the manual and adhoc parts of the web track. Given that our previous work focused on scalability, this year was a significant challenge as we started to really focus on effectiveness. We calibrated our techniques using the TREC-8 adhoc and web document collections. Early in the TREC-9 year, our best average precision was around 0.23 (this was about the average for effectiveness at TREC-8) and after applying a variety of fusion techniques we improved to roughly 0.28. At this point, we incorporated collection enrichment and then re-calibrated our fusion techniques. This left us at around 0.30. Finally, we improved our parsing and stemming algorithms using a combination of our own modified Porter stemmer and the U-Mass conflation classes <ref type="bibr" coords="2,247.66,295.22,31.09,9.94" target="#b14">[Xu96,</ref><ref type="bibr" coords="2,282.23,295.22,27.37,9.94" target="#b11">Xu98,</ref><ref type="bibr" coords="2,313.09,295.22,36.26,9.94" target="#b12">Pickens]</ref>. At this point we were at around .31 for the adhoc (disks 4 and 5) and .36 for the TREC-8 small web track. At about this time, the TREC-9 queries came out and we ran our best calibrated system against the TREC-9 collection. Details of our fusion techniques are given in Section 2.1, details of our parsing techniques are given in Section 2.2.</p><p>Additionally, we experimented with a means by which we could integrate information extraction with information retrieval. The idea of this technique is to use entities identified by an information extractor (we used SRA's) system and then only add entities in the feedback process. We submitted one adhoc run without the use of entities: iit00td (title + description), and iit00t (title) and one run with the use of entities iit00tde (title + description + entities). Such feedback with extraction is described in Section 2.3.</p><p>For the manual track we built a new user interface to facilitate manual query processing. Details of this user interface are given in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion</head><p>Prior work in fusion combined results from disparate retrieval systems <ref type="bibr" coords="2,403.94,513.74,34.70,9.94" target="#b3">[Fox94,</ref><ref type="bibr" coords="2,441.33,513.74,44.04,9.94" target="#b1">Bartell94,</ref><ref type="bibr" coords="2,488.06,513.74,29.21,9.94" target="#b7">Lee97]</ref>.</p><p>Our approach was to provide fusion via one common system. Using a common parser, stoplist, inverted index, etc, we implemented a variety of retrieval algorithms within our framework. Thus, we avoid confusing fusion improvements with simple parsing or other system differences. We conducted numerous calibrations using the vector space model [Singhal96], Robertson's probabilistic retrieval strategy <ref type="bibr" coords="2,225.70,576.98,61.97,9.94" target="#b9">[Robertson98]</ref>, and a modified vector space retrieval strategy. The following equations describe those used as the foundation of our retrieval strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robertson's Retrieval Status Value</head><formula xml:id="formula_0" coords="3,184.20,87.26,244.07,60.86">(RSV) ( ) ( ) ( ) ∑ ∈         + - + + + + + = Q T dl avdl dl avdl Q k qtf k qtf k tf K tf k w RSV 2 3 3 1 1 1</formula><p>where tf = frequency of occurrences of the term in the document qtf = frequency of occurrences of the term in the query dl = document length avdl = average document length k n are parameters set based on the nature of the queries and the collection.</p><p>( )</p><formula xml:id="formula_1" coords="3,192.12,262.92,224.66,35.44">qtf tf avdl dl tf n n N ∑         + +       + + - ) / * 75 (. 3 . 2 . 2 5 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">. log</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our implementation with constants as specified by Robertson</head><p>Singhal's Similarity Coefficient</p><formula xml:id="formula_2" coords="3,227.04,364.22,158.23,48.91">( ) ( ) ( ) ( ) | | 0 . 1 , 1 i t j ij qj i d s p s d q D Q SC + - = ∑ =</formula><p>where |d i | is the number of elements in the vector, or the number of distinct terms in the document, s is the slope which Singhal calculated for a variety of test corpuses (mostly TREC subsets) and found that 0.20 works well across most collections. The pivot, p, is the point, or document length at which the probability of relevance equals the probability of retrieval. This is estimated to be the average document length of the collection.</p><p>In addition to fusion of various retrieval strategies, AIRE permits the fusion of different query representations. Also, each input run has a scalar weight that indicates the relative importance of the run.</p><p>For our first pass retrieval, we focused on finding one retrieval strategy that did better for high recall and another strategy that performed well for high precision (at 30 documents). Our hypothesis was that the combination would perform better in terms of average precision than either input run. Our initial results showed a slightly modified Vector Space did well for high recall and that Robertson's probabilistic model did the best at for precision at 30. The combination we ultimately settled on was: Modified Vector Space title-only (weighted at 1.0), Robertson title-only (weighted at 0.1), Robertson description when applicable (weighted at 0.7). This fusion combination effectively emphasized title terms as most important while still benefiting from high-recall description terms.</p><p>For our second pass, we selected the top fifteen feedback terms from the top ten documents using the fused pass one run. In order to select the top fifteen terms, we first weighted each term found in the top documents using Robertson's term weighting. We then calculated the ultimate rank of the candidate term using Rocchio's relevance feedback formula <ref type="bibr" coords="4,390.69,87.02,53.25,9.94" target="#b13">[Rocchio71]</ref>. In addition to finding the top fifteen terms and phrases, a check is made to a list of nouns obtained from Wordnet to filter candidate terms and phrases so that only nouns are selected. The new query terms are then used in pass two as one of the query representations for a fusion input run. We found a scalar weight of .5 and the Roberston retrieval strategy to work well with this query representation.</p><p>We also used a collection enrichment representation for a pass two fusion input run. This query run consisted of terms selected from a pass one retrieval executed against the TREC disks 4 and 5. The fifteen top ranked terms are then used as a query against the search collection (10GB web) with Robertson retrieval strategy and a weight of .5 (same as relevance feedback terms).</p><p>Finally, two additional runs are included in the pass two fusion. The original title terms are used with modified vector space retrieval weighted at 1.0. The original description terms are used with Robertson weighted at 1.0.</p><p>Interestingly enough, for our final TREC submission, we did not normalize the fusion runs. Thus our scalar weights represent actual multipliers rather than relative importance in pass two. This choice was made based on prior calibrations.</p><p>In summary, our iit00t, iit00td and iit00tde submissions were fusions of the following four different representations of a single query: Ø Title words only Ø Description words only (this was only used for runs involving the description) Ø Relevance feedback terms obtained from running the title (and description when applicable) Ø Relevance feedback terms (and entities for tde) obtained from a collection enrichment run (TREC disks 4 and 5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Extraction</head><p>In previous years, our manual runs did well when the user added person and place names to queries. For example Kuhn Sa was very helpful on the query regarding drug triangle. This year, we propose entity-based feedback as a method to automatically select such person names, as well as place names and organization names and add them to the query. The technique required modifications to the inverted index in order to include term-type (term, phrase, person, location, etc.). Secondly, the document preprocessing was modified to include SRA's Name Tagger to identify entities within the text. Then, the original query of only terms and phrases was run for pass one. Pass two selects entities from the top documents returned and adds these terms to the query as in relevance feedback.</p><p>For our calibrations, we isolated the entities and added only person names, only locations, and only organizations to the query. In order to understand the real impact of each entity, we ran many calibrations where only one new word was added to the query. We found that good improvement is possible when we adding only a single organization, a single location, or a single person name. Details of these calibrations may be found in <ref type="bibr" coords="4,364.78,659.78,53.34,9.94">[McCabe00]</ref>. For example, query Ireland Peace Talks added the organization Sinn Fein, the person Jerry Adams, and the location Northern Ireland. Each of these improved the query effectiveness by over 100%. While more queries improved than degraded with each entity type, several queries degraded badly. Names that were associated with more than the query topic were harmful. For example, the query Estonian Economics selected the Estonian Prime Minister Mart Laar as the name to add. This degraded performance because the prime minister is in many documents having nothing to do with economics. In addition, ambiguous names were harmful. It turns out there are many individuals with the name Stirling. So that addition to the query Stirling Engine brought back documents about a California senator, a minister, etc.</p><p>For TREC-9 we selected entities from our collection enrichment corpus rather than our search corpus. That is simply because we already had our collection enrichment corpus (TREC disks 5 and 6) tagged and indexed with entities, while we had not yet tagged the web 10GB. In order to reduce the chances of a bad entity being selected, we added entities into the mix of potential feedback terms and only selected those that ranked in the top 15 terms or phrases. Our entitybased feedback run performed about the same as our title plus description run. This is because many queries did not select entities and of those that did some improved and some degraded, mostly canceling out the overall effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parsing</head><p>We improved our parsing algorithms in TREC-9. Previously, we did not use any stemming. This year, we indexed terms with a modified Porter stemmer that does both prefix and suffix stemming. In addition, we use equivalence classes based on term co-occurrence to further restrict the stemming <ref type="bibr" coords="5,152.10,343.58,29.22,9.94" target="#b14">[Xu96,</ref><ref type="bibr" coords="5,181.32,343.58,38.96,9.94" target="#b12">Pickens,</ref><ref type="bibr" coords="5,222.96,343.58,25.91,9.94" target="#b11">Xu98]</ref>. If the term is found in the conflation file (a set of terms with their equivalence classes), we use the first occurrence of the term as the root form. If the term is not found, the modified porter stemmer is used. This technique corrects for over-stemming of common words. For example the standard Porter stemmer would conflate policy and police while these equivalence classes would not.</p><p>In addition to single terms, our parser indexes standard statistical two-term phrases. Like numerous groups over the years, a sliding two-term window is used to detect these phrases. Any span punctuation or stop term prevents a phrase. We also eliminate phrases that do not occur more than 25 times.</p><p>In order to update our parser to accommodate web-type queries such as misspelled and misspaced terms, we incorporated a "find a real query term" algorithm using Wordnet. Our algorithm finds the longest common sub-string match in the query that is also a noun in Wordnet.</p><p>If the initial query found no documents, we use this algorithm as an automatic best guess approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual Query Processing</head><p>Our previous years at TREC have shown that a user who is given the ability to add related terms to a "concept" for a query is able to improve effectiveness. Our query expert now has six years of experience with TREC queries and is quite comfortable with defining terms for a query. About 5-10 minutes are spent on each query in which two different concepts are defined for "inclusion" into the query and one is defined for "exclusion". The ultimate query may be expressed such that if terms in the set C1 {c 11 , c 12 , …., c 1n } and the set C2 {c 21 , c 22 , …, c 2n ) are included and terms in the set X {x 1 , x 2 , …, x n } are excluded, the following Boolean processing is done on the query: ((c 11 OR c 12 OR …., OR c 1n ) AND (c 21 OR c 22 OR ….OR c 2n )) NOT (X 1 or X 2 or X n ). Additionally, for other related terms that are not used to filter a document a scoring concept is used. For these terms S {s 1 , s 2 , …, s n }, only the similarity measure is affected -these terms are not used to filter the document. Once the Boolean filters are incorporated, standard tf-idf VSM is employed to rank documents.</p><p>A Java servlet is used to provide quick feedback to the user. For each initial request, the user quickly views documents obtained in response to the request. Additionally, relevance feedback terms and phrases are suggested. Overall, our test user was quite pleased with the new user interface (only command line SQL processing has been available in previous years).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>We describe our adhoc results first, then our manual results, and finally we give some initial failure analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adhoc</head><p>Our title-only run was called iit00t and our title with description run was named iit00td. The run which used named entities as part of the collection enrichment was entitled iit00tde. The table below gives a summary of our results. The columns indicate the average precision for the median of all groups, IIT's average precision, the number of queries at or above the median, the number of queries below the median, the number of queries that gave the best results, and the number of queries that gave the worst results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual</head><p>For the manual query track, we had promising results. With all but seven queries over the median and twenty-five queries listed as achieving the highest average precision, we are pleased with this run. Unfortunately, one query was found to be the worst. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure Analysis</head><p>In failure analysis, we review poor performing queries and analyze the cause of failure. We started some failure analysis for TREC-9. The manual query with the worst average precision (0.000) was topic 485 which simply contained the terms "gps clock". For this query, there were only two relevant documents and we did not find either of them in our top 100. The reason is that we added numerous synonyms for gps and for clock and they overshadowed the basic phrase "gps clock." Worse, our adhoc system stemmed "gps clock" to "gp clock." Once we stemmed "gps" to "gp" we found documents about "general permit" (Document wtx082-b37-24) and grand prix, hockey statistics for games played, etc. A simple rule that precluded stemming three character terms would have improved this run tremendously. It is not clear how we would know that our manual run could be improved, but one approach might be to simply run the manual query and fuse it with the entire original query.</p><p>For topic 495, Where can I find information on the decade of the 1920's?, our user tried to think of events in the 1920's that would be of interest (e.g.; Charles Lindbergh, Calvin Coolidge, etc.). Unfortunately, he missed many useful events in the 1920's and missed some relevant documents.</p><p>In addition, our analysis indicates that our use of a scoring concept in the manual runs hurt us for some queries. It may well be helpful to use fusion to combine a query run with the scoring concept and without the scoring concept so as to ensure that high scoring documents without the scoring concept are included in the final result set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary and Future Work</head><p>Overall, we are pleased with our work on effectiveness this year. We plan to spend more time on failure analysis. Additionally, more cleanup of our parser is needed. More importantly, the potential of entity-based relevance feedback needs more research.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,95.88,333.00,87.71,12.99" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,143.02,364.34,378.97,9.94;7,90.00,376.94,431.99,9.94;7,90.00,389.54,360.36,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,361.45,364.34,160.55,9.94;7,90.00,376.94,105.98,9.94">Automatic combination of multiple ranked retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Bartell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Belew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,204.24,376.94,317.75,9.94;7,90.00,389.54,328.42,9.94">SIGIR &apos;94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,134.76,414.86,387.23,9.94;7,90.00,427.58,432.00,9.94;7,90.00,440.18,215.40,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,252.83,414.86,252.31,9.94">Corpus-specific stemming using word form co-occurence</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,90.00,427.58,427.66,9.94">Proceedings for the Fourth Annual Symposium on Document Analysis and Information Retrieval</title>
		<meeting>for the Fourth Annual Symposium on Document Analysis and Information Retrieval<address><addrLine>Las Vegas, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-04">April 1995</date>
			<biblScope unit="page" from="147" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,129.36,465.50,392.64,9.94;7,90.00,478.10,432.00,9.94;7,90.00,490.82,120.24,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,232.96,465.50,153.08,9.94">Combination of Multiple Searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,394.44,465.50,127.56,9.94;7,90.00,478.10,432.00,9.94;7,90.00,490.82,50.51,9.94">Proceedings of the 2nd Text Retrieval Conference (TREC2),National Institute of Standards and Technology Special Publication</title>
		<meeting>the 2nd Text Retrieval Conference (TREC2),National Institute of Standards and Technology Special Publication</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="500" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,137.22,516.14,384.59,9.94;7,90.00,528.74,431.88,9.94;7,90.00,541.34,24.84,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,381.72,516.14,140.09,9.94;7,90.00,528.74,127.35,9.94">Integrating Structured Data and Text: A Relational Approach</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,228.96,528.74,251.02,9.94">Journal of the American Society for Information Science</title>
		<imprint>
			<date type="published" when="1997-01">January 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,139.20,566.66,382.79,9.94;7,90.00,579.38,432.00,9.94;7,90.00,591.98,230.28,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,230.18,566.66,275.11,9.94">A new method of weighting query terms for ad-hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,90.00,579.38,432.00,9.94;7,90.00,591.98,167.26,9.94">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="187" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,161.03,617.30,360.96,9.94;7,90.00,629.90,292.08,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,236.04,617.30,285.96,9.94;7,90.00,629.90,236.09,9.94">Improving Information Retrieval Effectiveness with Databases, Fusion and Entity-Based Feedback. GMU PhD thesis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Mccabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-08">Aug.2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,129.35,655.22,392.65,9.94;7,90.00,667.82,432.00,9.94;7,90.00,680.54,126.72,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,179.52,655.22,194.37,9.94">Analysis of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,382.92,655.22,139.08,9.94;7,90.00,667.82,432.00,9.94;7,90.00,680.54,94.78,9.94">SIGIR &apos;97: Proceedings of the Twentieth Annual InternationalACM-SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,705.86,361.09,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,233.04,705.86,144.10,9.94">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,384.12,705.86,37.28,9.94">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,157.67,87.02,364.32,9.94;8,90.00,99.74,431.74,9.94;8,90.00,112.34,39.48,9.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,352.84,87.02,169.15,9.94;8,90.00,99.74,134.66,9.94">Okapi at TREC-7: Automatic ad hoc, filtering, VLC and Iinteractive</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,247.45,99.74,240.79,9.94">Proceedings of the Seventh Text Retrieval Conference</title>
		<meeting>the Seventh Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,145.38,137.66,376.62,9.94;8,90.00,150.26,432.00,9.94;8,90.00,162.98,199.20,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,323.06,137.66,182.01,9.94">Pivoted Document Length Normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,90.00,150.26,432.00,9.94;8,90.00,162.98,167.26,9.94">Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,127.13,188.30,394.87,9.94;8,90.00,200.90,37.79,9.94;8,157.17,200.90,364.82,9.94;8,90.00,213.50,107.88,9.94" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,274.67,188.30,247.32,9.94;8,90.00,200.90,33.59,9.94">Corpus-based stemming using co-occurrence of word variants</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno>TR96-67</idno>
		<imprint/>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Massachusetts/Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,134.51,238.82,299.76,9.94" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="8,214.41,238.82,214.65,9.94">Stemming and Cooccurrence on a Larger Corpus</title>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Pickens</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,152.99,264.14,369.00,9.94;8,90.00,276.86,431.99,9.94;8,90.00,289.46,73.56,9.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,215.13,264.14,219.02,9.94">Relevance Feedback in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,446.04,264.14,75.95,9.94;8,90.00,276.86,224.00,9.94">Smart System -Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,125.77,314.78,396.23,9.94;8,90.00,327.38,432.00,9.94;8,90.00,340.10,244.80,9.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,220.49,314.78,280.44,9.94">Query Expansion Using Local and Global Document Analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,90.00,327.38,432.00,9.94;8,90.00,340.10,167.26,9.94">Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
