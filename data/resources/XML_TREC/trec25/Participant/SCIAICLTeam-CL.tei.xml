<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.08,77.17,335.48,17.49;1,226.71,100.21,158.26,17.49">Siena&apos;s Clinical Decision Assistant with Machine Learning</title>
				<funder ref="#_SkZE6Kf">
					<orgName type="full">National Science Foundation&apos;s Research Experience for Undergraduates Grant</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.08,121.06,64.19,10.54"><forename type="first">Brendan</forename><surname>Kish</surname></persName>
							<email>bt13kish@siena.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.40,121.06,69.70,10.54"><forename type="first">Thomas</forename><surname>Walsh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.72,121.06,76.14,10.54"><forename type="first">Katherine</forename><surname>Small</surname></persName>
							<email>smallk1@hawkmail.newpaltz.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.02,121.06,69.77,10.54"><forename type="first">Steven</forename><surname>Gassert</surname></persName>
							<email>gasserts1@hawkmail.newpaltz.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.72,134.74,55.49,10.54"><forename type="first">Kylie</forename><surname>Small</surname></persName>
							<email>ssmall@siena.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.38,134.74,100.65,10.54"><forename type="first">Sharon</forename><surname>Gower Small</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.08,77.17,335.48,17.49;1,226.71,100.21,158.26,17.49">Siena&apos;s Clinical Decision Assistant with Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">183F320F0CB521164FBF80AC3BA89E3D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses Siena's Clinical Decision Assistant's (SCDA) system and its participation in the Text Retrieval Conference (TREC) Clinical Decision Support Track (CDST) of 2016. The overall goal of this track is to link medical cases to information that is pertinent to patient care. Participants were given a set of thirty topics in the form of medical case narratives and a snapshot of 1.25 million articles from PubMed Central (PMC). This snapshot was taken on March 28, 2016. New this year is that actual electronic health records (EHR) were used instead of synthetic cases. Admission notes from the MIMIC-III database were used to generate the topics. TREC describes the EHR notes as something that "describes a patient's chief complaint, relevant medical history, and any other information obtained during the first few hours of a patient's hospital stay, such as lab work." Each topic consists of three fields. There is a new field this year, the admission notes, which is the actual admission's data generated by the clinicians (mostly physicians, including residents, and nurses). The other two fields continue from last year: the description field, which is a layman-terms account of the patient visit, and a summary field , which is typically a one or two sentence summary of the main points of the visit. The thirty topics were annotated in three major subsets: diagnosis, test and treatment, with ten of each type. SCDA used several methods to attempt to improve the accuracy of medical cases retrieved. SCDA used the metathesaurus Unified Medical Language System (UMLS) that was implemented using MetaMap (NIH, 2013), machine learning and query and document framing (Small and Stzalkowski, 2004). SCDA also used Lucene for initial document indexing and retrieval. The track received a total of 115 runs from 26 different groups. We submitted two notes runs where our highest P(10) run was 0.16 and three runs where we used just the summary field and our highest P(10) was 0.2767. The average P(10) from CDST TREC 2015 Task A was 0.33, with a low of .0867 and a high of 0.4733. Our best Task A run last year had a P(10) of 0.3767. The work described here was performed by a team of undergraduate researchers working together for just ten weeks during the summer of 2016.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Clinical Decision Support Track <ref type="bibr" coords="2,279.95,102.58,112.20,10.54" target="#b8">(Simpson et al., 2014)</ref> is a program in the Text Retrieval Conference (TREC) <ref type="bibr" coords="2,237.31,116.50,81.97,10.54" target="#b10">(Voorhees, 2007)</ref>. TREC is a program co-sponsored by the National Institute of Standards and Technology (NIST) and the U.S. Department of Defense and it focuses on supporting research in information retrieval and extraction, and to increase availability of appropriate evaluation techniques. The Clinical Decision Support Track was run for the third time in 2016. Teams were allowed to submit 5 runs, where each run was required to process only one of the 3 fields of the topic. A maximum of three of these were allowed to use a non-note field, that is the description or summary field.</p><p>The highest ranked articles for each topic submitted by the participants were pooled and judged by medical librarians and physicians trained in medical informatics. In particular, the judgment sets were created using two strata: all documents retrieved in ranks 1-20 by any run and a union with a 20% sample of documents not retrieved in the first set that were retrieved in ranks 21-100 by some run. Assessors were instructed to judge articles as "definitely relevant" for answering questions of the specified type about the given case report, "definitely not relevant," or "possibly relevant." The latter judgment may be used if an article is not immediately informative on its own, but the assessor believes it may be relevant in the context of a broader literature review. The teams searched for relevant documents using medical patient case notes. Thirty different case studies subdivided into three types: diagnosis, test and treatment, were used to search the corpus of related documents. It was up to the team's discretion to determine the relevant information contained within these case notes and create a search program that utilized this medical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TREC 2016 Literature Review</head><p>While designing the experimental procedure for this year's clinical support track the team reviewed a significant amount of literature from the 2014 and 2015 tracks. The University of California, Los Angeles (UCLA) implemented the use of a manual run <ref type="bibr" coords="2,89.88,523.78,155.41,10.54" target="#b4">(Garcia-Gathright, et al., 2014)</ref>. Their manual run utilized domain experts for query expansion. Our work utilized domain experts to annotate last year's queries to improve the performance of framing for our automatic runs. Similarly to UCLA, we also utilized MetaMap, UMLS and Lucene <ref type="bibr" coords="2,239.28,565.06,122.80,10.54" target="#b7">(McCandless et al., 2010)</ref>. MetaMap is used to both relate biomedical text to the UMLS Metathesaurus and to flag Metathesaurus concepts that are present within biomedical texts. Lucene is a full text search engine library that is composed entirely in Java and is used to build the initial indices on the document corpus.</p><p>San Francisco State University <ref type="bibr" coords="2,252.32,634.18,116.65,10.54" target="#b2">(Bhandari et al., 2014)</ref> also used MetaMap but they translated their case reports into a list of structured medical concepts. Instead of using this method, we utilized our framing technique to add structure to the abstract of each case report to automatically score the retrieved documents relative to our query.</p><p>From the 2015 track we reviewed what Wayne State University did for their system. They were one of the top performing groups from the 2015 track. One concept that we used from their system was their idea of weighting certain query concepts. We implemented this by weighting our symptoms and passing those to Lucene as queries. They based their weighting on semantics or statistics <ref type="bibr" coords="3,358.68,130.18,158.04,10.54">(Balaneshin-kordon et al., 2015)</ref>. We used machine learning in order to decide on the weights that we used for our symptom queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The SCDA System Main Components</head><p>The main focus of the 2016 SCDA system was to improve the framing component from 2015's system to make it more accurate and useful. This meant utilizing machine learning on the 2015 data to pinpoint areas in which we could make improvements to our Lucene queries and thus provide a better data set for framing. We also used machine learning on the to discover the optimal set of frame attribute weights. The remainder of this paper will discuss the modules of our SCDA system in detail as well as the results of our NIST evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lucene Baselines</head><p>In order to run the initial retrieval on the corpus of documents, Apache Lucene 4.0.0 was utilized to create an index. Lucene is an open source search engine, written in Java, designed to function as a text search engine library.</p><p>Lucene was also used to generate the baseline run of our system by indexing the entire document and using the summary field of each topic to query our index. In another run we used the note field as the query, first automatically removing various characters that would make the Lucene query parser crash in the example note that was given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Weighted Lucene run</head><p>Once we learned the best weights of symptoms found in the 2015 track using logistic regression, we incorporated these weights in another run by altering our Lucene search at the time of the query. Lucene supports the boosting of terms in a query and we used this feature in two of our runs. We did this by scanning the topic summary for the existence of symptoms using UMLS symptom finder. We then boosted these symptom terms by their corresponding weight. Something worth noting is that Lucene's boosting technique also supports multiple term symptoms. If we found a symptom in our query that was contained within a larger symptom, only the larger phrase was ranked. For example "stomach pain following ingestion" was ranked rather than just ranking "stomach pain" if both were found within the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Framing Component</head><p>We expanded SCDA's Framing component using the 2015 model as a base, adding new attributes and altering the method for scoring. Last year's frame attributes included: age, gender, time, and symptoms. We expanded these by adding: country, topic-category, and key symptom. In Figures <ref type="figure" coords="4,215.92,88.66,5.33,10.54" target="#fig_0">1</ref><ref type="figure" coords="4,221.25,88.66,5.33,10.54">2</ref><ref type="figure" coords="4,226.58,88.66,5.33,10.54">3</ref>below we show a sample query frame for topic #20 where our P(10) = 1.0, as well as two data frames, one with a high score and one with a low score.</p><p>Query Frame:  Figure <ref type="figure" coords="5,117.67,668.16,3.48,7.80">3</ref>: Topic #20 frame scored low and removed from our top ten by our system with its corresponding text passage from a document judged to be not relevant by NIST assessors</p><p>In order to create frames from queries and passages of text, the text was taken through a number of different steps. First, MetaMap was used on the text to generate a list of negated concepts. For example, upon processing the phrase "cardiac arrest was ruled out", the function would add to the negated list any concept triggered in MetaMap for the frame "cardiac arrest". Later, any concept in the candidate target concept list that matched a concept negated in the same phrase was removed. The text was further automatically modified to replace potentially problematic phrases, especially those that would cause problems for the parser (for example, the Latinate medical terminology "status post" was replaced with "after") based on a dictionary we generated from 2014 analysis.</p><p>The text was then run through the Stanford Parser, in order to detect semantic roles and relationships. The parser's output was stored as a set of hierarchical clauses. This clausal hierarchy was searched for words that triggered concepts using MetaMap. Using the typology of "semantic types" employed by MetaMap to categorize triggered concepts. If trigger concepts were found with one of eight designated types, the relevant concept was added to the symptom list variable for the frame of the larger given area of text. For example, the sentence "64-year-old woman with uncontrolled diabetes, now with an oozing, painful skin lesion on her left lower leg" would have, among its many triggered concept referents from Metamap's database, a concept referent for skin lesions, likely classed under the semantic class [anab] (Anatomical Abnormalities). Since [anab] is one of the designated semantic types for denoting symptoms, the noun clause containing it, "oozing, painful skin lesion" is added to the symptoms list.</p><p>Referring to the temporality typology suggested by the medical professionals employed by the UCLA team in 2014, our frame's time attribute functions to classify conditions into classes of "acute", "progressive" and "chronic". The text of each triggered symptom clause was searched for temporal wording describing the symptom, and if it was found, the appropriate time class was saved to the frame's time attribute.</p><p>The country attribute was determined by searching the document or topic for any match with the Java system locales. If no match was found, the attribute is set to "United States" as a default. The topic category attribute was filled using a process similar to finding the time attribute. The entire document was searched for wording relating to one of the three topic categories: diagnosis, test, or treatment. When this wording was found, the frame would be assigned the related category. The highest ranked symptom (according to the symptoms list derived from machine learning) from among the query frame's symptoms becomes the key symptom attribute. If the document frame's symptom list contains a match to the key symptom, the key symptom match is set to true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Frame Scorer</head><p>After the Framing process was complete, SCDA had to rank each frame created by a document passage in order of its relevance to the query frame. The 2015 scoring algorithm simply looked for equality of the contents of each frame attribute. The total score of the frame was then calculated as the average of the scores from each individual frame attribute. In the 2016 scoring algorithm, the total score of the frame depends on which individual frame attributes are matches. For example, a match in the Key Symptom attribute is weighted more heavily than a match in the Gender attribute. The individual weight for each attribute was determined using Machine Learning as described next.</p><p>In our error analysis of the 2015 results, we discovered that we could improve the way we were previously determining whether two given symptoms from the query frame and a document frame were a match. MetaMap generates a list of content phrases, which are then checked against the topics symptoms to determine if the symptoms are a match. In 2015, symptoms were scored as a match if the content phrase was equal to a symptom from the query frame. In our error analysis we determined that symptoms such as "vomit" and "vomiting" were erroneously considered to not match. In 2016, we compared the two strings using Apache codec implementation of the Double Metaphone algorithm. The Double Metaphone algorithm improves upon the original metaphone algorithm which uses information variations and inconsistencies in English language and does a better job of matching words and phrases that sound familiar. After the double metaphone comparison has taken place, we then determine the longest common substring found between the two strings. If the substring length is larger than half of the smaller of the two input strings and the result of the Double Metaphone comparison is zero then the Frame Scorer will score the two symptoms as a match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example of 2015 SCDA Scoring algorithm:</head><p>Query After several rounds of analyzing results using the 2015 data we made another modification to our scoring algorithm. This change to our scoring algorithm lies in the way we treated frames when certain data types were not populated. For example, in the 2015 version of our scoring algorithm, when the query frame detected the gender of the patient, and the document we were scoring it against did not mention a gender (or our frame builder failed to locate it), we would not include that attribute in the calculation of the final overall score. In the 2016 version we instead assigned a score of 0 for that type.</p><p>Example of 2016 SCDA Revised Scoring algorithm: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Machine Learning</head><p>SCDA 2016 utilized WEKA for our machine learning. We tested out many different variables and algorithms to try and see what we could utilize in order to improve our accuracy. We completed three different experiments using the output from framing.</p><p>Experiment one contained the attributes; docID, genderMatch, ageMatch, timeMatch, diagnosisMatch, categoryMatch, countryMatch, symptomMatches, and topic number.</p><p>Experiment two contained the same attributes as one, with a substitution of symptomScore in place of symptomMatch. Experiment three was much more simplified and contained the attributes; docID, genderMatch, ageMatch, timeMatch, symptomMatches, and topic number. We wanted to see if we could find a pattern between any of these attributes and relevant and non-relevant documents. We did this by running framing on the 2015 topics and documents and using those attributes from the frames in the machine learning to see how WEKA used the attributes to predict relevant and non relevant documents. There were some patterns that we saw. For example, genderMatch, ageMatch, and timeMatch were consistently weighted the highest. Though utilizing the results of these experiments did not realize a significant improvement in our scores when we applied these weights to our frame scoring algorithm, so we had to look for a different approach.</p><p>We thought to try and just use the symptoms to help us make our frame scorer more precise. We decided to take every symptom that was identified during framing and use them in our machine learning. Each symptom was given a weight by WEKA based on how often it appeared in relevant versus non-relevant documents. After getting this weighted list we ordered it based on the weight it was given for relevant documents. The higher the weight, the more chance there is that symptom appeared in a document classified as relevant. We ended up using this list to help make our frame scorer more precise because during our testing incorporating this data we did see a significant boost in our 2015 precision. We also added these symptom weights to one of our Lucene runs. We used the weighted symptoms list to help add different symptoms to the queries that we passed to Lucene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The SCDA Architecture</head><p>In a standard run, we used Lucene as described above to generate a list of the top 20 documents for a topic. This list, containing document ids and scores, is passed to the Framer. The topic is framed to create a Query Frame, and each returned document's abstract is framed and then scored against the Query Frame. The Framer returns a new reranked list of the highest scoring documents based on their frame's score. Finally, the weights learned from the 2015 machine learning process are applied to further improve the ranked results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Early Conclusions Lucene Baselines</head><p>Our initial Lucene queries performed over 10% worse than during the 2015 track. This is most likely due to the different and more realistic nature of the topics.</p><p>Eleven topics in the baseline did not return any relevant documents. Due to the nature of the system, in the situation where Lucene generates no relevant documents, framing of course cannot improve the output, <ref type="bibr" coords="9,261.31,598.18,260.56,10.54;9,89.88,611.86,34.57,10.54">(specifically, topics 2, 4, 5, 7, 10, 12, 15, 19, 21, 23, and 27)</ref>.</p><p>In our error analysis the eleven topics where P(10)=0.0 were examined first. A common thread between these topics was the use of semi-complex and more rare medical terminology: interparenchymal hemorrhages (topic 12), tracheobronchomalacia (topic 15), biphenotypic ALL (topic 21). Many of these symptoms were not present or common in the 2015 and track and our system was not designed with this degree of medical terminology in mind. Another new addition to the 2016 topics was the common use of medical acronyms: "78 year old female with PMHx HTN, dCHF, Diabetes, CKD" (topic 19), "94 M with CAD s/p 4v-CABG, CHF, CRI presented with vfib arrest" (topic 22). These acronyms may have held valuable information which would have directed our system to more relevant documents that were unfortunately passed over.</p><p>It is also possible that the version of Lucene used in our system (version 4.0.0) is not suited to handle some of these terms as it is not the latest release and was built to run with Java 6. Using a more recent release of Lucene could yield better results. Furthermore, the use of other indexing/querying software such as Indri might be very helpful. If other querying software yielded different ranked lists, this could be extremely helpful for our framing. Finally, returning a larger list of ranked documents in Lucene could potentially improve the framing but would greatly increase the time needed to frame and score documents. If time allowed for the framing and scoring of the top 100 documents we may have found relevance after framing for our lower scoring topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Learning</head><p>Machine learning was a new method that we tried out with our system that we had not tried in the previous year. In our initial testing we saw an increase in precision when we implemented the weights that we found in machine learning. It is hard to say how much machine learning helped in our 2016 results without further experiments on the 2015 results. But given that our Lucene baseline went down from last year (2015 our Lucene P(10) was 0.3767) to this year (2016 our P(10) was 0.23) but our framing score went slightly up, from 0.2667 to 0.2770, implies our framing changes realized a strong improvement. Without further analysis it is hard to say what made this jump in precision occur. It could have been the machine learning that we implemented or it could have been some other minor tweaks that were done in our framing process.</p><p>We also saw an improvement between the Lucene run with and without the weights. It was a positive change. In the Lucene run without weights our P(10) was .2300 and in the Lucene run with weights incorporated from machine learning our P(10) was .2467.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,266.02,346.65,79.64,8.64;4,91.88,152.12,443.60,189.09"><head>FigureFigure 1 :</head><label>1</label><figDesc>Figure 1: Topic #20</figDesc><graphic coords="4,91.88,152.12,443.60,189.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,252.77,455.04,106.16,7.80;9,91.88,73.88,472.97,371.15"><head>Figure</head><label></label><figDesc>Figure 4: SCDA Architecture</figDesc><graphic coords="9,91.88,73.88,472.97,371.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,91.88,73.88,447.97,174.35"><head></head><label></label><figDesc></figDesc><graphic coords="5,91.88,73.88,447.97,174.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,91.88,503.70,431.89,160.25"><head></head><label></label><figDesc></figDesc><graphic coords="5,91.88,503.70,431.89,160.25" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>The team was funded under the <rs type="projectName">Siena College Institute for Artificial Intelligence</rs>'s <rs type="funder">National Science Foundation's Research Experience for Undergraduates Grant</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_SkZE6Kf">
					<orgName type="project" subtype="full">Siena College Institute for Artificial Intelligence</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,95.12,510.51,74.02,12.22" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,89.88,536.83,409.05,9.11;10,89.88,548.35,412.84,9.11;10,89.88,559.87,34.12,9.11" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,368.91,536.83,130.01,9.11;10,89.88,548.35,412.84,9.11;10,89.88,559.87,29.85,9.11">WSU-IR at TREC 2015 Clinical Decision Support Track: Joint Weighting of Explicit and Latent Medical Query Concepts from Diverse Sources</title>
		<author>
			<persName coords=""><surname>Balaneshin-Kordan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Saeid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Railan</forename><surname>Kotov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xisto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,89.88,583.05,431.87,8.64;10,89.88,594.57,431.95,8.64;10,89.88,606.09,144.86,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,357.47,583.05,164.29,8.64;10,89.88,594.57,249.46,8.64">San Francisco State University at TREC 2014: Clinical Decision Support Track and Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">Aayush</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Klinkhaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anagha</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,359.55,594.57,162.28,8.64;10,89.88,606.09,85.13,8.64">Proceedings of The Twenty-Third Text Retrieval Conference</title>
		<meeting>The Twenty-Third Text Retrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,89.88,629.13,431.94,8.64;10,89.88,640.65,160.19,8.64" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buttcher</surname></persName>
		</author>
		<title level="m" coord="10,326.93,629.13,194.89,8.64;10,89.88,640.65,155.51,8.64">Reciprocal Rank Fusion outperforms Condorcet and Individual Rank Learning Methods</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,89.88,663.69,431.83,8.64;10,89.88,675.21,431.93,8.64;10,89.88,686.73,224.27,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,355.73,663.69,165.99,8.64;10,89.88,675.21,327.13,8.64">UCLA at TREC 2014 Clinical Decision Support Track: Exploring Language Models, Query Expansion, and Boosting</title>
		<author>
			<persName coords=""><forename type="first">Jean</forename><forename type="middle">I</forename><surname>Garcia-Gathright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,439.41,675.21,82.41,8.64;10,89.88,686.73,164.54,8.64">Proceedings of The Twenty-Third Text Retrieval Conference</title>
		<meeting>The Twenty-Third Text Retrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,89.88,74.49,431.65,8.64;11,89.88,86.01,377.85,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,336.03,74.49,185.49,8.64;11,89.88,86.01,55.65,8.64">NovaSearch at TREC 2014 Clinical Decision Support Track</title>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Mourão</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Flávio</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">João</forename><surname>Magalhães</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,163.20,86.01,244.79,8.64">Proceedings of The Twenty-Third Text Retrieval Conference</title>
		<meeting>The Twenty-Third Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,89.88,109.05,431.82,8.64;11,89.88,120.57,431.96,8.64;11,89.88,132.09,105.74,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,342.60,109.05,179.10,8.64;11,89.88,120.57,190.30,8.64">NovaSearch at TREC 2013 Federated Web Search Track: Experiments with rank fusion</title>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Mourão</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Flávio</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">João</forename><surname>Magalhães</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,303.42,120.57,218.42,8.64;11,89.88,132.09,46.00,8.64">Proceedings of The Twenty-Second Text Retrieval Conference</title>
		<meeting>The Twenty-Second Text Retrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,89.88,155.13,431.98,8.64;11,89.88,166.65,90.47,8.64;11,89.88,189.69,431.88,8.64;11,89.88,201.21,64.39,8.64" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Mccandless</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Otis</forename><surname>Gospodnetic</surname></persName>
		</author>
		<title level="m" coord="11,372.42,155.13,149.43,8.64;11,89.88,166.65,90.47,8.64;11,89.88,189.69,155.70,8.64;11,282.88,189.69,238.88,8.64;11,89.88,201.21,60.11,8.64">Lucene in Action. Second Edition. Manning Publications. National Library of Medicine (NLM)</title>
		<imprint>
			<date type="published" when="2010">2010. 2013</date>
		</imprint>
	</monogr>
	<note>MetaMap-A Tool For Recognizing UMLS Concepts in Text. Software</note>
</biblStruct>

<biblStruct coords="11,89.88,224.25,431.97,8.64;11,89.88,235.77,415.90,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,369.94,224.25,151.90,8.64;11,89.88,235.77,93.67,8.64">Overview of the TREC 2014 Clinical Decision Support Track</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,201.21,235.77,244.83,8.64">Proceedings of The Twenty-Third Text Retrieval Conference</title>
		<meeting>The Twenty-Third Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,89.88,259.00,434.15,8.75;11,89.88,270.52,434.11,8.75;11,89.88,282.28,64.20,8.75" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,306.51,259.00,217.52,8.75;11,89.88,270.52,131.11,8.75">HITIQA: A Data Driven Approach to Interactive Analytical Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Sharon</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,228.42,270.52,254.00,8.75">Proceedings of Human Language Technology Conference</title>
		<meeting>Human Language Technology Conference<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,89.88,305.37,431.86,8.64;11,89.88,316.89,105.74,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,204.68,305.37,105.54,8.64">Overview of TREC 2007</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,332.58,305.37,189.16,8.64;11,89.88,316.89,46.00,8.64">Proceedings of The Sixteenth Text Retrieval Conference</title>
		<meeting>The Sixteenth Text Retrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
