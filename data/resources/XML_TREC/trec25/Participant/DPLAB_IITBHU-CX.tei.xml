<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.39,115.96,336.57,12.62;1,225.73,133.89,163.91,12.62">Employing open-web for Contextual Suggestion using tag-tag similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.04,171.56,70.51,8.74"><forename type="first">Sainyam</forename><surname>Kapoor</surname></persName>
							<email>sainyam.kapoor.cse12@iitbhu.ac.incmanajit.rs.cse14@iitbhu.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology (BHU)</orgName>
								<address>
									<postCode>221005</postCode>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.94,171.56,93.60,8.74"><forename type="first">Manajit</forename><surname>Chakraborty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology (BHU)</orgName>
								<address>
									<postCode>221005</postCode>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.74,171.56,107.57,8.74"><forename type="first">Ravindranath</forename><surname>Chowdary</surname></persName>
							<email>rchowdary.cse@iitbhu.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology (BHU)</orgName>
								<address>
									<postCode>221005</postCode>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.39,115.96,336.57,12.62;1,225.73,133.89,163.91,12.62">Employing open-web for Contextual Suggestion using tag-tag similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BEF7B2EEF75F44FB9A038976592D7CBB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Contextual Suggestion</term>
					<term>Points-of-interest</term>
					<term>Wikipedia</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC 2016 Contextual Suggestion task aims at providing recommendations on points of attraction for different kind of users and a varying context. Our group DPLAB IITBHU tries to recommend relevant point-of-interests to a user based on the information provided on the candidate attractions and her past preferences. We employ open-web information in a novel way to capture the best possible setting for a user's tastes and distastes in terms of tag scores. The scores are then ranked using a heuristic to suggest the most pertinent attraction to the user. One of our methods exceed the TREC-CS 2016 median of the standard evaluation scores of all participant runs, which reflects the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Contextual Suggestion track investigates search techniques for complex information needs that are highly dependent on context and user interests. The task is to provide suggestions to users based on their personal interests as well as their contexts. In our last year's attempt we tried to capture user interests in terms of tags and check its similarity to the profile of the users. This technique produced decent results but we did not consider the contexts of user's visits. Our aim this year was to specifically address that issue and in the process increasing the efficacy of the recommender system. Our proposed method ranks candidate suggestions based on the similarity of the tags between user visited attractions and the candidate suggestions. The candidate suggestion tags in turn are ranked based on their admittance to contexts. We prepare three different heuristics to capture the best possible setting so as to model the user's taste and corresponding catering to those tastes.</p><p>We reformulated the problem of contextual suggestion as the following question: "Can we find all the correct tags for an attraction for a user based on the context she's provided? ". To answer the question we took the help of open-web i.e. Wikipedia. We scored the mutual similarity of all the tags provided in the tagset and the contexts against themselves. The tags were then ranked based on their similarity scores and then a further matching of the candidate suggestion tags against the user profile attraction were carried out. The final score was computed using three different heuristics as explained in the next section, which yielded three runs submitted this year.</p><p>The details of our recommendation model include data pre-processing, tagging, scoring and finally ranking the candidate suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In this section we describe the methodology used to generate the suggestion list for users based on their profiles and the given context. While the data preprocessing and tagging methods are same across the three runs, the difference lies in the heuristic used to calculate the ranks of the candidate suggestions. The detailed work-flow is as follows:</p><p>-Data Preprocessing:</p><p>(i) For each of the 224 tags in the provided tag list, we extract its corresponding Wikipedia page. We perform the same task for each of the 15 context tags (i.e. Trip type, Season etc.). (ii) We construct a matrix M T T where both the rows and columns consist of Wikipedia pages corresponding to the tags. Each of the elements in the matrix represent a similarity score between the wiki pages of the tags, in effect implying the measure of closeness of a tag i to tag j (i = j). We have considered only the visible text from the wiki page excluding the title. For computing similarity, cosine similarity was used where idf 1 was computed on the wiki pages (documents) for tags. Additional cleaning of the text from the wikipedia pages was performed to remove copyright and other irrelevant text. (iii) Next, we pre-process the provided descriptions of the user profile and candidate suggestion attractions by removing sentences which contains irrelevant and garbled words. (iv) The stop words and special characters (!@#$%ˆetc.) except punctuations were removed and only intelligible words were retained in both the tag documents (wiki pages) and attraction descriptions. Also, stemming and lemmatization was carried out.</p><p>-Modeling User Profile:</p><p>The first step towards an efficient recommendation system is to understand the user's needs and tastes. Incorporation of this knowledge can positively affect the quality of recommender system. Since, we don't have any personal information about the users, we have to rely on the rating and tags of the attractions the user has previously visited.</p><p>1</p><p>The Stanford NLTK <ref type="bibr" coords="2,207.24,659.54,7.40,4.35" target="#b0">[1]</ref> was used for this (i) For each attraction in the user profile, we tag the attraction with n tags. The tags are associated based on its similarity to the description of the attraction. To determine the similarity between the tag to be assigned and the attraction, we construct a matrix M T D , whose elements represent the similarity between the Wikipedia page of the tag under consideration and the attraction description. For example, let us say, attractions are denoted by x 1 , x 2 , x 3 , ..., x m and the tags from the predefined taglist are denoted by t 1 , t 2 , t 3 , ..., t 224 . Then the matrix M T D would look something like: Once again, cosine similarity was used for computation. By attraction, we mean the description associated with the attraction. The figures presented in Table <ref type="table" coords="3,238.43,382.80,4.98,8.74" target="#tab_0">1</ref> are indicative and not the original figures. (ii) Next, we sort the tags based on their similarity scores for each attraction in the user profile. We retain only top k tags. Here, k = 10. A similar procedure is carried out for each of the candidate suggestions as well. (iii) Now, we take a weighted count of each unique tag across the set of attractions for a user profile. The weights differ from approach (run) to approach as explained later. (iv) Lastly, we retain the top-20 tags based on the value of its weights. These tags are henceforth referred to as user-profile tags. These user-profile tags capture the essential needs of the user. In other words, these selected tags model a user specific taste. Further, refinement of the user-profile tags is carried out by re-ranking the tags based on the contexts. This involves ranking the 20 tags based on their M T T score. In case of missing context, the original order of the tags is retained. The reason behind this exercise is to bring out the order among the tags which the user may prefer according to the specified context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Ranking candidate suggestions:</head><p>The final step involves ranking of candidate suggestions based on its score. The score is calculated on the basis of proximity of the candidate suggestion tags to the user-profile tags. To determine this proximity we first construct a matrix M T C , where each element represent the similarity of the candidate suggestion tags to the user-profile tags. We achieve this by assigning each tag present in the candidate the reciprocal rank of the corresponding userprofile tag list. In case, the candidate tags did not appear in the user-profile tag list, we assign it a score of zero. Finally, we take the summation of all the tag scores for each candidate suggestion and term it as the candidate score. The suggestions are then ranked based on the candidate scores. This method is common to all the three approaches. The difference lies in the tagging mechanism and assignment of weights to user-profile tags which we discuss next. Approach 1 (RUN 1): In this approach we assume that the user has not provided any tags. Thus, we tag all the attractions in user profile and candidates uniformly without bothering for the user-defined tags. This was done to ensure that there are no discrepancies involved in mixing the tags provided by users and the ones assigned by us. In other words, we eliminated the dilemma of choosing the best tags among the two sets, in effect reducing any error that may creep in due to misjudging the appropriate tag for an attraction.</p><p>For determining weights of tags we take the help of ratings that the user has assigned to each attraction. We segregated the ratings into two levels:</p><p>• +1 for all the attractions that have been rated 4 or 3 by the user, suggesting her liking for that particular attraction.</p><p>• -1 for all the other attractions that have been rated less than 3 but greater than 0, expressing user's disinterest in the same.</p><p>For all other attractions that the user rated -1, we simply discarded them. Each of tags associated with an attraction was initially assigned the same rating i.e if the attraction has been rated 4, all the tags would carry an initial weight of +1. Similarly, all the tags associated with a low rated attraction would carry a weight of -1. Now, for each of the unique tags, we summed the weights associated with it. The tags are then ranked based on their final weights w f .</p><p>Approach 2 (RUN 2): This approach is similar to Approach 1 differing only in the fact that we considered both the user-defined tags along with the tags assigned by us. We limited the number of tags for attractions to 10 as stated earlier. Now, say, the user has already provided p tags, then we assign the rest 10 -p tags using our method explained above. The consideration of the user tags was done to ensure that we were not completely alienating ourselves from user specified choices, a risk we run if we concentrate only on our tagging method. Rest of the method is in line with Approach 1.</p><p>Approach 3 (RUN 3): This approach differs from the other two in the fact that here we consider only the frequency of the tag in the user profile attraction set instead of assigning two-level weights. By doing so, we want to capture the user's tastes without accounting for the rating associated with the tag. We go by the simple notion that "the more frequent a tag is, the more chances that the user likes that facility".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>We employed three techniques for capturing the user tastes and deliver a recommendation list that caters to her needs. All the three techniques employed open-web information in the form of Wikipedia pages associated with the tags and designed heuristics to calculate the score of each candidate suggestion accordingly. From Table <ref type="table" coords="5,234.92,190.73,4.98,8.74" target="#tab_1">2</ref> it is clear that Approach 1 performed better than the other two approaches. In fact, our first run surpassed the median of this year's evaluation results against all three metrics. A comparative assessment of our methods against other teams could not be performed due to unavailability of their technical description and evaluation results. But as far as performance is considered, our first approach performs slightly better than the median results as listed in Table <ref type="table" coords="5,335.13,262.46,3.87,8.74" target="#tab_1">2</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,203.30,257.42,208.06,86.25"><head>Table 1 .</head><label>1</label><figDesc>Attraction to Tag similarity matrix MT D</figDesc><table coords="3,244.78,278.22,125.80,65.45"><row><cell></cell><cell></cell><cell>Tags</cell><cell></cell><cell></cell></row><row><cell cols="2">Attraction t1</cell><cell cols="3">t2 ... t224</cell></row><row><cell>x1</cell><cell cols="4">0.045 0.003 ... 0.135</cell></row><row><cell>x2</cell><cell cols="4">0.246 0.024 ... 0.008</cell></row><row><cell>:</cell><cell>:</cell><cell>:</cell><cell>:</cell><cell>:</cell></row><row><cell>xm</cell><cell cols="4">0.251 0.306 ... 0.019</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,187.90,292.01,239.56,74.34"><head>Table 2 .</head><label>2</label><figDesc>Evaluation Results</figDesc><table coords="5,187.90,311.07,239.56,55.29"><row><cell></cell><cell cols="2">NDCG@5 P@5 Reciprocal Rank</cell></row><row><cell>Approach 1 (iitbhu01)</cell><cell>0.2757 0.4138</cell><cell>0.6298</cell></row><row><cell>Approach 2 (iitbhu04)</cell><cell>0.2325 0.3310</cell><cell>0.5367</cell></row><row><cell>Approach 3 (iitbhu05)</cell><cell>0.2106 0.3034</cell><cell>0.4921</cell></row><row><cell cols="2">Average TREC Median 2 0.2562 0.3931</cell><cell>0.6015</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,439.65,342.25,7.86;5,146.91,450.61,333.68,7.86;5,146.91,461.57,333.68,7.86;5,146.91,472.53,196.69,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,287.85,439.65,151.18,7.86">NLTK: The natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,462.94,439.65,17.66,7.86;5,146.91,450.61,333.68,7.86;5,146.91,461.57,238.59,7.86">Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics</title>
		<meeting>the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
