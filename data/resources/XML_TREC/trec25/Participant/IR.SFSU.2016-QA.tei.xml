<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,128.36,8.54,354.79,24.98;1,153.70,40.12,304.10,24.98">San Francisco State University at LiveQA Track of TREC 2016</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,189.43,71.89,98.81,12.99"><forename type="first">Maria</forename><surname>Khvalchik</surname></persName>
							<email>maria.khvalchik@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">San Francisco State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.37,71.89,102.74,12.99"><forename type="first">Anagha</forename><surname>Kulkarni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">San Francisco State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,128.36,8.54,354.79,24.98;1,153.70,40.12,304.10,24.98">San Francisco State University at LiveQA Track of TREC 2016</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E5DC5E87B25A909F94DA853F473B37BD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are many situations in our everyday life where we look for answers to some questions -"Who wrote this book?", "How to grill a fish?" or "Where is the Opera House located?". Twenty years ago to answer these questions people were looking them up in the encyclopedias, recipe books or were asking other people. Moving the information into the electronic form and making it universally accessible helped to automate this process and save an enormous amount of time.</p><p>The developing an automatic question answering system is a complex problem which is at the intersection of multiple disciplines -Natural Language Processing (NLP), Information Retrieval (IR) and Machine Learning (ML). A typical question answering system is structured as a pipeline of three components: • Query formulation -uses NLP techniques to analyze the question and to transform it into a query. • Document extraction -uses IR/ML techniques to retrieve a list of documents that are likely to contain answers. • Passage retrieval and ranking -uses ML techniques to select a unit of text (typically, sentences) from the retrieved documents, as the best answer. Our goal is to build such a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Objective</head><p>Although the problem of answering factoid questions (e.g. "What is the capital of the US?") has been investigated extensively <ref type="bibr" coords="1,220.74,522.38,12.23,10.21" target="#b4">[5,</ref><ref type="bibr" coords="1,236.72,522.38,9.17,10.21" target="#b5">6,</ref><ref type="bibr" coords="1,249.64,522.38,9.17,10.21" target="#b6">7,</ref><ref type="bibr" coords="1,262.56,522.38,9.17,10.21" target="#b8">9,</ref><ref type="bibr" coords="1,274.73,522.38,15.29,10.21" target="#b10">11,</ref><ref type="bibr" coords="1,293.02,522.38,15.29,10.21" target="#b11">12]</ref> , answering open-domain non-factoid questions (e.g. "What is the best way to cook fish?") remains a challenging problem. We concentrate on the latter type of questions, with the added constraint of keeping the response time per question to one minute at most.</p><p>In addition, we focus only on query formulation, and passage retrieval and ranking. The document extraction component is delegated to one of popular Web search engines. Specifically, we perform linguistic analysis of the question to transform it into a boolean query with conjunction and disjunction operators. This query is executed against the Web-as-a-corpus using a popular search engine, and the top results are used to compile the answer for the question. The performance of the system is evaluated using Yahoo! Answers dataset which 1 consists of large set of questions along with the best answers, given and voted by users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Suryanto et al <ref type="bibr" coords="2,151.73,151.13,12.23,10.21" target="#b0">[1]</ref> focus on the Collaborative Question Answering (CQA) task. This task is defined as answering questions using community question answering portals such as Yahoo! Answers, Naver, etc. These portals contain millions of questions and answers contributed by users and can be used as a database to answer questions from any user. The paper defines an algorithm to solve the CQA task which uses the quality of the answers as the key feature.</p><p>Lin et al <ref type="bibr" coords="2,115.65,241.13,12.23,10.21" target="#b1">[2]</ref> propose a method to identify different variants of the same question with the goal of finding the right answer passage. For instance, "X wrote Y" is the same as "X is the author of Y". Their approach uses paths in the dependency trees to infer rules about question equivalence. The authors did not try to inject their inference rules into a working QA system, but they took 15 questions from TREC-8 which were manually paraphrased by humans and manually compared the output of the method with these paraphrased questions.</p><p>Lee et al <ref type="bibr" coords="2,128.32,346.13,12.23,10.21" target="#b2">[3]</ref> used lexico-semantic pattern matching and shallow NLP methods to create high-performance QA system for factoid questions. They define 18 types of answers such as QUANTITY, BODY_PART, LIFE_FORM, etc using the datasets of questions from previous TREC competitions. The type of the answer is then determined by using part-of-speech tagger and by converting the obtained terms into lexico-semantic patterns (LSP). LSP is defined as a pair of condition and conclusion where condition is a regular expression created from LSP grammar and conclusion is the type of answer. In terms of Mean Reciprocal Rank (MRR) their system performed better than the average of all other systems participating in TREC-10.</p><p>Cui et al <ref type="bibr" coords="2,119.73,481.13,12.23,10.21" target="#b3">[4]</ref> hypothesize that many question answering systems retrieve the wrong passages because of disregarding the semantics of the answer relative to the question. For instance, a question: "What percent of the nation's cheese does Wisconsin produce?" could produce a wrong answer: "the number of consumers who mention California when asked about cheese has risen by 14 percent, while the number specifying Wisconsin has dropped 16 percent." The proposed solution for that problem is similar to Lee et al <ref type="bibr" coords="2,355.86,556.13,12.23,10.21" target="#b2">[3]</ref> that is to analyze the dependency tree and extract similar paths. The difference is that Lee's paper proposes to generate the variations of the questions using these similar paths while Cui's proposes to use this similarity when doing matching the question with the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>We developed a complete end-to-end QA system which is capable of answering questions with the requirements specified in the objective. IN this section we describe the system architecture, and the various techniques and tools used in the different stages of the QA pipeline.</p><p>The architecture diagram for our QA system is depicted in Figure <ref type="figure" coords="3,414.53,181.13,4.59,10.21" target="#fig_0">1</ref>. The entry point of the system is the Query Formulation Module (QFM) which is responsible for generating a query from the given question. The resulting query is then sent to Google and Bing search engines and the top x results returned by the search engines are downloaded. At this point the Passage Extraction and Ranking (PER) module takes over to identify the best answer for the question. Each of these modules are described in detail next. Our approach for QFM was driven by two observations. 1. The law of diminishing returns is at play for longer questions. A longer question is not proportionality more informative than a shorter question. Often, the longer question repeats or paraphrases the same information multiple times. Also, longer questions are more likely to contain terms that are only tangentially related to the core question, and thus retrieve non-relevant documents. 2. The classic problem of vocabulary gap manifests here as well, where the question and a relevant document use a non-overlapping set of vocabulary and thus lead to a mis-match. For instance, a question about california might only specify the postal code CA in which case a relevant document that does not use the postal code will not match the question. Therefore, the main challenge of the query formulation module was keeping the balance between removing the unnecessary words and expanding the query with more terms. To achieve this balance the following approach was developed.</p><p>The input question is first analyzed using WordNet part-of-speech parser. The verbs and nouns are retained due to their informativeness. The original question is also analyzed with Stanford dependency parser to select the root word in the question. This is the most important word in 2 2 http://nlp.stanford.edu/software/lex-parser.shtml the question and it is necessary to keep it in the query. All stop words such as "be", "what", "the" are removed from the query. We used the list of stopwords from NLTK .</p><p>3 For all the remaining words, only the first 6 are retained. It reduces the quality of answers for some long questions, but improves the overall quality of the results. Next each term is expanded with the top 3 synonyms from the WordNet library. The original term and the synonyms are joined with OR operators, and the synonym groups are joined with AND operators. For instance, the question: "What is the capital of California?" is transformed into the following query by the above approach: "(capital) AND (California OR CA)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Document Retrieval</head><p>The query created by QFM was then used to obtain matching documents from two sources: Yahoo! Answers and Bing. Specifically, the top three documents returned by Yahoo! Answers, and the top 20 documents returned by Bing for the query were downloaded. Bing and Google were both evaluated for this task, and Bing was chosen since it supports both AND and OR operators, while Google supports only AND.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Passage Extraction and Ranking</head><p>Each of the downloaded webpages is first passed through the following text processing pipeline. The first step of this pipeline extracts the ASCII text from the webpage using a html2text library . <ref type="bibr" coords="4,531.75,376.74,3.89,6.49" target="#b3">4</ref> We refer to the extracted text as the document. Next, the document is split into passages, where each passage consists of five consecutive sentences. A sliding span of five consecutive sentences is used to generate the passages. Thus a document containing 6 sentences would generate two passages. This approach generates many passages, specifically, 1 + (n-5). The following rules are used to filter out passages that are not likely to contain the answer. Passage that do not contain any of the query terms, or that contain more than 2 line breaks, or more than 10 punctuation marks, or non-printable symbols are eliminated. Also, passages that are not in English are filtered out. The langdetect library is 5 employed for language identification. Out of the remaining passages, one passage is chosen as the best candidate answer from that document, as follows. A trained classification model is first employed to categorize each passage as being relevant or non-relevant to the query. (The details about the passage classification model are described in Section 4.4.1.) If none of the candidate passages from a document are classified as relevant or if there are multiple relevant passages, then the passage with the highest BM25 score is selected as the best candidate answer for the document. Thus, one candidate answer is generated by each of the downloaded documents.</p><p>Finally, a single answer is selected from the above pool of candidate passages using another classification model. A relevant passage, as per this second classification model, that has the highest BM25 score is chosen as the final answer. The details of second classification model are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Passage Classifiers</head><p>The first passage classification model is designed to distinguish between relevant and non-relevant passages from the same document. For training this passage classification model a feature vector is generated for each passage. The feature set consists of the BM25 rank of the passage relative to other passages, the ratio of query terms in the passages and the total terms in the passage, the average distance between query terms in the passage, and the length of the passage. The relevance label for each passage in the training set was derived by first estimating its cosine similarity with the best answer on Yahoo! Answers. The highest scoring passages, specifically, the top quartile, were labelled as being relevant, and the bottom quartile as non-relevant. The popular classification algorithm, Support Vector Machines with polynomial kernel was used to learn a binary classification model with the above described labelled data.</p><p>The second passage classification model is expected to learn finer distinctions between the passages and thus provide the best passage as the final answer. This is evident in the how the training data for this model is compiled. Only the single best candidate passage from each document is included in the training set for this model. Out of these only the top quartile that exhibit the highest similarity with best human generated answer from Yahoo! Answers are labelled as relevant. The bottom quartile are labeled as non-relevant. The feature set for this model is a superset of the one used by the previous model. Whether the document that sourced the passage was downloaded from Wikipedia is an additional feature. Another feature captures if the source document was downloaded from Yahoo! Answers. Both these features model the source quality. Whether the source document appears in the search results from both, Bing and Google, is another feature. The rank of the document in Bing search results is also a feature. We also tried using Google rank, but it resulted in model of a lower quality. SVM is used to learn a binary classification model using the above described training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>A subset of 500 question-answers from the L6 dataset from the Webscope datasets of Yahoo! Labs is used in this work. The first 400 questions in this dataset are used to train the two The quality of the generated answers is compared with the human generated best answer from Yahoo! Answers dataset, and is quantified using three text similarity metrics: Jaccard Coefficient, Cosine Similarity, a variant of Kullback-Leibler divergence that is symmetric and incorporates smoothing <ref type="bibr" coords="6,192.23,120.38,17.12,10.21" target="#b12">[13]</ref>. Question length can influence the effectiveness of our technique. Extremely short questions, as well as long questions are harder to answer. The former suffers from data sparsity, and the latter from noise. Thus we categorize the questions based on their length and report the results for each category separately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>The results are reported in Table <ref type="table" coords="6,243.23,549.38,4.59,10.21" target="#tab_0">1</ref>. The length in tokens used to categorize the questions is specified at the top of the table, along with the number of questions that get sorted into each category. Next, the average similarity between the answers generated by the baseline approach and the best answers are reported. Finally, the average similarity between answers generated by our approach and the best answers is reported.</p><p>The overall trend is that our approach performs consistently better than the baseline across the board. Jaccard coefficient measures fraction of term overlap between the system generated answer and the best answer. The Jaccard low values indicate small term overlap. Often time the reason for this is that the answers generated by the system are not succinct. Our approach extracts a sequence of 5 sentences as the answer. Typically only one of these sentences</p><p>contains the actual answer. This observation inspires one of the future directions: identifying the answer boundary for each question. This will also model that for some questions the answer maybe longer than 5 sentences, and for others much shorter than 5 sentences.</p><p>The cosine similarity improves over the previous metric by modeling term frequency and by incorporating length normalizing. With this metric, the improvement over the baseline is much larger. The next metric, KLD incorporates language modeling and smoothing, thus providing an even better metric for quantifying similarity between two texts. Again, the average similarity values of our system are substantially higher than those with the baseline. Of the three length categories, the KLD value is highest for mid-range questions, as we expected. We also see that the shorter questions, that is data sparsity, are slightly harder for our system than longer questions.</p><p>The main bottleneck in the system is downloading the document from the internet -it is the most time consuming operation, which takes 90% of the time to answer the question. Our system limits the impact of this bottleneck by setting a time cutoff of 2.5 seconds for downloading an individual URL, and a cutoff of 51 seconds for downloading all the URLs necessary for a question. With these constraints the average time to answer a question is 25 seconds. With 99% of the questions being answered within 1 minute.</p><p>Table <ref type="table" coords="7,104.00,375.38,6.12,10.21">2</ref> provides a few questions, and the corresponding answers generated by the baseline system, and our system. The selected questions are both factoid and non-factoid. It can be observed, that answers given by our system are more relevant to the question. On the other hand, the answers given by baseline rely only on the frequency of the search terms and generally do not answer the question and could very irrelevant. The answers given by our method are few sentences long even for factoid questions -this could be a potential way of improvement for our system.</p><p>Question #1 What is the breed of the dog in doge Internet meme? Baseline Reacting to the meme, she explained, "To be honest, some pictures are strange for me, but it's still funny. I'm very impressed with their skills and taste. Around me, nobody knows about the doge meme. Maybe I don't understand memes very well, because I'm living a such an analog life." Satō has also expressed that she had learned that "the risk of the internet is that anyone in the world can see my life on my blog".</p><p>Our system Until doge, the Shiba Inu wasn't even on my radar. I'm only mildly embarrassed to admit that it's become my new favorite meme, eliciting real life excitement whenever I see the breed, which, only a few months prior, was invisible to me.</p><p>Question #2 What is the best way to cook fish? Baseline How to test fish for doneness. Always check the fish at the minimum baking time. Insert a fork into the fish and gently twist. The fish is done as soon as it begins to flake.</p><p>Our system It's not only delicious, it's also versatile there are a number of healthy ways to prepare it. Here are five cooking techniques that will inspire you to add more fish to your meals . You can grill fish: Grilling, indoors or out, cooks fish fast and keeps it tender, juicy, and flavorful. Season the fish with your choice of fresh or dried herbs or a mix of herbs and spices. With a basting brush, lightly oil the fish with extra-virgin olive or canola oil so it won't stick to the grill or grill pan.</p><p>Question #3 When Mozart was born?</p><p>Baseline When Mozart lived, there were no music downloads. No records of any kind, just notes written on paper. People heard music only when it was 'live'. People sang while they worked. They danced at balls and parties.</p><p>Our system He was a composer, and one of the greatest musicians of all time. He was famous as a child because he could play and write music from the age of 4. He grew up to write some of the most beautiful music ever heard. When did he live. Mozart was born in 1756.</p><p>Question #4 How to learn skateboarding? Baseline You will learn the basic Teaching Model which serves as your class outline from introduction to review, you will learn how to break down each skill into three easy to understand steps and how to clearly communicate those steps to your students. You will learn how to manage your class effectively and safely.</p><p>You will learn about what makes for a great teacher so you can become one as well. Live sessions will give you a chance to practice teaching each of the steps.</p><p>Our system In a nutshell, no, there is no such thing as skateboarding classes. just pick up a board. get on it, and skate. just learn from your falls, and whatever you do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. Sample questions and answers 7 Conclusions</head><p>In this paper we presented our first attempt at tackling the challenging problem of answering open-domain non-factoid questions. Many of our system design choices were governed by the track requirement of answering every question under a minute. Our approach starts by translating the question into a boolean query which is then run using the Bing search engine to identify documents that may contain the answer. The last phase extracts candidate answers for each of the downloaded documents, and selects the final answer from this pool.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,233.25,373.88,144.83,10.21;3,165.75,284.25,280.50,81.75"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. System Architecture</figDesc><graphic coords="3,165.75,284.25,280.50,81.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,78.00,201.38,426.95,282.46"><head>Table 1 .</head><label>1</label><figDesc>Answer Quality Results</figDesc><table coords="6,78.00,201.38,426.95,261.46"><row><cell>Question Length (in</cell><cell>[0;6]</cell><cell>[7;12]</cell><cell>&gt;=13</cell></row><row><cell>tokens)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of questions</cell><cell>32</cell><cell>43</cell><cell>25</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell></cell></row><row><cell>Jaccard similarity</cell><cell>0.05∓.01</cell><cell>0.06∓.01</cell><cell>0.06∓.01</cell></row><row><cell>Cosine similarity</cell><cell>0.08∓.02</cell><cell>0.09∓.02</cell><cell>0.09∓.02</cell></row><row><cell>KL divergence</cell><cell>0.50∓.06</cell><cell>0.53∓.05</cell><cell>0.42∓.05</cell></row><row><cell></cell><cell>Our System</cell><cell></cell><cell></cell></row><row><cell>Jaccard similarity</cell><cell>0.09∓.03</cell><cell>0.10∓.02</cell><cell>0.10∓.03</cell></row><row><cell>Cosine similarity</cell><cell>0.21∓.05</cell><cell>0.20∓.05</cell><cell>0.20∓.06</cell></row><row><cell>KL divergence</cell><cell>0.65∓.08</cell><cell>0.75∓.08</cell><cell>0.68∓.10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,78.46,710.77,158.27,9.05"><p>http://webscope.sandbox.yahoo.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,78.46,686.77,149.02,9.05"><p>http://www.nltk.org/book/ch02.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,78.46,698.77,158.79,9.05"><p>https://pypi.python.org/pypi/html2text</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,78.46,710.77,169.65,9.05"><p>https://pypi.python.org/pypi/langdetect?</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="5,72.00,608.63,466.84,10.21;5,72.00,623.63,466.64,10.21;5,72.00,638.63,466.96,10.21;5,72.00,653.63,187.66,10.21;5,72.00,709.71,3.34,5.57;5,78.46,710.77,158.27,9.05"><p>classification models, and the remaining 100 questions for testing. For the baseline approach we treat the question as a query and download the top ranked webpage by Bing. The passage from this document with the highest BM25 score with respect to the query, is considered the final answer by the baseline approach.6 http://webscope.sandbox.yahoo.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.97,121.13,451.11,10.21;9,72.00,136.13,467.01,10.21;9,72.00,151.13,121.07,10.21" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,261.77,121.13,277.31,10.21;9,72.00,136.13,70.44,10.21">Quality-aware collaborative question answering: methods and evaluation</title>
		<author>
			<persName coords=""><forename type="first">Maggy</forename><surname>Suryanto</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Anastasia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,156.43,136.13,382.58,10.21;9,72.00,151.13,54.86,10.21">Proceedings of the second ACM international conference on web search and data mining</title>
		<meeting>the second ACM international conference on web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.22,166.13,448.96,10.21;9,72.00,181.13,258.02,10.21" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,270.30,166.13,258.64,10.21">Discovery of inference rules for question-answering</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,72.00,181.13,149.16,10.21">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="343" to="360" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.97,196.13,447.94,10.21;9,72.00,211.13,328.45,10.21" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,243.94,196.13,294.97,10.21;9,72.00,211.13,250.48,10.21">SiteQ: Engineering High Performance QA System Using Lexico-Semantic Pattern Matching and Shallow NLP</title>
		<author>
			<persName coords=""><forename type="first">Gary</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Geunbae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,336.88,211.13,26.39,10.21">TREC</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.97,226.13,448.42,10.21;9,72.00,241.13,467.36,10.21;9,72.00,256.13,238.36,10.21" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,192.96,226.13,337.60,10.21">Question answering passage retrieval using dependency relations</title>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,72.00,241.13,467.36,10.21;9,72.00,256.13,173.02,10.21">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.29,271.13,366.55,10.21" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,168.68,271.13,167.58,10.21">Data-Intensive Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,349.91,271.13,26.39,10.21">TREC</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,89.47,286.13,450.03,10.21;9,72.00,301.13,466.85,10.21;9,72.00,316.13,287.88,10.21" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,314.67,286.13,224.83,10.21;9,72.00,301.13,85.97,10.21">Learning surface text patterns for a question answering system</title>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,172.88,301.13,365.97,10.21;9,72.00,316.13,46.51,10.21">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.29,331.13,430.34,10.21" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,226.88,331.13,213.21,10.21">IBM&apos;s Statistical Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,454.05,331.13,26.40,10.21">TREC</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.97,346.13,451.24,10.21;9,72.00,361.13,276.91,10.21" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,298.98,346.13,167.59,10.21">MUC-7 named entity task definition</title>
		<author>
			<persName coords=""><forename type="first">Nancy</forename><surname>Chinchor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patricia</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,478.07,346.13,61.14,10.21;9,72.00,361.13,241.00,10.21">Proceedings of the 7th Conference on Message Understanding</title>
		<meeting>the 7th Conference on Message Understanding</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.97,376.13,450.65,10.21;9,72.00,391.13,441.58,10.21" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,180.89,376.13,357.72,10.21;9,72.00,391.13,26.37,10.21">Finding the right facts in the crowd: factoid question answering over social media</title>
		<author>
			<persName coords=""><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,111.98,391.13,334.05,10.21">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.08,406.13,445.28,10.21;9,72.00,421.13,467.60,10.21;9,72.00,436.13,309.89,10.21" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,212.50,406.13,326.86,10.21;9,72.00,421.13,98.64,10.21">A regression framework for learning ranking functions using relative relevance judgments</title>
		<author>
			<persName coords=""><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,184.85,421.13,354.75,10.21;9,72.00,436.13,244.56,10.21">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.08,451.13,441.87,10.21;9,72.00,466.13,467.23,10.21;9,72.00,481.13,291.55,10.21" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,224.53,451.13,314.43,10.21;9,72.00,466.13,85.97,10.21">The structure and performance of an open-domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,172.88,466.13,366.35,10.21;9,72.00,481.13,49.88,10.21">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.83,496.13,444.29,10.21;9,72.00,511.13,169.89,10.21" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,202.27,496.13,324.56,10.21">The use of predictive annotation for question answering in TREC8</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,72.00,511.13,101.49,10.21">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,95.58,526.13,443.33,10.21;9,72.00,541.13,450.03,10.21" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,291.59,526.13,247.32,10.21;9,72.00,541.13,112.33,10.21">Selective search: Efficient and effective search of large textual collections</title>
		<author>
			<persName coords=""><forename type="first">Anagha</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,196.28,541.13,245.02,10.21">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
