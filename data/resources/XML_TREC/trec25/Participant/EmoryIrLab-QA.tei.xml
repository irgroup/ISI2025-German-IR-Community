<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.13,112.05,291.73,15.12;1,79.30,133.97,453.39,15.12;1,164.81,155.88,282.37,15.12">Emory University at TREC LiveQA 2016: Combining Crowdsourcing and Learning-To-Rank Approaches for Real-Time Complex Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,179.06,188.37,79.56,10.48"><forename type="first">Denis</forename><surname>Savenkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Emory University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,331.46,188.37,89.58,10.48"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
							<email>eugene@mathcs.emory.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Emory University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.13,112.05,291.73,15.12;1,79.30,133.97,453.39,15.12;1,164.81,155.88,282.37,15.12">Emory University at TREC LiveQA 2016: Combining Crowdsourcing and Learning-To-Rank Approaches for Real-Time Complex Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AC1C38970C46793E00CEEEDF061ACA50</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the two QA systems we developed to participate in the TREC LiveQA 2016 shared task. The first run represents an improvement of our fully automatic real-time QA system from LiveQA 2015, Emory-QA. The second run, Emory-CRQA, which stands for Crowd-powered Real-time Question Answering, incorporates human feedback, in real-time, to improve answer candidate generation and ranking. The base Emory-QA system uses the title and the body of a question to query Yahoo! Answers, Answers.com, WikiHow and general web search and retrieve a set of candidate answers along with their topics and contexts. This information is used to represent each candidate by a set of features, rank them with a trained LambdaMART model, and return the top ranked candidates as an answer to the question. The second run, Emory-CRQA, integrates a crowdsourcing module, which provides the system with additional answer candidates and quality ratings, obtained in near real-time (under one minute) from a crowd of workers When Emory-CRQA receives a question, it is forwarded to the crowd, who can start working on the answer in parallel with the automatic pipeline. When the automatic pipeline is done generating and ranking candidates, a subset of them is immediately sent to the same workers who have been working on answering the questions. Workers then rate the quality of all human-or system-generated candidate answers. The resulting ratings, as well as original system scores, are used as features for the final re-ranking module, which returns the highest scoring answer. The official run results of the tasks indicate promising improvements for both runs compared to the best performing system from LiveQA 2015. Additionally, they demonstrate the effectiveness of the introduced crowdsourcing module, which allowed us to achieve an improvement of ∼20% in average answer score over a fully automatic Emory-QA system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The results of the previous iteration of TREC LiveQA shared task demonstrated that there is still significant room for improvement for automatic QA systems to be able to satisfy various user information needs. Even the winning system was able to return a fair or better answer to only half of the questions, which means that the other half of the users would get no benefit from system responses <ref type="bibr" coords="1,406.27,564.46,9.96,8.74" target="#b0">[1]</ref>. There are many directions to try to bridge this gap, and improve the core algorithms inside question answering systems, such as data sources, candidate extraction, ranking and answer generation. The analysis of answer ratings revealed that automatic QA systems often return a response that is covering a different topic and is non-relevant to the user question <ref type="bibr" coords="1,135.30,612.28,9.96,8.74" target="#b1">[2]</ref>. We, as humans, on the other hand, have no problems understanding such questions, and can usually make relevant suggestions or recommendations even if we do not know the right answer to the question. For TREC LiveQA 2016 we tried to approach the problem of improving question answering performance of our system by both improving the technical aspects and exploring if it is possible to put human workers in the loop and leverage the power of crowdsourcing for real-time question answering. As a basis we took the architecture of Emory QA system, built for the last year shared tasks <ref type="bibr" coords="1,457.97,672.05,9.96,8.74" target="#b1">[2]</ref>. The base automatic system, Emory-QA was improved in a number of ways:</p><p>• Extended the set of data sources, which now includes Yahoo! Answers, Answers.com, WikiHow and general web search. • Extended a set of features to match question against candidate answer text, topic and context.</p><p>• Switched to a non-linear ranking model (LambdaMART), trained on the qrels available from LiveQA 2015.</p><p>Additionally, we introduced a new system, Emory-CRQA, which aims to target the problems of lack of proper answer candidates and ranking errors by incorporating crowdsourcing into the ranking process. For each question, we asked workers to provide their answer if possible, and in addition to rate a subset of candidates generated by the system or other workers. In our preliminary study, the crowd was able to provide reliable input for both types of feedback under a one minute time-limit <ref type="bibr" coords="2,424.48,409.10,9.96,8.74" target="#b2">[3]</ref>. The following sections describe the two Emory QA systems in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>The architecture of our question answering system is presented on Figure <ref type="figure" coords="2,395.71,475.82,3.87,8.74" target="#fig_0">1</ref>. For the official TREC LiveQA 2016 run, we submitted two variations of our question answering system: with and without the crowdsourcing module. Similarly to the previous year our fully automatic system, Emory-QA, uses a set of vertical and general web search components to generate a set of candidate answers, which are represented by a set of features and ranked by a trained model, and the top scoring candidate is returned as the answer to the question. While this approach works reasonably good, there are still a big number of questions, where no appropriate candidates are generated, or the trained model was not able to rank them appropriately. Therefore, we decided to employ crowdsourcing and test whether it is possible to improve the performance of near real-time question answering system using worker contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Candidate generation</head><p>Each question issued to a QA system in TREC LiveQA consists of 3 main parts: title, body and category (Figure <ref type="figure" coords="2,132.06,629.70,3.87,8.74" target="#fig_1">2</ref>). The analysis of our system performance from last year TREC LiveQA shared task <ref type="bibr" coords="2,529.49,629.70,10.52,8.74" target="#b1">[2]</ref> demonstrated the effectiveness of reusing answers from similar questions, previously posted to CQA websites. Therefore, we decided to extend a set of vertical platforms and included Answers.com<ref type="foot" coords="2,466.21,652.03,3.97,6.12" target="#foot_0">1</ref> and WikiHow<ref type="foot" coords="2,532.76,652.03,3.97,6.12" target="#foot_1">2</ref> . To generate candidate answers from these platforms we first formulate one or more queries to the corresponding platforms search interfaces. The questions posted by users vary from short and concise to very verbose. Therefore, we use multiple query generation strategies, designed to increase the chances to find a good match:</p><p>• Question title, which most often captures the gist of the question • Two longest question sentences (detected by the presence of the question word at the beginning or question mark at the end of a sentence) from the title and body of the question. In some cases the real user question is hidden inside the body, while the title provides only the overall topic of the question.</p><p>• Concatenation of the question word, verbs and top-5 terms from the question title by inverse document frequency <ref type="foot" coords="3,138.72,336.59,3.97,6.12" target="#foot_2">3</ref> . This strategy targets over-specific questions (Figure <ref type="figure" coords="3,387.44,338.16,3.87,8.74" target="#fig_1">2</ref>), which often retrieve few if any search results.</p><p>Each of the generated queries is issued to a CQA platform search interface and top-10 retrieved questions along with the corresponding answers are added to the candidate answers pool.</p><p>Many user information needs are still unique or have unique details, which makes it hard to find a similar record in a CQA archive. Therefore, as in the previous year, we use standard web search <ref type="foot" coords="3,475.92,404.33,3.97,6.12" target="#foot_3">4</ref> . The abovementioned strategies are used once again to generate search queries, which are issued to a web search API to retrieve top-10 potentially relevant documents. To extract the content blocks of the web page our system uses a method similar to <ref type="bibr" coords="3,180.48,441.77,9.96,8.74" target="#b3">[4]</ref>. Such blocks of web page content are added to the candidate answers pool along with certain meta-information.</p><p>The meta-information we extract for CQA-and web-based candidates helps the ranking module to score candidate answers relevance. For regular web page paragraphs, it is useful to know the topic of the page (e.g., its title) and the context (e.g., text that immediately precedes the paragraph in the document). For CQA answers, our system stores the text of the corresponding question title, body and category. For convenience, we will refer to this question title and web page title as "answer topic", while the body of the retrieved question and the preceding text block for web candidates as "answer context".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Candidate ranking</head><p>To estimate the relevance of each answer candidate, we represent them with a set of features (Table <ref type="table" coords="3,528.37,571.74,3.87,8.74" target="#tab_0">1</ref>). On the highest level, we had three groups of features: answer text statistics, candidate source and textual match features. In the first group, we computed the lengths of candidates in characters, words and sentences; average length of words; number of non-alphanumeric characters; number of question marks and number of verbs. These features are designed to capture the readability of the question, as well as its informativeness in general (e.g., a candidate with many question marks is likely to just state a question). The answer source features are binary features, that can help the model to distinguish passages extracted from Yahoo! Answers, Answers.com, WikiHow or regular web search, and possibly learn to prefer one type to the other in certain circumstances. Finally, the match-based feature group is designed to capture semantic similarity between the question and a candidate answer. We have multiple pieces of text on both question and answer sides, e.g., title and body of the question; text, topic and context of an answer. All our match-based features are computed for different combinations of these texts. This solution is inspired by the clues-based approach of CMU OAQA system from TREC LiveQA 2015 <ref type="bibr" coords="4,278.14,99.07,9.96,8.74" target="#b4">[5]</ref>, and confirmed with analysis of the results from last year. Fragments with high textual similarity to the question often simply restate it and do not provide any useful information, which can often be found in the following passage. Our text matching features can be split into two subgroups: n-gram overlap and IR score features. Particularly, we compute uni-, bi-and trigram based cosine similarity, as well as the longest span of matching terms for text fragments. As IR metric we chose to use BM25 score, which was demonstrated to be a strong baseline in previous research <ref type="bibr" coords="4,448.83,158.84,9.96,8.74" target="#b5">[6]</ref>.  The next step is to rank candidate answers, and for that we used a LambdaMART model <ref type="bibr" coords="4,482.10,411.50,9.96,8.74" target="#b6">[7]</ref>. In a fully automated scenario the top ranked candidate was returned as the final answer to the given question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Training</head><p>To train our ranking LambdaMART model we used the labeled data from TREC LiveQA 2015 <ref type="foot" coords="4,505.88,468.16,3.97,6.12" target="#foot_4">5</ref> . This dataset contains 1087 questions along with labeled responses submitted by automatic system during the shared task last year. Each answer was rated by professional NIST assessors on a scale from 1 (bad) to 4 (excellent). Most of the rated answers also include source URLs, which we used to download the original web pages and extract topic and context for the candidate. We trained LambdaMART model to optimize the NDCG metric using RankLib library<ref type="foot" coords="4,249.74,527.93,3.97,6.12" target="#foot_5">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Emory-CRQA: Crowdsourcing module</head><p>TREC LiveQA 2015 demonstrated that automatic systems still struggle to answer many of real user questions, e.g., the winning system was able to return a fair or better answer to roughly half of the questions, and only ∼18% of the answers were excellent. Complimentary to improving the technical side of the systems, we decided to explore crowdsourcing as one of the ways to help the system to deal with these questions. Preliminary analysis <ref type="bibr" coords="4,163.62,623.60,10.52,8.74" target="#b2">[3]</ref> showed that under a limited time crowd workers can provide reliable answer quality ratings and in some cases even write answers to the questions. Therefore, we integrated the crowdsourcing module into our QA system. Emory-CRQA utilizes a crowd of workers to generate additional and rate existing answer candidates, while still operating under a one minute time limit.</p><p>To handle the real-time aspect of the task, Emory-CRQA starts a timer after it receives a question and waits to get all worker contributions for 50 seconds to leave itself 10 seconds to prepare and send back the final response. The pipeline of Emory-CRQA is pictured on Figure <ref type="figure" coords="5,362.28,75.16,4.98,8.74" target="#fig_0">1</ref> and Figure <ref type="figure" coords="5,419.82,75.16,4.98,8.74" target="#fig_3">3</ref> presents the user interface of our crowdsourcing module. The overall algorithm for obtaining crowdsourcing input is:</p><p>1. When a system receives a question, it is posted to workers, who have 50 seconds to provide their input 2. Workers are asked to write an answer if they can provide one (optional) 3. Otherwise they need to wait for answer candidates to appear 4. When a system is done generating and ranking candidates, it posts top-7 answers <ref type="foot" coords="5,457.46,422.66,3.97,6.12" target="#foot_6">7</ref> for rating (which usually happens ∼ 15 seconds after the question is posted)</p><p>5. Workers receive a list of answers and rate them until the timer expires. Answers, provided by the workers, are also rated by other workers. Each answer is rated on a scale from 1 to 4, using the official TREC LiveQA rating scale:</p><p>• 1 -Bad: contains no useful information</p><p>• 2 -Fair: marginally useful information</p><p>• 3 -Good: partially answers the question</p><p>• 4 -Excellent: fully answers the question 6. The worker interface displays 3 answers at a time, and when an answer gets rated, it disappears and its place is taken by another answer from the pool. The interface displays only the first 300 characters of the answer, which was experimentally shown to be enough on average to make a good judgment. Full answer can be revealed upon clicking the "show all" link.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">When the question expires, it disappears, and workers wait for the next question</head><p>The workers were hired on Amazon Mechanical Turk<ref type="foot" coords="5,320.66,639.48,3.97,6.12" target="#foot_7">8</ref> . Since the latency of hiring new workers for the task can be high, we adapted the retainer model <ref type="bibr" coords="5,289.38,653.01,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="5,303.49,653.01,7.01,8.74" target="#b8">9]</ref>, i.e., workers were paid by time, during which they had to stay on our interface and complete the tasks for 15 minutes. After 15 minutes expire, the task is submitted and the worker got paid. During 24 hours of the official shared task run we posted 10 tasks every 15 minutes, with the goal to have around 10 workers for each question. However, not all tasks are accepted immediately, therefore the actual number of workers per question fluctuates and can be greater than 10. When a worker first gets to our crowdsourcing interface, she is shown task instructions (Table <ref type="table" coords="6,484.72,99.07,4.43,8.74">2</ref>) and asked to wait for the questions to arrive. The workers were paid $1.00 for a 15 minutes task, no matter how many questions they received. Each 15 minutes we had 10 assignments for different workers, which translates to $15.00 per 15 minutes. Since not all assignments were accepted, overall, our experiment cost $0.88 per question, and we show some ideas to decrease these costs in our HCOMP paper <ref type="bibr" coords="6,422.90,146.89,14.61,8.74" target="#b9">[10]</ref>.</p><p>We should note, that the setup of TREC LiveQA shared task was favorable for the retainer crowdsourcing model, as the questions were arriving almost every minute uniformly over 24 hour period, which diminishes the waiting and worker idleness problems <ref type="bibr" coords="6,255.46,182.75,14.61,8.74" target="#b10">[11]</ref>.</p><p>Instructions 1. This HIT will last exactly 15 minutes 2. Your HIT will only be submitted after these 15 min. 3. In this period of time you will receive some questions, which came from real users on the Internet 4. Each question has a time limit after which it will disappear, and you will need to wait for the next one 5. If you know the answer to the question, please type it in the corresponding box 6. At some point, several candidate answers will appear at the bottom of the page 7. Please rate them from 1 (bad) to 4 (excellent) 8. Do not close the browser or reload the page as this will reset your assignment.</p><p>Table <ref type="table" coords="6,127.57,324.30,3.87,8.74">2</ref>: Crowdsourcing task instructions, displayed to the user when she first gets to the task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Emory-CRQA: Answer re-ranking and selection</head><p>The last stage in Emory-CRQA is answer re-ranking, which aggregates all the information received from the crowdsourcing and produces the final answer to the question. The input of the re-ranking module is a set of candidate answers with quality ratings provided by the crowd workers. During the TREC LiveQA 2016 run we used a simple heuristic model, which ordered the answers by the average rating, and returned either the top candidate, if its average score was ≥ 2.5, or the longest worker contributed answer. This heuristic was inspired by the observation that user contributed answers are usually relevant to the question and are better preferred to automatically generated answer, which covers completely different topic. However, some workers add "noise" to the candidate pool by submitting short and non-informative answers like "yes", "no" and "I don't know", etc. After the shared task was over, we collected all the questions, candidate answers and all crowdsourcing contributions and conducted a series of experiments with more sophisticated re-ranking techniques. The details as well as additional analysis can be found in our HCOMP paper <ref type="bibr" coords="6,464.67,503.14,14.61,8.74" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>From the final 24 hour run of the systems, responses to 1015 questions were judged by the organizers on a scale from 1 to 4: 4: Excellent -a significant amount of useful information, fully answers the question 3: Good -partially answers the question 2: Fair -marginally useful information 1: Bad contains no useful information for the question -2: the answer is unreadable (only 15 answers from all runs were judged as unreadable)</p><p>Similar to the previous year, the reported metrics were:</p><p>• avg-score(0-3): average score over all questions, where scores are translated to 0-3 range. This metric considers "Bad", unreadable answers and unanswered questions as having score 0</p><p>• succ@i+: the fraction of questions with the answer score of i or greater (i=1..4)</p><p>• p@i+: the fraction of provided answers with score i or greater (i=2..4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># ans</head><p>avg score (0-3) succ@2+ succ@3+ succ@4+ p@2+ p@3+ p@4+   <ref type="table" coords="7,99.44,239.80,4.98,8.74" target="#tab_2">3</ref> provides the results of our fully automatic and CRQA systems as well as average performance over all submitted systems.</p><p>One significant improvement over the last year was made towards system reliability, which resulted in higher number of answers, and therefore higher average score. The quality of the answers are not directly comparable between the years as the data is different, however, higher numbers for all the metrics adds some optimism. Crowdsourcing module turned out to be quite effective and allowed to boost the system average answer score by ∼ 20% from 1.0542 to 1.2601.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>During TREC LiveQA 2016 we submitted two runs with the fully automated system and the system that used crowdsourcing to collect and rate candidate answers. The results of the shared task clearly demonstrated the effectiveness of crowdsourcing even in near real-time scenario, which boosted the answer scores by ∼20% on average. This initial success opens up a lot of opportunities to explore different kind of inputs and feedback from the crowd workers to improve the performance of a general QA system. In the future research there are a number of problems that needs to be addressed, i.e., how to optimize the costs associated with the crowdsourcing module <ref type="bibr" coords="7,190.84,449.74,14.61,8.74" target="#b9">[10]</ref>, how to use crowdsourcing feedback more efficiently, e.g., make the system learn from crowd feedback over time and what kind of feedback from the workers is more efficient, etc. One can imagine the system that automatically decides when and what type of information to request from the workers (or from the user herself in a chat scenario), based on the predictions about different component performances, and then adjust its internal models to learn from the feedback provided.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,183.02,256.07,245.97,8.74;2,72.00,72.00,460.00,172.55"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of our question answering system</figDesc><graphic coords="2,72.00,72.00,460.00,172.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,97.55,197.09,416.91,8.74;3,72.00,72.00,460.01,113.56"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of a question from Yahoo! Answers community question answering platform</figDesc><graphic coords="3,72.00,72.00,460.01,113.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,107.58,182.03,85.24,8.77;4,107.58,194.41,175.59,8.74;4,107.58,206.37,181.77,8.74;4,107.58,218.32,189.31,8.74;4,107.58,230.28,129.12,8.74;4,107.58,242.23,85.90,8.74;4,107.58,254.56,72.96,8.77;4,107.58,266.94,396.85,8.74;4,107.58,278.89,60.88,8.74;4,107.58,291.22,82.65,8.77;4,107.58,303.60,396.85,8.74;4,107.58,315.56,172.24,8.74;4,107.58,327.51,396.85,8.74;4,107.58,339.47,125.75,8.74;4,107.58,351.79,138.07,8.77;4,107.58,364.17,376.28,8.74"><head></head><label></label><figDesc>Answer statistics -Length in chars, words and sentences -Average number of words per sentence -Fraction of non-alphanumeric characters -Number of question marks -Number of verbs Answer source -Binary feature for each of the search verticals: Web, Yahoo! Answers, Answers.com, WikiHow.com N-gram matches -Cosine similarities using uni-, bi-and tri-gram representations of the question title and/or body, and answer text, topic or context -The lengths of longest spans of matched terms between question title and/or body, and answer text, topic or context Information Retrieval score -BM25 scores between question title and/or body, and answer text, topic or context</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,115.44,320.55,381.12,8.74;5,83.70,107.54,444.61,201.48"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: User Interface for workers in our Crowd-Powered Question Answering system</figDesc><graphic coords="5,83.70,107.54,444.61,201.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.14,386.49,467.72,8.74"><head>Table 1 :</head><label>1</label><figDesc>The list of candidate answer ranking features used by the automatic module of our CRQA system</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,171.60,468.00,76.94"><head>Table 3 :</head><label>3</label><figDesc>Results of the TREC LiveQA 2016 evaluation of Emory University QA systems and average results of all systems. For convenience we also provide our and best overall results from LiveQA 2015 shared task, but since the data is different the results between years are not directly comparable. Percents show the improvements of crowdsourcing over fully automatic Emory-QA system Table</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,87.24,696.45,97.49,6.99"><p>http://www.answers.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,87.24,705.95,99.03,6.99"><p>http://www.wikihow.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,87.24,698.26,379.25,6.99"><p>IDF of terms are estimated using Google N-gram corpus: https://catalog.ldc.upenn.edu/LDC2006T13</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,87.24,707.77,271.06,6.99"><p>https://www.microsoft.com/cognitive-services/en-us/bing-web-search-api</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,87.24,702.31,228.19,6.99"><p>https://sites.google.com/site/trecliveqa2016/liveqa-qrels-2015</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,87.24,711.81,179.36,6.99"><p>https://sourceforge.net/p/lemur/wiki/RankLib/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,87.24,695.59,452.76,6.99;5,72.00,705.05,40.79,6.99"><p>The number is chosen based on the average number of answers workers could rate under 1 minute in our preliminary analysis<ref type="bibr" coords="5,103.85,705.05,8.94,6.99" target="#b2">[3]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,87.24,714.56,66.58,6.99"><p>http://mturk.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,92.48,552.08,447.52,8.74;7,92.48,564.04,299.98,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,450.17,552.08,89.83,8.74;7,92.48,564.04,72.53,8.74">Overview of the trec 2015 liveqa track</title>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Agichten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,186.03,564.04,175.80,8.74">Proceedings of Text Retrieval Conference</title>
		<meeting>Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,583.39,447.52,8.74;7,92.48,595.35,106.07,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,166.51,583.39,373.49,8.74;7,92.48,595.35,74.96,8.74">Ranking answers and web passages for non-factoid question answering: Emory university at trec liveqa</title>
		<author>
			<persName coords=""><forename type="first">Denis</forename><surname>Savenkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,614.71,447.52,8.74;7,92.48,626.66,447.52,8.74;7,92.48,638.62,22.69,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,339.04,614.71,200.96,8.74;7,92.48,626.66,131.92,8.74">Crowdsourcing for (almost) real-time question answering: Preliminary results</title>
		<author>
			<persName coords=""><forename type="first">Denis</forename><surname>Savenkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Weitzner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,244.80,626.66,290.37,8.74">2016 NAACL Workshop on Human-Computer Question Answering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,657.97,447.52,8.74;7,92.48,669.93,447.52,8.74;7,92.48,681.88,271.98,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,369.96,657.97,170.04,8.74;7,92.48,669.93,32.79,8.74">Boilerplate detection using shallow text features</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Kohlschütter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.18,669.93,393.82,8.74;7,92.48,681.88,47.46,8.74">Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM &apos;10</title>
		<meeting>the Third ACM International Conference on Web Search and Data Mining, WSDM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,701.24,447.52,8.74;7,92.48,713.20,57.06,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,214.94,701.24,307.14,8.74">Cmu oaqa at trec 2015 liveqa: Discovering the right answer with clues</title>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,92.48,713.20,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,75.16,447.52,8.74;8,92.48,87.11,349.04,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,367.06,75.16,172.95,8.74;8,92.48,87.11,130.99,8.74">Learning to rank answers to non-factoid questions from web collections</title>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,231.86,87.11,113.07,8.74">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="383" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,107.04,447.52,8.74;8,92.48,118.99,43.72,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,201.27,107.04,257.06,8.74">From ranknet to lambdarank to lambdamart: An overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,467.98,107.04,36.19,8.74">Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="581" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,138.92,447.52,8.74;8,92.48,150.87,447.52,8.74;8,92.48,162.83,284.24,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,434.51,138.92,105.49,8.74;8,92.48,150.87,191.81,8.74">Crowds in two seconds: Enabling realtime crowd-powered interfaces</title>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,308.96,150.87,231.04,8.74;8,92.48,162.83,166.16,8.74">Proceedings of the 24th annual ACM symposium on User interface software and technology</title>
		<meeting>the 24th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,182.75,447.52,8.74;8,92.48,194.71,447.52,8.74;8,92.48,206.67,447.52,8.74;8,92.48,218.62,141.21,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,384.64,194.71,155.36,8.74;8,92.48,206.67,67.83,8.74">Vizwiz: nearly real-time answers to visual questions</title>
		<author>
			<persName coords=""><forename type="first">Chandrika</forename><surname>Jeffrey P Bigham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanjie</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aubrey</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brandyn</forename><surname>Tatarowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samual</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,183.74,206.67,356.26,8.74;8,92.48,218.62,42.79,8.74">Proceedings of the 23nd annual ACM symposium on User interface software and technology</title>
		<meeting>the 23nd annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,238.55,447.52,8.74;8,92.48,250.50,59.59,8.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,267.66,238.55,272.34,8.74;8,92.48,250.50,27.85,8.74">Crqa: Crowd-powered real-time automatic question answering system</title>
		<author>
			<persName coords=""><forename type="first">Denis</forename><surname>Savenkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,270.43,447.52,8.74;8,92.48,282.38,447.52,8.74;8,92.48,294.34,269.63,8.74;8,384.26,294.34,90.83,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,134.27,282.38,220.24,8.74">Chorus: A crowd-powered conversational assistant</title>
		<author>
			<persName coords=""><forename type="first">Walter</forename><forename type="middle">S</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rachel</forename><surname>Wesley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anand</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,376.73,282.38,163.27,8.74;8,92.48,294.34,237.96,8.74">Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 26th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
