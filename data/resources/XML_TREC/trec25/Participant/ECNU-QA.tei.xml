<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.67,115.96,310.02,12.62;1,147.35,133.89,320.67,12.62;1,205.43,151.82,204.49,12.62">ECNU at 2016 LiveQA Track: A Parameter Sharing Long Short Term Memory Model for Learning Question Similarity</title>
				<funder ref="#_xMDA4Sx">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
				<funder ref="#_Qzv9wuk">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.36,189.49,40.68,8.74"><forename type="first">Weijie</forename><surname>An</surname></persName>
							<email>wjan@ica.stc.sh.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,192.62,189.49,50.09,8.74"><forename type="first">Mengfei</forename><surname>Shi</surname></persName>
							<email>mfshi@ica.stc.sh.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.19,189.49,50.40,8.74"><forename type="first">Xin</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.14,189.49,40.40,8.74"><forename type="first">Yan</forename><surname>Yang</surname></persName>
							<email>yanyang@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.83,189.49,46.50,8.74"><forename type="first">Qinmin</forename><surname>Hu</surname></persName>
							<email>qmhu@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,432.28,189.49,39.71,8.74"><forename type="first">Liang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.67,115.96,310.02,12.62;1,147.35,133.89,320.67,12.62;1,205.43,151.82,204.49,12.62">ECNU at 2016 LiveQA Track: A Parameter Sharing Long Short Term Memory Model for Learning Question Similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD4FF95443BB87F955A8963EFF64B4EF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our system which is evaluated in the TREC 2016 LiveQA Challenge. Same as the last year, the TREC 2016 LiveQA track focuses on "live" question answering for the real-user questions from Yahoo! Answer. In this year, we first apply a parameter sharing Long Short Term Memory(LSTM) network to learn a high embedding of question representation. Then we combine the question representation with the key words information to strengthen the representation of semantic-similar questions, followed by calculating the question similarity with a simple metric function. Our approach outperforms the average score of all submitted runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open-domain question answering is a very important research topic in recent years. The TrecQA task in the previous years mainly dealt with the factual questions. Same as the task in 2015 <ref type="bibr" coords="1,292.46,471.54,9.96,8.74" target="#b0">[1]</ref>, the LiveQA task in this year deals with the most recent questions submitted on the Yahoo! Answers<ref type="foot" coords="1,396.10,481.92,3.97,6.12" target="#foot_0">1</ref> site that have not yet been answered by humans, and the participants are requested to respond in one minute with a limitation of length less than 1000.</p><p>The major challenges of this task are: 1)there is not a big enough corpus to answer any questions from diverse users. 2)the given question dose not directly share the lexical units with its semantic-similar question. 3)the question, especially the question body, contains a large amount of irrelevant information.</p><p>Therefore, we search in the community question answering site, such as the Yahoo! answers and Answers.com 2 , to collect the semantic-similar question-answer pairs. The subsequent step is to measure the similarity of the original question and the candidate questions. Previous work on question answering task using deep learning technologies achieved the state-of-the-art result <ref type="bibr" coords="2,416.93,118.99,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,429.68,118.99,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,439.67,118.99,7.01,8.74" target="#b4">5]</ref>. These methods have a common purpose to learn the precise semantic representation of questions or answers. We apply a parameter sharing Long Short Term Memory(LSTM) network to learn a highly embedding of question representation, and we combine the question representation with the key words information to strengthen the semantic representation of question. Finally, we use a simple metric function to calculate the question-similarity. Our approach outperforms the average score of all submitted runs. The rest of the paper is organized as follows: Section 2 describes the whole architecture of our system. Section 3 presents and analyzes the results. The conclusion are drawn in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>Figure <ref type="figure" coords="2,167.31,341.96,4.98,8.74" target="#fig_0">1</ref> shows the architecture of our system, which consists of three parts: Online Searcher, Similarity Metric and Answer Re-rank.</p><p>The Online Searcher searches the similarity question from specific community question and answering(CQA) site. Then we obtain the set of candidate questionanswer pairs. The Similarity Metric measures the similarity of the original question and each candidate question obtained from previous step. The Answer Rerank part combines the similarity of each question pairs with the external information such as the order in the source CQA site, and then responds the answer of the highest candidate question as the best answer. In this step, we have a hypothesis that the answer of the most similar question is able to better answer the original question than the others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Online Searcher</head><p>Given each question Q = title, body, category, qid , we first search the web resources to generate the candidate question-answer pairs set. In this step we want to get the similar question-answer pairs as much as possible. A high quality and comprehensive candidate set determines the answer quality. At the beginning, we have selected more than a dozen CQA sites. With considering the following reasons:</p><p>-General answer quantity and quality -Access to data easily -Less time consuming -Network stability</p><p>We mainly construct two specific searchers, one for Yahoo! Answer and the other for Answers.com. Due to the different structure of the meta QA pair data in the two site, we define a common container for the QA pair. It mainly contains the following parts:</p><p>-Titles, the question titles -Body, the question body, usually the description of question -Best Answer, the best answer component in the Yahoo! answer question page, the only one answer in the answers.com question page -Source, which site the question-answer pair from -Weight, the original order in the search result list, more specifically, when we search a question, the site return a list of similar question, it is more like a location weight information</p><p>We do not pre-process the original question such as stemming and removing the stop words in this step. We find that if we remove the stop words in the question, the semantic features will be lost. For example, for the question "Do you feel Obama made the economy better or worse off?", if we remove the stop words "do", "you", "the", "or" and "off", it becomes "feel Obama economy better worse". It could be more useful for the key word matching, but meaningless for the similar question retrieval in the CQA site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Similarity Metric</head><p>Given the original question Q = title, body, category, qid and the candidate QA pairs set QA = {C 1 , C 2 , ..., C n }, C i = {title, body, BestAnswer, Source, W eight}, we measure the similarity of Q title and C i-title for each C i in QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Parameter Sharing LSTM</head><p>Long Short Term Memory(LSTM): Recurrent Neural Networks(RNN) have been widely used in modeling the variable length sequence. In order to deal with the vanish of the gradient during the long distance transmission, <ref type="bibr" coords="3,412.06,656.12,10.51,8.74" target="#b2">[3]</ref> proposed the LSTM model. Given an input sequence x = {x 1 , x 2 , ..., x n }, at each time step the LSTM cell updates the hidden vector h(t) using the following equations:</p><formula xml:id="formula_0" coords="4,240.25,149.93,240.35,84.37">i t = σ(W i x t + U i h t-1 + b i ) f t = σ(W f x t + U f h t-1 + b f ) o t = σ(W o x t + U o h t-1 + b o ) c t = tanh(W c x t + U c h t-1 + b c ) c t = i t * c t + f i * c t-1 h t = o t * tanh(c t )<label>(1)</label></formula><p>LSTM uses these gate operation to control the flow of information through the cell. Usually the last hidden unit output can be regarded as the representation of the whole sentence.</p><p>Parameter Sharing LSTM(PS-LSTM): In our work, we build two LSTM networks, one for modeling the original question and the other for modeling the candidate question. We simply use a last pooling layer acting on the hidden output, which means we select the output of the last step as the representation of the question. When we get both representations, we calculate the similarity with the cosine metric. During the training step, we use the same parameters in both LSTM networks such as the weight matrices  Suppose that h l is the representation of the original question title(Q title ), h r is the representation of a specific candidate question title(C i-title ). We define the loss function with the maximum cross entropy as:</p><formula xml:id="formula_1" coords="4,350.81,362.82,117.26,9.65">W i , W f , W o , W c , U i , U f , U o ,</formula><formula xml:id="formula_2" coords="4,205.01,603.76,275.59,41.20">L = - 1 n x [log(p x ) * y + log(1 -px) * (1 -y)] p x = 0.5 + 0.5 * cosine(h l , h r ) (2)</formula><p>where p x is the predictive similarity, y is the real label in the training data, it can either be 1 for similarity or 0 dissimilarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Combining the Key Word Information</head><p>As shown in <ref type="bibr" coords="5,195.00,177.21,9.96,8.74" target="#b5">[6]</ref>, the key word information improves the answer selection result. The key word in the question characterized the main topics. Although two question shared almost same representation, they may focus on different topics, because of the shortcomings of word representation. For example, "Japan" and "China" are two words very close in embedding space, but when these two words appear in the question, they may focus on different place. In order to mitigate this weak point, we employ the BM25 retrieval model for calculating the key word matching score.</p><p>We build the index in both question title and body in order to capture more information. In the PS-LSTM model, we concern the similarity between the titles. So in this step we consider the information of question body and title to make up the deficiencies of the previous step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Re-Ranking the results</head><p>After getting the candidate relevance score, we re-rank the candidate questionanswer pairs by the Weight attributes and rejudge the answer of each C i . Generally, according to the manual observation, we consider that the candidate question-answer pairs from Yahoo! Answer site are more valuable than Answers.com. Also we set a higher weight to the front question-answer pair according to the display order in the search result list. As mentioned in Section 2.1, the Source and Weight attributes characterize these impacts. Then we define a simple re-rank scoring function as shown in Equation <ref type="formula" coords="5,405.82,450.61,3.87,8.74">3</ref>. As we make a hypothesis that the answer of the most similar question is able to better answer the original question than the others, we judge the answer whether it is eligible. More specifically, we restrict the answer length less than 1000 and remove unreadable characters in the answer text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Result and Evaluation</head><p>Network Setup We use the pre-trained word embedding released from the glove project<ref type="foot" coords="5,165.51,624.35,3.97,6.12" target="#foot_2">3</ref> for projecting the word into 300 dimensions space. And the network parameters are randomly initialized using a Gaussian distribution(µ = 0, σ = 0.2).</p><p>The hidden layer size is tuned to 128. We train our model using a batch size 512, and the maximum sentence length is set to 40. In order to adapt different sentence lengths in each batch, we use the mask strategy in each batch train step.</p><p>Evaluation Metric For the final test of Live QA Track, the results are judged by TREC editors firstly using the 4-level scale as follows:</p><p>-4: Excellent a significant amount of useful information, fully answers the question -3: Good partially answers the question -2: Fair marginally useful information -1: Bad contains no useful information for the question --2: the answer is unreadable</p><p>The performance measures are:</p><p>-avg-score(0-3) average score over all queries (transferring 1-4 level scores to 0-3, hence comparing 1-level score with no-answer score, also considering -2-level score as 0) -succ@i+ number of questions with i+ score (i=2..4) divided by number of all questions -prec@i+ number of questions with i+ score (i=2..4) divided by number of answered only questions Table <ref type="table" coords="6,162.27,382.68,4.98,8.74" target="#tab_0">1</ref> shows the official result of our system and the average score of all runs in Trec LiveQA 2016 track. Our result outperform the average score in the 4 metric aspects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper describes our system architecture and three key components which are evaluated in the TREC 2016 LiveQA Challenge. We apply a PS-LSTM to learn a high embedding of question representation. Also we combine the key word information to enhance the semantic similarity matching. Finally, we define a simple score function that combines some manual discovered weight information.</p><p>In the future, we will focus on the question representation with the attention method. Also we will continue on the answer selection and auto-generation methods, such as merging the candidate answers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,228.75,630.00,157.85,7.89;2,134.77,507.23,360.00,108.00"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of our system.</figDesc><graphic coords="2,134.77,507.23,360.00,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,469.73,362.82,10.36,9.65;4,134.77,374.78,288.26,9.65"><head></head><label></label><figDesc>U c and the bias vectors b i , b f , b o , b c ,. The model is shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,190.92,528.74,233.51,7.89;4,199.68,405.97,216.00,108.00"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of parameter sharing LSTM model</figDesc><graphic coords="4,199.68,405.97,216.00,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,182.69,518.03,251.63,9.65;5,467.86,518.13,12.73,8.74;5,134.77,549.78,357.47,9.65;5,134.77,561.73,228.13,9.65"><head></head><label></label><figDesc>score = (αS P S-LST M + (1 -α)S KW M ) * weight * source (3) where α is a tuned value. Because the PS-LSTM model predict a value S P S-LST M ∈ [0, 1], we normalized the S KW M score into [0, 1] too.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,158.42,436.48,298.52,60.34"><head>Table 1 .</head><label>1</label><figDesc>Official TREC 2016 LiveQA track evaluation results.</figDesc><table coords="6,158.42,460.27,298.52,36.56"><row><cell>RunID</cell><cell cols="5">Answers avgScore(0-3) succ@2+ succ@3+ succ@4+</cell></row><row><cell cols="2">ECNU-ECNU 834</cell><cell>0.8365</cell><cell>0.4108</cell><cell>0.2906</cell><cell>0.1350</cell></row><row><cell cols="3">AVG SCORE 771.0385 0.5766</cell><cell>0.3042</cell><cell>0.1898</cell><cell>0.0856</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,645.84,76.41,7.86"><p>answers.yahoo.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,656.80,56.70,7.86"><p>answwers.com   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,656.80,160.40,7.86"><p>http://nlp.stanford.edu/projects/glove/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This research is funded by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61602179</rs>) and the <rs type="funder">Science and Technology Commission of Shanghai Municipality</rs> (No.<rs type="grantNumber">15PJ1401700</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Qzv9wuk">
					<idno type="grant-number">61602179</idno>
				</org>
				<org type="funding" xml:id="_xMDA4Sx">
					<idno type="grant-number">15PJ1401700</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,222.29,342.25,7.86;7,146.91,233.25,333.68,7.86;7,146.91,244.21,333.68,7.86;7,146.91,255.17,56.30,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,146.91,233.25,151.64,7.86">Overview of the trec 2015 liveqa track</title>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,317.56,233.25,163.03,7.86;7,146.91,244.21,27.87,7.86">The Twenty-Fourth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>TREC 2015) Proceedings. National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,266.12,342.24,7.86;7,146.91,277.08,333.68,7.86;7,146.91,288.04,333.67,7.86;7,146.91,299.00,333.67,7.86;7,146.91,309.96,333.67,7.86;7,146.91,320.92,123.38,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,146.91,277.08,314.61,7.86">Learning hybrid representations to retrieve semantically equivalent questions</title>
		<author>
			<persName coords=""><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luciano</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dasha</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.91,288.04,333.67,7.86;7,146.91,299.00,333.67,7.86;7,196.07,309.96,54.73,7.86">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="694" to="699" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct coords="7,138.35,331.88,342.24,7.86;7,146.91,342.84,125.83,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,323.34,331.88,98.57,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,431.38,331.88,49.21,7.86;7,146.91,342.84,31.76,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,353.80,342.24,7.86;7,146.91,364.75,333.68,7.86;7,146.91,375.71,333.68,7.86;7,146.91,386.67,333.68,7.86;7,146.91,397.63,333.68,7.86;7,146.91,408.59,109.28,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,321.37,364.75,159.22,7.86;7,146.91,375.71,74.75,7.86">Semi-supervised question retrieval with gated convolutions</title>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hrishikesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,245.98,375.71,234.62,7.86;7,146.91,386.67,333.68,7.86;7,146.91,397.63,48.61,7.86">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="1279" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,419.55,342.24,7.86;7,146.91,430.51,278.08,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,327.86,419.55,152.73,7.86;7,146.91,430.51,112.14,7.86">Lstm-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,138.35,441.47,342.24,7.86;7,146.91,452.43,333.68,7.86;7,146.91,463.38,333.67,7.86;7,146.91,474.34,333.68,7.86;7,146.91,485.30,280.65,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,261.53,441.47,219.06,7.86;7,146.91,452.43,124.03,7.86">A long short-term memory model for answer sentence selection in question answering</title>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,292.23,452.43,188.36,7.86;7,146.91,463.38,333.67,7.86;7,146.91,474.34,154.26,7.86;7,355.70,474.34,55.18,7.86">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="707" to="712" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
