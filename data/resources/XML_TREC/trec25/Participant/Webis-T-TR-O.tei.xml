<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,215.98,72.36,177.75,16.84;1,116.59,92.29,376.53,16.84">Webis at TREC 2016: Tasks, Total Recall, and Open Search Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,108.60,138.01,83.72,11.06"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.93,138.01,87.71,11.06"><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.23,138.01,76.67,11.06"><forename type="first">Payam</forename><surname>Adineh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,412.50,138.01,88.61,11.06"><forename type="first">Masoud</forename><surname>Alahyari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,136.81,150.46,83.43,11.06"><forename type="first">Ehsan</forename><surname>Fatehifar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.85,150.46,82.57,11.06"><forename type="first">Arefeh</forename><surname>Bahrami</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.01,150.46,49.16,11.06"><forename type="first">Pia</forename><surname>Fichtl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.77,150.46,65.13,11.06"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,215.98,72.36,177.75,16.84;1,116.59,92.29,376.53,16.84">Webis at TREC 2016: Tasks, Total Recall, and Open Search Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">73915B8DD7B548CBAB6C2DFA06616C36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We give a brief overview of the Webis group's participation in the TREC 2016 Tasks, Total Recall, and Open Search tracks.</p><p>Our submissions to the Tasks track are similar to our last year's system. In the task understanding subtask of the Tasks track, we use different data sources (ClueWeb12 anchor texts, AOL query log, Wikidata, etc.) and APIs (Google, Bing, etc.) to retrieve suggestions related to a given query. For the task completion and ad-hoc subtask, we combine the results of the Indri search engine for the different related queries identified in the task understanding subtask.</p><p>Our system for the the Total Recall track also is similar to our last year's idea with some slight changes in the details; we employ a simple SVM baseline with variable batch sizes equipped with a keyqueries step to identify potentially relevant documents.</p><p>In the Open Search track, we axiomatically re-rank a BM25-ordered result list to come up with a final document ranking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">TASKS TRACK</head><p>The Tasks track has three subtasks: task understanding, task completion, and the ad-hoc for each of which we briefly describe our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Task Understanding</head><p>The goal of the task understanding subtask is to automatically identify related queries for all possible aspects or topics a given user query may cover.</p><p>Like last year <ref type="bibr" coords="1,125.61,552.30,9.22,7.92" target="#b5">[5]</ref>, we implement a two-step approach: (1) collecting for each user query a number of related queries suggested by the different modules listed below, and (2) ranking them to select queries for the final output. The suggested related queries are ranked summing up simple scores that depend on the importance of the different suggestion modules. We derived these scores in a manual pilot experiment in which we assessed the output of the different modules for non-test-set queries (scores are shown in parenthesis below). Google Suggestions (100). Like in the last year, we submit the queries to Google and collect the query suggestions. Bing Suggestions (90). Analogous to the Google suggestions, but using Bing search. Anchor Text Graph (80). We adopted last year's idea of Bennett and White <ref type="bibr" coords="1,146.25,709.21,9.73,7.92" target="#b2">[2]</ref> and used the ClueWeb12 Anchor Text Graph to find similar queries. The assumption is that the texts in HTML anchors (with href attribute) are short descriptions of the documents they link to. Therefore, such anchor texts can be seen as alternative descriptions of the document they link to. Our approach works as follows. In a pre-processing step, we use Mirex <ref type="bibr" coords="1,452.75,270.85,9.73,7.92" target="#b8">[8]</ref> to extract anchor texts for the URLs in the ClueWeb12 and store them with their frequency of being anchor texts in an Apache Lucene 2.4.1 index. Given a query, we retrieve from the index the anchor texts similar to the query, ordered by their frequency. For post-processing, we remove all anchor texts that contain dates, fewer words than the original query (since we want to find more specific queries), or that have a tf -weighted cosine similarity of less than 0.3 to the original query (to remove vastly different suggestions). Google Autocomplete (75). We use the Google autocompletion API to get additional suggestions for related queries. Different to Google suggestions, this API only returns queries that have the original query as a prefix. AOL query log sessions (70). First, we split the AOL query log <ref type="bibr" coords="1,359.29,427.77,14.34,7.92" target="#b11">[11]</ref> into search sessions <ref type="bibr" coords="1,461.37,427.77,9.73,7.92">[4]</ref> (i.e., sets of queries submitted for the same information need by some user). For a given query q, we then retrieve all search sessions that contain a query having a tf -weighted cosine similarity of at least 0.8 compared to q. All queries of such sessions then form potentially related queries with the idea that other users submitted them for some information need related to q. Netspeak Frequent Phrases (60). For a query q, we use Netspeak <ref type="bibr" coords="1,357.27,511.45,14.34,7.92" target="#b13">[13,</ref><ref type="bibr" coords="1,374.96,511.45,11.78,7.92" target="#b14">14]</ref> to find related queries as follows. Let w1 be the first and wn be the last word of q. We then sent the request * w1 * wn * to Netspeak. The query results are the most frequent phrases containing w1 and wn, with * matching zero or more words. The top-10 results (ordered by frequency) are used as potentially related queries. Wikidata (50). Since most queries of the Tasks track come with annotated Freebase entities (ID and name), we submit requests for similar entities/topics to Wikidata <ref type="bibr" coords="1,508.97,595.14,13.52,7.92" target="#b15">[15]</ref>. All retrieved topics are used as potentially related queries. Freebase (30). The same as Wikidata, but using the Wikidata API that allows access to the old Freebase database. ChatNoir Keyphrase Extraction (5). Our last data source for related queries is ChatNoir <ref type="bibr" coords="1,472.01,647.44,13.52,7.92" target="#b12">[12]</ref>. We retrieve the top-10 results and extract the top-10 keyphrases from their main content <ref type="bibr" coords="1,372.62,668.36,9.73,7.92" target="#b9">[9]</ref> using a head noun phrase extractor <ref type="bibr" coords="1,531.25,668.36,9.22,7.92" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>We submitted the query suggestions with the highest scores (sum of the importance values of the modules suggesting a query) in the run webis1. In order to evaluate each module, we also submitted the queries that score highest in the individual modules but were not part of webis1 as runs webis2 and webis3. Since we did not know the pooling depth for the evaluation, we used a round-robin approach to distribute the individually highest scoring suggestions across these two runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Task Completion &amp; Ad-hoc</head><p>The setting of the task completion subtask is as follows: given a user query, return all documents that are relevant/useful to any task a user may be trying to fulfill with the query. For the ad-hoc subtask, a ranked list of documents fitting the user query should be returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>Our runs are on the full ClueWeb12 corpus (category A) and contain the top-3 results returned by the Indri search engine <ref type="bibr" coords="2,82.27,247.37,14.34,7.92" target="#b10">[10]</ref> for the queries we found in the task understanding subtask. We use the same documents and ranking for the task completion and the ad-hoc subtask. Runs webisC1 and webisA1 use the related queries from run webis1; we-bisC2 and webisA2 the ones from webis2; and webisC3 and webisA3 the ones from webis3. We create the list of documents by taking the top-3 documents that the Indri search engine returns for the highest ranking query from the task understanding subtask, then adding the top-3 from the second highest and so on. We then filter out all duplicates in the list, and sent the list to the server for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TOTAL RECALL TRACK</head><p>The objective of the Total Recall track is to return all(!) relevant documents for a given topic without too many irrelevant results. Like last year <ref type="bibr" coords="2,186.64,409.76,9.22,7.92" target="#b5">[5]</ref>, we submit documents in several iterations until a stopping criterion is met. Our two runs are equal in the first iteration and the stopping criterion, but differ in the other iterations.</p><p>In a pre-processing, we index the respective document set using Apache Lucene's BM25 retrieval model with default parameter settings. We also pre-compute the tf•idf scores for each document to use them for training a LibSVM classifier. For each topic (i.e., query) we proceed as follows. First Iteration. We obtain the first 512 results for the user query from Lucene and submit them for evaluation. Subsequent Iterations, Baseline Approach. We use the judgements obtained from the previous iteration(s) to train an SVM-classifier using the LibSVM library. We take the tf •idf vectors of all relevant documents as positive examples and the ones from all irrelevant documents as negative examples, but as most as many as we have positive samples since we expect a majority of the results being irrelevant. The trained SVM is then used to classify all documents that were not yet submitted for evaluation. We rank these documents by the classifier's confidence for the positive class and submit the top-n documents in the current iteration. The value of n may change from iteration to iteration depending on how well the classifier did in the previous iteration:</p><p>• If the ratio of relevant to irrelevant documents in the last submission was greater or equal to 2, we double n if it is not larger than 1024 and do not change it otherwise (i.e., maximum "batch size" is 2048).</p><p>• If the ratio of relevant to irrelevant documents in our last submission was less than 0.4, we halve n if it is larger than 128 and do not change it otherwise.</p><p>• If the ratio of relevant to irrelevant documents in our last submission was even less than 0.1, we halve n if it is larger than 64.</p><p>• Otherwise, we don't change n</p><p>In the rare case that the SVM cannot find relevant documents not submitted before, we submit 128 random documents that were not submitted before. Subsequent Iterations, Keyqueries Approach. In our second approach, we enhance the baseline with a diversification algorithm that uses keyqueries. A keyquery for a document set is a query that retrieves these documents in its top-k results <ref type="bibr" coords="2,465.18,218.38,9.22,7.92" target="#b3">[3]</ref>. We use keyqueries when at least 128 documents were judged as relevant in the previous iterations (otherwise we proceed as the baseline does). From the at least 128 relevant documents, we randomly choose 128 documents and compute the pairwise tf • idf -weighted cosine similarities for all document pairs. We select the four documents with the highest sum of their six pairwise cosine similarities, ignoring very low similarities below 0.2. Such four documents are assumed to "represent" a specific topic covered in the previous result lists (the previous random selection should ensure that changing the topic in foucs in some later iteration should be possible). In case that four documents could be identified, we extract the top-10 head noun phrases <ref type="bibr" coords="2,423.48,354.37,9.73,7.92" target="#b1">[1]</ref> from their concatenated main contents <ref type="bibr" coords="2,353.41,364.83,9.73,7.92" target="#b9">[9]</ref> (if no four documents are found due to the 0.2 lower bound on the similarity we proceed with a baseline iteration). The keyphrases are used to form a keyquery for as many of the relevant documents as possible against the Lucene BM25 index as the reference search engine with k set to 32. The top-128 results from the keyquery not previously judged as relevant are used as additional positive examples for training the SVM classifier (in addition to the documents already judged as relevant). Stopping Criterion. We stop submitting results when the following empirically determined inequality is satisfied:</p><formula xml:id="formula_0" coords="2,352.20,485.88,168.32,11.01">1.5 • |D relevant | + 1500 &lt; |D irrelevant | .</formula><p>Where |Dx| is the number of documents that have been submitted so far and were judged as relevant/irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>We submitted two runs, the first using the baseline iterations and the second using the iterations with keyqueries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OPEN SEARCH TRACK</head><p>The objective of the Open Search track is to rank a small set of candidate documents (e.g., papers) in return to a scholarly search query.</p><p>Our approach works as follows. We first rank the candidates with Lucene's BM25 implementation. This ranking is then run through our axiomatic re-ranking pipeline <ref type="bibr" coords="2,525.43,645.28,9.73,7.92" target="#b7">[7]</ref> with the axiom weights trained for BM25. We omitted the axioms from our pipeline that are not suited for the scholarly search setup due to missing information (e.g., we omitted the PageRank axiom due to the non-availability of a full citation graph at the time of submission).</p><p>The re-ranking obtained from the combined axioms then forms the submitted ranking similar to our runs for the Web track 2014 and the Session tracks 2014-2015 <ref type="bibr" coords="3,236.26,78.50,9.73,7.92" target="#b6">[6,</ref><ref type="bibr" coords="3,249.06,78.50,6.49,7.92" target="#b5">5]</ref>.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,58.28,100.34,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,115.57,198.96,7.92;3,72.62,126.03,217.35,7.92;3,72.62,136.49,99.69,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,196.45,115.57,75.13,7.92;3,72.62,126.03,152.70,7.92">Using noun phrase heads to extract document keyphrases</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cornacchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,244.08,126.03,45.88,7.92;3,72.62,136.49,41.61,7.92">Proceedings of AI 2000</title>
		<meeting>AI 2000</meeting>
		<imprint>
			<biblScope unit="page" from="40" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,147.94,208.36,7.92;3,72.62,158.41,213.97,7.92;3,72.62,168.87,184.56,7.92;3,72.62,179.33,50.80,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,207.39,147.94,73.59,7.92;3,72.62,158.41,213.97,7.92;3,72.62,168.87,109.43,7.92">Mining tasks from the web anchor text graph: MSR notebook paper for the TREC 2015 tasks track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,200.48,168.87,56.70,7.92;3,72.62,179.33,46.53,7.92">Proceedings of TREC 2015</title>
		<meeting>TREC 2015</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,190.78,213.84,7.92;3,72.62,201.24,208.45,7.92;3,72.62,211.71,197.15,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,265.32,190.78,21.13,7.92;3,72.62,201.24,208.45,7.92;3,72.62,211.71,13.63,7.92">From keywords to keyqueries: Content descriptors for the web</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,105.39,211.71,97.37,7.92">Proceedings of SIGIR 13</title>
		<meeting>SIGIR 13</meeting>
		<imprint>
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,223.16,208.74,7.92;3,72.62,233.62,220.28,7.92;3,72.62,244.08,162.40,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,260.22,223.16,21.13,7.92;3,72.62,233.62,204.97,7.92">From search session detection to search mission detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gomoll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,72.62,244.08,104.31,7.92">Proceedings of OAIR 2013</title>
		<meeting>OAIR 2013</meeting>
		<imprint>
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,255.54,193.66,7.92;3,72.62,266.00,210.70,7.92;3,72.62,276.46,220.28,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,167.72,266.00,115.60,7.92;3,72.62,276.46,92.19,7.92">Webis at TREC 2015: Tasks and Total Recall tracks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Göring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Keil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Anifowose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Othman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,182.75,276.46,105.88,7.92">Proceedings of TREC 2015</title>
		<meeting>TREC 2015</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,287.92,195.23,7.92;3,72.62,298.38,204.05,7.92;3,72.62,308.84,220.29,7.92;3,72.62,319.30,21.38,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="3,110.89,298.38,165.78,7.92;3,72.62,308.84,116.33,7.92">Webis at TREC 2014: Web, Session, and Contextual Suggestion tracks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Göring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,207.05,308.84,85.85,7.92">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,330.76,188.16,7.92;3,72.62,341.22,194.86,7.92;3,72.62,351.68,113.16,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="3,72.62,341.22,119.00,7.92">Axiomatic Result Re-Ranking</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Göring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,210.77,341.22,56.70,7.92;3,72.62,351.68,45.86,7.92">Proceedings of CIKM 2016</title>
		<meeting>CIKM 2016</meeting>
		<imprint>
			<biblScope unit="page" from="721" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,363.14,220.28,7.92;3,72.62,373.60,216.15,7.92;3,72.62,384.06,212.13,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="3,181.81,363.14,111.09,7.92;3,72.62,373.60,216.15,7.92;3,72.62,384.06,26.47,7.92">MapReduce for information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,121.66,384.06,105.00,7.92">Proceedings of CLEF 2010</title>
		<meeting>CLEF 2010</meeting>
		<imprint>
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
	<note>Let&apos;s quickly test this on 12 TB of data</note>
</biblStruct>

<biblStruct coords="3,72.62,395.52,189.58,7.92;3,72.62,405.98,208.46,7.92;3,72.62,416.44,177.26,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="3,72.62,405.98,193.15,7.92">Boilerplate detection using shallow text features</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kohlschütter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,72.62,416.44,109.96,7.92">Proceedings of WSDM 2010</title>
		<meeting>WSDM 2010</meeting>
		<imprint>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,427.89,86.33,7.92;3,72.62,438.36,200.80,8.17" xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Lemur</forename><surname>Project</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Indri</surname></persName>
		</author>
		<ptr target="http://www.lemurproject.org/indri.php" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,449.81,220.28,7.92;3,72.62,460.27,197.91,7.92" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="3,244.37,449.81,48.53,7.92;3,72.62,460.27,23.54,7.92">A picture of search</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Torgeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,114.67,460.27,115.46,7.92">Proceedings of Infoscale 2006</title>
		<meeting>Infoscale 2006</meeting>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,471.73,191.77,7.92;3,72.62,482.19,212.03,7.92;3,72.62,492.65,185.52,7.92;3,72.62,503.11,156.34,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="3,244.43,482.19,40.21,7.92;3,72.62,492.65,169.89,7.92">ChatNoir: A search engine for the ClueWeb09 corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,72.62,503.11,106.49,7.92">Proceedings of SIGIR 2012</title>
		<meeting>SIGIR 2012</meeting>
		<imprint>
			<biblScope unit="page">1004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,514.57,216.18,7.92;3,72.62,525.03,215.52,7.92;3,72.62,535.49,88.72,7.92" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="3,249.07,514.57,39.73,7.92;3,72.62,525.03,139.92,7.92">Netspeak: Assisting writers in choosing words</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trenkmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,231.44,525.03,56.70,7.92;3,72.62,535.49,43.49,7.92">Proceedings of ECIR 2010</title>
		<meeting>ECIR 2010</meeting>
		<imprint>
			<biblScope unit="page">672</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,546.95,208.19,8.17" xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Webis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Netspeak</surname></persName>
		</author>
		<ptr target="http://netspeak.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.62,558.41,129.45,7.92;3,72.62,568.87,144.31,8.17" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="3,115.62,558.41,81.68,7.92">Knowledge base API</title>
		<author>
			<persName coords=""><surname>Wikidata</surname></persName>
		</author>
		<ptr target="https://www.wikidata.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
