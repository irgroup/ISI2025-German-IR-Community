<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.15,68.35,429.45,21.07;1,278.55,95.95,54.67,21.07;1,79.60,123.55,452.54,21.07">Laval University at TREC Dynamic Domain 2016: Subtopic extraction focused on Named Entities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.59,160.61,63.55,9.48"><forename type="first">Robin</forename><surname>Joganah</surname></persName>
							<email>robin.joganah.1@ulaval.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Université Laval</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.78,160.61,71.08,9.48"><forename type="first">Richard</forename><surname>Khoury</surname></persName>
							<email>richard.khoury@ift.ulaval.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Université Laval</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.34,160.61,74.74,9.48"><forename type="first">Luc</forename><surname>Lamontagne</surname></persName>
							<email>luc.lamontagne@ift.ulaval.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Université Laval</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.15,68.35,429.45,21.07;1,278.55,95.95,54.67,21.07;1,79.60,123.55,452.54,21.07">Laval University at TREC Dynamic Domain 2016: Subtopic extraction focused on Named Entities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8409EF66E98B89B7EE80EE81E8A22821</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dynamic Domain</term>
					<term>Information Retrieval</term>
					<term>Topic Modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the results submitted by Laval University to the TREC 2016 Dynamic Domain track. We submitted five runs. For this year we decided to focus around Named Entities to interpret the subtopics. Named Entities are one of the two types of queries we identified in this challenge, the other being queries about concepts. We describe in this paper our experiments to determine if targeting this type of query with a specific pipeline can lead to a global improvement of the system by taking into account the specificity of the queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T he Dynamic Domain track challenge [12] consists of retrieving information (IR) for a specific domain (such as Ebola or Polar scientific documents) through a dynamic process that takes into account feedback from a simulated user. The aim of this challenge is to return a diversified set of documents that both explores the full range of topics within the domain and researches in greater depth topics the user shows interest in.</p><p>For this year's competition, we submitted 5 runs that use different parameters and different pipelines. The main pipeline we investigated focuses on named entities. We extended the dynamic search pipeline we described in our previous work <ref type="bibr" coords="1,539.53,449.97,11.61,8.64" target="#b5">[6]</ref> by taking advantage of Named Entities at multiple stages of the dynamic search. Our idea is that an improvement on a specific type of query can have an impact on the whole system. Indeed, queries that contain one or more named entities are frequent in the TREC 15 and TREC 16 dataset. We could divide the dynamic search problem in queries that are either concept-oriented or entity-oriented.</p><p>We compared this pipeline with our system from TREC 15 <ref type="bibr" coords="1,307.61,507.57,10.59,8.64" target="#b5">[6]</ref>. We also compared different similarity configurations, like BM25 and TF-IDF, and we will show that these parameters have a huge impact on the performance of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Dynamic Domain is a new domain and different approaches have been tested during the first TREC Dynamic Domain track. The spectrum of approaches is broad, ranging from Partially Observable Markov Decision Process (POMDP) <ref type="bibr" coords="1,493.17,582.93,16.61,8.64" target="#b10">[11]</ref> to re-ranking of documents by similarity with user feedback <ref type="bibr" coords="1,242.54,594.45,15.80,8.64" target="#b10">[11,</ref><ref type="bibr" coords="1,262.00,594.45,7.19,8.64" target="#b8">9]</ref>. As part of our previous participation, our team <ref type="bibr" coords="1,473.98,594.45,11.61,8.64" target="#b5">[6]</ref> experimented with techniques like clustering <ref type="bibr" coords="1,159.07,605.97,11.61,8.64" target="#b2">[3]</ref> and topic modeling <ref type="bibr" coords="1,263.20,605.97,11.61,8.64" target="#b0">[1]</ref> to inject diversification in the information retrieval process. These techniques had some success in the Judged-only task that contained documents assigned to at least one topic. However, for the main task where the collection contains documents that are not associated to any topics, the results we obtained were lesser due to the noise of un-judged documents. In these conditions, it becomes more difficult to effectively model the topics that are relevant to the interests of the user. That is why we decided this year to focus on information extraction and noise reduction.</p><p>Information extraction has an important role in natural language processing, most of the techniques <ref type="bibr" coords="1,490.33,666.45,11.61,8.64" target="#b4">[5]</ref> trying to take advantage of Named Entity Recognition (NER) and Relation Extraction (RE). This information helps to find keywords and keyphrases that can help to find relevant words. The extraction of keyphrases has also been studied <ref type="bibr" coords="1,476.31,689.49,11.61,8.64" target="#b3">[4]</ref> and usually takes advantage of information extraction and topic modeling to rank the most diversified and relevant phrases to cover the document entirely. These techniques remind us that one important aspect of the Dynamic Domain task is to interpret information from the user feedback, which is a textual passage taken from the document and returned by the system. During our TREC experiments of last year <ref type="bibr" coords="2,83.65,87.81,11.61,8.64" target="#b5">[6]</ref> we showed that our system pipeline was able to take advantage of named entity recognition on the text passages to expand our query and find new relevant documents.</p><p>In order to fully take advantage of information extraction algorithms, there is a need to clean our dataset to remove noise that can lead to poor data mining performances <ref type="bibr" coords="2,220.42,133.65,15.30,8.64" target="#b12">[13]</ref>. We pre-process the dataset to extract the relevant article text from webpages and remove the noise around the article introduced by page footers, menus and news sections. This pre-processing step was performed using the BoilerPipe library <ref type="bibr" coords="2,204.14,156.69,10.59,8.64" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GLOBAL VIEW OF THE SYSTEM</head><p>The system is composed of two phases: the information retrieval process and the feedback processing. The first one occurs during the initial phase and for each turn until the systems decide to stop. The feedback processing occurs after the initial search phase has occurred and when the user provides his feedback about the first results returned by the system. These phases are illustrated in Figure <ref type="figure" coords="2,128.07,264.69,3.73,8.64" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Initial Search Phase &amp; Information Retrieval Process</head><p>The first phase consists of a retrieval process to obtain an initial set of documents based on the user's original query. In our experiments, the retrieval process makes use of the popular Solr search engine to retrieve a set of n documents from the dataset. We retain five documents from the original query. A classic IR system like Solr can retrieve the top documents by keywords, but it does not provide the mechanisms to diversify the results as needed by the Dynamic Domain challenge. For this, it is necessary to discover topics present within the result set and to return one representative document per topic. We experimented with two algorithms for this purpose: we used K-means clustering to discover clusters of documents within the search results <ref type="bibr" coords="2,516.44,354.21,15.30,8.64" target="#b9">[10]</ref>, and an implementation of Latent Dirichlet Allocation (LDA) algorithm <ref type="bibr" coords="2,305.12,365.73,11.61,8.64" target="#b0">[1]</ref> for topic modelling. The models provided by these additional algorithms allow us to re-rank the documents and select the five best results to return to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Simulated User's feedback</head><p>The second phase of the dynamic retrieval process takes into account the user's feedback about the five documents submitted to him/her. This feedback consists of a highlighted passage from each relevant document returned by the system. If the document is relevant to multiple subtopics, then we have a passage for each subtopic with a rating between 1 and 4.</p><p>We use a NER algorithm on the highlighted passage to extract information to be added to the initial query. With this new query, we proceed with the same information retrieval process as we did in the initial search phase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stopping Criteria</head><p>As illustrated in Figure <ref type="figure" coords="2,153.83,661.89,3.73,8.64" target="#fig_0">1</ref>, phase 2 of our system is a loop and the system has to determine when to halt the search and break out of this loop. This should happen at the point when an additional iteration will no longer yield useful refinements of the search results. We used a hard-coded limit of two iterations, which means that the system will only consider once the user's feedback once. This decision is motivated by the empirical realization that we were not able, during our experiments, to acquire additional information to stabilize or improve the value of the CubeTest after two iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INFORMATION RETRIEVAL PROCESS</head><p>Hence retrieval occurs during two steps of our dynamic search process. The first time is during the initial search phase, which we discussed previously, where it must generate an initial set of five documents based on the original query. It occurs again at every iteration of the dynamic refinement phase of the system to obtain a new set of documents using the expanded query. The choice of IR process has an impact on the performance of our entire system, since the Cube Test (CT) and µ-ERR metrics used to evaluate the task penalize sessions with irrelevant documents as a waste of user's time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Solr and Similarity Measures</head><p>We used the Solr search engine as a baseline to evaluate our IR configurations. We tested two different similarity measures to find documents, default similarity (TFIDF) and BM25, and compared the results we obtained with those. We used them to retrieve documents given a query, either the user's original query or the expanded queries reformulated by our dynamic refinement stage. The top five documents returned are kept as Solr recommendations, the five most relevant documents given the query. Meanwhile, the top n documents returned by Solr are used as a corpus to build models using LDA or K-Means algorithms, where n is determined at the beginning of the run. We present in Fig. <ref type="figure" coords="3,135.31,389.25,4.92,8.64" target="#fig_1">2</ref> our comparison of the results obtained with BM25 and TF-IDF (which is the default similarity in Solr). We can see that the choice of similarity measure can have a huge impact on the results. In future works, we plan on comparing these measures with the language model which has been used by other participants during the last year's competition <ref type="bibr" coords="3,539.93,412.29,10.59,8.64" target="#b8">[9]</ref>. A language model is also the default similarity in other search engines like Indri, so this is a promising alternative to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Latent Dirichlet Allocation (LDA)</head><p>LDA is an algorithm that takes as input a collection of documents and discovers the groupings of topics it implies. Topics are represented as probability distributions over words contained in the documents. In our system, LDA is responsible of discovering five different topic groups from the top n documents (with n between 20 and 200) returned by Solr for a specific query. Next, the system builds five expanded queries, one for each of the five topic groups, by adding the five most probable words of each topic distribution to the current query. It then runs a new Solr search with each of the five new queries, and keeps the top document of each search to form the set of five LDA recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. K-Means</head><p>K-Means is a clustering algorithm that builds clusters around centroids. In our system, we use this algorithm to create five clusters of documents from a list of 200 documents returned by Solr. To build the clusters, we make TFIDF vectors to represent the documents and of a cosine distance to estimate their similarity. The system keeps the document closest to each cluster centroid to create the set of five K-means recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. NAMED ENTITY FOCUSED PIPELINE</head><p>The main difficulty with topic modeling and clustering is the topics of interest are sometimes buried in noise. For example, a query about a person can return a document containing a few sentences related to the person and a lot of irrelevant content. If we consider the whole document to be relevant during our model construction, the extra text not related to the user's interests will lead to a noisy or bad model.</p><p>To solve this issue, we apply a sentence segmentation algorithm to each document and keep only the sentences where at least some of the topic words appear. In the future, we plan on keeping neighboring sentences as well, to add a notion of context to our model, and see how it impacts the system. We tested this pipeline on the Ebola dataset and our results are presented in Figure <ref type="figure" coords="4,414.81,227.49,3.73,8.64" target="#fig_2">3</ref>. We did not make any distinction between concept-oriented or entity-oriented queries during our experiments, but our results indicate that this pipeline has a positive impact for the CubeTest metric. However, we can see that the average CubeTest remains the same. We intend to test this specific pipeline solely on entity-oriented queries in future work and process concept-oriented queries with a different pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DESCRIPTIONS OF THE SYSTEMS SUBMITTED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>The dataset proposed for this competition is separated in two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Ebola</head><p>The Ebola dataset is related to the Ebola outbreak in 2014-2015, it contains 194,481 web pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Polar</head><p>The polar dataset contains 244,536 files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Pre-Processing</head><p>We used BoilerPipe to extract articles from webpages in order to focus on the real content and to remove the noise found on web pages like frames, page footers, menu, sign-in forms, etc.</p><p>We also used Solr built-in filters with Porter stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Systems</head><p>Our five systems are based on a similar pipeline which uses NLTK Named Entity Recognition (NER) algorithms to extract named entities from the text passage highlighted by the simulated user. These words are then added to the query to retrieve documents and run our algorithms over these documents.</p><p>We can separate the systems into two different categories: -A baseline system that only uses the best results from the search engine -Systems using clustering or topic modeling to expand the queries. In this second category, we have three systems that use the whole document to perform the analysis and one system (UL_LDA_NE) that uses only sentences containing some part of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) UL_BM25</head><p>This is the baseline system that takes the 5 best results retrieved by Solr using the BM25 measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) UL_LDA_200</head><p>This system take the first 200 documents retrieved by Solr with the initial query and the BM25 measure of similarity. It performs topic modeling with LDA to find 5 different topics. These topics are composed of words that are more likely to be part of each topic. We select the top n words to reformulate a query and search for documents with this query. The best document is added to our list. If one document appears in two lists, we take the second result from the first topic. At the end of the process, we obtain 5 documents, one from each topic query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) UL_LDA_NE</head><p>This system also uses LDA and BM25 measures of similarity but only uses the top 20 documents to expand the initial query. We added sentence segmentation to have sentences intersecting with the initial query in order to focus topic modeling around content more likely to be relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) UL_Kmeans</head><p>This system is similar to UL_LDA_200, the only difference being the use of K-means instead of LDA to search for different documents. In this case, we perform clustering with K = 5 and we take the documents that are the most similar to each centroid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) UL_LDA_Psum</head><p>This system uses topic modeling in a different way. We normalize the probabilities for each document to be a part of each topic by the probability of each topic. Then we choose documents that cover most of the popular topics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. TREC SUBMISSIONS RESULTS ANALYSIS</head><p>The results reported in Table <ref type="table" coords="6,181.17,68.85,6.60,8.64">II</ref> are the scores from metrics used in TREC 16 such as CubeTest <ref type="bibr" coords="6,461.92,68.85,10.59,8.64" target="#b7">[8]</ref>, ERR <ref type="bibr" coords="6,503.09,68.85,11.61,8.64" target="#b1">[2]</ref> and NDCG (Normalized Discounted Cumulative Gain).</p><p>The results indicate that we are below the TREC median at iteration 2 when we stop the dynamic search process, except for the CubeTest of our UL_LDA_NE run. These results we present are limited by our baseline system using a BM25 measure that might be less accurate than those based on language models. The second limitation is that our technique (UL_LDA_NE) which targets mainly named entity topics is not intended to work well on queries pertaining to concepts. For instance, most queries for the Polar dataset are concept-oriented queries, which our system cannot handle as well.</p><p>However, with five different systems presented for the competition, we can compare the results to see how each technique influences the system's behaviour. We can see that LDA (UL_LDA_200) and K-means (UL_Kmeans) have the same impact and that they have a negative effect if we compare their results with the baseline. This can be explained by the fact that we used n=200, which means that we are working with a large number of document to update the initial query. We can see that our pipeline using NE recognition and only 20 documents lead to better clustering or topic modeling.</p><p>The Psum (UL_LDA_Psum) system uses LDA to find documents that might covers multiple topics. The poor performance of the system suggests that a document covering multiple well-represented topics does not necessarily have relevant information about new topics. However, this hypothesis would need to be tested in further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION AND FUTURE WORK</head><p>In this paper, we presented a pipeline focused around named entity topics. Experimental results indicate that it works better than our baseline system. We can observe some improvements on the entire IR process when using this pipeline, but its impact on the polar domain specifically seems to be limited due to the concept-oriented nature of queries in this domain. These results motivate us to devise a specific pipeline for concept-oriented queries and a process to classify queries by types, with the aim of providing specialized processing for different types of queries. We also plan to work on these concepts by putting more emphasis on keyphrase extraction that could help the unsupervised system to discover the user's interests. We also intend to devote more efforts to developing strategies to determine when the system should stop, since that is an important step of the automated process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,253.11,631.96,105.49,6.95;2,179.88,475.68,252.00,154.08"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Global System Overview</figDesc><graphic coords="2,179.88,475.68,252.00,154.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,216.00,370.60,179.77,6.95;3,203.88,216.00,203.04,152.40"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. TF-IDF and BM25 comparison on Ebola dataset</figDesc><graphic coords="3,203.88,216.00,203.04,152.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,206.58,206.44,198.55,6.95;4,215.88,50.64,189.36,141.84"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Impact of selection of sentences with part of the query</figDesc><graphic coords="4,215.88,50.64,189.36,141.84" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,82.68,411.33,482.20,8.64;6,82.68,422.61,415.19,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,253.21,411.33,186.23,8.64">Latent topic feedback for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buttler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,445.78,411.33,119.10,8.64;6,82.68,422.61,304.75,8.64">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="600" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,434.13,482.36,8.64;6,82.68,445.65,452.94,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,364.58,434.13,196.32,8.64">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,82.68,445.65,341.95,8.64">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,457.17,482.31,8.64;6,82.68,468.69,336.58,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,308.56,457.17,256.43,8.64;6,82.68,468.69,142.73,8.64">Semi-supervised clustering with user feedback, Constrained Clustering: Advances in Algorithms</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,232.31,468.69,100.20,8.64">Theory, and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="32" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,480.21,482.35,8.64;6,82.68,491.73,22.46,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,193.23,480.21,262.80,8.64">Automatic Keyphrase Extraction: A Survey of the State of the Art</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,462.78,480.21,18.25,8.64">ACL</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1262" to="1273" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,503.25,482.36,8.64;6,82.68,514.77,120.24,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,128.17,503.25,136.19,8.64">Information Extraction from Text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,431.34,503.25,71.78,8.64">Mining Text Data</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="11" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,526.29,482.34,8.64;6,82.68,537.81,457.12,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,313.20,526.29,251.82,8.64;6,82.68,537.81,265.91,8.64">Laval University and Lakehead University at TREC Dynamic Domain 2015: Combination of Techniques for Subtopics Coverage</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Joganah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamontagne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,355.72,537.81,76.80,8.64">Proceedings TREC</title>
		<imprint>
			<date type="published" when="2015">2015. 2016</date>
			<pubPlace>Gaithersburg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,549.33,482.36,8.64;6,82.68,560.61,460.18,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,351.09,549.33,210.08,8.64">Boilerplate detection using shallow text features</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kohlschütter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,82.68,560.61,349.72,8.64">Proceedings of the third ACM international conference on Web search and data mining</title>
		<meeting>the third ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,572.13,482.35,8.64;6,82.68,583.65,482.13,8.64;6,82.68,595.17,264.94,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,302.39,572.13,262.64,8.64;6,82.68,583.65,145.97,8.64">The water filling model and the cube test: multi-dimensional evaluation for professional search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,238.75,583.65,326.06,8.64;6,82.68,595.17,153.96,8.64">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,606.69,482.31,8.64;6,82.68,618.21,116.88,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,180.01,606.69,327.79,8.64">Re-ranking via User Feedback: Georgetown University at TREC 2015 DD Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,515.09,606.69,49.90,8.64;6,82.68,618.21,55.20,8.64">Proceedings of TREC 2015</title>
		<meeting>TREC 2015<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,629.73,482.30,8.64;6,82.68,641.25,158.84,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,288.10,629.73,194.68,8.64">A comparison of document clustering techniques</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,489.64,629.73,75.34,8.64;6,82.68,641.25,42.85,8.64">KDD workshop on text mining</title>
		<imprint>
			<biblScope unit="page" from="525" to="526" />
			<date type="published" when="2000">2000</date>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,652.77,426.80,8.64" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="6,116.56,652.77,354.65,8.64">Modeling Search Engine&apos;s Explorations in Dynamic Search: An Ontological Perspective</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,664.29,396.90,8.64" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<title level="m" coord="6,256.03,664.29,184.85,8.64">TREC 2015 Dynamic Domain Track Overview</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,82.68,675.81,482.21,8.64;6,82.68,687.09,415.19,8.64" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,188.90,675.81,243.99,8.64">Eliminating noisy information in web pages for data mining</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,440.50,675.81,124.39,8.64;6,82.68,687.09,304.75,8.64">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
