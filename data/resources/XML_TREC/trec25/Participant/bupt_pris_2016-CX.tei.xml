<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.86,82.60,407.48,14.36;1,129.50,113.80,336.40,14.36;1,220.97,145.00,153.29,14.36">Beijing University of Posts and Telecommunications(BUPT) at TREC 2016:A Rating Model Based on Tags for Contextual Suggestion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,164.18,170.10,49.12,8.96"><forename type="first">Danping</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.13,170.10,44.58,8.96"><forename type="first">Siyuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.21,170.10,54.12,8.96"><forename type="first">Zonghui</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.00,170.10,43.00,8.96"><forename type="first">Yameng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.28,170.10,48.60,8.96"><forename type="first">Ruifang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.86,82.60,407.48,14.36;1,129.50,113.80,336.40,14.36;1,220.97,145.00,153.29,14.36">Beijing University of Posts and Telecommunications(BUPT) at TREC 2016:A Rating Model Based on Tags for Contextual Suggestion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F212BF93A767EDB1B370567ABA42CECC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tags</term>
					<term>Ratings</term>
					<term>Collaborative Filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we focus on the effort of Beijing University of Posts and Telecommunications (BUPT) on the TREC 2016's Contextual Suggestion Track. The problem we are supposed to tackle is how to make suggestions for a particular person with the provided context as well as its preferences. Basically we regard tags as the most important factor, and get ratings for different attractions with the ratings of tags.</p><p>Also we use Collaborative Filtering to fill the missing ratings. A ranked list is generated after calculating users' ratings for candidate attractions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Contextual Suggestion Track has been engaged on searching techniques for proposals about a trip destination based on context and user interests since 2012. Participants are given a set of contextual information including a city the user is located in, a trip type, a trip duration, a type of group the user is travelling with, the season the trip will occur in, and attractions' details which the user prefers to. Besides, a list of 50 candidates is provided in the Phase2 experiment for participants to reorder.</p><p>Like the TREC 2015 Contextual Suggestion Track, participants are allowed to return result either in the Phase 1 experiment or in the Phase 2 experiment. The Phase1 experiment is a collection based task which requires suggestions consisting of a ranked list of up to 50 attractions based on the dataset either from the open web or the TREC CS Web Corpus released as an additional data. The Phase 2 experiment is a reranking task which aims to rerank the 50 candidates provided by the file "Phase2_requests.json". We participated both Phase 1 and Phase 2 experiments.</p><p>In this paper we present the detailed work of Beijing University of Posts and Telecommunications (BUPT) in the TREC 2016 Contextual Suggestion Track. The problem we aimed to solve is How to get the ratings of attractions depending on user's preferences and related context. In consideration of abundant tags given in the profile, we decided to take up the task in perspective of tags. Thus the detailed problems we figured out are as follows:</p><p>(1) How do we calculate the rating for a particular tag?</p><p>(2) How do we get the rating for a particular attraction with the rating of tags?</p><p>In Section 2 we detail our method of Contextual Suggestion. Section 3 is the description of the experimental result. Moreover, conclusions and future work is illustrated in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OUR METHOD</head><p>The procedures of our work are listed: <ref type="bibr" coords="1,90.02,747.37,11.69,8.96" target="#b0">(1)</ref> Crawling data from open web to obtain more comprehensive corpus, (2) Getting the candidates,</p><p>(3) Predicting the missing ratings, (4) Generating the ranked list, (5) Adjusting the scores.</p><p>The following sub-sections are the descriptions of the processes above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Gathering</head><p>From the profiles given by the Phase 1, we get useful information of attractions such as a city id indicating where a certain attraction lies in, a URL link containing more details of the attraction in the open web, and a particular title. However these are obviously not enough, as tags and ratings of attractions are particularly necessary for our method. Thus we tried to crawl more detailed items for attractions with the help of Yelp API and Foursquare API. The detailed items, which are picked from the crawled text, contain rating, a tag list, and the count of reviews for a certain attraction. We differed the crawling source by attractions' URL. For URLs possessed by Yelp domain, we searched information by Yelp API; For URLs belonging to Foursquare domain, we crawled details through Foursquare. We crawled data for Phase 1 experiment as much as possible, so that abundant data are available. For Phase 2 experiment, it is necessary to make sure that the candidates in lack of tags are supplied detailed items through crawling from the web.</p><p>After crawling information on Yelp and Foursquare, we extracted ratings, tags and count of reviews for further use. Finally, 14991 candidate suggestions are crawled out from 18755 candidate suggestions given in Phase 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Getting the candidates</head><p>We now gain a big dataset of plentiful attractions in different cities.</p><p>In Phase 1, we considered the attractions in the dataset located in the same city with which mentioned in the context of a particular user. After throwing away those with the count of reviews less than 300, we regarded the collection of attractions as the original candidates.</p><p>The problem we aim to solve in the Phase 2 experiment is generally the same as that in Phase 1, namely rating the candidate attractions and getting the ranked list. The difference is that candidates are given in the file with a set of suggestions requests that were made during the Phase 1 experiment. Because the candidates provided lack items like ratings and tags, we supplement the candidates' records with tags and other information crawled from the open web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Predicting the miss ratings by CF(Collaborative Filtering)</head><p>In view that attractions in the preference list are partly marked with ratings, an approach to filling the missing ratings is needed. After deeply diving into the data, we used CF (Collaborative Filtering) to predict the missing ratings.</p><p>There are 442 users along with 106 tags in the provided request file. Matrix 𝑊 𝑡 = [𝑣 1 , 𝑣 2 , 𝑣 3 … 𝑣 𝑛 ] is constructed by us to represent the data. The dimension of 𝑊 𝑡 is 442 × 106, where 𝑣 𝑛 is a vector with a dimension of 442 × 1, and 𝑛 is 106. The matrix 𝑊 𝑡 is partly filled with the ratings of each user.</p><p>Tag-based CF method predicts the missing ratings by making use of the tag-vector similarity.</p><p>The similarity is calculated by the formula (2-1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑆𝑖𝑚(𝑣 𝑒 , 𝑣</head><formula xml:id="formula_0" coords="2,242.69,686.13,140.59,19.59">𝑝 ) = 0.5 + 0.5 × 𝑣 𝑒 •𝑣 𝑝 |𝑣 𝑒 ||𝑣 𝑝 | (2 -1)</formula><p>𝑣 𝑒 represents the existed rating vector and 𝑣 𝑝 represents the to-be-predicted rating.</p><p>The miss rating is calculated by the formula (2-2).</p><p>𝑅𝑎𝑡𝑖𝑛𝑔+= 𝑟𝑎𝑡𝑖𝑛𝑔 𝑒 × 𝑆𝑖𝑚(𝑣 𝑒 , 𝑣 𝑝 ) (2 -2)</p><p>𝑟𝑎𝑡𝑖𝑛𝑔 𝑒 is the existed ratings, while 𝑅𝑎𝑡𝑖𝑛𝑔 is accumulated gradually. After this process, the matrix 𝑊 𝑡 is supposed to be all filled by ratings.</p><p>Also, we noticed that some tags of candidates which crawled from the Internet shared the same meaning with tags of the aforementioned 106 tags but shown as different words or phrases. After consideration, we used Word2Vector tools by Google for mapping the two tags with the same meaning but from various sources. For tags that do not exist in the Word2Vector training data, we map them manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The Model of Generating the ranked list</head><p>With the help of the profile and context, we are to obtain a candidates' ranked list of attractions by utilizing the following three pieces of information:</p><p>• the user's preferences,</p><p>• the average online rating 𝑟 𝑎 ,</p><p>• the set of category tags 𝐶 𝑎 associated with each attraction.</p><p>With the candidate attractions, we model our rating method as follows.</p><p>First, we collect every tag marked by a user as tag-user pairs, then calculate the rating for each tag in the tag-user pairs in terms of the user's preferences. For each pair, the rating ranges from 0 to 5.The formula The final ranking method of Phase 1 and Phase 2 is not the same.</p><p>In phase 1 experiment, the rating 𝑅(𝑢𝑠𝑒𝑟, 𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒) is divided into 10 levels in steps of 0.5 length.</p><p>Higher the score is, higher the level will be. We get a ranked list of attractions for each user through the attractions' level. Then we take the quality of the attractions into account, which is measured by the average online rating 𝑟 𝑎 . When attractions are at the same level (as mentioned above), the one owning higher 𝑟 𝑎 is ranked higher. Finally we get the final suggestion ranked list.</p><p>The ranking way of the Phase 2 is stated in the next sub-section 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">The Adjustment of Scoring</head><p>We score each candidate mainly from two aspects: One is how much a user may prefer the kind of an attraction, namely 𝑅(𝑢𝑠𝑒𝑟, 𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒) . Since ratings of tags to every user are generated using collaborative filtering, we can measure a user's preference to an attraction by using a mean or a max function on the all of its tags (Formula 2-4 only showed the mean function). For example, an attraction has three tags: sports, fishing and entertainment, with scores of 3, 4 and 5 respectively by a user; then the score of this situation will be 4 if we use a mean function or 5 if we use a max function. The other aspect considered is the rating of an attraction. As ratings of attractions have been crawled on Yelp and Foursquare, we can take advantage of the ratings from the two sources. However, what we should make clear is the ratio about the two sources. If it shows the rating on Yelp is more believable, more weights on ratings of attractions from Yelp should be put, otherwise we put more weights on ratings from Foursquare. Another factor of the weights is the count of reviews of the rating. This comes from the intuition that if a rating from Yelp or Foursquare has very few review counts, it is likely to have large variance, which is consequently unbelievable. Thus we put less weight on ratings if they have few review counts. And the final score is a combination of the three parts of scores mentioned above, i.e. users' rating of attractions, rating from Yelp and rating from Foursquare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENT RESULTS AND ANALYSIS</head><p>We submitted one run for Phase 1 experiment and three runs varying the function of calculating the users' rating of attractions and weights of users' rating of attractions, rating from Yelp and rating from Foursquare in the Phase 2 experiment. In runA we put a higher weight on ratings from Yelp (0.4), a lower weight on Foursquare (0.2), and use a max function when calculating the score of preference; In runB we put a higher weight on Foursquare(0.4), a lower weight on Yelp(0.2), and also use a max function; In runC we put average weight on the two sources(0.3) and use a mean function.  <ref type="table" coords="4,116.77,535.91,4.98,8.96">2</ref> shows the overall performances of our three runs as well as the median, worst and best performance of 30 Phase 2 runs. We can see from the table that runB outperforms the TREC median score in terms of all evaluating measures, while the other two runs float up or down around the TREC median score. This confirms that our model performs well towards the task. We also learn two tips from the result of our model:</p><p>(1) A max function is more suitable for task than a mean function while calculating the users' ratings for attractions.</p><p>(2) The rating derived from Foursquare should be put more weights than that from Yelp. Generally, all runs utilized our rating model to generate a ranked list, but there are differences between the Phase 1 experiment and the Phase 2 experiment. The differences are as follows:</p><p>(1) We filled the missing ratings for Phase 2 while ignoring them in Phase 1 experiment.</p><p>(2) We only used mean function to calculate the users' ratings for candidates in Phase 1 experiment.</p><p>(3) The candidates for Phase 2 experiment are more precise than those of Phase 1.</p><p>Through the comparison, we find out that our approach to filling the missing ratings by Collaborative</p><p>Filtering is effective for the task. It also gives evidence to the conclusion we made from table 2, that a max function outperforms a mean function for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In the TREC 2016's Contextual Suggestion Track, we built a corpus with plentiful attractions and their detailed information. Meanwhile we designed a model to generate the users' ratings for attractions mostly making use of tags. By Collaborative Filtering, we supplement the ratings for certain attractions in the profile. With patient adjustment of the function and parameters, we figured out an available ranking method for the suggesting task this year. Furthermore, if a lot of submitted runs and evaluations are available, a machine learning method is considerable to improve the adjustment of our model's parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.02,310.53,71.26,8.96;3,156.98,333.46,40.78,9.96;3,197.21,337.84,4.35,6.96;3,202.13,333.10,14.40,10.32;3,219.29,325.18,7.06,9.96;3,308.69,325.54,85.75,9.96;3,226.37,329.92,80.37,8.19;3,285.17,340.18,7.29,9.96;3,291.65,344.56,36.19,6.96;3,407.11,333.46,31.37,9.96;3,90.02,356.38,333.74,9.96;3,422.95,357.33,82.48,10.39;3,90.02,371.98,229.07,9.96;3,318.55,376.36,4.35,6.96;3,323.59,371.62,162.54,10.26;3,90.02,388.43,415.38,9.06;3,90.02,404.13,28.78,8.96;3,169.34,426.34,94.59,9.96;3,266.69,418.06,7.06,9.96;3,332.83,418.42,40.75,9.96;3,373.03,422.80,4.35,6.96;3,377.95,418.42,4.13,9.96;3,273.77,422.80,57.03,8.19;3,316.87,433.08,7.29,9.96;3,323.35,437.46,7.89,8.19;3,394.75,426.34,31.25,9.96;3,90.02,449.40,7.29,9.96;3,96.50,453.78,7.89,8.19;3,110.42,449.40,394.90,9.96;3,90.02,466.55,84.33,8.96"><head>( 2 -</head><label>2</label><figDesc>3) is as follow: 𝑅(𝑢𝑠𝑒𝑟, 𝐶 𝑎 ) = ∑ 𝑅(𝑢𝑠𝑒𝑟, 𝑎𝑡𝑡𝑟𝑎𝑐𝑡𝑖𝑜𝑛) , 𝑎𝑡𝑡𝑟𝑎𝑐𝑡𝑖𝑜𝑛) represents the rating for an attraction marked by an user, 𝑁 𝑎𝑡𝑡𝑟𝑎𝑐𝑡𝑖𝑜𝑛 represents the count of attractions in the preference list, 𝑅((𝑢𝑠𝑒𝑟, 𝐶 𝑎 )) represents user's rating about the tag.Next, we computed a rating for the candidates with user's ratings about tags. The formula (2represents the count of tags marked on a candidate, 𝑅(𝑢𝑠𝑒𝑟, 𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒) represents the user's rating of a candidate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.02,263.73,414.99,281.14"><head>Table 1 .</head><label>1</label><figDesc>Performance in Phase1Table1shows the comparison between our run bupt_runA with the comprehensive performance of 15 Phase 1 runs. It is clear that our result performs better than the TREC median score by ndcg@5 and</figDesc><table coords="4,90.02,278.81,378.00,266.06"><row><cell></cell><cell>ndcg@5</cell><cell>Reciprocal Rank</cell><cell>P@5</cell></row><row><cell>bupt_runA</cell><cell>0.2395</cell><cell>0.5366</cell><cell>0.3475</cell></row><row><cell>TREC Median</cell><cell>0.2133</cell><cell>0.5041</cell><cell>0.3508</cell></row><row><cell>TREC Worst</cell><cell>0.0367</cell><cell>0.1113</cell><cell>0.0721</cell></row><row><cell>TREC Best</cell><cell>0.5876</cell><cell>0.9809</cell><cell>0.7213</cell></row><row><cell>Reciprocal Rank.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Table 2.Overall Performances of Phase 2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ndcg@5</cell><cell>Reciprocal Rank</cell><cell>P@5</cell></row><row><cell>runA(cs.4_.2_max)</cell><cell>0.2758</cell><cell>0.5932</cell><cell>0.4238</cell></row><row><cell>runB(cs.2_.4_max)</cell><cell>0.2934</cell><cell>0.6250</cell><cell>0.4479</cell></row><row><cell>runC(cs.3_.3_avg)</cell><cell>0.2469</cell><cell>0.6009</cell><cell>0.3790</cell></row><row><cell>TREC Median</cell><cell>0.2562</cell><cell>0.6015</cell><cell>0.3931</cell></row><row><cell>TREC Worst</cell><cell>0.0266</cell><cell>0.1065</cell><cell>0.0517</cell></row><row><cell>TREC Best</cell><cell>0.6165</cell><cell>0.9914</cell><cell>0.8103</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.02,660.74,414.67,105.56"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="4,103.22,660.74,363.82,89.48"><row><cell cols="2">.Comparison among all the runs</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ndcg@5</cell><cell>Reciprocal Rank</cell><cell>P@5</cell></row><row><cell>bupt_runA</cell><cell>0.2395</cell><cell>0.5366</cell><cell>0.3475</cell></row><row><cell>runA(cs.4_.2_max)</cell><cell>0.2758</cell><cell>0.5932</cell><cell>0.4238</cell></row><row><cell>runB(cs.2_.4_max)</cell><cell>0.2934</cell><cell>0.6250</cell><cell>0.4479</cell></row><row><cell>runC(cs.3_.3_avg)</cell><cell>0.2469</cell><cell>0.6009</cell><cell>0.3790</cell></row></table><note coords="4,90.02,757.33,414.67,8.96"><p>It is apparently indicated that runs in Phase 2 performed better than the run in Phase 1 in the table 3.</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,93.79,341.73,411.55,8.96;5,108.02,357.22,65.32,9.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,203.83,341.73,218.41,8.96">An opinion-aware approach to contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,443.50,341.73,61.84,8.96;5,108.02,357.22,35.25,9.06">Proceedings of TREC&apos;13</title>
		<meeting>TREC&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.79,372.93,411.36,8.96;5,108.02,388.42,273.10,9.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,421.63,372.93,83.51,8.96;5,108.02,388.53,130.48,8.96">Overview of the trec 2015 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,255.77,388.42,96.30,9.06">Proceedings of TREC&apos;15</title>
		<meeting>TREC&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
