<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,193.70,98.99,224.60,12.90">BJUT at TREC 2016: LiveQA Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,263.52,137.64,69.56,10.75"><forename type="first">Weitong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Information</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,342.11,137.64,55.78,10.75;1,397.89,136.11,1.41,6.99"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
							<email>yangzhen@bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Information</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,193.70,98.99,224.60,12.90">BJUT at TREC 2016: LiveQA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D818D370D8D4367AB2794BFDB49DE959</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents the BJUT's liveQA system participating the TREC 2016. The Trec LiveQA track continues to use the last year's instruction, requiring that the system is able to answer the questions which had not been solved in one minutes based on Yahoo! Answers. Our work: (1) The key words are abstracted from the questions, so that more relevant questions will be returned. (2) The system searches in a larger scope on Yahoo! Answers to find the most accurate answers. (3) The answers should be detect whether they are more suitable for answering the given questions. The experiment results are presented at the end of the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The automated question answering (QA) track, which has been one of the most popular tracks in TREC for recent years, has focused on the task of automatically answering questions posed by humans in a natural language. The track primarily dealt with factual questions, and the answers provided by participants were extracted from a collection of news articles. While the task evolved to model increasingly realistic information needs, addressing question series, list questions, and even interactive feedback, a major limitation remained: the questions did neither come from real users, nor in real time <ref type="bibr" coords="1,168.79,498.16,123.72,8.64" target="#b2">(Robertson and Walker 1997;</ref><ref type="bibr" coords="1,54.00,509.12,79.86,8.64" target="#b1">Mikolov et al. 2013)</ref>.</p><p>The Trec LiveQA track mainly aims at providing the automatic answers for questions posed by humans in a natural language. There is also an additional demand that extracts the keywords from the question. This track revives and expands the QA track, focusing on live question answering for real-user. Real user questions, extracted from the stream of most recent questions submitted on the Yahoo Answers (YA) site that have not yet been answered by humans, will be sent to the participant systems. The systems will provide an answer in real time. The list of YA categories is limited to a certain range, which includes Arts &amp; Humanities, Beauty &amp; Style, Health, Home &amp; Garden, Pets, Sports and Travel. The question will be provided every minute for a whole day. The returned answers is restricted to 1000 characters and will later be judged by TREC editors on a 5-level Likert scale.</p><p>This paper introduces our liveQA system which we use to accomplish the Trec LiveQA track task answering the ques-tions in real time. Since the questions are all from Yahoo Answer, we assume that the questions input into the system have been asked by other people previously, and these similar questions have already had best answers. So we transfer the task from answering the questions to choosing the best answers by similar questions. We don't use any search engine, because we think the answer in Yahoo! Answers is more general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Overview</head><p>The input of the system is the questions which are written in spoken language. At first, the questions need to be pre-processed. The number of the questions' words should be shorten and the key words should be abstracted. Second, the results after last step should be sent to the related search engine, to get some similar questions and their best answer. In order to get more similar question, the system choose two ways to get more than five similar questions. Then the system formulate the answers and choose the best one as the final answer. In addition, the answer we have chosen need to be expanded. Finally, the keywords need to be selected as the additional function <ref type="bibr" coords="1,482.79,458.49,75.21,8.64" target="#b0">(Banea et al. 2012;</ref><ref type="bibr" coords="1,319.50,469.45,99.36,8.64" target="#b5">Yi, Wang, and Lan 2015;</ref><ref type="bibr" coords="1,421.21,469.45,66.30,8.64" target="#b4">Toba et al. 2014;</ref><ref type="bibr" coords="1,489.86,469.45,63.29,8.64;1,319.50,480.41,40.53,8.64" target="#b3">Shah and Pomerantz 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Processing Part</head><p>The system receives four parts during evaluation which include qid, title, body and category. Our system doesnt use category because we find that some similar questions are divided into several different categories. We use qid to filtering out from the search results with the same qid because the qid we get doesn't have any answers. Since most questions dont have body and some titles have enough information, the system will only use body when the body has enough meaning words, else add some body words as search requirements.The method is:</p><p>if(title ≥ T): use title as question else:</p><p>use title and some of the body as question We will delete these words, such as stop words, which are not useful. They don't have any meanings, so they can't take effect on finding best answers. At the same time, some verbs  and nouns are taken place with synonyms in order that more similar questions will be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clue Retrieval Part</head><p>Yahoo! Answers is chosen as the corpus from which we can find the answers. The system will use the result after the last step as the search requirements to find some similar questions first. Then the system will find all the best answers of these questions. If some questions dont have best answers, the system will formulate all the answers of its question and choose one as the best answer according to the similarity between the question and the answer, the length of the answer and the number of the supporters. If the question dont have similar ones, the system will find its related problems as the similar questions in order that any question given to the system will find its similar questions. As a result, every questions will find about five questions and their best answers which can be used to do next step and be able to get better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Processing Part</head><p>After the last step, the system gets about five similar questions and their best answers. In this part, the best answer will be chosen from about five answers according the similarity between the input question and their own question, the similarity between the answer and their own question and so on.</p><p>The system uses cosine similarity to calculate the level of similarity. At the same time, the limitation of the result is 1000 characters, so if the length of the best answer is less than 500 characters, the answer will be replenished according to the answer with the second highest score. The system will choose different meanings in the second and third highest score. We think this step can help the results to get good grade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>In this years LiveQA evaluation, all the questions were scored using 5-level scale:</p><formula xml:id="formula_0" coords="2,319.50,549.06,75.53,67.71">• -2: non-readable • 1: poor • 2: fair • 3: good • 4: excellent</formula><p>The evaluation measures used are:</p><p>• avg-score (0-3): average score over all queries (transferring 1-4 level scores to 0-3, hence comparing 1-level score with no-answer score, also considering -2-level score as 0) • succ@i+: number of questions with i+ score (i=1..4) divided by number of all questions Our team hasn't receive any results until now, maybe we didn't return any questions in a right way or other reasons. However, during evaluation, we think our results are good. But we won't lose confidence. We think we can get good results next year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We presented the BJUTs liveQA system above. This is my first time participation in LiveQA task. However, the results of our system are not satisfactory. On the one hand, we haven't receive any results to judge how the system works during evaluation. On the other hand, without sufficient time, we haven't done a satisfactory model. We will get over them next year, and hope to get good result next year. Because of the lesson this year, we have more experience for the next year evaluation.</p><p>The LiveQA track revives the task of automatic question answering in TREC. It provides an opportunity for the participants to try their QA systems on real-world questions, collected from Yahoo! Answers community question answering website. The approach we chose is based on picking key terms from a given question, submitting them to a search engine and extracting an answer.</p><p>We would like to improve the procedure of finding answer to a given question by analyzing existing human-generated question-answer pairs. We are hopeful that finding the ways in which an answer is related to the question will help extract more precise answers in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,291.81,75.83,33.48,9.41;2,290.42,122.04,36.29,9.41;2,291.81,133.33,33.48,9.41;2,289.67,168.24,37.70,9.41;2,294.63,179.53,27.91,9.41;2,301.09,190.82,15.02,9.41;2,299.12,225.73,18.75,9.41;2,292.79,237.02,31.45,9.41;2,290.42,277.59,36.29,9.41;2,292.79,288.87,31.45,9.41;2,292.79,335.08,31.45,9.41;2,123.29,110.01,118.76,7.44;2,123.29,118.80,56.15,7.44;2,369.86,153.11,119.13,7.45;2,369.86,161.80,115.42,8.26;2,369.86,171.65,68.66,7.44;2,369.86,180.34,112.38,8.25;2,369.86,190.16,50.64,7.45;2,369.86,198.85,76.98,8.26;2,123.29,223.51,127.40,8.25;2,123.29,233.33,14.64,7.44;2,123.29,242.02,118.82,8.25;2,123.29,251.84,24.45,7.45;2,123.29,260.53,120.64,8.26;2,123.29,270.36,18.57,7.45;2,369.86,270.90,104.26,7.45;2,369.86,279.69,110.14,7.44;2,369.86,288.48,51.84,7.44"><head></head><label></label><figDesc>which are not useful such as stop words Choose best answers for each questions ①length ②the defference between objection and approval ③whether the answers are chosen as the best answers ④quantity of information ①the similarity between question-answer pairs ②the similarity between questions and answers ③answers and questions have same key words Choose the best one and add some other meanings from the second one and the third one</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,246.68,378.86,118.64,8.64;2,365.12,251.29,130.75,64.98"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Framework.</figDesc><graphic coords="2,365.12,251.29,130.75,64.98" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,319.50,82.97,238.50,8.64;3,319.50,93.93,238.50,8.64;3,319.50,104.71,238.50,8.82;3,319.50,115.66,238.50,8.59;3,319.50,126.62,238.50,8.59;3,319.50,137.58,238.50,8.59;3,319.50,148.54,238.50,8.82;3,319.50,159.68,32.94,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,319.50,93.93,238.50,8.64;3,319.50,104.88,22.44,8.64">Unt: A supervised synergistic approach to semantic text similarity</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,358.73,104.71,199.27,8.59;3,319.50,115.66,134.57,8.59;3,498.71,115.66,59.30,8.59;3,319.50,126.62,156.30,8.59;3,540.19,126.62,17.81,8.59;3,319.50,137.58,238.50,8.59;3,319.50,148.54,42.02,8.59">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="635" to="642" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="3,319.50,177.20,238.50,8.64;3,319.50,188.16,238.50,8.64;3,319.50,198.94,238.50,8.82;3,319.50,209.89,177.49,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,391.00,188.16,167.00,8.64;3,319.50,199.11,137.55,8.64">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,479.37,198.94,78.62,8.59;3,319.50,209.89,123.56,8.59">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,319.50,227.59,238.50,8.64;3,319.50,238.37,238.50,8.82;3,319.50,249.51,90.36,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,473.03,227.59,84.97,8.64;3,319.50,238.55,125.21,8.64">On relevance weights with little relevance information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,461.81,238.37,73.53,8.59">ACM SIGIR Forum</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,319.50,267.03,238.50,8.64;3,319.50,277.81,238.50,8.82;3,319.50,288.77,238.50,8.59;3,319.50,299.73,216.98,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,463.80,267.03,94.21,8.64;3,319.50,277.99,143.82,8.64">Evaluating and predicting answer quality in community qa</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pomerantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,483.07,277.81,74.93,8.59;3,319.50,288.77,238.50,8.59;3,319.50,299.73,145.13,8.59">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,319.50,317.42,238.50,8.64;3,319.50,328.38,238.50,8.64;3,319.50,339.16,238.50,8.82;3,319.50,350.12,122.03,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,319.50,328.38,238.50,8.64;3,319.50,339.34,205.07,8.64">Discovering high quality answers in community question answering archives using a hierarchy of classifiers</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Toba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Adriani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,534.96,339.16,23.04,8.59;3,319.50,350.12,64.47,8.59">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,319.50,359.28,238.50,8.64;3,319.50,370.24,238.50,8.64;3,319.50,381.02,238.50,8.82;3,319.50,391.98,232.64,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,471.77,359.28,86.24,8.64;3,319.50,370.24,238.50,8.64;3,319.50,381.20,102.29,8.64">Ecnu: Using multiple sources of cqa-based information for answer selection and yes/no response inference</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,441.87,381.02,116.13,8.59;3,319.50,391.98,171.16,8.59">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>SemEval 236</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
