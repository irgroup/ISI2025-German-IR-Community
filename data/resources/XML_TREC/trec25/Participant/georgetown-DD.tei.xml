<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.06,116.95,329.24,12.62;1,213.01,134.89,189.34,12.62">An Investigation of Basic Retrieval Models for the Dynamic Domain Task</title>
				<funder ref="#_2HytbW9 #_krvM3h8">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_tarvMXm">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,229.04,172.56,64.48,8.74"><forename type="first">Razieh</forename><surname>Rahimi</surname></persName>
							<email>razieh.rahimi@georgetown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.21,172.56,70.11,8.74"><forename type="first">Grace</forename><forename type="middle">Hui</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.06,116.95,329.24,12.62;1,213.01,134.89,189.34,12.62">An Investigation of Basic Retrieval Models for the Dynamic Domain Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">684FAF437C02E9457A8089B4869BFD79</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC dynamic domain is a new challenging task, which aims to simultaneously optimize the performance of retrieval and the number of iterations to accomplish the search task in a session-based search environment with a more sophisticated feedback information from the user. As a first step towards developing an effective search systems for this task, we investigate the characteristics of the newly created dataset for this task, and performance of basic well-known retrieval models for it. Our investigation demonstrates that the query sets contain multiple difficult queries, where initial results may provide very limited evidence for improvement in subsequent iterations. The new setting of the task and characteristics of dataset stress the need for more comprehensive metrics of performance evaluation, in terms of result diversity as an example.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC dynamic domain (DD) task is an interactive search process where the retrieval system updates the search results based on more comprehensive feedback from a simulated user. Feedback information in this task indicates which passages of each retrieved document is related to which subtopics of the query. The goal is that the search system provides users, in the least number of iterations, with enough information regarding all aspects of a query, utilizing online feedback from users.</p><p>TREC DD task is a challenging retrieval problem due to several reasons. First of all, it is a session-based search task, which requires a more complex search system than the one with the independence assumption about queries. Second, the search system should decide to terminate the search session for a given query. Predicting whether or not the user information need is satisfied only based on partial user's feedback is difficult. Third, the granularity of search items and feedback information is different; the system searches over documents, while user's feedback specifies the relevant passages to the query. Last, receiving feedback on at most 5 documents for each query at each iteration, may lead to a limited amount of feedback information, especially for difficult queries.</p><p>We participated in TREC DD task 2016, and our main goal was to deduce properties of the task and its new dataset, that make it different from ad-hoc retrieval. Evaluation of basic retrieval models for the first iteration demonstrates that the query set of this task contains multiple difficult queries, which makes improvement in subsequent iterations based on previously received feedback information more challenging. For subsequent iterations of retrieval, given the passage-based feedback in the DD task, we adjust the well-known relevance feedback model RM3 <ref type="bibr" coords="2,213.07,155.86,10.52,8.74" target="#b5">[6]</ref> which results in higher performance of retrieval compared to the original feedback model.</p><p>In addition, we investigated the dataset for the DD task from different points of view. Most importantly, investigation of the query set reveals that a passage relevant to a subtopic of a query usually occurs in many documents. This fact, given the difference in the granularity of retrieval and feedback segments, can cause an issue in the DD task. Recall that in the dynamic domain task, the search system provides ranked lists of documents to the user, while receives passagebased feedback from the user. And finally, the system performance is evaluated using metrics measuring the quality of (ranked) lists of documents. This setting along with the provided test dataset poses an issue, since it does not affect the performance metrics, even novelty measures, that the search system provides new information regarding an already explored subtopic to the user or provides redundant information. Therefore, search systems may tend to return redundant information instead of exploring towards new information, which is not desirable for users.</p><p>The rest of this report is organized as follows. In Section 2, retrieval models adopted for the first and subsequent iterations are described. Characteristics of the dataset for the DD task and evaluation results are discussed in Section 3. Finally, the report is concluded in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval and Feedback Models</head><p>In this section, we describe the retrieval models used to provide results in our submitted runs to the TREC DD track. Before proceeding to the retrieval models, we first give a formal descriptions of the DD task. In the DD task, the search system receives an initial query q, and its goal is to satisfy the underlying information need using a given collection C of documents in an interactive search process. At each iteration i, the system presents at most 5 documents {d i j } 5 j=1 to the user, and receives feedback in a form that indicates which passages of the presented documents are relevant to which subtopics of the query, as well as their relevance degrees. Specifically, the feedback is a set of (p, s, r, d i j ) tuples where p is the passage of document d i j that is relevant to subtopic s of the query with relevance degree r. Based on the received feedback from iteration 1 to i, the search system decides to stop the search session for query q, or to start the new iteration i + 1 and present a new list of documents to the user adopting an adjusted retrieval model.</p><p>Following we describe how the results of each iteration are produced. We describe the retrieval models in two parts. First, the retrieval models used to generate the results of the first iteration is described where there is no feedback information from the (simulated) user. Incorporation of feedback information from the (simulated) user into the retrieval model is then described.</p><p>First iteration. In the first iteration, the document list to be shown to the user is prepared using the following methods.</p><p>Language modeling framework. In this approach, the top 5 documents retrieved using the language modeling framework are selected as the result of the first iteration. To rank documents using the language modeling framework, document language models are smoothed using Dirichlet prior smoothing <ref type="bibr" coords="3,442.42,179.98,14.61,8.74" target="#b9">[10]</ref>.</p><p>Relevance feedback. In this approach, feedback information from an initial retrieval is used to expand queries. Note that the query expansion in this iteration is based on the pseudo-relevance feedback information, not precise feedback information from the (simulated) user. The top 5 documents retrieved with respect to the expanded queries are then shown to the user as the result of the first iteration.</p><p>For query expansion based on feedback documents, we use the relevance model <ref type="bibr" coords="3,165.02,276.04,9.96,8.74" target="#b5">[6]</ref>. In the first estimation method of relevance model, expansion terms are selected from the top k initially retrieved documents as follows:</p><formula xml:id="formula_0" coords="3,237.79,309.19,242.81,30.32">p(w|θ RM1 ) = k i=1 p(q|d i ) Z p(w|d i ),<label>(1)</label></formula><p>where p(q|d i ) is the retrieval score of document d i in the initial ranking for query q, Z = k i=1 p(q|d i ) is to normalize the retrieval scores, and p(w|d i ) is estimated using the maximum likelihood estimate (MLE) method as c(w,di)</p><p>|di| . Initial retrieval scores p(q|d) are estimated using the KL-divergence between maximum likelihood query model and document model computed using Dirichlet prior smoothing. The query language model is estimated using the MLE method as follows:</p><formula xml:id="formula_1" coords="3,268.11,437.20,212.48,22.31">p(w|θ q ) = c(w, q) |q| ,<label>(2)</label></formula><p>where c(w, q) is the count of word w in query q and |q| shows the total number of words in the query. In RM3 model, relevance expansion terms are linearly combined with original query terms as follows:</p><formula xml:id="formula_2" coords="3,214.27,526.76,266.32,9.65">p(w|θ RM3 ) = λp(w|θ q ) + (1 -λ)p(w|θ RM1 ),<label>(3)</label></formula><p>where λ ∈ [0, 1] is the interpolation parameter. The documents retrieved using the θ RM3 query language model are used to produce the result of the first iteration. BM25. In this setting, the top 5 documents ranked by the BM25 retrieval model are presented to the user.</p><p>LDA Clustering on initial results. In this approach, We adopt a topic modeling algorithm to diversify search results in terms of their coverage of different subtopics, as done in several studies <ref type="bibr" coords="3,304.73,633.20,9.96,8.74" target="#b8">[9]</ref>. In this regard, we first rank documents by the language modeling framework where document language models are smoothed using Dirichlet prior smoothing. We then cluster the top k retrieved documents into 5 clusters using a variant of the Latent Dirichlet Allocation (LDA) algorithm proposed in <ref type="bibr" coords="4,266.20,131.95,9.96,8.74" target="#b0">[1]</ref>. The setting of k is discussed in Section 3. We opt to partition the top documents into 5 clusters, since queries of the DD track have 5 subtopics on average, as mentioned in the track description. Performing LDA on the document set, the document-cluster probability distributions are estimated. Let p(c i |d) denote the probability of cluster i given document d, where one has 5 i=1 p(c i |d) = 1. Based on these probability distributions, each document is assigned to the cluster that has the highest probability given the document, i.e., document d is assigned to the cluster argmax c p(c|d). The documents in each cluster are then sorted based on their retrieval scores. Finally, the top first document from each cluster is selected to be shown to the user.</p><p>Subsequent iterations. In the following iteration i &gt; 1, the search system has feedback information from the first i -1 iterations. Let us denote the set of all feedback information available prior to iteration i as F i which consists of feedback tuples. We aim to obtain a diversified expansion of the original query using the feedback set F i . In this regard, we try to adjust a relevance model for feedback in the language modeling retrieval framework according to the different type of feedback information in the DD task compared to ad-hoc retrieval. To adjust the relevance model for the DD task, the language model of feedback information from the Jig system is first estimated as follows:</p><formula xml:id="formula_3" coords="4,243.62,361.40,236.98,26.88">p(w|θ JRM1 ) = f ∈F r f Z p(w|p f ),<label>(4)</label></formula><p>where p(w|p f ) denotes the language model of passage texts which are estimated using the MLE method, r f is the rating of the passage, and Z is a normalization factor for ratings. In the next step, the language model of feedback information is combined with the language model of original query as follows:</p><formula xml:id="formula_4" coords="4,210.20,450.77,270.39,9.65">p(w|θ JRM3 ) = λp(w|θ q ) + (1 -λ)p(w|θ JRM1 ),<label>(5)</label></formula><p>We noticed that the passage texts in feedback information are usually short, containing one sentence where each word occurs only one time. Therefore, there is no difference between specific words and more general words in the language model of the passage text. This issue becomes acute when there are few passages in the feedback set for a query, assume the extreme case when there is only 1 passage. Thus, discrimination between specific and general words in feedback information for a query according to the language model θ JRM1 is usually not possible.</p><p>The issue of non-discrimination between words can be due to the fact that the inverse document frequency (IDF) effect is not considered in weighting feedback terms in relevance model for feedback information <ref type="bibr" coords="4,351.54,588.28,9.96,8.74" target="#b2">[3]</ref>. This fact has the potential to be more problematic in the setting of the DD task rather than ad-hoc retrieval. Thus, to resolve the non-discrimination issue for the DD task setting, we add an IDF-element in estimation of feedback language models as follows: where df(w) denotes the document frequency of term w, and |C| shows the number of documents in the collection. Finally, this language model is interpolated with the query language model similar to Eq. 5, referred to as JRM3-IDF model. The top 5 documents retrieved with respect to the expanded query using JRM3-IDF mdoel are selected as the result of iteration i to be presented to the user.</p><formula xml:id="formula_5" coords="4,201.80,640.55,278.80,27.35">p(w|θ JRM1-IDF ) = f ∈F i r f Z p(w|p f ) log |C| + 2 df(w) + 1 ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Results</head><p>In this section, we report the results of the retrieval and feedback models described in the previous section. Experimental setup. We index the collection of each domain separately using the Galago toolkit. <ref type="foot" coords="5,243.42,350.39,3.97,6.12" target="#foot_0">1</ref> Words are stemmed using Krovetz stemmer. Queries of each domain are searched over the index of the corresponding collection, since the domains of queries are specified according to the track guideline.</p><p>Evaluation metrics. The results of the DD tasks, according to the track guideline, is evaluated using three categories of metrics. The first category of metrics measures how well a search system performs considering the number of iterations done to obtain the document set. This category includes Cube Test (CT) and Average Cube Test (ACT) measures proposed in <ref type="bibr" coords="5,417.68,435.65,9.96,8.74" target="#b6">[7]</ref>.</p><p>The second category includes metrics to evaluate the diversification effectiveness of a document ranking, such as α-nDCG@k [2] and nERR-IA <ref type="bibr" coords="5,428.19,459.56,9.96,8.74" target="#b7">[8]</ref>. And the last category contains snDCG metric that evaluates the effectiveness of document rankings over an entire search session <ref type="bibr" coords="5,326.37,483.47,9.96,8.74" target="#b4">[5]</ref>.</p><p>Results of the first Iteration. In Table <ref type="table" coords="5,348.19,495.43,3.87,8.74" target="#tab_0">1</ref>, we report the performance evaluation of the results obtained for the first iteration using different retrieval models. The parameter settings for obtaining the results in this table are as follows. The parameter µ in Dirichlet prior smoothing is set to the default value of 1500. Then, for relevance feedback, the top 10 retrieved documents are used to estimate the language model θ RM1 in Eq. 1, and the top 5 terms in this language model are used to expand the original query terms, where the combination parameter λ in Eq. 3 is set to the default value 0.75. For the run using topic modeling, the top 200 retrieved documents are clustered. We also did not tune the parameters of the topic modeling algorithm, such as the number of iterations, and default parameter values are used. The parameters of BM25 retrieval model are also set to the default values. According to the results in Table <ref type="table" coords="5,455.91,626.94,3.87,8.74" target="#tab_0">1</ref>, the best performance is achieved using the language modeling framework. Since the language modeling framework achieves the highest performance, we use feedback models on this retrieval framework for later iterations, and next we report the performance of adopting feedback models for the second iteration.</p><p>Second iteration. Similar to the parameter settings of the original relevance model that we use for the first iteration, parameters of the JRM3 and JRM3-IDF models are not tuned, and default parameter values are used as mentioned above. The results reported in Table <ref type="table" coords="6,298.13,266.76,4.98,8.74" target="#tab_1">2</ref> are obtained when duplicate documents are removed from the ranked list of the second iteration, and the list contains 5 documents. <ref type="foot" coords="6,184.08,289.10,3.97,6.12" target="#foot_1">2</ref>The results in Table <ref type="table" coords="6,241.10,302.63,4.98,8.74" target="#tab_1">2</ref> show the performance of the proposed adjusted feedback models for the DD task. As reported in the table, adding IDF effect in JRM3-IDF model improves all performance measures. However, the values of CT and ACT scores are decreased in the second iteration, which is not desirable. Results of the first ten iterations. We now report the performance of the retrieval system for the DD task where the result of first iteration is obtained by the language modeling framework, and the results of subsequent iterations are obtained by JRM3-IDF feedback model. Figure <ref type="figure" coords="6,361.50,603.69,4.98,8.74" target="#fig_0">1</ref> shows the performance of this system for the first ten iterations. Similarly, duplicate documents in later iterations are removed, and the size of the result list at each iteration is 5. Based Table <ref type="table" coords="7,226.16,116.91,4.13,7.89">3</ref>. Results of ad-hoc retrieval on Ebola domain.</p><p>MAP P@10 0.2131 0.4519 on the diagram in Figure <ref type="figure" coords="7,251.23,177.53,4.10,8.74" target="#fig_0">1</ref>(a), the CT and ACT scores are decreasing as the number of iteration increases. Thus, first iteration is the best point to stop the search task according to these measures.</p><p>Discussion. The results of ad-hoc retrieval on ebola domain, only first iteration of interaction, are evaluated using mean average precision (MAP) and precision at the top 10 documents (P@10) metrics. The measures are calculated on the top 1,000 documents retrieved by the language modeling framework with Dirichlet prior smoothing. These evaluation results, reported in Table <ref type="table" coords="7,447.53,261.49,3.87,8.74">3</ref>, show that the provided query set contains many difficult queries. About 50% of queries have values of average precision lower than 0.016. These results demonstrate that the search system would not get valuable feedback for many queries after first iteration, and the search system requires more sophisticated method to handle such queries.</p><p>The evaluation results for the polar domain is not provided, since some files in the collection contain duplicate copy of documents. However, this query set also seems to be a difficult query set, since the query set is a mixture of short keyword and verbose queries. Each type of queries needs to be treated differently to achieve an acceptable performance <ref type="bibr" coords="7,301.14,381.32,9.96,8.74" target="#b3">[4]</ref>. Further investigation of dataset reveals that a passage relevant to a subtopic occurs in some documents of the collection. The diagram in Figure <ref type="figure" coords="7,429.85,597.34,4.98,8.74" target="#fig_1">2</ref> shows the frequency of the number of different documents having the same passage relevant to a subtopic of a query, where the maximum number of documents over different passages relevant to a subtopic of a query is counted for each subtopic, and the numbers greater than 50 (about 27 subtopics, and the maximum document frequency is 3231) are removed from the diagram for clearance. The maximum and average document frequencies of different passages relevant to 183 subtopics of 242 total subtopics are greater than 1. This characteristic of the query set along with fine-grained judgment information demonstrates that different documents presented to the user (in the same or different iterations) may contain redundant information, which is not desirable from the user perspective. The user desires new information even regarding an aspect of his/her query for which has already obtained some information. This desire can be interpreted as recall of passages relevant to each subtopic of a topic. The mentioned characteristic of the query set stresses the need for a more sophisticated metric to evaluate diversity in search results. Otherwise, a retrieval model that uses the feedback passages as a new query would probably have higher performance, than the one which uses the feedback passages for query expansion. Using the feedback passages as a new query for retrieval, in addition to provide high accuracy regarding the explored subtopics, may also provide information about new subtopics. This can happen because our investigation shows that there are 2,483 samples among 15,448 unique document-query judgment records (more than 16% of all) that their documents are relevant to more than one subtopic of the query.</p><p>Finally, comparison of different stopping strategies based on current performance metrics seems not trivial, since cube test metrics are generally decreasing with the increasing of iteration number, and diversity measures are always increasing due to evaluation on accumulated results. Therefore, the result of comparison between stopping strategies is readily obvious, the later a strategy decides to stop the search session, the lower the values of cube test metrics, the higher the values of diversity metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this report, we described the various retrieval models used for the dynamic domain task, and presented their results. Our evaluation shows that the query sets of the dynamic domain track contain several difficult queries, and it is not even trivial to get acceptable retrieval performance for the first iteration of interaction with the user. Therefore, improvement of search results for such queries based on the user's feedback is challenging.</p><p>In the dynamic domain task, the search system should deal with data segments of two different granularity levels; the search system provides ranked lists of documents to the user, while receives passage-based feedback from the user. And finally, the system performance is evaluated using metrics measuring the quality of (ranked) lists of documents. This setting along with the provided test dataset poses the following challenges. First, pseudo-relevance feedback techniques need to be adjusted for the setting of dynamic domain task to produce more effective expanded query, as demonstrated in the results of our experiments by adding the inverse-document frequency heuristic. Second, evaluation metrics need also to be adjusted to evaluate novelty in two levels, first covering different subtopics of a query, and then covering as diverse information as possible regard-ing each subtopic. This 2-level result diversification is possible in the setting of the dynamic domain task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,190.00,532.12,235.36,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Performance evaluation of results of ten iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,151.52,551.61,312.32,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Histogram of document frequency of a passage relevant to a subtopic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,176.23,116.91,262.91,58.46"><head>Table 1 .</head><label>1</label><figDesc>Results of the first iteration using different approaches.</figDesc><table coords="5,192.89,136.98,226.79,38.40"><row><cell></cell><cell>CT</cell><cell cols="2">ACT α-nDCG nERR-IA nSDCG</cell></row><row><cell>LM-Dirichlet</cell><cell cols="3">0.2174 0.1516 0.2952 0.2691 0.1901</cell></row><row><cell>RM3</cell><cell cols="2">0.1688 0.1270 0.2537</cell><cell>0.2408 0.1710</cell></row><row><cell cols="3">LM-Dirichlet + LDA 0.1966 0.1447 0.2692</cell><cell>0.2534 0.1652</cell></row><row><cell>BM25</cell><cell cols="2">0.1818 0.1291 0.2434</cell><cell>0.2261 0.1564</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,170.86,116.91,273.64,50.89"><head>Table 2 .</head><label>2</label><figDesc>Results of the second iteration using different approaches.</figDesc><table coords="6,192.87,136.98,229.61,30.82"><row><cell></cell><cell>CT</cell><cell>ACT α-nDCG nERR-IA nSDCG</cell></row><row><cell cols="3">1st Iter LM-Dirichlet 0.2174 0.1516 0.2952</cell><cell>0.2691 0.1901</cell></row><row><cell>2nd Iter</cell><cell cols="2">JRM JRM + IDF 0.1434 0.1422 0.3473 0.2952 0.1118 0.1384 0.1411 0.3340 0.289 0.1067</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,144.73,657.79,166.21,7.86"><p>http://www.lemurproject.org/galago.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,646.84,335.86,7.86;6,144.73,657.79,330.72,7.86"><p>The results reported by the track organizers do not have duplicate documents, but for some cases the ranked lists of the second iteration does not have 5 documents.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was supported by <rs type="funder">DARPA</rs> grant <rs type="grantNumber">FA8750-14-2-0226</rs>, <rs type="funder">NSF</rs> grant <rs type="grantNumber">IIS-145374</rs>, and <rs type="funder">NSF</rs> grant <rs type="grantNumber">CNS-1223825</rs>. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tarvMXm">
					<idno type="grant-number">FA8750-14-2-0226</idno>
				</org>
				<org type="funding" xml:id="_2HytbW9">
					<idno type="grant-number">IIS-145374</idno>
				</org>
				<org type="funding" xml:id="_krvM3h8">
					<idno type="grant-number">CNS-1223825</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,253.17,337.63,7.86;9,151.52,264.13,329.07,7.86;9,151.52,275.09,329.07,7.86;9,151.52,286.05,119.29,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,151.52,264.13,272.74,7.86">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,447.41,264.13,33.18,7.86;9,151.52,275.09,262.45,7.86">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
	<note>JMLR</note>
</biblStruct>

<biblStruct coords="9,142.96,297.01,337.63,7.86;9,151.52,307.97,329.07,7.86;9,151.52,318.93,329.07,7.86;9,151.52,329.89,329.07,7.86;9,151.52,340.84,92.40,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,234.18,307.97,228.54,7.86">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,318.93,329.07,7.86;9,151.52,329.89,216.63,7.86">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,351.80,337.63,7.86;9,151.52,362.76,329.07,7.86;9,151.52,373.72,290.15,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,283.93,351.80,196.66,7.86;9,151.52,362.76,47.46,7.86">A theoretical analysis of pseudo-relevance feedback models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,218.41,362.76,262.18,7.86;9,151.52,373.72,81.97,7.86">Proceedings of the 2013 Conference on the Theory of Information Retrieval, ICTIR &apos;13</title>
		<meeting>the 2013 Conference on the Theory of Information Retrieval, ICTIR &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,384.68,337.63,7.86;9,151.52,395.64,260.76,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,273.17,384.68,166.97,7.86">Information retrieval with verbose queries</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,447.77,384.68,32.82,7.86;9,151.52,395.64,168.36,7.86">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="209" to="354" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,406.60,337.63,7.86;9,151.52,417.56,329.07,7.86;9,151.52,428.52,329.07,7.86;9,151.52,439.47,257.57,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,435.87,406.60,44.72,7.86;9,151.52,417.56,250.12,7.86">Discounted cumulated gain based evaluation of multiple-query ir sessions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M L</forename><surname>Delcambre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,423.34,417.56,57.25,7.86;9,151.52,428.52,329.07,7.86;9,151.52,439.47,32.70,7.86">Proceedings of the IR Research, 30th European Conference on Advances in Information Retrieval, ECIR&apos;08</title>
		<meeting>the IR Research, 30th European Conference on Advances in Information Retrieval, ECIR&apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="4" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,450.43,337.64,7.86;9,151.52,461.39,329.07,7.86;9,151.52,472.35,329.07,7.86;9,151.52,483.31,48.38,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,279.59,450.43,133.99,7.86">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,434.71,450.43,45.88,7.86;9,151.52,461.39,329.07,7.86;9,151.52,472.35,174.83,7.86">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,494.27,337.64,7.86;9,151.52,505.23,329.07,7.86;9,151.52,516.19,329.07,7.86;9,151.52,527.15,247.08,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,329.25,494.27,151.34,7.86;9,151.52,505.23,232.73,7.86">The water filling model and the cube test: Multi-dimensional evaluation for professional search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,407.08,505.23,73.51,7.86;9,151.52,516.19,329.07,7.86;9,151.52,527.15,39.41,7.86">Proceedings of the 22Nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13</title>
		<meeting>the 22Nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,538.10,337.64,7.86;9,151.52,549.06,329.07,7.86;9,151.52,560.02,329.07,7.86;9,151.52,570.98,137.98,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,241.97,538.10,238.63,7.86;9,151.52,549.06,35.28,7.86">Evaluating diversified search results using per-intent graded relevance</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,210.18,549.06,270.41,7.86;9,151.52,560.02,253.57,7.86">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11</title>
		<meeting>the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1043" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,581.94,337.63,7.86;9,151.52,592.90,160.29,7.86" xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,336.30,581.94,144.29,7.86;9,151.52,592.90,67.12,7.86">Search result diversification. Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="90" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,603.86,337.97,7.86;9,151.52,614.82,329.07,7.86;9,151.52,625.78,329.07,7.86;9,151.52,636.73,247.28,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,246.21,603.86,234.38,7.86;9,151.52,614.82,125.16,7.86">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,297.77,614.82,182.82,7.86;9,151.52,625.78,329.07,7.86;9,151.52,636.73,39.60,7.86">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
