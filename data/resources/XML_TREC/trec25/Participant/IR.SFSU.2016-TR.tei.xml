<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.14,154.51,420.91,15.11;1,258.43,176.43,98.36,17.96">San Francisco State University (SFSU) at Total Recall Track of TREC 2016</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,213.26,200.32,90.36,10.48"><forename type="first">Mon-Shih</forename><surname>Chuang</surname></persName>
							<email>mchuang@mail.sfsu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">San Francisco State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.60,200.32,88.39,10.48"><forename type="first">Anagha</forename><surname>Kulkarni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">San Francisco State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.14,154.51,420.91,15.11;1,258.43,176.43,98.36,17.96">San Francisco State University (SFSU) at Total Recall Track of TREC 2016</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">851E3EB46C17F186C1DAAEB55861EE59</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of San Francisco State University group in Text Retrieval Conference (TREC) 2016 Total Recall Track from National Institute of Standard and Technology (NIST).</p><p>The TREC series provide large test collections and judgements for participant to design Information Retrieval (IR) systems for different proposes. The purpose of Total Recall Track is seeking text search system which achieves high recall with minimum number of return documents.</p><p>This year, our team participates all automatic tasks, including 34 topics in athome task and 2 datasets in sandbox task.</p><p>Our system is built based on the autonomous technology-assisted review (Auto TAR) model <ref type="bibr" coords="1,283.06,463.33,13.19,8.74" target="#b0">[1]</ref>, which is also the baseline of Total Recall Track. In this paper, we will introduce several approaches which have improved the evaluation metrics compare to the baseline model. Our enhanced model combines seed expansion and feature engineering including adding n-gram, eliminating stop words, and preserving words contain digits.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The objectives of Total Recall Track came from the technology-assisted review (TAR) problem, which is "the iterative retrieval and review of documents from a collection until a substantial majority or all of the relevant documents have been reviewed." <ref type="bibr" coords="1,250.37,653.99,10.52,8.74" target="#b0">[1]</ref> The goal of TAR is to maximize effectiveness of creating test collection for IR evaluation. That is, a optimal document selection algorithm to (1) send all relevant documents to reviewers with minimum number of returns (2) decide when to stop reviewing for reviewers. From TAR, Cormack and Grossman came up with autonomous technology-assisted review (Auto TAR) model, which is also the baseline model of Total Recall Track. The algorithm of baseline model will be described in section 2.</p><p>In Total Recall Track, the position of human reviewer is replaced by automated relevance assessor with pre-processed relevant judgements to evaluate systems from participants.</p><p>The objectives for participants are the inherited from TAR problem, "to submit as many documents containing relevant information as possible, while submitting as few documents as possible", and "indicate when the submission is reasonable to stop, because the effort to review more documents would be disproportionate to the value of any further relevant documents that might be found."</p><p>The first objective is evaluated by recall-precision curves, gain curves, and recall evaluated at aR+b documents submitted, for all combinations of a = 1, 2, 4 and b = 0, 100, 1000. R is number of total relevant documents for each topic. The second objective requires the participants to use "call your shot" API at the point that system should stop. Then the recall and precision at this point will be evaluated using measures like F1 and other utility measures. In section 3, our methodology to achieve these two objectives will be explained. Our system, is tested on athome1, athome2, athome3 datasets and topics, which are all provided by Total Recall Track coordi-nators. The experiment result and analysis will be demonstrated in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline Retrieval Model</head><p>The baseline system provided by track organizer adopts a supervised classification approach within the framework of continuous active learning. We provide a brief description of the baseline system here because our proposed approach builds on it. The provided baseline model implementation (BMI) adopts an iterative approach that uses support vector machine (SVM) to learn document classification models that label each candidate document as relevant or non-relevant. Specifically, the BMI approach proceeds as follows.</p><p>Preprocessing step: For each document in the collection, parse, and transform it into a tfidf feature vector.</p><p>For each query topic:</p><p>1. Label the topic itself as the first data-point from relevant class, and add it to the training set.</p><p>2. Use uniform sampling to select 100 documents at random from the collection, and add them to the training set as data-points from the non-relevant class. Let D: set of 100 sampled documents.</p><p>3. Learn a classification model using SVM and the compiled training set.</p><p>4. Apply the learned model to predict relevance label for each document not in the training set.</p><p>5. Sort the documents using weight returned from SVM, whose value represents the distance from data-points to the decision hyperplane, and with sign '+' for relevant class, '-' for non-relevant class. Then select L documents with highest weights, and request their actual relevance labels. L is initialized to 1, and increases by L/10 at every iteration.</p><p>6. Add the reviewed documents to the training set.</p><p>7. Remove documents from set D (step 2) that were not selected for review.</p><p>8. Go back to step 2 until 100 iteration are complete.</p><p>9. Send all unreviewed documents to be judged, then stop.</p><p>3 Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reducing Increasing Rate of Batch Size</head><p>After each iteration of re-classification, BMI sends a batch of documents to the reviewer for labelling. A drawback of sending a batch of documents at-a-time is that documents with similar contents are sent for labelling in the same iteration. This wastes labelling effort. For instance, requesting labels for all the duplicates (or near-duplicates) of a document is unnecessary. If we know the label for one of the copies, then all the duplicates will have high chance to be classified with the same label. In the current system this problem of wasted labelling effort occurs because documents with similar contents are likely to have same weight, and comparable ranks, and thus they tend to be sent in the same iteration. There are several reasons why duplicate documents occur in a collection. In athome1, which is an email collection, an email may be sent to multiple receivers, or an email received may be forwarded, both of which results in multiple copies of a document. In athome3, which is a local news collection, one article can be referenced by different news site, and the same event may be reported by multiple news site in slightly different words.</p><p>The problem of wasted effort can be completed eliminated by sending only one document at a time for labelling. However, one-document-at-a-time approach is highly inefficient. For instance, 300,000 iterations of re-classification will be required to process the athome1 dataset, which translates to prohibitively high runtime.</p><p>These observations motivate our experiments where we model the batch-size (L) as a parameter, and investigate its influence on effectiveness and efficiency. The baseline approach increases the batchsize, L, by L/10 after each iteration. In our runs, we increase the batch-size by L/12, L/15, and L/20. The rate at which the batch-size is increased is progressively slower for the three settings. We expect this to lower the wastage of labelling effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Seed Expansion using Wikipedia</head><p>In BMI, seed refers to the documents labeled as relevant before the first iteration. BMI uses the topic itself as the seed document, thus the typical length of a seed "document is between 2 to 5 words. Before the first relevant document is retrieved from the collection, the seed document is the only information about the users information need(s). For some topic like 109 "Scarlet Letter Law" and 2130 "Surely Bitcoins Can Be Used", the BMI retrieves more than 30 non-relevant documents before the first relevant document is retrieved, which hurts the precision significantly. In such topics, the terms in the topic do not provide enough information about the information need(s). This inspires our approach for enhancing the seed document. To expand information before any document is retrieved, we choose Wikipedia <ref type="bibr" coords="3,286.88,438.79,13.77,8.74" target="#b1">[2]</ref> for our external source. Wikipedia is an online collaborative encyclopedia which provides a wide coverage of topics and events. Since it a well curated, high-quality resource, it has been utilized for many problems such as clustering <ref type="bibr" coords="3,186.87,498.57,11.88,8.74" target="#b2">[3]</ref>, question answering <ref type="bibr" coords="3,283.33,498.57,12.98,8.74" target="#b3">[4]</ref>, and patent search <ref type="bibr" coords="3,146.73,510.53,12.06,8.74" target="#b4">[5]</ref>. A Wikipedia page is organized as data fields including title, url, summary, content, images, and links. In our approach, we conduct two different runs, one using Wikipedia summary, and another using the content data field as the seed document. In our approach, we send the original topic as the query to Wikipedia search API. If multiple pages are returned, only the top one Wikipedia page is used. If the search API returns an ambiguity page, then no Wikipedia source is used. The summary/content field of the Wikipedia page and original topic are combined to expanded seed. The expanded seed will remain in the training set for every iteration of reclassification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unigram/Bigram SVM Features</head><p>This approach is inspired by the concept of phrase search. When the query contains more than one term, often all of some of these terms form a phrase. For example, the following query barack obama white house contains two phrases barack obama and white house. Data analysis of the athome1 task revels that the relevant documents usually contain the query phrases, while the non-relevant documents contain only single terms, and often these single terms convey slightly different meaning from the query topic. The main reason for this pattern is word ambiguity. Individual words are often ambiguous, but the other words in the phrase help resolve the ambiguity. For instance, topic 103 "Manatee Protection" and topic 108 "Manatee County", both contain the word "Manatee", but the former refers to the name of a kind of mammal, the later refers the name of a county in Florida.</p><p>Since BMI use unigram inverted index and stores no term position information, it cannot support phrasal queries. As such, we add bigram features in our model. It is important to note that unigrams are necessary to achieve high recall. Not all the relevant documents contain the query phrases. Thus we use both, bigram and unigram features in our model. Unigrams to boost recall and bigrams to improve precision. The query topics are also convert to the seed documents with both unigram and bigram features.</p><p>While bigram model improves the precision of retrieving relevant documents, the runtime efficiency significantly drops since the feature space explodes. In our implementation, the bigram/unigram model contains 3 million features in total while the original unigram model only contains 160,000 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature Pruning</head><p>To balance the efficiency and effectiveness of our bigram implementation, we tried the following two simple feature pruning techniques. (1) Prune rare ngram. DF &lt; x. We experimented with cutoff of 3 and 5. (2) Prune ngram that contain one or more English stop words. A 25 word list provided in the Introduction to Information Retrieval book. <ref type="bibr" coords="3,505.62,665.94,12.97,8.74" target="#b5">[6]</ref>  used for feature pruning.</p><p>The 25 pruned stop words used are : "a", "an", "and", "are", "as", "at", "be", "by", "for", "from", "has", "he", "in", "is", "it", "its", "of", "on", "that", "the", "to", "was", "were", "will", "with"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Seed Expansion using Google Word2Vec</head><p>One of the major challenges for improving recall is the classic problem of vocabulary gap. Since most topics are short (2 to 5 words) many relevant documents do not contain any of the terms mentioned in the topic. That is, there is a gap in the vocabulary that generated the topic versus the one that generated such relevant documents. In order to bridge this gap we develop an approach that expands each topic with related words. To identify the set of related words for a topic we employ Word2Vec <ref type="bibr" coords="4,193.03,521.92,14.40,8.74" target="#b6">[7]</ref>, a popular approach introduced by Google that computes vector representation of words/phrases using neural network. The objective of Word2Vec neural network is to optimize prediction of nearby words by converting them to similar vectors. For example, in the training document, if the word "Paris" and the word "France" have very high possibility to occurs near to each other, the cosine of angle between the computed vectors representation of them will closer to positive 1.</p><p>In our approach, the corpus of current task was used as the training data for Word2Vec. Therefore, the prediction of surrounding words is com-puted according to the behavior of the dataset. Since Word2Vec doesn't apply any stemming for the training document, the dataset has to be preprocessed with porter stemmer.</p><p>To combine this approach with our bigram model mentioned in section 3.3, we add bigram information to the training text by concating adjacent words together by underscore ' '. For example, phrases "scarlet letter law" will be convert to scarlet letter, letter law after applying the bigram converter.</p><p>Finally, we use each topic as input of Word2Vec distance predicting tool. The distance predicting tool imports the vectors representation generated by neural network, and computes top 40 words/phrases having highest chance occurs nearby the topics.</p><p>In our experiments, we set the cosine score ≥ 0.5 as the threshold for adding terms into the seed document. If there are no terms with score ≥ 0.5, then the topic is not expanded. For topic 106, all returned terms by Word2Vec having cosine score ≥ 0.5. Table (2) shows the top 1 terms selected using this approach with bigram/stop-words-pruning training text for each topic in athome1.</p><p>From some of the words/phrases returned this way, we can see their clear relationship with the original information need(s). For example, the top ranked result from topic 105 "Affirmative Action" is "elimin race", which is related to the information need(s). For topic 108 "Manatee County", the top ranked result is "escambia counti", which is another county in Florida state. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Preserve Words Contain Digits</head><p>While analyzing the relevant documents for each ath-ome1 topic, we found the topic 109 "Scarlet Letter Law" is another name of "House Bill 141", and most of relevant documents of topic 109 contain only the phrase "HB 141" but don't contain any word in the topic.</p><p>Inspired by this characteristic of topic 109, and the fact that BMI skips words contain digits[0-9] when building the inverted index of corpus. We expect preserving words contain digits or pure numbers can improve the recall of BMI. The possible drawback of keeping numbers is that numbers can hold much more different kind of meanings than English words, which leads to retrieve non-relevant documents. For this reason, we combine this approach only with bigram model. As bigram ,the meaning of the numbers like "141" in "HB 141" can be determine from the context, we expect keeping numbers won't have much negative effect on precision.  while the increasing rate gets smaller except the one at 100% recall. The reason of this approach has no improvement on the late stage is that we don't increase the number of iteration, and with smaller batch size, our runs have more unreviewed documents than BMI in the last iteration. Which result in worse precisions on the late stage. If a model is able to retrieve most of the relevant documents before the last iteration, it will have less drawback from reducing the increasing rate of batch size. In other words, the better a model performs on precision , the effect of tuning increasing rate of batch size becomes more viable.    From either the experiment using Wikipedia or Word2Vec, we observed seed expansion is especially helpful to improve precision in the early stage. This observation matches our goal of the seed expansion approaches, which is expanding information need(s) before any review effort is spent. But although these 2 seed expansion approaches can retrieve the first relevant documents earlier than BMI, they don't improve the precision on late stage. The reason we is that after more documents have been reviewed and labelled, the influence of the seed documents becomes relatively smaller in the training set of classification.     The feature pruning is not included in our approaches of improving evaluation metrics at first. Our initial objective of feature pruning is to reduce the running time of the bigram model while having least impact on the evaluate metrics. Although we have expected eliminating less representative features will result in slightly drops on all of the evaluate metrics, in our experiment runs, we found the run of eliminating stop words has better result on evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Seed Expansions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unigram/Bigram SVM Features</head><p>By further analysis on the improved metrics topic by topic, we found the improvement on topic 109 is the main reason of the better metrics after removing stop words. As mentioned in the section 3.6, BMI performs bad on topic 109 "Scarlet Letter Law" since it fails to find documents contain "HB 141" on early stage. After removing stop words from the features, our model retrieves the documents contains phrase "HB 141" earlier than BMI. The reason is that removing stop words prevents the classifier comparing less meaningful common words, and can reduce some misclassification happened this way.   <ref type="figure" coords="7,353.00,361.42,3.87,8.74" target="#fig_11">6</ref>. shows the precision-recall curve before and after preserving words contain digits. This approach leads to a small improvement of the metrics either on early stage or late stage. As we expected, since it is capable to determine the meaning of numbers from the bigram context, keeping digits doesn't have negative effects on the evaluation metrics. Figure <ref type="figure" coords="7,352.38,570.30,3.87,8.74" target="#fig_14">7</ref>. shows the precision-recall curve of BMI and our experiment run. Figure <ref type="figure" coords="7,458.89,582.26,3.87,8.74">8</ref>. shows the gain curve of BMI and our experiment run. Table <ref type="table" coords="7,502.14,594.21,3.87,8.74" target="#tab_2">3</ref>. shows the recall when retrieved aR +b documents for all combinations of a = 1, 2, 4 and b = 0, 100, 1000 of BMI and our experiment run, and the percentage of improvement from BMI to our run. Figure <ref type="figure" coords="7,516.76,642.03,3.87,8.74" target="#fig_15">9</ref>. to Figure <ref type="figure" coords="7,341.07,653.99,8.49,8.74" target="#fig_23">17</ref>. show the precision-recall curve of BMI and our experiment run for each topic in athome1 task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Preserving Words Contain Digits</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">The Compilation Experiment Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our experiments shows combining the methodology of reducing increasing rate of batch size, seed expansion using Wikipedia source and Google Word2Vec tool sets, feature engineering include adding bigram features, removing stop words, preseving words contains digits will achieve very high recall, and outperforms BMI in every metrics used in overview paper 2015 <ref type="bibr" coords="8,89.40,451.77,13.05,8.74" target="#b7">[8]</ref> for athome1 task. Our compilation methodology run is also applied to all tasks in Total Recall Track 2016, and we are expecting a good result compares with the other participants this year.           </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.00,642.03,228.65,8.74;5,72.00,653.99,228.65,8.74;5,72.00,665.94,228.64,8.74"><head>4. 1 Figure 1 .</head><label>11</label><figDesc>Figure 1. shows the precision-recall curve before and after reducing the increasing rate of batch size. The precision at every certain recall point is improved</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,310.61,500.48,228.64,8.74;5,310.61,512.44,57.51,8.74;5,310.61,313.89,228.64,171.48"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PR-curve of Reduce Batch Increasing Rate Experiments.</figDesc><graphic coords="5,310.61,313.89,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,84.14,382.75,204.38,8.74;6,72.00,196.16,228.64,171.48"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PR-curve of Wikipedia Experiments.</figDesc><graphic coords="6,72.00,196.16,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,84.26,570.48,204.12,8.74;6,72.00,415.71,228.63,139.66"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PR-curve of Word2Vec Experiments.</figDesc><graphic coords="6,72.00,415.71,228.63,139.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,81.96,594.21,218.68,8.74;6,72.00,606.17,228.65,8.74;6,72.00,618.12,228.64,8.74;6,72.00,630.08,228.64,8.74;6,72.00,642.03,228.64,8.74;6,72.00,653.99,228.64,8.74;6,72.00,665.94,101.34,8.74"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. shows the Precision-Recall curve after combining Wikipedia summary/content and the original topic.Figure 3. shows the Precision-Recall curve after combining words/phrases returned by Word2Vec distance predicting tool and the original topic. Note we only apply the Word2Vec approach over the bigram model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,81.96,594.21,218.68,8.74;6,72.00,606.17,228.65,8.74;6,72.00,618.12,228.64,8.74;6,72.00,630.08,228.64,8.74;6,72.00,642.03,228.64,8.74;6,72.00,653.99,228.64,8.74;6,72.00,665.94,101.34,8.74"><head>Figure 3 .</head><label>3</label><figDesc>Figure 2. shows the Precision-Recall curve after combining Wikipedia summary/content and the original topic.Figure 3. shows the Precision-Recall curve after combining words/phrases returned by Word2Vec distance predicting tool and the original topic. Note we only apply the Word2Vec approach over the bigram model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,329.02,538.53,191.81,8.74;6,310.61,351.94,228.64,171.48"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PR-curve of Bigram Experiments.</figDesc><graphic coords="6,310.61,351.94,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,320.57,570.30,218.68,8.74;6,310.61,582.26,228.64,8.74;6,310.61,594.21,228.64,8.74;6,310.61,606.17,228.64,8.74;6,310.61,618.12,228.64,8.74;6,310.61,630.08,228.64,8.74;6,310.61,642.03,228.64,8.74;6,310.61,653.99,228.65,8.74;6,310.61,665.94,21.03,8.74"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. shows the precision-recall curve before and after adding bigram features. Contrast to expansing seed document, which has less effect after more documents are reviewed and labelled, the approach of enhancing SVM features start to having positive effect as more documents are added to the training set. The improvement is especially significant on the precisions after the recall is greater than 70%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,72.00,311.39,228.65,8.74;7,72.00,124.80,228.64,171.48"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: PR-curve of Feature Pruning Experiments.</figDesc><graphic coords="7,72.00,124.80,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="7,72.00,347.76,127.01,10.52"><head>4. 4</head><label>4</label><figDesc>Feature Pruning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="7,72.00,373.08,228.64,8.74;7,72.00,385.03,228.64,8.74;7,72.00,396.99,109.81,8.74;7,81.96,411.91,218.68,8.74;7,72.00,423.87,228.64,8.74;7,72.00,435.82,228.64,8.74;7,72.00,447.78,228.64,8.74;7,72.00,459.73,228.64,8.74;7,72.00,471.69,228.64,8.74;7,72.00,483.64,228.64,8.74;7,72.00,495.60,228.64,8.74;7,72.00,507.56,228.64,8.74;7,72.00,519.51,54.87,8.74;7,81.96,534.44,218.68,8.74;7,72.00,546.39,228.64,8.74;7,72.00,558.35,228.64,8.74;7,72.00,570.30,228.64,8.74;7,72.00,582.26,228.64,8.74;7,72.00,594.21,228.64,8.74;7,72.00,606.17,228.64,8.74;7,72.00,618.12,228.64,8.74;7,72.00,630.08,228.65,8.74;7,72.00,642.03,228.64,8.74;7,72.00,653.99,228.64,8.74;7,72.00,665.94,156.58,8.74"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. shows the Precision recall curve before and after pruning features with DF≤ 3, DF ≤ 5, and features contain stop words.The feature pruning is not included in our approaches of improving evaluation metrics at first. Our initial objective of feature pruning is to reduce the running time of the bigram model while having least impact on the evaluate metrics. Although we have expected eliminating less representative features will result in slightly drops on all of the evaluate metrics, in our experiment runs, we found the run of eliminating stop words has better result on evaluation metrics.By further analysis on the improved metrics topic by topic, we found the improvement on topic 109 is the main reason of the better metrics after removing stop words. As mentioned in the section 3.6, BMI performs bad on topic 109 "Scarlet Letter Law" since it fails to find documents contain "HB 141" on early stage. After removing stop words from the features, our model retrieves the documents contains phrase "HB 141" earlier than BMI. The reason is that removing stop words prevents the classifier comparing less meaningful common words, and can reduce some misclassification happened this way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="7,310.61,336.14,228.64,8.74;7,310.61,158.00,228.64,163.03"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: PR-curve of Preserving Digits experiments.</figDesc><graphic coords="7,310.61,158.00,228.64,163.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="7,320.57,361.42,218.68,8.74;7,310.61,373.38,228.64,8.74;7,310.61,385.33,228.64,8.74;7,310.61,397.29,228.65,8.74;7,310.61,409.25,228.64,8.74;7,310.61,421.20,228.65,8.74;7,310.61,433.16,205.75,8.74"><head>Figure</head><label></label><figDesc>Figure 6.  shows the precision-recall curve before and after preserving words contain digits. This approach leads to a small improvement of the metrics either on early stage or late stage. As we expected, since it is capable to determine the meaning of numbers from the bigram context, keeping digits doesn't have negative effects on the evaluation metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="7,310.61,485.74,228.64,8.74;7,310.61,497.69,228.64,8.74;7,310.61,509.65,228.64,8.74;7,310.61,521.60,228.65,8.74;7,310.61,533.56,228.64,8.74;7,310.61,545.51,228.65,8.74;7,310.61,557.47,61.76,8.74"><head></head><label></label><figDesc>Our compilation run combines (1) Reducing Increasing Rate of Batch Size to 20/L. (2) Seed Expansion using Wikipedia Summary. (3) Seed Expansion using Wikipedia Content. (4) Seed Expansion using Google Word2Vec. (5) Combining Bigram and Unigram Features (6) Pruning stop words. (7) Preserving words contain digits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="8,72.00,311.39,228.64,8.74;8,72.00,124.80,228.64,171.48"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: PR-curve of BMI and our compilation run.</figDesc><graphic coords="8,72.00,124.80,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="9,72.00,569.54,228.64,8.74;9,72.00,581.49,103.81,8.74;9,72.00,382.95,228.64,171.48"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: PR-curve of BMI and our compilation run on for topic athome100.</figDesc><graphic coords="9,72.00,382.95,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="9,310.61,569.54,228.64,8.74;9,310.61,581.49,103.81,8.74;9,310.61,382.95,228.64,171.48"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: PR-curve of BMI and our compilation run on for topic athome101.</figDesc><graphic coords="9,310.61,382.95,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="10,72.00,342.74,228.64,8.74;10,72.00,354.70,103.81,8.74;10,72.00,156.15,228.64,171.48"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: PR-curve of BMI and our compilation run on for topic athome102.</figDesc><graphic coords="10,72.00,156.15,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18" coords="10,72.00,620.70,228.64,8.74;10,72.00,632.66,103.81,8.74;10,72.00,434.11,228.64,171.48"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: PR-curve of BMI and our compilation run on for topic athome103.</figDesc><graphic coords="10,72.00,434.11,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19" coords="10,310.61,342.74,228.64,8.74;10,310.61,354.70,103.81,8.74;10,310.61,156.15,228.64,171.48"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: PR-curve of BMI and our compilation run on for topic athome104.</figDesc><graphic coords="10,310.61,156.15,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20" coords="10,310.61,620.70,228.64,8.74;10,310.61,632.66,103.81,8.74;10,310.61,434.11,228.64,171.48"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: PR-curve of BMI and our compilation run on for topic athome105.</figDesc><graphic coords="10,310.61,434.11,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21" coords="11,72.00,342.74,228.64,8.74;11,72.00,354.70,103.81,8.74;11,72.00,156.15,228.64,171.48"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: PR-curve of BMI and our compilation run on for topic athome106.</figDesc><graphic coords="11,72.00,156.15,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22" coords="11,72.00,620.70,228.64,8.74;11,72.00,632.66,103.81,8.74;11,72.00,434.11,228.64,171.48"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: PR-curve of BMI and our compilation run on for topic athome107.</figDesc><graphic coords="11,72.00,434.11,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23" coords="11,310.61,342.74,228.64,8.74;11,310.61,354.70,103.81,8.74;11,310.61,156.15,228.64,171.48"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: PR-curve of BMI and our compilation run on for topic athome108.</figDesc><graphic coords="11,310.61,156.15,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24" coords="11,310.61,620.70,228.64,8.74;11,310.61,632.66,103.81,8.74;11,310.61,434.11,228.64,171.48"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: PR-curve of BMI and our compilation run on for topic athome109.</figDesc><graphic coords="11,310.61,434.11,228.64,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,523.42,665.94,15.83,8.74"><head>Table 1 :</head><label>1</label><figDesc>Returned results of Wikipedia Search API using original information of topic for query.</figDesc><table coords="3,523.42,665.94,15.83,8.74"><row><cell>was</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.00,126.76,467.25,166.65"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="5,78.38,126.76,380.74,132.27"><row><cell>Topic</cell><cell>Information Needs</cell><cell>1st result</cell></row><row><cell>athome100</cell><cell>School and Preschool Funding</cell><cell>(None)</cell></row><row><cell>athome101</cell><cell>Judicial Selection</cell><cell>judici appoint</cell></row><row><cell>athome102</cell><cell>Capital Punishment</cell><cell>believ separ</cell></row><row><cell>athome103</cell><cell>Manatee Protection</cell><cell>protect plan</cell></row><row><cell>athome104</cell><cell>New Medical School</cell><cell>school enter</cell></row><row><cell>athome105</cell><cell>Affirmative Action</cell><cell>elimin race</cell></row><row><cell>athome106</cell><cell>Terri Schiavo</cell><cell>terri schindler</cell></row><row><cell>athome107</cell><cell>Tort Reform</cell><cell>reform bill</cell></row><row><cell>athome108</cell><cell>Manatee County</cell><cell>escambia counti</cell></row><row><cell>athome109</cell><cell>Scarlet Letter Law</cell><cell>dai scarlet</cell></row></table><note coords="5,116.67,272.72,422.58,8.74;5,72.00,284.67,256.51,8.74"><p>The First Word/Phrase returned from each topic as input of Word2Vec distance tool, using bigram/stop-words-pruning athome1collection for training.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,78.38,126.76,436.56,153.74"><head>Table 3 :</head><label>3</label><figDesc>Recall at aR+b when a = 1, 2, 4 and b = 0, 100, 1000 of BMI and our compilation run</figDesc><table coords="9,78.38,126.76,408.70,131.88"><row><cell></cell><cell>BMI</cell><cell>SFSU RUN</cell><cell>Percentage of Improvement</cell></row><row><cell>a=1, b=0</cell><cell>0.7144</cell><cell>0.8064</cell><cell>12.88%</cell></row><row><cell>a=1, b=100</cell><cell>0.7408</cell><cell>0.8507</cell><cell>14.84%</cell></row><row><cell>a=1, b=1000</cell><cell>0.9039</cell><cell>0.9628</cell><cell>6.52%</cell></row><row><cell>a=2, b=0</cell><cell>0.9050</cell><cell>0.9751</cell><cell>7.75%</cell></row><row><cell>a=2, b=100</cell><cell>0.9191</cell><cell>0.9809</cell><cell>6.72%</cell></row><row><cell>a=2, b=1000</cell><cell>0.9548</cell><cell>0.9890</cell><cell>3.58%</cell></row><row><cell>a=4, b=0</cell><cell>0.9681</cell><cell>0.9917</cell><cell>2.44%</cell></row><row><cell>a=4, b=100</cell><cell>0.9682</cell><cell>0.9926</cell><cell>2.52%</cell></row><row><cell>a=4, b=1000</cell><cell>0.9738</cell><cell>0.9936</cell><cell>2.03%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,87.50,548.05,213.15,8.74;8,87.50,560.01,213.15,8.74;8,87.50,571.96,213.15,8.74;8,87.50,583.92,95.80,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,87.50,560.01,213.15,8.74;8,87.50,571.96,170.59,8.74">Autonomy and reliability of continuous active learning for technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno>CoRR, abs/1504.06868</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.50,607.00,213.14,8.74;8,87.50,618.95,159.54,9.02" xml:id="b1">
	<monogr>
		<ptr target="https://en.wikipedia.org/" />
		<title level="m" coord="8,87.50,607.00,208.67,8.74">Wikipedia. Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.50,642.03,213.15,8.74;8,87.50,653.99,213.15,8.74;8,87.50,665.94,213.14,8.74;8,310.61,311.39,228.64,8.74;8,310.61,323.35,17.74,8.74;8,326.10,353.24,213.14,8.74;8,326.10,365.19,213.15,8.74;8,326.10,377.15,22.69,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,87.50,653.99,213.15,8.74;8,87.50,665.94,213.14,8.74;8,310.61,311.39,228.64,8.74;8,310.61,323.35,13.31,8.74">Clustering short texts using wikipedia. SIGIR &apos;07 Proceedings of the 30th annual international Figure 8: Gain-curve of BMI and our compilation run</title>
		<author>
			<persName coords=""><forename type="first">Ramanathan</forename><forename type="middle">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gupta</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,326.10,353.24,213.14,8.74;8,326.10,365.19,139.32,8.74">ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="787" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.10,397.07,213.14,8.74;8,326.10,409.03,213.15,8.74;8,326.10,420.98,213.15,8.74;8,326.10,432.94,213.15,8.74;8,326.10,444.89,213.14,8.74;8,326.10,456.85,43.72,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,450.14,397.07,89.11,8.74;8,326.10,409.03,213.15,8.74;8,326.10,420.98,143.54,8.74">Exploiting structure and content of wikipedia for query expansion in the context of question answering</title>
		<author>
			<persName coords=""><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,478.15,420.98,61.10,8.74;8,326.10,432.94,213.15,8.74;8,326.10,444.89,158.76,8.74">Proceedings of the international conference on recent advances in natural language processing(RANLP)</title>
		<meeting>the international conference on recent advances in natural language processing(RANLP)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="103" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.10,476.77,213.15,8.74;8,326.10,488.73,213.15,8.74;8,326.10,500.68,167.39,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,466.97,476.77,72.29,8.74;8,326.10,488.73,192.72,8.74">Wikipedia-based query phrase expansion in patent class search</title>
		<author>
			<persName coords=""><forename type="first">Myaeng</forename><forename type="middle">S H</forename><surname>Al-Shboul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,526.24,488.73,13.01,8.74;8,326.10,500.68,83.54,8.74">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="430" to="451" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.10,520.61,213.15,8.74;8,326.10,532.56,213.14,8.74;8,326.10,544.52,196.27,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,417.86,532.56,121.39,8.74;8,326.10,544.52,37.33,8.74">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.10,564.45,213.15,8.74;8,326.10,576.40,213.14,8.74;8,326.10,588.36,213.15,8.74;8,326.10,600.31,166.56,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,447.18,576.40,92.07,8.74;8,326.10,588.36,213.15,8.74;8,326.10,600.31,34.62,8.74">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1310.4546</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.10,620.24,213.14,8.74;8,326.10,632.19,213.15,8.74;8,326.10,644.15,93.03,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,420.05,632.19,119.20,8.74;8,326.10,644.15,61.43,8.74">Draft trec 2015 total recall track overview</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
