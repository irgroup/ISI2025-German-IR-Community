<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.08,89.03,246.21,12.00">e-Discovery Team at TREC 2016 Total Recall Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.03,133.55,71.60,12.00"><forename type="first">Ralph</forename><forename type="middle">C</forename><surname>Losey</surname></persName>
							<email>ralph.losey@gmail.com</email>
						</author>
						<author>
							<persName coords="1,252.10,133.55,57.16,12.00"><forename type="first">Jim</forename><surname>Sullivan</surname></persName>
							<email>jsullivan@kollontrack.com</email>
						</author>
						<author>
							<persName coords="1,316.63,133.55,97.25,12.00"><forename type="first">Tony</forename><surname>Reichenberger</surname></persName>
							<email>treichenberger@krollontrack.com</email>
						</author>
						<author>
							<persName coords="1,421.91,133.55,51.69,12.00"><forename type="first">Levi</forename><surname>Kuehn</surname></persName>
							<email>lkuehn@krollontrack.com</email>
						</author>
						<author>
							<persName coords="1,482.09,133.55,50.35,12.00"><forename type="first">Jani</forename><surname>Grant</surname></persName>
							<email>jani.grantz@krollontrack.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">National e-Discovery Counsel Sr. Discovery Services Consultants</orgName>
								<orgName type="institution">Jackson Lewis P.C. Kroll Ontrack, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>410 Condominiums Tony 1, 317 1, 415 George W. Bush Jim 12, 267 11, 554 94.188% 92.358% 93.264% 416 Marketing Jim 1, 485 911 61.347% 43.967% 51.223% 417 Movie Gallery Ralph 5, Lost Foster Child Rilya Wilson Levi 1</addrLine>
									<postBox>.772% 47.351% 64.223% 411 Stand Your Ground Ralph 59 59 100.000% 67.045% 80.272% 412 2000 Recount Tony 850 747 87.882% 45.410% 59.880% 413 James V. Crosby Jim 600 581 96.833% 98.308% 97.565% 414 Medicaid Reform Tony 844 783 92.773% 35.917% 51.786%945 100.000% 100.000% 100.000% 418 War Preparations Tony 141 114 80.851% 77.551% 79.167% 419, .092% 15.022% 26.089% 420 Billboards Jim 739 707 95.670% 95.541% 95.605%</postBox>
									<postCode>314 99, 945 5, 982 1, 964 99</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.08,89.03,246.21,12.00">e-Discovery Team at TREC 2016 Total Recall Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">802D26EE9226D5D29EF37B6D1E0B8807</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 Information Search and Retrieval: Search process, relevance feedback, supervised learning, best practices, legal search Hybrid Multimodal</term>
					<term>AI-enhanced review</term>
					<term>predictive coding</term>
					<term>predictive coding 4.0</term>
					<term>electronic discovery</term>
					<term>e-discovery</term>
					<term>active machine learning</term>
					<term>continuous active learning</term>
					<term>Intelligent Spaced Training</term>
					<term>IST</term>
					<term>Computer-assisted review</term>
					<term>CAR</term>
					<term>Technology-assisted review</term>
					<term>TAR</term>
					<term>relevant irrelevant training ratios</term>
					<term>keyword search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The e-Discovery Team participated in the 2016 TREC Total Recall Track, Athome division, where thirty-four prejudged topics were considered using 290,099 emails of former Florida Governor Jeb Bush. The Team participated in TREC 2016 primarily to test the effectiveness of the standard search methodology it uses commercially to search for relevant evidence in legal proceedings: Predictive Coding 4.0 Hybrid Multimodal IST. The Team's method uses a hybrid approach to continuous active learning with both manual searches and active machine learning based document ranking searches. This is a systematic process involving implementation of a variety of search functions by skilled searchers. The Team calls this type of search multimodal because all types of search methods are used. A single expert reviewer was used in each topic along with Kroll Ontrack's search and review software, eDiscovery.com Review (EDR). The Team classified 9,863,366 documents as either relevant or irrelevant in all 34 review projects. A total of 34,723 documents were correctly classified as Relevant, as per the Team's judgment and corrected standard. The 34,723 relevant documents were found by manual review of 6,957 documents, taking a total of 234.25 man-hours. This represent an average project time of 6.89 hours per topic. The Team thus reviewed and classified documents at an average speed of 42,106 files per hour. The Team's attained an average 88% Recall score across all 34 topics using the corrected standard. The Team also attained F1 scores of greater than 90% in twelve topics, including two perfect scores of 100% F1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="46" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="47" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="48" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="49" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="50" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="51" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="52" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="53" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="54" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="55" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="56" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="57" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="58" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="59" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="60" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="61" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="62" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="63" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="64" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="65" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="66" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="67" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="68" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="69" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="70" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="71" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="72" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="73" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="74" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="75" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="76" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="77" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="78" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="79" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="80" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="81" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="82" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="83" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="84" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="85" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="86" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="87" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="88" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="89" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="90" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="91" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="92" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="93" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="94" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="95" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="96" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="97" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="98" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="99" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="100" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="101" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="102" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="103" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="104" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="105" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="106" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="107" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="108" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="109" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="110" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="111" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="112" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="113" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="114" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="115" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="116" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="117" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="118" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="119" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="120" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="121" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="122" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="123" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="124" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="125" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="126" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="127" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="128" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="129" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="130" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="131" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="132" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="133" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="134" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="135" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="136" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="137" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="138" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="139" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="140" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="141" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="142" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="143" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="144" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="145" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="146" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="147" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="148" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="149" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="150" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="151" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="152" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="153" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="154" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="155" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="156" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="157" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="158" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="159" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="160" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="161" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="162" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="163" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="164" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.0">INTRODUCTION.</head><p>The Total Recall track offered multiple pre-judged topics for search in two different divisions, Athome and Sandbox. The Sandbox participants were only permitted to use fully automated systems and the data remained on TREC administrator computers. They searched the same Jeb Bush dataset as Athome, plus another dataset not included in the Athome division due to confidentiality restrictions. The Sandbox participants were prohibited from any manual review of documents or ad hoc search adjustments. 1 Even after the submissions ended, the Sandbox participants never look at any documents, even the unrestricted Athome Jeb Bush data.</p><p>In the Athome experiments the data was loaded onto the participants' own computers and there were no restrictions on the types of searches that could be performed. In the Sandbox division data could not be loaded onto the participants' own computers and only fully automated searches were permitted.</p><p>The Team only participated in the Athome experiment, which had thirty-four prejudged topics. This was the only division where the e-Discovery Team could use its standard Predictive Coding 4.0 Hybrid Multimodal IST method, which employs both manual review and machine learning.</p><p>The At Home and Sandbox participants both used a computer "jig" (TREC's quaint term) set up by TREC whereby instant feedback was provided to a participant as whether each document submitted as relevant was in fact previously judged to have been relevant by TREC assessors. When a participant determined that a reasonable effort had been made to find all relevant documents required, which is important in legal search and represents a stopping point for further machine training and document review, they would notify TREC of this supposition and "Call Reasonable." Continued submissions were made after that point so that all documents were classified as either relevant or irrelevant. The goal was to submit as many relevant documents as possible before the Reasonable call, and thereafter to have all false negatives appear in submissions as soon after the Reasonable Call as possible.</p><p>The Athome group searched the dataset of 290,099 emails of former Florida Governor Jeb Bush. In the version of the Jeb Bush emails used by TREC almost all metadata of these emails has been removed. Moreover, the associated attachments and images were not present. Other collections of the Jeb Bush email exist from PST files that include more information, but the Team did not utilize this information and limited its efforts and attention to the official TREC collection. The Team normally searches datasets with full metadata included, and all attachments and images. Their searches normally include metadata fields and family associations (relationships between emails and attachments). These omissions in the Jeb Bush dataset increased the difficulty of the Team's search, which normally includes a mixture of metadata specific searches.</p><p>A significant percentage of the Bush emails were form type lobbying emails from constituents, which repeated the same language with little of no variance. The unusually high prevalence of near-duplicate emails made search of many of the Bush topics easier than is typical in legal search.</p><p>This same Jeb Bush email collection was used by the Total Recall Track in 2015 for ten topics in which the Team also participated. In 2015 Losey searched all ten of these ten topics. None of these search topics was repeated in 2016. For this and other reasons, namely that Losey is a life-long resident of Florida, very familiar with Jeb Bush and his governance of the state, he was very familiar with this dataset in 2016 and with most of the topics presented. 1.1 Summary of Team's Efforts.</p><p>The e-Discovery Team's 2016 Total Recall Track Athome project started June 3, 2016, and concluded on August 31, 2016. Using a single expert reviewer in each topic the Team classified 9,863,366 documents in thirty-four review projects.</p><p>The topics searched in 2016 and their issue names are shown in the chart below. Also included are the first names of the e-Discovery Team member who did the review for that topic, the total time spent by that reviewer and the number of documents manually reviewed to find all of the relevant documents in that topic. The total time of all reviewers on all projects was 234.25 hours. All relevant documents, totaling 34,723 by Team count, were found by manual review of 6,957 documents. The thirteen topics in red were considered mandatory by TREC and the remaining twenty-one were optional. The e-Discovery Team did all topics.</p><p>They were all one-person, solo efforts, although there was coordination and communications between Team members on the Subject Matter Expert (SME) type issues encountered. This pertained to questions of true relevance and errors found in the gold standard for many of these topics. A detailed description of the search for each topic is contained in the Appendix.</p><p>In each topic the assigned Team attorney personally read and evaluated for true relevance every email that TREC returned as a relevant document, and every email that TREC unexpectedly returned as Irrelevant. Some of these were read and studied multiple times before we made our final calls on true relevance, determinations that took into consideration and gave some deference to the TREC assessor adjudications, but were not bound by them. Many other emails that the Team members considered irrelevant, and TREC agreed, were also personally reviewed as part of their search efforts. As mentioned, there was sometimes consultations and discussion between Team members as to the unexpected TREC opinions on relevance.</p><p>This contrasts sharply with participants in the Sandbox division. They never make any effort to determine where their software made errors in predicting relevance, or for any other reasons. They accept as a matter of faith the correctness of all TREC's prior assessment of relevance. To these participants, who were all academic institutions, the ground truth itself as to relevance or not, was of no relevance. Apparently, that did not matter to their research.</p><p>All thirty-four topics presented search challenges to the Team that were easier, some far easier, than the Team typically face as attorneys leading legal document review projects. (If the Bush email had not been altered by omission of metadata, the searches would have been even easier.) The details of the searches performed in each of the thirty-four topics are included in the Appendix. The search challenges presented by these topics were roughly equivalent to the most simplistic challenges that the e-Discovery Team might face in projects involving relatively simple legal disputes. A few of the search topics in 2016 included quasi legal issues, more than were found in the 2015 Total Recall Track. This is a revision that the Team requested and appreciated because it allowed some, albeit very limited testing of legal judgment and analysis in determination of true relevance in these topics. In legal search relevancy, legal analysis skills are obviously very important. In most of the 2016 Total Recall topics, however, no special legal training or analysis was required for a determination of true relevance.</p><p>At Home participants were asked to track and report their manual efforts. The e-Discovery Team did this by recording the number of documents that were human reviewed and classified prior to submission. More were reviewed after submission as part of the Team's TREC relevance checking. Virtually all documents human reviewed were also classified, although all documents classified were not used for active training of the software classifier. The Team also tracked effort by number of attorney hours worked as is traditional in legal services. Although the amount of time varied somewhat by topic, the average time spent per topic was only 6.89 hours. The average review and classification speed for each project was 42,106 files per hour (9,863,366/234.25). 1.2 e-Discovery Team Members.</p><p>The Team is composed of five legal search experts Ralph Losey, Jim Sullivan, Tony Reichenberger, Levi Kuehn, Jani Grantz --and one "robot," Mr. EDR (the software they used). The Team members are not scientists or in academia. Most are lawyers who spend their working hours looking for evidence in large, chaotic datasets, such as email. They typically assist other attorneys in lawsuits and legal investigations. Their work includes the identification, review, analysis, classification, production, and admission of Electronically Stored Information (ESI) as evidence in courts in the United States and elsewhere.</p><p>The Team leader and report author is Ralph C. Losey, J.D., a full-time practicing attorney, principal and National e-Discovery Counsel of Jackson Lewis P.C., a U.S. law firm with over 800 attorneys and fifty-five offices. He has over 37 years of experience doing legal document reviews. Losey is also a blogger at e-DiscoveryTeam.com where he has written over two million words on e-discovery, including six books and over sixty articles on document review. <ref type="bibr" coords="5,488.20,190.55,4.06,8.00">2</ref> The past six years Losey has participated in multiple public and private experiments, some competitive, to test and prove various predictive coding methods.</p><p>Jim Sullivan, J.D., Tony Reichenberger, J.D.,and Jani Grantz J.D., are attorney search and review specialists who work for Kroll Ontrack, Inc. (KO). Levi Kuehn is a non-attorney search and review specialists who works for KO. Kroll Ontrack is the primary e-discovery vendor used by Losey and his law firm. It is a global e-Discovery software, processing and project management company (eDiscovery.com). The Team robot, Mr. EDR, is the Team's personalization of KO's  software, eDiscovery.com Review (EDR). Losey, Sullivan and Reichenberger participated in the 2015 TREC Total Recall Track. So too did a prior version of Mr. EDR, which is in a process of constant enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0">E-DISCOVERY TEAM'S SEARCH METHOD.</head><p>The e-Discovery Team uses what they call a Predictive Coding 4.0 Hybrid Multimodal IST method for search and review of large document collections. <ref type="bibr" coords="5,368.90,395.87,4.06,8.00">3</ref> This method is a type of continuous active learning text retrieval system that employs supervised machine learning and a variety of manual search methods. <ref type="bibr" coords="5,249.60,425.12,4.06,8.00">4</ref> The various types of searches included in the Team's multimodal approach are shown in the search pyramid, below.</p><p>Linear review refers to an SME's examination of all documents by certain key witnesses in a lawsuit during certain time frames critical to the disputed facts in a lawsuit. Keyword search in our methodology refers to the use of terms originating from legal and document analysis, and from witness interviews. Judgmental sampling and verification by SMEs are also used to test the terms before they are used throughout a document collection. Our keyword search also includes a variety of Boolean functions and parametric targeting, wherein searches are limited to certain metadata fields of an electronic document. Similarity and concept searches refer to a variety of passive machine learning analytic search techniques. The AI search at the top of the pyramid refers to the use of active machine learning. The EDR KO software uses a proprietary type of logistic regression algorithm.</p><p>The standard eight-step workflow normally used by the Team in legal search projects is shown in the diagram below. To meet the Team's self-imposed time requirements of completing every review project with minimal time efforts, the standard steps Three and Seven were omitted as will be further explained. Further, due to the set-up of the TREC experiments, the first step of our workflow, ESI Communications, was severely constrained to the point of being practically meaningless, as will also be further explained. The Team's standard workflow was thus reduced from eight to five steps as shown below. Also, the amount of time the Team normally spends on each step was also limited.</p><p>In the first step of ESI Communications Team members on a legal review project typically spend hours in discussion and analysis of scope of relevance and the target documents. The communications often include hundreds of written exchanges, both informal, such as emails and chats, and formal, such as (1) detailed requests for information contained in court documents such a subpoenas or Request For Production; (2) input from a qualified SME, who is typically a legal expert with deep knowledge of the factual issues in the case, and thus deep knowledge of what the presiding judge in the legal proceeding will hold to be relevant and discoverable; and, (3) dialogues with the party requesting the production of documents to clarify the search target, and other parties. The ESI communications may lead to formal motions with the governing court, legal memorandums, hearings before the presiding judge and opinions rendered by one or more judges on the scope of relevance.</p><p>The only ESI communications in the TREC experimental set-up was a very short, one sentence description of relevance for each topic. Two topics had a two-sentence description (410-Condominiums and 423-National Rifle Association). The only other type of ESI communications in this TREC Track were the automated, instant returns of all documents submitted as to whether TREC considered them to be relevant or not. There were no appeals or other procedures set-up for Athome division participants who actually examined the documents for true relevance to challenge obvious errors in judgment. The Sandbox division participants who search the same topics and dataset never actually look at any documents or make any relevance decisions; it is a fully automated process for them. They only train based on the automatic feedback from TREC's assessor judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0">RELATED WORK</head><p>It is generally accepted in the legal search community that the use of predictive coding type search algorithms can improve the search and review of documents in legal proceedings. <ref type="bibr" coords="7,504.95,293.35,4.06,8.00">5</ref> The use of predictive coding has also been approved, and even encouraged by various courts around the world, including numerous courts in the U.S. <ref type="bibr" coords="7,345.65,322.60,4.06,8.00">6</ref> Although there is agreement on use of predictive coding, there is controversy and disagreement as to the most effective methods of use. <ref type="bibr" coords="7,338.63,351.85,4.06,8.00">7</ref> There are proponents for a variety of different methods to find training documents for predictive coding. Some advocate for the use of chance selection alone, others for the use of top ranked documents alone, others for a combination of top ranked and mid-level ranked documents where classification is unsure. 8 The e-Discovery Team uses a method that includes a combination of all three of these selection processes and more.</p><p>Some attorneys and predictive coding software vendors advocate for the use of predictive coding search methods alone, and forego other search methods when they do so, such as keyword search, concept searches, similarity searches and linear review. The e-Discovery Team members reject that approach and instead advocate for a hybrid multimodal approach they call Predictive Coding 4.0. 9 This method uses an approach to active machine learning that the Team calls IST, standing for "Intelligently Spaced Training." Under IST the attorney in charge decides exactly when to train. This is different from other systems where the machine retrains after each document is coded, or certain predetermined number, and the human trainer has no discretion as to timing. <ref type="bibr" coords="7,183.33,556.92,8.06,8.00">10</ref> The e-Discovery Team approach includes all types of search methods (thus the term multimodal) to find relevant documents, with primary reliance placed on predictive coding. The Team also uses a variety of methods to find suitable training documents for predictive coding, including high ranking documents, and all other search methods. This is a fundamental difference with other methods that rely entirely on predictive coding to find relevant documents, and rely entirely upon high-ranking documents for training. Grossman and Cormack have scientifically tested these high-ranking training methods, and measured their effectiveness, but this does not mean that they endorse them as an exclusive tool, nor claim this to be their own preferred method. 11 4.0 E-Discovery Team's Four Research Questions and Short Answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Primary Question (repeat from 2015).</head><p>What Recall, Precision and Effort levels will the e-Discovery Team attain in TREC test conditions over all thirty-four topics using the Team's Predictive Coding 4.0 Hybrid Multimodal IST search methods and Kroll Ontrack's software, eDiscovery.com Review (EDR).</p><p>Short Answer: Again, as in the 2015 Total Recall Track, the Team attained very good results with high levels of Recall and Precision in almost all topics, including perfect or near perfect results in several topics using the corrected gold standard, and very little human effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Second Question.</head><p>What is the impact of incorrect Subject Matter Expert ("SME") judgments by the TREC assessors on Recall and Precision. (Unplanned question that unfortunately arose out of the circumstances encountered.)</p><p>Short Answer: This had a substantial impact on many topics where there were many errors in the standard, and only minor impact on topics where the disagreements were small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Third Question.</head><p>What is the most effective search method from the Team's multimodal tool-set for retrieval of relevant documents in the relatively simplistic search challenges presented by most, but not all, of the thirty-four topics. (Unplanned question that arose out of the circumstances encountered.) Short Answer: For the easy topics what the Team calls "tested, parametric, Boolean keyword search" was the most effective search method to find relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Fourth Question.</head><p>What is the role of active machine learning in retrieval of relevant documents in the simplistic search challenges presented by many of the thirty-four topics. Short Answer: The Team found that for the easiest topics in the 2016 Total Recall Track the primary role of active machine learning was reduced to a quality assurance function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.0">EXPERIMENTS AND DISCUSSIONS</head><p>The e-Discovery Team sought to answer the four previously listed Research Questions in its experiments at the 2016 TREC Total Recall Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">First and Primary Research Question.</head><p>What Recall, Precision and Effort levels will the e-Discovery Team attain in TREC test conditions over all thirty-four topics using the Team's Predictive Coding 4.0 hybrid multimodal search methods and Kroll Ontrack's software, eDiscovery.com Review (EDR).</p><p>Again, as in the 2015 Total Recall Track, the Team attained very good results with high levels of Recall and Precision in all topics, including perfect or near perfect results in several topics using the corrected gold standard. The Team did so even though it only used five of the eight steps in its usual methodology, intentionally severely constrained the amount of human effort expended on each topic and worked on a dataset stripped of metadata. The Team's enthusiasm for the record setting results, which were significantly better than its 2015 effort, is tempered by the fact that the search challenges presented in most of the topics in 2016 were not difficult and the TREC relevance judgments had to be corrected in most topics.</p><p>Even using the given uncorrected TREC standard for scoring, and even though in most topics we did not train on the TREC returned-relevant documents that the Team considered irrelevant, the Team overall still attained excellent results. Under the corrected standard, the results were much better. The following chart compares the Team's Recall, Precision and F-Measure for each Athome topic with the results obtained by TREC's BMI and BMI-Desc runs. These comparative statistics show the scores at the time of reasonable call. This first chart uses the uncorrected defective standard and is thus of limited value in the topics that had many mistakes. In the precision category, which in Legal Search is the money shot that has the greatest impact on the cost of a document review project, the e-Discovery Team dominated, even using the uncorrected TREC standard. It had the highest precision level on 28 of the 34 topics (82%). They are highlighted in blue in the above chart. The e-Discovery Team's average precision score was 57.1%. The average precision of both BMI and BMI-Desc was 24.8%. Thus the Team's precision score was on average more two and a quarter times higher than that of the BMI standards.</p><p>In the F1-measure, which is the standard value used in legal search to evaluate overall precision and recall of a project, the e-Discovery Team again dominated. This is somewhat surprising in view of the fact that these measurements were based on the uncorrected TREC standard. The Team had the highest F1 scores on 23 of the 34 topics (68%). They are highlighted in blue in the above chart. The e-Discovery Team's average F1 score was 57.69%. The average F1 of BMI and BMI-Desc was 36.5%. Thus the Team's F1 score was on average more than 58% higher than that of the BMI standards. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Precision Across Topics</head><p>Ediscovery Team BMI BMI-Desc</p><p>Even using TREC's often erroneous standards, the Team still attained higher recall than both the BMI and BMI-Desc standards on two topics: topic 415 George Bush with a score of 94.08%; and, topic 419 Lost Foster Child Rilya Wilson with a score of 98.84%. Moreover, the Team attained recall levels in excess of 90% at the time of reasonable call in the following additional topics:</p><p>• In summary, even with the uncorrected TREC standards, where in most topics the Team did not use all documents returned as relevant for all of its training documents, it attained Recall scores greater than 90% in fourteen of the thirty-four topics. The Team attained Recall scores of 80% or higher in four additional topics. The average results obtained across all thirty-four topics at the time of reasonable call were as follows:</p><p>• Average F-Measure Across Topics</p><p>Ediscovery Team BMI BMI-Desc</p><p>The Team, composed as it is of trained attorneys who engage in relevance analysis on a daily basis in the context of actual lawsuits, believes strongly in the idea of a ground truth of relevance, in other words, True Facts, not Alternate Facts. The Team's work depends on an objective, consistent assessment of true relevant documents. The boundaries of true relevance or irrelevance is a judgment call based on somewhat subjective factors, but once the border is established, it must be consistently followed in legal search. For that reason the measurements of the effectiveness of the Team performance based on a defective, inconsistent standard, is of little interest to the Team. We consider the only significant measurement of our results to arise out of use of the corrected gold standard. These are described next.</p><p>This next chart uses the corrected standard. It is the primary reference chart we use to measure our results. Unfortunately, it is not possible to make any comparisons with BMI standards because we do not know the order in which the BMI documents were submitted. The average results obtained across all thirty-four topics at the time of reasonable call using the corrected standard are shown below in bold. The average scores using the uncorrected standard are shown for comparison in parenthesizes.</p><p>• 88.17% Recall (75.46%) • 64.94% Precision (57.12%) • 69.15% F1 (57.69%)</p><p>• 124 Docs Reviewed Effort (124)   At the time of reasonable call the Team had recall scores greater than 90% in twenty-two of the thirty-four topics and greater than 80% in five more topics. Recall of greater than 95% was attained in fourteen topics. These Recall scores under the corrected standard are shown in the below chart. The results are far better than we anticipated, including six topics with total recall -100%, and two topics with both total recall and perfect precision, topic 417 Movie Gallery and topic 434 Bacardi Trademark. At the time of reasonable call the Team had precision scores greater than 90% in thirteen of the thirty-four topics and greater than 75% in three more topics. Precision of greater than 95% was attained in nine topics. These Precision scores under the corrected standard are shown in the below chart. Again, the results were, in our experience, incredibly good, including three topics with perfect precision at the time of the reasonable call.</p><p>At the time of reasonable call the Team had F1 scores greater than 90% in twelve of the thirtyfour topics and greater than 75% in two more. F1 of greater than 90% was attained in eight topics. These F1 scores under the corrected standard are shown in the below chart. Note there were two topics with a perfect score, Movie Gallery (100%) and Bacardi Trademark (100%) and three more that were near perfect: Felon Disenfranchisement (98.5%), James V. Crosby (97.57%), and Elian Gonzalez (97.1%). We were lucky to attain two perfect scores in 2016 (we attained one in 2015), in topic 417 Movie Gallery and topic 434 Bacardi Trademark. The perfect score of 100% F1 was obtained in topic 417 by locating all 5,945 documents relevant under the corrected standard after reviewing only 66 documents. This topic was filled with form letters and was a fairly simple search.</p><p>The perfect score of 100% F1 was obtained in topic 434 Bacardi Trademark by locating all 38 documents relevant under the corrected standard after reviewing only 83 documents. This topic had some legal issues involved that required analysis, but the reviewing attorney, Ralph Losey, is an SME in trademark law so this did not pose any problems. The issues were easy and not critical to understand relevance. This was a simple search involving distinct language and players. All but one of the 38 relevant documents were found by tested, refined keyword search. One additional relevant document was found by a similarity search. Predictive coding searches were run after the keywords searches and nothing new was uncovered. Here machine learning merely performed a quality assurance role to verify that all relevant documents had indeed been found.</p><p>The Team proved once again, as it did in 2015, that perfect recall and perfect precision is possible, albeit rare, using the Team's methods and fairly simple search projects.</p><p>The Team's top ten projects attained remarkably high scores with an average Recall of 95.66%, average Precision of 97.28% and average F-Measure: 96.42%. The top ten are shown in the chart below. In addition to Recall, Precision and F1, the Team per TREC requirements also measured the effort involved in each topic search. We measured effort by the number of documents that were actually human-reviewed prior to submission and coded relevant or irrelevant. We also measured effort by the total human time expended for each topic. Overall, the Team humanreviewed only 6,957 documents to find all the 34,723 relevant documents within the overall corpus of 9,863,366 documents. The total time spent by the Team to review the 6,957 documents, and do all the search and analysis and other work using our Hybrid Multimodal Predictive Coding 4.0 method, was 234.25 hours.</p><p>It is typical in legal search to try to measure the efficiency of a document review by the number of documents classified by an attorney in an hour. For instance, a typical contract review attorney can read and classify an average of 50 documents per hour. The Team classified 9,863,366 documents by review of 6,957 documents taking a total time of 234.25 hours. The Team's overall review rate for the entire corpus was thus 42,106 files per hour (9,863,366/234.25).</p><p>In legal search it is also typical, indeed mandatory, to measure the costs of review and bill clients accordingly. If we here assume a high attorney hourly rate of $500 per hour, then the total cost of the review of all 34 Topics would be $117,125. That is a cost of just over $0.01 per document. In a traditional legal review, where a lawyer reviews one document at a time, the cost would be far higher. Even if you assume a low attorney rate of $50 per hour, and review speed of 50 files per hour, the total cost to review every document for every issue would be $9,863,366. That is a cost of $1.00 per document, which is actually low by legal search standards. <ref type="bibr" coords="17,123.05,205.33,8.06,8.00">13</ref> Analysis of project duration is also very important in legal search. Instead of the 234.25 hours expended by our Team using Predictive Coding 4.0, traditional linear review would have taken 197,267 hours (9,863,366/50). In other words, the review of thirty-four projects, which we did in our part-time after work in one Summer, would have taken a team of two lawyers using traditional methods, 8 hours a day, every day, over 33 years! These kinds of comparisons are common in Legal Search.</p><p>Detailed descriptions of the searches run in all thirty-four topics are included in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Research Question No. 2.</head><p>What is the impact of multiple errors in SME judgments by the TREC assessors on Recall and Precision.</p><p>The impact of assessor errors on Recall and Precision was significant, depending in part upon the number of errors made by TREC assessors in a particular topic. The importance of the computer maxim, "Garbage In, Garbage Out -GIGO," was shown to have direct application to machine learning and text retrieval. The impact seen here is, however, exaggerated by the presence of numerous near duplicate form emails in the Bush collection. More research on this question is needed to try to quantify the impact of SME errors using Predictive Coding 4.0 Hybrid Multimodal IST methods.</p><p>After the Team encountered numerous errors on the first topics undertaken, we were forced to create our own gold standard of true relevant documents for each topic. The Team's new gold standard corrected for the obvious errors seen in TREC's assessments of relevance. In all close questions on relevance the judgment of TREC's assessors was accepted as accurate.</p><p>The errors and inconsistencies seen by the Team's close study of the documents were not accepted. In most, but not all topics, the Team did not use the documents with obvious errors for its machine training. In all topics the Team created its own standard and made comparative recall, precision and F1 calculations based thereon. The observation and correction of TREC errors in gold standard became a collaborative effort among the Team to peer review and verify our corrected standard. Most of these efforts, many of which occurred after the conclusion of the Track in August, were not included in the time reports of efforts expended by attorneys in the search.</p><p>The Team was very reluctant to take this step. It meant a lot more work and make everything much more complicated. We would certainly have let pass a few errors or mere differences of opinion. We recognize that no standard is ever perfect. As lawyers the Team understands all too well that some, perhaps many judgments on relevance are subjective.</p><p>Again, in all close questions on relevance the judgments of TREC's assessors were accepted, even though we personally disagreed.</p><p>The Team means no disrespect by the creation of an alternate gold standard. We appreciate and respect the efforts made by the TREC assessors and organizers. Still, the volume of obvious errors encountered forced us to take this action. The integrity of our primary research question to test the effectiveness of our hands-on type multimodal hybrid methods demanded that we do so. We understand that the impact on other Total Recall Participants, ones that never actually examine documents, may be far less, perhaps even negligible. Still, there could be an impact, even for them, in some topics where more than an insignificant number of the same or similar documents were inconsistently judged.</p><p>The decision to not accept the errors seen, and to instead create our own gold standard, resulted in substantial additional work for the Team. In some topics, described in the Appendix, we even took the step of making two "reasonable calls." One was for TREC, and the second call, which always took place on the next submission, was for our own internal tracking. In the second call we would include emails that we knew from prior submissions of the same or similar document would again be incorrectly considered irrelevant by TREC. We knew they were true relevant and so waited until after our public reasonable call to TREC to submit them and then we make our own internal reasonable call. We were attempting to, in effect, play two games at once, and maximize our score in each game. Keeping track of two standards added an unexpected layer of difficulty to our work and we did not bother to do so in most topics.</p><p>In some topics the difference between the two standards was substantial. In a few topics only minor differences were seen. Disagreements on relevance are not unexpected in any standard involving at least somewhat subjective mass relevance adjudications. We do not intend to engage in a criticism of the specific gold standard creation methods used in 2016 Total Recall Track, except to note that the appeals procedure included in the 2008 and 2009 TREC Legal Tracks could have improved the accuracy of the results for the Total Recall Track Athome participants. 12 Further, the Team understands that the TREC assessors work was much more time constrained than was the work of the Team. Moreover, unlike the Team, the TREC assessors did not have the benefit of SME input from a native Floridian lawyer (Losey) who was familiar with Florida politics and Governor Bush and, since 2015, had put substantial time reviewing this email collection.</p><p>The following chart contains a detailed comparison of recall, precision and F1 the Team attained based under both the TREC and Team assessments. Again, the Appendix search descriptions include a few examples of the kind of classification errors encountered. Again, the Team recognizes that no gold standard is ever perfect, including its own revised standards. The Team invites input from other participants and organizers of the Total Recall Track concerning relevance of any document. Upon request and agreement we will provide any participant or organizer with a confidential spreadsheet listing the Team's gold standard for each topic by identification of TREC ID Document Numbers. We invite any challenges and questions concerning relevance. The Team continues to believe in meaningfulness of relevance, true facts and the importance of a correct gold standard to any text retrieval experiment.</p><p>The topics we found that had the largest assessor errors, and thus the largest changes in Recall measure at the time of reasonable call, are:</p><p>• Topic 401 Summer Olympics: 41.05% to 91.97%. • Topic 434 Bacardi Trademark: 86.84% to 100%. The standards with the highest changes in recall measure are shown below with the percent of recall change for each and the percent of error in recall measurement. The large error rate seen in Topic 403 is an anomaly explained by the presence of one contested form email (Protect Florida's Springs) that had 913 near duplicates. <ref type="bibr" coords="20,314.63,132.80,8.06,8.00">14</ref> The error rates in other topics were also magnified to varying degrees for the same reason, the high prevalence of forms emails in the Jeb Bush collection.</p><p>• o Error of 33%. This data shows the importance of correctly judged gold standards and the impact of erroneous, inconsistent SME judgments upon the effectiveness of any search. The impact of the SME type errors seen here is exaggerated by the fact that the Bush collection contains an unusually high number of form emails. Further work on this research question is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3</head><p>Research Question No. 3. What is the most effective search method from the Team's multimodal tool-set for retrieval of relevant documents in the relatively simplistic search challenges presented by most, but not all, of the thirty-four topics.</p><p>For most of the topics in 2016 the Team's use of what it calls "tested, parametric, Boolean keyword search" was the most effective search method to find relevant documents. <ref type="bibr" coords="20,481.45,620.70,8.06,8.00">15</ref> The Team was surprised by how well a sophisticated use of keywords could locate nearly all the target relevant documents in many of the topics. This shows the continued importance of a multimodal approach to legal search, including especially keyword search, when done properly, 16 especially in simple lawsuits involving relatively easy search issues.</p><p>In post hoc research the Team ran keyword only searches across all topics. We did so to calculate the scores that the Team would have accrued in each topic, if the Team had only run keyword searches, and had not supplemented these searches with other types of search, including similarity, concept and predictive coding based searches. Below is a chart showing a comparison of the BMI (pure machine learning) results to the Keyword-only results. The uncorrected standard is here used because comparisons are not possible under the corrected standard. Comparisons under the corrected standard are not possible because no information has been provided by TREC as to the order of BMI document submissions. Without that information the BMI results under the corrected standard cannot be calculated. Since uncorrected data is used for the standard, the specific measurements here are not perfect, although we think these comparisons still provide useful information. As shown in the above chart, machine learning provided a substantially better recall almost across the board in comparison to keyword alone (it had a smaller recall on only one of the thirty-four topics). However, machine learning alone improved on precision on only ten of the topics versus Keyword, and improved on F-measure on only 11. This would be indicative of a typically broad classifier, in need of narrowing its scope. It suggests that keywords can play a beneficial role in the initial searches (Step Two in the Team's eight-step process, Multimodal ECA).</p><p>Keyword search is shown to have its own drawbacks. They were often far too narrow and could be adversely impacted by context of the terms. To that end, machine learning exceeds and excels at expanding the scope of documents to consider and returning only those sets that are pertain to the issue at hand.</p><p>Going beyond the post hoc experiment results, and based on our general experience, we see a contrast between a pure machine learning approach, and a hybrid multi-modal approach, that is described by Team member Tony Reichenberger as follows:</p><p>A machine learning process takes the whole document set and seeks to narrow it down to find documents of relevance. A hybrid multi-modal approach starts by narrowly focusing on relevant documents to fuel machine learning, and then expands the set of documents to consider for relevance based on machine feedback. 5.4 Research Question No. 4.</p><p>The Team found that for the seven easiest topics in the 2016 Total Recall Track the primary role of active machine learning was reduced to a quality assurance function:</p><p>Topic 422 Non-Resident Aliens Topic 413 James V. Crosby Topic 417 Movie Gallery Topic 434 Bacardi Trademark Topic 426 Jeffrey Goldhagen Topic 405 Newt Gingrich Topic 411 Stand Your Ground Predictive coding based searches of high ranking documents would in some of these topics uncover a few relevant documents not already located by keyword search, or concept and similarity search, and thus improve recall somewhat. In some active machine learning searches we did not find any new relevant documents. Instead the predictive coding searches only confirmed that all relevant documents had already been found by the other methods. Again, the description of those searches in the Appendix provides further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>The Team has shown that it's standard method of document review, Predictive Coding 4.0 Hybrid Multimodal using continuous Intelligently Spaced Training, is extremely effective by all objective measures, including Recall, Precision, F1, project speed and effort. The Team method of finding relevant emails took an average of only 6.89 hours per project by review of an average of 124 documents reviewed per topic.</p><p>The Team classified 9,863,366 documents as either relevant or irrelevant in thirty-four review projects. A total of 34,723 were correctly classified as Relevant, as per the Team's judgment and corrected standard. The 34,723 relevant documents were found by manual review of 6,957 documents, taking a total of 234.25 man-hours. The Team thus reviewed and classified documents at an average speed of 42,106 files per hour.</p><p>Even at these speeds and reviewer time limitations, and even with the handicap of having to omit three of the Team standard eight-step protocol (1-ESI Communications, 3-Random Prevalence, 7-ZEN QC), the Team's average score across all thirty-four topics was: 88.17% Recall, 64.94% Precision and 69.15% F1. The Team's top ten projects attained remarkably high scores with an average of 95.66% Recall, 97.28% Precision and 96.42% F1. The Team attained an average 88% Recall score across all 34 topics using the corrected standard. The Team also attained F1 scores of greater than 90% in twelve topics, including two perfect scores of 100% F1. The Team cautions that these high scores in a short amount of time and other handicaps were only possible because of the ease of the searches and simplicity of the Bush email.</p><p>The Team found that the proper use of multimodal search, including especially keyword search, can, in the right case, with the right data, easy targets, and a skilled searcher and SME, be very effective, even without the use of active machine learning. For easy search challenges, such as those presented in the 2016 Total Recall Track topics, the primary role of active machine learning is reduced to a quality assurance function. Predictive coding can be used to verify that the other multimodal search methods have already found all relevant documents.</p><p>The success of the other methods alone, without predictive coding, was not expected. The Team knew from its experience in Legal Search that keyword search alone, even when done properly and even when supplemented by various passive analytic based searches, does not usually work well to attain high recall in search projects with complex relevance issues or with complex "dirty" data. These are the kind of searches that the Team typically works with every day in Legal Search. For complex projects active machine learning is required. In the more complex and difficult projects, using keyword search alone would be a significant danger. It can be very imprecise and can easily miss unexpected word usage and misspellings. That is one reason the e-Discovery Team always supplements keyword search with a variety of other search methods, including predictive coding. Still, our research in 2016 TREC has shown that tested, parametric Boolean keyword search alone can attain good recall and precision when there is simple data, clear targets and a skilled reviewer.</p><p>Finally, we found that a high number of errors made in relevance judgments by reviewers and SMEs, regardless of whether due to human carelessness or lack of expertise, can have a significant impact on the metrics evaluating the efficiency and effectiveness of a project. We do not have enough information yet to quantify this impact. Still, the data at hand confirms the commonsense GIGO notion that the impact of training errors can be significant and that the degree of impact varies according to the type and number of assessor errors. Much more research is needed in this area.</p><p>The assessor errors may have little or no impact on the metrics of the automatic Sandbox division participants in the Recall Track, where they anyway never look at documents, and are not concerned with true relevance, just with matching the TREC standard. Still, errors in TREC gold standard may also impact participants in the Sandbox division in some topics. Without a reliable standard, one that mirrors true relevance, and is so certified by diligent skilled humans, the auto-search exercises appear to be equivalent to a snake eating its own tail, an Ouroboros. <ref type="bibr" coords="23,127.80,688.98,8.06,8.00">17</ref> Without a proper gold standard, the auto runs in the impacted topics may only measure the ability of one software program to follow and match another. It is like a deluded, self-serving snake eating its own tail. This is a kind of blind leading the blind negative feedback loop. It does not measure the ability of the software to attain true recall of the target documents. It just measures the ability of one program to follow another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>The e-Discovery Team would like to thank Kroll Ontrack, Inc. and Jackson Lewis P.C. for their generous support of this project. We would also like to thank the employees at Kroll Ontrack who pitched in behind the scenes and on weekends to help make this happen. Losey also thanks his wife, Molly, for once again sacrificing a summer vacation so he would have time to participate in this project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">REFERENCES (Endnotes) [1]</head><p>The Losey, R., Predictive Coding 4.0 (e-Discovery Team, 2016) found at https://ediscoveryteam.com/doc-review/predictive-coding-4-0/. [4] The e-Discovery Team's hybrid multimodal approach relies upon and encourages participation of skilled reviewers in the search process, the hybrid approach. Our aim is augmentation of skilled attorneys to perform legal search, not automation, not replacement. In these respects the e-Discovery Team follows the teachings of Gary Marchionini, Dean of the School of Information and Library Sciences of U.N.C. at Chapel Hill, who explained in Information Seeking in Electronic Environments (Cambridge 1995) that information seeking expertise is a critical skill for successful search. Professor Marchionini argues, and we agree, that: "One goal of human-computer interaction research is to apply computing power to amplify and augment these human abilities."</p><p>We also follow the teachings of UCLA Professor Marcia J. Bates who has advocated for a multimodal approach to search since 1989. Bates, Marcia J. [10] The merits of the Team's approach to the timing of machine learning are detailed in We disagreed with 1,038 TREC relevance classifications on this topic. We found that 1,001 documents they coded as relevant, were actually irrelevant under that definition, and 37 documents they coded as irrelevant, were actually relevant. The total count of relevant documents according to TREC was 1,089. In fact the Team found only 125 relevant documents. We found 121 of those relevant documents before reasonable was called. Four more documents were found after the call. The TREC SME assessors made only a few errors, but the errors were magnified because they were in near duplicate form emails. The primary error seen pertained to omission of the following relevance restriction statement on some, but not all, documents: "for bottling by commercial enterprises." TREC correctly judged some emails that concerned the extraction of water in Florida, but did not pertain to bottling, to be irrelevant. But TREC also TREC incorrectly judged some emails that concerned the extraction of water in Florida, but did not pertain to bottling, to be relevant. One such was the mentioned form email (Protect Florida's Springs), with over 913 near duplicates. This form only pertained to the use of water for commercial development, Florida springs and protection of Manatees (a perennial Florida favorite). The form email was unrelated to commercial bottling. There are only a couple of commercial bottlers in Florida and it is easy to identify them, if you know this. The TREC assessor sometimes ignored the commercial bottling qualifier, and sometimes did not. It was not a relevance decision. The single error on the Protect Florida's Springs form emails was magnified because of the number of copies (913) of this form. That explains the high error rate anomaly seen in topic 403, which was otherwise a very low prevalence topic with only 125 relevant documents. Without this one error the judging on the topic would not have been that bad. Since most datasets do not have so many form emails in them, this kind of multiplying error would not usually happen.</p><p>[15] For a detailed description see the section, Tested, Parametric Boolean Keyword Search in Predictive Coding 4.0 (e-Discovery Team, 2016).</p><p>[16] In the legal profession keyword searches are often performed by unskilled attorneys in a very unsophisticated "improper" manner. had no need of eyes because there was nothing outside of him to be seen; nor of ears because there was nothing to be heard; ... his own waste providing his own food, and all that he did or suffered taking place in and by himself. For the Creator conceived that a being which was self-sufficient would be far more excellent than one which lacked anything. Plato, Timaeus, found at http://classics.mit.edu/Plato/timaeus.html. This is a danger inherent in any fully automated document ranking system. Losey, Why the 'Google Car' Has No Place in Legal Search (e-Discovery Team, February 2016) (caution against over reliance on fully automated methods of active machine learning) found at: https://ediscoveryteam.com/2016/02/24/why-the-google-car-has-no-place-in-legal-search/. The summaries were prepared by the attorney who ran that topic.</p><p>At the beginning of each Topic the results are reported for that Topic. Each has the same form and discloses metrics at the times when: (1) the Reasonable call was made; and, (2) the point where 97.5% Recall was attained. They are summarized along with a variation of a standard Confusion Matrix, a/k/a Contingency Table . The Confusion Matrix itself is highlighted in blue. It is followed by a list of the key the values attained: Recall, Precision, F1 Measure, Accuracy, Error, Elusion and Fallout.</p><p>Due to the poor judging by TREC Assessors as to relevant documents in some topics, we were forced to try to note the documents incorrectly judged in all topics. We provide a very short discussion of the some of the errors. We also provide corrected statistics of these topics to show how our Team did when a correct standard was used. The true, corrected measures were dramatically different in some topics.</p><p>The actual review counts shown in these counts do not include documents reviewed after submission. Each document returned by TREC with an unexpected coding was examined to try to guess the scope of relevance used in a topic, or determine if the adjudication was in error, the later being an all too frequent experience for Team members. The error in calling two documents relevant, that are obviously irrelevant, suggests a failure of quality control and over-reliance on software. Since there were so few relevant documents -38 -it would only have taken a few minutes to review them all. Anyone would quickly see that three (two plus a duplicate) of the documents were erroneously identified by the software to be relevant. We understand the assessors used Sofia-ml software to find the relevant documents, or software close thereto, just like most of the auto-run participants. The TREC assessors also supposedly verified the software's predictions with quality control efforts. We assume this meant a human actually looking at the documents. Obviously this human review control check did not happen here for some reason or they would have seen that 119771, 005283 (duplicate of 119771), 147890 were not relevant.</p><p>The failure of the assessors and Sofia-ml software (this software was used in 2015 and we assume was used again in 2016) to find the three relevant documents missed (actually only two, plus a chain) is easier to understand. That is simply a failure of the search software and the human search expert, the TREC assessors, who directed the search (assuming that there was in fact human assessor involvement, and TREC did not simply rely on automated procedures). An error in finding relevant documents is a result of skill and software deficiencies, not carelessness. Still, the net result in a low prevalence project like this of six errors is very significant -16% (6/38).</p><p>It is important to note that these errors are not merely disagreements as to relevance. In other topics we did encounter close calls that we disagreed with, but we could see had a rational basis. They were not obvious mistakes. We did not adjust the standards for such opinion divergences. In other topics we encountered many documents where duplicates or near duplicates of the same document were coded inconsistently. There is no question that some of them were coded incorrectly.</p><p>The differences in judgment reported here are all obvious errors or errors of consistency. All close calls were granted to TREC, as is appropriate, but these obvious bloopers should not stand. The e-Discovery Team protested the many obvious errors it saw in the 2015 Total Recall Track, and made some public comments thereon in its reports. We participated again in 2016 based on assurances that the quality control and judgments would be improved. We are unhappy to report that although there has been some improvement, it appears to be very spotty. Errors in gold-standard judgments were again made in 2016 that have consequences on metrics, especially in the low prevalence topics that are common in the Total Recall Track. These errors have little or no impact on the metrics of the automatic group participants, where they anyway never look at documents, and are not concerned with true relevance, just with matching the TREC standard. Still, a flawed gold standard does impact the validity of comparisons between ad hoc participants, such as our Team, where human searchers actually look at and evaluate the relevance of documents, and the auto run participant results. Moreover, without a valid objective standard, one that corrects for computer errors, the auto-search exercise would just be like a dog chasing its own tail. All it measures is the ability of one software program to follow and match another. It does not measure the ability of the software to attain true recall of the target documents.</p><p>In Losey's view the Bacardi Trademark issue was a relatively simple search, as explained further below. After correcting for the six obvious errors described above, Losey actually scored a perfect run on this issue with 100% Recall and 100% Precision as shown below. Although it may seem fast to some readers to see a review of 290,099 documents completed by one attorney in only four hours, please note that this time did not include time spent prior to the search and outside of this topic. This includes time on such things as general set-up, procedures, project orientation, and communication protocols. The time reported also does not include the time note taking and report creations.</p><p>Aside from encountering several obvious errors in judging this topic, this was an interesting search project. The only information provided by TREC of Topic 434 was as follows:</p><p>Bacardi Trademark Lobbying -Documents related to the Jeb Bush administration's involvement in a trademark dispute between Bacardi and the U.S. Patent and Trademark Office.</p><p>Losey chose this topic as he assumed it would be an easy topic for him to start with. Losey is an attorney in Florida with 36 years of legal experience, including a background in trademark law and analysis. Also, he is a native and sixty-five year resident of Florida who remembers well the Jeb Bush years and is familiar with many of the characters and issues mentioned in the Jeb Bush email.</p><p>Based on the description of this issue Losey hoped that the search would require some legal analysis and background. As it turned out, only a limited amount of such legal analysis and knowledge of trademark law and procedures was required, but it did help, especially in his full understanding of the relevant documents. From his perspective, this was a relatively easy search, even without legal or local knowledge. He found it comparable to legal search project in a simple, one issue lawsuit that had an easily defined target.</p><p>Losey began the project with a 30 minute Google search. Actually, the search itself took 3 minutes. The remaining 27 minutes were spent studying a political newspaper article that Losey knew from experience would likely be authoritative and complete. This provided important background information and was the equivalent to the Step One in the Team's standard Hybrid Multimodal workflow.</p><p>Based on this one newspaper article Losey identified the key persons involved, the timeline, and the key words likely to appear in any relevant documents, Based on that he formulated multiple keyword searches. The next day, June 8, 2016, he began Step Two, Multimodal Search Reviews. Losey spent two hours using parametric Boolean keyword searches. The searches were refined and new terms added based upon the documents seen. In this step 2 multimodal search review Losey found 37 of the 38 relevant documents found. A similarity search found one additional document. A concept search led to nothing new.</p><p>To summarize, the initial keyword and similarity searches conducted in step 2 found all 38 of the relevant documents in this collection. Losey spent another 1.5 hours in the submission process running multiple active machine learning training sessions, which is steps 4, 5 and 6 in our standard workflow. These did not lead to the discovery of any new documents, but did serve as an expedited quality control measure to verify that the keyword and similarity searches had in fact uncovered all relevant documents. Steps 3 and 7 were skipped for three reasons: (1) to save time;</p><p>(2) because Losey did not consider these additional quality control-assurance steps to be necessary in this simple project; and, (3) the predictive coding document-ranking work, where high-ranking documents were reviewed by Losey and coded as irrelevant, served as an effective quality assurance measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Bacardi Trademark topic, the 90% recall threshold had been attained by submitting only 0.01%% of the corpus, 35 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). He manually categorized 319 documents and studied 261 documents during the course of the 8 hours he spent on this project. The review was very much an on and off again type of project extending over three weeks. This is a poor way to do document review, necessitated by time demands at work, and probably did impact the results.</p><p>The full description provided as a relevance guide for this topic is: Summer Olympics -All documents concerning a bid to host the Summer Olympic Games in Florida.</p><p>Losey found this topic very interesting. The 2016 Olympics were on television at the same time. And he was fascinated that Florida had even made the attempt of Florida to bid on the 2012 Olympics back in 2001 because he had never heard of that. This was an effort by Tampa that received very poor press and only lukewarm political support by Central Florida, where Losey lives. It was interesting to learn from the Bush emails that the main reason Tampa lost the bid, and was disqualified early on, was the threat of Hurricanes. This is turn was triggered by the fact that Hurricane Cassandra threatened when the site committee was visiting. The two finalists were San Francisco and NYC, and NYC was selected as the bid City for the US. Of course, it did not get the 2012 Summer Olympics either. London did.</p><p>Multimodal review was done as usual, primarily by keywords (i.e, -"Olympi*"), similarity and predictive coding. The keyword searches were very effective in this topic in part because the main organizer of the Olympic bid was a man named Turanchik, which is novel name in Florida. Also, many of the emails with the word Olympic were relevant, but far from all. Losey would usually focus on ranking searches seen in the keyword folders.</p><p>There were several twists and turns that make the relevance hunt somewhat challenging (not totally simplistic, like many of the other topics). Mr. EDR has a role to play here, although I think most of what Losey found could have been found via keyword, and the rest by brute force by well-trained reviewers. Still, the AI made it much more efficient and is served as a good QC pushing up the scores attained here.</p><p>By these method Losey found a total of 127 documents at the time of reasonable call. Losey had submitted 129 documents as probable relevant at that point. Two of these submissions were later seen to be irrelevant and thus mistakes on Losey's part. The reasonable call was made after the eighth submission. The reporting for some reason is in error on this topic as it only shows 126 relevant found by that time, not 127. There were nine more submissions were made after the reasonable call. In these post call submissions 10 documents were returned by TREC as relevant that were relevant, or at least arguably so, and were not previously found by my search. The record incorrectly says 11 were found post call. The actual recall here was 92.7%, not the 91.97 shown above, but this error was found too late to correct and is anyway very minor.</p><p>For an example of two documents that Losey first considered them to be irrelevant, but later changed his mind, consider the emails bearing our Control # 3006405 and 3006419.</p><p>Based upon TREC's classification of these documents as relevant, we determined that Losey had made a mistake to classify them a relevant. The emails do not mention the Olympics, but do mention the Florida organizer, Turanchik. Upon closer study it is apparent that the emails did pertain to the Summer Olympics site committee, and so these two emails should be relevant. TREC got those emails right, but the errors usually went the other way.</p><p>TREC made many errors on this topic. As an example, many emails directly relevant to the Florida Olympics bid had to do with building certain trains and roads. The construction was needed for Olympic hosting infrastructure. TREC would often classify as relevant other emails concerning road and train construction, even though they had nothing to do with the Olympics. A human would have understood the difference, but these emails were obviously never read by a human assessor, just predicted by TREC's AI. We would run into errors like this all of the time in some topics like this, such that we began to play a game to hold our interest to try to figure out why the TREC AI made classification mistakes. It is sort of like reverse engineering from the often errors seen. We found that many of the obvious bloopers TREC made concerned relevant information not present at the beginning of an email. Instead, the relevant sections were found in the middle or end of a document. TREC's classifier algorithm seems to be front-ended, plus we suspect the human quality control did not look past the first couple of sentences either.</p><p>Another TREC error seen many times is the classification of an email as relevant, just because it had the word Olympics (especially near the front of an email), even though the word did not refer to the topic of Summer Olympics as required. An example is a reference seen many times to the Special Olympics, an event that did take place in Florida, but at a different time and place.</p><p>As an example of inconsistent coding by TREC, consider Control # 4600522 and Control # 4600409. The first is an email report on a Senate Bill -SB 1806 -that pertains to an aspect of funding related to the Olympic Committee. TREC correctly called the email relevant, which was a good catch. But then TREC incorrectly classified as irrelevant Jeb's email reply to the report, which simply said "thanks Pam" but otherwise included the original email from Pam giving the legislative report. We frequently ran into things like that.</p><p>One error made by TREC assessors on the gold standard here was somewhat funny. It is an email on Project Olympus, dated in 2003. This is long after Florida gave up on the Summer Olympics ( <ref type="formula" coords="37,127.72,581.08,21.94,12.00">2001</ref>), and of course, its Olympus, not Olympics. Turns out it pertains to a Boeing airplane assembly plant they were trying to get in Jacksonville. Lots of similar language as in getting the Summer Olympics venue, but this had to do with getting Boeing to build a plant. Any human who actually read the email would see the error right away, but this was beyond the grasp of the machine learning TREC employed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Summer Olympics topic, the 90% recall threshold had been attained by submitting only 0.04%% of the corpus, 126 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc.). The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc.). Usual Multimodal approach was used in what proved to be a simple keyword search type project. The people involved in this issue were well defined and distinct. No AI was used except for quality assurance purposes.</p><p>As described in the Team's Final Report (fn 14) the large error rate seen in Topic 403 is an anomaly explained by the wrong call of one contested form email (Protect Florida's Springs) that had 913 near duplicates. Losey knew this form email had that many copies and so submitted a test submission before submitting the rest. He submitted a test expecting it to come back irrelevant because the email did not pertain to bottling. In the test the form came back as irrelevant, as it should have. But, as it turned out, that test was deceiving, because on most copies of this form TREC incorrectly classified it a relevant.</p><p>TREC's many other errors in judging this project appeared to be either completely off, just random error, or based upon calling a document relevant just because it mentioned extraction of water from Florida, even though the extraction was not for purposes of bottling by commercial enterprises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Bottled Water topic, the 90% recall threshold had been attained by submitting only 0.04%% of the corpus, 112 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>The project was run by Losey from July 8 th to July 15 th 2016. He spent four hours, reviewed 66 documents and manually classified 432. He called Reasonable after 11 submissions and then did just one more submission (12 total).</p><p>The full description of this topic was: All documents concerning House Speaker Newt Gingrich or any entities or personnel associated with Newt Gingrich.</p><p>This was a fairly simple search because, fortunately for Florida, Newt Gingrich and his company had only limited impact on Florida and Governor Bush. Seventeen keyword search folders were created at the beginning of the project and tested. That took most of the time here. The work went easier than most topics because there were very few TREC errors seen.</p><p>The very first submission of documents to TREC located all but five of the relevant documents. They were all found by keyword search Newt OR Gingrich*. I only looked at two documents in that search folder and saw they were obviously relevant. So I assumed all of the others with hits were relevant too, since this is such an unusual name, and did not bother to review them before classifying them. In "real life" we would spend more time verifying, of course. We would look at all 183 docs, as this is a small number. But part of our experiment here was to see how little effort we could put into these searches and still do reasonably well. AI ranking based searches were used after the first searches and first submission to find the rest. Again, this was an experiment to see how well we could do in an easy project like this with minimal human efforts after an initial discovery of the easy to find documents by keywords.</p><p>After that first submission Losey decided not to look at any documents in this topic or manually search. Instead he relied on just AI ranking and simply trained high ranking documents. He just assumed the predicted coding was right and used all of Mr. EDR's top ranked documents without inspection. He did so with many small submissions of the unique, most highly ranked documents. This was done to allow training to continue to improve. The only slight effort here was to differentiate unique docs, and only submit the top 25 unique ones. If an Email had the same subject line, it was presumed "NON-Unique" and Losey would skip down to the next ranked document that did not have the exact same subject line. He continued this pattern until all documents with a 50% or higher probable relevance had been submitted and then called reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Newt Gingrich topic, the 90% recall threshold had been attained by submitting only 0.06%% of the corpus, 163 documents for adjudication. In fact, they were both relevant. We would typically, but not always, include both documents into training and ignore TREC errors. The color you see added in the above emails is not in the originals. It is added by the software per user direction to assist in the quick human review of a document. Typically keywords the user selects are colored. This feature is a terrific time saver and was heavily utilized by all reviewers in all topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Felon Disenfranchisement topic, the 90% recall threshold had been attained by submitting only 0.06%% of the corpus, 183 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Losey and was the last topic reviewed from August 28 th to 31 st 2016. He made 21 submissions and called reasonable after the 14 th . He reviewed 400 documents and categorized 1,791. Losey spent far more time on this topic than any of the others, 15 hours.</p><p>The full description of this topic is: Faith-Based Initiatives -All documents concerning grants or other initiatives in Florida to offload social services to so-called faith-based agencies. Services include but are not limited to education, prisons, and emergency relief.</p><p>Losey created 46 different searches and search folders, also a high-volume record that helps explain the 15 hours this topic took to complete. A full multimodal approach was used, not just keywords, as this was a relatively difficult topic. Bush had many emails concerning this topic as this was one of his pet projects as governor. In addition to the many keyword searches, similarity and near duplication searches were use in any correct, TREC verified relevant document. There was also heavy reliance placed on AI ranking searches as the project matured. As an experiment in this topic the relevant documents that were incorrectly labeled as irrelevant by TREC were excluded from training. The result of this alternate strategy was not clear. Of course, no documents incorrectly labeled as relevant by TREC were used in training. We wanted to avoid the avoid the phenomena we had observed many times by this point, and which the Team had started calling the Ouroboros effect. This is the negative feedback loop where one automated classifier blindly follows another with no regard to ground truth. We saw that as akin to a snake eating its own tail, the Ouroboros, that is discussed in the Conclusion to the Team's Final Report and Footnote 17.</p><p>This topic had many errors by TREC. Some were borderline, so, as we always did, we accepted them as correct, even though they were against our view of relevance. Only the clearly wrong were corrected. Here is an example. The data contained seven copies of the same email, or nearly the same. The emails were all ironically written by a person who lives just a few blocks from his home. Below is one copy.</p><p>Three copies of the emails were classified as relevant by TREC and four were classified by TREC as irrelevant. It is hard to understand how this could happen, but we saw it all the time.</p><p>Just before making his personal reasonable call after the 14 th submission, Losey submitted the highest ranked documents down to 50%, and select keyword folders documents regardless of rank. He did so with little or no review in the last several submissions, relying on AI ranking alone informed by keyword search folders. Losey noted that he was sure he could find more relevant at that point if he kept reviewing more documents, but, after expending almost 15 hours on this topic already, it would not be a reasonable effort to do so. It would be excessive for all but the largest cases under Rule 26(b)(1) Federal Rules of Civil Procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Faith Based Initiatives topic, the 90% recall threshold had been attained by submitting only 0.90%% of the corpus, 2,603 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Tony Reichenberger. A google search of non-native species in Florida and the state Invasive Species webpage served as the basis for creating a list of keywords to search for relevant documents. It was apparent from the first submission that only select invasive species were considered relevant. Documents solely relating to species found irrelevant from the TREC feedback were coded irrelevant. Documents were submitted until the keywords were exhausted at which point the Reasonable call was made.</p><p>However, the standard was inconsistent in coding; for instance within the first submission was a document explicitly about Burmese python (a well-known invasive species to Florida causing a myriad of problems in the Everglades) which was returned from TREC as irrelevant. However, later submissions relating to Burmese Pythons were found relevant.</p><p>Likewise, assessors seemed to confuse "endangered' species such as manatees, with "invasive" species on a number of calls. Assessors also made the mistake of confusing species that are nuisances, such as particular red algae blooms, with being invasive, even though they are native to the area.</p><p>As such, this was an issue that the standard (particularly for lawyers) was inherently flawed, and not really indicative of the issue. Therefore, it is not representative of comparisons between human-only or hybrid reviewers and machine learning auto-runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Invasive Species topic, the 90% recall threshold had been attained by submitting only 0.19%% of the corpus, 553 documents for adjudication.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). The full description of this topic is: Jeffrey Goldhagen -All documents related to Jeffrey Goldhagen's role in the Bush administration, his firing, and reinstatement.' Losey had never heard of this man but a Google search quickly provided the background. He was a doctor and medical director for Jacksonville that was fired by Bush, and then rehired.</p><p>This topic had a number of obvious errors in TREC judging, including such things as a tendency to call relevant any email about a physician in trouble, even if it was not Dr.</p><p>Goldhagen. Also, the TREC classifier often seemed incapable of knowing when an email by Dr. Goldhagen's enemy, Holly Kartsonis, to Bush pertained to issues other than Dr.</p><p>Goldhagen. She often wrote to Jeb on a number of topics, usually personal and flattering. She also asked for Jeb's help to get another job with the State. Kartsonis' husband was a doctor and Bush seemed to like to chat with her (part of his online nice guy persona, which is pretty much forced, but not entirely bogus). She appeared to think that creating an online relationship with the governor would help her, and it did to a point. In fact, it was amazing to see how the online relationship developed with Jeb. They had many emails over the years. There was no indication in this cleaned collection they ever met. Still, in the end, Jeb never intervened in the final decision by the State not to employ her. These emails have nothing to do with this topic, which is Dr. Goldhagen, not talkative Holly, although Losey found it interesting to read the many emails between them.</p><p>This was a topic that was once again driven primarily by keyword search. Losey used Mr. EDR primarily for QC. He also used use both Data Index and Concept based searches to look for misspelling and other words, and did find one useful variation, namely that Goldhagen was once referred to as "Dr. G." It turns out that there were two Dr. G's, and a few other false hits, but this abbreviation did allow location of two Relevant emails that otherwise would not have been found. They were found by concept search and manual review. This once again shows the power of using all search features -multimodal -and not just predictive coding, or keyword.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Jeffrey Goldhagen topic, the 90% recall threshold had been attained by submitting only 0.03%% of the corpus, 94 documents for adjudication.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="28,277.10,73.94,57.61,12.00;28,224.35,102.19,163.16,12.00;28,256.60,116.21,98.86,12.00;28,266.60,130.21,78.78,12.00;28,208.83,159.71,194.14,12.00;28,215.08,174.71,181.95,12.00;28,72.03,202.99,464.95,12.00;28,72.03,216.99,438.96,12.00;28,72.03,230.99,456.25,12.00;28,72.03,245.24,457.06,12.00;28,72.03,259.24,465.77,12.00;28,72.03,273.26,458.87,12.00;28,72.03,287.51,465.04,12.00;28,72.03,301.51,48.38,12.00"><head></head><label></label><figDesc>Report describes the search of all thirty-four Total Recall topics in TREC 2016 using the e-Discovery Team's Hybrid Multimodal method. The searches are reported here numerically by Topic number, except for topic 434 Bacardi Trademark. We did not review the topics in numerical order. The first project was started on June 7, 2016 by Losey. It was topic 434 Bacardi Trademark. The last Topic 415 George W Bush, concluded on August 30, 2016 by Sullivan. We report on the first topic we reviewed first to provide a background and further information as to why we went to the drastic step of correcting the standard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,173.00,467.51,283.75,206.90"><head></head><label></label><figDesc></figDesc><graphic coords="5,173.00,467.51,283.75,206.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,173.52,291.73,282.95,279.18"><head></head><label></label><figDesc></figDesc><graphic coords="6,173.52,291.73,282.95,279.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="16,72.00,72.00,432.00,235.70"><head></head><label></label><figDesc></figDesc><graphic coords="16,72.00,72.00,432.00,235.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="16,156.00,454.49,299.75,180.18"><head></head><label></label><figDesc></figDesc><graphic coords="16,156.00,454.49,299.75,180.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="33,132.40,283.17,347.20,276.97"><head></head><label></label><figDesc></figDesc><graphic coords="33,132.40,283.17,347.20,276.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="34,130.50,72.00,350.70,275.02"><head></head><label></label><figDesc></figDesc><graphic coords="34,130.50,72.00,350.70,275.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="34,134.77,417.45,342.45,271.44"><head></head><label></label><figDesc></figDesc><graphic coords="34,134.77,417.45,342.45,271.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="38,137.25,170.55,336.85,268.71"><head></head><label></label><figDesc></figDesc><graphic coords="38,137.25,170.55,336.85,268.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="39,131.13,72.00,349.75,274.27"><head></head><label></label><figDesc></figDesc><graphic coords="39,131.13,72.00,349.75,274.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="39,134.25,416.90,343.47,272.25"><head></head><label></label><figDesc></figDesc><graphic coords="39,134.25,416.90,343.47,272.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="42,135.68,72.00,340.65,271.75"><head></head><label></label><figDesc></figDesc><graphic coords="42,135.68,72.00,340.65,271.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="42,146.90,414.15,318.20,249.53"><head></head><label></label><figDesc></figDesc><graphic coords="42,146.90,414.15,318.20,249.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="43,133.50,142.40,344.35,272.95"><head></head><label></label><figDesc></figDesc><graphic coords="43,133.50,142.40,344.35,272.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="46,146.65,72.00,318.70,254.24"><head></head><label></label><figDesc></figDesc><graphic coords="46,146.65,72.00,318.70,254.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="46,147.75,396.65,316.30,248.04"><head></head><label></label><figDesc></figDesc><graphic coords="46,147.75,396.65,316.30,248.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="47,138.00,72.00,335.80,266.17"><head></head><label></label><figDesc></figDesc><graphic coords="47,138.00,72.00,335.80,266.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="50,146.02,72.00,319.95,255.23"><head></head><label></label><figDesc></figDesc><graphic coords="50,146.02,72.00,319.95,255.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="50,149.77,397.65,312.45,245.02"><head></head><label></label><figDesc></figDesc><graphic coords="50,149.77,397.65,312.45,245.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="51,150.75,86.08,310.25,245.92"><head></head><label></label><figDesc></figDesc><graphic coords="51,150.75,86.08,310.25,245.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="54,143.20,72.00,325.60,259.74"><head></head><label></label><figDesc></figDesc><graphic coords="54,143.20,72.00,325.60,259.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="54,152.15,402.15,307.70,241.30"><head></head><label></label><figDesc></figDesc><graphic coords="54,152.15,402.15,307.70,241.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="55,147.00,86.08,317.80,252.00"><head></head><label></label><figDesc></figDesc><graphic coords="55,147.00,86.08,317.80,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="57,74.00,524.52,464.23,181.90"><head></head><label></label><figDesc></figDesc><graphic coords="57,74.00,524.52,464.23,181.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="58,74.00,88.07,467.55,298.05"><head></head><label></label><figDesc></figDesc><graphic coords="58,74.00,88.07,467.55,298.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="59,142.50,72.00,326.55,260.50"><head></head><label></label><figDesc></figDesc><graphic coords="59,142.50,72.00,326.55,260.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="59,141.38,403.40,329.19,258.15"><head></head><label></label><figDesc></figDesc><graphic coords="59,141.38,403.40,329.19,258.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="60,138.00,86.08,335.80,266.17"><head></head><label></label><figDesc></figDesc><graphic coords="60,138.00,86.08,335.80,266.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="63,145.00,74.00,321.95,317.42"><head></head><label></label><figDesc></figDesc><graphic coords="63,145.00,74.00,321.95,317.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="64,135.00,72.00,341.55,272.46"><head></head><label></label><figDesc></figDesc><graphic coords="64,135.00,72.00,341.55,272.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="64,130.18,415.40,351.65,275.76"><head></head><label></label><figDesc></figDesc><graphic coords="64,130.18,415.40,351.65,275.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="65,131.25,128.33,349.05,276.67"><head></head><label></label><figDesc></figDesc><graphic coords="65,131.25,128.33,349.05,276.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="68,138.50,72.00,335.00,267.24"><head></head><label></label><figDesc></figDesc><graphic coords="68,138.50,72.00,335.00,267.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="68,139.25,409.65,333.50,261.53"><head></head><label></label><figDesc></figDesc><graphic coords="68,139.25,409.65,333.50,261.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="69,156.93,128.32,298.15,236.33"><head></head><label></label><figDesc></figDesc><graphic coords="69,156.93,128.32,298.15,236.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="71,137.10,367.65,337.80,269.47"><head></head><label></label><figDesc></figDesc><graphic coords="71,137.10,367.65,337.80,269.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="72,151.50,72.00,308.65,242.04"><head></head><label></label><figDesc></figDesc><graphic coords="72,151.50,72.00,308.65,242.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="72,132.43,412.61,347.15,275.17"><head></head><label></label><figDesc></figDesc><graphic coords="72,132.43,412.61,347.15,275.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="74,141.75,367.66,328.40,261.97"><head></head><label></label><figDesc></figDesc><graphic coords="74,141.75,367.66,328.40,261.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="75,144.00,72.00,323.95,254.04"><head></head><label></label><figDesc></figDesc><graphic coords="75,144.00,72.00,323.95,254.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="75,141.75,410.53,328.25,260.19"><head></head><label></label><figDesc></figDesc><graphic coords="75,141.75,410.53,328.25,260.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="77,144.75,452.12,322.10,256.95"><head></head><label></label><figDesc></figDesc><graphic coords="77,144.75,452.12,322.10,256.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="78,142.50,156.48,326.80,256.28"><head></head><label></label><figDesc></figDesc><graphic coords="78,142.50,156.48,326.80,256.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="79,141.38,72.00,329.21,260.95"><head></head><label></label><figDesc></figDesc><graphic coords="79,141.38,72.00,329.21,260.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="82,132.40,72.00,347.20,276.97"><head></head><label></label><figDesc></figDesc><graphic coords="82,132.40,72.00,347.20,276.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="82,140.07,419.90,331.85,260.24"><head></head><label></label><figDesc></figDesc><graphic coords="82,140.07,419.90,331.85,260.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="83,137.63,128.32,336.72,266.90"><head></head><label></label><figDesc></figDesc><graphic coords="83,137.63,128.32,336.72,266.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="86,149.77,72.00,312.45,249.25"><head></head><label></label><figDesc></figDesc><graphic coords="86,149.77,72.00,312.45,249.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="86,134.95,391.65,342.10,268.28"><head></head><label></label><figDesc></figDesc><graphic coords="86,134.95,391.65,342.10,268.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="87,149.93,86.08,312.15,247.43"><head></head><label></label><figDesc></figDesc><graphic coords="87,149.93,86.08,312.15,247.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="89,130.50,381.73,350.95,279.96"><head></head><label></label><figDesc></figDesc><graphic coords="89,130.50,381.73,350.95,279.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="90,135.75,86.08,340.20,266.79"><head></head><label></label><figDesc></figDesc><graphic coords="90,135.75,86.08,340.20,266.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="90,131.25,423.48,349.46,277.00"><head></head><label></label><figDesc></figDesc><graphic coords="90,131.25,423.48,349.46,277.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="93,134.75,72.00,342.50,273.22"><head></head><label></label><figDesc></figDesc><graphic coords="93,134.75,72.00,342.50,273.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="93,136.50,415.65,338.30,265.30"><head></head><label></label><figDesc></figDesc><graphic coords="93,136.50,415.65,338.30,265.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="94,126.00,128.32,359.45,284.92"><head></head><label></label><figDesc></figDesc><graphic coords="94,126.00,128.32,359.45,284.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="97,129.00,72.00,353.80,282.24"><head></head><label></label><figDesc></figDesc><graphic coords="97,129.00,72.00,353.80,282.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="97,135.45,424.65,341.10,267.49"><head></head><label></label><figDesc></figDesc><graphic coords="97,135.45,424.65,341.10,267.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="98,137.15,128.32,337.70,267.68"><head></head><label></label><figDesc></figDesc><graphic coords="98,137.15,128.32,337.70,267.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="101,131.93,86.08,348.15,277.73"><head></head><label></label><figDesc></figDesc><graphic coords="101,131.93,86.08,348.15,277.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="101,131.25,434.23,348.80,273.53"><head></head><label></label><figDesc></figDesc><graphic coords="101,131.25,434.23,348.80,273.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="102,133.50,142.40,344.41,273.00"><head></head><label></label><figDesc></figDesc><graphic coords="102,133.50,142.40,344.41,273.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="104,137.25,438.04,336.85,268.71"><head></head><label></label><figDesc></figDesc><graphic coords="104,137.25,438.04,336.85,268.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="105,140.20,142.40,331.60,260.04"><head></head><label></label><figDesc></figDesc><graphic coords="105,140.20,142.40,331.60,260.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="106,141.40,72.00,329.20,260.94"><head></head><label></label><figDesc></figDesc><graphic coords="106,141.40,72.00,329.20,260.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="108,127.23,367.64,357.55,285.23"><head></head><label></label><figDesc></figDesc><graphic coords="108,127.23,367.64,357.55,285.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="109,136.38,86.08,339.25,266.04"><head></head><label></label><figDesc></figDesc><graphic coords="109,136.38,86.08,339.25,266.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="109,138.00,422.53,335.80,266.17"><head></head><label></label><figDesc></figDesc><graphic coords="109,138.00,422.53,335.80,266.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="112,126.00,72.00,359.45,286.74"><head></head><label></label><figDesc></figDesc><graphic coords="112,126.00,72.00,359.45,286.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="112,144.75,429.15,322.00,252.51"><head></head><label></label><figDesc></figDesc><graphic coords="112,144.75,429.15,322.00,252.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="113,145.50,128.32,320.65,254.16"><head></head><label></label><figDesc></figDesc><graphic coords="113,145.50,128.32,320.65,254.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="116,124.90,72.00,362.20,288.94"><head></head><label></label><figDesc></figDesc><graphic coords="116,124.90,72.00,362.20,288.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="116,133.50,431.90,344.94,270.50"><head></head><label></label><figDesc></figDesc><graphic coords="116,133.50,431.90,344.94,270.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="117,134.77,142.40,342.45,271.44"><head></head><label></label><figDesc></figDesc><graphic coords="117,134.77,142.40,342.45,271.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="119,141.00,409.85,329.35,262.73"><head></head><label></label><figDesc></figDesc><graphic coords="119,141.00,409.85,329.35,262.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="120,136.38,128.32,339.25,266.04"><head></head><label></label><figDesc></figDesc><graphic coords="120,136.38,128.32,339.25,266.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="121,132.43,72.00,347.15,275.17"><head></head><label></label><figDesc></figDesc><graphic coords="121,132.43,72.00,347.15,275.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="124,136.50,72.00,338.75,270.23"><head></head><label></label><figDesc></figDesc><graphic coords="124,136.50,72.00,338.75,270.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="124,141.65,412.65,328.70,257.77"><head></head><label></label><figDesc></figDesc><graphic coords="124,141.65,412.65,328.70,257.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="125,141.75,142.40,328.25,260.19"><head></head><label></label><figDesc></figDesc><graphic coords="125,141.75,142.40,328.25,260.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="127,139.23,367.64,333.55,250.34"><head></head><label></label><figDesc></figDesc><graphic coords="127,139.23,367.64,333.55,250.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="128,116.77,72.00,378.45,296.78"><head></head><label></label><figDesc></figDesc><graphic coords="128,116.77,72.00,378.45,296.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="129,128.25,72.00,354.75,281.19"><head></head><label></label><figDesc></figDesc><graphic coords="129,128.25,72.00,354.75,281.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="132,136.50,72.00,338.75,270.23"><head></head><label></label><figDesc></figDesc><graphic coords="132,136.50,72.00,338.75,270.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="132,135.43,412.65,341.15,267.53"><head></head><label></label><figDesc></figDesc><graphic coords="132,135.43,412.65,341.15,267.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="133,141.75,128.32,328.25,260.19"><head></head><label></label><figDesc></figDesc><graphic coords="133,141.75,128.32,328.25,260.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="136,135.68,72.00,340.65,271.75"><head></head><label></label><figDesc></figDesc><graphic coords="136,135.68,72.00,340.65,271.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="136,128.73,400.07,354.55,278.04"><head></head><label></label><figDesc></figDesc><graphic coords="136,128.73,400.07,354.55,278.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="137,137.15,128.32,337.70,267.68"><head></head><label></label><figDesc></figDesc><graphic coords="137,137.15,128.32,337.70,267.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="140,143.20,72.00,325.60,259.74"><head></head><label></label><figDesc></figDesc><graphic coords="140,143.20,72.00,325.60,259.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="140,123.00,402.15,366.00,287.02"><head></head><label></label><figDesc></figDesc><graphic coords="140,123.00,402.15,366.00,287.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="141,119.18,128.32,373.65,296.17"><head></head><label></label><figDesc></figDesc><graphic coords="141,119.18,128.32,373.65,296.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="143,131.93,367.65,348.15,277.73"><head></head><label></label><figDesc></figDesc><graphic coords="143,131.93,367.65,348.15,277.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="144,138.75,72.00,334.48,262.30"><head></head><label></label><figDesc></figDesc><graphic coords="144,138.75,72.00,334.48,262.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="144,127.50,404.90,356.65,282.70"><head></head><label></label><figDesc></figDesc><graphic coords="144,127.50,404.90,356.65,282.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="147,129.75,72.00,351.90,280.72"><head></head><label></label><figDesc></figDesc><graphic coords="147,129.75,72.00,351.90,280.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="147,127.50,423.15,356.45,279.53"><head></head><label></label><figDesc></figDesc><graphic coords="147,127.50,423.15,356.45,279.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="148,139.50,128.32,332.95,263.91"><head></head><label></label><figDesc></figDesc><graphic coords="148,139.50,128.32,332.95,263.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="151,120.75,72.00,370.50,278.07"><head></head><label></label><figDesc></figDesc><graphic coords="151,120.75,72.00,370.50,278.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="151,131.25,420.50,348.80,273.53"><head></head><label></label><figDesc></figDesc><graphic coords="151,131.25,420.50,348.80,273.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="152,131.95,128.32,348.10,275.92"><head></head><label></label><figDesc></figDesc><graphic coords="152,131.95,128.32,348.10,275.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="154,129.75,395.81,351.90,280.72"><head></head><label></label><figDesc></figDesc><graphic coords="154,129.75,395.81,351.90,280.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="155,130.50,128.32,350.68,275.00"><head></head><label></label><figDesc></figDesc><graphic coords="155,130.50,128.32,350.68,275.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="156,138.75,72.00,333.85,264.63"><head></head><label></label><figDesc></figDesc><graphic coords="156,138.75,72.00,333.85,264.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="158,134.25,388.56,343.45,273.98"><head></head><label></label><figDesc></figDesc><graphic coords="158,134.25,388.56,343.45,273.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="159,129.00,86.08,353.60,277.29"><head></head><label></label><figDesc></figDesc><graphic coords="159,129.00,86.08,353.60,277.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="160,132.45,72.00,347.10,275.13"><head></head><label></label><figDesc></figDesc><graphic coords="160,132.45,72.00,347.10,275.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="163,126.00,72.00,359.45,286.74"><head></head><label></label><figDesc></figDesc><graphic coords="163,126.00,72.00,359.45,286.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="163,132.55,429.15,346.90,272.04"><head></head><label></label><figDesc></figDesc><graphic coords="163,132.55,429.15,346.90,272.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="164,136.20,128.32,339.60,269.18"><head></head><label></label><figDesc></figDesc><graphic coords="164,136.20,128.32,339.60,269.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,90.03,383.12,251.07,180.05"><head>93.55% on topic 422 Non Resident Aliens; • 99.60% on topic 424 Gulf Drilling; • 91.32% on topic 425 Civil Rights Act of 2003; • 93.10% on topic 428 New Stadiums and</head><label></label><figDesc></figDesc><table coords="11,90.03,383.12,251.07,180.05"><row><cell>95.08% on topic 406 Felon Disenfranchisement;</cell></row><row><cell>• 95.10% on topic 410 Condominiums;</cell></row><row><cell>• 96.34% on topic 413 James V. Crosby;</cell></row><row><cell>• 99.61% on topic 417 Movie Gallery;</cell></row><row><cell>• 92.54% on topic 420 Billboards;</cell></row><row><cell>• 90.48% on topic 421 Traffic Cameras;</cell></row><row><cell>• Arenas;</cell></row><row><cell>• 94.20% on topic 429 Elian Gonzalez;</cell></row><row><cell>• 99.11% on topic 433 Abstinence.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,217.87,324.06,176.09,385.12"><head>George W. Bush 94.188% Movie Gallery 100.000% War Preparations 80.851% Lost Foster Child Rilya Wilson 99</head><label></label><figDesc></figDesc><table coords="13,217.87,324.06,176.09,385.12"><row><cell>Name</cell><cell>Recall</cell></row><row><cell>Summer Olympics</cell><cell>91.971%</cell></row><row><cell>Newt Gingrich</cell><cell>100.000%</cell></row><row><cell>Felon Disenfranchisement</cell><cell>97.044%</cell></row><row><cell>Faith Based Initiatives</cell><cell>88.573%</cell></row><row><cell>Climate Change</cell><cell>88.393%</cell></row><row><cell>Condominiums</cell><cell>99.772%</cell></row><row><cell>Stand Your Ground</cell><cell>100.000%</cell></row><row><cell>2000 Recount</cell><cell>87.882%</cell></row><row><cell>James V. Crosby</cell><cell>96.833%</cell></row><row><cell>Medicaid Reform</cell><cell>92.773%</cell></row><row><cell></cell><cell>.092%</cell></row><row><cell>Billboards</cell><cell>95.670%</cell></row><row><cell>Traffic Cameras</cell><cell>96.296%</cell></row><row><cell>Non Resident Aliens</cell><cell>100.000%</cell></row><row><cell>Gulf Drilling</cell><cell>99.596%</cell></row><row><cell>Civil Rights Act of 2003</cell><cell>90.947%</cell></row><row><cell>Jeffrey Goldhagen</cell><cell>92.857%</cell></row><row><cell>Slot Machines</cell><cell>94.677%</cell></row><row><cell>New Stadiums and Arenas</cell><cell>93.908%</cell></row><row><cell>Elian Gonzalez</cell><cell>97.038%</cell></row><row><cell>Agency Credit Ratings</cell><cell>80.537%</cell></row><row><cell>Gay Adoption</cell><cell>91.241%</cell></row><row><cell>Abstinence</cell><cell>100.000%</cell></row><row><cell>Bacardi Trademark</cell><cell>100.000%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="20,90.03,178.30,288.56,294.10"><head></head><label></label><figDesc>Topic 403 Bottled Water: 7.16% to 78.05%. o Change of 70.89%. o Error of 990%. • Topic 407 Faith Based Initiatives: 31.02% to 88.57%. o Change of 57.55%. o Error of 186%. • Topic 401 Summer Olympics: 41.05% to 91.97%. o Change of 50.92%. o Error of 124%. • Topic 418 War Preparations: 39.57% to 80.85%.</figDesc><table /><note coords="20,126.05,326.60,108.46,12.00;20,126.05,341.35,87.68,12.00;20,90.03,356.62,231.89,12.00;20,126.05,371.37,108.57,12.00;20,126.05,385.87,81.80,12.00;20,90.03,401.12,288.56,12.00;20,126.05,415.87,108.57,12.00;20,126.05,430.65,81.80,12.00;20,90.03,445.90,252.60,12.00;20,126.05,460.40,108.57,12.00"><p>o Change of 41.28%. o Error of 104%. • Topic 412 2000 Recount: 57.37% to 87.88%. o Change of 30.51%. o Error of 53%. • Topic 423 National Rifle Association: 51.05% to 77.37%. o Change of 26.32%. o Error of 52%. • Topic 426 Jeffery Goldhagen: 70.00% to 92.86%. o Change of 22.86%.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="24,72.03,250.33,458.41,246.32"><head></head><label></label><figDesc>Total Recall Track fully automated method uses a type of monomodal search method where only certain defined high-ranking documents are used for training. This method is more fully described in paper by the Total Recall Track Administrators,</figDesc><table /><note coords="24,108.05,294.35,397.34,12.00;24,108.05,308.85,394.86,12.00;24,108.05,323.60,154.31,12.00;24,72.03,338.10,13.42,12.00;24,108.05,338.10,390.35,12.00;24,108.05,352.87,412.04,12.00;24,108.05,367.37,402.72,12.00;24,108.05,382.12,403.90,12.00;24,108.05,396.87,422.38,12.00;24,108.05,411.37,370.84,12.00;24,108.05,426.12,405.82,12.00;24,108.05,440.65,419.04,12.00;24,108.05,455.40,346.04,12.00;24,108.05,470.15,162.32,12.00;24,72.03,484.65,13.42,12.00"><p>Grossman &amp; Cormack, Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review, CoRR abs/1504.06868 (2015). They call the method "Autonomous TAR." Id. at pg. 6. [2] E-Discovery For Everyone, Ralph Losey; Foreword Judge Paul Grimm (ABA 2016); Perspectives On Predictive Coding And Other Advanced Search Methods for the Legal Practitioner; Editors: Jason R. Baron, Ralph C. Losey, Michael Berman; Foreword by Judge Andrew Peck (ABA 2016); Adventures in Electronic Discovery (West Thomson Reuters, 2011); Electronic Discovery: New Ideas, Trends, Case Law, and Practices (West Thomson Reuters, 2010); Introduction to E-Discovery: New Cases, Ideas, and Techniques (ABA 2009); e-Discovery: Current Trends and Cases (ABA 2008). Also see Predictive Coding Articles by Ralph Losey, (collection of over 60 articles by Ralph Losey further describing the hybrid multimodal approach) found at https://ediscoveryteam.com/doc-review/. [3]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="24,108.05,660.45,429.62,41.52"><head></head><label></label><figDesc>"An important thing we learned early on is that successful searching requires what I called "berrypicking." … Berrypicking involves 1) searching many different places/sources, 2) using different search techniques in different places, and 3) changing your search goal as you go along and learn things along the way. This may seem fairly obvious when stated this way, but, in fact, many searchers erroneously think they will find everything they want in just one place, and second, many information systems have been designed to permit only one kind of searching, and inhibit the searcher from using the more effective berrypicking technique." Also see: White &amp; Roth, Exploratory Search: Beyond the Query-Response Paradigm (Morgan &amp; Claypool, 2009). [5] Predictive Coding is defined by The Grossman-Cormack Glossary of Technology-Assisted Review, 2013 Fed. Cts. L. Rev. 7 (January 2013) (Grossman-Cormack Glossary) as: "An industry-specific term generally used to describe a Technology Assisted Review process involving the use of a Machine Learning Algorithm to distinguish Relevant from Non-Relevant Documents, based on Subject Matter Expert(s) Coding of a Training Set of Documents. " A Technology Assisted Review process is defined as: "A process for Prioritizing or Coding a Collection of electronic Documents using a computerized system that harnesses human judgments of one or more Subject Matter Expert(s) on a smaller set of Documents and then extrapolates those judgments to the remaining Document Collection. … TAR processes generally incorporate Statistical Models and/or Sampling techniques to guide the process and to measure overall system effectiveness." Also see: Technology-Assisted Review in E-Discovery Can Be More Effective and More Efficient Than Exhaustive Manual Review, Richmond Journal of Law and Technology, Vol.</figDesc><table coords="25,72.03,411.37,464.28,261.08"><row><cell></cell><cell>XVII, Issue 3, Article 11 (2011).</cell></row><row><cell>[6]</cell><cell>Da Silva Moore v. Publicis Groupe 868 F. Supp. 2d 137 (SDNY 2012) and numerous cases</cell></row><row><cell></cell><cell>later citing to and following this landmark decision by Judge Andrew Peck, including</cell></row><row><cell></cell><cell>another more recent opinion by Judge Peck, Rio Tinto PLC v. Vale S.A., 306 F.R.D. 125</cell></row><row><cell></cell><cell>(S.D.N.Y. 2015). Losey was defense counsel in charge of the predictive coding review in</cell></row><row><cell></cell><cell>Da Silva.</cell></row><row><cell>[7]</cell><cell>Grossman &amp; Cormack, Evaluation of Machine-Learning Protocols for Technology-</cell></row><row><cell></cell><cell>Assisted Review in Electronic Discovery, SIGIR'14, July 6-11, 2014; Grossman &amp;</cell></row><row><cell></cell><cell>Cormack, Comments on "The Implications of Rule 26(g) on the Use of Technology-</cell></row><row><cell></cell><cell>Assisted Review", 7 Federal Courts Law Review 286 (2014); Herbert Roitblat, series of</cell></row><row><cell></cell><cell>five OrcaTec blog posts (1, 2, 3, 4, 5), May-August 2014; Herbert Roitblat, Daubert, Rule</cell></row><row><cell></cell><cell>26(g) and the eDiscovery Turkey OrcaTec blog, August 11th, 2014; Hickman &amp;</cell></row><row><cell></cell><cell>Schieneman, The Implications of Rule 26(g) on the Use of Technology-Assisted Review,</cell></row><row><cell></cell><cell>7 FED. CTS. L. REV. 239 (2013); Losey, R. Predictive Coding 3.0, part one (e-Discovery</cell></row><row><cell></cell><cell>Team 10/11/15).</cell></row><row><cell>[8]</cell><cell>Id.; Webber, Random vs active selection of training examples in e-discovery (Evaluating</cell></row><row><cell></cell><cell>e-Discovery blog, 7/14/14).</cell></row><row><cell>[9]</cell><cell></cell></row></table><note coords="24,396.61,660.45,141.06,12.00;24,108.05,675.23,405.19,12.00;24,108.05,689.98,307.34,12.00;25,108.05,660.45,406.95,12.00;25,108.05,675.23,427.07,12.00;25,108.05,689.98,406.24,12.00;25,108.05,704.48,155.83,12.00"><p>, The Design of Browsing and Berrypicking Techniques for the Online Search Interface, Online Review 13 (October 1989): 407-424. As Professor Bates explained in 2011 in Quora: Losey, R., Predictive Coding 4.0 -Nine Key Points of Legal Document Review and an Updated Statement of Our Workflow (e-Discovery Team, 9/12/16) (Part One of an Eight Part Series explaining the recent advancements from our Predictive Coding method from version 3.0 to version 4.0).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="26,72.03,89.03,466.89,217.32"><head></head><label></label><figDesc>Predictive Coding 4.0 Part Two. [11] Grossman &amp; Cormack, Evaluation of Machine-Learning Protocols for Technology-Assisted Review in Electronic Discovery, SIGIR'14, July 6-11, 2014. [12] Participant appeal rights could have mitigated the errors seen in 2016, but this can be burdensome and, as seen in those Tracks in 2008 and 2009, can create their own issues. See: Oard, Hedlin, Tomlinson, Baron, Overview of the TREC 2008 Legal Track, found at http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf; and Oard, Hedlin, Tomlinson, Baron, Oard, Overview of the TREC 2009 Legal Track found at: http://trec.nist.gov/pubs/trec18/papers/LEGAL09.OVERVIEW.pdf. [13] See In re Fannie Mae Sec. Litig., 552 F.3d 814 (D.C. Cir. 2009) ($9.09 per file cost for a privilege review, using contract lawyers and linear method. Total cost of $6,000.000 to review 660,000); Losey, E-Discovery For Everyone (ABA 2016), Chapter Three Perspective on Legal Search and Document Review. [14] The full description of relevance for 403 is: "403-Bottled Water -All documents concerning the extraction of water in Florida for bottling by commercial enterprises."</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="27,72.03,89.03,462.49,246.57"><head></head><label></label><figDesc>They frequently simply guess as to what words are important and do not first test the words nor study the dataset. Also, they rarely used Boolean logic, nor limit the searches to specific document parameters. Child's Game of "Go Fish" is a Poor Model for e-Discovery Search, Losey, R., Adventures in ElectronicDiscovery, 209-211 (West 2011) at pgs. 204-210. Also See: William A. Gross  Constr. Assocs., Inc. v. Am. Mutual Mfrs. Ins. Co., 256 F.R.D. 134, 134 (S.D.N.Y. 2009).[17] The Ouroboros is an ancient symbol of continuing self-reference and recursivity, and thus an apt symbol, although not necessarily positive, for the iterative cycles in active machine learning. This symbol has been used before in machine learning. See eg.: Knud Thomsen, The Ouroboros Model in the light of venerable criteria, Journal Neurocomputing archive, Vol. 74 Issue 1-3, December, 2010, pgs. 121-128; Thomsen, Flow of Activity in the Ouroboros Model, arXiv:0903.5054 [cs.AI] (2009) found at https://arxiv.org/pdf/0903.5054v1.pdf. Also see: Wikipedia, Self-reference found at: https://en.wikipedia.org/wiki/Self-reference, and Recursivity found at https://en.wikipedia.org/wiki/Recursion. The first description in the West of the ouroboros can be found in Plato in the Dialogue Timaeus. The ouroboros is described as the first living thing created in the universe which:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="29,72.03,73.94,312.46,603.17"><head></head><label></label><figDesc>The TREC Total Recall project commenced on June 7, 2016 with work on Topic 434 Bacardi Trademark. This topic was run by Losey. He completed work on June 8, 2016 after spending a total of four hours on the project. In the course of the project he reviewed a total of 107 documents.</figDesc><table coords="29,72.03,73.94,312.46,603.17"><row><cell cols="2">Topic 434 -Bacardi Trademark Summary</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 38</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.01%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Bacardi Trademark</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90%</cell><cell>@95%</cell></row><row><cell cols="2">Errors in Gold Standard</cell><cell>Recall</cell><cell>Recall</cell></row><row><cell cols="2">True Positives 38</cell><cell>35</cell><cell>37</cell></row><row><cell cols="4">True Negatives 290,061 Unfortunately, multiple obvious errors in TREC's judging of relevant documents were 290,061 290,061</cell></row><row><cell cols="4">False Positives 0 immediately encountered. Although there only 38 relevant documents found, a quick 0 0</cell></row><row><cell cols="4">False review of the 38 documents TREC called relevant shows that three are not relevant. They 0 3 1</cell></row><row><cell cols="4">Negatives have nothing whatsoever to do with this topic. These three (two including a duplicate)</cell></row><row><cell cols="4">Recall obviously irrelevant documents have TREC ID number: 119771, 005283 (duplicate of 100.00% 92.11% 97.37%</cell></row><row><cell cols="4">Precision 119771), 147890. Three more documents (two including a partial duplicate chain email) 100.00% 100.00% 100.00%</cell></row><row><cell cols="4">F1 Measure are relevant to this topic, but were called irrelevant by TREC. Their TREC ID numbers are: 100.00% 95.89% 98.67%</cell></row><row><cell cols="3">Accuracy 110559, 110507 (same chain as 110559), 126174. 100.00% 99.9990%</cell><cell>99.9997%</cell></row><row><cell>Error</cell><cell>0.0%</cell><cell>0.0010%</cell><cell>0.0003%</cell></row><row><cell>Elusion</cell><cell>0.00%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>Fallout</cell><cell>0.00%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell cols="4">Topic 434 -Bacardi Trademark -UNCORRECTED</cell></row><row><cell cols="2">Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 38</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.01%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Bacardi Trademark</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90%</cell><cell>@95%</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>Recall</cell></row><row><cell cols="2">True Positives 33</cell><cell>35</cell><cell>37</cell></row><row><cell cols="2">True Negatives 290,058</cell><cell>290,058</cell><cell>289,659</cell></row><row><cell cols="2">False Positives 3</cell><cell>3</cell><cell>402</cell></row><row><cell>False</cell><cell>5</cell><cell>3</cell><cell>1</cell></row><row><cell>Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>86.84%</cell><cell>92.11%</cell><cell>97.37%</cell></row><row><cell>Precision</cell><cell>91.67%</cell><cell>92.11%</cell><cell>8.43%</cell></row><row><cell>F1 Measure</cell><cell>89.19%</cell><cell>92.11%</cell><cell>15.51%</cell></row><row><cell>Accuracy</cell><cell>100.00%</cell><cell>100.00%</cell><cell>99.86%</cell></row><row><cell>Error</cell><cell>0.00%</cell><cell>0.00%</cell><cell>0.14%</cell></row><row><cell>Elusion</cell><cell>0.00%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>Fallout</cell><cell>0.00%</cell><cell>0.00%</cell><cell>0.14%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="35,72.03,73.94,312.46,645.45"><head></head><label></label><figDesc>Topic 401 was run by Losey, who started on July 15 th, 2016 and ended on August 5 th , 2016.</figDesc><table coords="35,72.03,73.94,312.46,645.45"><row><cell cols="2">Topic 401 -Summer Olympics</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 137</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.05%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Summer Olympics</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90%</cell><cell>@95%</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>Recall</cell></row><row><cell cols="2">True Positives 126</cell><cell>124</cell><cell>131</cell></row><row><cell cols="2">True Negatives 289,960</cell><cell>289,960</cell><cell>289,950</cell></row><row><cell cols="2">False Positives 2</cell><cell>2</cell><cell>12</cell></row><row><cell>False</cell><cell>11</cell><cell>13</cell><cell>6</cell></row><row><cell>Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>91.97%</cell><cell>90.51%</cell><cell>95.62%</cell></row><row><cell>Precision</cell><cell>98.44%</cell><cell>98.41%</cell><cell>91.61%</cell></row><row><cell>F1 Measure</cell><cell>95.09%</cell><cell>94.30%</cell><cell>93.57%</cell></row><row><cell>Accuracy</cell><cell>99.9955%</cell><cell>99.9948%</cell><cell>99.9938%</cell></row><row><cell>Error</cell><cell>0.0045%</cell><cell>0.0052%</cell><cell>0.0062%</cell></row><row><cell>Elusion</cell><cell>0.00%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>Fallout</cell><cell>0.00%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell cols="3">Topic 401 -Summer Olympics -UNCORRECTED</cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 229</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.08%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Summer Olympics</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90%</cell><cell>@95%</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>Recall</cell></row><row><cell cols="2">True Positives 94</cell><cell>207</cell><cell>218</cell></row><row><cell cols="2">True Negatives 289,836</cell><cell>272,397</cell><cell>173,073</cell></row><row><cell cols="2">False Positives 34</cell><cell>17,473</cell><cell>116,797</cell></row><row><cell>False</cell><cell>135</cell><cell>22</cell><cell>11</cell></row><row><cell>Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>41.05%</cell><cell>90.39%</cell><cell>95.20%</cell></row><row><cell>Precision</cell><cell>73.44%</cell><cell>1.17%</cell><cell>0.19%</cell></row><row><cell>F1 Measure</cell><cell>52.66%</cell><cell>2.31%</cell><cell>0.37%</cell></row><row><cell>Accuracy</cell><cell>99.94%</cell><cell>93.97%</cell><cell>59.74%</cell></row><row><cell>Error</cell><cell>0.06%</cell><cell>6.03%</cell><cell>40.26%</cell></row><row><cell>Elusion</cell><cell>0.05%</cell><cell>0.01%</cell><cell>0.01%</cell></row><row><cell>Fallout</cell><cell>0.01%</cell><cell>6.03%</cell><cell>40.29%</cell></row><row><cell>Summary</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="44,72.03,73.94,312.46,589.17"><head>Bottled Water -All documents concerning the extraction of water in Florida for bottling by commercial enterprises.</head><label></label><figDesc>This project was run by Losey from June 11 th to June 15 th 2016. He spent six hours on the project, personally reviewed 218 documents and manually categorized 1,126. He called reasonable after nine submissions and made a total of nineteen submissions.The full description for the topic is: Again this topic was interesting to Losey because the extraction of Florida's precious water aquifer from spring water, for the purpose of sales of bottled water around the world, takes place near where he lives in Florida. He is also politically opposed to this since Nestle does so without payment for the water, just because they own land near a spring, and he contends it should be preserved for Floridians, or at the very least, Nestle should be charge full value for the state's critical resource. In spite of general familiarity with the situation, Losey began his work by Google searches to find out the names and other details of this controversial topic.</figDesc><table coords="44,72.03,73.94,312.46,589.17"><row><cell cols="2">Topic 403 -Bottled Water Summary</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 123</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.04%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Bottled Water</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90%</cell><cell>@95%</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>Recall</cell></row><row><cell cols="2">True Positives 96</cell><cell>111</cell><cell>117</cell></row><row><cell cols="2">True Negatives 289,975</cell><cell>289,975</cell><cell>289,975</cell></row><row><cell cols="2">False Positives 1</cell><cell>1</cell><cell>1</cell></row><row><cell>False</cell><cell>27</cell><cell>12</cell><cell>6</cell></row><row><cell>Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>78.05%</cell><cell>90.24%</cell><cell>95.12%</cell></row><row><cell>Precision</cell><cell>98.97%</cell><cell>99.11%</cell><cell>99.15%</cell></row><row><cell>F1 Measure</cell><cell>87.27%</cell><cell>94.47%</cell><cell>97.10%</cell></row><row><cell>Accuracy</cell><cell>99.9903%</cell><cell>99.9955%</cell><cell>99.9976%</cell></row><row><cell>Error</cell><cell>0.0097%</cell><cell>0.0045%</cell><cell>0.0024%</cell></row><row><cell>Elusion</cell><cell>0.01%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>Fallout</cell><cell>0.00%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell cols="3">Topic 403 -Bottled Water -UNCORRECTED</cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 1,090</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.38%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Bottled Water</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90%</cell><cell>@95%</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>Recall</cell></row><row><cell cols="2">True Positives 78</cell><cell>981</cell><cell>1,036</cell></row><row><cell cols="2">True Negatives 288,990</cell><cell>288,870</cell><cell>288,866</cell></row><row><cell cols="2">False Positives 19</cell><cell>139</cell><cell>143</cell></row><row><cell>False</cell><cell>1,012</cell><cell>109</cell><cell>54</cell></row><row><cell>Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>7.16%</cell><cell>90.00%</cell><cell>95.05%</cell></row><row><cell>Precision</cell><cell>80.41%</cell><cell>87.59%</cell><cell>87.87%</cell></row><row><cell>F1 Measure</cell><cell>13.14%</cell><cell>88.78%</cell><cell>91.32%</cell></row><row><cell>Accuracy</cell><cell>99.64%</cell><cell>99.91%</cell><cell>99.93%</cell></row><row><cell>Error</cell><cell>0.36%</cell><cell>0.09%</cell><cell>0.07%</cell></row><row><cell>Elusion</cell><cell>0.35%</cell><cell>0.04%</cell><cell>0.02%</cell></row><row><cell>Fallout</cell><cell>0.01%</cell><cell>0.05%</cell><cell>0.05%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25" coords="70,72.03,73.94,312.46,603.17"><head></head><label></label><figDesc>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</figDesc><table coords="70,72.03,73.94,312.46,603.17"><row><cell cols="2">Topic 409 -Climate Change Topic 410 -Condominiums Topic 411 -Stand Your Ground</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 224 Total Relevant: 1,317 Total Relevant: 59</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.08% Total Prevalence: 0.45% Total Prevalence: 0.02%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Climate Change Confusion Matrix -Condominiums Confusion Matrix -Stand Your Ground</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90% @Reasonable @90% @Reasonable @90%</cell><cell>@95% @95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall Recall Recall</cell><cell>Recall Recall Recall</cell></row><row><cell cols="2">True Positives 198 True Positives 1,314 True Positives 59</cell><cell>202 1,186 54</cell><cell>213 1,252 57</cell></row><row><cell cols="2">True Negatives 289,653 True Negatives 287,321 True Negatives 290,011</cell><cell>289,254 287,583 290,027</cell><cell>273,227 287,497 290,019</cell></row><row><cell cols="2">False Positives 222 False Positives 1,461 False Positives 29</cell><cell>621 1,199 13</cell><cell>16,648 1,285 21</cell></row><row><cell>False False False</cell><cell>26 3 0</cell><cell>22 131 5</cell><cell>11 65 2</cell></row><row><cell>Negatives Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall Recall</cell><cell>88.39% 99.77% 100.00%</cell><cell>90.18% 90.05% 91.53%</cell><cell>95.09% 95.06% 96.61%</cell></row><row><cell>Precision Precision Precision</cell><cell>47.14% 47.35% 67.05%</cell><cell>24.54% 49.73% 80.60%</cell><cell>1.26% 49.35% 73.08%</cell></row><row><cell>F1 Measure F1 Measure F1 Measure</cell><cell>61.49% 64.22% 80.27%</cell><cell>38.59% 64.07% 85.71%</cell><cell>2.49% 64.97% 83.21%</cell></row><row><cell>Accuracy Accuracy Accuracy</cell><cell>99.9145% 99.4953% 99.9900%</cell><cell>99.7784% 99.5415% 99.9938%</cell><cell>94.2575% 99.5346% 99.9921%</cell></row><row><cell>Error Error Error</cell><cell>0.0855% 0.5047% 0.0100%</cell><cell>0.2216% 0.4585% 0.0062%</cell><cell>5.7425% 0.4654% 0.0079%</cell></row><row><cell>Elusion Elusion Elusion</cell><cell>0.01% 0.00% 0.00%</cell><cell>0.01% 0.05% 0.00%</cell><cell>0.00% 0.02% 0.00%</cell></row><row><cell>Fallout Fallout Fallout</cell><cell>0.08% 0.51% 0.01%</cell><cell>0.21% 0.42% 0.00%</cell><cell>5.74% 0.44% 0.01%</cell></row><row><cell cols="4">Topic 409 -Climate Change -UNCORRECTED Topic 410 -Condominiums -UNCORRECTED Topic 411 -Stand Your Ground -UNCORRECTED</cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 202 Total Relevant: 1,346 Total Relevant: 89</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.07% Total Prevalence: 0.46% Total Prevalence: 0.03%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Climate Change Confusion Matrix -Condominiums Confusion Matrix -Stand Your Ground</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90% @Reasonable @90% @Reasonable @90%</cell><cell>@95% @95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall Recall Recall</cell><cell>Recall Recall Recall</cell></row><row><cell cols="2">True Positives 171 True Positives 1,280 True Positives 59</cell><cell>182 1,212 81</cell><cell>192 1,279 85</cell></row><row><cell cols="2">True Negatives 289,648 True Negatives 287,258 True Negatives 289,981</cell><cell>285,786 287,445 250,502</cell><cell>248,332 287,305 143,021</cell></row><row><cell cols="2">False Positives 249 False Positives 1,495 False Positives 29</cell><cell>4,111 1,308 39,508</cell><cell>41,565 1,448 146,989</cell></row><row><cell>False False False</cell><cell>31 66 30</cell><cell>20 134 8</cell><cell>10 67 4</cell></row><row><cell>Negatives Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall Recall</cell><cell>84.65% 95.10% 66.29%</cell><cell>90.10% 90.04% 91.01%</cell><cell>95.05% 95.02% 95.51%</cell></row><row><cell>Precision Precision Precision</cell><cell>40.71% 46.13% 67.05%</cell><cell>4.24% 48.10% 0.20%</cell><cell>0.46% 46.90% 0.06%</cell></row><row><cell>F1 Measure F1 Measure F1 Measure</cell><cell>54.98% 62.12% 66.67%</cell><cell>8.10% 62.70% 0.41%</cell><cell>0.92% 62.80% 0.12%</cell></row><row><cell>Accuracy Accuracy Accuracy</cell><cell>99.90% 99.46% 99.98%</cell><cell>98.58% 99.50% 86.38%</cell><cell>85.67% 99.48% 49.33%</cell></row><row><cell>Error Error Error</cell><cell>0.10% 0.54% 0.02%</cell><cell>1.42% 0.50% 13.62%</cell><cell>14.33% 0.52% 50.67%</cell></row><row><cell>Elusion Elusion Elusion</cell><cell>0.01% 0.02% 0.01%</cell><cell>0.01% 0.05% 0.00%</cell><cell>0.00% 0.02% 0.00%</cell></row><row><cell>Fallout Fallout Fallout</cell><cell>0.09% 0.52% 0.01%</cell><cell>1.42% 0.45% 13.62%</cell><cell>14.34% 0.50% 50.68%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26" coords="80,72.03,73.94,312.46,603.42"><head></head><label></label><figDesc>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</figDesc><table coords="80,72.03,73.94,312.46,603.42"><row><cell cols="2">Topic 412 -2000 Recount Topic 413 -James V. Crosby</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 850 Total Relevant: 600</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.29% Total Prevalence: 0.21%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -2000 Recount Confusion Matrix -James V. Crosby</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90% @Reasonable @90%</cell><cell>@95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall Recall</cell><cell>Recall Recall</cell></row><row><cell cols="2">True Positives 747 True Positives 581</cell><cell>765 540</cell><cell>808 570</cell></row><row><cell cols="2">True Negatives 288,351 True Negatives 289,489</cell><cell>287,968 289,492</cell><cell>285,458 289,492</cell></row><row><cell cols="2">False Positives 898 False Positives 10</cell><cell>1,281 7</cell><cell>3,791 7</cell></row><row><cell>False False</cell><cell>103 19</cell><cell>85 60</cell><cell>42 30</cell></row><row><cell>Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall</cell><cell>87.88% 96.83%</cell><cell>90.00% 90.00%</cell><cell>95.06% 95.00%</cell></row><row><cell>Precision Precision</cell><cell>45.41% 98.31%</cell><cell>37.39% 98.72%</cell><cell>17.57% 98.79%</cell></row><row><cell>F1 Measure F1 Measure</cell><cell>59.88% 97.57%</cell><cell>52.83% 94.16%</cell><cell>29.66% 96.86%</cell></row><row><cell>Accuracy Accuracy</cell><cell>99.6549% 99.9900%</cell><cell>99.5291% 99.9769%</cell><cell>98.6787% 99.9872%</cell></row><row><cell>Error Error</cell><cell>0.3451% 0.0100%</cell><cell>0.4709% 0.0231%</cell><cell>1.3213% 0.0128%</cell></row><row><cell>Elusion Elusion</cell><cell>0.04% 0.01%</cell><cell>0.03% 0.02%</cell><cell>0.01% 0.01%</cell></row><row><cell>Fallout Fallout</cell><cell>0.31% 0.00%</cell><cell>0.44% 0.00%</cell><cell>1.31% 0.00%</cell></row><row><cell cols="3">Topic 412 -2000 Recount -UNCORRECTED Topic 413 -James V. Crosby -UNCORRECTED</cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 1,410 Total Relevant: 546</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.49% Total Prevalence: 0.19%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -2000 Recount Confusion Matrix -James V. Crosby</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90% @Reasonable @90%</cell><cell>@95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall Recall</cell><cell>Recall Recall</cell></row><row><cell cols="2">True Positives 809 True Positives 526</cell><cell>1,269 492</cell><cell>1,340 519</cell></row><row><cell cols="2">True Negatives 287,853 True Negatives 289,488</cell><cell>276,191 289,495</cell><cell>215,249 289,493</cell></row><row><cell cols="2">False Positives 836 False Positives 65</cell><cell>12,498 58</cell><cell>73,440 60</cell></row><row><cell>False False</cell><cell>601 20</cell><cell>141 54</cell><cell>70 27</cell></row><row><cell>Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall</cell><cell>57.38% 96.34%</cell><cell>90.00% 90.11%</cell><cell>95.04% 95.05%</cell></row><row><cell>Precision Precision</cell><cell>49.18% 89.00%</cell><cell>9.22% 89.45%</cell><cell>1.79% 89.64%</cell></row><row><cell>F1 Measure F1 Measure</cell><cell>52.96% 92.52%</cell><cell>16.72% 89.78%</cell><cell>3.52% 92.27%</cell></row><row><cell>Accuracy Accuracy</cell><cell>99.50% 99.97%</cell><cell>95.64% 99.96%</cell><cell>74.66% 99.97%</cell></row><row><cell>Error Error</cell><cell>0.50% 0.03%</cell><cell>4.36% 0.04%</cell><cell>25.34% 0.03%</cell></row><row><cell>Elusion Elusion</cell><cell>0.21% 0.01%</cell><cell>0.05% 0.02%</cell><cell>0.03% 0.01%</cell></row><row><cell>Fallout Fallout</cell><cell>0.29% 0.02%</cell><cell>4.33% 0.02%</cell><cell>25.44% 0.02%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27" coords="88,72.03,73.94,312.46,603.42"><head></head><label></label><figDesc>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</figDesc><table coords="88,72.03,73.94,312.46,603.42"><row><cell cols="2">Topic 414 -Medicaid Reform Topic 415 -George W. Bush Topic 416 -Marketing Topic 417 -Movie Gallery Topic 418 -War Preparations</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 844 Total Relevant: 12,267 Total Relevant: 1,485 Total Relevant: 5,945 Total Relevant: 141</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.29% Total Prevalence: 4.23% Total Prevalence: 0.51% Total Prevalence: 2.05% Total Prevalence: 0.05%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Medicaid Reform Confusion Matrix -George W. Bush Confusion Matrix -Marketing Confusion Matrix -Movie Gallery Confusion Matrix -War Preparations</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90% @Reasonable @90% @Reasonable @90% @Reasonable @90% @Reasonable @90%</cell><cell>@95% @95% @95% @95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall Recall Recall Recall Recall</cell><cell>Recall Recall Recall Recall Recall</cell></row><row><cell cols="2">True Positives 783 True Positives 11,554 True Positives 911 True Positives 5,945 True Positives 114</cell><cell>760 11,041 1,337 5,351 127</cell><cell>802 11,654 1,411 5,648 134</cell></row><row><cell cols="2">True Negatives 287,858 True Negatives 276,876 True Negatives 287,453 True Negatives 284,154 True Negatives 289,925</cell><cell>288,177 277,056 269,283 284,154 287,707</cell><cell>286,907 275,461 263,314 284,154 286,196</cell></row><row><cell cols="2">False Positives 1,397 False Positives 956 False Positives 1,161 False Positives 0 False Positives 33</cell><cell>1,078 776 19,331 0 2,251</cell><cell>2,348 2,371 25,300 0 3,762</cell></row><row><cell>False False False False False</cell><cell>61 713 574 0 27</cell><cell>84 1,226 148 594 14</cell><cell>42 613 74 297 7</cell></row><row><cell>Negatives Negatives Negatives Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall Recall Recall</cell><cell>92.77% 94.19% 100.00% 80.85%</cell><cell>90.05% 90.01% 90.01% 90.07%</cell><cell>95.02% 95.00% 95.00% 95.04%</cell></row><row><cell>Precision Precision Precision Precision</cell><cell>35.92% 92.36% 100.00% 77.55%</cell><cell>41.35% 93.43% 100.00% 5.34%</cell><cell>25.46% 83.09% 100.00% 3.44%</cell></row><row><cell>F1 Measure F1 Measure F1 Measure F1 Measure</cell><cell>51.79% 93.26% 100.00% 79.17%</cell><cell>56.67% 91.69% 94.74% 10.08%</cell><cell>40.16% 88.65% 97.44% 6.64%</cell></row><row><cell>Accuracy Accuracy Accuracy Accuracy</cell><cell>99.4974% 99.4247% 100.0000% 99.9793%</cell><cell>99.5994% 99.3099% 99.7952% 99.2192%</cell><cell>99.1761% 98.9714% 99.8976% 98.7008%</cell></row><row><cell>Error Error Error Error</cell><cell>0.5026% 0.5753% 0.0000% 0.0207%</cell><cell>0.4006% 0.6901% 0.2048% 0.7808%</cell><cell>0.8239% 1.0286% 0.1024% 1.2992%</cell></row><row><cell>Elusion Elusion Elusion Elusion</cell><cell>0.02% 0.26% 0.00% 0.01%</cell><cell>0.03% 0.44% 0.21% 0.00%</cell><cell>0.01% 0.22% 0.10% 0.00%</cell></row><row><cell>Fallout Fallout Fallout Fallout</cell><cell>0.48% 0.34% 0.00% 0.01%</cell><cell>0.37% 0.28% 0.00% 0.78%</cell><cell>0.81% 0.85% 0.00% 1.30%</cell></row><row><cell cols="3">Topic 414 -Medicaid Reform -UNCORRECTED Topic 415 -George W. Bush -UNCORRECTED Topic 417 -Movie Gallery -UNCORRECTED Topic 418 -War Preparations -UNCORRECTED</cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 839 Total Relevant: 12,106 Total Relevant: 5,931 Total Relevant: 187</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.29% Total Prevalence: 4.17% Total Prevalence: 2.04% Total Prevalence: 0.06%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Medicaid Reform Confusion Matrix -George W. Bush Confusion Matrix -Movie Gallery Confusion Matrix -War Preparations</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90% @Reasonable @90% @Reasonable @90% @Reasonable @90%</cell><cell>@95% @95% @95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall Recall Recall Recall</cell><cell>Recall Recall Recall Recall</cell></row><row><cell cols="2">True Positives 0 True Positives 11,389 True Positives 5,908 True Positives 74</cell><cell>756 10,896 5,338 169</cell><cell>798 11,501 5,635 178</cell></row><row><cell cols="2">True Negatives 287,115 True Negatives 276,872 True Negatives 284,131 True Negatives 289,839</cell><cell>288,111 277,056 284,146 279,562</cell><cell>286,515 275,265 284,141 271,871</cell></row><row><cell cols="2">False Positives 2,145 False Positives 1,121 False Positives 37 False Positives 73</cell><cell>1,149 937 22 10,350</cell><cell>2,745 2,728 27 18,041</cell></row><row><cell>False False False False</cell><cell>839 717 23 113</cell><cell>83 1,210 593 18</cell><cell>41 605 296 9</cell></row><row><cell>Negatives Negatives Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall Recall Recall</cell><cell>0.00% 94.08% 99.61% 39.57%</cell><cell>90.11% 90.00% 90.00% 90.37%</cell><cell>95.11% 95.00% 95.01% 95.19%</cell></row><row><cell>Precision Precision Precision Precision</cell><cell>0.00% 91.04% 99.38% 50.34%</cell><cell>39.69% 92.08% 99.59% 1.61%</cell><cell>22.52% 80.83% 99.52% 0.98%</cell></row><row><cell>F1 Measure F1 Measure F1 Measure F1 Measure</cell><cell>#DIV/0! 92.53% 99.49% 44.31%</cell><cell>55.10% 91.03% 94.55% 3.16%</cell><cell>36.42% 87.34% 97.21% 1.93%</cell></row><row><cell>Accuracy Accuracy Accuracy Accuracy</cell><cell>98.97% 99.37% 99.98% 99.94%</cell><cell>99.58% 99.26% 99.79% 96.43%</cell><cell>99.04% 98.85% 99.89% 93.78%</cell></row><row><cell>Error Error Error Error</cell><cell>1.03% 0.63% 0.02% 0.06%</cell><cell>0.42% 0.74% 0.21% 3.57%</cell><cell>0.96% 1.15% 0.11% 6.22%</cell></row><row><cell>Elusion Elusion Elusion Elusion</cell><cell>0.29% 0.26% 0.01% 0.04%</cell><cell>0.03% 0.43% 0.21% 0.01%</cell><cell>0.01% 0.22% 0.10% 0.00%</cell></row><row><cell>Fallout Fallout Fallout Fallout</cell><cell>0.74% 0.40% 0.01% 0.03%</cell><cell>0.40% 0.34% 0.01% 3.57%</cell><cell>0.95% 0.98% 0.01% 6.22%</cell></row></table><note coords="90,72.03,369.28,458.86,12.00;90,72.03,383.28,460.53,12.00;90,72.03,397.28,83.84,12.00"><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28" coords="107,72.03,73.94,322.75,603.42"><head>Confusion Matrix -Lost Foster Child Rilya Wilson @Reasonable @90% Recall</head><label></label><figDesc>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</figDesc><table coords="107,72.03,73.94,322.75,603.42"><row><cell cols="3">Topic 419 -Lost Foster Child Rilya Wilson Topic 420 -Billboards Topic 421 -Traffic Cameras Topic 422 -Non Resident Aliens</cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 1,982 Total Relevant: 739 Total Relevant: 54 Total Relevant: 48</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.68% Total Prevalence: 0.25% Total Prevalence: 0.02% Total Prevalence: 0.02%</cell><cell></cell><cell></cell></row><row><cell cols="4">Confusion Matrix -Lost Foster Child Rilya Wilson Confusion Matrix -Billboards Confusion Matrix -Traffic Cameras Confusion Matrix -Non Resident Aliens</cell></row><row><cell></cell><cell cols="2">@Reasonable @90% @Reasonable @90% @Reasonable @90% @Reasonable @90%</cell><cell>@95% @95% @95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall Recall Recall Recall</cell><cell>Recall Recall Recall Recall</cell></row><row><cell cols="2">True Positives 1,964 True Positives 707 True Positives 52 True Positives 48</cell><cell>1,784 666 49 44</cell><cell>1,883 703 52 46</cell></row><row><cell cols="2">True Negatives 277,007 True Negatives 289,327 True Negatives 289,945 True Negatives 286,883</cell><cell>285,486 289,327 290,045 289,852</cell><cell>283,977 289,327 290,045 289,828</cell></row><row><cell cols="2">False Positives 11,110 False Positives 33 False Positives 100 False Positives 3,168</cell><cell>2,631 33 0 199</cell><cell>4,140 33 0 223</cell></row><row><cell>False False False False</cell><cell>18 32 2 0</cell><cell>198 73 5 4</cell><cell>99 36 2 2</cell></row><row><cell>Negatives Negatives Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall Recall Recall</cell><cell>99.09% 95.67% 96.30% 100.00%</cell><cell>90.01% 90.12% 90.74% 91.67%</cell><cell>95.01% 95.13% 96.30% 95.83%</cell></row><row><cell>Precision Precision Precision Precision</cell><cell>15.02% 95.54% 34.21% 1.49%</cell><cell>40.41% 95.28% 100.00% 18.11%</cell><cell>31.26% 95.52% 100.00% 17.10%</cell></row><row><cell>F1 Measure F1 Measure F1 Measure F1 Measure</cell><cell>26.09% 95.61% 50.49% 2.94%</cell><cell>55.78% 92.63% 95.15% 30.24%</cell><cell>47.05% 95.32% 98.11% 29.02%</cell></row><row><cell>Accuracy Accuracy Accuracy Accuracy</cell><cell>96.1641% 99.9776% 99.9648% 98.9080%</cell><cell>99.0248% 99.9635% 99.9983% 99.9300%</cell><cell>98.5388% 99.9762% 99.9993% 99.9224%</cell></row><row><cell>Error Error Error Error</cell><cell>3.8359% 0.0224% 0.0352% 1.0920%</cell><cell>0.9752% 0.0365% 0.0017% 0.0700%</cell><cell>1.4612% 0.0238% 0.0007% 0.0776%</cell></row><row><cell>Elusion Elusion Elusion Elusion</cell><cell>0.01% 0.01% 0.00% 0.00%</cell><cell>0.07% 0.03% 0.00% 0.00%</cell><cell>0.03% 0.01% 0.00% 0.00%</cell></row><row><cell>Fallout Fallout Fallout Fallout</cell><cell>3.86% 0.01% 0.03% 1.09%</cell><cell>0.91% 0.01% 0.00% 0.07%</cell><cell>1.44% 0.01% 0.00% 0.08%</cell></row><row><cell cols="4">Topic 419 -Lost Foster Child Rilya Wilson -UNCORRECTED Topic 420 -Billboards -UNCORRECTED Topic 421 -Traffic Cameras -UNCORRECTED Topic 422 -Non Resident Aliens -UNCORRECTED</cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 1,989 Total Relevant: 737 Total Relevant: 21 Total Relevant: 31</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.69% Total Prevalence: 0.25% Total Prevalence: 0.01% Total Prevalence: 0.01%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Billboards Confusion Matrix -Traffic Cameras Confusion Matrix -Non Resident Aliens</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90% @Reasonable @90% @Reasonable @90%</cell><cell>@95% @95% @95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall Recall Recall</cell><cell>Recall Recall Recall Recall</cell></row><row><cell cols="2">True Positives 1,966 True Positives 682 True Positives 19 True Positives 29</cell><cell>1,791 664 19 28</cell><cell>1,890 701 20 30</cell></row><row><cell cols="2">True Negatives 277,002 True Negatives 289,304 True Negatives 289,945 True Negatives 286,881</cell><cell>285,321 289,304 290,047 289,814</cell><cell>283,642 289,224 281,036 286,003</cell></row><row><cell cols="2">False Positives 11,108 False Positives 58 False Positives 133 False Positives 3,187</cell><cell>2,789 58 31 254</cell><cell>4,468 138 9,042 4,065</cell></row><row><cell>False False False False</cell><cell>23 55 2 2</cell><cell>198 73 2 3</cell><cell>99 36 1 1</cell></row><row><cell>Negatives Negatives Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall Recall Recall</cell><cell>98.84% 92.54% 90.48% 93.55%</cell><cell>90.05% 90.09% 90.48% 90.32%</cell><cell>95.02% 95.12% 95.24% 96.77%</cell></row><row><cell>Precision Precision Precision Precision</cell><cell>15.04% 92.16% 12.50% 0.90%</cell><cell>39.10% 91.97% 38.00% 9.93%</cell><cell>29.73% 83.55% 0.22% 0.73%</cell></row><row><cell>F1 Measure F1 Measure F1 Measure F1 Measure</cell><cell>26.10% 92.35% 21.97% 1.79%</cell><cell>54.53% 91.02% 53.52% 17.89%</cell><cell>45.29% 88.96% 0.44% 1.45%</cell></row><row><cell>Accuracy Accuracy Accuracy Accuracy</cell><cell>96.16% 99.96% 99.95% 98.90%</cell><cell>98.97% 99.95% 99.99% 99.91%</cell><cell>98.43% 99.94% 96.88% 98.60%</cell></row><row><cell>Error Error Error Error</cell><cell>3.84% 0.04% 0.05% 1.10%</cell><cell>1.03% 0.05% 0.01% 0.09%</cell><cell>1.57% 0.06% 3.12% 1.40%</cell></row><row><cell>Elusion Elusion Elusion Elusion</cell><cell>0.01% 0.02% 0.00% 0.00%</cell><cell>0.07% 0.03% 0.00% 0.00%</cell><cell>0.03% 0.01% 0.00% 0.00%</cell></row><row><cell>Fallout Fallout Fallout Fallout</cell><cell>3.86% 0.02% 0.05% 1.10%</cell><cell>0.97% 0.02% 0.01% 0.09%</cell><cell>1.55% 0.05% 3.12% 1.40%</cell></row></table><note coords="109,72.03,368.28,458.86,12.00;109,72.03,382.28,460.24,12.00;109,72.03,396.28,83.84,12.00"><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30" coords="130,72.03,73.94,312.46,603.42"><head>Confusion Matrix -Civil Rights Act of 2003 @Reasonable @90% Recall</head><label></label><figDesc>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). This project was run by Losey from August 8 th to 11 th 2016. He spent five hours, made 22 submissions, called reasonable after 11, and created 18 search folders. He reviewed a total of 112 documents and manually categorized 141.</figDesc><table coords="130,72.03,73.94,312.46,603.42"><row><cell cols="3">Topic 425 -Civil Rights Act of 2003 Topic 426 -Jeffrey Goldhagen Summary</cell><cell></cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 718 Total Relevant: 98</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.25% Total Prevalence: 0.03%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Civil Rights Act of 2003 Confusion Matrix -Jeffrey Goldhagen</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90% @Reasonable @90%</cell><cell>@95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall Recall</cell><cell>Recall Recall</cell></row><row><cell cols="2">True Positives 653 True Positives 91</cell><cell>623 89</cell><cell>658 94</cell></row><row><cell cols="2">True Negatives 289,355 True Negatives 289,996</cell><cell>289,371 289,996</cell><cell>286,331 288,587</cell></row><row><cell cols="2">False Positives 26 False Positives 5</cell><cell>10 5</cell><cell>3,050 1,414</cell></row><row><cell>False False</cell><cell>65 7</cell><cell>95 9</cell><cell>60 4</cell></row><row><cell>Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall</cell><cell>90.95% 92.86%</cell><cell>86.77% 90.82%</cell><cell>91.64% 95.92%</cell></row><row><cell>Precision Precision</cell><cell>96.17% 94.79%</cell><cell>98.42% 94.68%</cell><cell>17.75% 6.23%</cell></row><row><cell>F1 Measure F1 Measure</cell><cell>93.49% 93.81%</cell><cell>92.23% 92.71%</cell><cell>29.73% 11.71%</cell></row><row><cell>Accuracy Accuracy</cell><cell>99.9686% 99.9959%</cell><cell>99.9638% 99.9952%</cell><cell>98.9280% 99.5112%</cell></row><row><cell>Error Error</cell><cell>0.0314% 0.0041%</cell><cell>0.0362% 0.0048%</cell><cell>1.0720% 0.4888%</cell></row><row><cell>Elusion Elusion</cell><cell>0.02% 0.00%</cell><cell>0.03% 0.00%</cell><cell>0.02% 0.00%</cell></row><row><cell>Fallout Fallout</cell><cell>0.01% 0.00%</cell><cell>0.00% 0.00%</cell><cell>1.05% 0.49%</cell></row><row><cell cols="4">Topic 425 -Civil Rights Act of 2003 -UNCORRECTED Topic 426 -Jeffrey Goldhagen -UNCORRECTED</cell></row><row><cell cols="2">Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Relevant: 714 Total Relevant: 120</cell><cell></cell><cell></cell></row><row><cell cols="2">Total Prevalence: 0.25% Total Prevalence: 0.04%</cell><cell></cell><cell></cell></row><row><cell cols="3">Confusion Matrix -Jeffrey Goldhagen</cell><cell></cell></row><row><cell></cell><cell cols="2">@Reasonable @90%</cell><cell>@95% @95%</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>Recall Recall</cell></row><row><cell cols="2">True Positives 652 True Positives 84</cell><cell>643 108</cell><cell>679 114</cell></row><row><cell cols="2">True Negatives 289,362 True Negatives 289,967</cell><cell>289,365 289,613</cell><cell>286,345 287,627</cell></row><row><cell cols="2">False Positives 23 False Positives 12</cell><cell>20 366</cell><cell>3,040 2,352</cell></row><row><cell>False False</cell><cell>62 36</cell><cell>71 12</cell><cell>35 6</cell></row><row><cell>Negatives Negatives</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall</cell><cell>91.32% 70.00%</cell><cell>90.06% 90.00%</cell><cell>95.10% 95.00%</cell></row><row><cell>Precision Precision</cell><cell>96.59% 87.50%</cell><cell>96.98% 22.78%</cell><cell>18.26% 4.62%</cell></row><row><cell>F1 Measure F1 Measure</cell><cell>93.88% 77.78%</cell><cell>93.39% 36.36%</cell><cell>30.63% 8.82%</cell></row><row><cell>Accuracy Accuracy</cell><cell>99.97% 99.98%</cell><cell>99.97% 99.87%</cell><cell>98.94% 99.19%</cell></row><row><cell>Error Error</cell><cell>0.03% 0.02%</cell><cell>0.03% 0.13%</cell><cell>1.06% 0.81%</cell></row><row><cell>Elusion Elusion</cell><cell>0.02% 0.01%</cell><cell>0.02% 0.00%</cell><cell>0.01% 0.00%</cell></row><row><cell>Fallout Fallout</cell><cell>0.01% 0.00%</cell><cell>0.01% 0.13%</cell><cell>1.05% 0.81%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31" coords="138,72.03,73.94,312.46,603.42"><head>Confusion Matrix -New Stadiums and Arenas @Reasonable @90% Recall</head><label></label><figDesc>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</figDesc><table coords="138,72.03,73.94,312.46,603.42"><row><cell cols="5">Topic 427 -Slot Machines Topic 428 -New Stadiums and Arenas Topic 431 -Agency Credit Ratings Topic 432 -Gay Adoption Topic 433 -Abstinence</cell><cell></cell><cell></cell></row><row><cell cols="3">Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Total Relevant: 263 Total Relevant: 476 Total Relevant: 149 Total Relevant: 137 Total Relevant: 141</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Total Prevalence: 0.09% Total Prevalence: 0.16% Total Prevalence: 0.05% Total Prevalence: 0.05% Total Prevalence: 0.05%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Confusion Matrix -Slot Machines Confusion Matrix -New Stadiums and Arenas Confusion Matrix -Agency Credit Ratings Confusion Matrix -Gay Adoption Confusion Matrix -Abstinence</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">@Reasonable @90% @Reasonable @90% @Reasonable @90% @Reasonable @90% @Reasonable @90%</cell><cell cols="2">@95% @95% @95% @95% @95%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Recall Recall Recall Recall Recall</cell><cell cols="2">Recall Recall Recall Recall Recall</cell></row><row><cell cols="3">True Positives 249 True Positives 447 True Positives 120 True Positives 125 True Positives 141</cell><cell cols="2">237 429 135 124 127</cell><cell cols="2">250 453 142 131 134</cell></row><row><cell cols="3">True Negatives 289,484 True Negatives 287,645 True Negatives 289,841 True Negatives 289,949 True Negatives 289,931</cell><cell cols="2">289,727 288,628 289,268 289,949 289,950</cell><cell cols="2">289,351 280,685 289,109 267,375 289,950</cell></row><row><cell cols="3">False Positives 352 False Positives 1,978 False Positives 109 False Positives 13 False Positives 27</cell><cell cols="2">109 995 682 13 8</cell><cell cols="2">485 8,938 841 22,587 8</cell></row><row><cell>False False False False False</cell><cell>14 29 29 12</cell><cell>0</cell><cell>26 47 14 13</cell><cell>14</cell><cell>13 23 7 6</cell><cell>7</cell></row><row><cell>Negatives Negatives Negatives Negatives Negatives</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall Recall Recall Recall</cell><cell cols="2">94.68% 93.91% 80.54% 91.24% 100.00%</cell><cell cols="2">90.11% 90.13% 90.60% 90.51% 90.07%</cell><cell cols="2">95.06% 95.17% 95.30% 95.62% 95.04%</cell></row><row><cell>Precision Precision Precision Precision Precision</cell><cell cols="2">41.43% 18.43% 52.40% 90.58% 83.93%</cell><cell cols="2">68.50% 30.13% 16.52% 90.51% 94.07%</cell><cell cols="2">34.01% 4.82% 14.45% 0.58% 94.37%</cell></row><row><cell>F1 Measure F1 Measure F1 Measure F1 Measure F1 Measure</cell><cell cols="2">57.64% 30.82% 63.49% 90.91% 91.26%</cell><cell cols="2">77.83% 45.16% 27.95% 90.51% 92.03%</cell><cell cols="2">50.10% 9.18% 25.09% 1.15% 94.70%</cell></row><row><cell>Accuracy Accuracy Accuracy Accuracy Accuracy</cell><cell cols="2">99.8738% 99.3082% 99.9524% 99.9914% 99.9907%</cell><cell cols="2">99.9535% 99.6408% 99.7601% 99.9910% 99.9924%</cell><cell cols="2">99.8283% 96.9111% 99.7077% 92.2120% 99.9948%</cell></row><row><cell>Error Error Error Error Error</cell><cell cols="2">0.1262% 0.6918% 0.0476% 0.0086% 0.0093%</cell><cell cols="2">0.0465% 0.3592% 0.2399% 0.0090% 0.0076%</cell><cell cols="2">0.1717% 3.0889% 0.2923% 7.7880% 0.0052%</cell></row><row><cell>Elusion Elusion Elusion Elusion Elusion</cell><cell cols="2">0.00% 0.01% 0.01% 0.00% 0.00%</cell><cell cols="2">0.01% 0.02% 0.00% 0.00% 0.00%</cell><cell cols="2">0.00% 0.01% 0.00% 0.00% 0.00%</cell></row><row><cell>Fallout Fallout Fallout Fallout Fallout</cell><cell cols="2">0.12% 0.68% 0.04% 0.00% 0.01%</cell><cell cols="2">0.04% 0.34% 0.24% 0.00% 0.00%</cell><cell cols="2">0.17% 3.09% 0.29% 7.79% 0.00%</cell></row><row><cell cols="7">Topic 427 -Slot Machines -UNCORRECTED Topic 428 -New Stadiums and Arenas -UNCORRECTED Topic 431 -Agency Credit Ratings -UNCORRECTED Topic 432 -Gay Adoption -UNCORRECTED Topic 433 -Abstinence -UNCORRECTED</cell></row><row><cell cols="3">Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099 Total Documents: 290,099</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Total Relevant: 241 Total Relevant: 464 Total Relevant: 144 Total Relevant: 140 Total Relevant: 112</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Total Prevalence: 0.08% Total Prevalence: 0.16% Total Prevalence: 0.05% Total Prevalence: 0.05% Total Prevalence: 0.04%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Confusion Matrix -Slot Machines Confusion Matrix -Agency Credit Ratings Confusion Matrix -Gay Adoption Confusion Matrix -Abstinence</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">@Reasonable @90% @Reasonable @90% @Reasonable @90% @Reasonable @90%</cell><cell cols="2">@95% @95% @95% @95% @95%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Recall Recall Recall Recall</cell><cell cols="2">Recall Recall Recall Recall Recall</cell></row><row><cell cols="3">True Positives 215 True Positives 432 True Positives 109 True Positives 119 True Positives 111</cell><cell cols="2">217 418 130 126 101</cell><cell cols="2">229 441 137 133 107</cell></row><row><cell cols="3">True Negatives 289,472 True Negatives 287,642 True Negatives 289,835 True Negatives 289,940 True Negatives 289,930</cell><cell cols="2">289,178 288,549 289,242 279,621 289,957</cell><cell cols="2">275,153 280,554 277,498 245,846 289,956</cell></row><row><cell cols="3">False Positives 386 False Positives 1,993 False Positives 120 False Positives 19 False Positives 57</cell><cell cols="2">680 1,086 713 10,338 30</cell><cell cols="2">14,705 9,081 12,457 44,113 31</cell></row><row><cell>False False False False False</cell><cell>26 32 35 21</cell><cell>1</cell><cell>24 46 14 14</cell><cell>11</cell><cell>12 23 7 7</cell><cell>5</cell></row><row><cell>Negatives Negatives Negatives Negatives Negatives</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall Recall Recall Recall Recall</cell><cell cols="2">89.21% 93.10% 75.69% 85.00% 99.11%</cell><cell cols="2">90.04% 90.09% 90.28% 90.00% 90.18%</cell><cell cols="2">95.02% 95.04% 95.14% 95.00% 95.54%</cell></row><row><cell>Precision Precision Precision Precision Precision</cell><cell cols="2">35.77% 17.81% 47.60% 86.23% 66.07%</cell><cell cols="2">24.19% 27.79% 15.42% 1.20% 77.10%</cell><cell cols="2">1.53% 4.63% 1.09% 0.30% 77.54%</cell></row><row><cell>F1 Measure F1 Measure F1 Measure F1 Measure F1 Measure</cell><cell cols="2">51.07% 29.91% 58.45% 85.61% 79.29%</cell><cell cols="2">38.14% 42.48% 26.34% 2.38% 83.13%</cell><cell cols="2">3.02% 8.83% 2.15% 0.60% 85.60%</cell></row><row><cell>Accuracy Accuracy Accuracy Accuracy Accuracy</cell><cell cols="2">99.86% 99.30% 99.95% 99.99% 99.98%</cell><cell cols="2">99.76% 99.61% 99.75% 96.43% 99.99%</cell><cell cols="2">94.93% 96.86% 95.70% 84.79% 99.99%</cell></row><row><cell>Error Error Error Error Error</cell><cell cols="2">0.14% 0.70% 0.05% 0.01% 0.02%</cell><cell cols="2">0.24% 0.39% 0.25% 3.57% 0.01%</cell><cell cols="2">5.07% 3.14% 4.30% 15.21% 0.01%</cell></row><row><cell>Elusion Elusion Elusion Elusion Elusion</cell><cell cols="2">0.01% 0.01% 0.01% 0.01% 0.00%</cell><cell cols="2">0.01% 0.02% 0.00% 0.01% 0.00%</cell><cell cols="2">0.00% 0.01% 0.00% 0.00% 0.00%</cell></row><row><cell>Fallout Fallout Fallout Fallout Fallout</cell><cell cols="2">0.13% 0.69% 0.04% 0.01% 0.02%</cell><cell cols="2">0.23% 0.37% 0.25% 3.57% 0.01%</cell><cell cols="2">5.07% 3.14% 4.30% 15.21% 0.01%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="57,72.03,313.26,460.43,12.00;57,72.03,327.51,466.46,12.00;57,72.03,341.51,440.03,12.00;57,72.03,355.53,445.63,12.00;57,72.03,369.78,456.08,12.00;57,72.03,383.78,459.11,12.00;57,72.03,397.78,447.06,12.00;57,72.03,412.03,431.27,12.00;57,72.03,426.03,103.34,12.00;57,72.03,454.06,443.90,12.00;57,72.03,468.31,460.33,12.00;57,72.03,482.31,454.16,12.00;57,72.03,496.31,148.29,12.00"><p>Multimodal was used, with some keyword search up front, but there was special emphasis placed in this topic on the use of AI features and document ranking searches. This was done intentionally as an experiment and to make the review easier in this relatively difficult topic. Review of the top ranked documents was the primary search used. The AI ranked document review was improved by going lower on the keyword hit folders, where hidden gems of relevance were found low at lower than expected ranks. AI ranking searches were not only used as QC of other searches, but also to speed up the review and make it more efficient. The next-doc search and keyword list functions were also used this topic to maximize efficiency.The usual high number of TREC errors were seen on this topic, including many obvious mistakes, and inconsistencies. Below is an example, just to give an idea on the inconsistent coding. The first inquiry email was called irrelevant by TREC and the second reply email by Bush was called relevant.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multimodal hybrid model of training EDR.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This project was conducted by Tony Reichenberger. The full description of the topic is: Space-All documents concerning the space industry, the space program, space travel (whether manned or unmanned, public or private), and the study or exploration of space in Florida.</p><p>The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-50 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring, with the size of the submissions increasing (up to 100 documents); when additional relevant materials were found, subsequent searches for similar documents were partaken. When scores dropped to 5%, a final search for "space" was submitted another learning session run, and documents were submitted in probability order.</p><p>The reasonable call was made when following a learning session all remaining documents had scores less than 12.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Space topic, the 90% recall threshold had been attained by submitting only 1.08%% of the corpus, 3,125 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>The project was run by Tony Reichenberger. The full description of this topic is: Eminent Domain-All documents concerning the legality or morality of expropriating land in Florida for commercial development.</p><p>The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-50 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring, with the size of the submissions increasing (up to 100 documents); when additional relevant materials were found, subsequent searches for similar documents were partaken.</p><p>The reasonable call was made when following a learning session after all keyword hits had been exhausted.</p><p>With this topic, the assessors seemed to treat any land acquisition (or even suggestion of it) by the state as "eminent domain," even if it did not apply. For instance, a situation where the state actively sought a private purchaser of an amusement park (Cypress Gardens) was found to be relevant even though this is not eminent domain. Likewise, a situation where people protested the state turning an airstrip in the Everglades previously belonging to Homestead Air Force Base into a commercial airport is not eminent domain related. As such, this was an issue that the standard (particularly for lawyers who know the issue) was inherently flawed, and therefore was not really representative of comparisons between human-only or hybrid reviewers and machine learning auto-runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Eminent Domain topic, the 90% recall threshold had been attained by submitting only 0.90%% of the corpus, 2,602 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This project was run by Losey from August 20 th to 23 rd 2016. He expended at least seven hours on the project, probably longer (his record on his time here is uncertain). He reviewed 209 documents and categorized 232. He made a total of 17 submissions and called reasonable after the 9 th submission.</p><p>The full description of the topic is: Felon Disenfranchisement-All documents concerning the right of felons to vote in Florida, including but not limited to voter purges and reinstatement of voter rights. Individual clemency cases in Florida are not relevant.</p><p>The rules in play here on relevance were hard to follow, including the clemency exclusion. That, and the presence of many borderline, ambiguous documents, made this a relatively difficult search. Several hours of unreported time, in addition to the seven recorded, were expended in post submission analysis of TREC's return documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Levi Kuehn. The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-50 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring, with the size of the submissions increasing (up to 100 documents); when additional relevant materials were found, subsequent searches for similar documents were partaken. When scores dropped to 5%, a final search was submitted, another learning session run, and documents were submitted in probability order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Climate Change topic, the 90% recall threshold had been attained by submitting only 0.28%% of the corpus, 823 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Tony Reichenberger. The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (50-100 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring, with the size of the submissions increasing; when additional relevant materials were found, subsequent searches for similar documents were partaken.</p><p>Reasonable was called when keywords were exhausted and the precision within the submission dropped to less than 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Condominiums topic, the 90% recall threshold had been attained by submitting only 0.82%% of the corpus, 2,385 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic as run by Losey who worked on it from August 14 th to August 16 th 2016 for five hours. He reviewed 274 document and manually categorized 198.</p><p>The full description of h topic is: Stand Your Ground -All documents concerning a Florida bill permitting the use of deadly force to protect one's self or one's property.</p><p>Of course most everyone in Florida with half a brain knows all about this controversial law. Losey did not find this a difficult assignment, especially because the scope of relevance was clear and so were the documents. As an experiment Losey called reasonable with his first submission. Before the submission Losey created 28 search folders. His review was entirely based on keyword search, and similarity type searches. Most of his five-hour time was spent doing these searches.</p><p>Losey then used TREC as a QC check to see if he had missed anything. Unfortunately the judging by TREC on this topic was poor. TREC found 58 additional documents, but they were all False Positives, iw -not relevant. Trec also missed 29 docs in my first submission of all relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Stand Your Ground topic, the 90% recall threshold had been attained by submitting only 0.02%% of the corpus, 67 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This project was run by Tony Reichenberger. The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, within a date filter, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-50 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted within the initial date range, the filter was opened up and then finally, documents were submitted based solely on probability scoring, with the size of the submissions increasing (up to 100 documents); when additional relevant materials were found, subsequent searches for similar documents were partaken. The fourth submission size was in error, far in excess of what was intended to be submitted; however, other submission sizes were as appropriate given their scoring and expectation.</p><p>Reasonable was called when all keywords were exhausted, there was no longer a date filter being applied and scores on documents remaining dropped to 10%.</p><p>Common errors found in the TREC standard focused on issues for subsequent elections (2002-2008) that had similar problems as in 2000 (e.g. voter disenfranchisement, long lines at polling stations, etc.), but specifically referenced other elections (including a circuit court election, congressional elections, primaries for down ballot races, etc.). Without a reference to the 2000 election in these instances, they should be irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the 2000 Recount topic, the 90% recall threshold had been attained by submitting only 0.71%% of the corpus, 2,046 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Topic 413 was run by Jim Sullivan, who started on August 12, 2016 and concluded on the same day.</p><p>Sullivan entered this topic with no prior knowledge of James V. Crosby. At first he thought it was a legal case, with James as the Plaintiff and Crosby as the Defendant. That was not accurate.</p><p>Sullivan started by testing terms and creating a keyword highlight list, as was done on all topics reviewed. He started by submitting documents that hit on variations of crosby subject line, and moved broader variations of the name anywhere in the document. He called 70% recall after submitting 422 documents, with 397 relevant. Almost all of the 25 false positives were obvious errors in the TREC standard. 500 random documents were trained Not Relevant and a learning session was initiated.</p><p>Sullivan continued with variations of keyword terms until he called Reasonable after 591 documents submitted, with 526 being returned Relevant. Most of the 65 documents returned Not Relevant were again clear errors.</p><p>He submitted all remaining documents that contained the term Crosby, followed by the rest with the highest scores being submitted first. A total of 546 documents were returned relevant by TREC. In total, 3.0 hours were spent reviewing this very easy topic. The use of predictive coding on this topic was unnecessary.</p><p>This topic had an average TREC standard. Though he identified 56 documents that were clearly erroneous, overall the standard was clear and the inconsistencies weren't widespread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the James V. Crosby topic, the 90% recall threshold had been attained by submitting only 0.19%% of the corpus, 547 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Tony Reichenberger. The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-50 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring, with the size of the submissions increasing (up to 100 documents); when additional relevant materials were found, subsequent searches for similar documents were partaken.</p><p>Reasonable was called when all scores dropped below 7.5% probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Medicaid Reform topic, the 90% recall threshold had been attained by submitting only 0.63%% of the corpus, 1,838 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Topic 415 was run by Jim Sullivan, who started on August 22, 2016 and concluded on August 29, 2016.</p><p>Sullivan entered this topic with general knowledge of George W. Bush. Like most people, he is familiar with the former President of the United States, but he didn't have any special knowledge.</p><p>Sullivan started by testing terms and creating a keyword highlight list, as was done on all topics reviewed. This topic was especially tricky due to "Bush" appearing in every document in the database. He started by submitting documents that hit on obvious terms in the subject line, and moved broader variations anywhere in the document. By the end of the first day, he was comfortable that he had found most of the relevant material. He was way off. He called 70% recall after submitting 1,233 documents, with 1,207 returned Relevant. He disagreed with most returned Not Relevant, but the mistakes seemed reasonable given such a high prevalence.</p><p>On day two, he started submitting large batches of search term hits and found a very significant volume of new hits. He had previously missed a large collection of documents with nothing more than a reference to the "President." He trained 2,000 randomly selected documents as Not Relevant, and initiated a learning session. From there he decided to rely much more heavily on the predictive coding scores as to not miss another significant set of documents.</p><p>Relying on a combination of keywords and predictive coding scores, a large set of additional relevant documents were discovered. Reasonable recall wasn't called until 12,510 documents were submitted, with 11,389 being returned as Relevant.</p><p>To finish up, he submitted all remaining documents with the highest scores being submitted first. A total of 12,106 documents were returned relevant by TREC. In total, 3.5 hours were spent reviewing this high prevalence topic. This topic had an above average TREC standard. Though he identified 169 documents that were clearly erroneous, overall the standard was clear and the inconsistencies weren't widespread. He was impressed how TREC properly returned vague references to George W. Bush without any relevant keywords present. The small number of errors is very reasonable for a topic with such high overall prevalence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the George W. Bush topic, the 90% recall threshold had been attained by submitting only 4.07%% of the corpus, 11,817 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Topic 416 was run by Jim Sullivan, who started on July 27, 2016 and concluded on August 26.</p><p>Sullivan entered this topic blind to what could be meant by Marketing in Florida. He was far from being an expert by any standard.</p><p>Sullivan started by testing terms and creating a keyword highlight list, as was done on all topics reviewed. He started by submitting documents that hit on obvious terms, and moved to more generic lists. While he entered the topic blind, things only got more difficult once he began reviewing TREC's feedback on his initial submissions. Finding documents relating to "visit florida" or "marketing" were only returned relevant 1/3 of the time, and for seemingly indistinguishable reasons.</p><p>Though frustrated and confused by the TREC standard, 80% recall was called after 373 documents were submitted, with 130 relevant. He was only able to achieve 34.9% precision on his own. At this point he just started blindly submitting the highest scoring documents based on predictive coding, and got better results than he did by looking at anything. He continued iterations of submissions and learning sessions until calling reasonable after 2,072 submitted, with 872 relevant. Mr. EDR was able to get 43.7% precision without any input.</p><p>After the reasonable call, all remaining documents were submitted by predictive coding score with the highest scores being submitted first. A total of 1,446 documents were returned relevant by TREC. In total, 7.0 hours were spent reviewing this topic.</p><p>This topic was the poorest gold standard Sullivan faced of all his TREC topics. Though he could only identify 39 documents that were clearly erroneous, most of the errors were related to inconsistencies, where similar documents were classified differently. In the end, he was rarely able to understand what was supposed to be relevant well enough to determine what was a mistake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Marketing topic, the 90% recall threshold had been attained by submitting only 7.12%% of the corpus, 20,668 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Losey from July 6 th to 7 th 2016. He took a total of five hours on this fairly simple project with most of the time doing keyword searches. He created 27 search fodlers, reviewed only 66 documents, but manually catergorized 5,966 documents (bulk coding).</p><p>The topic is defined as: Movie Gallery-All documents concerning investments or divestments by the State of Florida in Movie Gallery.</p><p>The Movie Gallery is a publically traded pornography company in which the great State of Florida decided to invest some of its employee pension funds. When this was eventually discovered by the public, and then a form email campaign was launched by citizens and employees both.</p><p>The work began in an unusual fashion. Losey did keyword search and then submitted all 5,932 documents that have the keyword phrase "movie gallery" in them. He only did a 15 minute judgmental sample review of this folder to see they all were relevant. They seemed to all be pretty much the same form email. So, as an experiment, he decided to just submit them all at once. They were in fact all relevant. There were 5,945 Relevant documents on this issue out of the total of 290,099 (after correcting for the 58 obvious errors in coding made by the TREC assessor). By use of one keyword search "movie gallery" Losey found 5,932 of them. That is 99.78% RECALL, 100% Precision from one search. By use of a second series of keyword searches Losey found 7 more relevant documents, for a total of 5,939. That is 99.90% RECALL. 100% Precision. By use of Mr. EDR -AI based ranking -he found 6 more relevant documents, for a total of 5,945, and called REASONABLE. That is 100% RECALL and 100% Precision.</p><p>This topic was fairly easy, but did have some subtleties, including the selection of the right balance of irrelevant training docs and having the confidence to call reasonable early. The confidence was provided by Mr. EDR. Just before his perfect call, Losey looked all the way down to 3%, and only 8 new documents were seen, none even close to relevant). The document ranking served as an excellent quality assurance tool and made it easier to make the right Stop call.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Movie Gallery topic, the 90% recall threshold had been attained by submitting only 1.84%% of the corpus, 5,351 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This project was run by Tony Reichenberger. The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-20 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring; when additional relevant materials were found, subsequent searches for similar documents were partaken.</p><p>Reasonable was called too early on this topic, as precision and quality of documents preceding the call steeply diminished. Subsequent submissions post-call were confined to a date filter to enhance precision which resulted in additional relevant materials not previously considered being found. As additional relevant documents were found, additional searches and learning sessions were conducted as follow ups, with those documents being included in the next submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the War Preparations topic, the 90% recall threshold had been attained by submitting only 0.82%% of the corpus, 2,378 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Levi Kuehn. The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-50 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring, with the size of the submissions increasing (up to 100 documents); when additional relevant materials were found, subsequent searches for similar documents were partaken. When scores dropped to 5%, a final search was submitted, another learning session run, and documents were submitted in probability order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Lost Foster Child Rilya Wilson topic, the 90% recall threshold had been attained by submitting only 1.52%% of the corpus, 4,415 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Topic 420 was run by Jim Sullivan, who started on August 22, 2016 and concluded on August 25, 2016.</p><p>Sullivan entered this topic with little knowledge of billboard and their legal status in Florida. While he certainly has driven by his share of billboards on the highway, that's as far as his prior knowledge extends.</p><p>Sullivan started by testing terms and creating a keyword highlight list, as was done on all topics reviewed. He started by submitting documents that hit on obvious terms in the subject line, and moved broader variations anywhere in the document. By the end of the first day he had a very good understanding of what was relevant to the TREC standard for the topic. He called 70% recall after submitting 557 documents, with 516 returned Relevant.</p><p>On day two, the final search results were submitted and 80% recall was called after 628 submitted, with 573 returned relevant. He trained 1,000 randomly selected documents as Not Relevant and initiated a learning session.</p><p>On the final day, he submitted the highest scoring documents, and quickly called Reasonable after 740 submitted. 682 were returned relevant. He submitted all remaining documents with the highest scores being submitted first. A total of 737 documents were returned relevant by TREC. In total, 4.0 hours were spent reviewing this topic. This topic had an above average TREC standard. Though he identified 48 documents that were clearly erroneous, overall the standard was clear and the inconsistencies weren't widespread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Billboards topic, the 90% recall threshold had been attained by submitting only 0.24%% of the corpus, 699 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Topic 421 was run by Jim Sullivan, who started on August 20, 2016 and concluded on the same day.</p><p>Sullivan entered this topic with basic knowledge of traffic cameras and a solid understanding of related keywords. This knowledge was acquired by completing the traffic cameras topic in TREC 2015. This experience proved very helpful.</p><p>Sullivan started by testing terms and creating a keyword highlight list, as was done on all topics reviewed. He started by submitting documents that hit on obvious terms in the subject line, and moved broader variations anywhere in the document. He quickly realized the low prevalence rate of this topic and called 70% recall after submitting 43 documents, with 17 relevant. He disagreed with TREC the classification on the remaining 26.</p><p>Sullivan continued with variations of keyword terms and high predictive coding scores to find a couple more Relevant documents until he called Reasonable after 152 documents submitted, with 19 being returned Relevant.</p><p>He submitted all remaining documents with the highest scores being submitted first. 2 more relevant documents were returned, in which he did not disagree. A total of 21 documents were returned relevant by TREC. In total, 2.0 hours were spent reviewing this very easy topic. The use of predictive coding on this topic was unnecessary.</p><p>This topic had an average TREC standard. Though he identified 33 documents that were clearly erroneous, overall the standard was clear and the inconsistencies weren't widespread. Almost all errors were in situations where TREC had improperly classified a document as Not Relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Traffic Cameras topic, the 90% recall threshold had been attained by submitting only 0.02%% of the corpus, 49 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This project was run by Tony Reichenberger. Documents were submitted on this topic sparingly, based only on keywords initially. Feedback from TREC on the most documents relating to the topic came back as not relevant. Very few documents were being suggested by the machine learning as relevant, and those that were submitted were returned as not relevant. On the 10th submission, all remaining documents hitting on search terms were submitted (accidentally; it was only meant to be a subset of the remaining, but it was not realized until after the feedback from TREC that the whole set was submitted) and only 7 returned as relevant. With such low precision, reasonable was called.</p><p>The TREC judgments here were poor, missing many obviously relevant documents. The accessors did not seem to understand the topic, despite the fact that the definition of relevance here was fairly clear: Non-Resident Aliens (NRA) -All documents involving discussions of the non-resident alien issue. Documents concerning the National Rifle Association are not relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Non Resident Aliens topic, the 90% recall threshold had been attained by submitting only 0.08%% of the corpus, 243 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This project was run by Tony Reichenberger. It is the "other NRA" topic specifically defined as: National Rifle Association (NRA) -All documents concerning the National Rifle Association, its members, and its influences. Documents concerning the non-resident alien issue are not relevant.</p><p>The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-20 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring; when additional relevant materials were found, subsequent searches for similar documents were partaken.</p><p>An inconsistent standard resulted in poor and conflicting results. Documents containing the exact same text were often found with contradictory coding, and likewise there were scores of missed relevant documents and documents coded relevant for little or no reason. The result was confusion based on TREC feedback for both the human reviewer and the machine learning.</p><p>With the conflicting issues, Reasonable was called too early on this topic, as questions of what was irrelevant misled the human assessor. Submissions post-call of similar materials and keyword hits resulted in relevant materials that altered the Reasonable assessment. As additional relevant documents were found, additional searches and learning sessions were conducted as follow ups, with those documents being included in subsequent submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the National Rifle Association topic, the 90% recall threshold had been attained by submitting only 0.35%% of the corpus, 1,008 documents for adjudication. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Levi Kuehn. The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-50 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring, with the size of the submissions increasing (up to 100 documents); when additional relevant materials were found, subsequent searches for similar documents were partaken. When scores dropped to 5%, a final search was submitted, another learning session run, and documents were submitted in probability order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Gulf Drilling topic, the 90% recall threshold had been attained by submitting only 0.29%% of the corpus, 841 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Losey who put a substantial eight-hour effort into this search from June 15 th to 22 nd 2016. He reviewed 291 documents, created 35 different search folder and manually categorized 739 documents.</p><p>The topic was further defined as: Civil Rights Act of 2003 -All documents involving discussions of the Florida Civil Rights Act of 2003.</p><p>Losey began with a Google search to obtain detailed facts for the search beyond the obvious. He learned, among other things, that the legislation was called the "Dr. Marvin Davies Florida Civil Rights Act" and was signed into law by Governor Bush on June 18, 2003. Marvin Davies was a Florida civil rights leader who died cancer April 25, 2003.He also read the final law, and noted from its legislative history the various numbers associated with bill during the legislative process. The law supplemented to the original Florida Civil Rights Act of 1992. There was not much civil rights legislation during the Bush years so the relevant emails stuck out easily.</p><p>This was, fortunately, a topic with a well-judged TREC standard, one that required some legal acumen to do properly.</p><p>Losey would have cored even higher on this topic but for the fact he accidentally did not submit a set of documents he had identified as probable relevant until after the reasonable call. This is no doubt derived from rushing ad not using our usual quality controls. Such a mistake would not be possible under normal legal search conditions, or if the mistake was made, could be easily cured by a supplemental production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Civil Rights Act of 2003 topic, the 90% recall threshold had been attained by submitting only 0.22%% of the corpus, 633 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Topic 427 was run by Jim Sullivan, who started on July 21, 2016 and concluded on August 12, with four short days of review in that time period.</p><p>Sullivan has a long history with slot machines, both on the winning side and losing side. While he is no bona fide subject matter expert on the topic, he knows his way around the one-armed bandit.</p><p>Sullivan started by testing terms and creating a keyword highlight list, as was done on all topics reviewed. He started by submitting documents that hit on obvious terms in the subject line, and moved to more generic terms in broader fields. At the end of the first day he had submitted 204 documents, with 165 relevant. To end the day, he kicked off a learning session after training 500 randomly selected documents as Not Responsive.</p><p>Day two was quick and consisted of submitting the last few docs that hit on "slot machin*" in the document or "slots*" in the subject line. Called 70% recall after 258 docs submitted with 172 relevant and called it a day. Day three was just as short, where the last docs that hit on "slots*" anywhere in the text were submitted.</p><p>80% recall was called early on day four, and escalated reliance was placed on the predictive coding scores. Once the predictive coding scores stopped yielding valuable results, Reasonable recall was called. After the reasonable call, all remaining documents were submitted by predictive coding score with the highest scores being submitted first. A total of 241 documents were returned relevant by TREC. In total, 4.25 hours were spent reviewing this topic.</p><p>This topic was graded fairly and had a below average number of inconsistencies. There were only 46 documents where TREC had returned inconsistent or incorrect classifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Slot Machines topic, the 90% recall threshold had been attained by submitting only 0.12%% of the corpus, 346 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Levi Kuehn. The hybrid multimodal review was conducted by initially submitting keyword hits to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-50 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring, with the size of the submissions increasing (up to 100 documents); when additional relevant materials were found, subsequent searches for similar documents were partaken. When scores dropped to 5%, a final search was submitted, another learning session run, and documents were submitted in probability order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the New Stadiums and Arenas topic, the 90% recall threshold had been attained by submitting only 0.49%% of the corpus, 1,424 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc). Being his first attempted topic on the year, he spent more time understanding the dataset than was necessary on later topics.</p><p>While Sullivan had heard the name Elian Gonzalez in the past, he had not read any of the news about him prior to this exercise.</p><p>Sullivan started by testing terms and creating a keyword highlight list, as was done on all topics reviewed. He started by submitting documents that hit on obvious terms, and moved to more generic lists. At the end of the first day he had submitted 409 documents, with 404 relevant. At this point, he predicted 700 total relevant documents and kicked off a learning session after training 500 randomly selected documents as Not Responsive.</p><p>The second day of review was spent combining predictive coding scores with date searches. This was one of the few topics that had a very relevant time period. High scoring documents within the date range were submitted. He called 80% recall after 731 total documents submitted, with 699 relevant.</p><p>Day three was spent digging through any remaining search terms and high scoring documents. Exhausting all options, he called reasonable after finding 779 relevant documents. After the reasonable call, all remaining documents were submitted by predictive coding score with the highest scores being submitted first. A total of 827 documents were returned relevant by TREC. In total, 6.25 hours were spent reviewing this topic.</p><p>This topic was graded fairly and had a below average number of inconsistencies. There were only 63 documents where TREC had returned inconsistent or incorrect classifications. He was especially impressed by TREC's ability to identify misspellings of Elian and documents within the date range that referenced the event without any meaningful keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Elian Gonzalez topic, the 90% recall threshold had been attained by submitting only 0.27%% of the corpus, 775 documents for adjudication. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Jani Grantz. This was her first attempted topic.</p><p>She began the process by running keywords that seemed logical to the topic and set up highlighting with those words. She split this topic up into its two parts 1) restraints and 2) helmets. Then she did some informal Doc Review on the docs that hit on multiple terms/most important terms for responsiveness. She started with small submissions of documents that she marked responsive for Restraints and found that almost every doc that hit on a term was Relevant, so this topic seemed easy and complete quickly.</p><p>However for the Helmets topic she did the same thing but found little rhyme or reason to docs that were relevant versus not relevant. She tried people outside of Florida as not relevant. Some were not relevant, but some were, She tried generic form responses to be Relevant at first, but they returned as both evenly. After that I gave up on trying to determine which docs were relevant and ran learning sessions and just submitted by probability since she had nothing else to go on. She started with the highest probability from ones that hit on some terms and went from there. She called reasonable when she got below a certain threshold percent where no more docs seemed like they should be relevant.</p><p>There is not much work placed into determining a corrected gold standard for this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Restraints and Helmets topic, the 90% recall threshold had been attained by submitting only 2.98%% of the corpus, 8,641 documents for adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Tony Reichenberger. The hybrid multimodal review was conducted by initially submitting keyword hits (initially just the ratings agencies and various bond ratings) to train the machine learning, then letting the system suggest documents at various thresholds. Keyword hits were submitted in descending probability score order followed by learning sessions for the system, with submission sizes kept relatively small (10-20 documents each). Periodically, documents not hitting on keywords with high scores were submitted to ensure inclusiveness. Once all keyword hit documents were submitted, documents were submitted based solely on probability scoring; when additional relevant materials were found, subsequent searches for similar documents were partaken.</p><p>Reasonable was called when scores on keywords remaining were less than 25% and scores on all documents were less than 75%. Samples of keywords remaining at the time hit on only bond ratings but in a different context (AAA, B-, etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Agency Credit Ratings topic, the 90% recall threshold had been attained by submitting only 0.28%% of the corpus, 817 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>This topic was run by Jani Grantz. This was her second attempted topic. She began the process by running keywords that seemed logical to the topic and set up highlighting with those words. Then she did some informal Doc Review on the docs that hit on multiple terms/most important terms for responsiveness. She started with one moderately sized submission based on docs she found relevant. Then from the results that came back relevant she used Find Similar to find others that should be relevant. She did that to find additional keywords and relevant docs and then did a couple more submissions until she felt like she was out of clearly relevant docs. Then she ran learning sessions and submitted a few more that had a high percentage of likelihood to be relevant. When she felt like I exhausted those (reached a certain percentage) she called reasonable and submitted the rest.</p><p>There is not much work placed into determining a corrected gold standard for this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Gay Adoption topic, the 90% recall threshold had been attained by submitting only 0.05%% of the corpus, 137 documents for adjudication.</p><p>The next chart below represents the amount of effort (documents actually reviewed eyes on) versus how many were submitted to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Topic 433 was run by Jim Sullivan, who started on June 14, 2016 and concluded on June 16, with two days of review.</p><p>Sullivan is not an expert in abstinence, neither in practice nor in theory.</p><p>Sullivan started by testing terms and creating a keyword highlight list for term hits and common variations, as was done on all topics reviewed. He started by submitting documents that hit on obvious terms in the subject line, and moved to more generic keywords in broader fields. At the end of the first day he had submitted 67 documents, with 57 relevant. He disagreed with the TREC categorization on the remaining 10. He initiated a learning session after training 500 randomly selected documents as Not Responsive.</p><p>The second day of review was spent submitting documents with the highest predictive coding scores. He called 80% and reasonable recall after 168 total documents submitted, with 111 relevant. In total, 112 documents were returned Relevant by the TREC standard.</p><p>After the reasonable call, all remaining documents were submitted by predictive coding score with the highest scores being submitted first. Only 3.5 hours were spent reviewing this topic, considering 111 of the 112 TREC relevant documents hit on the term "abstinence," with only 40 documents in the entire database containing abstinence being returned as Not Relevant, and most of those being errors in the TREC standard.</p><p>This topic was graded poorly for such an easy topic. While there were only 31 documents where TREC had returned inconsistent or incorrect classifications, the scope of documents containing the word abstinence was so small the high error rate was surprising. There were 2 documents that contained a misspelling of abstinence that were clearly missed (TRECID 285286 and 285292), and one document not containing the term abstinence marked Relevant by TREC for no apparent reason (TRECID 267623).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs</head><p>The following chart shows Precision (blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Abstinence topic, the 90% recall threshold had been attained by submitting only 0.05%% of the corpus, 135 documents for adjudication.</p><p>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="148,94.01,73.94,437.05,12.00;148,72.03,87.94,460.20,12.00;148,72.03,102.19,83.84,12.00;152,72.03,73.94,458.90,12.00;152,72.03,87.94,460.20,12.00;152,72.03,102.19,83.84,12.00" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="148,94.01,73.94,437.05,12.00;148,72.03,87.94,460.20,12.00;148,72.03,102.19,79.45,12.00;152,72.03,73.94,458.90,12.00;152,72.03,87.94,405.82,12.00">last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents, 3x Recall documents, etc)</title>
		<imprint/>
	</monogr>
	<note>The last chart shows the progression through the database submissions based on attained recall at various recall points throughout the database (2x # of recall documents. 3x Recall documents</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
