<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.56,72.35,426.60,16.84;1,79.48,92.27,450.76,16.84">Real, Live, and Concise: Answering Open-Domain Questions with Word Embedding and Summarization</title>
				<funder>
					<orgName type="full">Qatar Foundation</orgName>
				</funder>
				<funder ref="#_3Q2HAck">
					<orgName type="full">Qatar National Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,114.84,137.97,70.43,11.06"><forename type="first">Rana</forename><surname>Malhas</surname></persName>
							<email>rana.malhas@qu.edu.qa</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Qatar University</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.06,137.97,70.97,11.06"><forename type="first">Marwan</forename><surname>Torki</surname></persName>
							<email>mtorki@qu.edu.qa</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Qatar University</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.83,137.97,55.14,11.06"><forename type="first">Rahma</forename><surname>Ali</surname></persName>
							<email>rahma.ali@qu.edu.qa</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Qatar University</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.76,137.97,57.82,11.06"><forename type="first">Evi</forename><surname>Yulianti</surname></persName>
							<email>evi.yulianti@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Qatar University</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.56,72.35,426.60,16.84;1,79.48,92.27,450.76,16.84">Real, Live, and Concise: Answering Open-Domain Questions with Word Embedding and Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">374B1AB1B9186074AC936D89CDD4F7A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LiveQA</term>
					<term>real-time question answering</term>
					<term>learning to rank</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Resorting to community question answering (CQA) websites for finding answers has gained momentum in the recent years with the explosive rate at which social media has been proliferating. With many questions left unanswered on those websites, automatic question answering (QA) systems have seen light. A main objective of those systems is to harness the plethora of existing answered questions; hence transforming the problem to finding good answers to newlyposed questions from similar previously-answered ones or composing a new concise one from those potential answers. In this paper, we describe the real-time Question Answering system we have developed to participate in TREC 2016 LiveQA track. Our QA system is composed of three phases: answer retrieval from three different Web sources (Yahoo! Answers, Google Search, and Bing Search), answer ranking using learning to rank models, and summarization of top ranked answers. Official track results of our three submitted runs show that our runs significantly outperformed the average scores of all participated runs across the entire spectrum of official evaluation measures deployed by the track organizers this year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The ubiquitous presence of community question answering (CQA) websites has motivated research in the direction of building automatic question answering (QA) systems that can benefit from previously-answered questions to answer newly-posed ones <ref type="bibr" coords="1,127.38,565.70,9.21,7.86" target="#b8">[8]</ref>. A core functionality of such systems is their ability to retrieve and effectively rank previouslysuggested answers with respect to their degree/probability of relevance to a posted question. The ranking functionality is vital to push away irrelevant and low quality answers, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.</p><p>TREC '16 Gaithersburg, Maryland USA which is commonplace in CQA as they are generally open with no restrictions on who can post or answer questions.</p><p>TREC 2016 LiveQA track is in succession for the second year in a row. It focused on "live" question answering for real-user questions. Real user questions, sampled from the stream of most recent questions submitted on the Yahoo! Answers site that have not been answered by humans, are sent live to the participating systems. The questions are sampled from a list of seven categories that include Arts &amp; Humanities, Beauty &amp; Style, Health, Home &amp; Graden, Pets, Sports and Travel. Each system should provide an answer in "real time", not exceeding 1 minute in response; and the length of each answer should not exceed 1000 characters.</p><p>The LiveQA track organizers have provided a training dataset of questions and answers previously judged on a 4point scale, ranging from poor to excellent, by the judges of TREC 2015 LiveQA track. The questions in this dataset are those sent to the QA systems of participating teams in TREC 2015 LiveQA track; and the answers are those returned by the participating teams. Henceforward, we shall refer to this dataset as the QRELS dataset.</p><p>In this paper, we describe the question answering system we have developed to participate in TREC 2016 LiveQA Track. Our QA system is composed of three phases: answer retrieval, answer ranking, and summarization.</p><p>1. The answer retrieval phase retrieves answers from three different sources: an archive of Yahoo! Answers (YA) recent questions and their answers, Bing search, and Google search.</p><p>2. The answer ranking phase adopts a supervised learning approach. We train a learning-to-rank (L2R) model over labeled QRELs data acquired from the previous TREC 2015 LiveQA track. We use the learned model to rank candidate answers retrieved from all sources. The features we use is a mix of traditional and nontraditional features such as language model/query likelihood and word embedding features generated from the GloVe (Global Vectors for Word Representation)<ref type="foot" coords="1,533.36,617.23,3.65,5.24" target="#foot_0">1</ref> pretrained model, respectively. A thorough description of each feature is presented in section 2.2.1.</p><p>3. Finally, we apply an optional summarization module on the top three ranked answers to presumably formulate a best and concise answer. For summarization, we followed <ref type="bibr" coords="1,376.31,692.21,9.72,7.86" target="#b2">[2]</ref> by using the model proposed by Takamura</p><p>and Okumura <ref type="bibr" coords="2,139.82,57.64,14.32,7.86" target="#b11">[11]</ref> to generate extractive summaries from the top-ranked passages.</p><p>Given an incoming question, the retrieval module retrieves top candidate answers from one or all the above mentioned three sources. The ranking module in turn ranks those candidate answers; and then the summarization module is optionally applied to summarize the top ranked answers to return a best answer, otherwise the system returns the top ranked answer as the best answer. We have used MAP (Mean Average Precision) as the evaluation measure in our pre-submission experiments to evaluate the effectiveness of our QA system. The rest of the paper is organized as follows: the approach and system design are introduced in section 2; the experimental evaluation and setup followed by our submissions to the LiveQA Track; finally, we conclude with final remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">APPROACH AND SYSTEM DESIGN</head><p>Our question answering system was designed with high modularity; it is composed of three modules: an answers retrieval module, an answers ranking module and an optional summarization module; they are described in sections 2.1, 2.2, and 2.3, respectively. The training pipeline of the system is illustrated in Figure <ref type="figure" coords="2,207.25,306.95,4.61,7.86" target="#fig_0">1</ref> and an overview of the system is depicted in Figure <ref type="figure" coords="2,186.03,317.41,3.58,7.86" target="#fig_1">2</ref>.</p><p>The retrieval module has leveraged previously-indexed Yahoo! Answer questions and answers, as well as Web resources such as Bing and Google. The ranking module tackled the answer ranking task with a supervised learning approach that leveraged learning-to-rank models. The features used in training those models were a mix of traditional and nontraditional features as described in section 2.2.1. Details regarding the retrieval methods, specific models and features used in our three submissions to TREC 2016 LiveQA Track are presented in section 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Answer Retrieval</head><p>The answer retrieval module is composed of three components (a component for each source); the system can work with any combination of these components; this makes our system highly extensible should we need to incorporate more answer sources. Although the components are currently pipelined, we are considering to parallelize them in future versions of our system to optimize the response time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Yahoo! Answers Search</head><p>The Lucene indexer is responsible for indexing questions and their answers from two datasets in offline mode. The first dataset <ref type="bibr" coords="2,107.58,665.41,34.44,7.86">(12 GB)</ref>   The second dataset (50 GB) was also crawled from Yahoo! Answers; it covers the years 2013 through 2016. Only questions that had a best answer were indexed along with all their other answers. The resulting index contains 16.9 million previously answered questions with a total size of 23.3 GB.</p><p>Given an incoming question, the Lucene index searcher (with its default retrieval model) is responsible for retrieving the best answers of the top 10 matching question titles; i.e., question-question similarity was adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Bing Search</head><p>This component is responsible for submitting an incoming question as a query using the Bing search API methods and retrieving the top 5 search results. Each search result constitutes a title, link, text and primary snippet. Each search result link is accessed and the HTML of the webpage returned is parsed to locate the primary snippet. Three cases are considered depending on the location of the primary snippet. If it is found in the body tag of the HTML webpage, the 1000 characters appearing right after the primary snippet are saved as a secondary snippet and returned as a candidate answer. Otherwise, if the primary snippet is from the meta-data description of the webpage, the first 1000 characters from the body tag of the HTML webpage are saved as a secondary snippet and returned as a candidate answer. Finally, if the primary snippet is not in the webpage, the "&lt;Snippet not found in HTML&gt;" string is saved as the secondary snippet and the primary snippet is returned as a candidate answer instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Google Search</head><p>This component is responsible for submitting an incoming question as a query to Google.com. It then parses the results page returned to retrieve the top 5 search results. Each search result constitutes a title, link, text and primary snippet. Each search result link is accessed and the HTML of the webpage returned is parsed to locate the primary snippet. The same three cases described for the Bing component above are adopted for returning candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer Ranking</head><p>The answer ranking module is composed of two components; a feature generation component, and a ranking component that comprises the ranking models and answers scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Feature Generation</head><p>We have opted to utilize covariance word embedding features and four of the Metzler-Kanungo (MK) features <ref type="bibr" coords="3,280.64,145.13,9.20,7.86">[6]</ref>; the MK features are computed at the answer level and not at the sentence level as computed in the original paper. The word embedding representation is computed offline following Mikolov et al approach <ref type="bibr" coords="3,230.65,292.26,9.20,7.86" target="#b7">[7]</ref>.</p><p>For a document that has kn words, we compute the covariance matrix C ∈ R Dim×Dim . The covariance matrix C is computed by treating each dimension as a random variable and every entry in Cn u,v is the covariance between the pair of variables (u, v). The covariance between two random variables u and v is computed as in eq. 1, where kn is the number of observations (words).</p><formula xml:id="formula_0" coords="3,123.40,387.12,169.51,22.47">Cn u,v = kn i=1 (ui -ū)(vi -v) kn -1<label>(1)</label></formula><p>The matrix Cn ∈ R Dim×Dim is a symmetric matrix. We compute a vectorized representation of the matrix Cn as the stacking of the lower triangular part of matrix Cn as in eq. 2. This process produces a vector</p><formula xml:id="formula_1" coords="3,76.21,458.98,216.69,37.85">vn ∈ R Dim×(Dim+1)/2 vn = Cn u,v : u ∈ {1, • • • , Dim} , v ∈ {u, • • • , Dim}<label>(2)</label></formula><p>• Metzler-Kanungo Features We have implemented four out of the six features used by Metzler and Kanungo <ref type="bibr" coords="3,102.78,530.15,9.20,7.86">[6]</ref>, and applied them at the answer level instead of the sentence level:</p><p>-AnswerLength: Number of terms in an answer, after stop-words are removed.</p><p>-ExactMatch: Equals 1 if there is an exact lexical match of the question string occurring in the answer, and 0 otherwise.</p><p>-TermOverlap: Fraction of question terms that occur in the answer after stop word removal and stemming.</p><p>-LanguageModelScore: Language model score is computed as the log likelihood of the question terms being generated from the answer. The answer language model is smoothed using Dirichlet smoothing <ref type="bibr" coords="3,140.03,700.73,13.49,7.86" target="#b12">[12]</ref>. It can also be referred to as query likelihood language model score <ref type="bibr" coords="3,226.40,711.19,9.20,7.86" target="#b4">[4]</ref>. This score is computed as in eq. 3.</p><formula xml:id="formula_2" coords="3,365.60,74.13,182.47,24.76">fLM (Q, A) = w∈Q tf w,Q log tf w,A + µP (w|C) |A| + µ</formula><p>(3) where tf w,Q is the number of times the word w occurs in the query (question), tf w,A is the number of times that w occurs in the answer, |A| is the length of the answers, P (w|C) is the background language model and µ is a parameter for Dirichlet smoothing. We tend to set µ as 1000; i.e towards the lower end of the range recommended by TREC which is 1000-2000 because answers are shorter in length than documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Ranking Models</head><p>A learning-to-rank (L2Rank) setup was adopted that is similar to <ref type="bibr" coords="3,359.34,240.17,9.71,7.86" target="#b3">[3,</ref><ref type="bibr" coords="3,372.76,240.17,7.16,7.86" target="#b5">5,</ref><ref type="bibr" coords="3,383.63,240.17,6.48,7.86" target="#b9">9]</ref>. The L2Rank models were trained over the labeled QRELS dataset provided by the LiveQA track organizers. The data constituted 1,087 questions and their potentially related answers (18 answers per question on average) making a total of 20,469 labeled answers. Each answer is labeled by either being poor, fair, good or excellent with respect to its question. The labels were the outcome of the judging process on a 4-point scale by the judges of TREC 2015 LiveQA Track. The questions in this dataset are those sent to the QA systems of participating teams in TREC 2015 LiveQA Track; and the answers are those returned by the 2015 participating teams.</p><p>The main algorithm used for training our learning-to-rank models that were deployed in our three submissions to the LiveQA track was the MART (Multiple Additive Regression Trees, a.k.a. Gradient boosted regression tree) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Answers Scoring</head><p>Upon receiving the computed features for an incoming question and its retrieved candidate answers, the trained L2Rank model is utilized to score these answers before returning them sorted in descending order using their attained scores to our QA server (system). If summarization is to be applied, the top three scored answers are forwarded to the summarization module; otherwise, only the top scored answer is returned by the QA server as the best answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Summarization</head><p>Upon receiving the top three scored answers for the given question, we used the model proposed by <ref type="bibr" coords="3,491.83,545.64,14.31,7.86" target="#b11">[11]</ref> to generate an extractive summary. The model solves the maximum coverage problem where the coverage of important terms is maximized while the redundancy is minimized. The model is transformed into a linear programming problem. It has been shown in <ref type="bibr" coords="3,356.34,597.94,9.71,7.86" target="#b2">[2]</ref> that adding more statements to the summary was better than returning only one statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL EVALUATION</head><p>In this section we present the experimental setup of our three submitted runs to the TREC 2016 LiveQA Track, and report their official results. All three submissions achieved above average scores across the spectrum of official performance evaluation measures described in section 3.3. We have used the labeled QRELS dataset of 1087 questions and their potentially related 20,469 answers provided by the LiveQA track organizers in training our models. For our three submissions, the MART (Multiple Additive Regression Trees, a.k.a. Gradient boosted regression tree) algorithm was the main algorithm used in training the L2Rank models. The models differed in the features used in their training. We evaluated those models using 5-fold cross validation over the MAP (Mean Average Precision) metric as the objective function. Among the most prominent preprocessing steps conducted on the QRELS dataset was mapping the 4-point scale labels ranging from 1-4 to the range 0-3. This mapping was necessary to get a more reliable evaluation of the L2Rank models. RankLib<ref type="foot" coords="4,208.57,411.96,3.65,5.24" target="#foot_1">3</ref> was used to create and evaluate our learning-to-rank models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>The pre-trained GloVe<ref type="foot" coords="4,153.84,432.88,3.65,5.24" target="#foot_2">4</ref> (Global Vectors for Word Representation) word vectors of 50 and 100 dimensions were used in generating the covariance word embedding feature as described in section 2.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-Submission Experiments</head><p>In our preliminary experiments to train and evaluate the L2Rank models, we have adopted an incremental strategy in the features used. We started with the 4 Metzler-Kanungo (MK) features in isolation before adding the word embedding features; the covariance word embedding (CovWE) features of 50 dimensions were added first, making a total of 1279 features; then they were replaced by the CovWE features of 100 dimensions, thus making a total of 5054 features. The MART algorithm was used in training all the models. Surprisingly, the model trained over the MK feature set outperformed the other two models trained over all features, including the covariance word embedding of 50 dimensions in one model, and 100 dimensions in the other model (Table <ref type="table" coords="4,282.17,626.34,3.58,7.86" target="#tab_1">1</ref>). This finding prompted us to conduct post-submission experiments that provided enough evidence to affirm that feature concatenation might not be the best way to combine the CovWE and MK features; and different approaches might be more effective in combining them.</p><p>An observation worth noting is that the covariance word embedding features CovWE-50 and CovWE-100 with the MK feature set have attained comparable MAP scores of 0.3182 and 0.3260 in Table <ref type="table" coords="4,435.50,89.02,3.58,7.86" target="#tab_1">1</ref>, respectively. As such, we decided to use the simpler model of CovWE-50 in one of our submissions (QU2) instead of using CovWE-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Evaluation Measures</head><p>For the TREC 2016 LiveQA Track, each question with the corresponding pool of answers was examined and evaluated by TREC editors. The answers were judged and scored using a 4-level scale (with 1 signifying 'Bad and non-useful' to 4 signifying an 'Excellent answer') based on their relevance and responsiveness to the question. The main metrics include: average answer score, precision (fraction of answered questions with a score above a threshold), and coverage (fraction of all questions answered) <ref type="foot" coords="4,475.10,224.56,3.65,5.24" target="#foot_3">5</ref> . The MAP (Mean Average Precision) metric was the main evaluation measure we used in our pre-submission experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Submissions to the TREC 2016 LiveQA Track</head><p>Every participating team was allowed three submissions to the LiveQA track. Each of our submissions was a variant version of our QA system/server as explained below.</p><p>Our preliminary feature extraction experiments revealed that the L2Rank models trained over the MK feature set outperformed all the other models trained so far (Table <ref type="table" coords="4,545.18,341.70,3.58,7.86" target="#tab_1">1</ref>). For this reason this feature set was used in all submissions, either in isolation or with word embedding features. Also the MART algorithm was used in training all the models. The main differences among submissions lie in the set of features used in training the L2Rank models, the sources used for answer retrieval, the size of the Lucene index used, and whether summarization was applied or not. We shall refer to the three submissions as QU, QU2, and QU3, respectively. QU submission. All components of the answer retrieval module were utilized; namely the Yahoo! Answer index, Bing search and Google search. But only the index of the 12 GB dataset crawled from Yahoo! Answers by Yahoo! Research was used. The size of this index amounted to 8.2 GB comprising 4.4 million questions and their answers. As for the answer ranking module, the 4 MK features and MART algorithm were used in training the L2Rank model. Summarization was not applied. QU2 submission. Again, all components of the answer retrieval module were utilized; namely the Yahoo! Answer index, Bing search and Google search. Here, the full sized Lucene index of 23.3 GB that comprised 16.9 million questions was used. As for the answer ranking module, all features including the MK and covariance word embedding (of 50 dimensions) features along with the MART algorithm were used in training the L2Rank model. Summarization was applied. For some cases when the summarizer failed to return an answer, the first ranked answer was returned using the trained learning to rank model.  It is worth noting that the MK feature values in the QU and QU3 submissions were linearly normalized; i.e each feature was normalized by its min/max values.</p><p>Table <ref type="table" coords="5,89.00,293.27,4.61,7.86" target="#tab_2">2</ref> shows that the best performing run among our submissions, across all evaluation measures, was the QU3 submission with an average score of 0.9005, followed by QU2 (0.8768) and QU (0.7842) submissions.</p><p>Figure <ref type="figure" coords="5,92.77,335.11,4.61,7.86" target="#fig_4">3</ref> exhibits examples of best answers returned by the system (from three different sources) to three LiveQA track questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Discussion</head><p>Contemplating over the results of our submissions led us to the following observations.</p><p>• Our best performing submission (QU3) is quite similar to the TREC 2015 LiveQA submission by Bagdouri and Oard <ref type="bibr" coords="5,117.43,435.81,9.72,7.86" target="#b1">[1]</ref> in that they both used a Yahoo! Answer index as the sole source in answer retrieval. However, our QU3 submission achieved an average score of 0.900 as compared to 0.615 by Bagdouri and Oard <ref type="bibr" coords="5,252.70,467.20,9.71,7.86" target="#b1">[1]</ref> despite the fact that we only used the crawl of the last four years and not the full crawl of Bagdouri and Oard <ref type="bibr" coords="5,280.64,488.12,9.20,7.86" target="#b1">[1]</ref>.</p><p>The difference in performance could be attributed to two reasons; our system used learning to rank over (MK) features while <ref type="bibr" coords="5,159.06,519.50,9.71,7.86" target="#b1">[1]</ref> did not. Moreover, we indexed questions and their answers only if a best answer was prevalent to the question.</p><p>• Our QU3 submission this year marked a significant improvement over our TREC 2015 LiveQA submission last year <ref type="bibr" coords="5,111.84,578.73,14.31,7.86" target="#b10">[10]</ref> which was also based on a Yahoo! Answer index as well. The main differences lie in the index size used (only 250K documents were used last year) and the deployment of an answer ranking module this year.</p><p>• Our expectations towards the performance of the QU submission was high because the answer retrieval module leveraged the three sources (Yahoo! Answer index, Bing and Google search), but with a smaller index size to increase the chances of getting best answers from the web. However, the performance of the QU submission was the least among our submissions which could be attributed to the time-out incidences encountered while searching the web.  • Likewise, our expectations towards the performance of the QU2 submission was even higher than QU and QU3 because it used the MK and word embedding features in addition to leveraging the three sources for answer retrieval. Moreover, the summarization module was applied as well. We suspect that the time-out issues encountered during web search to be the main reason for attaining a relatively lower average score of 0.877 as compared to QU3 (0.900).</p><p>• The number of answered questions in QU2 submission is less than the number of answered questions in QU3. We projected the score of QU2 on the number of answered questions by QU3. We found that score of QU2 would be 0.9333 if we answered 1007 questions like in QU3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This paper describes the question answering system we developed to participate in TREC 2016 LiveQA Track. Our system was developed with high modularity to make it easily extensible. It is composed of three modules: an answers retrieval module, an answers ranking module and an optional summarization module. The answer retrieval module constitutes three components to retrieve candidate answers from a Yahoo! Answer index, Bing search and Google search. For the answer ranking module we have adopted a supervised learning approach where learning-to-rank models were trained over four of the Metzler-Kanungo (MK) features and covariance word embedding features generated from the pre-trained GloVe (Global Vectors for Word Representation) model. The labeled QRELS dataset provided by the LiveQA track organizers was used in training the L2Rank models. We extracted summaries from top ranked answers using our learning to rank model. Three submissions were made to the LiveQA track; each constituting a variant version of our question answering system. The main differences among submissions lie in the set of features used in training the L2Rank models, the sources used for answer retrieval, the size of the Yahoo! Answer index used, and whether summarization was applied or not. Our three submissions achieved above average scores of all submitted runs to the LiveQA track, and across all the official evaluation measures used by TREC. The best performing run among our submissions was the one using the Yahoo! Answer index of 16.9M YA questions for answer retrieval (without Bing and Google), a L2Rank model trained over MK features for answer ranking, and without applying summarization. The next to best run (among our submissions) used all three sources (Yahoo! Answer index, Bing and Google) for answer retrieval, a L2Rank model trained over covariance word embeddings (of 50 dimensions) and MK features for answer ranking, in addition to applying summarization on the top three ranked answers. The least performing run among our submissions also used the three sources for answer retrieval, but it used the L2Rank model trained over MK features only; and without summarization.</p><p>More experiments are needed on our QA system to evaluate variant configurations of the three modules. Our participation in the LiveQA track was an eye opener to the evolving and promising potential of automatic and smart question answering systems that can harness the plethora of previously answered questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,109.91,510.60,126.90,7.89;2,53.80,439.53,241.01,56.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training Pipeline</figDesc><graphic coords="2,53.80,439.53,241.01,56.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,372.75,247.40,127.09,7.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,67.12,187.63,225.79,7.89;3,76.21,198.11,216.69,7.86;3,76.21,208.58,216.69,7.86;3,76.21,219.04,216.69,7.86;3,76.21,229.50,216.70,7.86;3,76.21,239.96,90.28,8.37;3,166.50,238.19,15.36,5.24;3,182.36,239.96,110.55,7.86;3,76.21,250.42,216.70,7.86;3,76.21,260.88,216.69,8.35;3,76.21,271.34,216.69,7.86"><head>•</head><label></label><figDesc>Covariance Word Embedding To compute this feature, we define a document d as the concatenation of an incoming question and a candidate answer. Every document dn ∈ D, where n ∈ { 1, • • • , N }, has a set of words. Each word has a fixed-length word embedding representation, w ∈ R Dim , where Dim is the dimensionality of the word embedding. Thus for every document dn in the set D, we define dn = {w1, • • • , w kn }, where kn is the number of words in the document dn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,316.81,676.41,239.11,7.89;4,316.81,686.90,239.11,7.86"><head></head><label></label><figDesc>QU3 submission. The answer retrieval module only utilized the full sized Yahoo! Answer index (23.3 GB compris-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,316.81,383.66,239.11,7.89;5,316.81,394.12,239.11,7.89;5,316.81,404.59,33.50,7.89"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of best answers returned by the QA system to three LiveQA questions from variant sources</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.80,60.65,511.31,658.21"><head>Bing Search Google Search aggregated answers Answer Retrieval YA! Search Ranking Feature Generation Summarization pre-trained GloVE model Answer Ranking ranked answers incoming question Q returned answer A features candidate answers candidate answers A A A A A answer ranking model</head><label></label><figDesc></figDesc><table coords="2,53.80,665.41,239.12,53.45"><row><cell>was originally crawled from Yahoo!</cell></row><row><cell>Answers during July 2005 till December 2006 and released</cell></row><row><cell>online 2 by Yahoo! Research for research purposes; it is com-</cell></row><row><cell>posed of about 4.4 million questions and all their answers.</cell></row><row><cell>2 http://webscope.sandbox.yahoo.com/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,63.80,239.11,186.87"><head>Table 1 :</head><label>1</label><figDesc>Pre-submission experiments comparing MAP scores of models trained using the MK feature set, and Covariance word Embedding (CovWE) features over a 5-fold cross validation data setup; suffix numbers 50 and 100 designate the dimensionality of the vectors representing the word embedding. Best MAP score is boldfaced.</figDesc><table coords="4,71.50,135.38,203.71,115.29"><row><cell></cell><cell></cell><cell>MAP</cell></row><row><cell>Feature Set</cell><cell>Features</cell><cell>with</cell></row><row><cell></cell><cell></cell><cell>5-Fold CV</cell></row><row><cell></cell><cell>ExactMatch</cell><cell></cell></row><row><cell>MK</cell><cell>AnswerLength TermOverlap</cell><cell>0.4705</cell></row><row><cell></cell><cell>LanguageModel</cell><cell></cell></row><row><cell>MK &amp; Word Embedding</cell><cell>MK + CovWE-50 MK + CovWE-100</cell><cell>0.3180 0.3260</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,53.80,63.80,245.84,195.49"><head>Table 2 :</head><label>2</label><figDesc>Official results of our run submissions to LiveQA track compared to the average of all submitted runs. Best scores are boldfaced</figDesc><table coords="5,53.80,93.17,245.84,166.12"><row><cell>Evaluation Measure</cell><cell>QU Run</cell><cell>QU2 Run</cell><cell>QU3 Run</cell><cell>Track Average</cell></row><row><cell>avg score (0-3)</cell><cell>0.7842</cell><cell>0.8768</cell><cell>0.9005</cell><cell>0.5766</cell></row><row><cell>succ@2+</cell><cell cols="3">0.4236 0.4670 0.4631</cell><cell>0.3042</cell></row><row><cell>succ@3+</cell><cell>0.2532</cell><cell>0.2956</cell><cell>0.2975</cell><cell>0.1898</cell></row><row><cell>succ@4+</cell><cell cols="2">0.1074 0.1143</cell><cell>0.1399</cell><cell>0.0856</cell></row><row><cell>prec@2+</cell><cell cols="3">0.4419 0.5011 0.4667</cell><cell>0.3919</cell></row><row><cell>prec@3+</cell><cell cols="3">0.2641 0.3171 0.2999</cell><cell>0.2429</cell></row><row><cell>prec@4+</cell><cell cols="2">0.1120 0.1226</cell><cell>0.1410</cell><cell>0.1080</cell></row><row><cell>Questions Answered</cell><cell>973</cell><cell>946</cell><cell>1007</cell><cell>771.0385</cell></row><row><cell cols="5">ing 16.9M questions). But the web search components (Bing</cell></row><row><cell cols="5">search and Google search) were not used. As for the answer</cell></row><row><cell cols="5">ranking module, the MK feature set and MART algorithm</cell></row><row><cell cols="5">were used in training the L2Rank model. Summarization</cell></row><row><cell>was not applied.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,321.31,60.19,241.49,105.66"><head>Question: Foot pain unable to walk? Best answer:</head><label></label><figDesc>X-rays and other imaging tests (CT scans, MRIs) may be ordered if there are not obvious signs of an underlying cause, or if you are unable to put weight on your foot. Physical examinations may also reveal where the problem is located in the foot. Treatment Options for Your Foot Pain There are numerous things you can do to control or treat your foot pain. The first step is seeing a foot specialist, a podiatrist, or a foot and ankle surgeon. You can sometimes help yourself by using the acronym RICE to relieve foot pain. RICE stands for Rest, Ice, Compression, and Elevation.</figDesc><table coords="5,321.31,146.27,225.95,19.58"><row><cell>Source: Bing...http://www.footvitals.com/problems/foot-pain.html. Run: QU</cell></row><row><cell>Question: Do</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,321.31,158.59,241.48,79.13"><head>you need an exotic animal permit to own a peacock? Best answer:</head><label></label><figDesc>You need a licence to keep some animals, eg: wild cats primates wild dogs, eg wolves certain pigs, eg wild boar marsupials See the full list of animals you need a licence for. Hybrid or cross-bred animals may need a licence, depending on how far removed the animal is from its wild ancestor. If you're not sure, ask your local council. Last updated: 12 November 2014 …</figDesc><table coords="5,321.31,217.97,152.15,19.75"><row><cell>Source: Summarized from mixed sources. Run: QU2</cell></row><row><cell>Question: Which</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,321.31,230.46,241.56,137.39"><head>is an example of the absolutism under peter the great? Best answer: fighting</head><label></label><figDesc>the great northern war required unprecedented economic resources, and russia's yawning budget deficit and aging infrastructure meant that the state could not effectively allocate resources and money in wartime. peter's government was constantly in dire need of money, and at first it responded by monopolizing certain strategic industries, such as salt, alcohol, oak, and tar. peter also taxed many russian cultural customs (such as bathing, fishing, beekeeping, and wearing beards) and issued tax stamps for paper goods. however, with each new tax came new loopholes and new ways to avoid them, and so it became clear that tax reform was simply not enough. **the solution was a sweeping new poll tax, which replaced a household tax on cultivated land. previously, peasants had skirted the tax by combining several households into one estate; now, however, each peasant was assessed individually for a tax of 70 kopeks, paid in cash.</figDesc><table /><note coords="5,321.31,360.59,118.19,7.26"><p>Source: Yahoo! Answer Index. Run: QU3</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,321.42,713.05,144.42,5.81"><p>http://nlp.stanford.edu/projects/glove/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,58.40,703.02,166.64,5.81"><p>https://sourceforge.net/p/lemur/wiki/RankLib/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,58.40,713.05,144.42,5.81"><p>http://nlp.stanford.edu/projects/glove/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,321.42,705.08,222.19,5.81;4,316.81,713.05,111.09,5.81"><p>https://yahooresearch.tumblr.com/post/138611916336/call-forparticipation-trec-2016-liveqa</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">ACKNOWLEDGMENTS</head><p>This work was made possible by NPRP grant# <rs type="grantNumber">NPRP 6-1377-1-257</rs> from the <rs type="funder">Qatar National Research Fund</rs> (a member of <rs type="funder">Qatar Foundation</rs>). The statements made herein are solely the responsibility of the authors.</p><p>We thank <rs type="person">Mossaab Bagdouri</rs> for providing the crawled database of Yahoo! Answers questions and their corresponding available answers.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3Q2HAck">
					<idno type="grant-number">NPRP 6-1377-1-257</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,58.28,553.68,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,573.20,212.24,7.86;6,72.59,583.66,184.77,7.86;6,72.59,594.13,204.06,7.86;6,72.59,604.59,74.69,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,198.15,573.20,86.68,7.86;6,72.59,583.66,89.94,7.86">CLIP at TREC 2015: Microblog and LiveQA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bagdouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,181.90,583.66,75.46,7.86;6,72.59,594.13,167.82,7.86">Proceedings of The Twenty-Fourth Text REtrieval Conference</title>
		<meeting>The Twenty-Fourth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,616.04,184.37,7.86;6,72.59,626.50,185.96,7.86;6,72.59,636.96,205.60,7.86;6,72.59,647.43,200.43,7.86;6,72.59,657.89,103.11,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,122.06,636.96,140.79,7.86">RMIT at the trec 2015 liveqa track</title>
		<author>
			<persName coords=""><forename type="first">R.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T</forename><surname>Damessie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mourad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yulianti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,72.59,647.43,200.43,7.86;6,72.59,657.89,42.85,7.86">Proceedings of The Twenty-Fourth Text REtrieval Conference</title>
		<meeting>The Twenty-Fourth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,669.34,220.32,7.86;6,72.59,679.80,213.02,7.86;6,72.59,690.26,207.43,7.86;6,72.59,700.73,195.35,7.86;6,72.59,711.19,143.40,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,118.76,679.80,166.85,7.86;6,72.59,690.26,32.04,7.86">Harnessing semantics for answer sentence retrieval</title>
		<author>
			<persName coords=""><forename type="first">R.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,122.75,690.26,157.27,7.86;6,72.59,700.73,195.35,7.86;6,72.59,711.19,34.46,7.86">Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval</title>
		<meeting>the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,57.64,204.00,7.86;6,335.61,68.10,220.08,7.86;6,335.61,78.56,126.54,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,513.62,57.64,25.98,7.86;6,335.61,68.10,165.23,7.86">Search engines: Information retrieval in practice</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Addison-Wesley Reading</publisher>
			<biblScope unit="volume">283</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,90.02,193.03,7.86;6,335.61,100.48,203.11,7.86;6,335.61,110.94,198.39,7.86;6,335.61,121.40,208.97,7.86;6,335.61,131.86,20.96,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,490.15,90.02,38.48,7.86;6,335.61,100.48,203.11,7.86;6,335.61,110.94,198.39,7.86;6,335.61,121.40,41.43,7.86">QU-IR at SemEval 2016 Task 3: Learning to rank on Arabic community question answering forums with word embedding</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Malhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,384.70,121.40,92.33,7.86">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="866" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,143.32,220.31,7.86;6,335.61,153.78,219.72,7.86;6,335.61,164.24,220.32,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,453.39,143.32,102.53,7.86;6,335.61,153.78,203.79,7.86">Machine learned sentence selection strategies for query-biased summarization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanungo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,335.61,164.24,138.78,7.86">SIGIR Learning to Rank Workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,175.69,192.13,7.86;6,335.61,186.16,213.62,7.86;6,335.61,196.62,180.39,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m" coord="6,335.61,186.16,213.62,7.86;6,335.61,196.62,20.08,7.86">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,335.60,208.07,191.51,7.86;6,335.61,218.53,219.53,7.86;6,335.61,228.99,214.72,7.86;6,335.61,239.46,220.32,7.86;6,335.61,249.92,20.96,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,335.61,218.53,219.53,7.86;6,335.61,228.99,49.42,7.86">Learning from the past: answering new questions with past answers</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,403.78,228.99,146.55,7.86;6,335.61,239.46,125.17,7.86">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,261.37,186.68,7.86;6,335.61,271.83,176.99,7.86;6,335.61,282.29,208.96,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,335.61,271.83,176.99,7.86;6,335.61,282.29,40.35,7.86">Learning to rank answers on large online qa collections</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,394.18,282.29,16.13,7.86">ACL</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="719" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,293.75,214.64,7.86;6,335.61,304.21,204.10,7.86;6,335.61,314.67,216.03,7.86;6,335.61,325.13,201.71,7.86;6,335.61,335.59,52.69,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,335.61,304.21,204.10,7.86;6,335.61,314.67,151.39,7.86">QU at TREC-2015: Building real-time systems for tweet filtering and question answering</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,505.77,314.67,45.86,7.86;6,335.61,325.13,197.43,7.86">Proceedings of The Twenty-Fourth Text REtrieval Conference</title>
		<meeting>The Twenty-Fourth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,347.05,212.09,7.86;6,335.61,357.51,209.30,7.86;6,335.61,367.97,211.32,7.86;6,335.61,378.43,162.40,7.86;6,335.61,388.89,220.28,7.86;6,335.61,399.36,147.23,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,466.82,347.05,80.88,7.86;6,335.61,357.51,209.30,7.86;6,335.61,367.97,26.87,7.86">Text summarization model based on maximum coverage problem and its variant</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,380.87,367.97,166.06,7.86;6,335.61,378.43,72.28,7.86">Proceedings of the 12th Conference of the European Chapter</title>
		<meeting>the 12th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,410.81,185.48,7.86;6,335.61,421.27,190.82,7.86;6,335.61,431.73,192.42,7.86;6,335.61,442.19,194.18,7.86;6,335.61,452.66,205.97,7.86;6,335.61,463.12,111.24,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,433.56,410.81,87.52,7.86;6,335.61,421.27,190.82,7.86;6,335.61,431.73,81.91,7.86">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,435.63,431.73,92.39,7.86;6,335.61,442.19,194.18,7.86;6,335.61,452.66,202.42,7.86">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
