<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.52,76.51,399.29,16.70;1,72.52,100.27,447.19,16.70;1,72.52,124.03,349.15,16.70">Evaluation of a Machine Learning Method to Rank PubMed Central Articles For Clinical Relevancy: NCH at TREC 2016 Clinical Decision Support Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,78.86,162.65,51.03,11.14"><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Research Information Solutions and Innovation</orgName>
								<orgName type="department" key="dep2">The Research Institute at Nationwide Children&apos;s Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,146.43,162.65,113.32,11.14"><forename type="first">Soheil</forename><surname>Moosavinasab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Research Information Solutions and Innovation</orgName>
								<orgName type="department" key="dep2">The Research Institute at Nationwide Children&apos;s Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.87,162.50,68.03,11.99"><forename type="first">Anna</forename><surname>Zemke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Medicine</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.02,162.65,91.38,11.14"><forename type="first">Ariana</forename><surname>Prinzbach</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Medicine</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,452.52,162.65,58.70,11.14"><forename type="first">Steve</forename><surname>Rust</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Research Information Solutions and Innovation</orgName>
								<orgName type="department" key="dep2">The Research Institute at Nationwide Children&apos;s Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,72.52,179.45,76.06,11.14"><forename type="first">Yungui</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Research Information Solutions and Innovation</orgName>
								<orgName type="department" key="dep2">The Research Institute at Nationwide Children&apos;s Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.70,179.45,53.37,11.14"><forename type="first">Simon</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Research Information Solutions and Innovation</orgName>
								<orgName type="department" key="dep2">The Research Institute at Nationwide Children&apos;s Hospital</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.52,76.51,399.29,16.70;1,72.52,100.27,447.19,16.70;1,72.52,124.03,349.15,16.70">Evaluation of a Machine Learning Method to Rank PubMed Central Articles For Clinical Relevancy: NCH at TREC 2016 Clinical Decision Support Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F530E59B4CE229011E26C0F41ACEFE3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of the TREC 2016 Clinical Decision Support track is to retrieve and rank PubMed Central (PMC) articles that are relevant to potential tests, treatments or diagnoses of a patient case narrative. Our objective was to develop a machine learning method to rank PMC articles by taking advantage of the previous years' gold standard TREC competition results. The classifier we trained on 2014 data achieved high accuracy when tested with 2015 data (P10=0.59 and infNDCG=0.67) compared with the Elasticsearch method (P10=0.19 and infNDCG=0.22). However, when we applied the same classifier approach with both the 2014 and 2015 data sets combined, and then tested this method against the 2016 cases, the results did not improve over the Elasticsearch method. We concluded that although the machine learning approach was found effective on predicting previous years' results, it was not as effective for 2016 data, most likely due to the change in the topic structures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>PubMed articles contain a wealth of medical evidence that is highly valuable to physicians for finding key medical knowledge for decision-making such as information on the diagnosis, tests and treatment of a disease. A computerized decision support system could improve the efficiency of evidence-based decision-making by automatically scanning millions of articles and providing a more pertinent retrieval of information. The Clinical Decision Support (CDS) track of Text REtrieval Conference (TREC) challenges participants to retrieve and rank PubMed Central (PMC) articles that are potentially relevant to given medical record topics <ref type="bibr" coords="1,358.70,659.93,12.51,11.14" target="#b0">[1]</ref>.</p><p>For this 2016 CDS task, we designed a machine learning approach that has not been previously tested within the TREC CDS track. This new method was compared against the Elasticsearch (ES) method, which we utilized as a baseline from data available across three consecutive years, in order to determine its efficacy. We hypothesized that a supervised learning approach could take advantage of existing implicit knowledge of judges from their decisions in determining the relevancy of articles in previous years, and if successful, this learning approach could be applied to predict new cases. In this paper, we report our design of this learning system and discuss the experiment results when using different systems as well as the different data from previous years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topical Data</head><p>2014-2016 topic data were used along with the gold standard data for 2014 and 2015. The gold standard data included relevancy scores for each article by each topic assigned by TREC CDS judges. The score had a value from 0 to 2 with 0 indicating irrelevant, 1 potentially relevant, and 2 definitely relevant. Given the fact that majority of the articles judged in previous years were graded as irrelevant (overall &gt;90% for all topics), we combined articles judged potentially relevant and definitely relevant into one group and assigned the value of 1 to these articles. In terms of the versions of topics, for the 2014 and 2015 topics we only used case summaries as this was the only data provided, while for the 2016 topics we used case summaries for automated runs, and then the newly included patient notes for the manual run and the test of the baseline method (Elasticsearch method) only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PubMed Central Data</head><p>About 1.25 million PubMed Central (PMC) articles provided by TREC were separated into title, keywords, abstract and body sections for Elasticsearch indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unified Medical Language System (UMLS)</head><p>The NLM UMLS was used as a knowledge component of our system for extracting key medical terminologies from the TREC topic text. UMLS was used to identify medical concepts in the topic text and relate them to semantic categories (i.e. disease, symptoms, findings, etc.) and alternative names (i.e. synonyms, preferred names, etc.). Previous studies have found UMLS to be very effective in NER (Named Entity Recognition) tasks on medical documents <ref type="bibr" coords="2,297.99,662.33,12.51,11.14" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>To build our system, we first used UMLS to extract medical terms from each topic. We then designed two components of our ranking system: the Elasticsearch (ES) component and the learning component. Both components have ranking capabilities to work independently or collaboratively. The focus of this work is on evaluating the learning method against the ES method, and a comparison to previous and this year training as well as other groups for this year's TREC CDS challenge. We also reported the results of our manual approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Processing Using UMLS</head><p>We used UMLS to extract key medical terminology from the summary and note text of each topic. The goal of UMLS processing is to find medical terms in the input text that are key for clinical decision making such as disease, symptoms, findings, etc. Information such as gender, age and ethnicity can be useful for diagnosing certain diseases. However, in other cases, such information is not particularly specific to the diagnosis, test or treatment of a disease and therefore were excluded from the UMLS parsing.</p><p>The most recent UMLS MetaMap Java API was used to first detect UMLS terms and their CUIs (Concept Unique Identifiers). For example, the case summary "a 78 year old male presents with frequent stools and melena." will be parsed into two concepts "frequent stools (C0848342)" and "melena (C0025222)". The numbers in the parenthesis are CUIs of each term, which can be used to uniquely identify a concept. Based on the CUI, we expanded the concept to include some of its variations, such as its English synonyms or preferred names. We did not include non-English variations, or English variations that were not synonyms or preferred names, or whose usage was suppressed.</p><p>UMLS is very effective in finding optimal phrase boundaries. In the above example, although "stools" is also a concept but because it is part of another meaningful phrase "frequent stools", the extraction of the longer phrase "frequent stools" triumphed over that of the shorter phrase "stools". We think this decision made by UMLS is intuitively appropriate because longer phrases usually tend to be more precise than single terms (i.e. "frequent stools" vs. "stools") and the correct boundary cutting can be critically important for effective clinical decision-making.</p><p>The results of UMLS processing include two lists of terms for each version of the topic: UMLS terms and UMLS expansion terms. All terms were converted to lower cases and all duplicates were removed. UMLS terms were used in building the Elasticsearch system to index PubMed articles while both UMLS terms and UMLS expansion terms were used in building the learning system. Different from previous years, the 2016 topic data included the medical history of a patient and conditions that could not be decided as either current or previous. To differentiate present conditions from previous, we implemented a regular expression parser based on a previous study <ref type="bibr" coords="4,253.31,217.85,12.51,11.14" target="#b2">[3]</ref>, to classify extracted medical terms in three categories: present, previous and unclassified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Learning</head><p>The availability of the gold standard data from the two previous years, makes it possible to build a supervised learning system to classify topic-specific articles. In the gold standard data, we combined a score of 1 and 2 to be 1 and made it a single pot of relevant set, that is, 1 indicating relevant and 0 irrelevant. From UMLS, we obtained two lists of keywords in a specific topic text: UMLS terms and UMLS expansion terms.</p><p>The extracted UMLS terms cannot be used as features directly either as a binary or as a count variable because the list is ever expanding given the content of a new topic. Therefore, we implemented a vectorization process to convert continuous keyword occurrences to a Weighted Keyword Count (WKC) for both UMLS terms and UMLS expansion terms. Based on WKC, we developed two features for training: ğ‘Šğ¾ğ¶ !"#$ , the feature accounting for the frequency of UMLS term occurrences in the article and ğ‘Šğ¾ğ¶ !"#$%&amp;'()*+,) , the feature accounting for the frequency of UMLS expansion term occurrences in the article. The two WKC features was calculated as follows:</p><formula xml:id="formula_0" coords="4,72.52,529.08,461.47,69.12">ğ‘Šğ¾ğ¶ !"#$ = ğ‘¡â„ğ‘’ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ ğ‘ˆğ‘€ğ¿ğ‘† ğ‘¡ğ‘’ğ‘Ÿğ‘š ğ‘šğ‘ğ‘¡ğ‘â„ğ‘’ğ‘  ğ‘–ğ‘› ğ‘¡â„ğ‘’ ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™ğ‘’ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘ˆğ‘€ğ¿ğ‘† ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘  ğ‘–ğ‘› ğ‘¡â„ğ‘’ ğ‘¡ğ‘œğ‘ğ‘–ğ‘ ğ‘Šğ¾ğ¶ !"#$%&amp;'()*+,) = ğ‘¡â„ğ‘’ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ ğ‘ˆğ‘€ğ¿ğ‘† ğ‘’ğ‘¥ğ‘ğ‘ğ‘›ğ‘ ğ‘–ğ‘œğ‘› ğ‘¡ğ‘’ğ‘Ÿğ‘š ğ‘šğ‘ğ‘¡ğ‘â„ğ‘’ğ‘  ğ‘–ğ‘› ğ‘¡â„ğ‘’ ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™ğ‘’ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘ˆğ‘€ğ¿ğ‘† ğ‘’ğ‘¥ğ‘ğ‘ğ‘›ğ‘ ğ‘–ğ‘œğ‘› ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘  ğ‘–ğ‘› ğ‘¡â„ğ‘’ ğ‘¡ğ‘œğ‘ğ‘–ğ‘</formula><p>The two features were calculated for each article by each topic in the 2014 and 2015 gold standard dataset. Due to the extreme unbalancing of the data in the gold standard set, we matched the total number of irrelevant entries to the total number of relevant entries to mandate a 50:50 class split for machine learning. This resulted in about 16,000 annotated machine learning instances for binary classification.</p><p>Additionally, we added a binary topic type feature indicating whether the topic type keyword is present in the article or not. The topic keywords were also derived using UMLS based on three primitive types: diagnosis, test and treatment. The training process aimed to find both the best features to use and the best model to classify instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elasticsearch Indexing</head><p>UMLS terms extracted from the topic text were used to index the title, keywords, abstract and body sections of a PMC article using Elasticsearch (ES). Among different algorithms provided by ES, we selected BM25 (parameters k1=3 and b=0.75) as our ranking algorithm due to slightly better performance we observed than using other algorithms. We also implemented a logical OR query in ES. To compensate the effect of the variations in the length of the queries generated from notes or summaries, we used a "minimum percentage match" criterion for search queries. We require an article to be a match for a note query if at least 15% of the keywords in that document. This percentage increases to 20% for summary note type because summaries are shorter.</p><p>For the 2016 task, we boosted present keywords more than unclassified and previous keywords. According to our domain experts, for all three topics: diagnoses, tests, and treatment, the data of the current condition of the patient is more useful in selecting relevant articles than data on the previous condition. The difference in importance of current versus previous conditions is most exaggerated in treatment and less so in diagnoses and tests. Therefore, we designed two boosting strategies. For treatment topics, we boosted present condition 3 times, unclassified condition twice and previous condition once. For diagnosis and test topics, we boosted present condition three times, unclassified condition twice and previous condition also twice. We applied this boosting only on summary notes because full notes are too "messy" to extract present and previous conditions effectively.</p><p>Other information retrieval techniques we tested (such as stemming, proximity matching, boosting phrases, and prefix matching) did not contribute to better results; therefore we did not apply these techniques to the final system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual Run</head><p>We invited 3 physicians to participate in our experiment. The search keywords were manually provided by our domain experts and we utilized Google to automate the retrieval of PMCIDs within the PubMed domain. Each physician was assigned 10 topics and provided a list of 2 to 4 key-phrases based on the full note of the topic. The keyphrases did not have to be part of the note, and could be derived from the person's own knowledge after reading the medical case. The key-phrases were then entered into google, and the PubMed articles on the first search page were reviewed to determine if these articles were relevant to the medical case and would assist in establishing the diagnosis. If the majority of articles were deemed pertinent, the domain experts and physician would confirm the utilized key-phrases as finalized. However, if the search returned articles that were not closely relevant to the diagnosis or medical note, the keyphrases were adjusted until the majority of articles on the first search page provided useful information. When the final list of key-phrases was compiled, the computer created a query and automatically retrieved the top 1000 available PMC-IDs for each topic by searching Google and restricting the results from site:ncbi.nlm.nih.gov/pmc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In machine learning, both features ğ‘Šğ¾ğ¶ !"#$ and ğ‘Šğ¾ğ¶ !"#$%&amp;'()*+,) were shown to have high discriminatory power based on the information gain and principle component feature ranking methods. However, the topic type feature was found to be not discriminatory enough to be included the model. Therefore, we had only two WKC features selected for machine learning. We also calculated the average of ğ‘Šğ¾ğ¶ !"#$ and ğ‘Šğ¾ğ¶ !"#$%&amp;!"#$%&amp;# using 2014 and 2015 data combined and test the statistical significance of the mean using the paired t-test (Table <ref type="table" coords="6,361.35,415.61,4.67,11.14" target="#tab_0">1</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The performance of the machine learning approach varies in testing against 2015 and 2016 gold standard data. The machine learning classifier trained using 2014 data and evaluated using 2015 data produced more accurate results than the classifier trained using both 2014 and 2015 data combined and tested using 2016 data. Since the classifier was trained using a different structure of data (e.g. 2016 topic data contained a mixture of present, previous and unclassified conditions) than the data provided in the 2014 and 2015 competition, we believe that this method of machine learning could be more effectively utilized in future years when both the training and testing data are utilizing the same level of detail in the topic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we experimented with a machine learning approach to classify PubMed articles for the TREC CDS challenge. This approach was very effective with the 2014 gold standard data to predict 2015 gold standard data, but was not effective on re-ranking the ES results for the 2016 task. We concluded that although the use is potentially promising for similar tasks, the machine learning approach was sensitive to the structural change of the input data. Better strategies need to be implemented to generalize this approach to adapt to heterogeneous nature of medical record narratives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,76.98,340.56,459.02,8.24;8,107.21,350.64,398.58,8.24;8,72.52,72.60,468.00,263.25"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our manual run (blue) performance compared with the median (green) and best (red) performance from all manual run submissions on each topic (Topic 31 is calculated as the group average).</figDesc><graphic coords="8,72.52,72.60,468.00,263.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,72.52,653.04,448.00,8.24;8,72.52,663.36,446.99,8.24;8,72.52,673.44,39.50,8.24;8,72.52,385.06,468.00,263.25"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Our best automatic run (SumES, blue) performance compared with the median (green) and best (red) performance from all automatic run submissions on each topic (Topic 31 is calculated as the group average).</figDesc><graphic coords="8,72.52,385.06,468.00,263.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,72.52,445.71,464.93,248.15"><head>Table 1 . Weighted Keyword Count features</head><label>1</label><figDesc></figDesc><table coords="6,72.52,467.79,461.64,194.39"><row><cell>Variable</cell><cell>Relevance=0</cell><cell>Relevance=1</cell><cell>p-value</cell></row><row><cell></cell><cell>(N=67410)</cell><cell>(N=8346)</cell><cell></cell></row><row><cell>Average ğ‘Šğ¾ğ¶ !"#$</cell><cell>0.14</cell><cell>0.27</cell><cell>&lt;0.01</cell></row><row><cell>Average ğ‘Šğ¾ğ¶ !"#$%&amp;'()*+,)</cell><cell>0.17</cell><cell>0.38</cell><cell>&lt;0.01</cell></row><row><cell cols="4">We first implemented logistic regression classifier using 2014 data (about 7000</cell></row><row><cell cols="4">instances with 50:50 split) for training and 2015 data for testing (about 10000 instances</cell></row><row><cell cols="4">with 50:50 split). The overall AUC of the classifier on 2015 test data is 0.79 (TP=0.72,</cell></row><row><cell cols="4">FP=0.28, F1=0.71). According to the standard TREC evaluation metrics, the overall</cell></row><row><cell cols="3">infNDCG was 0.67 and overall P10 was 0.59 on the 2015 test data.</cell><cell></cell></row><row><cell cols="4">To predict 2016 results, we then combined the gold standard data from two previous</cell></row><row><cell cols="4">years 2014 and 2015 to train a new logistic regression classifier. ES was used to</cell></row></table><note coords="6,72.52,666.89,464.93,11.14;6,72.52,682.73,456.89,11.14"><p>generate the top 1000 results for each topic. Our new classifier was then used to rerank the ES results. Both the ES results and the classifier reranking results were part of our</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC 2016 submission. Table <ref type="table" coords="7,238.60,75.05,6.67,11.14">2</ref> shows the results for 2016. All reported TREC performance measures are the averages of all topics. In comparing our results with submissions from the other 25 participating organizations, our manual run was the overall top performer among 8 manual runs using the note data, and our SumES automatic method was ranked top 10 overall among 46 automatic runs using the summary data (more specifically ranking 2 nd on infAP, 4th on P@10 and 7th on infNDCG among all automatic runs using the summary data). Figure <ref type="figure" coords="7,323.22,312.31,6.00,10.02">1</ref> and Figure <ref type="figure" coords="7,388.02,312.31,6.00,10.02">2</ref> show the performance comparison of our top manual and automatic runs against the median and best runs from each topic for TREC 2016 CDS. We omitted the worst performance here because the majority of the values were zeros. Topic 31 does not exist and is calculated as the average performance across all topics for each comparison (i.e. NCH, Median and Best).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,77.08,461.11,426.96,10.02;9,108.52,473.83,117.34,10.02" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,328.63,461.11,175.41,10.02;9,108.52,473.83,112.26,10.02">Overview of the TREC 2015 Clinical Decision Support Track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.08,486.55,431.23,10.02;9,108.52,499.03,404.09,10.02;9,108.52,511.75,143.63,10.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,339.59,486.55,168.72,10.02;9,108.52,499.03,306.89,10.02">Automatically Detecting Failures in Natural Language Processing Tools for Online Community Text</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Hartzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,423.39,499.03,89.22,10.02;9,108.52,511.75,87.36,10.02">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.08,524.47,461.17,10.02;9,108.52,536.95,430.45,10.02;9,108.52,549.67,234.13,10.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,84.74,524.47,354.65,10.02">ConText: An algorithm for identifying contextual features from clinical text</title>
	</analytic>
	<monogr>
		<title level="m" coord="9,446.64,524.47,91.61,10.02;9,108.52,536.95,425.23,10.02">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
