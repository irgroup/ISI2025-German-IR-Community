<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.86,79.96,238.11,12.54">RMIT at the TREC 2016 LiveQA Track</title>
				<funder ref="#_ARzPEvd">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder ref="#_Hf8XR2X">
					<orgName type="full">Australian Research Council&apos;s Discovery Projects Scheme</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.21,125.93,61.02,8.64"><forename type="first">Joel</forename><surname>Mackenzie</surname></persName>
							<email>joel.mackenzie@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.33,125.93,72.52,8.64"><forename type="first">Ruey-Cheng</forename><surname>Chen</surname></persName>
							<email>ruey-cheng.chen@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.04,125.93,79.19,8.64"><forename type="first">J</forename><forename type="middle">Shane</forename><surname>Culpepper</surname></persName>
							<email>shane.culpepper@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.86,79.96,238.11,12.54">RMIT at the TREC 2016 LiveQA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">67C39DAF6B50AA0FBA071653C0B67822</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TREC LiveQA 2016</term>
					<term>RMIT</term>
					<term>paragraph retrieval</term>
					<term>summarization</term>
					<term>learning to rank</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the four systems RMIT fielded for the TREC 2016 LiveQA task and the associated experiments. Similar to last year, the results show that simple solutions tend to work best, and that our improved baseline systems achieved an above-average performance. We use a commercial search engine as a first stage retrieval mechanism and compare it with our internal system which uses a carefully curated document collection. Somewhat surprisingly, we found that on average the small curated collection performed better within our current framework, warranting further studies on when and when not to use an external resource, such as a publicly available search engine API. Finally, we show that small improvements to performance can substantially reduce failure rates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. OVERVIEW</head><p>In the TREC LiveQA 2016 challenge, we continued to explore ideas within an established two-stage answerfinding framework used in last year's LiveQA challenge. Our long-term goal is to build a fully functional, modular multi-stage retrieval system that cascades candidate results through a series of increasingly complex filters. In our current system, a first-stage retrieval module is employed to first populate a good set of answer-bearing passages, and then a summarization module is used to generate the final answers via passage re-ranking or query-biased summarization.</p><p>We configured four different systems following two distinct strategies. First, we looked at the retrieval part and made comparisons between two common retrieval settings: retrieving passages from a home-made question-answering test collection, and retrieving snippets from a commercial search engine. Second, we made comparisons between two common answer-producing strategies: answer reranking using a Learning-to-Rank (LtR) model, and querybiased summarization using a max-coverage optimization model. This has led us to consider the following research questions: RQ 1:. Which strategy produces better passages, retrieving from a local test collection or pulling content from a commercial search engine? RQ 2:. Which strategy produces better answers, locating the best passages directly or generating a succinct summary out of the top passages? RQ 3:. Does the efficiency of the first-stage retrieval module contribute to the failure rate on longer questions? We also did a careful per query analysis using the 2015 queries to determine how efficiency impacted the performance with regard to the time budget. We found that returning the top-k paragraphs from our local collection followed by generating the answer with the coverage-based summarizer provided the most effective answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>We now describe the collection and retrieval setting used in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Server architecture</head><p>The servers are built on top of the computing resources we allocated from NecTAR,<ref type="foot" coords="1,430.24,533.01,3.49,6.05" target="#foot_0">1</ref> the Australian National Research cloud computing network. Throughout the challenge, we use only one instance to host all of the services.</p><p>All four RMIT systems implemented a two-stage system architecture, as illustrated in Figure <ref type="figure" coords="1,464.38,583.77,3.77,8.64" target="#fig_0">1</ref>. Upon receiving a question, the system would first convert it into a bagof-words query, and run the query through the Retrieval Module (cf. Section III). The Retrieval Module retrieved a set of passages from the underlying corpora and served them to the Summarization Module (cf. Section IV), which in turn produces a piece of text that is coherent and long enough to fill the required answer size.</p><p>The various modules were connected using a resource allocator called Answer Producer, and written in the Go Programming Language. It included graceful handling of timeouts, and guaranteed responses within the 60 second window.     </p><formula xml:id="formula_0" coords="2,82.77,185.56,160.72,113.79">• • • • • • • • • • • • • • • • • • • • • • • • • • • •<label>0</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.).</head><p>The horizontal lines at 40, 50 and 60 seconds denote an increasing chance of failure, as the retrieval budget becomes too high. Clearly, any query above the 60s line was a failure, and any above the 50s line was likely a failure, given the time required to generate an answer from the candidates and return the answer to the LiveQA server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Run descriptions</head><p>RMIT-1 (automatic): A WAND bag-of-words passage retrieval using all of the terms in the question title, with answers generated from top-k passages by using a Learning-to-Rank model. See Section IV-A for a full description of our Learning-to-Rank model.</p><p>RMIT-2 (automatic): Bing Search API snippets using all of the terms in the question title, with answers generated from top-k passages by using a Learning-to-Rank model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMIT-11 (automatic):</head><p>A WAND bag-of-words passage retrieval using all of the terms in the question title, with answers generated from top-k passages by using a coveragebased summarization algorithm. See Section IV-B for details on the summarization model.</p><p>RMIT-12 (automatic): Bing Search API snippets using all of the terms in the question title, with answers generated from top-k passages by using an optimizationbased summarization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RETRIEVAL MODULE</head><p>A. Passage Retrieval Using WAND Indexes Our first retrieval module was built on top of several question-answering test collections, including English Wikipedia, Yahoo! Answers CQA data 1.0, and the TREC 2015 LiveQA dataset. Table <ref type="table" coords="2,173.70,751.24,3.35,8.64" target="#tab_1">I</ref> shows the details for each of these test collections. To prepare the test collection for English Wikipedia, open-source tools such as wpdownload 3 and wikiextractor<ref type="foot" coords="2,420.98,406.20,3.49,6.05" target="#foot_3">4</ref> were used to fetch a recent dump and extract the XML contents. Paragraphs extracted from the output of wikiextractor were assigned a unique docno and got indexed using Indri. <ref type="foot" coords="2,464.35,442.07,3.49,6.05" target="#foot_4">5</ref> For the Yahoo! Answers CQA data, we stripped all answer items (not just the best answers) from the data and indexed them as documents. No further paragraph separation was done to these answer items as most of them were short. No other subject/content information was incorporated (i.e., question title or description). The TREC 2015 LiveQA data was prepared and indexed in a similar fashion, with answer items treated as standalone paragraphs. The approach taken here was similar to what we did last year <ref type="bibr" coords="2,369.75,564.36,10.65,8.64" target="#b1">[2]</ref>, but with a few subtle differences: Instead of using the Indri index to answer queries directly, and utilizing the passage retrieval operator #combine[passage100:50](...) to return a set of candidate passages from these documents, we chose to index paragraphs and passages directly, and using our own search system<ref type="foot" coords="2,407.99,634.42,3.49,6.05" target="#foot_5">6</ref> to perform the top-k retrieval. This allows us to use standard bag-of-words retrieval models to get a reasonable candidate set of passages more efficiently, which can then be passed onto the next stage of our system. This was motivated by the scalability of Indri passage retrieval: It does not scale well as query length increases (Figure <ref type="figure" coords="2,412.24,707.82,3.63,8.64" target="#fig_2">2</ref>). Given that there is a fixed time budget in the task, and queries can be long, our new approach allows us to more reliably retrieve a set of candidate documents within the time budget. Our custom indexing system (WANDbl) is a faithful reimplementation of WAND <ref type="bibr" coords="3,169.59,345.30,11.69,8.64" target="#b0">[1]</ref> with several engineering performance enhancements that maximize efficiency <ref type="bibr" coords="3,268.94,357.25,10.73,8.64" target="#b2">[3,</ref><ref type="bibr" coords="3,282.52,357.25,7.45,8.64" target="#b4">5,</ref><ref type="bibr" coords="3,54.00,369.21,7.15,8.64" target="#b5">6]</ref>. The index required for our system is generated from the Indri index, and we used Krovetz stemming and the default InQuery stoplist <ref type="foot" coords="3,118.83,391.45,3.49,6.05" target="#foot_6">7</ref> . This yielded a single 5.6 GB index that contained 79.2 million paragraphs and 15.5 million unique terms. The average length of an indexed paragraph was 47.4 terms. We use BM25 to rank the candidate passages, with the parameter configuration of k 1 = 0.9 and b = 0.4. <ref type="foot" coords="3,284.74,439.27,3.49,6.05" target="#foot_7">8</ref>The performance of the WANDbl index and Indri index are shown in Figure <ref type="figure" coords="3,143.75,465.17,3.77,8.64" target="#fig_3">3</ref>. Clearly, it is more beneficial to use the WANDbl system, as this allows more time for the second-stage re-ranking and summarization. In the production system, the module was deliberately configured to return only the top-10 passages as increasing this number showed no benefit in overall effectiveness with our current summarizer.</p><formula xml:id="formula_1" coords="2,327.36,211.52,189.40,90.09">• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • 10 0</formula><p>Another design decision was whether to use Title-only, or both the Title and the Description for our firststage retrieval. Given that we opted to use simple bag-ofwords models, we used the LiveQA2015 dataset to test the performance of Title-only queries, compared to Title + Description queries. Table <ref type="table" coords="3,175.24,608.96,6.65,8.64" target="#tab_3">II</ref> presents the effectiveness results for this experiment. We present Recall@k including only answers marked as "highly relevant", and also Recall@k including both "highly relevant" and "relevant" answers. We also present ERR@20 to gauge how well the first-stage system does in terms of early precision, using the weights 4, 3, 2 and 1, as found in the LiveQA2015 QRELs. Based on these simple experiments, we found that Title-only queries performed better in our current </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bing Search API snippets</head><p>Our second retrieval module was built on top of the Bing search engine, using its search result snippets as answer candidates. The advantage of this approach is that more answer-bearing passages can be directly discovered from the Web, although these passages might contain incomplete sentences or truncated texts. As will be shown in Section V-A, Bing snippets were found to be indicative of relevant webpages, but perhaps not optimized for revealing answer-bearing sentences.</p><p>Our implementation used the Bing Search API which was available via the Azure Data Market. <ref type="foot" coords="3,493.25,355.58,3.49,6.05" target="#foot_8">9</ref> For each incoming question, the question title was submitted to Bing, and then the top-50 search result snippets were retrieved and passed on to the next stage. Increasing the number of snippets to 100 showed no benefit in our earlier experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SUMMARIZATION MODULE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning-to-Rank Model</head><p>Our first answer generation module implements a Learning-to-Rank model proposed by Metzler and Kanungo <ref type="bibr" coords="3,348.27,491.40,10.65,8.64" target="#b3">[4]</ref>. The model was originally used in querybiased summarization, using six simple yet effective features to predict the relevance of sentences from retrieved documents. A summary can then be put together by repeatedly incorporating top-ranked sentences until the character limit is reached. This method was a common baseline in snippet generation, and in some recent work it was also used to rank answer sentences <ref type="bibr" coords="3,472.32,575.08,10.58,8.64" target="#b8">[9]</ref>. In our production system, this model was applied to directly ranking paragraphs/passages. One reason for not using finer-grained text units such as sentences is that the top-ranked sentences do not always produce a coherent text (even when sorted in their original order). Another reason, and perhaps more compelling, is that adapting the model to answer ranking provides an interesting comparison to conventional summarization methods. In our preliminary tests, the answer ranking strategy appeared to deliver comparable results to the sentence ranking approach.</p><p>Table <ref type="table" coords="3,342.92,706.77,10.05,8.64" target="#tab_4">III</ref> provides more details about the features we used. From the work by Metzler and Kanungo, five out of the original six features were adapted to our passage data. Two answer ranking models were developed separately Both models were trained via 5-fold cross validation using the LambdaMART implementation from RankLib 10 , and optimized for ERR@5. All the texts in the training data were stemmed using the Krovetz stemmer, and stopped using the InQuery stoplist. WordNet synsets were used for extracting the OverlapSyn feature. The GOV2 test collection was used as the background corpus in the computation of the LM feature. The parameter µ in LM was set to 10. We ran a small-scale grid search over the number of trees and the learning rate to choose the final parameter settings. Eventually, we settled on the latter model trained on LiveQA data as it delivered stronger results in our preliminary tests. The final model has this configuration: the number of trees was set to 1000, the number of leaves to 10, and the learning rate to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization Model</head><p>For summarization, we used the model proposed by Takamura and Okumura <ref type="bibr" coords="4,159.64,528.52,11.69,8.64" target="#b7">[8]</ref> to generate extractive summaries from the top-ranked passages. In this model, summarization is characterized as a two-way optimization problem, in which coverage over important words is maximized, and redundancies are minimized simultaneously. The mathematical formulation is given as follows:</p><formula xml:id="formula_2" coords="4,59.55,608.06,172.15,19.91">maximize (1 -λ) j w j z j + λ i j</formula><p>x i w j a ij subject to x i ∈ {0, 1} for all i; z j ∈ {0, 1} for all j;</p><formula xml:id="formula_3" coords="4,134.81,647.06,153.91,52.00">i c i x i ≤ K; i a ij x i ≥ z j for all j<label>(1)</label></formula><p>To produce an extractive summary, a choice over the set of sentences is made to decide what to include. This choice is modeled in the optimization problem as two sets of variables x i and z j , where the former indicating the 10 http://www.lemurproject.org/ranklib.php binary decision on keeping sentence i, and the latter on keeping word j in the summary. In other words, for each sentence i, x i is set to 1 if sentence i is to be included in the summary, or 0 otherwise. Analogously for each term j, z j is set to 1 if term j is included.</p><p>In this problem, c i denotes the cost of selecting sentence s i (i.e. number of characters in s i ), and w j denotes the weight of word j. We used a TF•IDF weighting scheme in which the term frequency (TF) is derived from the question title and body, and the inverse document-frequency (IDF) is learned from a background corpus. The term frequency collected from the question body is further penalized with a factor α &lt; 1 as the information given in the question body can be less precise than in the title.</p><formula xml:id="formula_4" coords="4,341.50,392.41,199.94,9.65">w j = [TF title (j) + α TF body (j)] * IDF(j)<label>(2)</label></formula><p>The correspondence between the sentence i and the word j is coded in the indicator variable a ij , whose value is set to 1 if the word j appears in sentence i, and 0 otherwise. With the first constraint, we limit the size of the summary to K characters at most (K is set to 1,000 throughout). With the second constraint, the word coverage is related to the sentence coverage, thus completing the formulation.</p><p>Empirically, we fine-tuned the parameters λ and α based on prior test runs. In the challenge, we set λ = 0.1 and α = 0.43. We used the IBM CPLEX solver to compute the optimal allocation. The background corpus used was the GOV2 test collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Effectiveness</head><p>The LiveQA challenge results are shown in Table <ref type="table" coords="4,529.55,595.66,8.75,8.64" target="#tab_5">IV</ref>, where our submitted runs and the average result across all runs are shown. Our RMIT-11 run delivered the best performance in our experiment, achieving 0.786 in Avg Score. The base run outperformed the average across all runs and metrics, except for P@4+, where it was very much equal to the average score.</p><p>A post-hoc analysis was performed to understand the effect of the test collection and that of our answerproducing algorithm, using RMIT-11 as the reference run. The distribution of score differences across query topics is shown in Table <ref type="table" coords="4,370.45,727.33,4.79,8.64" target="#tab_6">V</ref>, where two related runs RMIT-1 (shared the test collection) and RMIT-12 (shared the answerproducing algorithm) are compared directly. Query-biased summarization and the answer re-ranking algorithms were found to have differences on 29.8% of the queries, and query-biased summarization appeared to have a slight advantage. Regarding the influence of test collections, our local collection and the Bing snippets performed similarly for around 50% of the queries. For the remainder of the queries, our local collection was roughly 3.67 times more likely to produce a better result than the Bing snippets.</p><p>Inspired by this large difference on test collection, we carried out a further error analysis to investigate any potential issues that the test collections may have. A subset of 408 queries was formed which included the queries with the biggest score differences between RMIT-11 and RMIT-12. For these, 47 queries were randomly sampled, which is approximately 10% of the subset size. One of the co-authors was asked to carefully review all 47 queries and the associated answers, and identify possible causes of the observed score differences. A breakdown of the recorded error sources is provided in Table <ref type="table" coords="5,240.97,559.41,8.75,8.64" target="#tab_6">VI</ref>. For the Bing snippet system, missing answers and text truncation were responsible for the majority of differences. The local collection also struggles to retrieve the valid answers for some queries, but less-fragmented answer summaries provided by the system were in general more readable, which increased the odds of receiving a favorable judgment by the assessors.</p><p>Generally speaking, it would appear that our own firststage system provides better candidates to the second stage LtR/Opt systems, as both WANDbl-based systems clearly outperformed the Bing systems. This is likely due to how noisy the questions were. Given that our local collection was optimized for QA, there is a greater chance that the passages retrieved were relevant, whereas the Bing system may have returned other extraneous information (as the information need may be unclear), and the corpus used is orders of magnitude larger and more diverse. This  where performance begins to affect the likelihood that results will be returned within the time budget.</p><formula xml:id="formula_5" coords="5,331.42,178.81,171.60,116.92">• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • 0.1</formula><p>highlights the importance of query understanding and query rewriting for QA tasks -something we intend to focus on in future LiveQA initiatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Efficiency</head><p>Each RMIT system returned 1,006 ± 6 answers, which was considerably more than the average of 771 answers. This is likely due to how efficient the RMIT systems were -Figure <ref type="figure" coords="5,366.16,503.04,4.93,8.64" target="#fig_5">4</ref> compares the 4 systems fielded by RMIT in 2015 against the 4 RMIT systems used in this years challenge. The 2016 systems were able to generate answers under 10 seconds for every received query, a significant improvement over last year.</p><p>Additionally, Figure <ref type="figure" coords="5,400.06,562.73,4.93,8.64">5</ref> breaks down the timings for each system by the query length. Interestingly, the query length does not have an effect on the median query time. This is because the timings are dominated by the second-stage summarization module, which is not directly effected by the length of the query. Both System 1 and 2 shared a similar LtR last stage configuration, which is more computationally expensive than our summarizer, as the figure shows.</p><p>Clearly, there is much more time in our budget that can be utilized to improve our answer quality; future work includes adding additional stages to the retrieval system, such as a query-rewriting stage, which should help improve effectiveness, while utilizing the remaining time budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have explored four different system configurations for the TREC LIVEQA Track in 2016. While we remain</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,334.49,313.75,179.18,6.91;1,330.19,195.83,187.78,105.17"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. System architecture for the RMIT systems.</figDesc><graphic coords="1,330.19,195.83,187.78,105.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="2,54.00,333.46,234.72,6.91;2,54.00,342.43,236.11,6.96;2,53.75,351.40,234.97,6.91;2,54.00,360.36,235.00,6.91;2,54.00,369.33,234.72,6.91;2,54.00,378.30,234.72,6.91;2,54.00,387.26,181.45,6.91"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.Query times across our test set of queries, timing the Indri passage retrieval function, #combine[passage100:50](...).The horizontal lines at 40, 50 and 60 seconds denote an increasing chance of failure, as the retrieval budget becomes too high. Clearly, any query above the 60s line was a failure, and any above the 50s line was likely a failure, given the time required to generate an answer from the candidates and return the answer to the LiveQA server.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="2,306.72,333.46,234.72,6.91;2,306.72,342.43,235.72,6.91;2,306.72,351.40,234.72,6.91;2,306.72,360.36,236.11,6.91"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Query times across our test set of queries, comparing Indri to our WAND algorithm. These timings are for top-1000 paragraph retrieval, using the bag-of-words BM25 ranking function. Note that time is in log scale msec, while Figure2on the left is in seconds without a log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,306.72,333.21,234.97,7.17;5,306.72,342.43,234.72,6.91;5,306.72,351.40,150.76,6.91;5,489.16,351.40,52.28,6.91;5,306.72,360.11,235.72,7.17;5,306.43,369.33,235.01,6.91;5,306.72,378.30,105.52,6.91"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Response times compared to the 4 systems fielded by RMIT in 2015.Clearly, the improved early stage performance provides a much larger budget for late-stage processing. The orange and red horizontal lines denote the 40, 50 and 60 second intervals respectively, where performance begins to affect the likelihood that results will be returned within the time budget.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,109.42,72.25,376.15,71.05"><head>Table I SUMMARY</head><label>I</label><figDesc>OF COLLECTIONS INDEXED TO ANSWER QUESTIONS.</figDesc><table coords="2,109.42,103.97,376.15,39.33"><row><cell>Collection</cell><cell cols="2"># Paragraphs # Words Description</cell></row><row><cell>Wikipedia-EN</cell><cell>47,193K</cell><cell>1,775M Online knowledge base</cell></row><row><cell>Yahoo! Answers CQA v1.0</cell><cell>31,972K</cell><cell>1,462M Answers items from the Yahoo! Answers website.</cell></row><row><cell>TREC 2015 LiveQA Data</cell><cell>22K</cell><cell>1.8M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,57.40,72.25,229.59,194.68"><head>Table II COMPARING</head><label>II</label><figDesc>TITLE-ONLY TO TITLE + DESCRIPTION FIRST-STAGE BAG-OF-WORDS QUERIES. WE REPORT RECALL@k, AND ERR@20. NOTE THAT ALL OF THE TITLE-ONLY RESULTS WERE FOUND TO BE SIGNIFICANTLY BETTER WITH A p &lt; 0.01, USING A TWO-TAILED PAIRED t-TEST.</figDesc><table coords="3,88.19,138.81,166.34,128.12"><row><cell>Metric</cell><cell cols="2">Title-only Title + Description</cell></row><row><cell></cell><cell>4+ Relevance</cell><cell></cell></row><row><cell>Recall@10</cell><cell>0.1268</cell><cell>0.0995</cell></row><row><cell>Recall@100</cell><cell>0.1870</cell><cell>0.1518</cell></row><row><cell>Recall@1000</cell><cell>0.2485</cell><cell>0.2036</cell></row><row><cell></cell><cell>3+ Relevance</cell><cell></cell></row><row><cell>Recall@10</cell><cell>0.2134</cell><cell>0.1606</cell></row><row><cell>Recall@100</cell><cell>0.3108</cell><cell>0.2457</cell></row><row><cell>Recall@1000</cell><cell>0.4129</cell><cell>0.3277</cell></row><row><cell></cell><cell>Weights 4,3,2,1</cell><cell></cell></row><row><cell>ERR@20</cell><cell>0.1624</cell><cell>0.1351</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,306.72,72.25,234.72,139.16"><head>Table III LIST</head><label>III</label><figDesc>OF THE 5 QUERY-MATCHING FEATURES</figDesc><table coords="3,306.72,103.97,234.72,107.44"><row><cell>Feature</cell><cell>Description</cell></row><row><cell cols="2">ExactMatch Query is a substring in the passage</cell></row><row><cell>Overlap</cell><cell>Fraction of query terms covered</cell></row><row><cell>OverlapSyn</cell><cell>Fraction of query-term synonyms covered</cell></row><row><cell>LM</cell><cell>Log-likelihood from the passage language model</cell></row><row><cell>Length</cell><cell>Number of terms in the passage</cell></row><row><cell cols="2">system configuration. Thus, we use only the Title during</cell></row><row><cell cols="2">the first-stage candidate retrieval.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,53.50,72.25,484.09,237.85"><head>Table IV EFFECTIVENESS</head><label>IV</label><figDesc>SUMMARY FOR ALL FOUR RMIT SYSTEMS WHEN COMPARED TO THE AVERAGE ACROSS ALL SYSTEMS PARTICIPATING IN THE 2016 LIVEQA TRACK.</figDesc><table coords="4,53.50,113.23,418.26,172.96"><row><cell>Run ID</cell><cell>Description</cell><cell>Avg. Score (0-3)</cell><cell>@2+</cell><cell>Success @3+</cell><cell>@4+</cell><cell>@2+</cell><cell>Precision @3+</cell><cell>@4+</cell></row><row><cell>RMIT-1</cell><cell>WAND + LtR</cell><cell>0.723</cell><cell cols="3">0.384 0.239 0.099</cell><cell cols="3">0.388 0.242 0.100</cell></row><row><cell>RMIT-2</cell><cell>Bing + LtR</cell><cell>0.422</cell><cell cols="3">0.250 0.132 0.039</cell><cell cols="3">0.254 0.134 0.040</cell></row><row><cell cols="2">RMIT-11 WAND + Opt</cell><cell>0.786</cell><cell>0.428</cell><cell>0.252</cell><cell>0.106</cell><cell>0.431</cell><cell>0.254</cell><cell>0.107</cell></row><row><cell cols="2">RMIT-12 Bing + Opt</cell><cell>0.447</cell><cell cols="3">0.273 0.137 0.037</cell><cell cols="3">0.274 0.137 0.037</cell></row><row><cell>All Runs</cell><cell></cell><cell>0.577</cell><cell cols="3">0.304 0.190 0.086</cell><cell cols="3">0.392 0.243 0.108</cell></row><row><cell cols="4">using the Yahoo! Answers CQA data 1.0 (WebScope</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">L6) and the TREC 2015 LiveQA data. The model from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the Yahoo! Answers data was trained on a sample of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">1,000 questions, with the best answer labeled as 1 and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the other answer items as 0. The second model was</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">trained on the TREC 2015 LiveQA data using graded</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="4,54.00,289.18,234.72,8.96;4,54.00,301.14,236.46,8.96"><p>relevance: All 1,087 questions with score-4 answers are labeled as 2, score-3 answers as 1, and the others as 0.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,56.79,72.25,452.71,252.55"><head>Table V DISTRIBUTION</head><label>V</label><figDesc>OF SCORE DIFFERENCES ACROSS QUERIES BETWEEN RMIT-11 AND TWO RELATED RUNS</figDesc><table coords="5,56.79,103.97,452.71,220.83"><row><cell>System Pair</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Score Differences</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-4</cell><cell>-3</cell><cell>-2</cell><cell>-1</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>Pr(diff &lt; 0) Pr(diff = 0) Pr(diff &gt; 0)</cell></row><row><cell>RMIT-11 vs. RMIT-1</cell><cell>1</cell><cell>8</cell><cell cols="6">30 86 700 126 26 21</cell><cell>0</cell><cell>12.5%</cell><cell>70.2%</cell><cell>17.3%</cell></row><row><cell>RMIT-11 vs. RMIT-12</cell><cell>0</cell><cell cols="8">10 29 72 483 211 99 61 37</cell><cell>11.1%</cell><cell>48.2%</cell><cell>40.7%</cell></row><row><cell cols="2">Table VI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">ERROR CAUSES, BASED ON AN ANALYSIS ON THE OBSERVED SCORE</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">DIFFERENCES BETWEEN RMIT-11 AND RMIT-12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cause</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2"># Queries</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Local collection errors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Navigational intent</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Formatting (i.e., answer in HTML table)</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Query drift caused by question body</cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Irrelevant answer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Assessor disagreement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bing snippets errors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Result filled with query terms but no answer text</cell><cell>13</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Answer truncated</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Assessor disagreement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,318.18,754.63,81.91,6.91"><p>https://www.nectar.org.au</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,318.18,764.49,163.96,6.91"><p>https://github.com/TimothyJones/trec-liveqa-server</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,318.18,734.90,129.94,6.91"><p>https://github.com/babilen/wp-download</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,318.18,744.77,125.09,6.91"><p>https://github.com/attardi/wikiextractor</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,318.18,754.63,122.43,6.91"><p>http://www.lemurproject.org/indri/php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="2,318.18,764.49,101.24,6.91"><p>https://github.com/jsc/WANDbl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="3,65.46,727.73,122.21,6.91"><p>http://www.lemurproject.org/indri.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="3,65.46,737.34,223.54,7.71;3,54.00,746.56,234.72,6.91;3,54.00,755.53,235.17,6.91;3,54.00,764.49,146.34,6.91"><p>The values for b and k 1 are different than the defaults reported by Robertson et al.<ref type="bibr" coords="3,110.55,746.56,8.48,6.91" target="#b6">[7]</ref>. These parameter choices were reported for Atire and Lucene in the 2015 IR-Reproducibility Challenge, see github.com/ lintool/IR-Reproducibility for further details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="3,318.18,764.49,169.03,6.91"><p>https://datamarket.azure.com/dataset/bing/searchweb</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment.</head><p>This work was supported in part by the <rs type="funder">Australian Research Council's Discovery Projects Scheme</rs> (<rs type="grantNumber">DP140102655</rs>). <rs type="person">Shane Culpepper</rs> is the recipient of an <rs type="funder">Australian Research Council</rs> <rs type="grantName">DECRA Research Fellowship</rs> (<rs type="grantNumber">DE140100275</rs>). The statements made herein are solely the responsibility of the authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Hf8XR2X">
					<idno type="grant-number">DP140102655</idno>
				</org>
				<org type="funding" xml:id="_ARzPEvd">
					<idno type="grant-number">DE140100275</idno>
					<orgName type="grant-name">DECRA Research Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>pleasantly surprised with the performance of our simple system configurations, we are hopeful that further improvements can still be realized through better filtering steps, better query parsing and prediction, and increasing the coverage in our current documents collections. In summary, we found that retrieving the top-k paragraphs from a local test collection combined with generating a succinct summary of these paragraphs provided the most effective solution. Additionally, we show that the firststage retrieval efficiency cost is dominated by the secondstage re-ranking/summarizing stage. Finally, we show large efficiency improvements compared to our systems from the 2015 LiveQA challenge, which allows us to include more expensive stages in future work.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,323.32,123.20,218.12,8.64;6,323.13,135.15,218.31,8.64;6,323.32,146.93,218.12,8.82;6,323.32,158.88,218.12,8.59;6,323.32,170.84,167.20,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,359.92,135.15,181.52,8.64;6,323.32,147.11,66.18,8.64">Efficient query evaluation using a two-level retrieval process</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,414.46,146.93,126.97,8.59;6,323.32,158.88,218.12,8.59;6,323.32,170.84,41.41,8.59">Proceedings of the 12th ACM international conference on Information &amp; knowledge anagement</title>
		<meeting>the 12th ACM international conference on Information &amp; knowledge anagement</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,323.32,182.97,218.12,8.64;6,323.32,194.93,219.37,8.64;6,323.32,206.88,218.12,8.64;6,323.32,218.66,219.86,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,459.02,206.88,82.42,8.64;6,323.32,218.84,67.03,8.64">RMIT at the TREC 2015 liveqa track</title>
		<author>
			<persName coords=""><forename type="first">Ruey-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Shane</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tadele</forename><surname>Tadela Damessie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Mourad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Falk</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evi</forename><surname>Yulianti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,407.93,218.66,106.07,8.59">Proceedings of TREC 2015</title>
		<meeting>TREC 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,323.32,230.79,219.87,8.64;6,323.32,242.57,218.12,8.82;6,322.97,254.71,58.28,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,323.32,242.75,144.14,8.64">Efficient location-aware web search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,487.82,242.57,21.94,8.59">ADCS</title>
		<imprint>
			<date type="published" when="2015">1-4.8, 2015</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,323.32,266.66,166.64,8.64;6,506.23,266.66,35.21,8.64;6,323.32,278.62,218.13,8.64;6,323.32,290.39,219.37,8.82;6,323.32,302.53,79.47,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,506.23,266.66,35.21,8.64;6,323.32,278.62,218.13,8.64;6,323.32,290.57,56.73,8.64">Machine learned sentence selection strategies for query-biased summarization</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tapas</forename><surname>Kanungo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,399.81,290.39,138.26,8.59">SIGIR Learning to Rank Workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,323.32,312.32,218.12,8.64;6,323.32,324.10,214.28,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,501.19,312.32,40.25,8.64;6,323.32,324.28,81.11,8.64">Exploring the magic of WAND</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,427.25,324.10,21.92,8.59">ADCS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,323.32,336.23,218.12,8.64;6,323.32,348.19,218.12,8.64;6,323.07,359.97,120.31,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,498.97,336.23,42.46,8.64;6,323.32,348.19,200.54,8.64">Score-safe term dependency processing with hybrid indexes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,323.07,359.97,22.82,8.59">SIGIR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="899" to="902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,323.32,372.10,219.78,8.64;6,323.32,383.88,219.87,8.82;6,322.76,395.83,61.05,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,433.06,384.06,68.33,8.64">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,521.99,383.88,21.20,8.59;6,322.76,395.83,30.13,8.59">Proc. TREC-3</title>
		<meeting>TREC-3</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,323.32,407.97,184.24,8.64;6,523.84,407.97,17.60,8.64;6,323.32,419.92,218.12,8.64;6,323.32,431.70,218.12,8.82;6,323.07,443.83,63.26,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,523.84,407.97,17.60,8.64;6,323.32,419.92,218.12,8.64;6,323.32,431.88,98.52,8.64">Text summarization model based on maximum coverage problem and its variant</title>
		<author>
			<persName coords=""><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,447.74,431.70,60.59,8.59">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,323.32,455.79,218.12,8.64;6,323.32,467.74,218.12,8.64;6,323.32,479.70,218.12,8.64;6,323.32,491.47,219.86,8.82;6,323.32,503.43,128.79,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,380.70,479.70,160.74,8.64;6,323.32,491.65,177.35,8.64">Beyond Factoid QA: Effective Methods for Non-factoid Answer Sentence Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damiano</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruey-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Falk</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,521.56,491.47,21.62,8.59;6,323.32,503.43,30.94,8.59">Proc. of ECIR</title>
		<meeting>of ECIR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="115" to="128" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
