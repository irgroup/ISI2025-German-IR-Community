<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.43,80.99,343.15,12.90;1,207.71,98.92,196.59,12.90">Assorted Textual Features and Dynamic Push Strategies for Real-time Tweet Notification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,162.01,133.70,53.28,10.75;1,215.29,132.17,1.41,6.99"><forename type="first">Kathy</forename><surname>Lee</surname></persName>
							<email>kathy.lee1@philips.com</email>
						</author>
						<author>
							<persName coords="1,221.27,133.70,71.88,10.75"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
							<email>ashequl.qadir@philips.com</email>
						</author>
						<author>
							<persName coords="1,301.55,133.70,57.93,10.75"><forename type="first">Vivek</forename><surname>Datla</surname></persName>
							<email>vivek.datla@philips.com</email>
						</author>
						<author>
							<persName coords="1,367.61,133.70,76.46,10.75"><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
							<email>sadid.hasan@philips.com</email>
						</author>
						<author>
							<persName coords="1,194.49,147.76,41.75,10.75"><forename type="first">Joey</forename><surname>Liu</surname></persName>
							<email>joey.liu@philips.com</email>
						</author>
						<author>
							<persName coords="1,244.46,147.76,86.02,10.75;1,330.48,146.22,1.88,6.99"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
							<email>aaditya.prakash@philips.com</email>
						</author>
						<author>
							<persName coords="1,336.46,147.76,81.06,10.75"><forename type="first">Oladimeji</forename><surname>Farri</surname></persName>
							<email>dimeji.farri@philips.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Brandeis University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.43,80.99,343.15,12.90;1,207.71,98.92,196.59,12.90">Assorted Textual Features and Dynamic Push Strategies for Real-time Tweet Notification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">02BC87212DA880195F08EE16F76BD471</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our systems and corresponding results submitted to the Real-Time Summarization (RTS) track at the 2016 Text Retrieval Conference (TREC). The task involved identifying relevant tweets based on a user's interest profile. In Scenario A of the task, tweets relevant to an interest profile were pushed to a live user in real-time. In Scenario B, a daily digest of relevant tweets was sent to a user. We submitted three automatic runs for each scenario. Our overall method for identifying relevant tweets was based on 1) automatically identifying key textual features from a set of interest profiles provided by the Track organizers, 2) expanding the textual phrases with their paraphrases, and 3) exploiting the features for message filtering and relevance measurement after novelty recognition. We experimented with different push strategies to decide when to deliver a message to a user. The evaluation results (by mobile and NIST assessors) show that our system ranked 3rd for Scenario A and 6th for Scenario B.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social media and online blogs are valuable sources of real-time information, but the volume, variety and velocity of generated data make it difficult to find the information one needs. Over the last decade, Twitter has emerged as a form of online social communication and networking platform with global popularity among people who like to write brief status messages and share information with others in these messages. Aided by the widespread adoption of smartphones, information sharing via Twitter has become convenient, instant and frequent.</p><p>Given the limited human cognitive capacity such that we can only consume a small amount of information at any instance, individuals often want to receive only information that is relevant to the topics of interest in real-time. These topics can be specific to an event, location, time, person, product, opinion, experience, etc. For example, someone may be interested in knowing opinions/reviews on a recently launched fitness watch, while another person may be interested in posts describing side effects similar to his/her experience with the same medications. With such specific interests, when new tweets or other social media contents are posted, these messages need to be identified as relevant and pushed to the individuals who wish to be updated on such topics.</p><p>Identifying relevant information is important, but is not the only challenge in this task. If many tweets are sent to a person in a day as relevant, it will likely constitute a cognitive burden for the user. So, the number of messages needs to be kept at a manageable quantity. In addition, the extent of relevance among several qualifying tweets needs to be computed before pushing the top ranked ones to the user, especially because new messages that are more relevant are sometimes generated at a later time in the day. Furthermore, relevant tweets pushed to the user should not be redundant.</p><p>We address all of these challenges in our submitted runs for the Real-Time Summarization (RTS) track at the 2016 Text Retrieval Conference (TREC). To determine the relevance of tweets to a user's interest, we automatically extract various categories of textual features (e.g., named entities, general noun Topic/Profile ID: MB408 Title: amphetamines and ADHD Description: Find tweets that discuss amphetamines and ADHD. Narrative: The user's daughter has been diagnosed with attention deficit hyperactivity disorder (ADHD) and wants to follow the personal experiences of others that have used amphetamines in the treatment of ADHD. Relevant tweets discuss types of amphetamines used, adverse side effects, drug interactions, physical and psychological reactions, addiction, etc.</p><p>Topic/Profile ID: RTS2 Title: Zika in Ecuador Description: Find updates on the current Zika crisis in the country of Ecuador. Narrative: The user has family in Ecuador and wants to see how her family might be affected by the Zika crisis. She's interested in reports of new cases as well as measures being taken to control the outbreak. phrases, phrases within quotations) from the user's interest profile, and expand the textual features using a paraphrase database. We determine the novelty of the tweets by comparing lexical and semantic similarity with those previously pushed to the user. For the push notification strategy, we present three methods that correspond to our runs in Scenario A: 1) strict threshold-based, 2) time-adjusted dynamic threshold-based, and 3) quota-restricted thresholdbased message push. In scenario B, we use additional filtering strategies and relevant tweet identification methods to ensure tweet relevancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Description</head><p>The task in the RTS track required participating teams to monitor the Twitter sample stream and identify relevant tweet posts with respect to a number of "interest profiles". There were two scenarios in the task with regard to when relevant messages will be delivered to a user. The following sections provide further details about the interest profiles and scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Interest Profiles</head><p>The track organizers provided a total of 203 interest profiles for this challenge. Among them, 51 profiles were assessed in the TREC-2015 Microblog Track <ref type="bibr" coords="2,72.00,699.35,75.55,9.46" target="#b3">(Lin et al., 2015)</ref>, 107 profiles were retained from the same track that may still be current in terms of ongoing events and issues around the world, and 45 profiles were newly developed for the TREC-2016 RTS track. Each profile contains four fields that refer to a topic/profile id, title, description, and narrative. The topics or interest profiles are diverse and belong to many domains such as health, politics, sports, etc. Figure <ref type="figure" coords="2,395.08,410.97,5.45,9.46" target="#fig_0">1</ref> presents some example profiles with interests in health related topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scenario A: Push notifications</head><p>In this scenario, when a system identifies a relevant post, the Twitter post is immediately sent to the user's mobile phone via a push notification. The post should be relevant (on topic), timely (provide updates as close to the time of the actual event occurrence as possible), and novel (users should not be pushed multiple notifications that are essentially identical or simply retweets). For evaluation purposes, the pushed tweets are first sent to the TREC RTS evaluation broker (via a REST API), from where the messages are immediately delivered to the mobile phones of a group of assessors (users-in-theloop) according to the platform described by <ref type="bibr" coords="2,510.47,630.89,24.61,9.46;2,313.20,644.43,66.05,9.46" target="#b5">Roegiest et al. (2016)</ref>. The challenge ran for ten days.</p><p>Each participating team at the challenge was allowed to submit up to three runs for scenario A. Each run was only allowed to push up to ten tweets per day per interest profile. We submitted three runs for this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scenario B: Email digest</head><p>For this scenario, a daily digest of relevant tweets is sent to the users at the end of a day. This scenario reflects the case that a user may want to receive a daily email digest that summarizes the facts and conversations that emerged during the day with respect to an interest profile. These messages also need to be relevant and novel, but timeliness is not as important as in scenario A because the digests were not sent until the end of the day. The digests are to include a batch of up to 100 ranked tweets per day per interest profile. Similar to scenario A, each participating team was allowed to submit up to three runs for scenario B. We submitted three runs for this scenario too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Run Categories</head><p>For either scenario of the challenge, there were three categories. In the automatic run category, a system must operate without human intervention (such as, manually judging the quality of query expansion terms) before and during the evaluation period. In the manual preparation category, the system must operate without human input during the evaluation period, but human involvement is acceptable before the evaluation period (e.g., human examination of the interest profiles to add query expansion terms or manual relevance assessment on a related collection to train a classifier). And finally, in the manual intervention category, there were no limitations on human involvement before or during the evaluation period. (e.g., crowd-sourcing judgments, human-inthe-loop search, etc. were allowed). Our submitted runs for both scenarios fall under the automatic run category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Run Descriptions for Scenario A</head><p>Since, for each interest profile, there is a limit imposed on the maximum number of tweets that can be sent to a user, identifying a relevant tweet is necessary but not sufficient. This is because, when a tweet is identified as relevant by the system, at that time it remains unknown whether or not a more relevant tweet will be created later in the day. So, if the daily quota of 10 tweets for an interest profile is used up early on, and later, more relevant tweets are generated, the new tweets cannot be pushed to the user. Conversely, if a relevant tweet is not instantly pushed, there may not be any more relevant tweets generated later in the day.</p><p>Due to the need for instant decision making in scenario A, we implemented the same approach to compute tweet relevancy and novelty across all three runs, but we used different push strategies for each run. Following subsections describe our methods for scenario A runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Textual Features from Interest Profiles</head><p>We first pre-process the interest profiles to extract and expand a number of textual features from the interest profiles as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Text Pre-processing</head><p>To normalize the content in the interest profiles, we initially transform all text to lowercase characters. We use Elasticsearch<ref type="foot" coords="3,425.61,325.27,3.99,6.91" target="#foot_0">1</ref> as our back-end database for indexing the profiles (and also tweets) for faster retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Textual Feature Extraction</head><p>For each interest profile, we extract a number of textual features for measuring tweet relevancy. We use a total of seven categories of textual features. These categories are not mutually exclusive, and a phrase or word from an interest profile may belong to multiple categories. However, each of our textual feature categories captures different types of information focus in the text.</p><p>For extracting phrases, we use the NLTK chunker<ref type="foot" coords="3,327.02,517.79,3.99,6.91" target="#foot_1">2</ref> and the NLTK interface of the Stanford Parser. 3  Tan et al. (2015) argued that texts in title tend to make more influential impact than the description and narrative. Consequently, many of our textual feature categories only consider the texts in profile title, whereas a few of them also consider other fields. Our textual feature categories are:</p><p>• Title words: we extract all unigrams (individual words) from the profile title after excluding stopwords and punctuations.</p><p>• Title phrases: we extract all noun phrases and verb phrases that only appear in the title of an interest profile.</p><p>• Noun phrases: we identify all noun phrases from the title, description and narrative fields of the interest profiles.</p><p>• Phrases within quotations: we extract phrases from title, description, and narrative that appear within quotation marks. Intuitively, phrases within quotation carry special importance, and tweets that mention these phrases exactly, could be highly relevant to the profiles.</p><p>• Named Entity Phrases: we extract phrases that contain a named entity. For extracting named entities, we use the NLTK toolkit.</p><p>• Location Named Entity Phrases: We extract all named entity phrases that mention locations.</p><p>• TF-IDF phrases from narrative: We calculate TF-IDF scores for words in profile narratives, considering each narrative of an interest profile as a document. We take the top 10 words with the highest TF-IDF scores (excluding stopwords), and extract noun phrases and verb phrases that contain one of these high scoring TF-IDF words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Feature Expansion with Paraphrases</head><p>A tweet may contain words or phrases that do not lexically match with the textual features that we extract from the interest profiles. Therefore, we expand the textual feature categories by including paraphrases of the extracted phrases so that phrases that are synonymous can contribute towards measuring relevance. We use the PPDB Paraphrase Database <ref type="bibr" coords="4,114.83,578.31,109.75,9.46" target="#b0">(Ganitkevitch et al., 2013</ref>) (L-size) for the paraphrase-based feature expansion. We do not expand the named Entity phrases and phrases within quotations. For the other four categories, we create four new categories with only the paraphrase terms. After feature expansion, we have a total of 11 categories of textual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identifying Relevant Tweets</head><p>Once the textual features are extracted from the interest profiles and expanded with paraphrases, the next step is to monitor the Twitter feed to identify relevant messages. During this process, we filter out tweets using some constraints and measure their relevance with respect to the interest profiles as detailed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Tweet Filtering</head><p>Twitter feed generates a massive number of tweets daily, and the majority of the tweets will have no relevance to the interest profiles. As we monitor the Twitter feed, we discard all non-English tweets using the language meta-field. For a given interest profile, we discard any tweet that has no common word with the title of the interest profile. We also require that at least half of the title phrases from an interest profile are mentioned in the tweet to give it further considerations.</p><p>Furthermore, if an interest profile has named entities in any of its fields, we require that at least one named entity phrase appears in the tweet. For example, if an interest profile wants to know about armed conflicts in Syria, and a tweet is about an armed conflict which did not take place in Syria, then there is no point in further evaluating the tweet. So for this interest profile, the system would require that the named entity 'Syria' is also mentioned in the tweet. Similarly, if there is a phrase within quotations in the interest profile, any tweet not having the exact same phrase is discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Relevance Measurement</head><p>Once the filtering step is done, to determine tweet relevance with an interest profile, our method looks for the presence of different textual features in the tweet message. Instead of relying only on an exact match with a textual feature, our relevance measurement also takes partial matches into account.</p><p>Let, T = {t i | 1 ≤ i ≤ K} is the set of interest profiles. For each t ∈ T , there are a total of C categories of textual features. Let C i be the set of textual features for the i th textual feature category. For every textual feature category C i of some interest profile t ∈ T , we first calculate a category relevance using the following:</p><formula xml:id="formula_0" coords="4,340.81,673.03,199.19,28.38">relevance(x, C i ) = c∈Ci l 2 c n c × max n (C i )<label>(1)</label></formula><p>In equation ( <ref type="formula" coords="4,381.80,712.90,3.86,9.46" target="#formula_0">1</ref>), x is a tweet, and l c is the maxi-mum number of rightmost words from phrase c that appears in the tweet consecutively and in the same order. The reason for using word sequences that are the rightmost in phrase c is that the extracted textual features are most commonly noun phrases, in which case the rightmost word is typically a head noun and the words left to the head noun are typically noun or adjective modifiers. For example, for the phrase 'subway commuting problem', the word 'problem' is the head noun, and 'subway' and 'commuting' are modifiers. <ref type="foot" coords="5,115.94,209.30,3.99,6.91" target="#foot_3">4</ref>Here, n c is the total number of words in c, and max n (C i ) is the maximum phrase length (in terms of words) among all of the phrases in C i . The category relevance score for the category C i would be 1.0 when the longest phrase in C i would appear in a tweet. That is, for the same example, 'subway commuting problem' is rewarded more over 'commuting problem' for appearing in a tweet, and contributes with the highest score if it is also the longest phrase in its category.</p><p>Once the category relevance score is calculated, we find a weight for each category, and our final score for an interest profile t ∈ T is the weighted sum of the category relevance scores, calculated by:</p><formula xml:id="formula_1" coords="5,78.25,426.08,220.55,39.91">prof ile relevance(x) = C i=1 w i × relevance(x, C i ) (2)</formula><p>All profiles with the profile relevance score above a threshold are then considered as candidate profiles for the tweet. For learning the weights w i , we used tweets with their relevance judgements from the 2015 TREC Microblog track. For this, we use each category of textual features individually, and determine the weight that maximizes the Expected Gain (EG) (described in Section 5.2). We then normalize the weights so that they sum to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Novelty Detection</head><p>Once the relevance of a tweet is established, we ensure that the tweet is novel, i.e., the content of the tweet has new information relative to tweets previously sent to the user for the same interest profile. Similar to last year <ref type="bibr" coords="5,161.44,671.01,87.02,9.46" target="#b1">(Hasan et al., 2015)</ref>, we deter-mine novelty by comparing ordered lexical and semantic overlap of the tweet content with previously sent tweets, using a textual similarity algorithm by <ref type="bibr" coords="5,313.20,116.50,65.79,9.46" target="#b2">Li et al. (2006)</ref>. This algorithm computes pairwise text similarity between two given texts and returns a similarity score between 0 and 1. Similarity score 1 indicates the two text strings are exactly same. We used 0.65 as our threshold. As soon as the system finds a similar tweet with similarity score 0.65 or higher in the already-pushed tweets pool, it discards the tweet (because it is not novel), and, otherwise, the tweet is considered novel and pushed to the user (i.e., no similar tweet have been pushed for the interest profile). For novelty detection, we need to compare a new tweet against a pool of all pushed tweets for an interest profile. Thus the size of this pool increases as our system keeps pushing more tweets and, as a consequence, the novelty detection processing time per tweet also increases with time.</p><p>To reduce this processing time, we ran several instances of the semantic similarity algorithm in parallel on the incoming tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relevant Tweet Delivery</head><p>In scenario A, it is also important to use effective push strategies because the number of tweets that can be sent to a user per day per interest profile is limited. We use three different push strategies for our three runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Strict Threshold-based Push Notification</head><p>For our run 1, we use a simple push strategy that prioritizes how relevant a tweet is to an interest profile, and only pushes a tweet when the system has determined that the message is highly relevant for an interest profile. In this method, to ensure strong relevancy, relevancy measurement threshold is set at 0.75 for all interest profiles. Any tweet for which the system measures a relevancy score of 0.75 or above is greedily pushed to the evaluation broker unless the daily limit of ten tweets per interest profile is already met. In run 1, our system pushed a total of 389 tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Time-adjusted Dynamic Threshold-based Push Notification</head><p>One of the issues with the strict threshold-based method is that it uses a single uniform threshold for all of the interest profiles. But in reality, some topics may be more popular at the present time and many relevant tweets for these topics can be generated throughout the day. For example, tweets about Zika virus may get posted a lot more frequently than tweets about ADHD. Therefore a system can afford to wait for more relevant tweets for a popular topic, but not for a topic that is relatively less popular. Also, a uniform relevancy threshold may not be effective across all interest profiles.</p><p>To address this, in our run 2 for scenario A, we introduce Time-adjusted Dynamic Threshold-based Push Notification method. In this method, the system starts monitoring Twitter feed at the beginning of a day and pushes tweets above a uniform relevance threshold, but re-evaluates its expectations at mid-day for each interest profile individually.</p><p>During the first half of a day, the system runs identically as our method in run 1 that uses a strict uniform 0.75 threshold for all interest profiles. After 12 hours, the system checks what percentage of an interest profile's daily quota (i.e., 10 tweets per day per profile limit) has been met. If, for an interest profile, more than 50% of the daily quota is already met (i.e., #messages pushed ≥ 5), then the system expects that within the remaining time left for the day, it can expect to receive enough relevant tweets to fulfill its daily quota, and thus keeps the 0.75 threshold unchanged.</p><p>However, if the system discovers that, for an interest profile, 50% of the daily quota is yet to be met (i.e., 0 &lt; #messages pushed &lt; 5), then a medium relevance threshold (we use 0.6 as our medium relevance threshold) is set to allow for more tweets to be pushed during the remaining half of the day. If, for some interest profile, the system does not push any tweet during the first half of the day (i.e., #messages pushed = 0), then for these interest profiles, the threshold is lowered further, and a weak relevance threshold is used for the remaining part of the day. We use 0.5 as our weak relevance threshold. In Run 2, our system pushed a total of 2,158 tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Quota-restricted Threshold-based Push Notification</head><p>In our final push strategy in run 3 of the scenario A, we employ a Quota-restricted Threshold-based Push method. In this method, a tweet is pushed when its relevance score is above a weak relevance threshold (we use 0.5 in this case), but only until 50% of the quota is met. The remaining 50% is always reserved for tweets with a strong relevance threshold (we use 0.75 in this case). So, in this method, not more than five tweets can be pushed for an interest profile on a day that are within a relevance threshold range of 0.5 to 0.75, but up to 10 tweets can be pushed if they all have relevance score above 0.75. In Run 3, our system pushed a total of 1,506 tweets.</p><p>The thresholds for different push strategies were determined from the relevance scores that the system assigns on the 2015 TREC Microblog challenge data and the Expected Gain (EG) scores these thresholds obtain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System Description for Scenario B</head><p>The challenges in scenario B are fundamentally different from the challenges in scenario A. In scenario B, the uncertainty that there may be more relevant tweets generated at a later time during the day is not considered, because all of the tweets for the day are already posted in Twitter. The challenge is to find tweets that are most relevant for a given interest profile, among all monitored tweets for the day. Therefore for scenario B, we shift our focus from applying different push strategies to identifying relevant tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Daily Digest for Run 1</head><p>In run 1 of scenario B, we use a distinct tweet filtering strategy. For identifying messages that are relevant to an interest profile, we still require that at least one title word, or one named entity or phrase within quotation (if they are in the interest profile) must appear in the tweet, but rescind on the constraint that at least half of the title phrases must also appear in a relevant tweet. It is possible that in some cases, the number of important title phrases may account for more than half of the title phrases. On the other hand, a tweet may mention a paraphrase of a title phrase, or part of a title phrase may appear as a synonym in the tweet. For the latter, looking for exact matches would be limiting.</p><p>For each title phrase of an interest profile, we create a cluster of text alternatives (title phrase inclu-sive). Each text alternative can be further divided into one or more internal text blocks. These text alternatives are typically synonyms or paraphrases, but since it is not always possible to find paraphrases or synonyms of long and specific phrases, the phrases are divided into internal text blocks. We require that at least one members from each cluster of text alternatives must be present in the tweet message for the tweet to be relevant for the interest profile. A text alternative is considered present in the tweet if a member from each internal text block (that forms a text alternative) is also present in the tweet.</p><p>To illustrate this with an example, let's consider the title phrase "subway commuting problems" (from topic: MB299). Our basic idea here is that all components of this phrase must be present in a tweet for the tweet to be relevant for this profile. First, we need to find a set of alternative representations of this phrase. These may include: "issues arising from subway commute" or "problems of subway travel". We use the PPDB (large subset) paraphrase corpus to build our cluster of text alternatives. If such alternative phrases exist in the paraphrase database, then we consider these paraphrases as the members of the cluster, where each member has exactly one text block (i.e., the entire phrase).</p><p>However, being able to directly find such paraphrases is not always practical as the coverage of existing paraphrase databases are not extensive. In some cases, the original phrase may be too specific or too long to have its paraphrases in a database. Therefore, we break the phrase into smaller chunks to create a number of text blocks.</p><p>Our phrasal chunking is done by incrementally removing the leftmost words from the phrase, and building a text block for each that contains the word and its synonyms/paraphrases. This is because many of the meaningful title phrases are noun phrases, in which case the rightmost word tends to be the head noun, whereas the leftmost words typically play the role of noun or adjective modifiers. For the example above, the system would first look up the exact title phrase in the paraphrase database. If it does not exist in the database, then we create two subphrases: "subway" and "commuting problems" as text blocks. We then look for paraphrases of the two text blocks. The word subway and any para-phrase of subway (such as train, tube, etc.) forms one text block, and "commuting problems" and any paraphrase of "commuting problems" form a second text block. We require that at least one member of each text block must be present in the tweet.</p><p>If "commuting problems" does not have any paraphrase in the database, then we further chunk the phrase into "commuting" and "problems" and retrieve the corresponding paraphrases. We perform the phrasal chunking for all title phrases. Together all the text blocks represent the text alternatives for "subway commuting problem".</p><p>We use the same relevance measurement method used in scenario A, and rank all tweets for a day based on the relevance score. We lower the relevance threshold to 0.2 because the filtering step already enforces a stronger relevance criteria on the tweet. The top 100 tweets are then retained to prepare a daily digest for each interest profile. In case two tweets have the same relevance score, we push the lengthier tweet based on the assumption that more words are likely to covey more description and meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Daily Digest for Run 2</head><p>In run 2, we used four types of textual features: noun phrases, title phrases, named entity phrases, and location named entity phrases. We computed the optimal weights for each textual feature category that maximizes the overall EG using the TREC2015 Microblog evaluation topics, tweets and scores.</p><p>At the end of each day, for each interest profile, we search the entire tweets indexed for that day that contain phrases in at least one of the four aforementioned categories. Then the relevancy score is computed for each tweet extracted using the weights of the phrase/textual feature category that was used to filter the tweet. For each interest profile, we rank the tweets by the relevancy score and the tweet length (in case two or more tweets have the same relevancy score) and send daily digest of up to 100 tweets per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Daily Digest for Run 3</head><p>In run 3, we combined all identified tweets from the previous runs in scenarios A and B. For each interest profile, we take the tweets from scenario A runs that exceed a relevance score threshold 0.3, in addition to tweets from run 1 and run 2 of scenario B. We then sort the collection by the relevance score of the tweets and keep the top 100 ranked tweets for that interest profile as the daily digest for the day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Methods</head><p>In the RTS track, the organizers provided two types of evaluation: 1) live user-in-the-loop assessments, and 2) post hoc batch evaluation.</p><p>In the live user-in-the-loop assessments, tweets submitted by the participating systems to the RTS evaluation broker were immediately routed to the mobile phone of an assessor. Each tweet was rendered as a push notification containing the text of the tweet and the corresponding interest profile. The assessors then could judge the tweets as relevant, relevant but redundant (on topic, but contains information conveyed previously), not relevant, or could ignore and leave unjudged. This evaluation follows the framework described in <ref type="bibr" coords="8,194.01,363.05,91.95,9.46" target="#b5">Roegiest et al. (2016)</ref>.</p><p>In contrast to live user-in-the-loop assessments, the post hoc batch evaluation method evaluates tweets from scenario A and scenario B submissions at the end of the competition. The tweets are judged as not-relevant, relevant, or highly relevant. Relevant tweets are then semantically clustered into groups and on retrieving one tweet from a cluster and determining that it is relevant, all other tweets from the same cluster are considered as not relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>Tweets pushed in scenario A are evaluated with several evaluation metrics. Expected gain (EG) (for an interest profile on a particular day) is defined as follows:</p><formula xml:id="formula_2" coords="8,143.13,596.38,155.67,24.43">EG = 1 N G(t)<label>(3)</label></formula><p>where N is the number of tweets submitted by a system and G(t) is the gain of each tweet, where highly-relevant, relevant and not relevant tweets receive a gain 1.0, 0.5 and 0 respectively.</p><p>Normalized Cumulative Gain (nCG) (for an interest profile on a particular day) is defined as follows:</p><formula xml:id="formula_3" coords="8,381.97,83.53,158.03,24.43">nCG = 1 Z G(t) (4)</formula><p>where Z is the maximum possible gain (given the ten tweets per day limit).</p><p>Both metrics have two variations each. On a day when there are no relevant tweets for a particular interest profile (silent day), a system receives a score of one (i.e., perfect score) if it does not push any tweet, or zero otherwise, in the EG-1 and nCG-1 metrics. Thus, systems are rewarded for recognizing that there are no relevant tweets for an interest profile on a particular day, and remaining silent (i.e., not pushing any tweet). In the EG-0 and nCG-0 variants of the metrics, for a silent day, all systems receive a gain of zero.</p><p>The last evaluation metric is Gain Minus Pain (GMP), defined as follows:</p><formula xml:id="formula_4" coords="8,355.69,328.98,184.31,9.81">GM P = α × G -(1 -α) × P (5)</formula><p>Here G (gain) is computed in the same manner as above; and P (pain) is the number of non-relevant tweets that are pushed, and controls the balance between the two. Evaluations are done at three α settings: 0.33, 0.5, and 0.66.</p><p>For scenario B runs, Normalized Discounted Cumulative Gain nDCG@10 is used as the evaluation metrics with two variants as before. nDCG@10-1 rewards a system for not pushing tweets on a silent day when there are no relevant tweets, and nDCG@10-0 does not reward for not pushing tweets on a silent day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>For benchmarking the results, the organizers provided a baseline system called YoGosling, which is a simpler version of the UWaterloo system from the 2015 TREC Microblog track <ref type="bibr" coords="8,441.68,577.41,72.59,9.46" target="#b7">(Tan et al., 2016)</ref>.</p><p>Table <ref type="table" coords="8,351.32,590.96,5.45,9.46">1</ref> presents our scenario A results with post hoc batch evaluation. Among our three runs, run 1 (Strict Threshold-based Push Notification) performed better than the other two runs for EG1, nCG1, and all of the GMP metrics. Run 1 also outperformed the YoGosling baseline results and was the 5th best system among all automatic runs (this includes multiple runs by the same teams as some of the top runs were from the same teams). The results for the EG0 and nCG0 metrics, that do not reward Table 1: Scenario A batch evaluation results. EG=Expected Gain (1 = with silent day reward, 0 = no silent day reward), nCG1 = Normalized Cumulative Gain (1 = with silent day reward, 0 = no silent day reward), GMP = Gain Minus Pain (at α = 0.33, 0.5 and 0.66). for not pushing tweets on a silent day, were very low for all of our runs as well as for the YoGosling baseline. This indicates that for many of the interest profiles, there were very few or no relevant tweets generated for the duration of the competition. While our run 2 and 3 did not perform as good as our run 1, they still outperformed the YoGosling baseline on the EG1 and all of the GMP metrics, and were comparable to the nCG1 results of the YoGosling baseline. Table <ref type="table" coords="9,109.87,523.21,5.45,9.46" target="#tab_1">2</ref> shows the mean and median time latency for our runs and the baseline. Our run 1 took significantly less time to push a tweet. The main time bottleneck in our system was the computations for novelty detection, which can be improved significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>Table <ref type="table" coords="9,109.69,590.96,5.45,9.46" target="#tab_2">3</ref> shows the evaluation results for live userin-the-loop assessments. We report the ratio of relevant and redundant tweets pushed by our system over the judged tweets. It should be noted that this result includes tweets which had multiple judgements, as no adjudications were performed. The table also shows the percentage of tweets that had multiple judgements.</p><p>Overall estimates from the live user-in-the-loop judged samples suggest that our run 3 (Quota- The percentages of relevant tweets for all of our runs were better than the YoGosling baseline based on the assessed samples. Among our three runs, 293 tweets were sent for live user assessment in run 1, whereas 1,640 and  <ref type="bibr" coords="10,368.89,101.89,66.04,8.64" target="#b4">(Lin et al., 2016)</ref>. Columns show the rank, team name, the best run by each team, "strict" and "lenient" precision. 1,134 tweets were sent for judgement in run 2 and run 3. 5 Among our three runs, in run 1 we pushed significantly fewer number of tweets compared to run 2 and 3. In the batch evaluation (Table <ref type="table" coords="11,269.78,116.50,3.94,9.46">1</ref>), the metrics were averaged across the days of the competition and also across the topics (comparable to macro average). Hence it is likely that because of pushing fewer tweets, the system received more silent day rewards in run 1, and yielded better results for EG1, nCG1 and GMP; this is also supported by the lower scores that run 1 received with the EG0 and nCG0 evaluation metrics. On the other hand, with the provided statistics from the live user-inthe-loop assessment (Table <ref type="table" coords="11,195.87,252.00,3.94,9.46" target="#tab_2">3</ref>), we calculated overall percentage of relevant tweets (among the judged tweets) across all topics and all days of the competition (comparable to micro average), and found that we pushed a slightly higher percentage of relevant tweets in run 3 when compared to our other two runs.</p><p>Finally, Table <ref type="table" coords="11,150.03,347.83,5.45,9.46">4</ref> shows results for our scenario B runs, using nDCG1 and nDCG0 evaluation metrics. For nDCG1, our run 1 results are comparable to the YoGosling baseline results, but did not outperform. For nDGCG0, our run 3 outperformed the YoGosling baseline results, but these metrics yield very low scores, as many of the days did not have any relevant tweet for many of the interest profiles.</p><p>Tables 5-7 highlights the results for Scenario A (Evaluation by mobile and NIST assessors) and Scenario B (Evaluation by NIST assessors) in the automatic run category <ref type="bibr" coords="11,158.61,497.87,74.09,9.46" target="#b4">(Lin et al., 2016)</ref>. As shown in these tables, we ranked 3rd for Scenario A and 6th for Scenario B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we described our system runs for the RTS track at the TREC 2016. We submitted a total of six runs: three runs in each scenario of the challenge, under the automatic submission category. The evaluation results show that our system ranked 3rd for Scenario A and 6th for Scenario B, which demonstrates that our system using assorted textual 5 The number of tweets sent for assessment is lower than the actual number of tweets pushed in each run because only a subset of the interest profiles were evaluated instead of all of 203 interest profiles features and dynamic push strategies is highly effective in finding relevant and novel tweets for an interest profile in real-time. For future work, we will explore identifying different sub-types of interests in the profiles (e.g., opinion, experience, news) and personalized relevance modeling of the interest profiles.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,183.26,297.25,245.48,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of interest profiles (health related topics).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,97.33,399.73,176.15,104.77"><head>Table 2 :</head><label>2</label><figDesc>Time lantency in Scenario A</figDesc><table coords="9,97.33,412.15,176.15,92.35"><row><cell>latency (seconds)</cell><cell>mean</cell><cell>median</cell></row><row><cell cols="3">YoGosling Baseline System</cell></row><row><cell>YoGosling Run</cell><cell cols="2">120,908.6 8,718.0</cell></row><row><cell cols="2">PRNA System</cell><cell></cell></row><row><cell>PRNA Run 1</cell><cell>81,480.3</cell><cell>317.0</cell></row><row><cell>PRNA Run 2</cell><cell>120,734.6</cell><cell>210.0</cell></row><row><cell>PRNA Run 3</cell><cell cols="2">172,795.8 3,321.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,313.20,256.41,226.80,409.22"><head>Table 3 :</head><label>3</label><figDesc>Evaluation by human assessors in scenario A.</figDesc><table coords="9,313.20,268.33,226.80,397.30"><row><cell></cell><cell>#Relevant</cell><cell>#Redundant</cell><cell>#Multi</cell></row><row><cell></cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell></cell><cell>#Judged</cell><cell>#Judged</cell><cell>#Judged</cell></row><row><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell></cell><cell cols="2">YoGosling Baseline System</cell><cell></cell></row><row><cell>YoGosling</cell><cell>33.18</cell><cell>2.69</cell><cell>(4.26)</cell></row><row><cell></cell><cell cols="2">PRNA System</cell><cell></cell></row><row><cell>PRNA R1</cell><cell>37.14</cell><cell>0.00</cell><cell>(7.14)</cell></row><row><cell>PRNA R2</cell><cell>37.14</cell><cell>2.22</cell><cell>(3.81)</cell></row><row><cell>PRNA R3</cell><cell>38.83</cell><cell>4.85</cell><cell>(3.88)</cell></row><row><cell cols="4">Table 4: Scenario B evaluation batch results. nDCG =</cell></row><row><cell cols="4">Normalized Discounted Cumulative Gain (1 = with silent</cell></row><row><cell cols="3">day reward, 0 = no silent day reward).</cell><cell></cell></row><row><cell cols="2">Evaluation Metrics</cell><cell cols="2">nDCG1 nDCG0</cell></row><row><cell cols="3">YoGosling Baseline System</cell><cell></cell></row><row><cell cols="4">YoGosling Run 0.2352 0.0299</cell></row><row><cell></cell><cell cols="2">PRNA System</cell><cell></cell></row><row><cell cols="2">PRNA Run 1</cell><cell cols="2">0.2334 0.0352</cell></row><row><cell cols="2">PRNA Run 2</cell><cell cols="2">0.2244 0.0226</cell></row><row><cell cols="2">PRNA Run 3</cell><cell cols="2">0.1987 0.0665</cell></row><row><cell cols="4">restricted Threshold-based Push Notification) per-</cell></row><row><cell cols="4">formed best among our submissions, and outper-</cell></row><row><cell cols="4">formed the Yogosling baseline results by pushing</cell></row><row><cell cols="4">5.65% more relevant tweets, although percentage of</cell></row><row><cell cols="4">redundant tweets pushed in this run was relatively</cell></row><row><cell>more.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,72.00,101.89,294.21,8.64"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of scenario A automatic runs by the mobile assessors</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,72.00,113.84,468.00,369.36"><head>Table 6 :</head><label>6</label><figDesc>Table shows top 10 teams ranked by P(strict) from their best automatic runs. Evaluation of scenario A automatic runs by the NIST assessors<ref type="bibr" coords="10,363.08,327.15,65.97,8.64" target="#b4">(Lin et al., 2016)</ref>. Table shows top 10 teams ranked by EG-1 from their best automatic runs.</figDesc><table coords="10,140.93,135.67,330.14,347.53"><row><cell cols="2">Rank</cell><cell>Team</cell><cell>Best Run</cell><cell></cell><cell>P (strict) P (lenient)</cell></row><row><cell>1</cell><cell></cell><cell>CLIP</cell><cell>CLIP-A-1-08</cell><cell></cell><cell>0.5028</cell><cell>0.5083</cell></row><row><cell>2</cell><cell></cell><cell>umd hcil</cell><cell cols="2">UmdHcilBaseline-49</cell><cell>0.4762</cell><cell>0.4762</cell></row><row><cell>3</cell><cell></cell><cell>prna</cell><cell>PRNATaskA3-36</cell><cell></cell><cell>0.3883</cell><cell>0.4369</cell></row><row><cell>4</cell><cell></cell><cell>IRIT</cell><cell>iritRunBiAm-21</cell><cell></cell><cell>0.3792</cell><cell>0.3906</cell></row><row><cell>5</cell><cell></cell><cell>PKUICST</cell><cell>run2-32</cell><cell></cell><cell>0.3769</cell><cell>0.4015</cell></row><row><cell>6</cell><cell></cell><cell>QU</cell><cell>QUExpP-38</cell><cell></cell><cell>0.3689</cell><cell>0.3770</cell></row><row><cell>7</cell><cell></cell><cell cols="2">WaterlooClarke WaterlooBaseline-50</cell><cell></cell><cell>0.3318</cell><cell>0.3587</cell></row><row><cell>8</cell><cell></cell><cell>ISIKol</cell><cell>MyBaseline-24</cell><cell></cell><cell>0.3189</cell><cell>0.3501</cell></row><row><cell>9</cell><cell></cell><cell>WaterlooLin</cell><cell>WaterlooBaseline-51</cell><cell></cell><cell>0.3180</cell><cell>0.3355</cell></row><row><cell cols="2">10</cell><cell>NUDTSNA</cell><cell>nudt sna-29</cell><cell></cell><cell>0.3112</cell><cell>0.3515</cell></row><row><cell>Rank</cell><cell></cell><cell>Team</cell><cell>Best Run</cell><cell></cell><cell>EG-1</cell><cell>nCG-1 GMP(.50)</cell></row><row><cell>1</cell><cell></cell><cell>QU</cell><cell>QUBaseline-37</cell><cell cols="2">0.2643 0.2479</cell><cell>-0.0888</cell></row><row><cell>2</cell><cell></cell><cell>IRIT</cell><cell>iritRunBiAm-21</cell><cell cols="2">0.2493 0.2541</cell><cell>-0.3817</cell></row><row><cell>3</cell><cell></cell><cell>prna</cell><cell>PRNABaseline-34</cell><cell cols="2">0.2423 0.2402</cell><cell>-0.0522</cell></row><row><cell>4</cell><cell></cell><cell>CLIP</cell><cell>CLIP-A-2-09</cell><cell cols="2">0.2407 0.2382</cell><cell>-0.1656</cell></row><row><cell>5</cell><cell></cell><cell>NUDTSNA</cell><cell>nudt sna-30</cell><cell cols="2">0.2392 0.2417</cell><cell>-0.3067</cell></row><row><cell>6</cell><cell></cell><cell>PKUICST</cell><cell>run2-32</cell><cell cols="2">0.2347 0.2433</cell><cell>-0.5183</cell></row><row><cell>7</cell><cell cols="2">DPLAB IITBHU</cell><cell>iitbhu-15</cell><cell cols="2">0.2339 0.2339</cell><cell>0.0000</cell></row><row><cell>8</cell><cell></cell><cell>QUT RTS</cell><cell>QUT RTS-40</cell><cell cols="2">0.2315 0.2306</cell><cell>-0.0509</cell></row><row><cell>9</cell><cell></cell><cell>WaterlooLin</cell><cell cols="3">WaterlooBaseline-51 0.2298 0.2315</cell><cell>-0.4165</cell></row><row><cell>10</cell><cell cols="2">WaterlooClarke</cell><cell cols="3">WaterlooBaseline-50 0.2289 0.2330</cell><cell>-0.4317</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,72.00,542.50,468.00,156.05"><head>Table 7 :</head><label>7</label><figDesc>Evaluation of scenario B automatic runs by the NIST assessors<ref type="bibr" coords="10,362.84,542.50,66.04,8.64" target="#b4">(Lin et al., 2016)</ref>. Table shows top 10 teams ranked by nDCG-1 from their best automatic runs.</figDesc><table coords="10,167.18,566.38,277.64,132.17"><row><cell>Rank</cell><cell>Team</cell><cell>Best Run</cell><cell cols="2">nDCG-1 nDCG-0</cell></row><row><cell>1</cell><cell>NUDTSNA</cell><cell>nudt sna</cell><cell>0.2708</cell><cell>0.0529</cell></row><row><cell>2</cell><cell>QU</cell><cell>QUJM16</cell><cell>0.2621</cell><cell>0.0300</cell></row><row><cell>3</cell><cell>IRIT</cell><cell>RunBIch</cell><cell>0.2481</cell><cell>0.0321</cell></row><row><cell>4</cell><cell>WaterlooLin</cell><cell>YoGoslingBSL</cell><cell>0.2352</cell><cell>0.0299</cell></row><row><cell>5</cell><cell>PKUICST</cell><cell>PKUICSTRunB3</cell><cell>0.2348</cell><cell>0.0151</cell></row><row><cell>6</cell><cell>prna</cell><cell>PRNATaskB1</cell><cell>0.2334</cell><cell>0.0352</cell></row><row><cell>7</cell><cell>ISIKol</cell><cell>isikol tag</cell><cell>0.2213</cell><cell>0.0196</cell></row><row><cell>8</cell><cell>udel</cell><cell>udelRunBM25B</cell><cell>0.2151</cell><cell>0.0008</cell></row><row><cell>9</cell><cell>IRLAB DA IICT</cell><cell>IRLAB2</cell><cell>0.1972</cell><cell>0.0169</cell></row><row><cell>10</cell><cell>CCNU2016NLP</cell><cell>CCNUNLPrun1</cell><cell>0.1732</cell><cell>0.0018</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,329.34,680.86,161.29,7.77"><p>https://www.elastic.co/products/elasticsearch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,329.34,692.03,72.99,7.77"><p>http://www.nltk.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,329.34,703.20,166.64,7.77;3,313.20,714.16,58.77,7.77"><p>http://www.nltk.org/api/nltk.tag.html#modulenltk.tag.stanford</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,88.14,692.24,210.66,7.77;5,72.00,703.20,226.80,7.77;5,72.00,714.16,49.31,7.77"><p>Although, some of the textual features we extract are also verb phrases, in this work we mainly focus on matching the noun phrases.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,313.20,211.32,226.80,8.64;11,324.11,223.27,63.37,8.64;11,403.11,223.27,22.42,8.64;11,441.16,223.27,98.84,8.64;11,324.11,235.06,215.89,8.81;11,324.11,247.18,215.89,8.64;11,324.11,259.14,79.15,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,441.16,223.27,98.84,8.64;11,324.11,235.23,32.70,8.64">PPDB: The paraphrase database</title>
		<author>
			<persName coords=""><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,375.65,235.06,111.15,8.58">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06">2013. June</date>
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,313.20,271.84,226.80,8.64;11,324.11,283.80,215.89,8.64;11,324.11,295.58,215.89,8.81;11,324.11,307.54,215.89,8.58;11,324.11,319.49,139.91,8.58" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,507.34,271.84,32.66,8.64;11,324.11,283.80,215.89,8.64;11,324.11,295.75,14.39,8.64">Exploiting Neural Embeddings for Social Media Data Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,362.08,295.58,177.92,8.58;11,324.11,307.54,72.38,8.58">Proceedings of the Twenty-Fourth Text REtrieval Conference</title>
		<meeting>the Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11-17">2015. 2015. November 17-20, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,313.20,332.36,226.80,8.64;11,324.11,344.32,215.89,8.64;11,324.11,356.27,215.89,8.64;11,324.11,368.06,215.89,8.58;11,324.11,380.02,89.38,8.81" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,482.55,344.32,57.44,8.64;11,324.11,356.27,212.54,8.64">Sentence similarity based on semantic nets and corpus statistics</title>
		<author>
			<persName coords=""><forename type="first">Yuhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zuhair</forename><forename type="middle">A</forename><surname>Bandar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keeley</forename><surname>O'shea</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Crockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,324.11,368.06,215.89,8.58;11,324.11,380.02,11.42,8.58">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1138" to="1150" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,313.20,392.89,226.80,8.64;11,324.11,404.84,215.89,8.64;11,324.11,416.80,139.58,8.64" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,356.57,404.84,179.70,8.64">Overview of the trec-2015 microblog track</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="11,313.20,429.50,226.80,8.64;11,324.11,441.45,215.89,8.64;11,324.11,453.41,215.89,8.64;11,324.11,465.37,165.57,8.64" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,324.11,453.41,215.89,8.64;11,324.11,465.37,18.67,8.64">Overview of the TREC 2016 real-time summarization track</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Mc-Creadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="11,313.20,478.07,226.80,8.64;11,324.11,490.02,215.89,8.64;11,324.11,501.81,215.89,8.81;11,324.11,513.76,215.89,8.58;11,324.11,525.72,215.89,8.81;11,324.11,537.84,49.69,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,382.91,490.02,157.09,8.64;11,324.11,501.98,99.08,8.64">A platform for streaming push notifications to mobile assessors</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles La</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,443.84,501.81,96.16,8.58;11,324.11,513.76,215.89,8.58;11,324.11,525.72,155.48,8.58">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1077" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,313.20,550.55,226.80,8.64;11,324.11,562.50,215.89,8.64;11,324.11,574.29,215.89,8.81;11,324.11,586.24,215.89,8.58;11,324.11,598.20,139.91,8.58" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,350.21,562.50,189.79,8.64;11,324.11,574.46,18.67,8.64">University of waterloo at TREC 2015 microblog track</title>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles La</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,363.21,574.29,176.79,8.58;11,324.11,586.24,72.38,8.58">Proceedings of The Twenty-Fourth Text REtrieval Conference</title>
		<meeting>The Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11-17">2015. 2015. November 17-20, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,313.20,611.07,226.80,8.64;11,324.11,623.02,215.89,8.64;11,324.11,634.81,215.89,8.81;11,324.11,646.77,215.89,8.58;11,324.11,658.72,215.89,8.81;11,324.11,670.84,49.69,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,406.77,623.02,133.23,8.64;11,324.11,634.98,105.38,8.64">Simple dynamic emission strategies for microblog filtering</title>
		<author>
			<persName coords=""><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,446.89,634.81,93.11,8.58;11,324.11,646.77,215.89,8.58;11,324.11,658.72,155.48,8.58">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1009" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
