<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.03,72.35,253.64,16.84;1,174.91,92.27,259.88,16.84;1,264.72,129.03,80.27,12.90">An Exploration of Total Recall with Multiple Manual Seedings Final Version</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,120.03,157.90,81.29,11.06"><forename type="first">Jeremy</forename><surname>Pickens</surname></persName>
							<email>jpickens@catalystsecure.com</email>
							<affiliation key="aff0">
								<orgName type="department">Catalyst Repository Systems</orgName>
								<address>
									<addrLine>1860 Blake Street, 7th Floor Denver</addrLine>
									<postCode>80202</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.19,157.90,57.48,11.06"><forename type="first">Tom</forename><surname>Gricks</surname></persName>
							<email>tgricks@catalystsecure.com</email>
							<affiliation key="aff0">
								<orgName type="department">Catalyst Repository Systems</orgName>
								<address>
									<addrLine>1860 Blake Street, 7th Floor Denver</addrLine>
									<postCode>80202</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.27,157.90,56.78,11.06"><forename type="first">Bayu</forename><surname>Hardi</surname></persName>
							<email>bhardi@catalystsecure.com</email>
							<affiliation key="aff0">
								<orgName type="department">Catalyst Repository Systems</orgName>
								<address>
									<addrLine>1860 Blake Street, 7th Floor Denver</addrLine>
									<postCode>80202</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.69,157.90,52.39,11.06"><forename type="first">Mark</forename><surname>Noel</surname></persName>
							<email>mnoel@catalystsecure.com</email>
							<affiliation key="aff0">
								<orgName type="department">Catalyst Repository Systems</orgName>
								<address>
									<addrLine>1860 Blake Street, 7th Floor Denver</addrLine>
									<postCode>80202</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,402.98,157.90,86.71,11.06"><forename type="first">John</forename><surname>Tredennick</surname></persName>
							<email>jtredennick@catalystsecure.com</email>
							<affiliation key="aff0">
								<orgName type="department">Catalyst Repository Systems</orgName>
								<address>
									<addrLine>1860 Blake Street, 7th Floor Denver</addrLine>
									<postCode>80202</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.03,72.35,253.64,16.84;1,174.91,92.27,259.88,16.84;1,264.72,129.03,80.27,12.90">An Exploration of Total Recall with Multiple Manual Seedings Final Version</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">107FF6A4752791AB5B8AFB520A021D08</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Catalyst participation in the manual at home Total Recall Track had one fundamental question at the core of its run: What effect various kinds of limited human effort have on a total recall process. Our two primary modes were one-shot (single query) and interactive (multiple queries).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">OFFICIAL RUN</head><p>Our official run approach consisted of two main parts:</p><p>1. Manual selection of a minimal number of initial documents (seeding)</p><p>2. Continuous learning with active learning as relevance feedback, only</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Manual seeding</head><p>Four different reviewers participated in every topic. For each topic, a reviewer was assigned to manually seed that topic either by doing a single query (one-shot) and flagging (reviewing) the first (i.e. not necessarily the best) 25 documents returned by the query, or run as many queries (interactive) as desired within a short time period, but stop after 25 documents had been reviewed. In the one-shot approach, the reviewer is not allowed to examine any documents before issuing the query, i.e. the single query is issued "blind" after only reading the topic title and description. The first 25 documents returned by that query are flagged as having been reviewed, but because there is no further interaction with the system, it does not matter whether or not the reviewer spends any time looking at those documents. In the interactive case, reviewers were free to read documents in as much or little depth as they wished, issue as many or as few queries as they wished, and use whatever affordances were available to them from the system to find documents (e.g. synonym expansion, timeline views, communication tracking views, etc.)</p><p>Every document that the reviewer laid eyeballs on during this interactive period had to be flagged as having been seen and submitted to the Total Recall server, whether or not the reviewer believed the document to be relevant. This was of course done in order to correctly assess total effort, and therefore correctly measure gain (recall as a function of effort). We also note that the software did not strictly enforce the 25 document guideline. As a result, sometimes the interactive reviewers went a few documents over their 25 document limit and sometimes they went a few documents under, as per natural human variance and mistake, but we do not consider this to be significant. Regardless, all documents reviewed, even with duplication, were noted and sent to the Total Recall server.</p><p>The reviewers working on each topic were randomized, assigned to run each topic either in one-shot or in interactive mode. Each topic had two one-shot and two interactive 25-document starting points. For our one allowed official manual run, these starting points were combined (unioned) into a separate starting point. Because we did not control for overlap or duplication of effort, the union of these reviewed documents is often smaller than the sum. Reviewers working asynchronously and without knowledge of each other often found (flagged as seen) the same exact documents.</p><p>In this paper, we augment the official run with a number of unofficial runs, four for each topic, two one-shot starting points and two interactive starting points. This will be discussed further in Section 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Continuous Learning, Active Learning</head><p>In more consumer facing variations of our technology, multiple forms of active learning such as explicit diversification are used together in a continuous setting. As the purpose of those additional types of active learning are to balance out and augment human effort, we turned them off for this experiment in order to be able to more clearly assess the effects of just the manual seeding. Therefore, in this experiment, we do continuous learning with relevance feedback as the only active document selection mechanism <ref type="bibr" coords="1,506.62,523.43,9.21,7.86" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OFFICIAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gain Curves</head><p>Figure <ref type="figure" coords="1,354.60,585.66,4.61,7.86">1</ref> is a plot of the official results in the form of gain curves over the relevant documents produced by averaging performance across all 34 athome4 topics. The plots on the left have the entire collection along the x-axis, and the plots on the right are narrowed down to the more interesting top few thousand. There are two BMI baselines: Topic title only (red) and title+description (blue), with our method given in black. Figure <ref type="figure" coords="1,376.01,658.88,4.61,7.86">2</ref> is a plot of the official results in the form of gain curves over the highly relevant documents, similarly characterized.</p><p>There is not much to say about these results other than, on average, we come close but do not beat the baseline. The difference at 25% and 50% recall is small -only an extra 38 0 0.58 1.16 1.74 2.32 2.9 documents are needed on average with our technique to get to 25% recall, and an extra 165 documents on average to get to 50% recall. The difference grows at 85% recall, with an extra 2692 documents needed. This is not a huge difference relative to all 290,000 documents in the collection, but is a non-zero difference nonetheless. We have some hypotheses related to this gap, but the purpose of this work is not to focus on comparisons to other methods, but on the effects of a variety of manual seeding approaches on our own methods.</p><formula xml:id="formula_0" coords="2,276.19,184.12,3.93,7.86">â€¢</formula><p>However, we do note one interesting aspect of the results in Figures <ref type="figure" coords="2,97.80,547.90,4.61,7.86">1</ref> and<ref type="figure" coords="2,123.96,547.90,3.58,7.86">2</ref>. And that's the fact that our technique seems to do better relative to the baseline on the highly relevant documents than it does relative to the baseline on the standard relevant documents. We have attempted to quantify that narrowing gap in the following manner: First, at all levels of recall in 5% increments, we calculate the difference in precision (d rel ) between our technique and each baseline for the relevant documents. Second, we calculate this difference in precision (d high ) between the techniques for the highly relevant documents. Then we calculate how much smaller d high is than d rel at each of these recall points, expressed as a percentage difference, using the following formula:</p><formula xml:id="formula_1" coords="2,136.24,697.59,74.22,20.24">|d rel -d high | 0.5 * (d rel + d high )</formula><p>The result is shown in Figure <ref type="figure" coords="2,442.14,443.29,3.58,7.86">3</ref>. A negative value indicates that our highly relevant result is closer to the highly relevant baseline than our regular relevant result is to the relevant baseline. Across most recall points, the highly relevant result is between 100% and 300% closer to the baseline than is the regular relevant result.</p><p>However, while this is an interesting pattern in the data, it is difficult to know how to interpret this. Does the gap narrow on the highly relevant results because the highly relevant documents are also the more findable ones, and there is not a big difference between any reasonable technique? I.e., is the overall dynamic range on the highly relevant documents smaller? Or does the gap narrow because there is something specific about our technique that is doing a better job on highly relevant documents than on regular relevant documents?</p><p>Stepping back for a moment, the larger question that we are trying to answer is how one would compare two methods against each other in terms of their ability to find highly relevant documents, when what they are retrieving is relevant documents. The confounding factor is that one method may retrieve more highly relevant documents simply because it retrieves more total relevant documents for the same level of effort. So is that technique doing better than the other because it is better at highly relevant documents, or because it is better at all relevant documents? 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Figure <ref type="figure" coords="3,83.03,210.72,3.58,7.86">3</ref>: % Difference in the size of the gap between highly relevant and regular relevant runs, for our approach versus the BMI baseline title-only (red), and our approach versus BMI baseline title+description (blue) At one level, this question doesn't matter, as a technique that retrieves fewer relevant documents requires the reviewer to expend more effort. And avoiding that additional effort is of primary concern. However, it would also be interesting to see from a purely exploratory standpoint how two highly relevant gain curves would appear if all non-relevant documents were removed and only relevant and highly relevant documents remained. In that manner, the highly relevant document ordering could be compared more directly. Again, this would be more of an exploratory comparison, but could yield interesting insights.</p><p>Another approach to the evaluation of highly relevant document finding would be to run systems in which training is done either purely on the highly relevant documents, or in which the highly relevant documents are given a larger weight than the regular relevant documents when they are encountered during review. This becomes especially pertinent to the main focus of this paper, which is the effect of manual seeding on the process. At some point we would like to be able to see not only whether reviewers are able to more quickly or easily find highly relevant documents, but whether that makes a difference to the underlying machine learning algorithms supporting the review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Call Your Shot</head><p>Our main focus in this paper was not on calling our shot, but on exploring the effects of manual seeding. Nevertheless, "call your shot" was part of the track, so we borrowed with attribution the basic intuition of <ref type="bibr" coords="3,210.30,554.28,9.73,7.86" target="#b2">[2]</ref> and implemented a quick, naive version thereof. <ref type="bibr" coords="3,179.47,564.74,9.73,7.86" target="#b2">[2]</ref> had noted that a reasonable stopping point seems to be when batch richness (total relevant / total reviewed) drops to about 1/10th of the high water mark. We implemented this by calling the following function periodically throughout the continuous learning review.</p><p>Not allowing the possibility of a positive value for the stopping condition until at least 500 documents have been reviewed is a hack, and is based on the fact that we have noticed through prior experience on TREC 2015 data that when only manual seeds are used in the initial iteration, early richness can fluctuate unpredictably on some topics. Whether this is a function of the reviewers choosing seeds in a biased manner, or whether it is a broader reflection of the nature of certain topics, we are not sure. of our our prior experience that the fluctuation could exist, we made a blanket decision to never stop before the 500th reviewed document. This of course affected our shot-calling performance on some of the topics for which there were only a few hundred, sometimes a few dozen, relevant documents. We also note that the size of the window (chunk size) over which we calculated richness was arbitrary and unoptimized; we did not investigate other settings, either prior to the run nor since.</p><p>Finally, we note that the code to calculate the stopping condition was a rushed last minute endeavor, put together in about half an hour. As such, it contains (at least) one glaring logical hole: The lowest to highest richness ratio is unordered. What should have been done is that only the more recent windows should have been emphasized, i.e. the ratio to be tested should have been the more recent richness window to the highest richness window.</p><p>Normally, that shouldn't be a problem, because overall richness is generally monotonically decreasing. However, in at least one case the opposite was true: Topic 403. The initial review started off moderately rich, then flattened for a longer period of time before rising sharply again. A a result, about halfway up that sharp ascent, the lowest richness window (centered around document 400) became 1/10th as rich as the highest richness window (centered around document 900). Since the algorithm ignored window order, it then called the stopping point in the middle of that sharp rise, while documents were still being found at a very high rate (see Figure <ref type="figure" coords="4,120.25,492.39,3.58,7.86">8</ref>, Topic 403) That boundary condition aside, Figure <ref type="figure" coords="4,225.79,502.85,4.61,7.86">4</ref> shows that for most of the topics, and evaluating using F1, our naive approach beat the baselines for both relevant and highly relevant documents. However, Figure <ref type="figure" coords="4,198.06,534.23,4.61,7.86">5</ref> tells a different story: When it comes to pure recall (with no consideration of precision) the point at which our algorithm calls the shot is almost invariably at a lower recall point than the baseline. This of course raises the issue of what "reasonable" means. The task is a total recall task, so one would imagine that the higher recall point is the better evaluation metric. However, anyone can get higher recall, simply by continuing to review, i.e. calling a later stopping point. In the extreme, everyoneeven those randomly reviewing documents -can always get 100% recall by reviewing the entire collection. This is not the intent of the task, because there is also a requirement to avoid wasted effort, i.e. to keep precision high.</p><p>F1 is a traditionally common way to balance both precision and recall, and it was the one chosen by the Total Recall Track as an official metric, which is why we show it above. But it places equal weight on both recall and precision. So might it not be more reasonable, in a total recall task, to use F2 or even F3 instead? Or why not F2.647? Is that not a more reasonable metric than F2.883? Why or why not? The question still remains: What is reasonable, and how do we measure it?</p><p>Part of the difficulty is that the measurement of the quality of a stopping point algorithm is inextricably linked with the quality of the review ordering itself. Different review orderings are going to limit (or expand) the highest achievable stopping metric score of even the best stopping point algorithm. Perhaps future work from the community could separate out the review order quality issue from the stopping point issue, by creating stopping point test collections that standardize on fixed orderings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MANUAL SEED EXPERIMENTS</head><p>The main subject of investigation for our TREC 2016 Total Recall run was what effect various manual initial selections (seedings) of documents has on overall task outcome. Section 1 explored the union of all four sets of seeds (two one-shot, two iterative), and in this section we explore each of the starting points, individually.  <ref type="figure" coords="5,265.44,453.10,3.58,7.86">6</ref>: Manual Effort Statistics Docs Reviewed, and Review Overlap. In the Query Count section, the number of queries that each reviewer (R1 through R4) issued for each topic is shown. For ease of reading, if the reviewer did a one-shot (single) query, that is indicated with a dash "-". If the reviewer did an interactive run, the actual number of queries is shown. The averages shown are averages of just the interactive runs; one-shot averages are 1.0, of course.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Manual Effort Statistics</head><p>In the Time Spent section, the pattern is similar: dash indicates one-shot query, which likely look a minute or two but we did not record the exact amount of time each reviewer spent pondering his or her one query before issuing it. Actual numbers indicate time spent in an interactive run. The averages shown are averages of just the interactive runs.</p><p>In the third section (Docs Reviewed) the exact number of documents that the reviewer laid eyeballs on, both relevant and non-relevant, is indicated. The review software did not have explicit controls to stop a reviewer from going beyond 25 documents; this was left up to the individual reviewer. So as per normal human variance the count is sometimes a few docs over, sometimes a few docs under, but generally around the 25 document mark. The averages shown for Docs Reviewed are averages of all runs, both one-shot and iterative.</p><p>The final section is Review Overlap. Since the reviewers worked with no knowledge of each other, it was often the case that (even when issuing different queries) they reviewed some of the same documents. Therefore, we show statistics on not only the total number of documents reviewed, but the total number of unique documents reviewed. The ratio of unique to total is also shown, as is the log of the size of the topic (total number of available relevant documents for that topic). The averages shown are averages of all runs, both one-shot and iterative.</p><p>When examining these values, we noticed an interesting, though possibly spurious, relationship between uniqueness ratio and topic size (number of relevant documents available for that topic). Figure <ref type="figure" coords="5,407.89,641.05,4.61,7.86" target="#fig_3">7</ref> is a plot of the relationship between the ratio of unique to total documents reviewed (x-axis) and the (log of the) size of the topic (y-axis). A line is fitted to this data, and seems to suggest that the more total available relevant documents there are to be found in the collection, the less overlap there was between what the four reviewers found. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Individual Topic Runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Individual Gain Curves</head><p>Gain curves for the 34 topics are shown in Figures 8 through 14 (left). The reviewer ID is indicated with a color: Red, blue, green, and brown for reviewers 1 through 4, respectively. If the reviewer ran that topic as a one-shot seeding process, the gain curve is a solid line. If the reviewer ran it iteratively, the line is dotted.</p><p>We acknowledge that there is an overabundance of graphs in this paper. The reason we chose to show them all in their entirety, rather than just show some sort of summary statistic such as precision@90% recall or average area under the gain curve is that this gives us a chance to observe the fine differences between topics and reviewers. These differences can be nuanced, which is both the strength and the weakness of presenting research results in this manner. It's a weakness, because it makes the results more difficult to summarize. It's a strength, because it lets one see where and how distinctions can arise. Furthermore, this is a TREC paper, rather than peer-reviewed scientific conference or journal paper, and we the authors feel it is more valuable and in keeping with the spirit of openness around TREC to show as much detail about our runs as possible. Displaying every graph allows us to do that.</p><p>That said, one general observation is that, no matter who the reviewer doing the initial seeding was, or what method they employed, high recall is achieved by almost every reviewer on almost every topic without having to review the vast majority of the collection. There remains a belief among many industry practitioners engaged in high recall tasks that only experts may select seed documents, that only experts have the capability to initiate a recall-oriented search by selecting the initial training documents. In our experiments, only one of our reviewers qualified as a practicing search expert: Reviewer 1 (red). Nevertheless, for 132 of the 136 starting points in Figures 8 through 14, all starting points hit high recall within a relatively similar amount of effort. For example, on Topic 405, Reviewer 2 (blue) gets to 90% recall a little faster, and Reviewer 1 (red) gets there a little later, with the other two reviewers somewhere in the middle. But the difference between the "best" and "worst" is literally 68 documents. Out of a collection of 290,000 documents, that is an insignificant difference. Other topics, such as Topic 411, show a bit of a back and forth as the review progresses between the various starting points. But all achieve high recall at about the same point.</p><p>The four starting points where there is a significant difference between best and worst were Reviewer 4 (brown) on Topic 415, Reviewers 2 and 3 (blue and green) on topic 418 and Reviewer 3 on Topic 419. Of those, 3 of the 4 were all one-shot queries. That is, the reviewer did not do any of the review of the collection before issuing his single query, did not receive any feedback on his or her single query, and did not issue more than one query. The final underperforming starting point was done by an iterative reviewer (4 queries in 8 minutes, as per Figure <ref type="figure" coords="6,417.33,422.99,3.58,7.86">6</ref>), but is still the exception rather than the rule.</p><p>There is one more data point worth noting in the gain curves on the left of each figure. The black curve is based on pooling all the unique seed documents from each of the four reviewers before running a continuous learning review. In the majority of the cases, the pooled seed starting point is at least as good as, if not better, than the best individual starting point. However, in a number of instances, the pooled seeds yield a result that is equal to the worst individual case, and in rare instances, even worse than the worst individual case. Given the casualness of this TREC paper, we don't have a formal metric, a concrete quantification of "better", "equal", "worse", etc. Rather, we did a casual eyeballing of the curves and looked at where the combined seeding came out generally, often, as per the individual seedings themselves, by the time the process hit high (85%-95%) recall, all the methods converged, anyway. So in this particular exploratory analysis, we are often looking at differences at lower levels of recall. Nevertheless, given that we're displaying all curves, the reader can see and judge for themselves whether or not significant differences exist.</p><p>The following table is a rough count of the number of times that the combined seeding approach was better than the best individual, approximately equal to the best individual, somewhere in the middle of all individuals, equal to the worst individual, or worse than the worse individual seeding run. For the most part, the pooled seeds tended to be equal to or better than the best individual. However, it would be worth exploring those cases in which the pooled seeds do worse, and try to determine why. That analysis is beyond the scope of this paper. One more thing to keep in mind: Even when the pooled approach is better than the best, or worse than the worst, the absolute magnitude of the differences, especially relative to the size of the collection, are small. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combined</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Seeding Method-Averaged Gain Curves</head><p>Lastly, for each topic in Figures 8 through 14, we show the average of the two one-shot seeding approaches, as well as the average of the two iterative seedings, in the charts on the right side. Perhaps the better approach would have been to pool the iterative and the one shot seeds, respectively, before running CAL on the combined seed pools. For now, however, we show the average of the gain curves of the individually seeded runs. There is variability among individual reviewers, so by averaging multiple runs and randomizing reviewers across topics, we hope to get a better sense of the the general approach, separate from the individual reviewer vagaries. Ideally we would have more than two reviewers doing each method, but resources are always limited.</p><p>Basically, we can see that for the most part, there is not much difference between the two approaches for most topics. The iterative approach may have a slight edge on Topics 415, 418, 419, 421, and 422. However, the one shot approach has a slight edge on topics 402, 411, 416, 428, and 433. On the remainder of the topics, where there are differences, those differences are slight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GROUND TRUTH ANALYSIS</head><p>There is one final analysis of the data that we would like to present. It is perhaps a bit non-standard, and we do not offer any hard conclusions. But it was analysis that we found interesting so we would like to show it to provoke future thought.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Explanation of the Analysis</head><p>Our understanding of how the ground truth was created for this track was that the NIST assessors used a combination of methods, their own searches plus an algorithm that used the same underlying feature extraction mechanism as the baseline model implementation (BMI). The collection was not fully judged for each topic, but was judged to a depth proportionally deeper than the number of relevant documents that were found. So for example, for Topic 422, NIST assessors judged 31 relevant documents, and 317 nonrelevant docs. No other documents were assessed. For Topic 423, 286 relevant documents and 1113 non-relevant documents were judged. No other documents were assessed.</p><p>As per common TREC practice, documents that are not assessed (non-judged) are presumed to be non-relevant, and treated as such for both training and evaluation purposes. In this last section, we wish to separate out, for the purpose of deeper analysis, judged non-relevant documents from non-judged documents. To this end we present Figures 15 through 21. For each topic, the figure on the left side shows recall on the x-axis, and the raw number of judged nonrelevant documents on the y-axis. The blue line represents the number of judged non-relevant documents to that depth in the review unique to our pooled seeds method (see previous section), the red line shows the same information unique to the baseline (BMI) title+description method, and the dotted grey line is the number of judged non-relevant documents that both methods have in common. We only plot to 90% recall for all topics, as high non-judged counts can skew the visualization after that point.</p><p>So for example, see Topic 421 in Figure <ref type="figure" coords="7,495.04,183.17,7.84,7.86">19</ref>. Let's start with the figure on the left, which shows judged non-relevant documents on the y-axis. By the time that the baseline method (red) has hit 10% recall, it has seen 3 judged nonrelevant documents, while our method (blue) has hit 0 judged non-relevant documents, and none of those documents are the same documents. At 75% recall, the baseline method has seen 28 judged non-relevant documents, while our method has seen 16 judged non-relevant documents. However, 13 of those documents are in common between the two methods. So at 75% recall, the baseline method has seen 15 judged non-relevant documents that our method has not (unique to BMI), and our method has seen about 3 judged non-relevant documents that the baseline method has not (unique to our method). In comparison, see the figure on the right, which shows non-judged documents on the y-axis. At 75% recall, there are only 5 documents that the baseline method has seen that are have not been judged, but 27 documents that our method has seen that have not been judged. None of these documents are in common, as the dotted grey line only starts to rise after about 82% recall. What this means is that, at 75% recall, the baseline method has only hit 15 judged + 5 non-judged = 20 non-relevant documents, but our method has hit 3 judged + 27 non-judged = 30 nonrelevant documents.</p><p>From an overall evaluation standpoint, this means that (at 75% recall) the baseline method is better than our method, because it has hit fewer non-relevant documents. But when the majority of the documents in that comparison were judged for one method, and not judged for the other method, it raises questions about how things might be different if some of the non-judged documents had been judged. Would there be more relevant documents in those non-judged documents? Would there be more relevant documents in the non-judged documents unique to the baseline method, or unique to our method? And how would that affect overall recall and stopping points, not to mention training, especially when a half dozen newly found relevant documents could have significant effect in such low prevalence topics.</p><p>We did some casual, non-comprehensive spot checks on some of the topics by looking at the top 20 highest ranked documents that were unique to each method (i.e. 40 docs per topic in total). And we did find a fairly significant number of (what we thought were) relevant documents for some topics, almost none for other topics, at least within those first 20 documents.</p><p>However, we are not going to go into detail about how many additional relevant documents we believe that there were, for four reasons: (1) We did not do a full assessment of every topic, so any information we do present would be misleading, <ref type="bibr" coords="7,365.21,706.21,11.77,7.86" target="#b2">(2)</ref> We are not the same assessors as the NIST assessors, and unless we were to go back and also review all the same documents that the NIST assessors reviewed, any assessment would be a skewed or biased by the disjointedness of the assessment, (3) Even if we did do a full reassessment of all judged and top-ranked non-judged documents, it would be unfair to the baseline method, because that method would not have had a chance to train on any newly-judged relevant documents, and finally (4) It is not within the spirit of TREC to publish research whose only goal it is to "beat" other systems. Rather, the purpose of TREC is to dive deep in to interesting questions, to challenge assumptions, to learn by trying crazy, unproven methods, to basically poke and prod a problem, and see what happens. This research hopefully accomplishes that by comparing multiple reviewers doing multiple approaches to seeding (one-shot query, iterative querying). To understand what ground truth data is being used and how that might affect things is a side goal, but not the primary one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussion</head><p>Nevertheless, to understand the full context of this work, we felt it necessary to break out the analysis of the results into these two components: judged non-relevant and nonjudged. Without even knowing whether the non-judged documents were truly relevant or non-relevant, some interesting patterns emerge. The first patterns is one exemplified by the topic that we already discussed above, Topic 421. In this pattern, our method has higher precision (lower number of non-relevant documents) on the judged set (left graph), but lower precision (higher number of non-relevant documents) on the non-judged set. This is a pattern seen in 10 other (11 total) instances: Topics 405, 410, 415, 422, 423, 425, 431, 432, 433, and 434. The next pattern is topics for which both methods find judged non-relevant documents at about the same rate, but our method hits a lot more non-judged documents. This pattern is found in 7 instances: Topics 401, 402, 406, 412, 418, 426, and 429. The remaining 16 topics are ones for which our method hits more non-relevant documents, both judged and non-judged, than does the baseline method.</p><p>How does one interpret this? Why is it that the two methods are finding, at times, vastly different non-relevant (judged or non-judged documents, while finding the same number of relevant documents. Or more specifically: For a large number of topics, why does our method find more non-judged documents, even as it is finding fewer judged non-relevant documents, at the same level of recall. By itself, finding more non-judged documents is not difficult: One can simply select documents at random. But this isn't a random selection of documents, because our method is finding relevant documents at a reasonably fast clip, while sometimes simultaneously finding fewer judged non-relevant documents at the same level of effort.</p><p>It's also interesting to note that for a fairly large number of topics, the baseline method finds almost no non-judged documents at all. Almost all non-relevant documents that it finds through the course of the review are ones that have already been judged. For example, see <ref type="bibr" coords="8,209.69,658.88,44.95,7.86">Topics 401,</ref><ref type="bibr" coords="8,257.41,658.88,16.37,7.86">402,</ref><ref type="bibr" coords="8,276.54,658.88,16.37,7.86">403,</ref><ref type="bibr" coords="8,53.80,669.34,16.37,7.86">404,</ref><ref type="bibr" coords="8,72.49,669.34,16.37,7.86">406,</ref><ref type="bibr" coords="8,91.17,669.34,16.37,7.86">407,</ref><ref type="bibr" coords="8,109.86,669.34,16.37,7.86">408,</ref><ref type="bibr" coords="8,128.56,669.34,16.37,7.86">409,</ref><ref type="bibr" coords="8,147.25,669.34,16.37,7.86">416,</ref><ref type="bibr" coords="8,165.93,669.34,16.37,7.86">417,</ref><ref type="bibr" coords="8,184.62,669.34,16.37,7.86">418,</ref><ref type="bibr" coords="8,203.32,669.34,16.37,7.86">419,</ref><ref type="bibr" coords="8,222.01,669.34,16.37,7.86">427,</ref><ref type="bibr" coords="8,240.69,669.34,16.37,7.86">428,</ref><ref type="bibr" coords="8,259.39,669.34,16.37,7.86">433,</ref><ref type="bibr" coords="8,278.08,669.34,14.84,7.86;8,53.80,679.80,12.27,7.86">and 434</ref>, which are almost half the topics in the track. We are not certain why the unique documents found by one method are almost thoroughly judged while the unique documents found by the other are not. There may be something inter-esting in the way in which our method is working that is more naturally diverse (even without explicit diversification activated as explained in Section 1.2) relative to the baseline. It may mean that there was an aspect or facet of relevance that was found by our method that was not found during the ground truth assessment. Different doesn't necessarily mean better, however, as the non-judged documents are not necessarily going to be relevant, were they to be judged. We cannot answer this question now; we simply wish to show that there does seem to be consistent patterns of judged versus non-judged documents in the non-relevant set. This is a good opening into future work on Total Recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In conclusion, this paper examined the effect of multiple manual approaches to seeding using four different reviewers applying one of two different manual seeding strategies (one-shot vs iterative). For over 97% (132 of the 136) seedings, by the time high recall was hit, there was relatively little difference between the starting points no matter the method. When averaged across strategy (one-shot vs iterative), five topics slightly favored the iterative approach, five slightly favored the one-shot approach, and the remainder of the topics came out about the same.</p><p>Perhaps one of the challenges is that the iteration was brief; reviewers were only allowed to work until they had marked up to 25 documents. With more time or more queries, perhaps a larger difference could have been observable. On the other hand, many of these topics were relatively straightforward, and perhaps no differences and improvements via manual efforts may be possible. Nevertheless, we note that all topics achieved high recall without having to review the vast majority of the collection, no matter if an expert or a non-expert was used to manually seed each topic.</p><p>We also noted a possible relationship between reviewer overlap and topic size. Where different reviewers manually find many of the same documents, the topic may have a smaller number of documents, and vice versa when different reviewers manually find many different documents. Whether such an approach could be formalized enough to be broadly predictive remains an open question.</p><p>Additionally, we did some analysis of the ground truth itself, and examined the relative difference between our method and the baseline method in terms of how many judged nonrelevant versus non-judged documents each found over the course of each topic's review. The visualization of these differences are interesting, but anything conclusive at this point would be pure speculation.  Figure <ref type="figure" coords="22,83.38,643.44,7.84,7.86">21</ref>: Analysis of Judged and Non-Judged Non-Relevant Documents. On both graphs, x-axis is recall level. y-axis is Judged Non-Relevant (left) and Non-Judged Non-Relevant (right). Red = documents unique to baseline method, blue = documents unique to our method, grey = documents common to both methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.80,212.46,502.13,7.86;2,53.80,222.92,157.38,7.86"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Relevant documents. Full collection gain curve (left), top 20k documents (right). BMI title-only = red, BMI description = blue, this paper = black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,53.80,210.85,502.12,7.86;4,53.80,221.31,208.78,7.86"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: "Call Your Shot": Per topic histogram of F1 score % change of our approach relative to the BMI baselines. Relevant documents (left), Highly relevant documents (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,96.90,245.50,415.92,7.86;6,166.79,53.80,270.00,188.51"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Log Topic Size (y-axis) against Ratio of Multi-Reviewer Unique to Total Seed Count (x-axis)</figDesc><graphic coords="6,166.79,53.80,270.00,188.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,53.80,737.24,502.12,7.86;9,53.80,747.70,502.12,7.86;9,53.80,758.16,502.13,7.86;9,53.80,768.62,311.42,7.86"><head>Figure 8 :Figure 9 :Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :Figure 15 :Figure 16 :</head><label>8910111213141516</label><figDesc>Figure 8: Gain Curves. x-axis = reviewed documents (in order), as a percentage of the entire collection. y-axis = recall. [Left] Red = Reviewer 1, Blue = Reviewer 2, Green = Reviewer 3, Brown = Reviewer 4, Black = Pooled seeds from all reviewers. For individual reviewers, solid line indicates one-shot query; dashed line indicates iterative searching. [Right] Solid line is the average of the one-shot reviewers; dashed is the average of iterative reviewers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="18,53.80,748.35,502.12,7.86;18,53.80,758.81,502.12,7.86;18,53.80,769.27,322.57,7.86"><head>Figure 17 :Figure 18 :Figure 19 :Figure 20 :</head><label>17181920</label><figDesc>Figure17: Analysis of Judged and Non-Judged Non-Relevant Documents. On both graphs, x-axis is recall level. y-axis is Judged Non-Relevant (left) and Non-Judged Non-Relevant (right). Red = documents unique to baseline method, blue = documents unique to our method, grey = documents common to both methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,316.81,700.73,239.11,18.32"><head></head><label></label><figDesc>Figure 6 contains statistics of the 34 athome4 topics. This data is presented in four parts: Query Count, Time Spent,</figDesc><table coords="5,65.99,55.50,501.67,405.47"><row><cell></cell><cell></cell><cell cols="2">Query Count</cell><cell></cell><cell cols="4">Time Spent (minutes)</cell><cell></cell><cell cols="2">Docs Reviewed</cell><cell></cell><cell></cell><cell cols="2">Review Overlap</cell><cell></cell></row><row><cell>Topic</cell><cell cols="4">R1 R2 R3 R4</cell><cell cols="3">R1 R2 R3</cell><cell>R4</cell><cell>R1</cell><cell>R2</cell><cell cols="6">R3 R4 Unique Sum Ratio TopicSize</cell></row><row><cell>athome401d</cell><cell>-</cell><cell>-</cell><cell>6</cell><cell>7</cell><cell>-</cell><cell>-</cell><cell>11</cell><cell>31</cell><cell>25</cell><cell>25</cell><cell>31</cell><cell>25</cell><cell>67</cell><cell>106</cell><cell>0.63</cell><cell>229</cell></row><row><cell>athome402d</cell><cell>3</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>16</cell><cell>-</cell><cell>7</cell><cell>-</cell><cell>23</cell><cell>24</cell><cell>33</cell><cell>25</cell><cell>85</cell><cell>105</cell><cell>0.81</cell><cell>638</cell></row><row><cell>athome403d</cell><cell>3</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>16</cell><cell>11</cell><cell>-</cell><cell>-</cell><cell>25</cell><cell>13</cell><cell>25</cell><cell>25</cell><cell>56</cell><cell>88</cell><cell>0.64</cell><cell>1090</cell></row><row><cell>athome404d</cell><cell>-</cell><cell>2</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>4</cell><cell>-</cell><cell>25</cell><cell>20</cell><cell>25</cell><cell>25</cell><cell>54</cell><cell>95</cell><cell>0.57</cell><cell>545</cell></row><row><cell>athome405d</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>7</cell><cell>26</cell><cell>25</cell><cell>25</cell><cell>29</cell><cell>25</cell><cell>62</cell><cell>104</cell><cell>0.60</cell><cell>122</cell></row><row><cell>athome406d</cell><cell>3</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>16</cell><cell>6</cell><cell>-</cell><cell>-</cell><cell>25</cell><cell>25</cell><cell>25</cell><cell>25</cell><cell>69</cell><cell>100</cell><cell>0.69</cell><cell>127</cell></row><row><cell>athome407d</cell><cell>4</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>16</cell><cell>7</cell><cell>-</cell><cell>-</cell><cell>25</cell><cell>16</cell><cell>25</cell><cell>25</cell><cell>42</cell><cell>91</cell><cell>0.46</cell><cell>1586</cell></row><row><cell>athome408d</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>7</cell><cell>-</cell><cell>11</cell><cell>-</cell><cell>32</cell><cell>19</cell><cell>25</cell><cell>25</cell><cell>25</cell><cell>60</cell><cell>94</cell><cell>0.64</cell><cell>116</cell></row><row><cell>athome409d</cell><cell>-</cell><cell>-</cell><cell>8</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>12</cell><cell>22</cell><cell>24</cell><cell>25</cell><cell>22</cell><cell>25</cell><cell>67</cell><cell>96</cell><cell>0.70</cell><cell>202</cell></row><row><cell>athome410d</cell><cell>4</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>15</cell><cell>-</cell><cell>6</cell><cell>-</cell><cell>25</cell><cell>25</cell><cell>35</cell><cell>25</cell><cell>91</cell><cell>110</cell><cell>0.83</cell><cell>1346</cell></row><row><cell>athome411d</cell><cell>4</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>16</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>24</cell><cell>22</cell><cell>22</cell><cell>25</cell><cell>41</cell><cell>93</cell><cell>0.44</cell><cell>89</cell></row><row><cell>athome412d</cell><cell>-</cell><cell>6</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>12</cell><cell>-</cell><cell>34</cell><cell>22</cell><cell>25</cell><cell>25</cell><cell>25</cell><cell>81</cell><cell>97</cell><cell>0.84</cell><cell>1410</cell></row><row><cell>athome413d</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>6</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>36</cell><cell>25</cell><cell>25</cell><cell>25</cell><cell>25</cell><cell>77</cell><cell>100</cell><cell>0.77</cell><cell>546</cell></row><row><cell>athome414d</cell><cell>1</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>16</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>25</cell><cell>25</cell><cell>27</cell><cell>25</cell><cell>59</cell><cell>102</cell><cell>0.58</cell><cell>839</cell></row><row><cell>athome415d</cell><cell>3</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>15</cell><cell>12</cell><cell>-</cell><cell>-</cell><cell>22</cell><cell>16</cell><cell>25</cell><cell>25</cell><cell>64</cell><cell>88</cell><cell>0.73</cell><cell>12106</cell></row><row><cell>athome416d</cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>6</cell><cell>-</cell><cell>9</cell><cell>-</cell><cell>36</cell><cell>25</cell><cell>35</cell><cell>25</cell><cell>25</cell><cell>76</cell><cell>110</cell><cell>0.69</cell><cell>1446</cell></row><row><cell>athome417d</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>7</cell><cell>-</cell><cell>-</cell><cell>7</cell><cell>30</cell><cell>25</cell><cell>25</cell><cell>34</cell><cell>25</cell><cell>108</cell><cell>109</cell><cell>0.99</cell><cell>5931</cell></row><row><cell>athome418d</cell><cell>5</cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>16</cell><cell>-</cell><cell>8</cell><cell>-</cell><cell>25</cell><cell>24</cell><cell>25</cell><cell>25</cell><cell>63</cell><cell>99</cell><cell>0.64</cell><cell>187</cell></row><row><cell>athome419d</cell><cell>2</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>16</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>26</cell><cell>15</cell><cell>25</cell><cell>25</cell><cell>50</cell><cell>91</cell><cell>0.55</cell><cell>1989</cell></row><row><cell>athome420d</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>9</cell><cell>-</cell><cell>25</cell><cell>25</cell><cell>33</cell><cell>25</cell><cell>25</cell><cell>66</cell><cell>108</cell><cell>0.61</cell><cell>737</cell></row><row><cell>athome421d</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>36</cell><cell>25</cell><cell>25</cell><cell>26</cell><cell>25</cell><cell>50</cell><cell>101</cell><cell>0.50</cell><cell>21</cell></row><row><cell>athome422d</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>25</cell><cell>25</cell><cell>32</cell><cell>25</cell><cell>75</cell><cell>107</cell><cell>0.70</cell><cell>31</cell></row><row><cell>athome423d</cell><cell>2</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>16</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>26</cell><cell>25</cell><cell>25</cell><cell>25</cell><cell>32</cell><cell>101</cell><cell>0.32</cell><cell>286</cell></row><row><cell>athome424d</cell><cell>2</cell><cell>2</cell><cell>-</cell><cell>4</cell><cell>16</cell><cell>7</cell><cell>-</cell><cell>28</cell><cell>25</cell><cell>20</cell><cell>25</cell><cell>25</cell><cell>60</cell><cell>95</cell><cell>0.63</cell><cell>497</cell></row><row><cell>athome425d</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>7</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>36</cell><cell>25</cell><cell>25</cell><cell>29</cell><cell>25</cell><cell>65</cell><cell>104</cell><cell>0.63</cell><cell>714</cell></row><row><cell>athome426d</cell><cell>3</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>15</cell><cell>-</cell><cell>6</cell><cell>-</cell><cell>25</cell><cell>25</cell><cell>32</cell><cell>25</cell><cell>54</cell><cell>107</cell><cell>0.50</cell><cell>120</cell></row><row><cell>athome427d</cell><cell>2</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>16</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>25</cell><cell>19</cell><cell>25</cell><cell>25</cell><cell>53</cell><cell>94</cell><cell>0.56</cell><cell>241</cell></row><row><cell>athome428d</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>6</cell><cell>-</cell><cell>7</cell><cell>-</cell><cell>32</cell><cell>25</cell><cell>24</cell><cell>25</cell><cell>25</cell><cell>72</cell><cell>99</cell><cell>0.73</cell><cell>464</cell></row><row><cell>athome429d</cell><cell>-</cell><cell>-</cell><cell>6</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>18</cell><cell>25</cell><cell>25</cell><cell>30</cell><cell>25</cell><cell>82</cell><cell>105</cell><cell>0.78</cell><cell>827</cell></row><row><cell>athome430d</cell><cell>3</cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>16</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>25</cell><cell>24</cell><cell>28</cell><cell>25</cell><cell>88</cell><cell>102</cell><cell>0.86</cell><cell>991</cell></row><row><cell>athome431d</cell><cell>3</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>16</cell><cell>10</cell><cell>-</cell><cell>-</cell><cell>25</cell><cell>26</cell><cell>25</cell><cell>25</cell><cell>57</cell><cell>101</cell><cell>0.56</cell><cell>144</cell></row><row><cell>athome432d</cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>27</cell><cell>25</cell><cell>23</cell><cell>25</cell><cell>25</cell><cell>51</cell><cell>98</cell><cell>0.52</cell><cell>140</cell></row><row><cell>athome433d</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>4</cell><cell>23</cell><cell>25</cell><cell>25</cell><cell>27</cell><cell>25</cell><cell>64</cell><cell>102</cell><cell>0.63</cell><cell>112</cell></row><row><cell>athome434d</cell><cell>4</cell><cell>-</cell><cell>7</cell><cell>-</cell><cell>16</cell><cell>-</cell><cell>11</cell><cell>-</cell><cell>25</cell><cell>25</cell><cell>29</cell><cell>25</cell><cell>45</cell><cell>104</cell><cell>0.43</cell><cell>38</cell></row><row><cell>average</cell><cell cols="12">3.0 3.4 4.3 5.3 15.8 8.3 6.6 29.5 23.6 24.6 26.9 25</cell><cell>64.3</cell><cell>100.2</cell><cell>0.64</cell><cell>1056</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Figure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,321.30,576.56,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.01,591.83,208.12,7.86;8,331.01,602.29,201.99,7.86;8,331.01,612.76,206.31,7.86;8,331.01,623.22,214.91,7.86;8,331.01,633.68,159.24,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,485.32,591.83,53.81,7.86;8,331.01,602.29,201.99,7.86;8,331.01,612.76,115.94,7.86">Evaluation of machine learning protocols for technology-assisted review in electronic discovery</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,465.37,612.76,71.94,7.86;8,331.01,623.22,96.95,7.86">Proceedings of the ACM SIGIR Conference</title>
		<meeting>the ACM SIGIR Conference<address><addrLine>Gold Coast, Australia; Gold Coast, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07-11">6-11 July 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.01,645.13,190.91,7.86;8,331.01,655.60,208.17,7.86;8,331.01,666.06,204.06,7.86;8,331.01,676.52,139.76,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,485.32,645.13,36.59,7.86;8,331.01,655.60,208.17,7.86;8,331.01,666.06,19.21,7.86">Waterloo (cormack) participation in the trec 2015 total recall track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,368.60,666.06,166.47,7.86;8,331.01,676.52,31.01,7.86">Proceedings of Text REtrieval Conference (TREC)</title>
		<meeting>Text REtrieval Conference (TREC)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
