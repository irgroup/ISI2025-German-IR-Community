<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.08,115.96,309.18,12.62;1,160.16,133.89,295.03,12.62;1,279.98,151.82,55.40,12.62">Popularity Ranking for Scientific Literature Using the Characteristic Scores and Scale Method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,205.91,189.49,63.54,8.74"><forename type="first">Philipp</forename><surname>Schaer</surname></persName>
							<email>philipp.schaer@th-koeln.de</email>
						</author>
						<author>
							<persName coords="1,296.61,189.49,108.38,8.74"><forename type="first">Narges</forename><surname>Tavakolpoursaleh</surname></persName>
							<email>narges.tavakolpoursaleh@gesis.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">TH Köln (University of Applied Sciences)</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.08,115.96,309.18,12.62;1,160.16,133.89,295.03,12.62;1,279.98,151.82,55.40,12.62">Popularity Ranking for Scientific Literature Using the Characteristic Scores and Scale Method</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">54F7C5C7582DA50AD2F940F1E49D9EF1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC 2016 OpenSearch track is focused on ad-hoc search for scientific literature. Three scientific search engines and document repositories were part of this living lab-centered evaluation campaign: (1) CiteSeerX, (2) Microsoft Academic Search, and (3) SSOAR -Social Science Open Access Repository. The authors of this paper are also responsible for the implementation of the living lab infrastructure and the LL4IR API that is necessary to include an online system into the OpenSearch evaluation campaign. This work is based on a Master's thesis at University of Bonn <ref type="bibr" coords="1,259.90,391.28,9.96,8.74" target="#b6">[7]</ref>. Implementation details can be found there and in the lab's overview paper <ref type="bibr" coords="1,255.81,403.24,10.52,8.74" target="#b0">[1]</ref> and from a higher perspective in <ref type="bibr" coords="1,415.08,403.24,9.96,8.74" target="#b5">[6]</ref>.</p><p>In this paper we will present our work on popularity-based relevance ranking within the two systems CiteSeerX and SSOAR. Both offer different types of usage and popularity data. We would like to test a normalization method for these kind of data known as the Characteristic Scores and Scale Method (CSS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In our first Living Lab paper from 2015 <ref type="bibr" coords="1,310.32,505.98,10.52,8.74" target="#b4">[5]</ref> we used historical usage data (clickthrough rates of the online toy store Regio Játék) to augment a Solr-based relevance ranking of the available products in the online catalogue. We simply used the log of the raw click-through rates as a boosting factor with-in Solr's ranking formula. This straight-forward application of usage and popularity data is considered being to simplistic as it introduces some flaws and drawbacks, like:</p><p>the biases raw data introduces into the ranking due to e.g. different publication dates; the lack of a common scale to compare the different usage data with each other.</p><p>Biases are an immanent problem of usage data. Take the example of the raw clickthrough rate for a given product in the online toy store catalogue. A product that We therefore need a method to normalize the usage data to remove biases and to enable some kind of comparability. In the next section we will present a simple, yet effective way to normalize the usage data distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Normalizing Usage Data Distributions with the Characteristic Scores and Scale Method</head><p>Our method is based on a procedure called the Characteristic Scores and Scales method (CSS) described by Glänzel <ref type="bibr" coords="2,292.46,454.09,9.96,8.74" target="#b1">[2]</ref>. The CSS method is used to find characteristic partitions for citation distributions. They used the method to establish classes of papers that they interpreted as "poorly cited", "fairly cited", "remarkably cited", or "outstandingly cited". These partitions can then be used to normalize different kinds of usage data distributions. As described by Plassmeier et al. <ref type="bibr" coords="2,303.57,514.39,10.52,8.74" target="#b2">[3]</ref> these classes are constructed by calculating the class boundaries in the following way: First we take the mean of the distribution β 1 = µ. The distribution is truncated at the first boundary and the second boundary is found at the mean of the truncated distribution,</p><formula xml:id="formula_0" coords="2,134.77,562.21,103.68,9.65">β 2 = mean(x i |x i ≤ β 1 )</formula><p>. For the rest of the distribution the next k-th class boundary is defined by</p><formula xml:id="formula_1" coords="2,251.48,600.20,224.86,9.65">β k = mean(x i |x i ≤ β k-1 ). (<label>1</label></formula><formula xml:id="formula_2" coords="2,476.35,600.20,4.24,8.74">)</formula><p>This method iterates as long as a previously defined threshold of number of classes has been reached or if the number of elements in class k drops below a previously defined threshold. The found classes are used to construct a continuous transformation function of the original distribution values to the normalized values. To do so, we map the class boundaries to the interval [0, 1] and linearly interpolate between the class boundaries, i.e. β k = k/k max . Plassmeier et al. <ref type="bibr" coords="3,227.71,142.90,10.52,8.74" target="#b2">[3]</ref> showed that this normalization steps can be used for different kinds of usage data distributions, like number of citations, number of record views or the number of loans at local libraries. We therefore were eager to learn if we can apply them to the usage data found in SSOAR or CiteSeerX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Popularity-based Relevance Ranking for SSOAR and CiteSeerX</head><p>As listed in table 1 we extracted three different sets of usage data: Document views and PDF download rates from SSOAR and citation counts from CiteSeerX. We gathered these data by using internal APIs of SSOAR<ref type="foot" coords="3,393.88,249.14,3.97,6.12" target="#foot_0">1</ref> and by using web scraping technologies in the case of CiteSeerX. The usage data dates to the middle of May 2016. The data was gathered once and was not updated during the campaign. In table <ref type="table" coords="3,236.74,286.58,4.98,8.74" target="#tab_1">2</ref> we see an overview on the popularity data used in the following section.</p><p>We define our new popularity-based relevance rank scoring s spop (q, d, U d ) for a given query q, a specific document d and the set of popularity and usage values U d for that given document. U d can contain different values like u down being the download, u view being the view counts for each document, or u cite being the citation count for this document.</p><formula xml:id="formula_3" coords="3,224.07,382.22,256.52,9.65">s spop (q, d, U d ) = s solr (q, d) * s pop (d, U d )<label>(2)</label></formula><p>The original Solr score<ref type="foot" coords="3,235.97,397.92,3.97,6.12" target="#foot_1">2</ref> is augmented with a document dependent popularity score s pop (d, U d ) that is defined a follows:</p><formula xml:id="formula_4" coords="3,228.63,435.36,251.96,20.73">s pop (d, U d ) = ( i∈U d s cssi (d, u d,i )) + 1.<label>(3)</label></formula><p>For SSOAR and it's two popularity values this is:</p><formula xml:id="formula_5" coords="3,194.96,486.45,285.63,9.65">s pop (d, U d ) = (s css (d, u d,view ) + s css (d, u d,down )) + 1.<label>(4)</label></formula><p>For CiteSeerX and it's citation counts this is:</p><formula xml:id="formula_6" coords="3,239.01,524.53,241.59,9.65">s pop (d, U d ) = s css (d, u d,cite ) + 1.<label>(5)</label></formula><p>The popularity values s css (d, u d,view ), s css (d, u d,down ), and s css (d, u d,cite ) which are dependent on the document view, download and citation rate are calculated on basis of the CSS method described earlier in section 2.1 and are summed up.</p><p>In case that there is no usage data available the value of s pop (d, U d ) is 1. Therefore with no usage data available s spop (q, d, u d , u v ) = s solr (q, d), or in other word the original Solr ranking score is not modified. The behaviour of our boosting method is verbalized as follows: A high download rate in combination with a high document view rate leads to a general high boost rate of s pop while a single high value of one of these alone is not enough to guarantee a significant boost and without any usage data available the original Solr relevance ranking is not altered.</p><p>In our experiments we used three different popularity values: record views, PDF downloads and citation counts. In a literature-related field like TREC OpenSearch other popularity values are available, as listed by <ref type="bibr" coords="4,407.35,336.21,9.96,8.74" target="#b2">[3]</ref>:</p><p>-Citation related values, like number of citation of an item or the citation impact for a journal or publication venue; -Author metrics, like h-index or other citation-reltated impact mesuares for authors; -Usage data, like number of record views, number of clicks on full text or downloads, or number of library loans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In table <ref type="table" coords="4,174.10,474.39,4.98,8.74" target="#tab_1">2</ref> we can see the results of our popularity data crawling for SSOAR and CiteSeerX. For SSOAR we analyzed approx. 25,000 documents and 10,000 for CiteSeerX. On average there were 264 downloads and 396 record views per document for SSOAR and 364 citations for the CiteSeerX documents. Other corpus statistics are listed in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Impact on the Usage Date Distributions</head><p>We applied the CSS normalization method on the usage data of SSOAR. Due to an error in our submission process we uploaded a wrong CiteSeerX ranking. This wrong submission used raw citation counts not the CSS normalized values. Therefore we can not directly compare the values in the following section. In the two figures 1 and 2 we can see the effects of this normalization method for SSOAR data. The highly skewed and scattered data distributions are smoothed and values get more comparable. In the figure we see the usage data of five different years of publication: 2005, 2007, 2009, 2011 and 2013. On the double-logarithmic scale we see that the raw download and view numbers for the different publication year's differ. The range of the raw numbers is mostly between 10 3 and 10 4 . After applying the CSS normalization these values drop between 10 2 and 10 3 while having generally fewer differences between the years. We can see that the biases introduced by the different publication years are not as present as in the raw data distributions. Nevertheless the distributions keep their skewed characteristics which is usually a good thing if we want to use these data for re-ranking purposes <ref type="bibr" coords="5,224.07,202.68,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Impact on the Document Ranking</head><p>To quantify the impact of our adjusted ranking formula we calculated Kendall's τ comparing the rank correlation between the original ranking of the two systems SSOAR and CiteSeerX and our experimental ranking. For SSOAR the original ranking is a more or less standard Solr ranking without any special modifications. For CiteSeerX the original ranking is that of the original platform. Our experimental ranking is the CSS-normalized popularity ranking described in section 2.2 using full text downloads and record view counts for SSOAR and raw citation rates for CiteSeerX.</p><p>For SSOAR the two different rankings are totally different and more-or-less independent from each other with a τ -value of -0.013. The minimum τ -value of -1 is measured for query ssoar-q62 and the maximum τ -value of 0.666 for query ssoar-q42. The absolute distribution of τ -values can be seen in figure <ref type="figure" coords="5,472.84,384.57,3.87,8.74" target="#fig_2">3</ref>. With most of the values being in the range of [-0.2, 0.2] we see that most of the rankings are different, showing the very high impact of our new weighting method.</p><p>For CiteSeerX the impact is quantified with an average τ -value of 0.491. The minimum τ -value of 0.021 is measured for query citeseerx-q137 while the maximum τ -value of 1 is recored for queries citeseerx-q190 and citeseerx-q127. In contrast to the clear and obvious re-ranking behaviour in the SSOAR use case the influence on the CiteSeerX rankings is not that big. Please keep in mind that these values are based on a wrong submission. They are therefore not suitable for direct comparison with the rankings in SSOAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We could see a high re-ranking impact on the SSOAR ranking using our CSSnormalized popularity ranking method. In CiteSeerX this impact is also measurable but not as clear as in the SSOAR use case. This can be explained with the fact that in SSOAR there is just a plain Solr text-based relevance ranking while CiteSeerX itself uses popularity measures (citation counts) to influence their default ranking and of course our submission process error as mentioned before. Nevertheless the high number on average citation per CiteSeerX document (364 citations) is an indicator for the citation-based ranking method used in CiteSeerX. The candidate documents are generally highly-cited documents,   making our popularity-ranking method not as effective compared to the simple text-based ranking of SSOAR where usage data and the popularity ranking idea can show some effects.</p><p>In fact the CSS normalized popularity ranking approach performed best in TREC OpenSearch 2016. However we must admit that the number of wins was to low to generalize on this single living lab evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgement</head><p>We would like to thank the whole SSOAR team that supported us to implement the LL4IR API into the productive system and opening up the system for academic research. They were patient and big-hearted during the deployment phase when some bugs seriously hit the server. Thank you for your understanding and your support.</p><p>This work was supported by the ESF Research Networking Programme ELIAS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,134.77,585.99,345.83,8.74;6,134.77,597.94,345.83,8.74;6,134.77,609.90,335.19,8.74;6,169.35,366.96,276.66,207.50"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Plots of the counter cumulative distribution function of document downloads for three years of publication 2007, 2009, and 2014. Top: Absolute number of document downloads. Bottom: Smoothed numbers using the CSS method.</figDesc><graphic coords="6,169.35,366.96,276.66,207.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.77,585.99,345.83,8.74;7,134.77,597.94,345.82,8.74;7,134.77,609.90,302.25,8.74;7,169.35,366.96,276.66,207.50"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Plots of the counter cumulative distribution function of document views for three years of publication 2007, 2009, and 2014. Top: Absolute number of document views. Bottom: Smoothed numbers using the CSS method.</figDesc><graphic coords="7,169.35,366.96,276.66,207.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,134.77,247.61,345.83,8.74;8,134.77,259.57,159.87,8.74"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Plot of Kendall's τ values for each query from round #1 and #2 for SSOAR (left) and CiteSeerX (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,127.36,345.83,220.04"><head>Table 1 :</head><label>1</label><figDesc>Overview on different types of usage data found in systems and evaluation campaigns</figDesc><table coords="2,134.77,154.84,345.83,192.57"><row><cell>System Available usage data</cell><cell>Used in</cell></row><row><cell>Regio Játék Historical click-through data</cell><cell>CLEF 2015 LL4IR [5]</cell></row><row><cell>SSOAR Number of document views and</cell><cell>TREC 2016 OpenSearch</cell></row><row><cell>PDF full text downloads</cell><cell></cell></row><row><cell>CiteSeerX Number of citations</cell><cell>TREC 2016 OpenSearch</cell></row><row><cell cols="2">is quite new is not able to gather as many clicks as a best-selling item that is part</cell></row><row><cell cols="2">of the catalogue for quite some time. This is directly connected to the second</cell></row><row><cell cols="2">issue: A direct comparison of the click-through rates without a normalization or</cell></row><row><cell>homogenization is not possible.</cell><cell></cell></row><row><cell cols="2">Click-through data is not the only way to measure usage. In table 1 we see</cell></row><row><cell cols="2">the three systems used in last year's CLEF LL4IR workshop and this year's</cell></row><row><cell cols="2">TREC OpenSearch workshop. All systems offer a different set of usage data:</cell></row><row><cell cols="2">Click-through data in Regio Játék, document views and PDF download rates in</cell></row><row><cell>SSOAR, and citation counts in CiteSeerX.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,127.36,345.83,95.14"><head>Table 2 :</head><label>2</label><figDesc>Corpus statistics on the popularity data gathered from SSOAR and CiteSeerX.</figDesc><table coords="4,136.16,154.84,328.65,67.66"><row><cell cols="2">SSOAR downloads SSOAR views</cell><cell>CiteSeerX cites</cell></row><row><cell># docs total 24,760</cell><cell>24,760</cell><cell>9,724</cell></row><row><cell># docs w. usage data 21,523</cell><cell>24,724</cell><cell>4,682</cell></row><row><cell>max 504,720</cell><cell>21,788</cell><cell>11,648</cell></row><row><cell>sum 6,549,674</cell><cell>9,822,049</cell><cell>3,545,420</cell></row><row><cell>avg 264.505</cell><cell>396.658</cell><cell>364.605</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,623.92,335.87,7.86;3,144.73,634.88,234.98,7.86"><p>As we had direct access to the SSOAR productive system, we were able to get these data without using any additional steps like web scraping.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,645.84,335.87,8.12;3,144.73,657.44,188.29,7.47"><p>See implementation details at https://lucene.apache.org/core/2_9_4/api/core/ org/apache/lucene/search/Similarity.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,545.62,342.24,7.86;8,146.91,556.58,333.68,7.86;8,146.91,567.54,278.18,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,197.77,556.58,183.44,7.86">Overview of the trec 2016 open search track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,404.17,556.58,76.42,7.86;8,146.91,567.54,163.76,7.86">Proceedings of the Twenty-Fifth Text REtrieval Conference</title>
		<meeting>the Twenty-Fifth Text REtrieval Conference</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,579.29,342.24,7.86;8,146.91,590.25,333.68,7.86;8,146.91,601.21,73.85,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,202.95,579.29,277.64,7.86;8,146.91,590.25,215.04,7.86">Characteristic scores and scales: A bibliometric analysis of subject characteristics based on long-term citation observation</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Glänzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,368.28,590.25,91.08,7.86">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="102" />
			<date type="published" when="2007-01">Jan 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,612.96,342.24,7.86;8,146.91,623.92,333.68,7.86;8,146.91,634.88,333.68,7.86;8,146.91,645.84,333.68,8.12;8,146.91,657.44,169.46,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,390.98,612.96,89.61,7.86;8,146.91,623.92,234.47,7.86">Evaluating popularity data for relevance ranking in library information systems</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Plassmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Behnert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewandowski</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2857195" />
	</analytic>
	<monogr>
		<title level="m" coord="8,404.45,623.92,76.14,7.86;8,146.91,634.88,333.68,7.86;8,146.91,645.84,73.42,7.86">Proceedings of the 78th ASIS&amp;T Annual Meeting: Information Science with Impact: Research in and for the Community</title>
		<meeting>the 78th ASIS&amp;T Annual Meeting: Information Science with Impact: Research in and for the Community</meeting>
		<imprint>
			<publisher>American Society for Information Science</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,119.67,342.25,7.86;9,146.91,130.63,333.68,7.86;9,146.91,141.59,333.68,8.11;9,146.91,153.20,117.68,7.47" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,192.31,119.67,288.28,7.86;9,146.91,130.63,252.63,7.86">Der Nutzen informetrischer Analysen und nicht-textueller Dokumentattribute für das Information Retrieval in digitalen Bibliotheken</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<ptr target="http://kola.opus.hbz-nrw.de/frontdoor.php?source_opus=896&amp;la=de" />
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
		<respStmt>
			<orgName>Universität Koblenz-Landau</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="9,138.35,163.51,342.24,7.86;9,146.91,174.47,333.68,7.86;9,146.91,185.43,333.68,7.86;9,146.91,196.39,333.68,7.86;9,146.91,207.34,239.93,8.12" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,284.10,163.51,196.49,7.86;9,146.91,174.47,15.89,7.86">Historical clicks for product search: Gesis at clef ll4ir</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1391/26-CR.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,446.53,174.47,34.06,7.86;9,146.91,185.43,286.86,7.86;9,276.49,196.39,121.92,7.86">Working Notes of CLEF 2015 -Conference and Labs of the Evaluation forum</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</editor>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-08">2015. September 8-11, 2015. 2015</date>
			<biblScope unit="volume">1391</biblScope>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct coords="9,138.35,218.30,342.24,7.86;9,146.91,229.26,333.68,7.86;9,146.91,240.22,333.68,7.86;9,146.91,248.91,333.68,10.13;9,146.91,262.14,333.18,8.12;9,146.91,273.74,80.03,7.47" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,285.16,218.30,195.42,7.86;9,146.91,229.26,145.79,7.86">Ideas for a standard ll4ir extension -living labs from a system operator&apos;s perspective</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1609/16090591.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,222.98,240.22,257.62,7.86;9,146.91,251.18,69.56,7.86;9,387.81,251.18,92.79,7.86;9,146.91,262.14,31.91,7.86">Working Notes of CLEF 2016 -Conference and Labs of the Evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-08">5-8 September, 2016. 2016</date>
			<biblScope unit="volume">1609</biblScope>
			<biblScope unit="page" from="591" to="592" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct coords="9,138.35,284.06,342.24,7.86;9,146.91,295.02,291.07,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,237.44,284.06,243.16,7.86;9,146.91,295.02,136.91,7.86">A Living Lab Evaluation Envirtonment for Academic Document Repositories. Master&apos;s thesis</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tavakolpoursaleh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Bonn</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
