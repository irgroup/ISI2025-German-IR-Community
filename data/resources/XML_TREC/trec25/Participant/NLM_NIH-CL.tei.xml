<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,195.98,161.71,219.28,15.15;1,170.14,183.63,270.96,15.15">NLM NIH at TREC 2016 Clinical Decision Support Track</title>
				<funder>
					<orgName type="full">Intramural Research Program at the U.S. National Library of Medicine, National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,251.06,217.55,109.13,10.52"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">U.S. National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,195.98,161.71,219.28,15.15;1,170.14,183.63,270.96,15.15">NLM NIH at TREC 2016 Clinical Decision Support Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">586C6D098357ABCE673E9D1A27BDA600</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our approach for TREC 2016 Clinical Decision Support (CDS) track. We combined methods for question analysis, query expansion, document retrieval and result fusion to find relevant documents to a given clinical question. We submitted three automatic runs using the summaries and two automatic runs using the notes, provided for the first time at the CDS track. Our experiments showed that query expansion and rank-based result fusion led to the best performance. Our runs exploring the clinical notes used MeSH for topic analysis and achieved our best P10 score of 0.2533. Using the summaries, we obtained an infNDCG score of 0.1872 and a R-prec score of 0.1465 (score in the top 10 of 107 automatic runs submitted to the 2016 CDS track).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Clinical Decision Support (CDS) track 1 focuses on the retrieval of relevant biomedical arti-1 http://www.trec-cds.org/ cles for answering clinical questions about medical records. Like previous years, participants are tasked to retrieve full-text biomedical articles pertinent to answer questions related to three types of generic clinical questions: Treatment, Test and Diagnosis.</p><p>The topics are EHR admission notes curated by physicians from the MIMIC-III data. The notes are extracted from the history of present illness (HPI) section of the note. The HPI describes information related to the patient such as medical history, performed tests and the current diagnosis. 10 topics are provided for each question type.</p><p>For each topic, three versions of the patient records are provided (i) the EHR admission note (only the HPI section, which is the "case"), (ii) a description which removes much of the jargon and replaces clinical abbreviation and (iii) a summary which is a 1-2 sentence summary of the description. We present below an example from 2016 CDS topics: • Summary: A 63 year-old male with biphenotypic ALL, Day +32 after BMT, h/o CMV infection, aspergillus and Leggionare's disease, presents with acute onset of hypoxia accompanied by fever and two days of productive cough. His CXR showed an opacification of the left basilar lobe and also right upper lobe concerning for pneumonia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Collection</head><p>The document collection for the CDS tracks (2014-2016) is the Open Access Subset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Description</head><p>Figure <ref type="figure" coords="2,346.72,159.33,5.45,9.57" target="#fig_2">3</ref> presents an overview of our retrieval system for the 2016 CDS track. Semantic analysis of natural language questions is an important step towards the construction of relevant queries for information retrieval and question answering <ref type="bibr" coords="2,431.92,227.94,10.91,9.57" target="#b1">[2]</ref>. We explored the use of medical Subject Headings (MeSH) by (i) adding MeSH terms to the query and (ii) giving a higher weight to MeSH terms in the query.</p><p>We used the Terrier IR platform<ref type="foot" coords="2,482.48,281.05,4.23,6.99" target="#foot_1">3</ref> for indexing and retrieving documents in the collection. Terrier implements various IR models such as the Okapi BM25 probabilistic model, In expB2 (Inverse Expected Document Frequency model with Bernoulli after-effect and normalization), the classic tf-idf vector space model and Hiemstra's language model (Hiemstra LM).</p><p>To combine results of IR models, two unsupervised approaches are usually used: rank fusion and score-based fusion <ref type="bibr" coords="2,418.31,419.36,11.76,9.57" target="#b0">[1,</ref><ref type="bibr" coords="2,430.07,419.36,7.84,9.57" target="#b3">4]</ref>. Rank fusion aims at combining different ranked document lists into a single rank list in order to improve the rankings of individual systems. Several methods can be used such as CombMAX (max of individual similarities), CombMED (median of of individual similarities) or CombSUM (sum of individual similarities) <ref type="bibr" coords="2,369.49,514.21,10.90,9.57" target="#b4">[5]</ref>. Score-based fusion aims at combining different document lists into a single one based on the score. Different methods can be used such as Reciprocal Rank Fusion (RRF) <ref type="bibr" coords="2,524.70,554.85,10.91,9.57" target="#b2">[3]</ref>.</p><p>In our experiments on the CDS 2014 test set, CombSUM outperformed the other rank-based methods and also the RRF score-based method <ref type="bibr" coords="2,310.61,609.92,10.91,9.57" target="#b0">[1]</ref>. Table <ref type="table" coords="2,359.99,609.92,5.45,9.57">1</ref> summarizes these results.</p><p>For the 2016 CDS track, we selected three IR models (Okapi BM25, TF-IDF and In expB2 ) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs</head><p>This year, participants are required to use only EHR notes, only descriptions, or only summaries for any given run submission and allowed to submit a maximum of five automatic or manual runs. At least two runs must use the EHR note. Each run consists of a ranked list of up to one thousand PMCIDs.</p><p>We submitted five automatic runs to 2016 CDS track. Two runs use only the notes and the other three runs use only the summaries:</p><p>• Run1: uses the summaries, query expansion performed using 30 expanded terms within top 20 documents, and combines the results of TF-IDF and In expB2 models using the CombSum method. • Run3: uses MeSH terms extracted from the summaries and the Okapi BM25 model.</p><p>• Run4: uses MeSH terms extracted from the notes and the Okapi BM25 model.</p><p>• Run5: uses MeSH terms extracted from the notes and combines TF-IDF and In expB2 ranks using the CombSum method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The evaluation of submissions followed standard TREC evaluation procedures. Runs are scored according to precision at 10 (P@10), R-precision (R-prec) and two inferred retrieval measures infNDCG and infAP. Table <ref type="table" coords="4,114.73,539.77,5.45,9.57" target="#tab_2">2</ref> presents the official results at CDS 2016 for our submitted runs. Bold values show best results. Our best infNDCG and infAP are obtained using the summaries (NLMrun1 using query expansion and combining TF-IDF and In expB2 ranks). The best R-prec is obtained using the summaries too (NLMrun2) and is ranked in the top 10 over 107 automatic runs. Interestingly, the best P@10 is obtained using the notes (NLMrun5). The infNDCG scores for each topic are presented in Figure <ref type="figure" coords="4,478.72,279.35,5.45,9.57" target="#fig_1">2</ref> (NLMrun1 vs. median, using summaries) and in Figure <ref type="figure" coords="4,533.79,292.90,5.45,9.57" target="#fig_2">3</ref> (NLMrun5 vs. median, using notes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we described our participation in the TREC 2016 CDS Track. We submitted five automatic runs using summaries or notes. Our experiments showed that query expansion and result fusion led to the best performance in our runs. The runs exploring the clinical notes achieved a P10 score of 0.2533 using MeSH for topic analysis and the combSUM method to combine TF-IDF and In expB2 ranks. Using the summaries, we achieved an infNDCG score of 0.1872 and a R-prec score of 0.1465 ranked in the top 10 over 107 automatic runs submitted to the CDS track.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,204.05,340.61,203.15,9.57;3,72.00,129.78,467.25,194.77"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our retrieval system</figDesc><graphic coords="3,72.00,129.78,467.25,194.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,141.74,365.84,327.77,9.57;5,100.03,129.78,411.19,220.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: infNDCG scores for each topic -using the summaries only</figDesc><graphic coords="5,100.03,129.78,411.19,220.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,154.51,622.55,302.23,9.57;5,100.03,387.37,411.18,219.12"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: infNDCG scores for each topic -using the notes only</figDesc><graphic coords="5,100.03,387.37,411.18,219.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,82.91,139.87,409.37,162.59"><head>Table 2 :</head><label>2</label><figDesc>TREC CDS 2016 results for our submitted runs</figDesc><table coords="4,82.91,162.71,409.37,139.76"><row><cell>Measure</cell><cell>NLMrun1</cell><cell>NLMrun2</cell><cell>NLMrun3</cell><cell>NLMrun4</cell><cell>NLMrun5</cell></row><row><cell></cell><cell>(Summary)</cell><cell>(Summary)</cell><cell>(Summary)</cell><cell>(Note)</cell><cell>(Note)</cell></row><row><cell>R-prec</cell><cell>0.14</cell><cell>0.1465</cell><cell>0.1405</cell><cell>0.0849</cell><cell>0.0866</cell></row><row><cell>P10</cell><cell>0.25</cell><cell>0.2467</cell><cell>0.24</cell><cell>0.24</cell><cell>0.2533</cell></row><row><cell>infAP</cell><cell>0.0239</cell><cell>0.0230</cell><cell>0.0228</cell><cell>0.0146</cell><cell>0.0202</cell></row><row><cell cols="2">infNDCG 0.1872</cell><cell>0.1853</cell><cell>0.1871</cell><cell>0.1477</cell><cell>0.1687</cell></row><row><cell cols="3">• Run2: uses the summaries, MeSH for query</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">expansion and the Okapi BM25 model.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,88.59,652.72,178.88,7.47;2,72.00,663.68,51.78,7.47"><p>http://www.ncbi.nlm.nih.gov/pmc/tools/ openftlist/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,327.19,663.68,51.78,7.47"><p>terrier.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported by the <rs type="funder">Intramural Research Program at the U.S. National Library of Medicine, National Institutes of Health</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,88.97,157.64,211.68,10.18;6,88.97,171.19,211.67,9.57;6,88.97,184.74,211.68,9.57;6,88.97,198.29,211.68,9.57;6,88.97,211.84,211.67,9.57;6,88.97,225.39,211.68,9.57;6,88.97,238.94,79.15,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,263.67,157.64,36.98,9.57;6,88.97,171.19,211.67,9.57;6,88.97,184.74,211.68,9.57;6,88.97,198.29,17.50,9.57">LIST at TREC 2015 clinical decision support track: Question analysis and unsupervised result fusion</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khelifi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,133.67,198.29,166.97,9.57;6,88.97,211.84,132.88,9.57">Proceedings of The Twenty-Fourth Text REtrieval Conference</title>
		<meeting>The Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11-17">2015. November 17-20, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.97,263.30,211.67,8.33;6,88.97,275.00,211.67,10.18;6,88.97,288.55,211.67,9.57;6,88.97,302.10,211.67,9.57;6,88.97,315.65,211.67,9.57;6,88.97,329.20,59.32,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,110.76,275.00,189.88,9.57;6,88.97,288.55,207.15,9.57">Medical question answering: Translating medical questions into sparql queries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,102.15,302.10,198.49,9.57;6,88.97,315.65,142.52,9.57">ACM SIGHIT International Health Informatics Symposium, IHI 2012</title>
		<meeting><address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.97,353.56,211.67,8.33;6,88.97,365.27,211.67,10.18;6,88.97,378.81,211.67,9.57;6,327.58,133.29,211.67,9.57;6,327.58,146.84,211.67,9.57;6,327.58,160.39,211.67,9.57;6,327.58,173.94,211.68,9.57;6,327.58,187.49,122.95,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,192.38,365.27,108.25,9.57;6,88.97,378.81,211.67,9.57;6,327.58,133.29,79.17,9.57">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,428.95,133.29,110.30,9.57;6,327.58,146.84,211.67,9.57;6,327.58,160.39,211.67,9.57;6,327.58,173.94,141.46,9.57">Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
	<note>SIGIR 2009</note>
</biblStruct>

<biblStruct coords="6,327.58,221.88,211.67,10.18;6,327.58,235.43,211.68,9.57;6,327.58,248.98,211.67,9.57;6,327.58,262.52,211.68,9.57;6,327.58,276.07,211.67,9.57;6,327.58,289.62,139.08,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,515.92,221.88,23.33,9.57;6,327.58,235.43,211.68,9.57;6,327.58,248.98,193.27,9.57">CRP henri tudor at TREC 2014: Combining search results for clinical decision support</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,327.58,262.52,211.68,9.57;6,327.58,276.07,86.69,9.57">Proceedings of The Twenty-Third Text REtrieval Conference</title>
		<meeting>The Twenty-Third Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,327.58,324.01,211.67,10.18;6,327.58,337.56,211.67,9.57;6,327.58,351.11,211.67,9.57;6,327.58,364.66,211.68,9.57;6,310.61,378.21,211.67,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,477.73,324.01,61.51,9.57;6,327.58,337.56,93.94,9.57">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,448.13,337.56,91.12,9.57;6,327.58,351.11,211.67,9.57;6,327.58,364.66,20.52,9.57">Proceedings of The Second Text REtrieval Conference, TREC 1993</title>
		<meeting>The Second Text REtrieval Conference, TREC 1993<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-09-02">August 31 -September 2, 1993. 1993</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
