<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.86,165.85,323.60,15.12;1,131.90,187.76,329.51,15.12;1,212.96,209.68,167.40,15.12">RMIT @ TREC 2016 Dynamic Domain Track: Exploiting Passage Representation for Retrieval and Relevance Feedback</title>
				<funder ref="#_zB7EuQf">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Real Thing Entertainment Pty Ltd</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,171.12,242.17,83.23,10.48"><forename type="first">Ameer</forename><surname>Albahem</surname></persName>
							<email>ameer.albahem@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.69,242.17,78.52,10.48"><forename type="first">Damiano</forename><surname>Spina</surname></persName>
							<email>damiano.spina@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.22,275.91,95.75,10.48"><forename type="first">Lawrence</forename><surname>Cavedon</surname></persName>
							<email>lawrence.cavedon@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.56,275.91,62.59,10.48"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
							<email>falk.scholer@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.86,165.85,323.60,15.12;1,131.90,187.76,329.51,15.12;1,212.96,209.68,167.40,15.12">RMIT @ TREC 2016 Dynamic Domain Track: Exploiting Passage Representation for Retrieval and Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">43C37F3B70F7634D78E15B3E7E7B191E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Dynamic Domain search task addresses search scenarios where users engage interactively with search systems to tackle domain specific information needs. In our participation, we focused on utilizing passage-based representations in document retrieval and user feedback processing. In addition, we submitted a baseline retrieval method and a manual run that considers only relevant documents in the top 1000 retrieved documents. Results show that the passage based representation is inferior to the baseline method but differences are not statistically significant in terms of the Cube Test and the Average Cube Test metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Dynamic Domain (DD) Track <ref type="bibr" coords="1,312.67,504.94,10.52,8.74" target="#b6">[7]</ref> assumes a search scenario where a user uses a search system and provides feedback to address domain specific and diverse information needs earlier in the interaction. In this task, a program called JIG acts as a simulated user to give feedback about the retrieved documents to the system. The system consumes the feedback and decides to terminate the search or provide more documents to the user. In each iteration, the system is allowed to return up to 5 documents. The final system output is then judged using the Cube Test evaluation measure <ref type="bibr" coords="1,350.70,588.63,9.96,8.74" target="#b3">[4]</ref>. In addition to the Cube Test, the track also uses the Precision at Recall <ref type="bibr" coords="1,343.13,600.59,10.52,8.74" target="#b1">[2]</ref> and Expected Reciprocal Rank <ref type="bibr" coords="1,151.23,612.54,10.52,8.74" target="#b2">[3]</ref> evaluation metrics.</p><p>We submitted a total of four runs. The first is a baseline retrieval method based on a document language model <ref type="bibr" coords="1,291.05,636.45,10.52,8.74" target="#b4">[5]</ref> as implemented in Apache Solr 1 . The second is a manual run that filters out non-relevant documents from the top 1000 documents. The remaining two runs utilize a passage representation in document ranking and query expansion. Our hypothesis is that, given that the interactive relevance feedback is given at passage-level, using a passage-based representation would make an effective use of the relevance feedback.</p><p>In the following, we first describe the submitted runs in Section 2, then we detail the experimental setup in Section 3. We report the results in Section 4 with discussion in Section 5. Section 6 concludes this paper and outlines future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>For every run, we execute 10 iterations and return the top 5 unseen documents in each iteration. The following describes the methods used to generate the runs. <ref type="foot" coords="2,146.47,241.89,3.97,6.12" target="#foot_0">2</ref>1. rmit-lm: In this method, we used the language modeling approach as implemented in Apache Solr<ref type="foot" coords="2,274.71,275.58,3.97,6.12" target="#foot_1">3</ref> using Dirichlet smoothing and default parameters. For each iteration, we return the next 5 documents in the list.</p><p>2. rmit-lm-oracle-1000: We use the document language model to retrieve the first 1000 documents, then we use the ground truth to remove non relevant documents from the initial list of documents. For each iteration, we return the next 5 relevant documents in the relevant document list. A document is relevant if it was found as relevant in the topic's list of judged documents. Note that only global topic relevance was considered.</p><p>3. rmit-lm-psg-max We split documents into half overlapped passages with a passage size of 200 words and index them in Apache Solr. We then score documents based on the maximum of their passage level relevance scores.</p><p>In particular, given a document d and query q, we score d using Equation <ref type="formula" coords="2,149.71,436.41,3.87,8.74" target="#formula_0">1</ref>:</p><p>score(q, d) = max p∈dpsg rel(q, p)</p><p>where d psg is a list of passages generated as described above for document d and rel(q, p) is calculated using a passage language model. In initial experiments using the TREC DD 2016 collections, we tested different aggregation methods such as the sum and average of scores, but the maximum produced the best results.</p><p>4. rmit-lm-rocchio-Rp-NRd-10 This run is inspired by recent work <ref type="bibr" coords="2,458.00,553.71,10.52,8.74" target="#b0">[1]</ref> that exploits negative and positive feedback based on different document representations to improve ad hoc retrieval. In this run, we use the baseline method to retrieve the top 5 documents in the first iteration. In the following iterations, we use the Rocchio algorithm <ref type="bibr" coords="2,374.20,601.53,10.52,8.74" target="#b5">[6]</ref> to reformulate the previous query using the feedback provided by the JIG program from the previous iteration. To represent relevant documents, we concatenate relevant passages from relevant documents into a pseudo-relevant passage (Rp), whereas we use the content of the non-relevant documents as the non-relevant units of Rocchio (NRd). Lastly, we use the top 10 nonnegative terms, measured by TFIDF, from the new query vector generated by Rocchio to build the new query. We set the Rocchio parameters to α = 1, β = 0.75 and γ = 0.25. We experimented with various combinations of representations of relevant and non-relevant units such as the whole content and the search keyword snippets from non-relevant documents, but the aforementioned representation produced the best results. We also tried different numbers of terms to form the new query such as 10, 20, 30, 100 and 1024 terms, but using 10 terms performed the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>In all runs, we used the TREC Dynamic Domain 2016 dataset. The dataset consists of two topic sets from two domains. We indexed each dataset using Apache Solr <ref type="foot" coords="3,177.62,301.70,3.97,6.12" target="#foot_2">4</ref> and ran experiments on each index separately. In both datasets, we stripped out all HTML tags and used Boiler Pipe<ref type="foot" coords="3,369.57,313.66,3.97,6.12" target="#foot_3">5</ref> to extract the main content. We then used Solr's English analysis to process the extracted text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Duplicate Removal</head><p>We noticed that there are many duplicate documents in the search results.</p><p>Since we want to retrieve documents as soon as possible, duplicate removal is necessary. To tackle this, we used Solr's duplication component <ref type="foot" coords="3,410.34,395.80,3.97,6.12" target="#foot_4">6</ref> to generate document signatures, iterated through duplicated signatures, and removed the duplicate documents from the index. In addition, documents that are duplicates and found in the ground truth were retained. Table <ref type="table" coords="3,355.61,433.24,4.98,8.74" target="#tab_0">1</ref> describes the number of documents and duplicates in each domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Table <ref type="table" coords="3,152.77,612.62,4.98,8.74" target="#tab_1">2</ref> and Table <ref type="table" coords="3,209.53,612.62,4.98,8.74" target="#tab_2">3</ref> show the results of the different runs at iterations 1 and 2 respectively, whereas Figure <ref type="figure" coords="3,264.94,624.58,4.98,8.74" target="#fig_0">1</ref> and Figure <ref type="figure" coords="3,328.57,624.58,4.98,8.74" target="#fig_1">2</ref> show the performance of the different methods at different iterations using the Cube Test (CT) <ref type="bibr" coords="3,420.73,636.53,10.52,8.74" target="#b3">[4]</ref> and the Average Cube Test (ACT) <ref type="bibr" coords="3,241.94,648.49,10.52,8.74" target="#b6">[7]</ref> respectively. The min, max, median and avg runs are the minimum, maximum, median and mean of all runs submitted to TREC Dynamic Domain submission system respectively.</p><p>From the tables and figures, several observations can be seen. First, the passage based representations do not beat the baseline method (rmit-lm) as measured by CT. In particular, ranking documents using passage retrieval is inferior to the rmit-lm, average and median runs. Second, the manual run (rmit-oracle-lm-100) is actually the best scoring run. Nevertheless, looking at its performance at different iterations, it can be seen that it is not always the best; see iteration 4 onward in Figure <ref type="figure" coords="4,289.15,200.69,3.87,8.74" target="#fig_0">1</ref>. However, it is the best in ACT (Figure <ref type="figure" coords="4,124.80,212.64,3.87,8.74" target="#fig_1">2</ref>).</p><p>The third observation is the use of query expansion as reported in Table <ref type="table" coords="4,124.80,236.55,3.87,8.74" target="#tab_2">3</ref>. We can see a slight improvement over the baseline method (rmit-lm) in the ACT, but a degradation in CT. The fourth observation is that the performance measured in CT and ACT decreases as many iterations are run for all methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In general, the results show that using passage representations leads to a fall in effectiveness with respect to the document-level baseline method. However, these differences are not significant. That means we have a high variety on their performances across different queries. It will be interesting to investigate what factors affect the performance for the individual queries.  The strictly decreasing performance of all runs (in particular the oracle method) across iterations indicates that it is more appropriate to compare run performances at each iteration separately than expecting to have higher scores at subsequent iterations. The ACT and CT are new measures and understanding their behavior at different iterations helps in interpreting their results; we plan to study that in future work.</p><p>The performance of our submitted runs against the mean, median and minimum of all runs submitted to the TREC Dynamic Domain might be due to the fact our duplicate removal was biased toward relevant documents. As a result, we also ran rmit-lm against an index with a blind duplicate removal (remove all duplicates without regard to their relevance), and without duplicate removal.  <ref type="table" coords="6,152.88,290.40,4.98,8.74" target="#tab_3">4</ref> shows that the Blind Removal is inferior to other methods that have comparable performance. We also conducted a significance test on the differences between the No Removal with the Blind and Biased removals, and found no significant difference at p &lt; 0.05. The slight difference between the Blind removal and the other removals might be because it misses some of the relevant documents, which leads to differences in the scores, but this is not severe enough to cause a significant degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>The TREC Dynamic Domain Track is a novel task that addresses complex search scenarios under multiple dimensions of search aspects such as search time, diversification, user feedback and relevance granularity. In our participation, in addition to a manual and a baseline runs, we submitted two runs that investigate two aspects: relevance granularity and user feedback. The former attempts to address relevance granularity by tackling the search problem as a passage retrieval, whereas the latter utilizes different combinations of document granularity (global representation and local representation using passages) to formulate new queries. Overall, the passage retrieval approach was not as effective as document retrieval, but the difference is not significant. Utilizing passages in query expansion resulted in small improvements (but not significant) over the baseline method. In future work, we plan to investigate these differences in more detail.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,193.34,329.73,206.64,8.74;5,130.34,350.42,332.64,188.81"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CT as number of iterations increases.</figDesc><graphic coords="5,130.34,350.42,332.64,188.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,189.74,554.35,213.84,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ACT as number of iterations increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,147.85,475.97,297.61,74.51"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the TREC DD 2016 datasets used in our runs.</figDesc><table coords="3,198.74,488.92,195.82,61.56"><row><cell>Domain</cell><cell>Number of</cell><cell>Number of</cell></row><row><cell></cell><cell>Documents</cell><cell>Duplicates</cell></row><row><cell>Ebola</cell><cell>194,481</cell><cell>23,496</cell></row><row><cell>Polar</cell><cell>244,536</cell><cell>11,593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,124.80,289.66,343.71,131.40"><head>Table 2 :</head><label>2</label><figDesc>Runs at iteration 1. * indicates a statistically significant improvement over the baseline method (rmit-lm) using a paired t-test with p &lt; 0.05</figDesc><table coords="4,210.36,318.63,170.32,102.43"><row><cell>Run ID</cell><cell>ACT</cell><cell>CT</cell></row><row><cell>min</cell><cell>0.0197</cell><cell>0.0291</cell></row><row><cell>max</cell><cell>0.3322</cell><cell>0.4176</cell></row><row><cell>avg</cell><cell>0.1473</cell><cell>0.2051</cell></row><row><cell>median</cell><cell>0.1516</cell><cell>0.2174</cell></row><row><cell>rmit-lm</cell><cell>0.1857</cell><cell>0.2526</cell></row><row><cell>rmit-lm-psg.max</cell><cell>0.1260</cell><cell>0.1758</cell></row><row><cell cols="2">rmit-oracle.lm.1000 0.3322</cell><cell></cell></row></table><note coords="4,335.53,410.75,2.72,6.12;4,350.71,412.32,27.67,8.74;4,378.38,410.75,4.08,6.12"><p>* 0.4176 *</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,124.80,457.64,343.71,143.35"><head>Table 3 :</head><label>3</label><figDesc>Runs at iteration 2. * indicates a statistically significant improvement over the baseline method (rmit-lm) using a paired t-test with p &lt; 0.05</figDesc><table coords="4,192.92,486.61,206.97,114.38"><row><cell>Run ID</cell><cell>ACT</cell><cell>CT</cell></row><row><cell>min</cell><cell>0.0274</cell><cell>0.0438</cell></row><row><cell>max</cell><cell>0.2899</cell><cell>0.2643</cell></row><row><cell>avg</cell><cell>0.1379</cell><cell>0.1425</cell></row><row><cell>median</cell><cell>0.1452</cell><cell>0.1469</cell></row><row><cell>rmit-lm</cell><cell>0.1614</cell><cell>0.1539</cell></row><row><cell>rmit-lm-psg.max</cell><cell>0.1178</cell><cell>0.1275</cell></row><row><cell cols="2">rmit-lm-rocchio.Rp.NRd.10 0.1644</cell><cell>0.1439</cell></row><row><cell>rmit-oracle.lm.1000</cell><cell cols="2">0.2899  *  0.2643  *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,124.80,136.43,343.71,162.70"><head>Table 4 :</head><label>4</label><figDesc>Performance of the baseline (rmit-lm) with and without duplicate removal. No Removal means no duplicates were removed; Blind means duplicates were removed regardless whether they are relevant or not, and Biased means only duplicate documents not found in the ground truth file were removed.</figDesc><table coords="6,124.80,183.99,277.67,115.14"><row><cell cols="2">Iteration Duplicate Removal</cell><cell>ACT</cell><cell>CT</cell></row><row><cell>1</cell><cell>No Removal</cell><cell cols="2">0.1857 0.2515</cell></row><row><cell></cell><cell>Blind</cell><cell cols="2">0.1807 0.2372</cell></row><row><cell></cell><cell>Biased</cell><cell cols="2">0.1857 0.2526</cell></row><row><cell>2</cell><cell>No Removal</cell><cell cols="2">0.1632 0.1545</cell></row><row><cell></cell><cell>Blind</cell><cell cols="2">0.1586 0.1492</cell></row><row><cell></cell><cell>Biased</cell><cell cols="2">0.1614 0.1539</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,140.05,668.76,328.46,6.99;2,124.80,678.22,343.71,6.99;2,124.80,687.69,343.71,6.99;2,124.80,697.15,343.71,6.99;2,124.80,706.62,343.71,6.99"><p>In the descriptions submitted to the TREC DD submission system, we reported using a combination of a unigram query and bigram phrases queries in ranking documents. However, we discovered a bug in the implementation which was causing the used search engine (Apache Solr) to rank documents based on only unigram queries. This is also applied to the other runs. In addition, the manual run's submitted file didn't include results for topic number DD16-07</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,140.05,716.12,232.17,6.99"><p>http://apache.mirror.serversaustralia.com.au/lucene/solr/6.2.0</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,140.05,691.33,232.17,6.99"><p>http://apache.mirror.serversaustralia.com.au/lucene/solr/6.2.0</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="3,140.05,700.83,161.47,6.99"><p>https://github.com/kohlschutter/boilerpipe</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="3,140.05,710.34,158.95,6.99"><p>using TextProfileSignature implementation</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This research was partially supported by <rs type="funder">Australian Research Council</rs> Project <rs type="grantNumber">LP130100563</rs> and <rs type="funder">Real Thing Entertainment Pty Ltd</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zB7EuQf">
					<idno type="grant-number">LP130100563</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,140.30,681.84,328.21,8.74;6,140.30,693.80,328.21,8.74;7,140.30,128.96,328.21,8.74;7,140.30,140.91,200.11,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,328.31,681.84,140.20,8.74;6,140.30,693.80,18.15,8.74">Utilizing focused relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brondwine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,185.81,693.80,282.70,8.74;7,140.30,128.96,296.18,8.74">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;16</title>
		<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1061" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,140.30,160.84,328.21,8.74;7,140.30,172.79,328.21,8.74;7,140.30,184.75,328.22,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,291.69,160.84,173.08,8.74">Evaluating evaluation measure stability</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,152.84,172.79,315.67,8.74;7,140.30,184.75,214.42,8.74">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,140.30,204.67,328.21,8.74;7,140.30,216.63,328.21,8.74;7,140.30,228.58,303.94,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,381.78,204.67,86.73,8.74;7,140.30,216.63,110.39,8.74">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,273.55,216.63,194.96,8.74;7,140.30,228.58,174.93,8.74">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,140.30,248.51,328.21,8.74;7,140.30,260.46,328.22,8.74;7,140.30,272.42,328.21,8.74;7,140.30,284.37,233.07,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,342.72,248.51,125.79,8.74;7,140.30,260.46,304.28,8.74">The water filling model and the cube test: multi-dimensional evaluation for professional search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,140.30,272.42,328.21,8.74;7,140.30,284.37,103.98,8.74">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,140.30,304.30,328.22,8.74;7,140.30,316.25,328.21,8.74;7,140.30,328.21,328.22,8.74;7,140.30,340.16,73.34,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,278.79,304.30,189.73,8.74;7,140.30,316.25,55.26,8.74">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,216.74,316.25,251.77,8.74;7,140.30,328.21,275.54,8.74">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,140.30,360.09,328.22,8.74;7,140.30,372.05,159.54,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,197.45,360.09,198.19,8.74">Relevance Feedback in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood, Cliffs, New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,140.30,391.97,328.22,8.74;7,140.30,403.93,40.44,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,316.76,391.97,151.75,8.74;7,140.30,403.93,35.94,8.74">Trec 2015 dynamic domain track overview</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
