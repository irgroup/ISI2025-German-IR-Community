<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,180.14,96.93,251.73,12.90;1,168.16,112.87,275.68,12.90">BJUT at TREC 2016 OpenSearch Track: Search Ranking Based on Clickthrough Data</title>
				<funder ref="#_C5Z3k2R">
					<orgName type="full">Guangxi Colleges and Universities Key Laboratory of Cloud Computing and Complex Systems</orgName>
				</funder>
				<funder ref="#_58tbtYr">
					<orgName type="full">Excellent Talents Foundation of Beijing</orgName>
				</funder>
				<funder ref="#_BYNr4uB">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,206.29,140.67,47.50,10.75"><forename type="first">Cheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Information</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,264.50,140.67,55.78,10.75"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
							<email>yangzhen@bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Information</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.22,140.67,59.17,10.75"><forename type="first">David</forename><surname>Lillis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Beijing-Dublin International College</orgName>
								<orgName type="institution" key="instit2">Beijing University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,180.14,96.93,251.73,12.90;1,168.16,112.87,275.68,12.90">BJUT at TREC 2016 OpenSearch Track: Search Ranking Based on Clickthrough Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC8E447450FDCC574446D37453F5D1EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our efforts for the TREC OpenSearch task. Our goal for this year is to evaluate the effectiveness of: (1) a ranking method using information crawled from an authoritative search engine; (2) search ranking based on clickthrough data taken from user feedback; and</p><p>(3) a unified modeling method that combines knowledge from the web search engine and the users' clickthrough data. Finally, we conduct extensive experiments to evaluate the proposed framework on the TREC 2016 OpenSearch data set, with promising results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In this year's OpenSearch Track, our main aims are: (1) Building an efficient ranking function that uses the information from a web search engine and the documents. (2) Explore a novel method that can use the clickthrough data from the feedback to improve the performance of the ranking function. (3) Create a unified model that combines both of these. As search engines play an ever more crucial role in information consumption in our daily lives, ranking relevance is always a challenge. The advancing state of the art for search engines presents new relevance challenges, which drives us towards using user feedback (e.g. clicks) to optimize the retrieval performance of search engines.</p><p>In addressing the OpenSearch task, we first crawled the document information for each query from Google Scholar, mainly document titles. We used this information to reflect the content for each document. Next, word2vector was used to build matrices to represent the documents after a series of data processing operations, and training was performed on the dataset to generate a classifier for each query. Then, we used the classifiers to make judgments about the scores of documents and uploaded the ranks to the API. In addition, we used the dataset to train the classifier including the clickthough data received as feedback through the from API. Clickthrough data has been shown to be highly useful for generating search engine rankings <ref type="bibr" coords="1,190.42,640.72,102.09,8.64;1,54.00,651.68,22.69,8.64" target="#b5">(Schuth, Balog, and Kelly 2015;</ref><ref type="bibr" coords="1,78.61,651.68,61.69,8.64" target="#b3">Joachims 2002;</ref><ref type="bibr" coords="1,142.21,651.68,150.30,8.64" target="#b4">Radlinski, Kurup, and Joachims 2008;</ref><ref type="bibr" coords="1,54.00,662.64,137.76,8.64" target="#b0">Agichtein, Brill, and Dumais 2006)</ref>. Finally, we used both of the above classifiers to create a unified modeling based on structural risk minimization, and ranked the candidate documents.</p><p>The remainder of the paper is organized as follows: In Section 2, we present our approach for ranking using web information and clickthough data. In Section 3, we report our experimental results. In Section 4, we conclude the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking System Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Gathering</head><p>The first step of the ranking process is to gather useful information. Candidate data was received from the Living Labs API, including training set and test set, each query and its corresponding doclist (a list of candidate documents) and user feedback for each query. With regard to information from the web, we crawled a doclist for each query from Google Scholar. For each query, we crawled the top documents up to a maximum of 100 documents. Document titles are the primary information received during this process. For some queries, 100 documents were not available, but the more than 70 were retrieved in each case. Moreover, we gathered a text corpus from Wikipedia to train the word2vector model. This corpus was approximately 11 GiB in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Annotation</head><p>The different search engines used in the OpenSearch task make different information available about each document. For instance, CiteSeerX provides only a single field with the full document text. In contrast, SSOAR returns many fields including abstract, author, description, identifier, language etc. The Microsoft Academic Search results include abstract and URL. Document title is common to each, so this was used for the generation of the document matrices.</p><p>Before generating the document matrices, we preprocessed the data. Then, we trained the word2vector model with the corpus and used this model to construct document matrices both of the sites and Google Scholar.</p><p>Two labels are applied to each document. One is based on its position in the Google Scholar results, and the other is The other label is based on clickthrough data. It can be understood as the probability of the document is clicked, and is defined by following formula:</p><formula xml:id="formula_0" coords="2,324.48,683.75,233.52,23.23">P (d) = 0.5+ d t -d f d t + d f + 0.5 × 0.5 1 + e -k(dt+d f -s-0.25) (1)</formula><p>where P (d) is the probability for document d, d t is the number of times the document was clicked, d f is the number of times the document was not clicked. The latter part of this formula is a logistic function to dampen the effect of clicks for small sample sizes (so that a document that has appeared once but has not been clicked will not get a probability of zero). K affects how quickly the function begins to rise and s is the shift factor. Values of K = 0.33 and s = 10 were used. A document that has never been clicked begins with a probability of 0.5, and this probability will rise or fall depending on the user clickthrough data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking Model and Unified Model</head><p>The ranking model consisted of the web information ranking model and the clickthrough data ranking model.</p><p>Existing search engines exhibit excellent performance in retrieving information, therefore the starting point for the model is to train the classifier using Google Scholar. We adopted Least Squares method into the framework, which can learn a linear model to fit the training data <ref type="bibr" coords="3,252.70,267.09,39.80,8.64;3,54.00,278.05,21.44,8.64" target="#b2">(Hu et al. 2013)</ref>. The square loss is:</p><formula xml:id="formula_1" coords="3,125.38,292.00,167.12,11.03">L(H, I) = ||GH -I|| 2 (2)</formula><p>where G is the content matrix of Google document data, H is the linear classifier, and I is the label matrix we defined. Square loss is a widely used method for text analytics. However, simple square loss alone is insufficient for training a stable and robust classifier. For the documents, it is observed that not all the words in titles are relevant to the query but instead we can use some key words to represent the documents. A mature method is sparse learning, which has been used in various fields to obtain an effective model. To ensure sparsity of the model, we used the L1-norm penalization based on square loss. To overcome overfitting, the most popular method is regularization. One of the most widely used methods is ridge regression, which introduces an L2-norm penalization. At the moment, the problem can be solved by the elastic net, which does automatic variable selection and continuous shrinkage, and selects groups of correlated variables.</p><p>This model is suitable for the web information ranking model, and can be used as clickthrough data ranking model by replacing Google document matrix G with the site document matrix C, and replacing the score labels with the second label outlined above.</p><p>A unified model was then created. As with the ranking model, we used square loss to train the model, which can be written as:</p><formula xml:id="formula_2" coords="3,58.98,587.71,233.52,11.72">min L(H 1 , H 2 ) = ||GH 1 -GH 2 || 2 +||CH 1 -CH 2 || 2 (3)</formula><p>where H 1 and H 2 are the classifiers obtained from the ranking model. L2-norm penalization was then used to control the robustness of the learned model <ref type="bibr" coords="3,210.38,628.37,82.12,8.64;3,54.00,639.33,21.44,8.64" target="#b1">(Beck and Teboulle 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Generation</head><p>For the training dataset, we used the classifier obtained from ridge regression to rank. As for the test dataset, first we calculated the cosine similarity between the test queries and training queries, which can be defined as following formula:</p><formula xml:id="formula_3" coords="3,385.32,73.29,172.68,23.23">sim(q i , q j ) = q i • q j ||q i || • ||q j || (4)</formula><p>where q i and q j are two vectors representing a test query and a training query and ||q|| is the length of the vector q. And then, after normalizing the cosine similarity vector, we could get the classifiers of test queries, as follows:</p><formula xml:id="formula_4" coords="3,405.35,156.67,152.65,30.32">H i = n j=1 α ij H j (5)</formula><p>where α ij is the parameter of the cosine similarity with normalization, H i is the test query classifier and H j is the training query classifier.</p><p>If the two queries are semantically similar to each other, they should have a close representative in the word2vector model within the large scale corpus. Considering this aspect, we used the equation ( <ref type="formula" coords="3,431.63,264.39,3.87,8.64">5</ref>) to express the classifiers, and ranked the test dataset with these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>This section, we introduce the evaluation methods and our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Measures</head><p>During the OpenSearch runs, CiteSeerX and SSOAR both use interleaved comparisons on their live websites. The specific type of interleaving used is Team Draft Interleaving (TDI), whereby the rankings produced by participating teams are interleaved with the current production ranking of the site. Users are shown this interleaved ranking, but are unaware of the origin of the results. The ranker (participant or production) that contributes more documents that users click is preferred.</p><p>OpenSearch used five metrics for evaluation, as follows <ref type="bibr" coords="3,343.05,478.25,127.69,8.64" target="#b5">(Schuth, Balog, and Kelly 2015)</ref>:</p><p>• Impressions: the total number of times when rankings (for any of the test queries) from the given team have been displayed to users.</p><p>• Wins: a win occurs when the ranking of the participant has more clicks on results assigned to it by Team Draft Interleaving than clicks on results assigned to the ranking of the site.</p><p>• Losses: A loss is the opposite to a win.</p><p>• Ties: a tie occurs when the ranking of the participant obtains the same number of clicks as the ranking of the site.</p><p>• Outcome: Outcome is defined as:</p><formula xml:id="formula_5" coords="3,395.00,642.53,163.00,22.31">wins = wins wins + losses (6)</formula><p>An outcome value below 0.5 means that the ranking of the participant performed worse than the ranking of the site (i.e., in overall, it has more losses than wins). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Analysis</head><p>Two sets of results were obtained: one based on the training data and the other based on the test data.</p><p>Table <ref type="table" coords="4,89.35,270.72,4.98,8.64" target="#tab_0">1</ref> shows our results from CiteSeerX for the training data. The evaluation method is Team Draft Interleaving (TDI), it statistically test whether the number of wins for the better retrieval function is indeed significantly larger by using a test against outcome ≤ 0.5. The outcome of the training data is 0.4468, which means the ranking function of the site performs better than ours, although not to a substantial degree.</p><p>Table <ref type="table" coords="4,88.31,358.39,4.98,8.64" target="#tab_1">2</ref> shows our from CiteSeerX on the test data. There were three rounds for the test period. In the first round, the site had the better performance than our system, in the second round, our system performed better, and in the third round, the outcome was 0.5432, our system performed better too. Around the all rounds, our ranking function is effective and not bad.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we study the problem of optimizing the ranking function with clickthrough data. We used the word2vector model to represent the queries and the documents effectively instead of traditional vector space or other model. A unified framework is proposed, incorporating web information and clickthrough data. In the optimization process, the popular regularization methods of elastic net regression and ridge regression are adopted. Experiments over long periods were conducted to evaluate the proposed frame-work on a real world academic search engine and the experimental results demonstrate the effectiveness of our proposed framework.</p><p>In the future work, we will consider more features to better represent documents and conduct further extensive experiments to improve the robustness and stability of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,319.50,302.52,238.50,8.64;1,319.50,313.48,238.50,8.64;1,319.50,324.44,238.50,8.64;1,319.50,335.40,44.54,8.64"><head>Figure 1</head><label>1</label><figDesc>Figure 1 shows our system framework. It consists of four principal parts: (1) Information gathering, (2) Document annotation, (3) Ranking model and Unified model, (4) Results generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,246.68,576.07,118.64,8.64;2,82.80,54.00,446.41,507.63"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Framework.</figDesc><graphic coords="2,82.80,54.00,446.41,507.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,59.98,64.27,227.27,45.00"><head>Table 1 :</head><label>1</label><figDesc>Training Data Performances.</figDesc><table coords="4,59.98,89.28,227.27,20.00"><row><cell cols="6">type Outcome Wins Losses Ties Impressions</cell></row><row><cell>train</cell><cell>0.4468</cell><cell>21</cell><cell>26</cell><cell>5</cell><cell>52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,59.98,138.01,235.58,66.92"><head>Table 2 :</head><label>2</label><figDesc>Test Data Performances.</figDesc><table coords="4,59.98,163.02,235.58,41.91"><row><cell cols="6">Round Outcome Wins Losses Ties Impressions</cell></row><row><cell>1</cell><cell>0.3333</cell><cell>3</cell><cell>6</cell><cell>1</cell><cell>10</cell></row><row><cell>2</cell><cell>0.6</cell><cell>6</cell><cell>4</cell><cell>1</cell><cell>11</cell></row><row><cell>3</cell><cell>0.5432</cell><cell>44</cell><cell>37</cell><cell>15</cell><cell>96</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61671030</rs>), the <rs type="funder">Excellent Talents Foundation of Beijing</rs>, the <rs type="projectName">Importation and Development of High-Caliber Talents Project of Beijing Municipal Institutions</rs> (No.<rs type="grantNumber">CIT&amp;TCD201404052</rs>), and the <rs type="funder">Guangxi Colleges and Universities Key Laboratory of Cloud Computing and Complex Systems</rs> (<rs type="grantNumber">No15205</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BYNr4uB">
					<idno type="grant-number">61671030</idno>
				</org>
				<org type="funded-project" xml:id="_58tbtYr">
					<idno type="grant-number">CIT&amp;TCD201404052</idno>
					<orgName type="project" subtype="full">Importation and Development of High-Caliber Talents Project of Beijing Municipal Institutions</orgName>
				</org>
				<org type="funding" xml:id="_C5Z3k2R">
					<idno type="grant-number">No15205</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,319.50,269.67,238.50,8.64;4,319.50,280.63,238.50,8.64;4,319.50,291.41,238.50,8.82;4,319.50,302.36,238.50,8.59;4,319.50,313.32,112.54,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,516.09,269.67,41.91,8.64;4,319.50,280.63,238.50,8.64;4,319.50,291.58,14.39,8.64">Improving web search ranking by incorporating user behavior information</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,353.41,291.41,204.59,8.59;4,319.50,302.36,238.50,8.59;4,319.50,313.32,50.65,8.59">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,319.50,327.66,238.50,8.64;4,319.50,338.44,238.50,8.82;4,319.50,349.40,171.75,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,455.38,327.66,102.63,8.64;4,319.50,338.62,204.27,8.64">A fast iterative shrinkagethresholding algorithm for linear inverse problems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,535.32,338.44,22.68,8.59;4,319.50,349.40,112.53,8.59">SIAM journal on imaging sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,319.50,363.74,238.50,8.64;4,319.50,374.52,238.50,8.82;4,319.50,385.47,238.50,8.82;4,319.50,396.61,34.59,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,506.31,363.74,51.70,8.64;4,319.50,374.69,124.64,8.64">Social spammer detection in microblogging</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,515.70,374.52,42.30,8.59;4,319.50,385.47,132.60,8.59">Joint Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2633" to="2639" />
			<date type="published" when="2013">2013</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,319.50,410.77,238.50,8.64;4,319.50,421.55,238.50,8.82;4,319.50,432.51,238.50,8.59;4,319.50,443.47,98.39,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,401.26,410.77,156.74,8.64;4,319.50,421.73,49.50,8.64">Optimizing search engines using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,390.35,421.55,167.65,8.59;4,319.50,432.51,238.50,8.59;4,319.50,443.47,25.86,8.59">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,319.50,457.80,238.50,8.64;4,319.50,468.58,238.50,8.82;4,319.50,479.54,238.50,8.59;4,319.50,490.50,111.46,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,518.15,457.80,39.85,8.64;4,319.50,468.76,170.59,8.64">How does clickthrough data reflect retrieval quality?</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kurup</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,508.65,468.58,49.35,8.59;4,319.50,479.54,238.50,8.59;4,319.50,490.50,48.36,8.59">Proceedings of the 17th ACM conference on Information and knowledge management</title>
		<meeting>the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,319.50,504.84,238.50,8.64;4,319.50,515.80,238.50,8.64;4,319.50,526.58,238.50,8.82;4,319.50,537.54,238.50,8.82;4,319.50,548.67,57.27,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,507.12,504.84,50.89,8.64;4,319.50,515.80,238.50,8.64;4,319.50,526.76,53.05,8.64">Overview of the living labs for information retrieval evaluation (ll4ir) clef lab 2015</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,397.78,526.58,160.22,8.59;4,319.50,537.54,211.87,8.59">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="484" to="496" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
