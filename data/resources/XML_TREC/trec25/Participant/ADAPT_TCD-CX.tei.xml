<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.46,152.67,304.20,12.64;1,188.06,170.67,219.04,12.64">ADAPT_TCD: An Ontology-Based Context Aware Approach for Contextual Suggestion</title>
				<funder ref="#_2GvAYn7">
					<orgName type="full">European Regional Development Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.06,209.92,73.50,9.94"><forename type="first">Mostafa</forename><surname>Bayomi</surname></persName>
							<email>bayomim@tcd.ie</email>
						</author>
						<author>
							<persName coords="1,334.99,209.92,73.14,9.94"><forename type="first">SÃ©amus</forename><surname>Lawless</surname></persName>
							<email>seamus.lawless@scss.tcd.ie</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">ADAPT Centre, Knowledge and Data Engineering Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.46,152.67,304.20,12.64;1,188.06,170.67,219.04,12.64">ADAPT_TCD: An Ontology-Based Context Aware Approach for Contextual Suggestion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">31872072E2EC591F4C07AED31063334D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we give an overview of the participation of the ADAPT Centre, Trinity College Dublin, Ireland, in both phases of the TREC 2016 Contextual Suggestion Track. We present our ontology-based approach that consists of three models that are based on an ontology that was extracted from the Foursquare category hierarchy. The three models are: User Model, Document Model and Rule Model. In the User Model we build two models, one for each phase of the task, based upon the attractions that were rated in the user's profile. The Document Model enriches documents with extra metadata from Foursquare and categories (concepts) from the ontology are attached to each document. The Rule model is used to tune the score for candidate suggestions based on how the context of the trip aligns with the rules in the model. The results of our submitted runs, in both phases, demonstrate the effectiveness of the proposed methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In TREC 2016, we participate in both phases of the Contextual Suggestion track <ref type="foot" coords="1,446.74,473.84,3.24,5.83" target="#foot_0">1</ref> . The track goal is to provide a venue for the evaluation of systems that are able to make suggestions for a particular person (based upon their profile) in a particular context <ref type="bibr" coords="1,456.46,498.95,10.69,8.96" target="#b1">[2]</ref>. This year the track consists of two phases. In Phase 1 we were provided with 495 user profiles. Each profile consists of attractions and the user's opinion regarding them. A user could rate an attraction from 0 (strongly uninterested) to 4 (strongly interested). Items are labelled as -1 if the user didn't provide a rating. Additionally, a user's profile has information about the context of the trip that they are about to take. Information such as the Group they are travelling with (alone, friends, family, other), the trip Season (summer, winter, autumn, spring), the trip Type (holiday, business, other), and the trip Duration (night out, day trip, weekend, longer). This year the organisers released a fixed set of candidate suggestions for 272 contexts, each context representing a city in the United States. The candidate suggestions set consists of approximately 1.2 million candidate attractions.</p><p>The first phase of the task required participants to provide a ranked list of up to 50 suggested attractions for the user to visit, from the provided set of attractions, tailored to each user based on his/her profile. In Phase 1, unlike previous years, we were not asked to setup a live server that could listen to requests from users, rather, we were provided with profiles and the suggestion process was to be completed offline.</p><p>In Phase 2 (the batch phase), for each provided user profile and contextual preferences, we were asked to rank sets of candidate attractions that had been suggested during the first phase.</p><p>In the following we describe our proposed approach for both Phase 1 and Phase 2, and then discuss our submitted runs along with the results achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Approach</head><p>Our approach is based on three models: User Model, Document Model and Rule Model. The three models are based on an ontology that was built from the Foursquare category hierarchy <ref type="foot" coords="2,199.37,338.10,3.24,5.83" target="#foot_1">2</ref> .</p><p>The ontology is considered the central point of our approach. In the Document Model we enrich the provided documents and identify the set of classes from the ontology that a document belongs to (see Section 2.2). The Rule Model consists of contextspecific rules, and penalises documents that are instances of classes that don't match these rules (see Section 2.3). In the User Model we suggest and rank the candidate documents based on their semantic and tag similarity scores with the user model (Sections 3 &amp; 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Building The Ontology</head><p>An ontology is a formal naming and definition of the types, properties, and interrelationships of the entities that exist for a particular domain. It describes individuals (instances), classes (concepts), attributes, and relations. In our approach we used an ontology to model the users and the attractions. We exploited the Foursquare category hierarchy to build this ontology.</p><p>The Foursquare category hierarchy is a hierarchy that represents Foursquare categories and the relations between them. For example, "American Restaurant" is a category in the hierarchy that is a child of the "Food" category. Each attraction in Foursquare belongs to one or more of these categories. Hence, we considered a category as a class in the ontology and the attractions that belong to that category as instances of that class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Model</head><p>The first step in our approach is to model documents (attractions) and enrich them. The organisers released a set of approximately 1.2 million candidate suggestions situated across 272 context cities. Each candidate attraction has an attraction ID, a context city ID, a title, and a URL. Some of the provided attractions' URLs are the attraction's home page. The home page of an attraction is expected to contain content that is highly relevant to the attraction. This means that the content of the web page should only be related to what this attraction is, or what this attraction serves (in the case where the attraction is a restaurant), and this page should not contain any information about users' ratings or opinions about that attraction. Hence, we leveraged Foursquare to enrich the attractions and model them.</p><p>For each attraction, we started by querying its title and location against the Foursquare website <ref type="foot" coords="3,183.50,319.14,3.24,5.83" target="#foot_2">3</ref> . The Foursquare search returns a list of attractions that are sorted based on their relevance to the query. We select the first (most relevant) attraction from that list.</p><p>In some cases, the selected attraction is not the actual attraction that we are looking for. This is usually due to one of three reasons:</p><p>1-The attraction that we are looking for is not in Foursquare and thus Foursquare search returns any available attractions for the location that was given in the query. 2-The original title of the attraction in the TREC dataset, which we used to generate a query, is not formatted in a manner suited for use as a query on Foursquare. For example, the title is too long or is the description of the attraction.</p><p>3-The original title of the attraction in the TREC dataset is not correct for the attraction. For example, the attraction could be a profile page for someone on Facebook and the title is for a coffee house.</p><p>Hence, we apply a filter process to check if the attraction returned from Foursquare is, in fact, the attraction that we are looking for. We measure the lexical overlap between the attraction's original title (from the TREC dataset) and the returned attraction's title (from Foursquare). If the lexical overlap score between the two titles exceeds a predefined threshold, the returned attraction is considered to be a correct match and we use it in the next step. If the overlap score is under the threshold, we ignore that attraction and do not include it in the final dataset. The threshold was set based on the length of the attraction's original title after removing common words.</p><p>After retrieving the attraction's page from Foursquare we start to parse and extract valuable information such as: a) List of categories (e.g. American Restaurant, Park, Museum). b) Average users' rating. c) List of tags<ref type="foot" coords="4,205.73,173.31,3.24,5.83" target="#foot_3">4</ref> (e.g. "good for a quick meal", "clean", "good for families"). d) Users' rating count (number of users who rated this attraction). e) Users' review count (number of users who gave a review for this attraction). The list of categories identifies to which category (or set of categories) an attraction belongs. As we mentioned before, we consider a category as a class in our ontology. This class will be used to measure the semantic similarity between an attraction and the user model (see Section 4.2).</p><p>We extract users' rating and review counts to be used in the inter-ranking process between attractions. The intuition behind using those counts is: if an attraction a has been rated by 500 users and attraction b has been rated by 3 users, this means that a is more credible than b even if b has a rate higher than a. The same applies for the review count as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Rule Model</head><p>The Rule Model consists of rules that identify the relevance of an attraction to the context of the trip using the class (or classes) that the attraction is an instance of. The trip context comprises: the Group that a user is travelling with (alone, friends, family, other), the trip Season (summer, winter, autumn, spring), the trip Type (holiday, business, other), and the trip Duration (night out, day trip, weekend, longer). Each of these properties has a set of classes in the ontology that are not suitable for that property. For example, the "beach" class is of less relevance when a trip has a Season of "winter".</p><p>If at least one of the classes that the attraction is an instance of violates at least one rule in the model, then this attraction is ignored (in Phase 1) or given a lower rank (in Phase 2). For example, consider a user for whom the "Museum" class is listed as one of their favourite classes in the user model. This user's trip has a Duration of "night out". Since a museum is typically not suitable for a night out, in the rule model the "Museum" category is incompatible with the "night out" trip Duration. Hence, all attractions that are instances of "Museum" are ignored (or given lower rank) and are not selected by our approach even if the user has a preference for museums in her/his user model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Phase 1 3.1 User Modelling</head><p>In Phase 1, we model the users based on the positively rated attractions in the user's profile. A different strategy was taken in Phase 2 (see section 4.1). The positively rated attractions are those that have got a rating of either 3 or 4 by the user. The user model was built as follows:</p><p>1-For each user and for each attraction we create an index of all the classes (e.g. American Restaurant), based on Foursquare data, that the user has rated positively along with the tag set tc that were found on that attraction's page on Foursquare. 2-We compute the count per class and merge all the tags into one list. The tags are then added to the positive user model. 3-By having all positive classes, we compute the percentage of each class in the positive model. For example, "American Restaurant" class represents 31% of the classes in the positive model. Figure <ref type="figure" coords="5,153.26,365.73,4.98,8.96">1</ref> shows a sample of a user's positive model.</p><p>For a given place p where a user is travelling to, we start to select the documents that match the classes in the positive model. Then we eliminate the documents that belong to a class that violates at least one rule in the rule model. As the target for this phase is to return a list of up to 50 ranked suggestions, we mapped the class percentage in the user model to 50 and represented it as a number, say x. Then we start to select the top x attractions of this class from the retrieved documents after ranking them.</p><p>Ranking between documents of the same class is achieved based on four features: 1-The average users' rating. 2-The users' rating count.</p><p>3-The users' review count. 4-The tag similarity measure between a document's tags set td and the classes' tags set tc (see Section 4.3). For example, for a given class c, say "American Restaurant", in the user model, we select all documents that are instances of this class (Dc) and eliminate documents that are also instances of classes that violate rules in the rule model (Dc'). Then we rank the remaining documents based on the aforementioned features. Suppose that class makes up 32% of the classes in the positive user model -32% of 50 is 16. Hence, we select the top 16 documents of the final set.</p><p>After we select all documents for all classes in the user model, according to their percentage, we start to rank the documents based on the first three of the features mentioned before and return the final ranked list. The notion behind not including the tags similarity measure in ranking the documents in the final list is that these documents are not instances of the same class (category), which means that they have different sets of tags. For example, documents that are instances of class "Gym" have completely different tags than documents that are instances of class "Japanese Restaurant". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Not Enough Attractions</head><p>For some profiles and for some places, it is possible that the number of attractions belonging to a specific class, in a specific city do not meet the required number. For example, for a specific profile and a specific city, suppose that the required number of attractions that are instances of the "American Restaurant" class is 16. However, in that city, there are only 10 attractions with type "American Restaurant". In this instance, we supplement the list to make up for the shortfall using two different approaches (for two different runs): a) For the first run, we compensate for the shortfall by getting more attractions from the other classes in the user model. For example, the "American Restaurant" class has only 10 attractions in the city and the required number is 16. The other classes in the user model e.g. "Museum, Chinese Restaurant, etc." have more attractions than what are required by the user model. We combine the remaining, overflow attractions from the other classes into a list. We then rank them based on the aforementioned criteria. Finally we then source the missing 6 attractions from the top of the ranked overflow list. b) For the second run, we compensate for the shortfall for a specific class by looking, in the ontology, for the class that has the highest ontological similarity to that class (see Section 4.2). For example, the "New American Restaurant" class is considered the highest ontologically similar class to the "American Restaurant" class. We then retrieve the attractions that belong to that class, rank them and select the number that we require.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Phase 2</head><p>In Phase 2, we used the same Document Model (Section 2.2) and Rule Model (Section 2.3) but we followed a different strategy to build the user model. The following sub-sections demonstrate how the Phase 2 user model was built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">User Modelling</head><p>For a given user (U), we build two user models: a positive model (Upos) and a negative model (Uneg). The positive model is built based on the positively rated attractions in the user's profile (attractions that were given a rating of either 3 or 4 by the user) and the negative model is built based on the negatively rated attractions in the user's profile (attractions that were given a rating of either 0 or 1 by the user).</p><p>In this phase, we do not compute the count per class, instead, we just collect the documents' classes and their tags in each model. Figure <ref type="figure" coords="7,362.83,395.61,4.98,8.96" target="#fig_0">2</ref> illustrates what the user model looks like in Phase 2.</p><p>Both user models (positive and negative) may have classes in common, i.e. the user could have positively rated an attraction that is an instance of "American Restaurant" and have negatively rated another attraction that is also an instance of "American Restaurant". This is why we harvest the tag list from Foursquare. This list of tags is considered an indication of why the user has given a positive or negative rating for that attraction. Furthermore, the negative model could have some classes that are not in the positive model, which in turn means that these classes are not preferable for that user. Hence, we add these classes to the rule model to give a lower rank to documents that are instances of these classes along with the classes that are already in the rule model.</p><p>After modelling the user profile and modelling the candidate suggestions, we divide these candidate suggestions into three groups:</p><p>Group 1: Working URLs and do not violate rules. Group 2: Working URLs and violate rules. Group 3: Not working URLs.</p><p>While checking the URLs of the candidate suggestions, it was noted that some were not working, hence, we put these documents, for each profile, in one group (Group 3) and move this group to the end of the final ranked list. Group 1 contains the documents that have working URLs and their classes do not violate any rule in the rule model while Group 2 contains the documents that have working URLs but their classes violate at least one of the rules in the rule model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ontology-Based Semantic Similarity</head><p>Our approach is based upon the ontological similarity between a document and all the documents in the user's positive profile Upos. i.e. for a given document d, the semantic similarity between that document and the user's positive model is the summation of weighted semantic similarity scores between the document's class set and the class set of every document in the positive model.</p><p>Semantic similarity has been widely used in many research fields such as Information Retrieval <ref type="bibr" coords="8,199.73,567.23,11.69,8.96" target="#b2">[3]</ref> and Text Segmentation <ref type="bibr" coords="8,313.25,567.23,10.70,8.96" target="#b0">[1]</ref>. Ontology-based similarity can be classified into three main approaches: Edge-counting, Feature-based and Information Content (IC) based approaches. In our approach, we rely on an Edge-counting approach proposed by Wu and Palmer <ref type="bibr" coords="8,239.21,604.43,11.69,8.96" target="#b4">[5]</ref> as its performance is deemed better than other methods <ref type="bibr" coords="8,124.70,616.79,10.69,8.96" target="#b3">[4]</ref>.</p><p>The principle behind Wu and Palmer's similarity computation is based on the edgecounting method, whereby the similarity of two concepts is defined by how closely they are related in the hierarchy, i.e., their structural relations. Given two concepts c1 and c2, the conceptual similarity between them is:</p><formula xml:id="formula_0" coords="9,195.53,150.42,137.57,8.96">ConSim (c1, c2) = 2*N / (N1+N2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ï¨ï±ï©</head><p>Where N is the distance between the closest common ancestor (CS) of c1 and c2 and the hierarchy root, and N1 and N2 are the distances between the hierarchy root on one hand and c1 and c2 on the other hand respectively.</p><p>The similarity between two documents can be defined as a summation of weighted similarities between pairs of classes in each of the documents. Given two documents d1 and d2, where d1 is a document in the candidate suggestions set and d2 is a document in the user's positive model, the similarity between the two documents is:</p><formula xml:id="formula_1" coords="9,153.74,274.33,291.25,39.64">ðððððð(ð1, ð2) = â â ð¶ððððð(ð ð , ð ð ) ð ð=1 ð ð=1 ð Ã ð ï¨2ï©</formula><p>Where m and n are the two sets of classes that d1 and d2 have respectively. As mentioned above, the semantic similarity between one of the candidate suggestion documents and the user's positive model can be defined as a summation of weighted semantic similarity scores between the document's class set and the class set of every document in the positive model:</p><formula xml:id="formula_2" coords="9,155.30,385.72,186.10,34.68">ð ððððð¡ððððð(ð, ð ððð  ) = â ðððððð(ð, ðð) ð· ð=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ï¨3ï©</head><p>Where D is the set of documents in the positive model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tags similarity</head><p>The list of tags associated with an attraction is the list of keywords extracted from the users' reviews about that attraction on Foursquare. These keywords (tags) are considered a representation of the "taste" of the attraction and depicts why a user has rated this attraction positively or negatively. For example, consider a user who has positively rated an attraction that has a list of tags that contains: "sushi bar, good for dates". This means that a candidate suggestion that has similar tags is deemed to be more preferable by the user. In our approach we use tag similarity as a factor in the ranking process. We measure the lexical (word) overlap between an attraction's tags list and the documents' tags list in the user's positive model and add the score to the final ranking score.</p><p>Consider d1 is a document in the candidate suggestion set and d2 is a document in the user's positive model, the similarity between the two documents is:</p><formula xml:id="formula_3" coords="9,168.26,631.20,273.37,24.36">ð¡ððð(ð1, ð2) = ðð. ðð ðð£ðððððððð ð¤ðððð  ð¥ Ã ð¦ ï¨4ï©</formula><p>Where x and y are the two sets of tags that d1 and d2 have respectively. The total tag similarity score of a candidate suggestion is:</p><formula xml:id="formula_4" coords="10,201.89,147.73,113.21,34.68">ð ð¡ððð ððð = â ð¡ððð(ð, ðð) ð· ð=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ï¨5ï©</head><p>Where D is the set of documents in the positive model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ranking Methodology</head><p>For each candidate suggestion in the user profile, we measure two similarity scores: 1-Semantic Similarity (semanticSim) 2-Tag Similarity (tagSim). The final ranking score is calculated as follows: dtotalSim = dsemanticSim + dtagSim + drating ï¨6ï© Where drating is the document's rating on Foursquare.</p><p>As mentioned above, we divided the candidate suggestions into three main groups: 1-Working URLs and do not violate rules. 2-Working URLs and violate rules.</p><p>3-Not working URLs.</p><p>We applied our ranking methodology separately for each group and then merged the groups, after ranking, into the afore-mentioned order to create the final list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We participated in the two phases of the Contextual Suggestion track. In Phase 1, we submitted two runs (see Section 3.2) and in Phase 2 we submitted 3 runs. This year, the Track organisers evaluated all submitted runs using three evaluation metrics, namely, Reciprocal Rank, P@5 and ndcg@5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Phase 1 Results</head><p>In this phase we submitted two runs:</p><p>1-ADAPT_TCD_r1 (Section 3.2.a) 2-ADAPT_TCD_r2 (Section 3.2.a) As the results show, our runs are competitive and perform above the TREC median in both cases. In particular, the first run is the best of the two. Overall, the results for both of our runs exhibit promising performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Phase 2 Results</head><p>In this phase we submitted three runs:</p><p>1-ADAPT_TCD_br1: in this run, we measure the similarity between the candidate suggestions and the user positive model based on the semantic and tag similarity measures (see Section 4.4). 2-ADAPT_TCD_br2: in this run we divided the positive user model into two positive models: positive_4 contains the documents that received a rating of 4 from the user. While positive_3 contains the documents with a rating of 3. The similarity between every candidate suggestion and each positive profile attraction is measured. If the score of a candidate suggestion with positive_4 is higher than with positive_3, it is placed in a group that is at the top of the final ranked list. 3-ADAPT_TCD_br3: in this run we followed the same approach as the first run but we did not measure the tag similarity between documents (see Section 4.3). The intuition behind this run is to assess the impact of tags on measuring the relatedness of a candidate suggestion to a user model. Table <ref type="table" coords="11,150.01,412.65,4.98,8.96" target="#tab_4">2</ref> reports the performance of our submitted runs in Phase 2 along with the TREC median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>Reciprocal The results show that the performance of our first two runs outperform the TREC median in two of the evaluation metrics (P@5 and ndcg@5). Also the results depict that the second run has the same performance as the first one. Which in turn means that dividing the user positive model into two models does not provide any benefit in the ranking process.</p><p>The third run has the lowest performance of the three runs (except in the Reciprocal Rank metric), which means that the use of tag similarity (run1 &amp; run2) has a positive impact on ranking performance.</p><p>Overall, the results for the first two runs exhibit promising above-median performances, and hence merit further study in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>This paper describes our participation in both phases of the TREC 2016 Contextual Suggestion track. We proposed an approach that consists of three models that are based on an ontology that was extracted from the Foursquare category hierarchy. The User Model is used to model users based upon their profiles, the Document Model is used to model documents and enrich them, and the Rule Model is used to tune the score for candidate suggestions based upon the context of the trip and how it aligns with the rules in the model. In the first phase we submitted two runs. The results exhibit a good performance where both runs outcome the TREC median. In the second phase we submitted three runs, and results also show that our approach for the second phase is promising.</p><p>Moving forward, viable future work may involve: a) Adding some exceptions to the rule model: There could be an allowance made for instances of a class which violate a rule, but have exceptionally high rankings within that city (or exceptionally high rating and review counts). For instance, a visitor to Sydney is likely to want to go see Bondi Beach, even if it is winter time. b) Enhancing the user model in phase 1 by following the same strategy that we have followed in phase 2 (Section 4.1).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,219.53,442.46,156.08,8.10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A sample of Phase 2 user models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,124.70,569.87,345.95,85.40"><head>Table 1</head><label>1</label><figDesc>reports the performance of our two submitted runs together with the TREC Median</figDesc><table coords="10,130.34,607.55,313.66,47.72"><row><cell>Runs</cell><cell>Reciprocal Rank</cell><cell>P@5</cell><cell>ndcg@5</cell></row><row><cell>ADAPT_TCD_r1</cell><cell>0.5777</cell><cell>0.4066</cell><cell>0.2643</cell></row><row><cell>ADAPT_TCD_r2</cell><cell>0.5512</cell><cell>0.4098</cell><cell>0.2595</cell></row><row><cell>TREC Median</cell><cell>0.5041</cell><cell>0.3508</cell><cell>0.2133</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,227.21,659.11,140.59,8.10"><head>Table 1 .</head><label>1</label><figDesc>Results of our runs in Phase 1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,130.34,450.45,314.98,72.38"><head>Table 2 .</head><label>2</label><figDesc>Results of our runs in Phase 2</figDesc><table coords="11,130.34,450.45,314.98,60.82"><row><cell></cell><cell>Rank</cell><cell>P@5</cell><cell>ndcg@5</cell></row><row><cell>ADAPT_TCD_br1</cell><cell>0.5472</cell><cell>0.4241</cell><cell>0.2720</cell></row><row><cell>ADAPT_TCD_br2</cell><cell>0.5472</cell><cell>0.4241</cell><cell>0.2720</cell></row><row><cell>ADAPT_TCD_br3</cell><cell>0.5996</cell><cell>0.3931</cell><cell>0.2612</cell></row><row><cell>TREC Median</cell><cell>0.6015</cell><cell>0.3931</cell><cell>0.2562</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,129.74,678.61,159.93,7.24"><p>https://sites.google.com/site/treccontext/trec-2016</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,136.10,679.09,147.33,7.24"><p>https://developer.foursquare.com/categorytree</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,136.10,678.61,74.41,7.24"><p>https://foursquare.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,136.10,678.53,192.42,7.32"><p>Foursquare tags are keywords extracted from users' reviews</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements.</head><p>The <rs type="institution">ADAPT Centre for Digital Content Technology</rs> is funded under the <rs type="grantName">SFI Research Centres Programme</rs> (Grant <rs type="grantNumber">13/RC/2106</rs>) and is co-funded under the <rs type="funder">European Regional Development Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2GvAYn7">
					<idno type="grant-number">13/RC/2106</idno>
					<orgName type="grant-name">SFI Research Centres Programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,156.74,491.87,309.73,8.96;12,156.74,503.39,295.85,8.96;12,156.74,514.91,288.06,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,392.13,491.87,74.33,8.96;12,156.74,503.39,244.98,8.96">OntoSeg: A Novel Approach to Text Segmentation Using Ontological Similarity</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bayomi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Levacher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Ghorab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lawless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,431.06,503.39,21.53,8.96;12,156.74,514.91,250.96,8.96">IEEE International Conference on Data Mining Workshop (ICDMW)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,156.74,534.47,275.12,8.96;12,156.74,545.87,307.29,8.96" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>E. Overview of the TREC 2013 contextual suggestion track</note>
</biblStruct>

<biblStruct coords="12,156.74,565.43,291.38,8.96;12,156.74,576.95,297.97,8.96;12,156.74,588.47,282.58,8.96;12,156.74,599.87,308.93,8.96;12,156.74,611.39,36.73,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,255.58,588.47,179.90,8.96">Information Retrieval by Semantic Similarity</title>
		<author>
			<persName coords=""><forename type="first">Angelos</forename><forename type="middle">;</forename><surname>Hliaoutakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tuc)</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voutsakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Petrakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Milios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,156.74,599.87,298.49,8.96">International Journal on Semantic Web and Information Systems (IJSWIS)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Technical University of Crete (TUC), G., Varelas, Giannis (Technical University of Crete</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,156.74,630.95,310.10,8.96;12,156.74,642.35,17.61,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,187.77,630.95,191.93,8.96">An information-theoretic definition of similarity</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,386.11,630.95,21.10,8.96">ICML</title>
		<imprint>
			<biblScope unit="page" from="296" to="304" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,156.74,661.94,306.39,8.96;12,156.74,673.46,300.65,8.96;12,156.74,684.98,242.18,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,251.28,661.94,155.00,8.96">Verbs Semantics and Lexical Selection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,413.11,661.94,50.02,8.96;12,156.74,673.46,296.76,8.96">Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 32Nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
