<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,99.79,84.43,410.14,14.93">The University of Stavanger at the TREC 2016 Tasks Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.71,116.75,80.03,10.37"><forename type="first">Darío</forename><surname>Garigliotti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Stavanger</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.63,116.75,75.38,10.37"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Stavanger</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,99.79,84.43,410.14,14.93">The University of Stavanger at the TREC 2016 Tasks Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A2D26A6892BD6E79BC11B6BC11A2FB19</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the Task understanding task of the Tasks track at TREC 2016. We introduce a general probabilistic framework in which we combine query suggestions from web search engines with keyphrases generated from top ranked documents. We achieved top performance among all submitted systems, on both official evaluation metrics, which attests the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aim of the TREC Tasks track <ref type="bibr" coords="1,205.74,333.98,87.16,8.64">(Yilmaz et al., 2015)</ref> is to devise evaluation methodology for measuring retrieval systems' ability to understand the tasks that users aim to achieve. In our participation, we focus on Task understanding, one of the problems addressed by the track. Specifically, it asks for a ranked list of keyphrases "that represent the set of all tasks a user who submitted the query may be looking for" <ref type="bibr" coords="1,72.29,417.67,80.26,8.64">(Yilmaz et al., 2015)</ref>. The goal is to provide a complete coverage of subtasks for an initial query, while avoiding redundancy. The suggested keyphrases are judged with respect to each subtask on a three point scale (non-relevant, relevant, and highly relevant). Subtasks are only used in the evaluation, and not available when generating the keyphrases. In addition to the initial query string, the entities mentioned in there are also made available (identified by their Freebase IDs). The quality of the ranked list of keyphrases is evaluated using diversity-aware metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Our approach to task understanding makes use of two keyphrase sources: (i) keywords extracted from relevant documents and (ii) web search engine suggestions. The two sources are combined linearly in a probabilistic scoring model for estimating the probability that a keyphrase q was generated by the initial query q 0 : P(q|q 0 ). The overview of our approach is depicted in Fig. <ref type="figure" coords="1,182.47,652.08,3.74,8.64" target="#fig_0">1</ref>. Formally:</p><formula xml:id="formula_0" coords="1,94.29,668.26,158.12,17.29">P(q|q 0 ) = σP s (q|q 0 ) + (1 -σ)P k (q|q 0 ),</formula><p>where the keyphrase generation probabilities from web search engine suggestion (subscript s) and keyword extraction (subscript k) are combined using the mixture weight σ.</p><p>The web search engine suggestions are the top-10 query suggestions from the Google and Bing search engines. We obtained these through their RESTful suggestion APIs, using the original query as input.</p><p>Estimating P k (q|q 0 ) entails three main steps. First, we obtain the top-10 results from web search engines using the original query and extract a set of keywords from them. Second, we construct possible keyphrase candidates from the original query and the extracted keywords. In the final, third step, we score each candidate keyphrase by combining our confidence scores from the previous steps in a generative probabilistic framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extracting keywords</head><p>We issue the initial query (q 0 ) against two web search engines (Google and Bing) and collect from each the top-K documents (K = 10). Each document is represented with two fields: d snippet holds the snippet that was displayed on the SERP and d content stores the full document content (stripped from HTML elements). We extract keywords from each document field using the RAKE keyword extraction system <ref type="bibr" coords="1,334.20,710.82,71.39,8.64">(Rose et al., 2010)</ref>. We note that these keywords are ac-tually phrases, i.e., may consist of more than a single term. For each keyword k extracted from document field d f , the associated confidence score is denoted by s(k, d f ). We filter the extracted keywords from noise by retaining only those that: (i) have an extraction confidence above a given threshold; (ii) are at most 5 terms long; (iii) each of the terms has a length between 4 and 15 characters and is either a meaningful number (i.e., max. 4 digits) or a term (excluding noisy substrings and reserved keywords from mark-up languages).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating candidate keyphrases</head><p>Given the initial query and the extracted keywords, we form a weighted set of candidate keyphrases. We consider three different ways of combining a keyword k and the initial query q 0 :</p><p>(1) adding k as a suffix to q 0 ;</p><p>(2) adding k as a suffix to an entity mentioned in q 0 ;</p><p>(3) returning k as-is, without considering q 0 . If the initial query contains multiple entities, then (2) is performed for each of the mentioned entities. Each generated keyphrase q is assigned a weight based on which rule generated it:</p><formula xml:id="formula_1" coords="2,92.38,373.95,200.52,34.95">s(q, q 0 , k) =    α, if q = q 0 ⊕ k, β, if q = e ⊕ k, γ, otherwise,<label>(1)</label></formula><p>where α + β + γ = 1. When a keyphrase could be generated by multiple rules, we take the one with the highest weight.</p><p>We use a custom operator for concatenation, ⊕, which first removes the longest common subphrase from the rightside concatenation term. For example: "aa bb" ⊕ "bb cc" = "aa bb cc" and "choose bathroom decor" ⊕ "bathroom decor style" = "choose bathroom decor style." Such a removal strategy is motivated by the fact that in English most of the usual (non-adjectival) refinements are syntactically performed as an addition to the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scoring keyphrases</head><p>We now introduce the generative probabilistic model P k (q|q 0 ), which operates as follows. First, we consider the documents that are relevant to the initial query. Then, we take the keywords that are generated by these documents. Finally, we use both the initial query and the keywords as generators of keyphrases. The graphical representation of the model is depicted in Figure <ref type="figure" coords="2,179.98,655.50,3.74,8.64" target="#fig_1">2</ref>. Formally:</p><formula xml:id="formula_2" coords="2,90.09,678.01,166.52,19.30">P(q|q 0 ) = ∑ d P(d|q 0 ) ∑ k P(q|q 0 , k)P(k|d).</formula><p>(2)</p><p>This model has three components: • P(d|q 0 ) expresses the relevance of document d to the initial query q 0 . Since we do not have relevance scores for documents, we make the simplifying assumption that all documents are equally important, i.e., set this probability uniformly across documents. This is reasonable as we only consider documents from the first search result page.</p><formula xml:id="formula_3" coords="2,416.78,60.93,46.74,95.07">d q 0 d k k q q • • • • • • • • •</formula><p>• P(k|d) is the keyword generation probability, which is estimated according to:</p><formula xml:id="formula_4" coords="2,398.17,342.57,157.74,24.17">P(k|d) ∝ ∑ f ∈F λ f s(k, d f ),<label>(3)</label></formula><p>where s(k, d f ) is the confidence score from keyword extraction, F is the set of document fields (snippet, content), and λ f are the corresponding field weights (∑ f ∈F λ f = 1). To simplify notation, we write λ to denote λ snippet , which implies that λ content = 1λ.</p><p>• P(q|q 0 , k) is the keyphrase generation probability, which is set proportional to the weight of the rule that was used when creating that candidate keyphrase (cf. Eq. ( <ref type="formula" coords="2,356.33,482.26,3.54,8.64" target="#formula_1">1</ref>)): P(q|q 0 , k) ∝ s(q, q 0 , k).</p><p>(4)</p><p>We omitted the normalizer terms in Eqs. ( <ref type="formula" coords="2,481.63,520.80,3.87,8.64" target="#formula_4">3</ref>) and (4) for readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Official Runs and Results</head><p>We submitted the following three runs:</p><p>UiS4 For a given initial query, we use uniformly the web search engine suggestions with the highest priority in the ranking. Formally, P s (q|q 0 ) is uniformly distributed in Eq. ( <ref type="formula" coords="2,366.49,644.14,3.53,8.64">2</ref>), and σ is chosen such that σP s (q|q 0 ) is larger than the maximum of the scores of any other possible candidate. Next in the ranking follow the keyphrases generated as described in Sect. 2.2, using only the extracted keywords as-is (i.e., by making γ = 1 in Eq. ( <ref type="formula" coords="2,541.75,691.96,3.54,8.64" target="#formula_1">1</ref>)); λ is set such that keywords originating from the snippet field have priority over the ones from the content field. Table <ref type="table" coords="3,101.53,263.10,3.88,8.64">1</ref>: Configuration settings used in our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Web search Generated engine suggestions keyphrases</p><formula xml:id="formula_5" coords="3,90.21,317.20,157.73,33.21">UiS4 Set γ = 1 UiS8 List γ = 1 UiS9 Not used α, β &gt; γ</formula><p>UiS8 This approach is essentially the same as UiS4, but here the web search engine suggestions are ranked with a score P s (q|q 0 ) proportional to their rank position, as retrieved from the suggestion APIs. As before, they are still placed in positions higher than any other keyphrase. The other keyphrases in the ranking are obtained and scored as before. UiS9 In this approach we obtain keyphrases by using all the generation rules, cf. Sect. 2.2. The scoring schema is such that it forces to rank first keyphrases that are not from the last rule (i.e., by setting α and β in Eq. ( <ref type="formula" coords="3,264.87,497.00,3.87,8.64" target="#formula_1">1</ref>) such that α &gt; γ and β &gt; γ) and from any field; then follow the extracted keywords as-is from the snippet field; and finally the extracted keywords from the content field. We do not make use of web search engine suggestions at all, i.e. σ = 0 in Eq. ( <ref type="formula" coords="3,169.08,556.77,3.53,8.64">2</ref>).</p><p>A more concise summary of our three systems is shown in Table <ref type="table" coords="3,78.61,591.27,3.74,8.64">1</ref>, by indicating how each of the two main keyphrase sources contributes to the final ranking.</p><p>Table <ref type="table" coords="3,88.72,615.18,4.98,8.64" target="#tab_0">2</ref> presents the evaluation scores of our runs, and the minimum, mean, and maximum performances of each query across all the participating systems. The relative ordering of our three systems is UiS9 &lt; UiS4 &lt; UiS8. UiS9 is improved significantly (p &lt; 0.05 using a two-tailed paired T-test) by UiS4, and UiS8 outperforms UiS4 by the same level of significance. The differences between UiS8 and UiS9 are highly significant (p &lt; 0.001). Our run UiS8 is the best performing among all submitted system according to both evaluation metrics <ref type="bibr" coords="3,500.31,434.58,55.60,8.64;3,316.81,446.54,21.44,8.64">(Verma et al., 2016)</ref>. Further, UiS4 is the second best performing system according to the main evaluation metric ERR-IA@20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>Additionally, we compare, for each of our submissions, the differences per query between our α-NDCG@20 and the mean performance according to the same metric. Fig. <ref type="figure" coords="3,550.93,555.41,4.98,8.64" target="#fig_2">3</ref> shows these differences for each of our three approaches.</p><p>After comparing Fig. <ref type="figure" coords="3,416.03,579.32,9.40,8.64" target="#fig_2">3a</ref> and Fig. <ref type="figure" coords="3,466.45,579.32,8.30,8.64" target="#fig_2">3b</ref>, it is clear that the web search engine suggestions, while keeping their relative ranking positions as they are provided, contribute substantially. This might also give some clues about the nature of the ground truth dataset, as most of those results come from the same kind of search engine suggestions. The lack of such suggestions in UiS9 is somehow compensated with our keyphrase generation method. However, relying only on generated keyphrases (UiS9) is not very effective on its own; roughly half of the queries have a negative difference. Our other two systems (UiS4 and even more so UiS8) end up with a good amount of large positive differences.</p><p>We have described our participation in the TREC 2016 Tasks track. We have introduced a general probabilistic framework in which we combine query suggestions from web search engines with keyphrases generated from top ranked documents. Our initial results have been solid in terms of absolute scores. Furthermore, we have been able to observe significant differences between the various approaches we experimented with.</p><p>In future work, we plan to extend our approach to incorporate diversity, consider other sources for extracting keywords, and investigate additional methods for generating keyphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">References</head><p>Rose, S., <ref type="bibr" coords="4,91.02,266.94,179.59,8.64">Engel, D., Cramer, N., and Cowley, W. (2010)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,349.46,570.25,173.81,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our task understanding approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,316.81,169.25,239.10,8.64;2,316.81,181.20,162.17,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical representation of our generative probabilistic model for generating keyphrases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,144.62,233.22,320.47,8.64"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evaluation of the performances of our approaches on the 2016 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,316.81,263.10,239.10,127.83"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results for our runs, as well as the mean and maximum scores across all participants. All numbers are averaged over the set of test queries.</figDesc><table coords="3,332.47,312.52,205.29,78.42"><row><cell>Run</cell><cell cols="2">α-NDCG@20 ERR-IA@20</cell></row><row><cell>UiS4</cell><cell>0.6596</cell><cell>0.5333</cell></row><row><cell>UiS8</cell><cell>0.6985</cell><cell>0.5670</cell></row><row><cell>UiS9</cell><cell>0.6056</cell><cell>0.4738</cell></row><row><cell>Participants Mean</cell><cell>0.4941</cell><cell>0.4149</cell></row><row><cell>Participants Max</cell><cell>0.7664</cell><cell>0.6821</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,266.94,239.11,179.77"><head></head><label></label><figDesc>. Automatic keyword extraction from individual documents. In Berry, M. W. and Kogan, J., editors, Text Mining: Applications and Theory. John Wiley &amp; Sons, Ltd, Chichester, UK. Verma, M., Kanoulas, E., Yilmaz, E., Mehrotra, R., Carterette, B., Craswell, N., and Bailey, P. (2016). Overview of the TREC tasks track 2016. In TREC 2016 Working Notes. Yilmaz, E., Verma, M., Mehrotra, R., Kanoulas, E., Carterette, B., and Craswell, N. (2015). Overview of the TREC 2015 tasks track. In Proceedings of The Twenty-Fourth Text REtrieval Conference, TREC 2015, Gaithersburg, Maryland, USA, November 17-20, 2015.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
