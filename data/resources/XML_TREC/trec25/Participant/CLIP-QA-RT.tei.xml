<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.49,72.22,312.73,16.98">CLIP at TREC 2016: LiveQA and RTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.96,117.84,99.85,11.32"><forename type="first">Mossaab</forename><surname>Bagdouri</surname></persName>
							<email>mossaab@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.47,117.84,90.72,11.32"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="department">iSchool and UMIACS</orgName>
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.49,72.22,312.73,16.98">CLIP at TREC 2016: LiveQA and RTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CA13F97548C90C50EDDAAF47E40DFC7E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Computational Linguistics and Information Processing lab at the University of Maryland participated in two TREC tracks this year. The LiveQA and the Real-Time Summarization tasks both involve information processing in real time. We submitted eight runs in the total. In both tasks, our best system had the highest precision among all automatic participating systems. This paper describes the architecture and configuration of the systems behind those runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We participated in the LiveQA and the Real-Time Summarization TREC 2016 tracks <ref type="bibr" coords="1,177.32,343.81,9.73,7.86" target="#b1">[1,</ref><ref type="bibr" coords="1,189.98,343.81,7.17,7.86" target="#b4">4]</ref> with systems that were derived from our earlier participation in the TREC 2015 LiveQA and Microblog tracks. Lessons learned from our previous participation were taken in consideration. We describe the systems for these tasks in the following two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">LIVEQA</head><p>We developed two systems for the LiveQA track, depicted in Figure <ref type="figure" coords="1,92.20,431.72,3.58,7.86" target="#fig_1">1</ref>. CLIP-YA searches in an extensive crawl of Yahoo! Answers (Y!A). CLIP-TW searches in a large corpus of tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Answering with Old Yahoo! Answers</head><p>Last year, we obtained 226M questions and 1.3B answers through an extensive crawl of Yahoo! Answers. We searched in a subset of those to return answers to new questions. We updated this collection by recrawling all of the questions (to gather new answers and user identifiers) and revisiting the pages of all the users (to gather more questions and users identifiers). Then, we crawled all of the new questions and users through several iterations as explained in <ref type="bibr" coords="1,240.03,556.62,9.21,7.86" target="#b2">[2]</ref>, yielding a total of 260M questions and 1.4B answers. We, then, limited our focus to the questions and answers downloaded from the main Yahoo! Answers website. <ref type="foot" coords="1,173.57,586.24,3.65,5.24" target="#foot_0">1</ref> We used this subset of 123M questions and 673M answers for both searching for candidate answers (Section 2.1.1) and training a deep neural network that ranks these candidate answers (Section 2.1.2).</p><p>The architecture of our system CLIP-YA is designed as a cascade of four stages. First, we search in the corpus to extract a list of candidate answers. Second, we score the answers based on a classifier trained on old question-answer pairs from the the Y!A crawl. Third, we rescore the answers based in part on this first score, using a classifier trained on TREC 2015 LiveQA qrels. Fourth, we optionally combine several short answers into a long one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Searching in Old Yahoo! Answers</head><p>Preliminary evidence from our participation in the first edition of this track was in favor of using more terms from the question for searching for candidate answers. In fact, the average score of 0.82 for the 11 questions for which we had used both the title and the body of the question was higher than the average score of 0.62 for the 1,068 questions for which only the title of the question was issued to the search engine. However, long queries come with a high efficiency risk, as it might take over the allowed 60 seconds to extract the posting lists and the corresponding documents from disk. For this reason, we put our index on a solid-state drive (SSD), instead of a traditional hard disk drive (HDD). This allowed us to use all of the terms of the title and the body of the question without worrying about timing out.</p><p>From last year, as well, we found that the 186 cases in which we searched in the combination of the title and the body of the old questions had an average score better than the 109 cases in which we searched only in the title field (average scores of 0.54 and 0.43, respectively). Searching in old answers appears to be better than either approach, however, as the average score of 0.67 for the corresponding 784 cases indicates. Overall, it appears that the more content we search in, the better the result we can expect. Consequently, we decided to search in the combination of the title, body, and answer of old answers, for each incoming question.</p><p>The query and indexed documents are preprocessed by substituting spaces for all characters other than alphanumeric and apostrophe, lower casing, and applying the Porter stemmer <ref type="bibr" coords="1,353.67,608.63,9.73,7.86" target="#b6">[6]</ref> from Lucene 6.<ref type="foot" coords="1,425.00,606.86,3.65,5.24" target="#foot_1">2</ref> A list of 100 candidate answers is returned, ranked by the BM25 implementation of Lucene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Initial Scoring with Old Yahoo! Answers</head><p>The Y!A crawl is useful as a repository for searching for candidate answers. A retrieval model might be able to find topically relevant answers, but it might fail to find the good answers among those. Fortunately, there is content in Y!A that we can use to train a classifier to rank topically rel- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LiveQA 2015</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates</head><p>Microblog 2015</p><formula xml:id="formula_0" coords="2,131.44,247.00,347.34,133.36">• Reranking • Summarization • Concatenation BM25 • TF-IDF • BM25 • Doc2Vec</formula><p>• Question features   evant answers. For a given old question, we assume that all (or most) of its answers are relevant, but that some are more useful than others. We extract this usefulness from the social interaction of the crowd with the answers. The Y!A community can choose up to one best-answer for any given question. Y!A users can also vote for different answers by providing thumb-ups and thumb-downs. We define a ranking of the answers for any given question by ranking the best-answer, if one is available, at the top of the list. Then we sort the remaining answers in decreasing order according to the difference between the number of thumb-ups and thumb-downs, breaking ties arbitrarily. The question then arises how best to select the training data, of negative and positive instances, on which we can train a classifier. Obviously, the answer at the top of the ranked list can be a positive instance. How best to select negative instances is, perhaps, less obvious. An answer ranked near the top of the list might actually be as good as the top one (consider a case where two identical good answers are present, but the website forces the asker to select no more than one best-answer). Some of the answers at the bottom of the list might be completely irrelevant to the question (e.g., spam). Hence, we decided to choose, as a negative instance, the answer located at the middle of the ranked list, after limiting ourselves to questions that have at least three answers.</p><p>Answers are often accompanied by user information. When this is the case, we extract the following seven integer features, which might serve as a surrogate of the reputation or the expertise of that user: the user level; the number of points; the number of questions, answers and best answers; and the numbers of friends and followers. Otherwise, we simply stuff that feature vector with zeros. Finally, each training instance is composed of the binary label for inferred utility, the title and body of the question, the answer content, and the seven-element feature vector for the answerer.</p><p>The top left corner of Figure <ref type="figure" coords="2,446.94,629.54,4.61,7.86" target="#fig_1">1</ref> shows a deep neural network for training on this collection, which we implement in Keras. <ref type="foot" coords="2,342.46,648.70,3.65,5.24" target="#foot_2">3</ref> Each text field is represented with an embedding layer of 200 dimensions, followed by an LSTM layer of 100 dimensions (the choice of LSTM was inspired by last year's best performing system <ref type="bibr" coords="2,417.53,681.84,9.51,7.86" target="#b7">[7]</ref>). Each of the user features is normalized to a value between 0 and 1, where the scaling parameters are inferred from training. The three text layers and the user layer are, then, concatenated, forming a layer of 307 dimensions, which we connect to a stack of three fully connected layers of dimensions 100, 50 and 100, respectively, followed by the output layer (i.e., label of the answer). The sigmoid activation is applied between every pair of layers, as well as within the LSTM layer.</p><p>At prediction time, this network returns, for the title and body of the new question, and the content and the user of candidate answers, a score that we use in the rescoring stage (Section 2.1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Rescoring with LiveQA 2015 Qrels</head><p>The process in Section 2.1.2 is useful for scoring old answers with respect to a new question. However, it does not use the similarity between the old and new questions. By crawling the URLs of the answers that last year's participants returned from Y!A, we construct a training corpus that contains, for each instance, the new question, the old question, the answer returned, and the annotated label. For each instance, we extract:</p><p>• Old question features: number of follows and answers.</p><p>• Old asker features: asker level (divided by 7), the ratio of the best answers that the asker has to all his answers, and the logarithms of one plus the asker points, questions, answers, friends and followers.</p><p>• Old answer features: whether the answer is a best answer, the number of thumb-ups and thumb-downs, the rating of the answer (a value provided by the asker between 0 and 5), and the count of comments that answer received.</p><p>• Similarity between the old and new questions: TF-IDF, BM25 and embedding-based similarities between the title, the body, their concatenation and the answer for the old question from one side, and the title and the body and their concatenation of the new question from the other side (i.e., 3 × 4 × 3 = 36 similarity values).</p><p>With the SVM rank software <ref type="bibr" coords="3,182.97,490.41,9.21,7.86" target="#b3">[3]</ref>, we train a learning-torank classifier using the features above, in addition to the score returned from the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Answer Generation</head><p>The TREC LiveQA limit the answer length to a maximum of 1,000 characters. We summarize each candidate answer exceeding 1,000 characters in the following way. We split it into sentences based on periods and retain the first and last sentence, and as many of the sentences with the highest Jaccard similarity to the title of the question as possible until the 1,000-character limit would be exceeded by adding an additional sentence.</p><p>For candidate answers that contain less than 1,000 characters we take a different approach. Last year's best performing system often combined multiple answers into a single one <ref type="bibr" coords="3,70.71,658.88,9.21,7.86" target="#b7">[7]</ref>. This has motivated us to create a synthetic answer in the following way. We start with the first summary and then concatenate the subsequent summaries in the ranked list that have at least 100 characters, and for which, the concatenation would not violate the 1,000-character limit. This synthetic answer is what we return as a final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answering from Twitter</head><p>Answers might also be present in a platform different from the one where the questions were posted. As an illustrative example, we build a system that attempts to answer LiveQA questions from Twitter. In this section, we describe our approach for collecting a large number of tweets that are likely to contain answers to the seven categories of the track, we describe a method for ranking the tweets using a model trained on a the qrels of a different task, and we explain how multiple tweets were combined to form a single answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Collection</head><p>We want to construct a repository of tweets that contains a substantial number of tweets that are likely to be useful for answering LiveQA questions. Both of our Twitter based systems performed poorly last year, but we learned several lessons that guided our redesign of the Twitter based system for this year's participation.</p><p>Similarly to last year <ref type="bibr" coords="3,414.80,250.35,9.21,7.86" target="#b2">[2]</ref>, we continued to use two major sources of tweets, which are (1) the tweets published through Twitter's sample stream (i.e., 1% of all public tweets) for over four years,<ref type="foot" coords="3,377.82,279.96,3.65,5.24" target="#foot_3">4</ref> and (2) a selected set of tweets collected by tracking the terms of the core vocabulary of each category. Hoping to cover questions about current events, we add, this year, (3) a third source of tweets. From the title and body of the question, we extract word bi-and tri-grams. Using Twitter's search API, we issue a series of queries using those n-grams (starting with tri-grams first), requesting up to 100 tweets for each query. We stop when the total time spent in communicating with Twitter reaches 40 seconds.</p><p>We had found that searching in tweets that are detected to be not questions (system CLIP-TW-A of our participation in TREC 2015 LiveQA) yields an average performance better than that we get when we search in tweets detected to be questions and return their replies (system CLIP-TW-Q). Hence, our system this year (CLIP-TW), is based entirely on tweets detected to be not questions.</p><p>We had also found that, for system CLIP-TW-A, the average score of the 24 tweets retrieved from the small corpus of selected tweets (collected over a period of 3 weeks), was substantially better than the average score of the 781 tweets collected from Twitter's sample stream (0.46 and 0.19, respectively). Thus, we decided to collect more selected tweets. We did so over a period of 55 days.</p><p>We observed last year that, although the performance of our Twitter based system was substantially lower than that of our Y!A based system, the average score of the answers of the Travel category was comparable between CLIP-YA (2015) and CLIP-TW-A (0.29 and 0.25, respectively). Moreover, this category appears to be the most difficult one for the former, and the easiest for the latter. We, therefore, decided to give more attention to this category, and gathered more selected tweets. We did so by collecting the tweets using the terms of the 27 sub-categories of travel defined by Yahoo! Answers, in addition to the main travel category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Rescoring with TREC 2015 Microblog Models</head><p>Our reliance only on the BM25 score last year might have contributed to the low performance of our Twitter based systems. This year, we enhance our scoring with a learning-torank (L2R) model. With a limited number of good Twitter-Table <ref type="table" coords="4,83.31,55.00,3.58,7.86">1</ref>: Performance of participating systems in the LiveQA task. This year, the average of all runs include manual runs.</p><p>Year System score (0-3) succ@2+ succ@3+ succ@4+ prec@2+ prec@3+ prec@4+ Track. The topics of that track contain three fields: a short query (usually two to three terms), a description (in the order of a sentence), and a narrative (in the order of a short paragraph). For every &lt;topic, tweet&gt; pair, we extract the following features:</p><p>• Tweet features: word and stem counts and their ratio; the number of characters in the stemmed tweet; the presence of URLs, hashtags and mentions; and the logarithm of the ratio of number of followers to the number of friends.</p><p>• Topic -tweet similarity features: similarity value between the stemmed tweet on one side, and the stemmed topic description and narrative on the other side, using TF-IDF, BM25, Jaccard similarity and doc2vec similarity (where the document vector is the mean of the vectors of its terms, trained with word2vec <ref type="bibr" coords="4,251.87,419.68,9.51,7.86">[5]</ref>).</p><p>We apply the trained model to each LiveQA question by substituting the topic description with the question title, and the topic narrative with the question body. This produces a ranked list of the candidate tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Answer Generation</head><p>One of the possible explanations for the poor performance of our Twitter based system last year is the length of the tweet. In fact, we might have been penalized by restricting ourselves to a maximum of 140 characters, while 1,000 characters could have been used. For this reason, we consider returning a concatenation of several tweets, instead of only one. First, we remove near-duplicate tweets by running a single-link clustering algorithm using Jaccard similarity with a threshold of 0.6 (which was the best threshold we had obtained in our TREC 2015 Microblog participation). With the remaining ranked tweets, we create a synthetic answer, starting with the first tweet, and then concatenating the subsequent tweets that have at least six words, without exceeding the 1,000 characters limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>Similarly to last year's scoring measure, each answer was given a score of -2 by NIST annotators if the answer is unreadable. Otherwise, the annotators assigned a score between 1 (bad) and 4 (excellent). The following performance measures are reported:</p><p>• score (0-3) is the average score over all questions after transferring 1-4 level scores to 0-3, and giving unreadable answers a score of 0.</p><p>• succ@i+ is the number of questions with a score of at least i, divided by the total number of answered and unanswered questions.</p><p>• prec@i+ is the number of questions with a score of at least i, divided by the number of answered questions.</p><p>Unfortunately, we had a couple of bad surprises regarding the annotation of our answers. Although our internal logs show that CLIP-YA answered all of the 1,015 questions, each in less than one minute, the TREC 2016 LiveQA organizers reported having received only 642 answers. The case with CLIP-TW is even worse. We timed out in only 5 out of 1,015 questions, but all of the answers were missed from the annotation pipeline. We have no clue about the cause of the missed CLIP-YA answers. However, for CLIP-TW, we found that the Java default XML parser has a known bug prohibiting it from loading XML files with emojis. <ref type="foot" coords="4,551.77,427.13,3.65,5.24" target="#foot_4">5</ref>Since emojis are present in the answers we have returned, we speculate that they raised an error that removed all of the Twitter-based answers from the annotation pipeline.</p><p>Table <ref type="table" coords="4,350.55,470.74,4.61,7.86">1</ref> shows the official scores of our CLIP-YA system, as well as the scores that we would have expected to see if all of our answers had been annotated, and if the unannotated questions had been of comparable difficulty to those that were annotated<ref type="foot" coords="4,378.50,510.81,3.65,5.24" target="#foot_5">6</ref> (CLIP-YA*). For reference, we show the average scores over all systems that have participated this year. We also show the scores of our three systems from our previous participation, the score of the best performing system over all teams (CMUOAQA) that year, as well as the corresponding average scores over all participating systems.</p><p>Looking at the score and prec@2+ columns, we see that if we were to assume that the questions are of comparable difficulty across years, our CLIP-YA system has improved substantially when compared to CLIP-YA <ref type="bibr" coords="4,499.32,606.73,24.11,7.86">(2015)</ref>. Comparing with our old selves, the evaluation results doubled (compare 0.615 vs. 1.344 and 0.632 vs. 0.328). Compared to the best performing system of last year, our score is higher by 24.3% (compare 1.081 vs. 1.344), and our prec@2+ is Figure <ref type="figure" coords="5,87.66,340.09,3.58,7.86">2</ref>: Distribution of questions over seven categories.</p><p>higher by 16.4% (compare 0.543 vs. 0.632). Finally, we observe that about two thirds of our answers (0.632) are at least fair, every second answer (0.470) is at least good, and one out of four answers (0.241) is excellent.</p><p>The improvement in the evaluation results of our CLIP-YA system between last year and this year is consistent across all categories, as can be seen in Table <ref type="table" coords="5,238.10,433.80,3.58,7.86" target="#tab_3">2</ref>. We tripled our score for the Beauty &amp; Style category, making it our best one (while last year it was second to the last). We doubled our score in Pets, Health, Art &amp; Humanities, and Travel. But the latter is, still, the most difficult category suffering, perhaps, from a low prevalence in training (Figure <ref type="figure" coords="5,259.89,486.10,3.58,7.86">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">REAL-TIME SUMMARIZATION</head><p>We participated in the push notifications task (a.k.a Scenario A), and the email digest task (a.k.a Scenario B) with automated systems similar to those we had developed last year for the Microblog Real-Time Filtering Track. The main differences, this year, are (1) we trained our reranker with the TREC 2015 Microblog qrels instead of the 2014 qrels, (2) we used the narrative field for the first time, and (3) we tried a new method for deciding whether a candidate tweet should be returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tweet Scoring</head><p>A topic is represented as a triple of a title that contains a few keywords, a description that summarizes the topic in one sentence, and a narrative that consists of a paragraph that gives more details. We stem the topic fields with the Porter stemmer as implemented in Lucene 6.0 using its default list of stopwords. We use regular expressions to normalize all the tweets before stemming by removing emoticons, user mentions, URLs, retweet (RT) indicators, and punctuation. Similarly to last year, we expand the title field of the profile with the most similar terms using word-embeddings trained on a corpus of tweets downloaded from Twitter's sample stream and a probabilistic structured query scoring scheme based on Okapi BM25. Terms from longer fields (i.e., description and narrative) were not expanded. Instead, we used BM25, Jaccard similarity and a cosine similarity based on a doc2vec representation (where the document vector is the mean of the embedding vectors of its terms). Those query-tweet similarity scores, as well as tweet-specific features (count of stems, count of stems that are not stopwords, ratio of the previous two features, count of characters in the stemmed tweet, count of URLs, count of hashtags, and count of user mentions), and a sender feature (log of the ratio of the number of followers to the number of friends) are used to train a learning-to-rank classifier with SVM rank software <ref type="bibr" coords="5,316.81,326.64,9.72,7.86" target="#b3">[3]</ref> using TREC 2015 Microblog qrels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Near-Duplicate Detection</head><p>Our near-duplicate detection algorithm is identical to last year's. We limit ourselves to tweets that have a score above a manually selected score threshold (described below). Then, we apply single-link clustering with Jaccard similarity and some manually selected clustering threshold (described below) to group near-duplicate tweets. A new cluster is created whenever a new tweet has a similarity value below the clustering threshold with all of the previously seen tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deciding when to Answer</head><p>Deciding when to set the cutoff point for returning candidate tweets is a difficult task. We tried to estimate that cutoff point in the following way. Given the title of a profile, we issue all of its terms as a query to Twitter. <ref type="foot" coords="5,526.65,495.13,3.65,5.24" target="#foot_6">7</ref> If no tweet is returned, we consider the union of tweets returned from issuing subqueries in which one term of the title was removed, and so on. We score all of the returned tweets as explained in Section 3.1. We compute the minimum, mean and maximum scores, which give us three possible relevance thresholds to be used to decide whether a tweet should be returned during the evaluation period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>Table <ref type="table" coords="5,351.17,602.44,4.61,7.86" target="#tab_4">3</ref> shows the performance of our three systems that participated in Scenario B. CLIP-B-0.6-2015 is the best system we had used in our TREC 2015 Microblog track, which had a clustering threshold of 0.6, and always returned 100 tweets per day. CLIP-B-0.6-MAX is our 2016 system with a manually selected clustering threshold of 0.6 that uses the maximum relevance threshold for deciding when to return a tweet. CLIP-B-0.3-MIN is our 2016 system with a manually selected clustering threshold of 0.3 that uses the minimum relevance threshold for deciding when to return a tweet. After the results were disseminated, we found a bug in the doc2vec similarity component that caused the scores to be very low. 8 Tables <ref type="table" coords="6,91.22,297.52,4.61,7.86" target="#tab_5">4</ref> and<ref type="table" coords="6,115.78,297.52,4.61,7.86" target="#tab_6">5</ref> show the performance of our three systems that participated in Scenario A. CLIP-A-0.7-MAX has a manually selected clustering threshold of 0.7; it uses the maximum relevance threshold for deciding when to return a tweet. CLIP-A-0.5-MEAN and CLIP-A-0.5-0 both have a manually selected clustering threshold of 0.5. The former uses the mean relevance threshold for deciding when to return a tweet, while the latter uses a fixed relevance threshold of 0, shared between all topics.</p><p>The scores of Table <ref type="table" coords="6,145.35,391.67,4.61,7.86" target="#tab_5">4</ref> are based on assessments made by mobile assessors (i.e., volunteer users who installed an application on their smartphone, and were actually receiving the notifications in real time). In this realistic setup, our three systems scored among the top four automatic systems, with system CLIP-A-0.7-MAX scoring the highest among all automatic systems. Thanks to its high clustering and relevance thresholds, it attained high precision.</p><p>Our three systems ranked lower (between 6 th and 9 th , among 34 automatic systems) based on NIST assessors (Table <ref type="table" coords="6,67.85,496.28,3.58,7.86" target="#tab_6">5</ref>). But interestingly, the system that did best in the first evaluation setup performed the worst in the second evaluation framework. This, perhaps, suggests that factors other than topical relevance (e.g., time of the day, fatigue) also affected the mobile assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We have presented the general architecture and the implementation details for the six runs we submitted for the Real-Time Summarization (RTS) task, and the two systems we built for the LiveQA task. The results suggest that a per-topic rescoring threshold and a high clustering similarity threshold can, each, improve the performance of our RTS systems. We are satisfied with the performance of our Scenario A systems, both in terms of recall and precision. We 8 Due to a multi-threading concurrency error, we were computing the average vector representation of a tweet using words different than the ones that constituted that tweet. This problem did not arise in Scenario A because the tweets were processed sequentially. plan to fix the bug we have in our Scenario B systems and evaluate their performance using the released annotations.</p><p>The lessons we have learned from our participation in the LiveQA track of last year proved to be useful. In particular, search using most of the content both in the live question and in the indexed corpus, the deep neural network, and the combination of multiple answers, all participated in the high performance of our system based on old Yahoo! Answers. With respect to the answers that have not been annotated (i.e., one third of the answers returned by the system based on old Yahoo! Answers, and all of the answers returned by our Twitter-based system), we will use the similarity of their content with the annotated answers to estimate our performance. To avoid those silent exceptions, we recommend that the next edition of the LiveQA track returns an acknowledgment message for each incoming answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was made possible by NPRP grant# NPRP 6-1377-1-257 from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,184.49,452.06,240.76,7.86"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General architectures of the two LiveQA systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,53.80,55.00,239.12,265.46"><head>Table 2 :</head><label>2</label><figDesc>Improvement of CLIP-YA between 2015 and 2016.</figDesc><table coords="5,72.38,78.66,198.90,241.79"><row><cell>Category</cell><cell cols="3">2015 2016 Improvement</cell></row><row><cell>Beauty &amp; Style</cell><cell cols="2">0.50 1.60</cell><cell>+220%</cell></row><row><cell>Pets</cell><cell cols="2">0.65 1.55</cell><cell>+138%</cell></row><row><cell>Health</cell><cell cols="2">0.77 1.50</cell><cell>+114%</cell></row><row><cell cols="3">Arts &amp; Humanities 0.64 1.37</cell><cell>+114%</cell></row><row><cell>Sports</cell><cell cols="2">0.51 0.98</cell><cell>+ 92%</cell></row><row><cell>Home &amp; Garden</cell><cell cols="2">0.60 0.96</cell><cell>+ 60%</cell></row><row><cell>Travel</cell><cell cols="2">0.29 0.63</cell><cell>+117%</cell></row><row><cell></cell><cell cols="2">Home &amp;</cell></row><row><cell></cell><cell cols="2">Garden</cell></row><row><cell>Travel</cell><cell>8%</cell><cell></cell></row><row><cell>8%</cell><cell></cell><cell></cell></row><row><cell>Pets 9%</cell><cell></cell><cell>Health 39%</cell></row><row><cell>Sports</cell><cell></cell><cell></cell></row><row><cell>10%</cell><cell></cell><cell></cell></row><row><cell cols="2">Arts &amp;</cell><cell></cell></row><row><cell cols="2">Humanities 13%</cell><cell>Beauty &amp; Style 13%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,316.81,55.00,239.12,77.88"><head>Table 3 :</head><label>3</label><figDesc>Performance of participating systems in Scenario B of the RTS task.</figDesc><table coords="5,357.59,89.12,154.50,43.75"><row><cell>System</cell><cell cols="2">nDCG1 nDCG0</cell></row><row><cell>CLIP-B-0.6-2015</cell><cell>0.0718</cell><cell>0.0718</cell></row><row><cell cols="2">CLIP-B-0.6-MAX 0.1244</cell><cell>0.0173</cell></row><row><cell>CLIP-B-0.3-MIN</cell><cell>0.0312</cell><cell>0.0312</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,97.55,55.00,411.57,67.42"><head>Table 4 :</head><label>4</label><figDesc>Performance of participating systems in Scenario A of the RTS task.</figDesc><table coords="6,97.55,78.66,411.57,43.75"><row><cell>System</cell><cell cols="7">relevant redundant not relevant unjudged length P(strict) P(lenient)</cell></row><row><cell>CLIP-A-0.7-MAX</cell><cell>91</cell><cell>1</cell><cell>89</cell><cell>507</cell><cell>679</cell><cell>0.5028</cell><cell>0.5083</cell></row><row><cell>CLIP-A-0.5-MEAN</cell><cell>158</cell><cell>7</cell><cell>171</cell><cell>911</cell><cell>1,227</cell><cell>0.4702</cell><cell>0.4911</cell></row><row><cell>CLIP-A-0.5-0</cell><cell>170</cell><cell>7</cell><cell>189</cell><cell>1,071</cell><cell>1,418</cell><cell>0.4645</cell><cell>0.4836</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,60.11,152.35,486.45,67.42"><head>Table 5 :</head><label>5</label><figDesc>Performance of participating systems in Scenario A of the RTS task.</figDesc><table coords="6,60.11,176.02,486.45,43.75"><row><cell>System</cell><cell>EG1</cell><cell>EG0</cell><cell cols="2">nCG1 nCG0 GMP.33 GMP.5 GMP.66 mean latency median latency</cell></row><row><cell>CLIP-A-0.7-MAX</cell><cell cols="3">0.2366 0.0206 0.2254 0.0093 -0.0950 -0.0629 -0.0328</cell><cell>227,092</cell><cell>178,997</cell></row><row><cell cols="4">CLIP-A-0.5-MEAN 0.2407 0.0354 0.2382 0.0328 -0.2556 -0.1656 -0.0809</cell><cell>121,940</cell><cell>12,090</cell></row><row><cell>CLIP-A-0.5-0</cell><cell cols="3">0.2397 0.0361 0.2415 0.0380 -0.3149 -0.2085 -0.1083</cell><cell>122,959</cell><cell>3,346</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,58.40,636.73,117.48,7.47"><p>https://answers.yahoo.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,321.42,711.83,112.77,7.47"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,321.42,711.83,75.18,7.47"><p>https://keras.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,321.42,711.19,214.68,8.12"><p>From http://archive.org/details/twitterstream.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,321.42,665.64,202.04,7.47"><p>http://stackoverflow.com/questions/31867818</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,321.42,675.32,234.52,7.86;4,316.81,684.29,239.12,7.86;4,316.81,693.25,239.12,7.86;4,316.81,702.22,239.11,7.86;4,316.81,711.19,234.47,7.86"><p>We are not aware of any systematic difference in question difficulty between the annotated and the unannotated set. The expected score is computed by multiplying the official score by the ratio of the total number of answers to the number of answers that were annotated: 1.58 = 1015/642.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,321.42,693.25,234.51,7.86;5,316.81,702.22,239.11,7.86;5,316.81,711.83,223.38,7.47"><p>Instead of using Twitter's search API, which is limited to tweets posted in the last two weeks, we scrape the web page https://twitter.com/i/search/timeline?q=[QUERY].</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.30,491.95,96.80,10.53" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.00,504.71,203.23,7.86;6,331.01,515.17,224.92,7.86;6,331.01,525.64,168.28,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,381.53,515.17,170.56,7.86">Overview of the TREC 2016 LiveQA track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,342.50,525.64,22.94,7.86">TREC</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.00,537.09,212.25,7.86;6,331.01,547.55,217.53,7.86;6,331.01,558.01,45.50,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,456.57,537.09,86.69,7.86;6,331.01,547.55,89.95,7.86">CLIP at TREC 2015: Microblog and LiveQA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bagdouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,440.31,547.55,22.94,7.86">TREC</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.00,569.47,212.77,7.86;6,331.01,579.93,125.98,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,385.64,569.47,142.54,7.86">Training linear SVMs in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,331.01,579.93,35.34,7.86">KDD &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.00,591.39,224.93,7.86;6,331.01,601.85,207.18,7.86;6,331.01,612.31,208.26,7.86;6,331.01,622.77,45.50,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,384.42,601.85,153.77,7.86;6,331.01,612.31,81.65,7.86">Overview of the TREC 2016 real-time summarization track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,431.05,612.31,22.94,7.86">TREC</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.00,634.23,192.14,7.86;6,331.01,644.69,213.63,7.86;6,331.01,655.15,142.00,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,331.01,644.69,213.63,7.86;6,331.01,655.15,20.08,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,369.65,655.15,74.31,7.86">Workshop at ICLR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.00,666.61,224.92,7.86;6,331.01,677.07,224.93,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,387.53,666.61,168.39,7.86;6,331.01,677.07,134.46,7.86">Readings in information retrieval. chapter An Algorithm for Suffix Stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.00,688.52,218.41,7.86;6,331.01,698.99,209.15,7.86;6,331.01,709.45,52.69,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,434.58,688.52,114.84,7.86;6,331.01,698.99,193.98,7.86">CMU OAQA at TREC 2015 LiveQA: Discovering the right answer with clues</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,331.01,709.45,22.94,7.86">TREC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
