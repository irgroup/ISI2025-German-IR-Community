<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.73,164.85,299.77,15.11">Overview of the TREC 2016 LiveQA Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.97,197.33,89.58,10.48"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
							<email>eugene@math.emory.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Emory University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.42,197.33,71.38,10.48"><forename type="first">David</forename><surname>Carmel</surname></persName>
							<email>dcarmel@yahoo-inc.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Yahoo Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.68,197.33,55.61,10.48"><forename type="first">Dan</forename><surname>Pelleg</surname></persName>
							<email>pellegd@acm.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Yahoo Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.17,197.33,64.55,10.48"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Yahoo Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.83,211.28,78.84,10.48"><forename type="first">Donna</forename><surname>Harman</surname></persName>
							<email>donna.harman@nist.gov</email>
							<affiliation key="aff3">
								<orgName type="institution">NIST</orgName>
								<address>
									<settlement>Gaithersburg</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.73,164.85,299.77,15.11">Overview of the TREC 2016 LiveQA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8E68BA154D97BC7AD692167566D2F027</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The LiveQA track, now in its second year, is focused on real-time question answering for real-user questions. During the test period, real user questions are drawn from those newly submitted on a popular community question answering site, Yahoo Answers (YA), that have not yet been answered. These questions are sent to the participating systems, who provide an answer in real time. Returned answers are judged by the NIST assessors on a 4-level Likert scale.</p><p>The most challenging aspects of this task are that the questions can be on any one of many popular topics, are informally stated, and are often complex and at least partly subjective. Furthermore, the participant systems must return an answer in under 60 seconds, which places additional, and realistic, constraints on the kind of processing that a system can do.</p><p>In addition to the main real-time question answering task, this year we introduced a pilot task aimed at identifying the question intent. As human questions submitted on forums and CQA sites are verbose in nature and contain many redundant or unnecessary terms, participants were challenged to identify the significant parts of the question. The main theme of the question is marked by the systems by specifying a list of spans that capture its main intent. This automatic "summary" of the question was evaluated by measuring its ROUGEand METEOR-based similarity to a succinct rephrase of the question, manually provided by NIST assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset: Yahoo Answers Questions</head><p>In contrast to factoid questions used in previous QA tracks, the questions posted on forum and community question answer sites such as Yahoo Answers (YA) questions are much more diverse, including opinion, advice, polls, and many other question types, thus making the task far more realistic and challenging.</p><p>The YA questions for submission were sampled from the stream of new questions arriving on the YA site, immediately upon arrival. The questions were extracted and submitted to the registered participants during a time period of 24 hours starting May 31, 2016. The questions passed a shallow automatic filtering process (spam, adult, non-English, etc.) before submission to participants. Each submitted question included a YA id (called qid), the question title, the body (if any), and the category of the question, as selected by the question's asker. The question categories, selected from the YA taxonomy, and announced in advance to participants, were:</p><formula xml:id="formula_0" coords="2,148.71,448.66,93.13,127.68">• Arts &amp; Humanities • Beauty &amp; Style • Health • Home &amp; Garden • Pets • Sports • Travel</formula><p>The distribution of categories is presented in Figure <ref type="figure" coords="2,363.66,587.31,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Testing System</head><p>Each registered participant provided a Web service that receives, as input, a YA question that contains the question title, body and category, and responds with an answer and optionally with the question summary. The testing system, developed and handled by the organizers, called all registered Web services upon any new arriving question from selected categories and stored the system responses into a pool of answers to be judged.</p><p>As in the previous year, the answer length was limited to 1000 characters, and the response time was limited to one minute, thus preventing participants from answering manually, or reusing human answers that may already accumulate on the YA site. In addition, systems could decide to answer only some of the questions, by returning a null response. Metrics were designed in order to consider both total performance (where non-answered questions are treated the same as bad answers) and system precision (where the decision not to answer a question is not penalized).</p><p>For the second task of question understanding, the systems provided a list of spans that capture the focus of the question within the title and body. In additions, systems could provide an additional free-text summary of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Data</head><p>Being the second year for the LiveQA task, the judged answers from last year could be used by participants for training their system. The Trec LiveQA 2015 dataset contains more than 1000 YA questions, each associated with approximately 20 judged answers.</p><p>In addition, YA data is publicly available and many exiting datasets could be reused for training. Among them is the a large collection of 4M question and answer pairs provided by Yahoo Research on the WebScope site (http: //webscope.sandbox.yahoo.com/catalog.php?datatype=l(setL6)).</p><p>Furthermore, a semi-official training dataset, created for last-year track, provides a set of 1000 YA questions, randomly selected from the predefined categories. A scraping program was provided, to enable extraction of the question and corresponding answers from the question ID. On top of this, one of the participants, Di Wang from CMU, shared the search results retrieved by the YA internal search engine for all the questions in the dataset. The search results are resolved questions in the site archive, each associated with many human answers that are similar to the searched question and therefore can be further used for training.</p><p>Finally, in the months leading up to the official test day (May 31, 2016), participants were allowed to experiment and validate their answering service with the testing system that called all live registered systems in a low rate of one unresolved YA question per 1-5 minutes. From March to May, 15 of these dry runs (each either 1 hour or 24 hours long) were conducted to let participants validate their systems before the official run. During the training stage, system answers were not stored nor analyzed by the testing system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Test Runs and Data</head><p>Starting May 31 at 10:00 GMT, and continuing for 24 hours, the testing system submitted newly posted questions from YA to all live registered systems at a rate of 1 question per minute. The answers and question intent were then stored, conditioned on meeting the one-minute time limit, and 1000-character length limit.</p><p>During the test period, 1,088 questions were submitted to 26 systems from 14 institutions. 21,410 valid answers were collected. The average response time was 24.3 seconds. Some questions were later filtered out by the organizers, due to late deletion on the YA site (implying spam or abusive content, reported or discovered at some later time) and due to several other constraints. The final set of 1015 questions, with their pools of valid answers, were submitted to be judged by NIST assessors. The judgment scores were: 0 -unanswered (or unreadable); 1 -poor; 2 -fair; 3 -good; 4 -excellent.</p><p>We computed 7 measures per run:</p><p>• avgScore(0-3): The average score over all questions (transferring 1-4 level grades to 0-3 score, hence treating a 1-level grade answer the same as an unanswered question). This is the main score used to rank the runs.</p><p>• succ@i+: the number of questions with score i or above (i ∈ {2..4}) divided by the total number of questions. For example, succ@2+ measures the percent of questions with at least fair grade answered by the run.</p><p>• prec@i+: the number of questions with score i or above (i ∈ {2..4}) divided by number of questions answered by the system. This measures the precision of the run, designed not to penalize unanswered questions.</p><p>In addition, each question was succinctly rephrased by its assessor. The system's summaries were compared with the manual summaries using METEOR, a standard summarization evaluation measure. This year, in addition to the participants' answers, we also crawled the YA site a week after the challenge took place, and collected two sets of communityposted answers to be evaluated by the NIST assessors. One set (denoted here as 'HumanSPEED') contained the first answers submitted to each question on the site (chronologically). Note that not all questions were answered before the date of our crawl of the YA site, highlighting even more the importance of automatic question answering for this type of questions. The other set (denoted 'HumanQUAL') contained the best answers as selected by the asker, if such a selection took place, or else by Yahoo's quality scoring algorithm. If there was only one answer which was not selected as best by the asker, it was discarded from this set (thus this set is smaller than HumanSPEED). The fact that these answers were written by users of the YA site was obfuscated from the NIST assessors by assigning them a fictitious participant ID (i.e., not identified as a . The purpose of including these additional runs was to serve as a sort of gold-standard benchmark for the automatic set -the best performance humans can offer in optimal conditions (at least for HumanQUAL). As can be seen in the Results section, these contributed answers indeed are judged higher than the participant systems' answers on all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The following tables report the list of participating systems with their performance. Table <ref type="table" coords="5,196.60,562.30,4.98,8.74">1</ref> reports the list of participants' runs and their respective institutions, ranked by performance on the avgScore metric. Table <ref type="table" coords="5,419.70,574.25,4.98,8.74" target="#tab_2">2</ref> reports the average score and succ@i+ for each run. Table <ref type="table" coords="5,339.17,586.21,4.98,8.74" target="#tab_4">3</ref> reports the prec@i+ measures for each run.</p><p>At the time of writing, we are not informed about the different approaches taken by the participating systems for answering live questions. However, we can certainly identify that most runs tried to answer all questions. This is not true for a few systems such as Y ahooLabs -Q2A, and U niversityOf M aryland -CLIP Y A who answered much fewer questions than the others. The selective Pl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>#Answered questions avgScore(0-3) succ@2+ succ@3+ succ@4+  approach taken by these runs severely affected its AvgScore, where unanswered questions are treated as poor answers by this measure. However, their precision scores which ignore unanswered questions, prec@i+, are relatively high, probably due to invoking a clever filtering rule which filters out difficult questions or poor answers. The leading participant run, Emory -Crowd, did very well compared to all other runs, by all metrics. That system is italicized as it uses a real-time crowdsourcing component as part of the overall system, while managing to respond within the allotted 60 seconds. The average score of the Emory -Crowd system 1.260 can be interpreted as follows: the automatic answers returned by this run are fair on average (recall that 2-level grade for fair answers is transformed to a score of 1). In terms of precision, Emory -Crowd did also very well, only second to the U niversityOf M aryland -CLIP Y A which answers fewer questions.</p><p>The next leading runs, CM U -OAQA, Emory -OutOf mEmory, and Y ahooLabs -Q2A, exhibit roughly similar performance on average scores, suggesting that leading participant systems are beginning to approach limitations of existing methods. More details about these runs and systems are provided in the TREC notebook paper.</p><p>The human answers contributed through the YA site (HumanQUAL and  HumanSPEED) outperformed all of the participant runs, on all metrics, by a wide margin. Note, however, that the Human-* answers were crawled one week after the question was submitted, given human contributors sufficient time to generate good answers. Interestingly, NIST judges found only half of the humancontributed answers to be "Good", and only a third to be "Excellent", which still far exceeds any of the participant runs.<ref type="foot" coords="7,324.42,477.02,3.97,6.12" target="#foot_0">1</ref> Figure <ref type="figure" coords="7,179.80,490.55,4.98,8.74" target="#fig_1">2</ref> shows the average scores of the systems broken down into the eight question categories contributing questions to the challenge, while Table <ref type="table" coords="7,445.81,502.50,4.98,8.74" target="#tab_5">4</ref> spells out the average score for each category of all runs. The categories can be classified according to question difficulty. The most difficult one, as last year, is the Travel category, for which most runs had difficulty to provide decent answers. On the other hand, the Health and the Computer&amp; Internet categories seem to be easier. This dichotomy calls for further investigation what makes some of the categories more difficult than others. One fact which may be related is that the latter categories are the most frequent of the eight, comprising roughly half of the questions sent to participants (see Figure <ref type="figure" coords="7,345.39,598.15,3.87,8.74" target="#fig_0">1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Pilot Sub-Task: Question Summary</head><p>The intent of this pilot task was to move towards question intent understanding, through summarizing the question in a shorter, more usable form. The participating systems returned spans of characters drawn from the original question text. The final summaries were determined by concatenating the returned spans within the question content as provided by the participants. In order to obtain a reasonable baselines for performance, we also generated two simple baselines, baseline : title (title of the original question, and baseline : title b ody (concatenation of the original title with the question body).</p><p>In order to measure the quality of the automatic question summaries we used an established approach in evaluating summarization by using the METEOR   score<ref type="foot" coords="9,155.44,344.70,3.97,6.12" target="#foot_1">2</ref> of each summary, as compared to the manually generated gold-standard summary provided by the NIST assessors. The METEOR score is an established evaluation metric for translation and summarization systems, as it computes similarity between a provided summary and a gold-standard summary while using paraphrases to expand the notion of similarity beyond simple word match. This is the first, pilot, year of this task, and only five systems participated. Table <ref type="table" coords="9,161.58,418.01,4.98,8.74" target="#tab_7">5</ref> summarizes the results on the METEOR metric. As can be seen, the performance of the automatically generated summaries is relatively low, and does not exceed the baselines of using the original question title or body. There is also a large variance between participant systems scores. The NUDT organization submitted 5 runs for this task, and outperformed the rest of the pilot participants. Still, this challenge seems to be far from solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>This is the second year that we ran the LiveQA track, reviving the popular QA track which has been frozen for several years. The track attracted significant attention from the Question Answering research community; 14 teams from around the world took the challenge of answering complex YA questions with original intent of being answered by humans. The quality of results is still below the human level, but is improving compared to previous year. The question summarization task is far from solved. Our plan is to run the LiveQA challenge next year, thus allowing the participants to further improve and extend their systems. We hope that additional teams will join this joint research effort of answering real users' questions in real-time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,133.77,265.83,343.72,8.74;2,133.77,277.78,284.72,8.74;2,185.32,124.80,240.60,125.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of categories in questions sent to participants. Questions moved to different categories by Yahoo editors not included here.</figDesc><graphic coords="2,185.32,124.80,240.60,125.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,153.11,488.92,305.04,8.74;8,133.77,124.77,412.49,349.03"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average scores of each systems, for each question category.</figDesc><graphic coords="8,133.77,124.77,412.49,349.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,133.77,384.95,343.71,20.69"><head>Table 2 :</head><label>2</label><figDesc>Main Task: Average Scores, and average Success scores of each run for varying success thresholds.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,133.77,376.98,343.72,20.69"><head>Table 3 :</head><label>3</label><figDesc>Matin task: Average Precision scores of each run for varying success thresholds.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,150.36,126.77,310.54,42.96"><head>Table 4 :</head><label>4</label><figDesc>Average scores across all systems, for each question category.</figDesc><table coords="9,175.22,126.77,260.81,21.09"><row><cell>Arts</cell><cell cols="3">Beauty Health Home Pets</cell><cell>Sports Travel</cell></row><row><cell cols="2">0.811 0.878</cell><cell>0.909</cell><cell cols="2">0.759 0.993 0.762</cell><cell>0.619</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,133.77,289.99,343.72,32.65"><head>Table 5 :</head><label>5</label><figDesc>Pilot task: Question summarization results: The METEOR scores for each submitted run, and for the two baselines (original question title, and original question body).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,149.01,617.63,328.47,6.99;7,133.77,627.09,225.52,6.99"><p>Including the human performance benchmark also demonstrates that human contributors, for now, still significantly outpeform our future AI overlords.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="9,149.01,667.31,267.47,8.75"><p>Standard implementation at: http://www.cs.cmu.edu/ ~alavie/METEOR/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
