<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.45,72.36,322.81,16.84;1,215.07,92.29,179.58,16.84">Overview of the TREC 2016 Real-Time Summarization Track</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder ref="#_gZxDgEU #_yCGT3k9">
					<orgName type="full">U.S. National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.80,135.13,52.12,14.80"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.22,135.13,79.44,14.80"><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
							<email>aroegies@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.74,135.13,59.40,14.80"><forename type="first">Luchen</forename><surname>Tan</surname></persName>
							<email>luchen.tan@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,150.15,147.00,99.35,14.80"><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
							<email>richard.mccreadie@glasgow.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country>Scotland, the United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.07,147.00,77.33,14.80"><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
							<email>ellen.voorhees@nist.gov</email>
							<affiliation key="aff2">
								<orgName type="institution">National Institute for Standards and Technology</orgName>
								<address>
									<settlement>Maryland</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.07,147.00,78.34,14.80"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
							<email>fdiaz@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.45,72.36,322.81,16.84;1,215.07,92.29,179.58,16.84">Overview of the TREC 2016 Real-Time Summarization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8C100A27D18C72E691E98F5E26530257</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The TREC 2016 Real-Time Summarization (RTS) Track aims to explore techniques and systems that automatically monitor streams of social media posts such as Twitter to keep users up to date on topics of interest. We might think of these topics as "interest profiles", specifying the user's prospective information needs. In real-time summarization, the goal is for a system to "push" (i.e., recommend or suggest) interesting and novel content to users in a timely fashion. For example, the user might be interested in poll results for the 2016 U.S. presidential elections and wishes to be notified whenever new results are published. We can imagine two methods for disseminating updates:</p><p>• Scenario A: Push notifications. As soon as the system identifies a relevant post, it is immediately sent to the user's mobile device via a push notification. At a high level, push notifications should be relevant (on topic), novel (users should not be pushed multiple notifications that say the same thing), and timely (provide updates as soon after the actual event occurrence as possible). • Scenario B: Email digests. Alternatively, a user might wish to receive a daily email digest that summarizes "what happened" that day with respect to the interest profiles. One might think of these emails as supplying "personalized headlines". At a high level, these results should be relevant and novel; timeliness is not particularly important, provided that the tweets were all posted on the previous day.</p><p>For expository convenience and to adopt standard information retrieval parlance, we write of users desiring relevant content, even though "relevant" in our context might be better operationalized as interesting, novel, and timely. Real-Time Summarization is a new track at TREC 2016 and represents a merger of the Microblog (MB) Track, which ran from 2010 to 2015, and the Temporal Summarization (TS) Track, which ran from 2013 to 2015 <ref type="bibr" coords="1,224.57,637.96,9.22,7.86" target="#b2">[2]</ref>. The creation of RTS was designed to leverage synergies between the two tracks in exploring prospective information needs over document streams containing novel and evolving information. The task this year directly evolved from the real-time filtering task in the TREC 2015 Microblog Track <ref type="bibr" coords="1,235.64,690.26,9.22,7.86">[6]</ref>.</p><p>Despite superficial similarities, our task is very different from document filtering in the context of earlier TREC Fil-tering Tracks, which ran from 1995 <ref type="bibr" coords="1,472.63,254.78,9.73,7.86" target="#b4">[4]</ref> to 2002 <ref type="bibr" coords="1,523.50,254.78,9.22,7.86" target="#b9">[9]</ref>, and the general research area of topic detection and tracking (TDT) <ref type="bibr" coords="1,346.88,275.70,9.22,7.86" target="#b1">[1]</ref>. The TREC Filtering Tracks are best understood as binary classification on every document in the collection with respect to standing queries, and TDT is similarly concerned with identifying all documents related to a particular event-with an intelligence analyst in mind. In contrast, we are focused on identifying a small set of the most relevant updates to deliver to users. Furthermore, in both TREC Filtering and TDT, systems must make online decisions as soon as documents arrive. In our case, for scenario A, systems can choose to push older content (latency is one aspect of the evaluation), thus giving rise to the possibility of algorithms operating on bounded buffers. Finally, previous evaluations, including TDT, TREC Filtering, and Temporal Summarization, merely simulated the streaming nature of the document collection, whereas participants in our evaluation actually operated on tweets posted in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">EVALUATION DESIGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Setup</head><p>The design of the TREC 2016 Real-Time Summarization Track largely follows the design of the real-time filtering task in the TREC 2015 Microblog Track <ref type="bibr" coords="1,468.67,509.91,9.22,7.86">[6]</ref>. Although we are interested in exploring filtering techniques over streams of social media posts in general, this year's track restricted the content under consideration to tweets due to their widespread availability. In particular, Twitter provides a streaming API through which clients can obtain a sample (approximately 1%) of public tweets, colloquially known as the "spritzer". This level of access is available to anyone who signs up for an account.</p><p>During the official evaluation period, which began Tuesday, August 2, 2016 00:00:00 UTC and lasted until Thursday, <ref type="bibr" coords="1,336.86,624.98,66.87,7.86">August 11, 2016</ref> 23:59:59 UTC, participants' systems "listened" to Twitter's live tweet sample stream to identify relevant tweets with respect to users' interest profiles.</p><p>System behavior during the evaluation period varied according to the evaluation scenario: Scenario A: Push notifications. As soon as the system identifies a relevant tweet with respect to an interest profile, it pushes (i.e., submits) the tweet to the RTS evaluation broker (via a REST API). The evaluation broker records the system submission and then immediately delivers the tweet to the mobile devices of a group of human assessors as a push notification in real time (more details in Section 2.4).</p><p>Each system was allowed to push at most ten tweets per interest profile per day. This per-day tweet delivery limit represents a crude attempt to model user fatigue in mobile push notifications. Note, however, that in this design we are not modeling real-world constraints such as "don't send users notifications in the middle of the night". This simplification was intentional. Scenario B: Email digests. The system is tasked with identifying up to 100 tweets per day per interest profile. These posts are putatively delivered to the user daily. For simplicity, all tweets from 00:00:00 to 23:59:59 UTC are valid candidates for that particular day. It is expected that systems will compute the results in a relatively short amount of time after the day ends (e.g., at most a few hours), but this constraint was not enforced. Each system recorded the results (i.e., ranked lists) for each day, which were then uploaded to NIST servers in batch shortly after the evaluation period ended.</p><p>The per-day limit of 100 tweets was arbitrarily set, but at a value that is larger than what one might expect from a daily email digest, primarily to enrich the judgment pool (more details in Section 2.5). As with scenario A, we neglected to model real-world constraints in favor of simplicity, since defining a "day" in terms of UTC does not take into account the reading habits of users in different time zones around the world. For scenario A, the RTS evaluation broker records system outputs as they are received and thus we can be sure that the participating systems are actually operating in real time. For scenario B, systems were expected to conform to the temporal constraints imposed by the task scenario (for example, to not use "future knowledge" when ranking the tweets), but there was no enforcement mechanism due to the post-hoc batch submission setup.</p><p>An important consequence of the evaluation design is that, unlike in most previous TREC evaluations, no collection or corpus was distributed ahead of time. Since each participant "listened" to tweets from Twitter's streaming API, the collection was generated in real time and delivered to each participant independently. In a 2015 pilot study <ref type="bibr" coords="2,249.17,501.97,9.22,7.86" target="#b7">[7]</ref>, we verified that multiple listeners to the public Twitter sample stream receive effectively the same tweets (Jaccard overlap of 0.999 across six independent crawls over a three day sample in March 2015). This evaluation setup was adopted in the TREC 2015 Microblog Track without any issue, thus providing large-scale validation of the design. For evaluation purposes (i.e., pool formation for judgments), the organizers also collected the live Twitter stream: this was accomplished by two independent crawlers in two geographicallydistributed datacenters on Amazon's EC2 service. Note that independent crawls do not increase coverage of the tweets received; the sole purpose of the setup was to increase redundancy, particularly robustness with respect to transient network glitches that sometimes affect tweet delivery. The union of these two crawls was designated as the "official" collection.</p><p>Another substantial departure from most previous TREC evaluations is the requirement that participants maintain a running system that continuously monitors the tweet sample stream during the evaluation period. The track orga-nizers provided boilerplate code and reference implementations, but it was the responsibility of each individual team to run its own system, connect with the RTS evaluation broker to submit results, and cope with crashes, network glitches, power disruptions, etc. The TREC 2015 Microblog Track, as well as other recent tracks at TREC that required participants to maintain "live" systems, showed that this requirement does not present an onerous barrier to entry for participating teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Run Submission</head><p>In both scenarios, systems were asked to only consider tweets in English. Each team was allowed to submit up to three runs for scenario A and three runs for scenario B. Runs for scenario A involved registering with the RTS evaluation broker to request a unique token, which was used to associate all submitted tweets to a particular run submission (see Section 2.4).</p><p>Runs were categorized into three different types based on the amount of human involvement:</p><p>• Automatic Runs: In this condition, system development (including all training, system tuning, etc.) must conclude prior to downloading the interest profiles from the track homepage (which were made available before the evaluation period). The system must operate without human input before and during the evaluation period. Note that it is acceptable for a system to perform processing on the profiles (for example, query expansion) before the evaluation period, but such processing cannot involve human input. • Manual Preparation: In this condition, the system must operate without human input during the evaluation period, but human involvement is acceptable before the evaluation period (i.e., after downloading the interest profile). Examples of manual preparation include human examination of the interest profiles to add query expansion terms or manual relevance assessment on a related collection to train a classifier. However, once the evaluation period begins, no further human involvement is permissible. • Manual Intervention: In this condition, there are no limitations on human involvement before or during the evaluation period. Crowd-sourcing judgments, humanin-the-loop search, etc. are all acceptable.</p><p>Participants were asked to designate the run type at submission time for the scenario B runs. For scenario A runs, we asked each team about the type of each of their runs over email after the evaluation period. All types of systems were welcomed; in particular, manual preparation and manual intervention runs are helpful in understanding human performance and enriching the judgment pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Interest Profiles</head><p>Interest profiles for real-time summarization are difficult to develop because of their prospective nature-this was one of the lessons learned from the real-time filtering task in the TREC 2015 Microblog Track <ref type="bibr" coords="2,439.16,679.80,9.22,7.86">[6]</ref>. For retrospective ad hoc topics over a static collection, it is possible for topic developers to explore the document collection to get a sense of the amount of relevant material, range of topical facets, etc. for a particular information need. Typically, topic developers prefer topics that have neither too many nor too few relevant documents. This is not possible for RTS interest profiles, which essentially requires "predicting the future".</p><p>The track overview paper from TREC 2015 <ref type="bibr" coords="3,225.72,99.48,9.73,7.86">[6]</ref> provides more discussion of these issues.</p><p>Just as in the TREC 2015 Microblog Track, we adopted the "standard" TREC ad hoc topic format of "title", "description", and "narrative" for the interest profiles. The socalled title consists of two to three keywords that provide the gist of the information need, akin to something a user might type into the query box of a search engine. The description is a one-sentence statement of the information need, and the narrative is a paragraph-length chunk of prose that sets the context of the need and expands on what makes a tweet relevant. By necessity, these interest profiles are more generic than the needs expressed in typical retrospective topics because the topic developer does not know what future events will occur. Thus, despite superficial similarities in format, we believe that interest profiles are qualitatively different from ad hoc topics.</p><p>Given the prospective nature of interest profiles, we employed the strategy of "overgenerate and cull". That is, we created many more interest profiles than there were resources available for assessment, with the understanding that we could cull a set of profiles after the fact to assess, guided by actual assessor interest. For 2016, the interest profiles were drawn from three sources:</p><p>1. 51 interest profiles that were assessed from the TREC 2015 Microblog Track, so that participants have access to training data.</p><p>2. 107 additional interest profiles culled from the TREC 2015 Microblog Track-the old profiles were manually filtered to retain those that were still applicable (e.g., throwing away profiles about events that have happened already) and profiles for which there would hopefully be a reasonable volume of relevant tweets.</p><p>3. 45 new interest profiles that were specifically developed from scratch for this year's track.</p><p>All interest profiles were made available to the participants before the beginning of the evaluation period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Online Judgments and Metrics</head><p>On key feature introduced in this year's track is an online evaluation component for scenario A whereby system outputs are assessed in an online manner. Our general approach builds on growing interest in so-called "Living Labs" <ref type="bibr" coords="3,278.57,575.88,14.34,7.86" target="#b11">[11]</ref> and related Evaluation-as-a-Service (EaaS) <ref type="bibr" coords="3,234.65,586.34,9.73,7.86" target="#b3">[3]</ref> approaches that attempt to better align evaluation methodologies with user task models and real-world constraints to increase the fidelity of research experiments. Our evaluation architecture is shown in Figure <ref type="figure" coords="3,252.97,628.18,4.61,7.86" target="#fig_0">1</ref> and was previously described in Roegiest et al. <ref type="bibr" coords="3,205.08,638.64,13.52,7.86" target="#b10">[10]</ref>; the entire evaluation infrastructure is open source and available on GitHub. 1  As the participating systems identify relevant tweets, they are immediately pushed to the RTS evaluation broker, which then immediately routes the tweets to assessors who have installed a custom app on their mobile devices. The tweets are rendered as push notifications on the assessors' mobile devices and are added to an assessment queue in their app for consideration. This setup has a number of distinct advantages over traditional post-hoc batch evaluations:</p><p>• Gathering relevance judgments in an online fashion has the potential to yield more situationally-accurate assessments, particularly for rapidly developing events. With post-hoc batch evaluations, there is always a bit of disconnect as the assessor needs to "imagine" herself at the time the update was pushed. With our evaluation framework, we remove this disconnect.</p><p>• An online evaluation platform allows for the possibility of user-submitted information needs, thus giving assessors the ability to judge tweets for interest profiles they are genuinely interested in.</p><p>• An online evaluation platform opens the door to providing realistic, online feedback to participants, thus potentially facilitating active learning approaches.</p><p>In this first year of the evaluation, we did not provide a mechanism for user-submitted interest profiles or an API for participants to receive feedback. However, we hope to introduce these features in the future, and the existing infrastructure provides a solid foundation to build on. In more detail, the evaluation proceeded as follows:</p><p>1. Prior to the beginning of the evaluation period, each participant's system "registers" with the RTS evaluation broker (via a REST API call) to request a unique token, which is used in future requests to associate all submitted tweets to a particular system. For the purposes of this discussion, each participant "run" is considered a separate system.</p><p>2. Whenever a system identifies a relevant tweet with respect to an interest profile, the system submits the result to the RTS evaluation broker via a REST API, which records the submission time.</p><p>3. The RTS evaluation broker immediately routes the tweet to the mobile device of an assessor, where it is rendered as a push notification containing both the text of the tweet and the corresponding interest profile.</p><p>4. The assessor may choose to judge the tweet immediately, or if it arrived at an inopportune time, to ignore it. Either way, the tweet is added to the queue in a custom app on the assessor's mobile device, which she can access at any time to judge the queue of accumulated tweets.</p><p>Users have the option of logging out of the app completely, at which point they will cease to receive notifications.</p><p>5. As the assessor examines tweets and provides judgments, the results are relayed back to the RTS evaluation broker and recorded.</p><p>Our setup largely follows the interleaved evaluation methodology for prospective notifications proposed by Qian et al. <ref type="bibr" coords="4,67.41,483.55,9.22,7.86" target="#b8">[8]</ref>. For each tweet, the user makes one of three judgments:</p><p>• relevant, if the tweet contains relevant and novel information;</p><p>• redundant, if the tweet contains relevant information, but is substantively similar to another tweet that the assessor had already seen;</p><p>• not relevant, if the tweet does not contain relevant information.</p><p>A screenshot of the mobile assessment app is shown in Figure <ref type="figure" coords="4,70.50,617.04,3.58,7.86" target="#fig_1">2</ref>. The icons below each tweet represent the relevant, not relevant, and redundant judgments, respectively. The entire evaluation is framed as a user study (with appropriate ethics review and approval). A few weeks prior to the beginning of the evaluation period, we recruited assessors from the undergraduate and graduate student population at the University of Waterloo, via posts on various email lists as well as paper flyers on bulletin boards. The assessors were compensated $5 CAD to install the mobile assessment app and $1 CAD per 20 judgments.</p><p>As part of the assessor training process, they subscribed to receive notifications for profiles they were interested in, selecting from the complete list given to all participants via an online web interface. To encourage diversity, we did not allow more than three assessors to select the same profile (on a first come, first served basis).</p><p>The RTS evaluation broker followed the temporal interleaving strategy proposed by Qian et al. <ref type="bibr" coords="4,487.52,130.86,9.22,7.86" target="#b8">[8]</ref>, which meant that tweets were pushed to the assessors as soon as the broker received the submitted tweets. Although Qian et al. only discussed interleaving the output of two systems, it is straightforward to extend the strategy to multiple systems. The broker made sure that each tweet was only pushed once (per profile), in the case where the same tweet is submitted by multiple systems at different times. Although one can imagine a variety of different "routing" algorithms for pushing tweets to different assessors that have subscribed to a topic, this year we implemented the simplest possible algorithm where the tweet was pushed to all assessors (that had subscribed to the profile). This meant that the broker might receive more than one judgment per tweet.</p><p>Another implication of this interleaved evaluation setup is that an assessor will likely encounter tweets from different systems, which makes proper interpretation of redundant judgments more complex. A tweet might only be redundant because the same information was contained in a tweet pushed earlier by another system (and thus it is not the "fault" of the particular system that pushed the tweet). That is, the interleaving of outputs from different systems was directly responsible for introducing the redundancy. Although Qian et al. <ref type="bibr" coords="4,391.58,371.46,9.73,7.86" target="#b8">[8]</ref> proposed a heuristic for more accurate credit assignment to cope with interleaving, in this evaluation we simply counted the absolute number of judgments of each type. From these counts, we computed "strict" precision, defined as: relevant relevant + redundant + not relevant <ref type="bibr" coords="4,544.14,435.95,11.78,7.86" target="#b1">(1)</ref> as well "lenient" precision, defined as:</p><formula xml:id="formula_0" coords="4,364.52,478.76,191.39,19.74">relevant + redundant relevant + redundant + not relevant<label>(2)</label></formula><p>Precision seemed like an appropriate metric given the cost of push notifications in terms of interrupting the user. Note that these precision computations represent a micro-average (and not an average across per-topic scores). This choice was made because of the sparisty of judgments, which would magnify the effects of interest profiles with few judgments. Finally, we made the (arbitrary) decision of using "strict" precision as the primary metric for assessing scenario A runs using mobile assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Batch Judgments and Metrics</head><p>In addition to the online evaluation by mobile assessors, the track also employed a standard post-hoc batch evaluation methodology that has been refined and validated over many iterations in previous TREC evaluations. For scenario A, the dual evaluation approach helps us validate the reliability of our online mobile assessment methodology. We adopted the Tweet Timeline Generation (TTG) evaluation methodology that was originally developed for the TREC 2014 Microblog Track <ref type="bibr" coords="4,435.21,711.19,9.73,7.86" target="#b5">[5]</ref> and also used in the TREC 2015 Microblog Track <ref type="bibr" coords="5,146.88,57.64,9.22,7.86">[6]</ref>. The methodology has been externally validated <ref type="bibr" coords="5,126.68,68.10,13.52,7.86" target="#b15">[15]</ref>, and similar methodologies have been deployed in evaluations dating back at least a decade; thus, we can consider this approach mature and reliable. The assessment workflow proceeded in two major stages: relevance assessment and semantic clustering. Both were accomplished by NIST assessors.</p><p>Relevance assessments were performed using pooling with a single pool across both scenario A and scenario B runs. The pools were constructed from all submitted runs, taking all tweets from Scenario A runs and up to 90 tweets (per profile) from Scenario B runs. For scenario B runs, tweets were added to the judgment pool in a round-robin fashion across days. That is, the top-ranked tweet from each day was first added to the pool, then the second-ranked tweet from each day, and so on. If the process exhausted tweets from a particular day before the 90 tweet limit had been reached, tweets were selected from the remaining days until the limit.</p><p>After pool formation, the next decision was the selection of interest profiles to manually assess. In this case, the selections of the mobile assessors provided an obvious guide. Profiles to assess were selected by first taking those interest profiles that had at least 50 distinct tweets judged by the mobile assessors (there were 67 of these), and then eliminating profiles whose pools were enormous or those about events from 2015. NIST assessors ended up judging 56 profiles. The mean size of the pools was 1206 tweets, with minimum 917 and maximum 1651.</p><p>These pools were then examined by NIST assessors. To facilitate consistent judgments, tweets were first clustered by lexical similarity. Each tweet was independently assessed on a three-way scale of "not relevant", "relevant", and "highly relevant". Non-English tweets were marked as not relevant by fiat. If a tweet contained a mixture of English and non-English content, discretion was left to the assessor. As with previous TREC Microblog evaluations, assessors examined links embedded in tweets, but did not explore any additional external content beyond those. Retweets did not receive any special treatment and were assessed just like any other tweet.</p><p>All 56 profiles judged by NIST assessors have at least one relevant judgment from the mobile assessors. However, based on the NIST assessors, one interest profile has no relevant tweets, three other interest profiles have exactly one relevant tweet, and a total of 14 interest profiles have fewer than 10 relevant tweets. At the other end of the scale, three interest profiles have more than 200 relevant tweets, the maximum being RTS10 (Hiroshima bomb reactions), with 364 relevant tweets.</p><p>After the relevance assessment process, the NIST assessors proceeded to perform semantic clustering on the relevant tweets using the tweet timeline generation (TTG) protocol, originally developed for the TREC 2014 Microblog Track <ref type="bibr" coords="5,283.17,601.60,9.73,7.86" target="#b5">[5,</ref><ref type="bibr" coords="5,53.80,612.06,10.75,7.86" target="#b15">15]</ref>. Unlike in previous years, where the clustering was performed outside NIST, this year the same assessor performed both the relevance judgments and the clustering.</p><p>The TTG protocol was designed to reward novelty (or equivalently, to penalize redundancy) in system output. In both scenario A and scenario B, we assume that users would not want to see multiple tweets that "say the same thing", and thus the evaluation methodology should reward systems that eliminate redundant output. Following the TREC 2014 Microblog Track, we operationalized redundancy as follows:</p><p>for every pair of tweets, if the chronologically later tweet contains substantive information that is not present in the earlier tweet, the later tweet is considered novel; otherwise, the later tweet is redundant with respect to the earlier one. In our definition, redundancy and novelty are antonyms, so we use them interchangeably but in opposite contexts.</p><p>Due to the temporal constraint, redundancy is not symmetric. If tweet A precedes tweet B and tweet B contains substantively similar information found in tweet A, then B is redundant with respect to A, but not the other way around. We also assume transitivity. Suppose A precedes B and B precedes C: if B is redundant with respect to A and C is redundant with respect to B, then by definition C is redundant with respect to A.</p><p>In the instructions given to the NIST assessors, they were not provided a particular target regarding the number of clusters to form. Instead, they were asked to use their best judgment, considering both the interest profile and the actual tweets.</p><p>For the semantic clustering, the assessors were shown all the relevant tweets (from the judgment pool) for a single interest profile within a custom assessment interface. The tweets were shown in the left pane in chronological order, while the list of current clusters were shown in a pane on the right side. For each tweet in the left pane, the assessor could either use that tweet as the basis for a new cluster, or add it to one of the existing clusters. In this way, clusters representing important pieces of information (comprised of semantically similar tweets) are constructed incrementally. To aid the clustering process, assessors could enter a short textual description for each cluster and then sort the tweets by similarity to a selected cluster, as a way to speed up the process of finding additional relevant tweets for that cluster. Users could also retroactively move a tweet from a cluster back into the left pane, such that it could then be assigned to a different cluster. The output of the assessment process (for each interest profile) is a list of clusters, where tweets in each cluster represent a particular "facet" of the overall information need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Scenario A Metrics</head><p>For Scenario A, we computed a number of metrics from the relevance judgments and clusters provided by the NIST assessors, detailed below. As previously discussed, push notifications should be relevant (on topic), novel (users should not be pushed multiple notifications that say the same thing), and timely (provide updates as soon after the actual event occurrence as possible). Unlike the TREC 2015 Microblog Track as well as previous Temporal Summarization Tracks (cf. <ref type="bibr" coords="5,333.16,575.20,9.52,7.86" target="#b2">[2]</ref>), which devised single-point metrics that attempted to incorporate both relevance, novelty, and timeliness, we decided this year to separately compute metrics of output quality (relevance and novelty) and latency (timeliness).</p><p>We envision that systems might trade off latency with output quality: For example, a system might wait to accumulate evidence before pushing tweets, thus producing high-quality output at the cost of high latency. Alternatively, a low-latency system might aggressively push results that it might "regret" later. Computing metrics of output quality separately from latency allows us to understand the potential tradeoffs. Additionally, we believe this approach is appropriate because we have no empirical evidence as to what the "human response curve" to latency looks like-that is, how much should we discount a quality metric based on tardiness? Attempting to formulate a single-point metric collapses meaningful distinctions in what users may be looking for in systems. Expected Gain (EG) for an interest profile on a particular day is defined as follows:</p><formula xml:id="formula_1" coords="6,152.13,135.85,136.85,19.74">1 N G(t) (<label>3</label></formula><formula xml:id="formula_2" coords="6,288.98,141.64,3.93,7.86">)</formula><p>where N is the number of tweets returned and G(t) is the gain of each tweet:</p><p>• Not relevant tweets receive a gain of 0.</p><p>• Relevant tweets receive a gain of 0.5.</p><p>• Highly-relevant tweets receive a gain of 1.0.</p><p>Once a tweet from a cluster is retrieved, all other tweets from the same cluster automatically become not relevant. This penalizes systems for returning redundant information.</p><p>Normalized Cumulative Gain (nCG) for an interest profile on a particular day is defined as follows:</p><formula xml:id="formula_3" coords="6,152.61,304.80,136.37,19.74">1 Z G(t) (<label>4</label></formula><formula xml:id="formula_4" coords="6,288.98,310.60,3.93,7.86">)</formula><p>where Z is the maximum possible gain (given the ten tweet per day limit). The gain of each individual tweet is computed as above. Note that gain is not discounted (as in nDCG) because the notion of document ranks is not meaningful in this context. The score for a run is the mean of scores for each day over all the profiles. Since each profile contains the same number of days, there is no distinction between micro-vs. macroaverages. An interesting question is how scores should be computed for days in which there are no relevant tweets: for rhetorical convenience, we call days in which there are no relevant tweets for a particular interest profile (in the pool) "silent days", in contrast to "eventful days" (where there are relevant tweets). In the EG-1 and nCG-1 variants of the metrics, on a "silent day", the system receives a score of one (i.e., a perfect score) if it does not push any tweets, or zero otherwise. In the EG-0 and nCG-0 variants of the metrics, for a silent day, all systems receive a gain of zero no matter what they do. For more details about this distinction, see Tan et al. <ref type="bibr" coords="6,96.04,534.74,13.52,7.86" target="#b14">[14]</ref>. Therefore, under EG-1 and nCG-1, systems are rewarded for recognizing that there are no relevant tweets for an interest profile on a particular day and remaining silent (i.e., does not push any tweets). The EG-0 and nCG-0 variants of the metrics do not reward recognizing silent days: that is, it never hurts to push tweets. Gain Minus Pain (GMP) is defined as follows:</p><formula xml:id="formula_5" coords="6,129.02,637.56,163.88,7.86">α • G -(1 -α) • P<label>(5)</label></formula><p>The G (gain) is computed in the same manner as above.</p><p>Pain P is the number of non-relevant tweets that the system pushed, and α controls the balance between the two. We investigated three α settings: 0.33, 0.50, and 0.66. Note that this metric is the same as the linear utility metric used in the TREC Filtering Tracks <ref type="bibr" coords="6,166.13,711.19,9.73,7.86" target="#b4">[4,</ref><ref type="bibr" coords="6,178.88,711.19,6.49,7.86" target="#b9">9]</ref>, although our formulation is slightly different. Thus, our metric is not novel, which we see as an advantage since it builds on previous work.</p><p>In summary, for scenario A, we report EG-1, EG-0, nCG-1, nCG-0, and GMP (with α = {0.33, 0.50, 0.66}). EG-1 was considered the primary metric.</p><p>Latency. In addition to the quality metrics above, we report, only for tweets that contribute to gain, the mean and median difference between the time the tweet was pushed and the first tweet in the semantic cluster that the tweet belongs to (based on the NIST assessors). For example, suppose tweets A, B, and C are in the same semantic cluster, and were posted 09:00, 10:00, and 11:30, respectively. No matter which of the three tweets is pushed, the latency is computed with respect to the creation time of A (09:00). Therefore, pushing tweet C at 11:30 and pushing tweet A at 11:30 gives the same latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Scenario B Metrics</head><p>Scenario B runs were evaluated in terms of nDCG as follows: for each interest profile, the list of tweets returned per day is treated as a ranked list and from this nDCG@10 is computed. Note that in this scenario, the evaluation metric does include gain discounting because the email digests can be interpreted as ranked lists of tweets. Gain is computed in the same way as in scenario A with respect to the semantic clusters. Systems only receive credit for the first relevant tweet they report from a cluster.</p><p>The score of an interest profile is the mean of the nDCG scores across all days in the evaluation period, and the score of the run is the mean of scores for each profile. Once again, the micro-vs. macro-average distinction is not applicable here. As with scenario A, we computed two variants of the metric: with nDCG-1, on a "silent day", the system receives a score of one (i.e., a perfect score) if it does not push any tweets, or zero otherwise. In nDCG-0, for a silent day, all systems receive a gain of zero no matter what they do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS</head><p>To provide a track-wide baseline and also a point of comparison for this year's participants, we deployed the "YoGosling" system <ref type="bibr" coords="6,348.22,491.33,13.52,7.86" target="#b13">[13]</ref>, which is a simplified reimplementation of the best-performing automatic system from the TREC 2015 Microblog Track <ref type="bibr" coords="6,375.67,512.25,13.52,7.86" target="#b12">[12]</ref>. The system was originally designed for scenario A, but we adapted it for scenario B by simply running the system on all tweets collected at the end of the day, keeping the same exact scoring model and scoring thresholds as implemented for scenario A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scenario A</head><p>For Scenario A, we received 41 runs from 18 groups. These runs pushed a total of 161,726 tweets, or 95,113 unique tweets after de-duplicating within interest profiles (but not de-duplicating across profiles).</p><p>For the online evaluation of scenario A systems, we recruited a total of 18 assessors, 13 of whom ultimately provided judgments. Of these, 11 were either graduates or undergraduate students at the University of Waterloo. In total, we received 12,115 judgments over the assessment period, with a minimum of 28 and a maximum of 3,791 by an individual assessor. We found that 10,605 tweets received a single judgment, 743 tweets received two judgments, and 8 tweets received three judgments.  <ref type="table" coords="7,79.24,345.06,3.58,7.86" target="#tab_0">1</ref>. The columns list: assessor id, the number of judgments provided, the number of profiles subscribed to. The fourth column shows the sum of all push notifications for the profiles that each assessor subscribed to: this captures the maximum number of push notifications that the assessor could have received during the evaluation period. Note that we do not have the actual number of notifications each assessor received because the assessor could have logged out during some periods of time or otherwise adjusted the local device settings (e.g., to disable notifications). The final column shows the response rate, computed as the fraction between the second and fourth columns, which is a lowerbound estimate. From this table, we see that some assessors are quite diligent in providing judgments, while others are more sporadic.</p><p>It was originally our intention to build mobile assessment apps for both Android and iOS, but due to technical issues with the app development framework we were using, we were unable to deploy a stable iOS app in time. As a result, all assessors used the Android app. Some assessors encountered display issues with tweets during the evaluation period, due to the wide range of devices owned by the assessors. Since this was not anticipated during testing, we did our best to support these assessors and to provide workarounds on the fly. While the overall assessment experience could have been more refined, the entire setup worked as expected.</p><p>After the evaluation, while compiling results, we discovered that from the RTS evaluation broker's perspective, some tweets were pushed before they were actually posted on Twitter. Since it is unlikely that participants had created time traveling devices, we attributed the issue to clock skew on the broker. Note that since the broker was an EC2 instance in the cloud that was shut down soon after the evaluation ended, there was no way to debug this issue to obtain confirmation. The only reasonable solution we could come up with was to add a temporal offset to all pushed tweets.</p><p>We set this offset to 139 seconds, the maximum gap between a system push time and the posted time of the tweet (on Twitter itself).</p><p>Results of the evaluation by the mobile assessors are shown in Table <ref type="table" coords="7,355.20,99.48,3.58,7.86">2</ref>. For each run, the columns show the number of tweets that were judged relevant (R), redundant (D), and not relevant (N); the number of unjudged tweets (U); the length of each run (L), defined as the total number of tweets pushed by the system for the interest profiles that have at least one judgment. The next column shows the fraction of pushed tweets that were judged (C), defined as (R+D +N )/L. The table also reports the mean ( t) and median ( t) latency of pushed tweets in seconds, measured with respect to the time the original tweet was posted. Next, the table shows "strict" and "lenient" precision (as defined in Section 2.4), with 95% binomial confidence intervals. The final column shows the run type: 'A' denotes automatic and 'P' manual preparation.</p><p>The rows in Table <ref type="table" coords="7,402.91,245.93,4.61,7.86">2</ref> are sorted by "strict" precision, but sorting by "lenient" precision doesn't greatly affect the rankings of the systems. The YoGosling baseline (Waterloo-Clarke, WaterlooBaseline-50) is noted in the results table. The placement of the YoGosling baseline suggests that the community has made quite a bit of progress on this task, since the best performing run from last year now falls in the middle of the pack.</p><p>Results of the evaluation by NIST assessors are shown in Table <ref type="table" coords="7,353.77,340.08,3.58,7.86" target="#tab_4">3</ref>. The columns list the various metrics discussed in Section 2.5 and also the mean and median latency in seconds. Note that latency is computed with respect to the first tweet in each cluster, and thus a system may have a high latency even if it pushes a tweet immediately. The second to last column shows the length of each run, defined as the number of notifications pushed for the interest profiles that were assessed. The final column shows the run type: 'A' denotes automatic and 'P' manual preparation. The rows are sorted by EG-1, the primary metric. The YoGosling baseline is also marked in the results table; we see that it also places in the middle of the pack.</p><p>For reference, an empty run (i.e., doing nothing) would receive a score of 0.2339 for EG-1 and nCG-1 (with all other scores being zero). This is also shown in Table <ref type="table" coords="7,511.72,486.53,3.58,7.86" target="#tab_4">3</ref>. As with the TREC 2015 Microblog Track, the baseline of doing nothing is surprisingly competitive given the current battery of metrics. The same observation has been noted in previous TREC Filtering Tracks. In a precision-focused task such as this, it is very important for systems to "keep quiet", which translates into the task of recognizing when there are no relevant documents.</p><p>Figure <ref type="figure" coords="7,355.13,570.21,4.61,7.86">3</ref> shows a heatmap of the distribution of relevant and highly-relevant tweets by the NIST assessors: each column corresponds to an interest profile and each row corresponds to a day in the evaluation period. Figure <ref type="figure" coords="7,518.09,601.60,4.61,7.86">4</ref> is organized in the same manner, but we only show the first tweet in each cluster.</p><p>Figure <ref type="figure" coords="7,353.99,632.98,4.61,7.86">5</ref> shows scatterplots for "strict" precision (left) and "lenient" precision (right) vs. median latency. Each solid square represents a run. We do not see, overall, a tradeoff between output quality and latency. That is, systems with higher latencies, which have more time to accumulate evidence on relevance and novelty, do not tend to perform better in terms of the various quality metrics.</p><p>The same scatterplots for batch evaluation metrics are shown in Figure <ref type="figure" coords="8,124.05,347.25,4.61,7.86">6</ref> (EG-1, EG-0, nCG-1, and nCG-0) and Figure <ref type="figure" coords="8,83.44,357.71,4.61,7.86">7</ref> (GMP with α = {0.33, 0.50, 0.66}). The scenario A runs are shown as solid squares. Once again, we do not observe any strong relationship between system output quality (as measured by the various metrics) and latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scenario B</head><p>For scenario B, we received a total of 40 runs from 15 groups. Evaluation results based on NIST assessors are shown in Table <ref type="table" coords="8,79.64,442.33,3.58,7.86" target="#tab_5">4</ref>. Runs are sorted by nDCG-1, with the YoGosling baseline (YoGoslingBSL) marked. For reference, the empty run would have received an nDCG-1 score of 0.2339, also shown in the results table.</p><p>The separation of quality metrics from latency allows us to unify the evaluation of scenario A and scenario B runswe can simply convert scenario B runs into scenario A runs by pretending that all tweets were emitted at 23:59:59, and then running the evaluation scripts for scenario A exactly as before. The results of this conversion are shown in Figure <ref type="figure" coords="8,70.58,546.94,4.61,7.86">6</ref> and Figure <ref type="figure" coords="8,127.91,546.94,3.58,7.86">7</ref>, where the scenario B runs are shown as empty squares. We would have expected that scenario B runs, on the whole, outperform scenario A runs (on quality metrics), since they had the advantage of accumulating evidence throughout the entire day. This, however, does not appear to be the case. Nevertheless, we believe this way of visualizing the results frames mobile push notifications and email digests as variants of the same underlying task, just differing in the amount of latency that is tolerated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>The TREC 2016 Real-Time Summarization Track had several innovative elements. Building on previous Microblog evaluations, we emphasized working systems that operate on the live Twitter stream, in an attempt to narrow the gap between research and practice. We continued to refine eval-uation metrics as we better understand the nuances of push notifications. Most notably, this track represents, to our knowledge, the first deployment of an interleaved evaluation framework for prospective information needs, providing an opportunity to examine user behavior in a realistic setting. Our efforts will continue with another instance of the track in TREC 2017. Table <ref type="table" coords="10,82.30,521.37,4.13,7.89">2</ref>: Evaluation of scenario A runs by the mobile assessors. The first two columns show the participating team and run. The next columns show the number of tweets that were judged relevant (R), redundant (D), and not relevant (N); the number of unjudged tweets (U); the length of each run (L), defined as the total number of tweets pushed by the system for the interest profiles that have at least one judgment. The next columns show the fraction of pushed tweets that were judged (C), defined as (R + D + N )/L; the mean ( t) and median ( t) latency of pushed tweets in seconds, measured with respect to the time the original tweet was posted; "strict" and "lenient" precision, with 95% binomial confidence intervals. The final column shows the run type: 'A' denotes automatic and 'P' manual preparation. Rows are sorted by "strict" precision, and the YoGosling baseline (WaterlooClarke, WaterlooBaseline-50) is noted.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,316.81,166.06,239.10,7.89;3,316.81,176.52,239.10,7.89;3,316.81,186.98,214.36,7.89"><head>1Figure 1 :</head><label>1</label><figDesc>Figure 1: Evaluation setup for scenario A showing the use of mobile assessors who judge tweets in real time, mediated by the RTS evaluation broker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,53.99,344.22,238.71,7.89;4,90.50,53.80,165.69,276.15"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Screenshot of the mobile assessment app.</figDesc><graphic coords="4,90.50,53.80,165.69,276.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,53.80,654.51,502.12,7.89;13,53.80,664.97,502.12,7.89;13,53.80,675.43,360.10,7.89"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Scatterplots for various metrics vs. median latency. Each point represents a run: solid squares denote scenario A runs; empty squares denote scenario B runs treated as if they were scenario A runs. The solid horizontal line denotes the score of the empty run for EG-1 and nCG-1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,453.78,711.19,102.13,7.86"><head>Table 1 :</head><label>1</label><figDesc>Overall, 122 interest pro-Assessor statistics. For each assessor, columns show the number of judgments provided, the number of interest profiles subscribed to, the maximum number of push notifications received, and the response rate.</figDesc><table coords="7,53.80,55.39,239.10,297.53"><row><cell cols="5">Assessor Judgments Profiles Messages Response</cell></row><row><cell>1</cell><cell>53</cell><cell>4</cell><cell>1619</cell><cell>3.27%</cell></row><row><cell>2</cell><cell>3305</cell><cell>10</cell><cell>7141</cell><cell>46.28%</cell></row><row><cell>3</cell><cell>136</cell><cell>10</cell><cell>5860</cell><cell>2.32%</cell></row><row><cell>4</cell><cell>327</cell><cell>8</cell><cell>3795</cell><cell>8.62%</cell></row><row><cell>5</cell><cell>949</cell><cell>12</cell><cell>6330</cell><cell>14.99%</cell></row><row><cell>6</cell><cell>28</cell><cell>12</cell><cell>7211</cell><cell>0.39%</cell></row><row><cell>7</cell><cell>281</cell><cell>10</cell><cell>4162</cell><cell>6.75%</cell></row><row><cell>8</cell><cell>1908</cell><cell>15</cell><cell>7754</cell><cell>24.61%</cell></row><row><cell>9</cell><cell>3791</cell><cell>33</cell><cell>16654</cell><cell>22.76%</cell></row><row><cell>10</cell><cell>680</cell><cell>16</cell><cell>7257</cell><cell>9.37%</cell></row><row><cell>11</cell><cell>107</cell><cell>43</cell><cell>22676</cell><cell>0.47%</cell></row><row><cell>12</cell><cell>324</cell><cell>2</cell><cell>938</cell><cell>34.54%</cell></row><row><cell>13</cell><cell>226</cell><cell>12</cell><cell>7058</cell><cell>3.20%</cell></row><row><cell cols="5">files received at least one judgment; 93 received at least 10</cell></row><row><cell cols="5">judgments; 67 received at least 50 judgments; 44 received at</cell></row><row><cell cols="2">least 100 judgments.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">The distribution of judgments by assessors is shown in</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,51.72,54.76,504.20,270.72"><head></head><label></label><figDesc>Heatmap of the distribution of the first tweet in each semantic cluster: interest profiles in columns, days of the evaluation in rows.</figDesc><table coords="8,51.72,54.76,504.19,260.26"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Topics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MB226</cell><cell cols="2">MB229</cell><cell cols="2">MB230</cell><cell cols="2">MB239</cell><cell cols="2">MB254</cell><cell cols="2">MB256</cell><cell cols="2">MB258</cell><cell cols="2">MB265</cell><cell cols="2">MB267</cell><cell cols="2">MB276</cell><cell cols="2">MB286</cell><cell cols="2">MB319</cell><cell cols="2">MB320</cell><cell>MB332</cell><cell>MB351</cell><cell>MB358</cell><cell>MB361</cell><cell>MB362</cell><cell>MB363</cell><cell>MB365</cell><cell>MB371</cell><cell>MB377</cell><cell>MB381</cell><cell>MB382</cell><cell>MB391</cell><cell>MB392</cell><cell>MB409</cell><cell>MB410</cell><cell>MB414</cell><cell>MB420</cell><cell>MB425</cell><cell>MB431</cell><cell>MB436</cell><cell>MB438</cell><cell>MB440</cell><cell>RTS1</cell><cell>RTS2</cell><cell>RTS4</cell><cell>RTS5</cell><cell>RTS6</cell><cell>RTS10</cell><cell>RTS13</cell><cell>RTS14</cell><cell>RTS19</cell><cell>RTS21</cell><cell>RTS24</cell><cell>RTS25</cell><cell>RTS27</cell><cell>RTS28</cell><cell>RTS31</cell><cell>RTS32</cell><cell>RTS35</cell><cell>RTS36</cell><cell>RTS37</cell><cell>RTS43</cell></row><row><cell></cell><cell></cell><cell>2016/08/02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2016/08/03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2016/08/04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2016/08/05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Days</cell><cell>2016/08/06 2016/08/07 2016/08/08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2016/08/09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2016/08/10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2016/08/11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="56">Figure 3: Heatmap of the distribution of all relevant and highly-relevant tweets: interest profiles in columns,</cell></row><row><cell cols="27">days of the evaluation in rows.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Topics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MB226</cell><cell cols="2">MB229</cell><cell cols="2">MB230</cell><cell cols="2">MB239</cell><cell cols="2">MB254</cell><cell cols="2">MB256</cell><cell cols="2">MB258</cell><cell cols="2">MB265</cell><cell cols="2">MB267</cell><cell cols="2">MB276</cell><cell cols="2">MB286</cell><cell cols="2">MB319</cell><cell cols="2">MB320</cell><cell cols="2">MB332</cell><cell>MB351</cell><cell>MB358</cell><cell>MB361</cell><cell>MB362</cell><cell>MB363</cell><cell>MB365</cell><cell>MB371</cell><cell>MB377</cell><cell>MB381</cell><cell>MB382</cell><cell>MB391</cell><cell>MB392</cell><cell>MB409</cell><cell>MB410</cell><cell>MB414</cell><cell>MB420</cell><cell>MB425</cell><cell>MB431</cell><cell>MB436</cell><cell>MB438</cell><cell>MB440</cell><cell>RTS1</cell><cell>RTS2</cell><cell>RTS4</cell><cell>RTS5</cell><cell>RTS6</cell><cell>RTS10</cell><cell>RTS13</cell><cell>RTS14</cell><cell>RTS19</cell><cell>RTS21</cell><cell>RTS24</cell><cell>RTS25</cell><cell>RTS27</cell><cell>RTS28</cell><cell>RTS31</cell><cell>RTS32</cell><cell>RTS35</cell><cell>RTS36</cell><cell>RTS37</cell><cell>RTS43</cell></row><row><cell></cell><cell cols="2">2016/08/02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">2016/08/03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">2016/08/04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">2016/08/05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Days</cell><cell cols="2">2016/08/06 2016/08/07</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">2016/08/08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">2016/08/09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">2016/08/10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">2016/08/11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,53.80,558.68,502.12,60.19"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of scenario A runs by NIST assessors. The columns marked "mean" and "median" show the mean and median latency with respect to the first tweet in each cluster. The second to last column shows the length of each run, defined as the number of notifications pushed for the interest profiles that were assessed. The final column shows the run type: 'A' denotes automatic and 'P' manual preparation. Rows are sorted by EG-1. The YoGosling baseline (WaterlooClarke, WaterlooBaseline-50) and the empty run are noted.</figDesc><table coords="12,134.19,151.14,341.33,440.54"><row><cell>team</cell><cell>run</cell><cell cols="2">nDCG-1 nDCG-0 type</cell></row><row><cell>COMP2016</cell><cell>PolyURunB3</cell><cell>0.2898</cell><cell>0.0684 manual preparation</cell></row><row><cell>NUDTSNA</cell><cell>nudt sna</cell><cell>0.2708</cell><cell>0.0529 automatic</cell></row><row><cell>QU</cell><cell>QUJM16</cell><cell>0.2621</cell><cell>0.0300 automatic</cell></row><row><cell>QU</cell><cell>QUJMDR24</cell><cell>0.2558</cell><cell>0.0237 automatic</cell></row><row><cell>COMP2016</cell><cell>PolyURunB1</cell><cell>0.2536</cell><cell>0.0215 manual preparation</cell></row><row><cell>COMP2016</cell><cell>PolyURunB2</cell><cell>0.2523</cell><cell>0.0184 manual preparation</cell></row><row><cell>IRIT</cell><cell>RunBIch</cell><cell>0.2481</cell><cell>0.0321 automatic</cell></row><row><cell>WaterlooLin</cell><cell>YoGoslingBSL</cell><cell>0.2352</cell><cell>0.0299 automatic</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunB3</cell><cell>0.2348</cell><cell>0.0151 automatic</cell></row><row><cell>QU</cell><cell>QUDR8</cell><cell>0.2344</cell><cell>0.0094 automatic</cell></row><row><cell>Empty run</cell><cell></cell><cell>0.2339</cell><cell>0.0000</cell></row><row><cell>prna</cell><cell>PRNATaskB1</cell><cell>0.2334</cell><cell>0.0352 automatic</cell></row><row><cell>NUDTSNA</cell><cell>nudt biront</cell><cell>0.2306</cell><cell>0.0681 automatic</cell></row><row><cell>WaterlooLin</cell><cell>YoGoslingLMGTFY</cell><cell>0.2273</cell><cell>0.0327 automatic</cell></row><row><cell>prna</cell><cell>PRNATaskB2</cell><cell>0.2244</cell><cell>0.0226 automatic</cell></row><row><cell>ISIKol</cell><cell>isikol tag</cell><cell>0.2213</cell><cell>0.0196 automatic</cell></row><row><cell>IRIT</cell><cell>AmILPWSEBM</cell><cell>0.2208</cell><cell>0.1262 automatic</cell></row><row><cell>ISIKol</cell><cell>isikol ti</cell><cell>0.2189</cell><cell>0.0171 automatic</cell></row><row><cell>udel</cell><cell>udelRunBM25B</cell><cell>0.2151</cell><cell>0.0008 manual preparation</cell></row><row><cell>udel</cell><cell>udelRunTFIDFQB</cell><cell>0.1991</cell><cell>0.0330 manual preparation</cell></row><row><cell>prna</cell><cell>PRNATaskB3</cell><cell>0.1987</cell><cell>0.0665 automatic</cell></row><row><cell cols="2">IRLAB DA-IICT IRLAB2</cell><cell>0.1972</cell><cell>0.0169 manual preparation</cell></row><row><cell>udel</cell><cell>udelRunTFIDFB</cell><cell>0.1970</cell><cell>0.0363 automatic</cell></row><row><cell>CCNU2016NLP</cell><cell>CCNUNLPrun1</cell><cell>0.1732</cell><cell>0.0018 manual preparation</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunB2</cell><cell>0.1569</cell><cell>0.1569 automatic</cell></row><row><cell>CCNU2016NLP</cell><cell>CCNUNLPrun2</cell><cell>0.1554</cell><cell>0.0000 manual preparation</cell></row><row><cell cols="2">IRLAB DA-IICT IRLAB</cell><cell>0.1532</cell><cell>0.0711 manual preparation</cell></row><row><cell>udel fang</cell><cell>UDInfo TlmN</cell><cell>0.1451</cell><cell>0.1416 automatic</cell></row><row><cell>udel fang</cell><cell>UDInfo TlmNlm</cell><cell>0.1445</cell><cell>0.1410 automatic</cell></row><row><cell>PKUICST</cell><cell>PKUICSTRunB1</cell><cell>0.1423</cell><cell>0.1423 automatic</cell></row><row><cell>udel fang</cell><cell>UDInfo TN</cell><cell>0.1315</cell><cell>0.1279 automatic</cell></row><row><cell>CLIP</cell><cell>CLIP-B-MAX</cell><cell>0.1244</cell><cell>0.0173 automatic</cell></row><row><cell>BJUT</cell><cell>bjutgbdt</cell><cell>0.1200</cell><cell>0.0914 automatic</cell></row><row><cell>HLJIT</cell><cell>HLJIT LM</cell><cell>0.1155</cell><cell>0.1155 automatic</cell></row><row><cell>HLJIT</cell><cell>HLJIT LM TIME</cell><cell>0.1145</cell><cell>0.1145 automatic</cell></row><row><cell>IRIT</cell><cell>IritIrisSDB</cell><cell>0.1062</cell><cell>0.0651 automatic</cell></row><row><cell>BJUT</cell><cell>bjutdt</cell><cell>0.0978</cell><cell>0.0978 automatic</cell></row><row><cell>CLIP</cell><cell>CLIP-B-2015</cell><cell>0.0718</cell><cell>0.0718 automatic</cell></row><row><cell>HLJIT</cell><cell>HLJIT LM URL</cell><cell>0.0638</cell><cell>0.0638 automatic</cell></row><row><cell>BJUT</cell><cell>bjutrf</cell><cell>0.0582</cell><cell>0.0582 automatic</cell></row><row><cell>CLIP</cell><cell>CLIP-B-MIN</cell><cell>0.0312</cell><cell>0.0312 automatic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,169.73,613.67,270.26,7.89"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of scenario B runs by NIST assessors. Scatterplots for "strict" precision (left) and "lenient" precision (right) vs. median latency. Each point represents a run.</figDesc><table coords="13,53.80,94.00,495.12,163.57"><row><cell></cell><cell>����</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>����</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>����������������������</cell><cell>���� ���� ���� ���� ����</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>������������������������������������</cell><cell>���� ���� ���� ���� ����</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>��</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>��</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>��</cell><cell>���</cell><cell>���</cell><cell>���</cell><cell>���</cell><cell>����</cell><cell>����</cell><cell>����</cell><cell>����</cell><cell>����</cell><cell>��</cell><cell>���</cell><cell>���</cell><cell>���</cell><cell>���</cell><cell>����</cell><cell>����</cell><cell>����</cell><cell>����</cell><cell>����</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">����������������������</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">����������������������</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC)</rs> of <rs type="funder">Canada</rs>. Additional support came from the <rs type="funder">U.S. National Science Foundation</rs> under <rs type="grantNumber">IIS-1218043</rs> and <rs type="grantNumber">CNS-1405688</rs>. Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gZxDgEU">
					<idno type="grant-number">IIS-1218043</idno>
				</org>
				<org type="funding" xml:id="_yCGT3k9">
					<idno type="grant-number">CNS-1405688</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,321.30,528.82,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,540.82,213.03,7.86;8,335.63,551.29,179.62,7.86;8,335.63,561.75,187.38,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,373.39,540.82,175.27,7.86;8,335.63,551.29,101.44,7.86">Topic Detection and Tracking: Event-Based Information Organization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,573.20,220.28,7.86;8,335.63,583.66,187.52,7.86;8,335.63,594.12,213.87,7.86;8,335.63,604.59,202.60,7.86;8,335.63,615.05,152.29,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,433.80,583.66,89.35,7.86;8,335.63,594.12,123.14,7.86">TREC 2015 Temporal Summarization Track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ekstrand-Abueg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,477.51,594.12,71.99,7.86;8,335.63,604.59,202.60,7.86;8,335.63,615.05,20.95,7.86">Proceedings of the Twenty-Fourth Text REtrieval Conference (TREC 2015)</title>
		<meeting>the Twenty-Fourth Text REtrieval Conference (TREC 2015)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,626.50,202.34,7.86;8,335.63,636.96,186.23,7.86;8,335.63,647.42,210.88,7.86;8,335.63,657.89,213.15,7.86;8,335.63,668.35,203.09,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.07454</idno>
		<title level="m" coord="8,452.33,657.89,96.46,7.86;8,335.63,668.35,87.32,7.86">Evaluation-as-a-Service: Overview and outlook</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,679.80,184.40,7.86;8,335.63,690.26,211.76,7.86;8,335.63,700.73,210.33,7.86;8,335.63,711.19,20.99,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,389.31,679.80,114.98,7.86">The TREC-4 Filtering Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,335.63,690.26,211.76,7.86;8,335.63,700.73,39.30,7.86">Proceedings of the Fourth Text REtrieval Conference (TREC-4)</title>
		<meeting>the Fourth Text REtrieval Conference (TREC-4)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.62,57.64,220.29,7.86;9,72.62,68.10,217.24,7.86;9,72.62,78.56,213.53,7.86;9,72.62,89.02,152.29,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,255.24,57.64,37.66,7.86;9,72.62,68.10,141.73,7.86">Overview of the TREC-2014 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sherman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,233.16,68.10,56.70,7.86;9,72.62,78.56,213.53,7.86;9,72.62,89.02,20.95,7.86">Proceedings of the Twenty-Third Text REtrieval Conference (TREC 2014)</title>
		<meeting>the Twenty-Third Text REtrieval Conference (TREC 2014)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.62,100.48,181.47,7.86;9,72.62,110.94,211.95,7.86;9,72.62,121.40,195.47,7.86;9,72.62,131.86,206.02,7.86;9,72.62,142.32,65.55,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,126.32,110.94,158.24,7.86;9,72.62,121.40,21.14,7.86">Overview of the TREC-2015 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.57,121.40,155.52,7.86;9,72.62,131.86,143.31,7.86">Proceedings of the Twenty-Fourth Text REtrieval Conference (TREC 2015)</title>
		<meeting>the Twenty-Fourth Text REtrieval Conference (TREC 2015)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.62,153.78,202.23,7.86;9,72.62,164.24,220.28,7.86;9,72.62,174.70,191.37,7.86;9,72.62,185.16,201.98,7.86;9,72.62,195.62,119.10,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,164.00,153.78,110.84,7.86;9,72.62,164.24,220.28,7.86">Do multiple listeners to the public Twitter sample stream receive the same tweets?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,84.13,174.70,179.86,7.86;9,72.62,185.16,201.98,7.86;9,72.62,195.62,24.19,7.86">Proceedings of the SIGIR 2015 Workshop on Temporal, Social and Spatially-Aware Information Access</title>
		<meeting>the SIGIR 2015 Workshop on Temporal, Social and Spatially-Aware Information Access<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.62,207.08,181.70,7.86;9,72.62,217.54,190.40,7.86;9,72.62,228.00,197.31,7.86;9,72.62,238.46,205.33,7.86;9,72.62,248.92,209.13,7.86;9,72.62,259.38,211.81,7.86;9,72.62,269.84,67.50,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,210.00,207.08,44.31,7.86;9,72.62,217.54,190.40,7.86;9,72.62,228.00,181.62,7.86">Interleaved evaluation for retrospective summarization and prospective notification on document streams</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,72.62,238.46,205.33,7.86;9,72.62,248.92,209.13,7.86;9,72.62,259.38,144.59,7.86">Proceedings of the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016)</title>
		<meeting>the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.62,281.30,187.86,7.86;9,72.62,291.76,215.20,7.86;9,72.62,302.22,168.22,7.86;9,72.62,312.68,124.08,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,193.67,281.30,66.81,7.86;9,72.62,291.76,87.17,7.86">The TREC 2002 Filtering Track report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,178.30,291.76,109.52,7.86;9,72.62,302.22,164.02,7.86">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.62,324.14,207.29,7.86;9,72.62,334.60,206.54,7.86;9,72.62,345.06,176.94,7.86;9,72.62,355.52,205.89,7.86;9,72.62,365.98,204.22,7.86;9,335.63,57.64,167.96,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,273.00,324.14,6.91,7.86;9,72.62,334.60,206.54,7.86;9,72.62,345.06,33.89,7.86">A platform for streaming push notifications to mobile assessors</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,124.86,345.06,124.70,7.86;9,72.62,355.52,205.89,7.86;9,72.62,365.98,204.22,7.86;9,335.63,57.64,20.95,7.86">Proceedings of the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016)</title>
		<meeting>the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1077" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.63,69.09,206.61,7.86;9,335.63,79.55,198.24,7.86;9,335.63,90.02,206.14,7.86;9,335.63,100.48,203.66,7.86;9,335.63,110.94,71.00,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,478.21,69.09,64.03,7.86;9,335.63,79.55,198.24,7.86;9,335.63,90.02,99.59,7.86">Overview of the Living Labs for Information Retrieval Evaluation (LL4IR) CLEF Lab 2015</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,454.01,90.02,87.76,7.86;9,335.63,100.48,203.66,7.86;9,335.63,110.94,42.24,7.86">Proceedings of the 6th International Conference of the CLEF Association (CLEF&apos;15)</title>
		<meeting>the 6th International Conference of the CLEF Association (CLEF&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.63,122.39,210.19,7.86;9,335.63,132.85,182.07,7.86;9,335.63,143.32,197.06,7.86;9,335.63,153.78,209.05,7.86;9,335.63,164.24,20.99,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,494.04,122.39,51.78,7.86;9,335.63,132.85,166.32,7.86">University of Waterloo at TREC 2015 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,335.63,143.32,197.06,7.86;9,335.63,153.78,101.77,7.86">Proceedings of the Twenty-Fourth Text REtrieval Conference (TREC 2015)</title>
		<meeting>the Twenty-Fourth Text REtrieval Conference (TREC 2015)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.63,175.69,197.31,7.86;9,335.63,186.15,198.92,7.86;9,335.63,196.62,173.10,7.86;9,335.63,207.08,205.89,7.86;9,335.63,217.54,204.22,7.86;9,335.63,228.00,167.96,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,335.63,186.15,198.92,7.86;9,335.63,196.62,30.43,7.86">Simple dynamic emission strategies for microblog filtering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,384.03,196.62,124.70,7.86;9,335.63,207.08,205.89,7.86;9,335.63,217.54,204.22,7.86;9,335.63,228.00,20.95,7.86">Proceedings of the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016)</title>
		<meeting>the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1009" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.63,239.45,212.41,7.86;9,335.63,249.92,198.31,7.86;9,335.63,260.38,191.06,7.86;9,335.63,270.84,205.89,7.86;9,335.63,281.30,204.22,7.86;9,335.63,291.76,158.74,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,536.01,239.45,12.03,7.86;9,335.63,249.92,198.31,7.86;9,335.63,260.38,48.06,7.86">An exploration of evaluation metrics for mobile push notifications</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.98,260.38,124.71,7.86;9,335.63,270.84,205.89,7.86;9,335.63,281.30,204.22,7.86;9,335.63,291.76,20.95,7.86">Proceedings of the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016)</title>
		<meeting>the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="741" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.63,303.22,218.30,7.86;9,335.63,313.68,197.39,7.86;9,335.63,324.14,183.86,7.86;9,335.63,334.60,205.89,7.86;9,335.63,345.06,204.22,7.86;9,335.63,355.52,179.05,7.86;10,281.63,201.14,261.77,6.10;10,60.35,209.25,17.06,6.10;10,122.65,209.25,40.28,6.10;10,194.69,209.25,122.27,6.10;10,332.44,209.25,10.72,6.10;10,362.22,209.25,181.19,6.10;10,60.35,217.36,17.06,6.10;10,122.65,217.36,40.28,6.10;10,194.69,217.36,122.27,6.10;10,332.44,217.36,10.72,6.10;10,362.22,217.36,181.19,6.10;10,60.35,225.47,14.30,6.10;10,122.65,225.47,53.42,6.10;10,198.26,225.47,118.70,6.10;10,332.44,225.47,10.72,6.10;10,362.22,225.47,181.19,6.10;10,60.35,233.58,15.58,6.10;10,122.65,233.58,50.62,6.10;10,194.69,233.58,122.27,6.10;10,336.01,233.58,7.14,6.10;10,362.22,233.58,181.19,6.10;10,60.35,241.68,32.64,6.10;10,122.65,241.68,23.83,6.10;10,194.69,241.68,122.27,6.10;10,336.01,241.68,7.14,6.10;10,362.22,241.68,181.19,6.10;10,60.35,249.79,14.30,6.10;10,122.65,249.79,56.10,6.10;10,198.26,249.79,22.63,6.10;10,236.38,249.79,29.77,6.10;10,281.63,249.79,35.33,6.10;10,332.44,249.79,10.72,6.10;10,362.22,249.79,181.19,6.10;10,60.35,257.90,14.30,6.10;10,122.65,257.90,53.42,6.10;10,194.69,257.90,148.47,6.10;10,362.22,257.90,181.19,6.10;10,60.35,266.01,10.91,6.10;10,122.65,266.01,37.90,6.10;10,198.26,266.01,22.63,6.10;10,236.38,266.01,29.77,6.10;10,281.63,266.01,35.33,6.10;10,332.44,266.01,10.72,6.10;10,362.22,266.01,181.19,6.10;10,60.35,274.12,32.64,6.10;10,122.65,274.12,23.83,6.10;10,194.69,274.12,122.27,6.10;10,336.01,274.12,7.14,6.10;10,362.22,274.12,181.19,6.10;10,60.35,282.23,10.91,6.10;10,122.65,282.23,38.20,6.10;10,198.26,282.23,22.63,6.10;10,236.38,282.23,29.77,6.10;10,281.63,282.23,35.33,6.10;10,332.44,282.23,10.72,6.10;10,362.22,282.23,181.19,6.10;10,60.35,290.34,32.64,6.10;10,122.65,290.34,23.83,6.10;10,194.69,290.34,122.27,6.10;10,336.01,290.34,7.14,6.10;10,362.22,290.34,181.19,6.10;10,60.35,298.45,10.91,6.10;10,122.65,298.45,46.18,6.10;10,198.26,298.45,67.88,6.10;10,281.63,298.45,35.33,6.10;10,332.44,298.45,10.72,6.10;10,362.22,298.45,181.19,6.10;10,60.35,306.87,256.61,6.10;10,336.01,306.87,7.14,6.10;10,362.22,306.87,181.19,6.10;10,60.35,315.28,20.24,6.10;10,122.65,315.28,45.59,6.10;10,194.69,315.28,122.27,6.10;10,336.01,315.28,7.14,6.10;10,362.22,315.28,181.19,6.10;10,60.35,323.39,38.81,6.10;10,122.65,323.39,194.31,6.10;10,336.01,323.39,7.14,6.10;10,362.22,323.39,181.19,6.10;10,60.35,331.50,36.01,6.10;10,122.65,331.50,36.94,6.10;10,194.69,331.50,122.27,6.10;10,336.01,331.50,7.14,6.10;10,362.22,331.50,181.19,6.10;10,60.35,339.61,36.01,6.10;10,122.65,339.61,36.94,6.10;10,198.26,339.61,22.63,6.10;10,236.38,339.61,29.77,6.10;10,281.63,339.61,35.33,6.10;10,336.01,339.61,7.14,6.10;10,362.22,339.61,181.19,6.10;10,60.35,347.72,13.10,6.10;10,122.65,347.72,194.31,6.10;10,336.01,347.72,7.14,6.10;10,362.22,347.72,181.19,6.10;10,60.35,355.83,28.97,6.10;10,122.65,355.83,47.62,6.10;10,194.69,355.83,348.71,6.10;10,60.35,363.94,482.56,6.10;10,60.35,372.05,21.23,6.10;10,122.65,372.05,45.59,6.10;10,198.26,372.05,67.88,6.10;10,281.63,372.05,35.33,6.10;10,336.01,372.05,7.14,6.10;10,362.22,372.05,181.19,6.10;10,60.35,380.16,28.97,6.10;10,122.65,380.16,46.33,6.10;10,194.69,380.16,348.71,6.10;10,60.35,388.27,28.97,6.10;10,122.65,388.27,46.13,6.10;10,194.69,388.27,348.71,6.10;10,60.35,396.37,21.23,6.10;10,122.65,396.37,45.59,6.10;10,198.26,396.37,118.70,6.10;10,336.01,396.37,7.14,6.10;10,362.22,396.37,181.19,6.10;10,60.35,404.48,13.10,6.10;10,122.65,404.48,82.76,6.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,520.17,303.22,33.76,7.86;9,335.63,313.68,197.39,7.86;9,335.63,324.14,40.52,7.86;10,538.05,233.58,5.36,6.10;10,60.35,241.68,32.64,6.10;10,122.65,241.68,23.83,6.10;10,194.69,241.68,109.25,6.10;10,538.05,249.79,5.36,6.10;10,60.35,257.90,14.30,6.10;10,122.65,257.90,53.42,6.10;10,194.69,257.90,71.46,6.10">Assessor differences and user preferences in tweet timeline generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<idno>1640 0.192 1074 77 0.3714 (0.3199, 0.4260) 0.3937 (0.3413, 0.4486) A QU QUExpP-38 45 1 76 348 463 0.263 155 15 0.3689 (0.2885, 0.4573) 0.3770 (0.2960, 0.4656) A PKUICST run1-31 220 18 360 1997 2566 0.233 38 38 0.3679 (0.3302, 0.4073) 0.3980 (0.3595, 0.4378) A QU QUExpT-39 33 1 56 280 365 0.247 108 17 0.3667 (0.2745, 0.4698) 0.3778 (0.2846, 0.4810) A PKUICST run3-33 200 15 333 1830 2347 0.233 38 37 0.3650 (0.3257, 0.4061) 0.3923 (0.3523, 0.4338) A QU QUBaseline-37 56 3 108 477 635 0.263 219 17 0.3353 (0.2681, 0.4099) 0.3533 (0.2848, 0.4283) A WaterlooClarke WaterlooBaseline-50 148 12 286 1461 1888 0.236 44 42 0.3318 (0.2897, 0.3768) 0.3587 (0.3156, 0.4043) A ISIKol MyBaseline-24 184 18 375 2610 3169 0.182 13 14 0.3189 (0.2822, 0.3580) 0.3501 (0.3123, 0.3899) A WaterlooLin WaterlooBaseline-51 145 8 303 1367 1804 0.253 46 46 0.3180 (0.2769, 0.3621) 0.3355 (0.2937, 0.3801) A NUDTSNA nudt sna-29 262 34 546 3187 4011 0.210 47 46 0.3112 (0.2808, 0.3432) 0.3515 (0.3200, 0.3844) A NUDTSNA nudt sna-30 49 19 94 776 937 0.173 35 34 0.3025 (0.2370, 0.3771) 0.4198 (0.3465, 0.4967) A udel udelRunTFIDF-44 119 8 292 1101 1504 0.279 30 28 0.2840 (0.2429, 0.3290) 0.3031 (0.2610, 0.3487) A udel fang UDInfoDFP-47 632 91 1526 6945 9133 0.246 37954 33732 0.2810 (0.2628, 0.3000) 0.3215 (0.3025, 0.3411) A IRLAB DA-IICT runA daiict irlab-23 105 10 259 1721 2083 0.180 1314 1224 0.2807 (0.2376, 0.3283) 0.3075 (0.2629, 0.3560) P HLJIT MyBaseline-17 86 3 220 692 993 0.311 34 23 0.2783 (0.2313, 0.3308) 0.2880 (0.2404, 0.3409) A udel fang UDInfoSPP-48 467 66 1180 5171 6841 0.250 37900 33537 0.2726 (0.2521, 0.2942) 0.3112</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,394.79,324.14,124.71,7.86;9,335.63,334.60,205.89,7.86;9,335.63,345.06,204.22,7.86;9,335.63,355.52,20.95,7.86">Proceedings of the 38th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">113</biblScope>
		</imprint>
	</monogr>
	<note>A prna PRNATaskA2-35 117 7 191 1337. 0.2897, 0.3335) A udel fang UDInfoSFP-46 591 89 1537 7014 9179 0.242 37522 33162 0.2666 (0.2486, 0.2854) 0.3067 (0.2879, 0.3262) A HLJIT MyBaseline-18 63 7 168 833 1067 0.223 16 22 0.2647 (0.2127, 0.3242) 0.2941 (0.2399, 0.3549) A udel udelRunTFIDFQ-45</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
