<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.11,165.54,285.09,15.12">Overview of the TREC Tasks Track 2016</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.35,197.83,80.38,10.48"><forename type="first">Manisha</forename><surname>Verma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.61,197.83,102.07,10.48"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.56,197.83,72.68,10.48"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,421.11,197.83,41.85,10.48;1,147.59,211.58,47.80,10.48"><forename type="first">Rishabh</forename><surname>Mehrotra</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.27,211.58,76.57,10.48"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
						</author>
						<author>
							<persName coords="1,295.73,211.58,70.30,10.48"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.91,211.58,63.08,10.48"><forename type="first">Peter</forename><surname>Bailey</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.11,165.54,285.09,15.12">Overview of the TREC Tasks Track 2016</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D85B5AB6D1F1269E7B082E93A4DF81C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research in Information Retrieval has traditionally focused on serving the best results for a single query, ignoring the reasons (or the task) that might have motivated the user to submit that query. Often times search engines are used to complete complex tasks (information needs); achieving these tasks with current search engines requires users to issue multiple queries. For example, booking travel to a location such as London could require the user to submit various queries such as flights to London, hotels in London, points of interest around London etc.</p><p>Standard evaluation mechanisms focus on evaluating the quality of a retrieval system in terms of the relevance of the results retrieved, completely ignoring the fact that user satisfaction mainly depends on the usefulness of the system in helping the user complete the actual task that led the user issue the query. Similar to Tasks Track 2015 <ref type="bibr" coords="1,256.15,493.64,9.96,8.74" target="#b0">[1]</ref>, Tasks Track 2016 is an attempt investigate quality of retrieval systems in terms of (1) how well they can understand the underlying task that led the user submit a query, and (2) how useful they are for helping users complete their tasks.</p><p>In this overview, we first summarise the three categories of evaluation mechanisms used in the track and briefly describe the corpus, topics, and tasks that comprise the test collections. We then give an overview of the runs submitted to the Tasks Track and present evaluation results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation Goals</head><p>This year we kept the same evaluation goals as 2015 <ref type="foot" coords="1,344.53,628.93,3.97,6.12" target="#foot_0">1</ref> . The three evaluation goals are: (1) Task understanding, (2), Task completion, and (3) Adhoc Retrieval. Participants were provided with a set of 50 queries, together with the Freebase ID 2 for each entity in these queries. Participants were asked to use these 50 queries and Clueweb12 document collection (A or B) for each of these tasks. The details of each task and metrics are described in following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Understanding</head><p>The goal of this task is to test whether systems can understand the possible tasks users might be trying to achieve given a query. For this task, participants were asked to submit key phrases that captured user's search task given this query.</p><p>For each query, the participants were asked to submit a ranked list of up to 1000 key phrases that represent the set of all tasks the user of query may be looking for. For example, for the query '"hotels in London", some relevant key phrases can be: "cheap hotels in London", "reviews of hotels in London", "hotels in London city centre", etc. The goal of this task is to return a ranked list of key phrases that provide a complete coverage of tasks for each query, while avoiding redundancy.</p><p>Evaluating the coverage and relevance of the tasks submitted by the participants requires that a set of "gold standard" tasks that cover the set of all possible tasks are identified in advance. These gold standard tasks were constructed by the organizers, but were not be provided to the participants until the evaluation results were disclosed (i.e. post completion of track).</p><p>In order to guarantee higher coverage of tasks and be fair to all participants, tasks were developed based on information extracted from the logs of a commercial search engine, as well as by pooling the key phrases submitted by the participants. An example set of tasks for the query "hotels in London" may be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• hotels in in London [price]</head><p>• hotels in London [location]</p><p>• hotels [reviews] in London</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• London [other accommodation]</head><p>• hotels [in locations around] London Given the gold standard tasks, each key phrase submitted by the participants were judged by NIST assessors with respect to each of the gold standard tasks by using a three level judging scheme:</p><p>• Highly relevant (2): The key phrase completely describes the task and could be used as a query to a search engine to complete the task.</p><p>• Relevant (1):</p><p>The key phrase somehow describes the task but not fully, it can be used as a query to achieve the task but there are better queries than that.</p><p>• Non Relevant (0): The key phrase is not relevant to the task and cannot be used to complete it.</p><p>In the aforementioned example, the key phrase "cheap hotels in London city centre" would be judged as relevant to both "hotels in London [price]" and "hotels in London [location]. Similar to Tasks Track 2015, the quality of each ranked list has been evaluated using diversity metrics such as ERR-IA <ref type="bibr" coords="2,438.35,702.97,10.52,8.74" target="#b1">[2]</ref> and α-NDCG <ref type="bibr" coords="2,167.95,714.76,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Completion</head><p>The aim of this evaluation goal is to test the usefulness of a retrieval system in helping a user complete her search task.</p><p>Participants had to retrieve a ranked list of up to 1000 documents that could be relevant to any task a user may be trying to achieve given a query. The ranked lists provided by the participants were evaluated for diversity and relevance with respect to predefined list<ref type="foot" coords="3,298.24,204.54,3.97,6.12" target="#foot_2">3</ref> of possible tasks given a query.</p><p>Each document submitted by the participants (up to a certain rank, 20 in 2016) has been assessed in terms of its usefulness to complete each possible 'gold standard" task using a three level judging scheme:</p><p>• Key (2): The document is essential towards the completion of the task.</p><p>The document is enough on its own to complete the task.</p><p>• Useful (1): The document is useful towards the completion of the task. However, more documents need to be investigated in order to complete the task.</p><p>• Not Useful (0): The document is not useful towards completion of the task.</p><p>Similar to Tasks Track 2015, we also obtained relevance judgements for pooled list of documents from NIST assessors. Each document was labelled using four level judging scheme:</p><p>• Highly Relevant (2): The page contains significant amount of information about the task.</p><p>• Relevant (1): The content of this page provides some information on the task, which may be minimal.</p><p>• Non Relevant (0): The content of this page does not provide useful information about the task.</p><p>• Spam (-2): This page does not appear to be useful for any reasonable purpose; it may be spam or junk.</p><p>Given these judgements, similar to Task Understanding evaluation, the quality of each ranked list has been evaluated using diversity metrics: ERR-IA <ref type="bibr" coords="3,458.00,552.95,10.52,8.74" target="#b1">[2]</ref> and α-NDCG <ref type="bibr" coords="3,187.32,564.74,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adhoc Retrieval</head><p>For comparison purposes, we continued to have a traditional Web adhoc evaluation mechanism this year as well <ref type="bibr" coords="3,286.33,622.44,9.96,8.74" target="#b2">[3]</ref>. Participants were asked to submit a ranked list of up to 1000 documents for each topic. Participants were provided a short description of query along with query text and Freebase ID of entities in query.</p><p>Similar to Task track 2015, we used Task completion judgements for relevance to evaluate quality of the runs submitted by the participants. We ignored the usefulness category for adhoc evaluation. For the task completion, given a query, NIST assessors assigned document multiple relevance grades, each for possible tasks provided in ground truth. For Adhoc, we derived document relevance by using the maximum relevance label assigned for that document over all possible tasks.</p><p>Once these relevance judgements were obtained, ERR and NDCG were used as the primary metrics for evaluation, similar to previous years' Web Track <ref type="bibr" coords="4,455.23,187.90,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participants and Runs</head><p>Table <ref type="table" coords="4,153.07,242.24,4.98,8.74" target="#tab_0">1</ref> summarizes the participation in Tasks track. Overall we received 24 runs from four groups: 12 task understanding and 9 task completion and 3 adhoc runs. Number of submissions for each task from every group is given below:</p><p>• Webis Group (Webis): 3 adhoc, 3 completion and 3 understanding runs.</p><p>• University of Delaware (Udel-fang): 3 completion and 3 understanding runs.</p><p>• University of Delaware (Udel): 3 understanding and 3 completion runs.</p><p>• University of Stavanger (UiS) : 3 understanding runs. Overall, this year participants submitted more runs as compared to 2015. In 2015, only 21 runs were submitted, of which 11 were task understanding, 6 were task completion and 4 were adhoc runs. This year we had similar number of groups participating in the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Results</head><p>This year all 50 topics were judged for three tasks as compared to 2015 Tasks track. Detailed results per task are provided in following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Understanding</head><p>Task understanding runs were evaluated depth-20 pools of key phrases. Each key-phrase was labelled using judging guidelines described in Section 2.1.</p><p>This year 47689 keywords were evaluated across 50 queries, of which 34246 were 'not-relevant', 12003 were assigned 'relevant' and only 1440 were assigned 'highly-relevant' by the assessors.</p><p>We evaluate each task understanding submission with α-NDCG@20 and ERR-IA@20, where ERR-IA@20 is the primary metric. For each run, we report the average α-NDCG@20 and ERR-IA@20 for all topics. Participant results This year 12 runs were submitted compared to 11 submitted last year <ref type="bibr" coords="5,455.23,353.03,9.96,8.74" target="#b0">[1]</ref>. The maximum ERR@20 and α-NDCG@20 in 2015 was 0.471 and 0.573 respectively. This year, however, participants from UiS have achieved much higher values, even though more topics were evaluated this year <ref type="foot" coords="5,354.41,386.82,3.97,6.12" target="#foot_3">4</ref> . The minimum ERR@20 and α-NDCG@20 is also higher this year, they were 0.234 and 0.313 respectively in 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task completion results</head><p>Adhoc and task completion runs were evaluated using depth-20 pools of documents submitted by participants. Each document was labelled in terms of usefulness and relevance to each task, based on the judging schemes described in Section 2.2.</p><p>This year 33525 documents were labelled for 50 queries for both relevance and usefulness. For usefulness, 282 documents across 50 topics that were assigned 'Key' label. There were 3799 and 29444 documents marked 'useful' and 'not-useful' across 50 topics by NIST assessors this year. For relevance, 394 documents were marked 'highly-relevant', 4673 were labelled as 'relevant', 24184 were assigned 'non-relevant' and remaining were marked as 'spam' by assessors.</p><p>Given the judgements based on usefulness and relevance, both α-NDCG and ERR-IA metrics were computed at rank 10, focusing on ERR-IA at rank 10 computed using judgements based on usefulness as the primary metric. Table <ref type="table" coords="5,463.53,599.33,4.98,8.74" target="#tab_3">4</ref> shows the evaluation results for this category, sorted on ERR-IA in descending order. All participants used Category A collection of Clueweb documents.</p><p>This year some documents were not rendered properly at time of evaluation at NIST. Such documents were assigned -3 label by the assessors. Since, document relevance was not known, we ignored these documents from evaluating for relevance. However, these documents were marked 'not-useful' by assessors. Of 33525 documents, 3512 documents were not rendered properly and have been ignored for evaluations in Table <ref type="table" coords="5,266.33,693.63,3.87,8.74" target="#tab_2">3</ref>.  <ref type="table" coords="6,166.30,448.86,4.98,8.74" target="#tab_2">3</ref> shows the evaluation results based on judgements based on document relevance. The ranking of systems when evaluation metrics are computed based on relevance versus usefulness differ in some positions. Pearson's ρ correlation between relevance and usefulness based evaluation is 0.910 and 0.922 for ERR-IA@10 and α-NDCG@10 respectively. Kendall tau Rank Correlation between relevance and usefulness based evaluation is 0.77 (p-val = 0.004) and 0.72 (p-val=0.009) for ERR-IA@10 and α-NDCG@10 respectively.</p><p>Figure <ref type="figure" coords="6,171.13,531.37,4.98,8.74" target="#fig_0">1</ref> shows how the ranking of systems change when evaluation metrics are computed using usefulness judgements (x axis in the plots) versus using judgements in terms of relevance (y axis in the plots). As it can be seen in these plots, α-NDCG@10 has higher variance in scores than ERR-IA@10.</p><p>On comparison to Tasks track 2015, participants achieved lower ERR-IA@10 and α-NDCG@10 this year. In Tasks track 2015, maximum ERR-IA@10 and α-NDCG@10 for usefulness based evaluation was 0.442 and 0.518 respectively. However, this year participants the maximum maximum ERR-IA@10 and α-NDCG@10 for usefulness is 0.243 and 0.347 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adhoc Retrieval results</head><p>In order to evaluate the quality of Adhoc Retrieval runs, the judgements obtained for Task Completion were used in the way described in Section 2.3. ERR and NDCG at rank 10 values were then computing, using ERR at rank 10 as the primary metric. Table <ref type="table" coords="6,246.31,706.95,4.98,8.74">5</ref> shows the evaluation results for the adhoc runs, The metric values for the adhoc runs are low but similar to what participants achieved in 2015 where maximum (minimum) ERR@10 and NDCCG@10 were 0.124 (0.001) and 0.455 (0.003) respectively. When the evaluation results for runs submitted by the same groups for Task Completion and Adhoc are compared, the evaluation results seem much higher for Task Completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5: Adhoc results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Run ERR@10 NDCG@10 Webis webisA2 0.12 0.44 Webis webisA3 0.11 0.43 Webis webisA1 0.11 0.42</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The TREC 2016 Tasks Track ran for second year to build test collections to evaluate retrieval systems on relevance and usefulness of retrieved documents for a given user search task. We organized the track with three tasks: task understanding, completion and adhoc retrieval. We did not observe a significant rise in number of participants, however, the number of runs submitted this year were higher than 2015. Task understanding and task completion task received over 20 submissions from four participants. Submitted runs achieved higher accuracy for task understanding completion tasks. Tasks Track shall be organized again in 2017 with slight modifications to existing tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,124.80,231.26,343.71,8.74;7,124.80,243.05,343.71,8.74;7,124.80,254.84,22.19,8.74;7,157.52,128.29,137.48,91.56"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of evaluation results based on (left) ERR-IA, and (right) α-NDCG metrics when judgements based on usefulness versus relevance are used.</figDesc><graphic coords="7,157.52,128.29,137.48,91.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,180.46,399.09,232.40,54.46"><head>Table 1 :</head><label>1</label><figDesc>Tasks Track 2016 participation</figDesc><table coords="4,180.46,420.81,232.40,32.74"><row><cell>Task</cell><cell cols="3">Understanding Completion Adhoc</cell></row><row><cell>Groups</cell><cell>4</cell><cell>3</cell><cell>1</cell></row><row><cell>Runs</cell><cell>12</cell><cell>9</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,124.80,127.25,343.71,222.73"><head>Table 2 :</head><label>2</label><figDesc>Task understanding results</figDesc><table coords="5,124.80,149.39,343.71,200.58"><row><cell>Group</cell><cell>Run</cell><cell cols="2">ERR-IA@20 αNDCG@20</cell></row><row><cell>UiS</cell><cell>UiS 8</cell><cell>0.57</cell><cell>0.70</cell></row><row><cell>UiS</cell><cell>UiS 4</cell><cell>0.53</cell><cell>0.66</cell></row><row><cell>Webis</cell><cell>webis1</cell><cell>0.51</cell><cell>0.68</cell></row><row><cell>Webis</cell><cell>webis3</cell><cell>0.51</cell><cell>0.67</cell></row><row><cell>Webis</cell><cell>webis2</cell><cell>0.50</cell><cell>0.67</cell></row><row><cell>UiS</cell><cell>UiS 9</cell><cell>0.47</cell><cell>0.61</cell></row><row><cell>Udel</cell><cell>udelRun3</cell><cell>0.41</cell><cell>0.56</cell></row><row><cell>Udel</cell><cell>udelRun1</cell><cell>0.40</cell><cell>0.52</cell></row><row><cell cols="2">Udel-fang udelRun4</cell><cell>0.40</cell><cell>0.52</cell></row><row><cell cols="2">Udel-fang udelRun6</cell><cell>0.38</cell><cell>0.50</cell></row><row><cell cols="2">Udel-fang udelRun5</cell><cell>0.36</cell><cell>0.46</cell></row><row><cell>Udel</cell><cell>udelRun2</cell><cell>0.35</cell><cell>0.45</cell></row><row><cell cols="4">for task understanding are shown in 2, where evaluation results are sorted on</cell></row><row><cell cols="2">ERR-IA@20 in descending order.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,181.05,127.25,231.22,140.56"><head>Table 3 :</head><label>3</label><figDesc>Task Completion (Relevance) results</figDesc><table coords="6,181.05,149.39,231.22,118.41"><row><cell>Group</cell><cell>Run</cell><cell cols="2">ERRIA@10 αNDCG@10</cell></row><row><cell cols="2">Udel-fang udelRun5C</cell><cell>0.293</cell><cell>0.406</cell></row><row><cell cols="2">Udel-fang udelRun4C</cell><cell>0.286</cell><cell>0.398</cell></row><row><cell>Udel</cell><cell>udelRun1C</cell><cell>0.284</cell><cell>0.395</cell></row><row><cell>Webis</cell><cell>webisC2</cell><cell>0.274</cell><cell>0.418</cell></row><row><cell>Udel</cell><cell>udelRun3C</cell><cell>0.267</cell><cell>0.392</cell></row><row><cell>Udel</cell><cell>udelRun2C</cell><cell>0.263</cell><cell>0.366</cell></row><row><cell>Webis</cell><cell>webisC1</cell><cell>0.259</cell><cell>0.396</cell></row><row><cell cols="2">Udel-fang udelRun6C</cell><cell>0.257</cell><cell>0.372</cell></row><row><cell>Webis</cell><cell>webisC3</cell><cell>0.243</cell><cell>0.364</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,139.75,283.21,272.52,174.38"><head>Table 4 :</head><label>4</label><figDesc>Task Completion (Usefulness) results</figDesc><table coords="6,139.75,305.36,272.52,152.24"><row><cell>Group</cell><cell>Run</cell><cell cols="2">ERRIA@10 αNDCG@10</cell></row><row><cell cols="2">Udel-fang udelRun5C</cell><cell>0.243</cell><cell>0.347</cell></row><row><cell cols="2">Udel-fang udelRun4C</cell><cell>0.231</cell><cell>0.334</cell></row><row><cell>Udel</cell><cell>udelRun2C</cell><cell>0.230</cell><cell>0.323</cell></row><row><cell>Udel</cell><cell>udelRun1C</cell><cell>0.229</cell><cell>0.330</cell></row><row><cell>Webis</cell><cell>webisC2</cell><cell>0.223</cell><cell>0.349</cell></row><row><cell>Udel</cell><cell>udelRun3C</cell><cell>0.222</cell><cell>0.339</cell></row><row><cell cols="2">Udel-fang udelRun6C</cell><cell>0.215</cell><cell>0.320</cell></row><row><cell>Webis</cell><cell>webisC1</cell><cell>0.214</cell><cell>0.335</cell></row><row><cell>Webis</cell><cell>webisC3</cell><cell>0.199</cell><cell>0.305</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,140.05,684.79,328.47,6.99;1,124.80,694.13,76.26,6.99"><p>Ranking stability subtask was proposed, however, due to shortage of resources and time was not evaluated in</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2016" xml:id="foot_1" coords="1,135.89,702.14,153.44,8.44"><p>2 https://developers.google.com/freebase/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,140.05,712.10,230.36,6.99"><p>These subtasks were manually designed by organizers in 2016.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,140.05,712.56,138.60,6.99"><p>only 34 topics were evaluated in 2015</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.26,616.23,330.25,6.12;7,138.27,624.09,223.72,6.12" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,399.01,616.23,69.50,6.12;7,138.27,624.09,198.69,6.12">Nick Craswell, and Rishabh Mehrotra. Overview of the trec 2015 tasks track</title>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manisha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.26,639.92,330.25,6.12;7,138.27,647.78,330.25,6.12;7,138.27,655.64,146.14,6.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,370.24,639.92,98.27,6.12;7,138.27,647.78,56.52,6.12">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,211.46,647.81,257.06,6.08;7,138.27,655.67,43.08,6.08">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.26,671.47,330.25,6.12;7,138.27,679.32,159.38,6.12" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,345.38,671.47,123.13,6.12;7,138.27,679.32,16.64,6.12">Overview of the trec-2012 microblog track</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,171.75,679.36,20.34,6.08">TREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.26,695.15,330.25,6.12;7,138.27,703.01,289.18,6.12" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,175.23,703.01,101.71,6.12">Trec 2014 web track overview</title>
		<author>
			<persName coords=""><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
