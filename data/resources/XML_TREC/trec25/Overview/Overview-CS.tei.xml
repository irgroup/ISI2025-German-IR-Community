<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,63.28,72.35,483.16,16.84">Overview of the TREC 2016 Contextual Suggestion Track</title>
				<funder ref="#_JQjYKZJ">
					<orgName type="full">European Community</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,46.22,118.05,117.78,11.06"><forename type="first">Seyyed</forename><forename type="middle">Hadi</forename><surname>Hashemi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.12,118.05,103.76,11.06"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.00,118.05,66.21,11.06"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.33,118.05,72.19,11.06"><forename type="first">Julia</forename><surname>Kiseleva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,462.63,118.05,96.72,11.06"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NIST</orgName>
								<address>
									<settlement>Gaithersburg</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,63.28,72.35,483.16,16.84">Overview of the TREC 2016 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B4C009CF42636C38EAD2944A8E01230</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Contextual Suggestion Track offers a personalized point of interest (POI) recommendation task, in which participants develop systems to give a ranked list of suggestions related to a profile and a context pair available in the tasks' requests provided by the track organizers. Previously, reusability of the contextual suggestion track suffered from using dynamic collections and a shallow pool depth. The main innovations at TREC 2016 are the following. First, the TREC CS web corpus, consisting of a web crawl of the TREC contextual suggestion collection, was made available. The rich textual descriptions of the web pages makes far more information available for each candidate POI in the collection. Second, we released endorsements (end user tags) of the attractions as given by NIST assessors, potentially matching the endorsements of POIs in another city as given by the person issuing the request as part of her profile. Third, a multi-depth pooling approach extending beyond the shallow top 5 pool was used. The multi-depth pooling approach has created a test collection that provides more reliable evaluation results in ranks deeper than the traditional pool cut-off.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The TREC Contextual Suggestion Track ran for the fifth and last year as an independent track in 2016 <ref type="bibr" coords="1,248.88,488.95,7.16,7.86" target="#b3">[4]</ref><ref type="bibr" coords="1,256.04,488.95,3.58,7.86" target="#b4">[5]</ref><ref type="bibr" coords="1,256.04,488.95,3.58,7.86" target="#b5">[6]</ref><ref type="bibr" coords="1,259.62,488.95,7.16,7.86" target="#b6">[7]</ref>. The track has the primary goal of providing reusable test collection for evaluation of point-of-interest (POI) recommendation systems. The test collection is open to anyone who is willing to do research in contextual suggestion problem.</p><p>The contextual suggestion track assumes a traveller in a specific context (e.g., a city and trip type) seeking things to do that reflects their own interests, which is supposed to be inferred from their interests in the given context and a visited city (seed cities in the track). Given a user's contexts and profile including a POI list, their tags/endorsements, and ratings from the seed cities, participants make recommendations for attractions in a new context (including the target city as the location).</p><p>For example, imagine a group of information retrieval researchers with a November evening to spend in beautiful Gaithersburg, Maryland. A contextual suggestion system might recommend a beer at the Dogfish Head Alehouse<ref type="foot" coords="1,286.20,665.02,3.65,5.24" target="#foot_0">1</ref> , dinner at the Flaming Pit<ref type="foot" coords="1,158.48,675.48,3.65,5.24" target="#foot_1">2</ref> , or even a trip into Washington on the metro to see the National Mall 3 .</p><p>If you are familiar with the track, which has been operated since 2012, the main changes in this year is listed as follows:</p><p>1.</p><p>The track provides a fixed TREC Contextual Suggestion Web corpus as an additional data to overcome the dynamic nature of the open web.</p><p>2. The track provides endorsements (i.e., tags) of venues.</p><p>3. The track was split into two phases:</p><p>(a) Phase 1 experiment, which is a collection based task similar to the TREC 2015 Contextual Suggestion Track's Live Experiment. The main change is that the track does not require participants set up and register a live server. However, the track distributes a set of profiles and contexts and collect responses in a batch wise fashion, as was used in the track until 2014.</p><p>(b) Phase 2 experiment, which is a reranking task similar to the TREC 2015 Contextual Suggestion Track's Batch Experiment.</p><p>4. The track used a multilayer pooling approach that aimed creating a reusable test collection, which was very challenging in previous years of the track <ref type="bibr" coords="1,524.66,484.64,14.32,7.86" target="#b9">[10,</ref><ref type="bibr" coords="1,541.60,484.64,10.74,7.86" target="#b11">12]</ref>.</p><p>The rest of this paper is organized in the following way. Next, in §2, we will detail the track's tasks. This is followed by a discussion of the resulting test collection in §3 and the pooling method in §4. Then, §5 details the evaluation results of all submissions and teams. We conclude the paper in §6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TASK OVERVIEW</head><p>This section will discuss the tasks of the TREC 2016 contextual suggestion track.</p><p>The track followed the setup of 2015 with two distinct phases. In both phase 1 and phase 2 tasks, participants were asked to develop a system that is able to make suggestions for a specific person based on their given profile and context. As input of the task, the track organizers provide a set of profiles, a set of contexts and a set of example suggestions (URLs of pages corresponding to POIs in a given context). Each profile corresponded to a single user's preferences in example suggestions of another context or city, their gender and age, and each context includes information about the target city (i.e., the target location), a trip type, a trip duration, a type of group the person is travelling with, and a season the trip will occur in.</p><p>Profiles correspond to the stated preferences of real individuals, who either recruited through crowdsourcing or recruited editorial judges. These assessors first judged example attractions in seed locations, later returning to judge suggestions proposed by the phase 1 participants for various contexts. Both for the profile (i.e., seed pages) and for the suggested recommendations, assessors were able to choose the context or city for which recommendations were judged.</p><p>As output of the phase 1 task, for each context/profile pair, participants were required to return a ranked list of 50 suggestions. Each suggestion was expected to be relevant to the given profile and the context. As output of the phase 2 task, participants were expected to rerank the given suggestion candidates with respect to the user's profile and context and return them as the phase 2 response. To be precise:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase 1 Experiments</head><p>The phase 1 experiment is a collection based task, in which participants are asked to develop a contextual suggestion system that is able to make suggestion for a particular person in a specific context. In particular, for each given request (including profile and context), participants has to retrieve 50 suggestions from the TREC contextual suggestion collection as a response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase 2 Experiments</head><p>The phase 2 experiment is a reranking task, in which a suggestion candidates set is provided for each request. In fact, all the suggestion candidates available in phase 2 requests were made by participants in phase 1. Therefore, we have all the judgments of the suggestions available in the suggestion candidates, which facilitates the reuse of the contextual suggestion test collection.</p><p>The track continues to use a collection of URLs corresponding to POIs in each context that was released in 2015, see the examples in Table <ref type="table" coords="2,162.32,546.25,3.58,7.86" target="#tab_0">1</ref>. For the future studies on the contextual suggestion problem using the TREC contextual suggestion track qrels, due to the dynamic nature of the collection, we strongly recommend to use the TREC Contextual Suggestion Web corpus, which will be introduced in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TEST COLLECTION</head><p>This section discusses the resulting test collection. TREC 2016 contextual suggestion test collection consists of a corpus (including TREC contextual suggestion collection and the web corpus), a set of requests, and relevance judgments. In addition we have also released suggestions' endorsements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TREC CS Collection</head><p>The TREC Contetxual Suggestion collection was collected by asking participants as volunteers to retrieve suggestion candidates related to each city from the open web in a pretask phase. This collection was created in TREC 2015 contextual suggestion track. The collection consists of a set of attractions. For each attraction there are: An example of the TREC Contextual Suggestion collection is given in Table <ref type="table" coords="2,405.06,606.81,3.58,7.86" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TREC CS Web Corpus</head><p>In addition to the TREC contextual suggestion collection, which is available since 2015, we released TREC contextual suggestion web corpus. The TREC CS web corpus is a web crawl of the suggestions' URLs available at the TREC contextual suggestion collection. In this crawl, we have managed to fetch 77.39 % of the whole TREC Contextual Suggestion collection, which is 956,437 web pages out of 1,235,844 URLs.</p><p>{"id":743, "body": { "group": "Friends", "season":"Summer", "trip_type":"Holiday", "duration":"Weekend trip", "location":{ "state":"TX", "id":306, "name":"Waco", "lat":31.54933, "lng":-97.14667}, "person": { "gender": "Male", "age": 28, "id": 15012, "preferences":[ { "rating":4, "documentId":"TRECCS-00211395-161", "tags":[ "Beer", "Culture", "Cocktails", "Restaurants", "Food", "pub-hopping", "cocktails", This crawl includes web pages from different domains like yelp, tripadvisor and foursquare. Yelp was the most difficult domain to crawl, and we managed to crawl about 153K out of 220K yelp web pages available in the TREC contextual suggestion collection. Figure <ref type="figure" coords="4,203.73,99.48,4.61,7.86" target="#fig_0">1</ref> indicates percentage of available POIs from the most popular tourist attraction domains in the TREC Contextual Suggestion Web corpus. As it is shown in this figure, Foursquare, Yelp and Tripadvisor are the most popular domains in the TREC Contextual Suggestion Web corpus.</p><p>The TREC Contextual Suggestion Web Corpus includes attraction web pages of 272 different North American cities. In this corpus, there are 3,516.31 tourist attraction web pages in average per city. The corpus is in a WARC (Web ARChive) format. In order to have access to the data designated as the TREC CS Web Corpus, organizations must first fill in a data release Organizational Application Form. Then, the signed form must be scanned and sent by email to data@list.uva.nl. On receipt of the form, participants will be sent information on how to download the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Requests</head><p>In both phase 1 and phase 2 experiments, each request contains information about assessors' preferences as profiles and their chosen context. Moreover, phase 2 requests contains suggestion candidates related to each profile and context pair. Each profile consists of a list of attractions the assessor has previously rated, their gender and their age. For each attraction the profile will include: Each context consists of a city name which represents which city the trip will occur in and several pieces of data about the trip. The context is as follows:</p><p>1. A city the trip will occur in (e.g., Seattle) 2. A trip type (e.g., Business) 3. A trip duration (e.g., Weekend trip) 4. A type of group the person is travelling with (e.g.,</p><p>Travelling with a group of friends as "Friends") 5. A season the trip will occur in (e.g., Summer)</p><p>An example of the TREC Contextual Suggestion phase 2 request is shown in Example 1. The track organizers provide 438 input requests in total, in which requests having identifiers from 700 to 922 are used for the official experiments in TREC 2016 contextual suggestion track. In particular, TREC 2016 Phase 1 test collection consists of judgments of 61 requests, and TREC 2016 Phase 2 test collection includes all the phase 1 requests except requests having 707, 912 and 922 as identifiers, hence 58 requests in total. The difference is a result of some additional judged requests coming available after the release of the phase 2 requests. Some examples of official phase 1 requests' context and profile statistics are shown in Figure <ref type="figure" coords="4,384.50,89.02,3.58,7.86" target="#fig_4">2</ref>.</p><p>In building profiles for the TREC 2016 official requests (request IDs &gt;= 700), two seed cities were chosen (Seattle and Detroit). Each seed city had 30 POIs to be judged as user profiles. Users could choose which seed city to judge. If they just rate POIs of one of the cities, their profiles have 30 rated POIs. If they rate both of the seed cities' POIs, their profiles have 60 rated POIs. For example, in Phase 2 official requests, there are 39 requests having 30 judged example suggestions and 19 requests having 60 judged example suggestions in their profiles.</p><p>In phase 2 requests, due to the use of multi-depth pooling, which will be detailed in Section 4, the size of provided suggestion candidates is varied per request. Specifically, average number of suggestion candidates over the 58 phase 2 requests is 96.53, maximum number of suggestion candidates is 119 and minimum number of suggestion candidates is 79.</p><p>The rest of the requests, which were collected in TREC 2015, were used as train set of the TREC 2016 contextual suggestion track, as the qrels of those requests were available since TREC 2015. The TREC 2016 identifiers of those requests are same as the one used in TREC 2015, which facilitates evaluation of these requests based on the TREC 2015 contextual suggestion test collection. However, we have created a new pool and new sets of suggestions as suggestion candidates using the multi-depth pooling approach, which will be discussed in Section 4. Therefore, suggestion candidates of those requests available in TREC 2015 are different from the ones in TREC 2016. In fact, TREC 2015 batch requests contain a set of suggestion candidates with a very high probability of being relevant to the request. To make it a more realistic and challenging problem, we have injected more noise into the original batch requests of TREC 2015, hence the sets of candidates for the 2015 requests included this year differs from those of last year.</p><p>There are further requests that are based on requests made during the TREC 2015 live tasks. There were left out of the TREC 2015 data, privileging only a single request per crowdsourced assessor, but judgement are available to be used. As these requests were not as deeply pooled as the official TREC 2016 requests, they are excluded again from the official test collection in 2016, but may be released separately at a later date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relevance Judgments</head><p>Relevance judgments were collected through crowdsourcing and by the help of a group of graduate students. They were asked to rate suggestions in a same scale that presented in Section 3.3.</p><p>However, in the qrels, we have shifted the raw assessors' 5 point scale judgments with -2, making the judgments in the range -3 to 2, and making a score of 1.0 or higher correspond to a "interested" or "strongly interested" judgment. Therefore, the trec eval can be used to evaluate contextual suggestion runs based on all the common IR measures, included graded measures like NDCG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Suggestions Endorsements</head><p>In addition to the relevance judgments based on the ratings, we also asked the assessors to endorse the suggestions  using the tag field, which is shown in Figure <ref type="figure" coords="5,235.34,505.36,3.58,7.86" target="#fig_5">3</ref>. In practice, endorsement was not an easy task for them, and they were not willing to give tags to all the given suggestions. Therefore, NIST assessors endorsed all the pooled suggestions, and we include those tags/endorsements to both profiles and suggestion candidates of the phase 2 requests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">POOLING APPROACH</head><p>This section discusses the pooling approach used at TREC 2016.</p><p>Previously, TREC contextual suggestion organizers used the traditional pooling approach and pooled all the top-N suggestions of the submissions, in which N is a pool cut-off. They created a pool using 5 as the pool cut-off. According to the studies done on the reusability of the TREC contextual suggestion test collection <ref type="bibr" coords="5,176.79,679.80,7.45,7.86" target="#b8">[9]</ref><ref type="bibr" coords="5,184.24,679.80,3.72,7.86" target="#b9">[10]</ref><ref type="bibr" coords="5,184.24,679.80,3.72,7.86" target="#b10">[11]</ref><ref type="bibr" coords="5,187.96,679.80,11.17,7.86" target="#b11">[12]</ref>, reusability of the test collection suffered a lot from the personalization effects and respectively the shallow pool cut-off. To address this issue, we experimented with a "multi-depth" pooling approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-Depth Pooling</head><p>In the multi-depth pooling approach, in addition to the pool cut-off (hard pool cut-off), they defined two others pool cut-offs, namely, soft pool cut-off and very soft pool cut-off. In the multi-depth pooling approach, they have pooled the following suggestions:</p><p>1. All the suggestions/documents ranked higher than the hard pool cut-off by any of the submissions is pooled. This would guarantee an stable measures up to the traditional pool cut-off.</p><p>2. In addition, if a suggestion/document ranked higher than the soft pool cut-off by at least one submission, and also ranked higher than the very soft pool cut-off by at least one run from another participated team, the suggestion is pooled. This would have effects on having more stable measures deeper than the traditional hard pool cut-off in the ranking.</p><p>Following last years of the TREC contextual suggestion track, we have used 5 as the hard pool cut-off. In addition, taking into account the effort needed to create the test collection, we have set 25 as the soft pool cut-off and 50 as the very soft pool cut-off as this leads to a pool size of about 100 suggestions per request. The proposed pooling approach would give us more stable evaluation results over deeper ranks than the traditional pool cut-off. The traditional pooling approach with 5 as the pool cut-off would cost 3,377 judgments for the 61 official phase 1 requests. Interestingly, the above multi-depth pooling approach spend even less effort than pooling top-10 documents/suggestions provided by the submissions. Specifically, for the official qrels of the TREC 2016 contextual suggestion, we have collected 5,898 judgments using multidepth pooling approach, in which we have got 5,782 official judgments after filtering some noises. If we had used the traditional pooling approach with 10 as the pool cut-off, we would have collected 6,206 judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fraction of Judged Documents</head><p>In multi-depth pooling, we have pooled deeper and expected a larger fraction of judged documents after the pool cut-off. Figure <ref type="figure" coords="6,114.39,711.19,4.61,7.86">4</ref> shows a comparison of the cumulative over-lap@N <ref type="bibr" coords="6,346.32,490.96,14.31,7.86" target="#b9">[10]</ref> in TREC 2015 and 2016 Contextual Suggestion tracks. As it is shown in Figure <ref type="figure" coords="6,442.19,501.42,3.58,7.86">4</ref>, the fraction of judged documents is gently decreases after the hard pool cut-off (i.e., <ref type="bibr" coords="6,547.74,511.88,8.18,7.86" target="#b4">5)</ref> in TREC 2016 contextual suggestion test collection. However, in TREC 2015 contextual suggestion track, fraction of judged documents dropped dramatically after the pool cutoff (i.e., 5). We have also plotted just-in-rank overlap@N in Figure <ref type="figure" coords="6,346.43,564.18,3.58,7.86">4</ref>, in which we just consider fraction of judged and unjudged documents at rank N and calculate the overlap. This figure indicates that the multi-depth pooling is effective in minimizing the fraction of unjudged documents in ranks deeper than the pool cut-off. The larger fraction of judged documents in TREC 2016 helps us to have a more stable evaluation over ranks deeper than the traditional pool cut-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reusability</head><p>As shown in Figure <ref type="figure" coords="6,409.39,669.34,3.58,7.86">4</ref>, the fraction of judged documents has improved in ranks deeper than the hard pool cut-off using multi-depth pooling. However, effects of this improvement on the reusability of the test collection are not a priori clear.</p><p>Figure <ref type="figure" coords="7,91.79,57.64,4.61,7.86">5</ref> demonstrates reusability of the TREC 2016 test collection based on Leave-One-Team-Out (i.e., LOTO) <ref type="bibr" coords="7,283.19,68.10,9.72,7.86" target="#b2">[3]</ref> test. According to Figure <ref type="figure" coords="7,157.37,78.56,3.58,7.86">5</ref>, the TREC 2016 contextual suggestion test collection should be used with some care based on P@5 metric. The official runs are completely judged up to rank 5, by design of the pooling approach, but postsubmission experiments not contributing to the pool of judged documents risk being underrated. We have observed a similar system ranking correlation based on NDCG@5 metric having Kendall's τ = 0.43.</p><p>There is also good news: the test collection appears to be reusable when considering the more stable evaluation measures for incomplete test collections. Specifically, the test collection has got perfect system ranking correlation between official TREC system ranking and the LOTO system ranking based on the Kendall's τ using statistical significant inversions using MAP and bpref metrics. In this test, 54% of the pairwise comparisons are significant based on MAP and we have had 64% significant differences based on bpref.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION RESULTS</head><p>In this section, we first list our official evaluation measures. Then, we detail the evaluation results of phase 1 and 2 experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Measures</head><p>Three measures are used to rank both phase 1 and phase 2 runs. Our main measure is NDCG@5; in addition, P@5 and MRR are also used as two other metrics have been used since 2012 in TREC contextual suggestion track. As early rank cut-off measures are notably unstable, we also include measures taking more of the ranking into account, such as P@10, NDCG, MAP, Rprec and bpref, also profiting from the deeper pooling approach of this year.</p><p>The official results for the phase 1 task are shown in Table <ref type="table" coords="7,53.80,435.22,3.58,7.86" target="#tab_1">2</ref>. The best phase 1 runs from top-5 teams out of 8 participated teams in phase 1 will be detailed in Section 5.2. Table <ref type="table" coords="7,53.80,456.14,4.61,7.86" target="#tab_2">3</ref> shows the official results for the phase 2 task. The best phase 2 runs from top-5 teams out of 13 participated teams in phase 2 will be summarized in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Best Performing Phase 1 Submissions</head><p>The five best performing teams in the phase 1 evaluation are the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">USI</head><p>USI <ref type="bibr" coords="7,81.16,554.27,9.54,7.86" target="#b0">[1]</ref>'s best performing phase 1 run is "USI2", in which they crawled Foursquare for virtually 600K venues. Using the crawled data, they created positive and negative category profiles consisting of all categories a user liked/disliked as well as their corresponding normalized frequencies. The initial category profiles are then used to measure the similarity between a new venue and a particular user. They created the initial ranking and picked the top 10 venues for each user to gather extra information about them. For each user they also created positive and negative frequency-based venue taste keyword profiles. For the new set of venues, they extracted venue taste keywords and measured the similarity between the venues and a particular user. They reranked the top 10 venues for each user in the initial ranking using a linear combination of the venue category and taste keyword scores</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">IAPLab</head><p>Nanjing University's IAP Lab did not provide a description of their approach by the time of writing, nor submitted a participants' paper to the TREC Notebook or TREC Proceedings. Therefore, we cannot provide a further description of their approach in the overview paper, apart from noting that their system did well for the phase 1 task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">ADAPT_TCD</head><p>ADAPT TCD <ref type="bibr" coords="7,386.85,155.52,9.72,7.86" target="#b1">[2]</ref> proposed an ontology-based approach, using an ontology that was constructed using the Foursquare Category Hierarchy. The three models, each based upon this ontology, are: User Model, Document Model and Rule Model. For the User Model they build two models, one for each phase of the task, based upon the attractions that were rated in the user's profile. In the first phase they use only the positively rated attractions from each user. In the second phase they use both positive and negatively rated attractions to build the user model. The Document Model enriches documents with extra metadata (tags) from Foursquare and categories (concepts) from the ontology are attached to each document. The Rule model is used to tune the score for each candidate suggestion based upon the context of the trip and how it aligns with the rules in the model.</p><p>Their best performing run is "ADAPT TCD r1" in which, they build the user positive model based on the positively rated attractions in the user's profile. For each of these attractions, they create an index of all the classes, based on Foursquare data, that these attractions are an instance of, along with the tag set that was found on that attraction's page on Foursquare. They then compute the count per class and then the percentage of each class in the positive model. For a given place p that a user is travelling to, they select the documents that match the classes in the positive model. They eliminate the documents that belong to a class that violates at least one rule in the rule model. They retain the class percentage breakdown from the user model and map these percentages to 50 and represented this as a number, x, for each class. Following this, they select the top x attractions of this class from the retrieved documents after ranking them based on the features that have been collected in the Document Model from Foursquare, which are: the average users' rating, the users' rating count, the users' reviews count and the tag similarity measure between a document's tag set and the class tag set. After they select the required number of documents for all classes in the user model, they start to rank the documents based on the first three features mentioned before and return the final ranked list. If the number of attractions belonging to a specific class, in a specific city, do not meet the required number, they compensate for the shortfall by getting more attractions from the highest ranked class/classes in the user model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">FUM-IRLAB</head><p>FUM-IRLAB <ref type="bibr" coords="7,382.57,627.50,14.31,7.86" target="#b14">[15]</ref> followed two main approaches for finding suitable attractions for a given user: a content-based approach and a category-based approach.</p><p>In the content-based approach, all Web pages related to attractions are modeled as vectors of real numbers using word embedding and document embedding techniques. Then, similarities between attractions in the profile of a given user and new attractions are calculated using methods for finding similarities between vectors. In the category-based method, a subset of attractions is modeled as a vector of categories. These categories are extracted from the category information of the related Yelp, TripAdvisor, or Foursquare pages of the attractions. In addition, a user profile is modeled as a vector of categories, where these are categories extracted based on a mapping from the tags provided in the user's profile and the categories extracted for the attractions. Finally, similarities between attractions and user profiles are calculated based on similarities between these vectors. They submitted three methods of combining these two approaches to this track as three different runs.</p><p>Their best performing run is "FUM-IRLAB 3", in which the document-embedding vectors and the similarities between them are employed to produce a list of the most similar attractions to each attraction in the user profile. They found that despite a lot of very related results, this list contains a couple of completely unrelated pages. Hence, they decided to filter the result set for having a more precise list of attractions. They made an intersection between these lists with the attractions provided by category-based approach, making them more precise in the cost of decreasing recall. For each liked attraction in the user profile, they created a list of similar attractions, and then they iteratively selected two top attractions from each list and merged them to the final result set. They continue their iterations until they find 50 results from these lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">ExPoSe</head><p>ExPoSe <ref type="bibr" coords="9,97.03,362.66,9.71,7.86" target="#b7">[8]</ref> focused on one of the key steps of contextual suggestion methods is estimating a proper model for representing different objects in the data like users and attractions. They used the Significant Words Language Models (SWLM) as an effective method for estimating models representing significant features of sets of attractions as user profiles and sets of users as group profile. The SWLM model outperformed the standard language model, and is robust against negative examples.</p><p>For phase 1, the tag based run "ExPoSe response tags" obtained a better score than the content-based, and the combined run-although the differences between the runs were small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Best Performing Phase 2 Submissions</head><p>The five best performing teams in the phase 2 evaluation are the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">DUTH</head><p>DUTH <ref type="bibr" coords="9,94.75,564.74,14.31,7.86" target="#b13">[14]</ref> have further developed and built upon the two methods they first presented in Contextual Suggestion 2013, which they have fine-tuned using TREC 2015 data. They address the task by individually using two classification methods, namely, a weighted k-NN classifier and a modified Rocchio classifier. Also, as a third method, they explore the use of election systems, namely Borda Count, as a means of fusing the results of the two aforementioned classifiers.</p><p>Their best performing run is "DUTH rocchio", which is based on a Rocchio-like classifier. Using a user's rated venues as training examples, they build a custom query for the user using a modified Rocchio relevance feedback method. Specifically, they build a centroid per rating and combine/add those using their corresponding ratings as contributing fac-tors, offset by 2 so as ratings 0 and 1 provide negative feedback with -2 and -1 weights respectively. Rating 2 is eliminated as neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">LavalLakehead</head><p>LavalLakehead <ref type="bibr" coords="9,388.21,111.19,14.31,7.86" target="#b15">[16]</ref> formulate a customized query according to user profile to retrieve the 100 initial attractions. Then these 100 candidates are ranked by two independent ranking models who cover global trend of interests and contextual individual preference respectively. The first model is a pre-trained regressor on 2015 TREC data thus it can prioritize popular places and categories loved by all users (E.g. Museums and National Parks). The second model introduces word embedding to captures individual user preference. Both user profiles and candidate places are represented as word vectors in a same Euclidean space. So that a similarity score between user and attraction can be calculated by measuring their vector distance. In the end, a final ranking is given by summing up the two models' scores, and "Laval batch 3" is a result of the combination of the two above models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">USI</head><p>USI <ref type="bibr" coords="9,344.17,300.73,9.54,7.86" target="#b0">[1]</ref>'s best performing phase 2 run is "USI5", in which they computed a set of multimodal scores from multiple locationbased social networks (LBSNs) and combined them with a score that predicts the level of appropriateness of a venue to a given user context. Briefly, the scores are calculated as follows: positive and negative reviews are used to create user profiles to train a classifier which then predicts how much a particular user will like a new venue. Moreover, the frequency-based scores are calculated based on the venue categories and taste keywords. As for the prediction of appropriateness, they created two datasets using crowdsourcing and trained a classifier with the features they extracted from the datasets. A linear combination of all the scores produced the final ranking of the candidate suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">bupt_pris_2016</head><p>BUPT <ref type="bibr" coords="9,355.20,469.34,14.32,7.86" target="#b17">[18]</ref> collected data by crawling from the Yelp API and Foursquare API. With attractions marked with rating and tags in the preference list, they calculated users' average rating for each tag. For tags without a rating of the user in the profile, that is, the missing ratings, they filled them by Collaborative Filtering. Next, they got the users' rating for an attraction with either a mean function or a max function. By ranking the ratings of candidates, they git a ranked list for each user.</p><p>Their best performing run is " bupt pris 2016 cs.2 .4 max", in which they put a higher weight on ratings from Foursquare (0.4), a lower weight on ratings from Yelp (0.2), and used a max function to calculate the users' rating for attractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">UAmsterdam</head><p>UAmsterdam <ref type="bibr" coords="9,382.89,627.50,14.32,7.86" target="#b12">[13]</ref> studied contextual suggestion problem through neural user profiling and neural category preference modeling by the help of suggestions' endorsements being released by the TREC 2016 contextual suggestion track organizers. Their best performing run is "UAmsterdamDL", in which they studied how to predict relevant suggestions to the given user and context using category preference models.</p><p>In UAmsterdamDL, they cast the context-aware recommendation problem to a binary classification problem. In order to learn a user preference model, they have used a deep neural network with 4 hidden layers having 478 units, in which 123 suggestion-category relevance features have been used as inputs of the network. In this model, for each user, preferences in the user's profile considered as a train set and suggestion candidates available in the phase 2 requests considered as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>This section concludes our overview of the TREC 2016 contextual suggestion track. The track's main aim is the creation of a reusable test collections for the personalized POI recommendation task, which has proved a difficult task according to the previous studies <ref type="bibr" coords="10,191.75,202.26,14.31,7.86" target="#b9">[10,</ref><ref type="bibr" coords="10,209.53,202.26,10.74,7.86" target="#b11">12]</ref>. To this aim, we released the TREC CS web corpus, which is a crawl of the TREC contextual suggestion test collection. But fixing the test collection's content, we can overcome the dynamic nature of the contextual suggestion collection, and separate this effect from the personalization effects. We have also used a multi-depth pooling approach to improves reliability of the contextual suggestion systems scores based on measures at ranks deeper than the traditional pool cut-off. Moreover, we released attractions' endorsements being collected by NIST assessors, and participants showed considerable interest in using the endorsements to improve their contextual suggestion systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,316.81,329.79,239.10,7.86;2,316.81,340.25,98.31,7.86"><head>3Figure 1 :</head><label>1</label><figDesc>Figure 1: Most popular domains in the TREC Contextual Suggestion Web Corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,327.59,452.32,228.34,7.86;2,339.23,462.78,52.26,7.86;2,342.72,479.95,104.54,7.86;2,342.21,493.58,91.42,7.86;2,343.23,507.21,212.69,7.86;2,358.95,517.67,69.22,7.86;2,327.59,534.84,228.33,7.86;2,339.23,545.30,7.67,7.86;2,327.59,562.47,220.05,7.86;2,327.59,579.64,37.97,7.86"><head>1 .</head><label>1</label><figDesc>An attraction ID, which contains three parts separated by dashes (-) (a) The string 'TRECCS' (b) An 8 digit number (c) A three digit number corresponding to that attraction's city ID 2. A city ID which indicates which city this attraction is in 3. A URL with more information about the attraction 4. A title</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,201.18,388.30,61.09,7.47;3,201.18,398.76,14.10,7.47;3,177.69,409.22,14.10,7.47;3,177.69,419.68,4.71,7.47;3,177.69,430.14,14.10,7.47;3,83.72,440.60,65.79,7.47;3,102.52,451.06,169.15,7.47;3,102.52,461.52,37.60,7.47;3,121.31,471.98,32.90,7.47;3,121.31,482.44,56.39,7.47;3,121.31,492.90,84.58,7.47;3,121.31,503.36,65.79,7.47;3,121.31,513.82,28.20,7.47;3,121.31,524.28,14.10,7.47;3,102.52,534.75,169.15,7.47;3,102.52,545.21,37.60,7.47;3,102.52,555.67,46.99,7.47;3,102.52,566.13,65.79,7.47;3,102.52,576.59,65.79,7.47;3,102.52,587.05,75.18,7.47;3,102.52,597.51,56.39,7.47;3,102.52,607.97,14.10,7.47;3,107.21,618.43,14.10,7.47;3,102.52,628.89,4.71,7.47;3,102.52,639.35,4.71,7.47;3,122.69,677.30,364.35,7.86"><head>Example 1 :</head><label>1</label><figDesc>TREC Contextual Suggestion Track phase 2 request example in JSON format</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,64.58,368.04,48.22,7.86;4,79.70,385.62,103.79,7.86;4,79.19,399.62,67.98,7.86;4,80.21,413.62,162.97,7.86;4,79.19,427.62,79.22,7.86;4,80.20,441.62,113.50,7.86;4,80.79,455.62,148.90,7.86;4,64.58,473.20,161.95,7.86"><head>1 :</head><label>1</label><figDesc>Not loaded or no rating given 2. Tags/endorsements if it is applicable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,148.03,247.46,313.66,7.86;5,66.35,267.69,477.01,195.48"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of official phase 1 requests' contexts and profiles statistics.</figDesc><graphic coords="5,66.35,267.69,477.01,195.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,116.81,474.33,376.11,7.86"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of how assessors give rating and tags/endorsements to the suggestions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,73.27,257.16,459.09,7.86"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Cumulative and just-in-rank Overlap@N in TREC 2015 and 2016 contextual suggestion test collections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,59.18,55.00,492.35,70.66"><head>Table 1 :</head><label>1</label><figDesc>TREC Contextual Suggestion track collection example.</figDesc><table coords="2,59.18,78.30,139.03,6.99"><row><cell>Attraction ID</cell><cell>City ID URL</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,59.18,78.13,491.36,192.95"><head>Table 2 :</head><label>2</label><figDesc>Official TREC 2016 Contextual Suggestion Track's phase 1 submissions evaluated over 61 requests.</figDesc><table coords="8,59.18,101.77,491.36,169.31"><row><cell>Rank RunID</cell><cell>NDCG@5</cell><cell>P@5</cell><cell>MRR NDCG MAP</cell><cell>bpref</cell><cell>P@10</cell><cell>Rprec</cell></row><row><cell>USI2</cell><cell>0.2826</cell><cell cols="5">0.4295 0.6150 0.2083 0.0868 0.1772 0.3148 0.1619</cell></row><row><cell>IAPLab1</cell><cell>0.2789</cell><cell cols="5">0.3770 0.6245 0.2000 0.0729 0.1672 0.2721 0.1458</cell></row><row><cell>ADAPT TCD r1</cell><cell>0.2643</cell><cell cols="5">0.4066 0.5777 0.2333 0.0992 0.2046 0.3246 0.1886</cell></row><row><cell>FUM-IRLAB 3</cell><cell>0.2601</cell><cell cols="5">0.3803 0.5824 0.1494 0.0566 0.1124 0.2623 0.1133</cell></row><row><cell>FUM-IRLAB 1</cell><cell>0.2596</cell><cell cols="5">0.4000 0.5501 0.1928 0.0696 0.1672 0.2721 0.1498</cell></row><row><cell>ADAPT TCD r2</cell><cell>0.2595</cell><cell cols="5">0.4098 0.5512 0.2088 0.0895 0.1753 0.3230 0.1770</cell></row><row><cell>USI1</cell><cell>0.2578</cell><cell cols="5">0.3934 0.6139 0.2030 0.0839 0.1769 0.3148 0.1578</cell></row><row><cell>FUM-IRLAB 2</cell><cell>0.2544</cell><cell cols="5">0.3705 0.5945 0.1719 0.0677 0.1315 0.2885 0.1356</cell></row><row><cell>ExPoSe response tags</cell><cell>0.2461</cell><cell cols="5">0.3639 0.5206 0.1398 0.0496 0.1138 0.2033 0.0926</cell></row><row><cell>ExPoSe response all</cell><cell>0.2445</cell><cell cols="5">0.3541 0.5128 0.1735 0.0672 0.1413 0.2393 0.1282</cell></row><row><cell>ExPoSe response content</cell><cell>0.2443</cell><cell cols="5">0.3541 0.5114 0.1731 0.0669 0.1416 0.2393 0.1278</cell></row><row><cell>bupt runA</cell><cell>0.2395</cell><cell cols="5">0.3475 0.5366 0.2255 0.0843 0.2075 0.2689 0.1899</cell></row><row><cell>UAmsterdam1</cell><cell>0.2026</cell><cell cols="5">0.2951 0.4387 0.1169 0.0369 0.0936 0.1754 0.0803</cell></row><row><cell>Laval run1</cell><cell>0.1932</cell><cell cols="5">0.3115 0.4391 0.2209 0.0893 0.2054 0.2770 0.1936</cell></row><row><cell>UAmsterdam2</cell><cell>0.1641</cell><cell cols="5">0.2656 0.4095 0.1046 0.0338 0.0918 0.1607 0.0788</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,53.80,330.18,502.12,360.32"><head>Table 3 :</head><label>3</label><figDesc>Official TREC 2016 Contextual Suggestion Track's phase 2 submissions evaluated over 58 requests (excluding 707, 912, 922).</figDesc><table coords="8,59.18,364.28,491.36,326.22"><row><cell>Rank RunID</cell><cell>NDCG@5</cell><cell>P@5</cell><cell>MRR NDCG MAP</cell><cell>bpref</cell><cell>P@10</cell><cell>Rprec</cell></row><row><cell>DUTH rocchio</cell><cell>0.3306</cell><cell cols="5">0.4724 0.6801 0.6835 0.4497 0.4704 0.4552 0.4245</cell></row><row><cell>Laval batch 3</cell><cell>0.3281</cell><cell cols="5">0.5069 0.6501 0.6770 0.4536 0.4666 0.4500 0.4168</cell></row><row><cell>USI5</cell><cell>0.3265</cell><cell cols="5">0.5069 0.6796 0.6804 0.4590 0.4507 0.4603 0.4177</cell></row><row><cell>DUTH bcf</cell><cell>0.3259</cell><cell cols="5">0.4724 0.5971 0.6829 0.4606 0.4845 0.4431 0.4312</cell></row><row><cell>USI4</cell><cell>0.3234</cell><cell cols="5">0.4828 0.6854 0.6813 0.4576 0.4494 0.4552 0.4229</cell></row><row><cell>Laval batch 2</cell><cell>0.3118</cell><cell cols="5">0.4345 0.6287 0.6746 0.4378 0.4721 0.4207 0.4158</cell></row><row><cell>DUTH knn</cell><cell>0.3116</cell><cell cols="5">0.4345 0.6131 0.6763 0.4456 0.4825 0.4448 0.4189</cell></row><row><cell>bupt pris 2016 cs.2 .4 max</cell><cell>0.2936</cell><cell cols="5">0.4483 0.6255 0.6625 0.4318 0.4476 0.3983 0.3956</cell></row><row><cell>Laval batch 1</cell><cell>0.2889</cell><cell cols="5">0.4276 0.6372 0.6680 0.4397 0.4409 0.4310 0.4246</cell></row><row><cell>UAmsterdamDL</cell><cell>0.2824</cell><cell cols="5">0.4448 0.5924 0.6544 0.4168 0.4452 0.4310 0.3881</cell></row><row><cell>bupt pris 2016 cs.4 .2 max</cell><cell>0.2761</cell><cell cols="5">0.4241 0.5937 0.6602 0.4308 0.4465 0.4155 0.4031</cell></row><row><cell>DPLAB IITBHU iitbhu01</cell><cell>0.2757</cell><cell cols="5">0.4138 0.6298 0.6594 0.4269 0.4461 0.4034 0.4042</cell></row><row><cell>uogTrCs</cell><cell>0.2756</cell><cell cols="5">0.4207 0.5886 0.6585 0.4253 0.4500 0.3983 0.4005</cell></row><row><cell>UAmsterdamCB</cell><cell>0.2730</cell><cell cols="5">0.4069 0.5631 0.6499 0.4076 0.4337 0.4000 0.3780</cell></row><row><cell>ADAPT TCD br1</cell><cell>0.2720</cell><cell cols="5">0.4241 0.5472 0.6570 0.4357 0.4350 0.4103 0.4065</cell></row><row><cell>ADAPT TCD br2</cell><cell>0.2720</cell><cell cols="5">0.4241 0.5472 0.6570 0.4357 0.4328 0.4103 0.4068</cell></row><row><cell>SCIAICLTeam CasualChocolate</cell><cell>0.2650</cell><cell cols="5">0.3828 0.5853 0.6574 0.4213 0.4278 0.3931 0.3885</cell></row><row><cell>IAPLab2</cell><cell>0.2615</cell><cell cols="5">0.4034 0.5635 0.6524 0.4140 0.4547 0.3828 0.3934</cell></row><row><cell>ADAPT TCD br3</cell><cell>0.2612</cell><cell cols="5">0.3931 0.5996 0.6585 0.4342 0.4366 0.4034 0.4090</cell></row><row><cell>uogTrCsContext</cell><cell>0.2582</cell><cell cols="5">0.3828 0.5475 0.6566 0.4265 0.4454 0.4052 0.4058</cell></row><row><cell>SCIAICLTeam SassyStrawberry</cell><cell>0.2543</cell><cell cols="5">0.3690 0.5931 0.6556 0.4189 0.4275 0.3810 0.3863</cell></row><row><cell>bupt pris 2016 cs.3 .3 avg</cell><cell>0.2471</cell><cell cols="5">0.3793 0.6014 0.6505 0.4186 0.4396 0.3862 0.3879</cell></row><row><cell>USI3</cell><cell>0.2470</cell><cell cols="5">0.4103 0.6231 0.6596 0.4425 0.4471 0.4259 0.4151</cell></row><row><cell>ExPoSe SWLM</cell><cell>0.2375</cell><cell cols="5">0.3448 0.5285 0.6526 0.4125 0.4467 0.3845 0.3979</cell></row><row><cell>DPLAB IITBHU iitbhu04</cell><cell>0.2325</cell><cell cols="5">0.3310 0.5367 0.6507 0.4145 0.4363 0.3741 0.3933</cell></row><row><cell>FUM-IRLAB phase2 2</cell><cell>0.2318</cell><cell cols="5">0.3655 0.5191 0.6376 0.3985 0.4357 0.3759 0.3732</cell></row><row><cell>FUM-IRLAB phase2 1</cell><cell>0.2298</cell><cell cols="5">0.3517 0.5335 0.6378 0.3974 0.4344 0.3776 0.3696</cell></row><row><cell>SCIAICLTeam VerbatimVanilla</cell><cell>0.2119</cell><cell cols="5">0.3310 0.5371 0.6463 0.4099 0.4477 0.3707 0.3916</cell></row><row><cell>DPLAB IITBHU iitbhu05</cell><cell>0.2106</cell><cell cols="5">0.3034 0.4921 0.6347 0.3923 0.4207 0.3362 0.3638</cell></row><row><cell>CityUHKGeng 1st subminssion</cell><cell>0.1662</cell><cell cols="5">0.2414 0.3357 0.3882 0.2119 0.3312 0.2483 0.2157</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,58.40,700.86,103.40,7.86"><p>www.dogfishalehouse.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,58.40,711.19,123.59,7.86"><p>www.flamingpitrestaurant.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank all participants and the many volunteers who contributed to the test collections built over the last five years of the <rs type="institution">TREC Contextual Suggestion Track</rs>. This research is funded in part by the <rs type="funder">European Community</rs>'s <rs type="programName">FP7</rs> (project meSch, grant # <rs type="grantNumber">600851</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JQjYKZJ">
					<idno type="grant-number">600851</idno>
					<orgName type="program" subtype="full">FP7</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,71.45,451.84,221.46,6.99;10,71.45,460.81,221.46,6.99;10,71.45,469.77,34.49,6.99" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,222.73,451.84,70.17,6.99;10,71.45,460.81,151.20,6.99">Venue appropriateness prediction for contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,242.73,460.81,50.18,6.99;10,71.45,469.77,16.15,6.99">Voorhees and Ellis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.45,478.74,221.46,6.99;10,71.45,487.71,221.46,6.99;10,71.45,496.67,86.96,6.99" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,184.39,478.74,108.51,6.99;10,71.45,487.71,205.94,6.99">ADAPT TCD: An ontologybased context aware approach for contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bayomi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lawless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,71.45,496.67,68.62,6.99">Voorhees and Ellis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.45,505.64,221.45,6.99;10,71.45,514.61,221.46,6.99;10,71.45,523.57,111.45,6.99" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,276.99,505.64,15.91,6.99;10,71.45,514.61,168.28,6.99">Bias and the limits of pooling for large collections</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,248.23,514.61,44.68,6.99;10,71.45,523.57,29.63,6.99">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="491" to="508" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.45,532.54,221.46,6.99;10,71.45,541.51,221.46,6.99;10,71.45,550.47,221.45,6.99;10,71.45,559.44,221.46,6.99;10,71.45,568.40,221.45,6.99;10,71.45,577.37,197.59,6.99" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,140.27,541.51,152.63,6.99;10,71.45,550.47,59.64,6.99">Overview of the TREC 2012 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,102.37,559.44,190.54,6.99;10,71.45,568.40,29.72,6.99">The Twenty-First Text REtrieval Conference Proceedings</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2012">2012. 2013</date>
			<biblScope unit="page" from="500" to="298" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.45,586.34,221.45,6.99;10,71.45,595.30,221.46,6.99;10,71.45,604.27,221.46,6.99;10,71.45,613.24,221.46,6.99;10,71.45,622.20,221.45,6.99;10,71.45,631.17,178.54,6.99" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,181.95,595.30,110.95,6.99;10,71.45,604.27,103.79,6.99">Overview of the TREC 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,71.45,613.24,221.46,6.99;10,71.45,622.20,47.12,6.99">The Twenty-Second Text REtrieval Conference Proceedings (TREC 2013</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="500" to="302" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.45,640.14,221.46,6.99;10,71.45,649.10,221.46,6.99;10,71.45,658.07,221.46,6.99;10,71.45,667.03,221.46,6.99;10,71.45,676.00,221.45,6.99;10,71.45,684.97,178.54,6.99" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,140.27,649.10,152.63,6.99;10,71.45,658.07,59.40,6.99">Overview of the TREC 2014 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,71.45,667.03,221.46,6.99">Proceedings of the Twenty-Third Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>the Twenty-Third Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2015</date>
			<biblScope unit="page" from="500" to="308" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.45,693.93,221.45,6.99;10,71.45,702.90,221.46,6.99;10,71.45,711.87,221.46,6.99;10,334.46,58.32,221.45,6.99;10,334.46,67.28,221.46,6.99;10,334.46,76.25,195.95,6.99" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,140.27,702.90,152.63,6.99;10,71.45,711.87,59.40,6.99">Overview of the TREC 2015 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiseleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.46,58.32,221.45,6.99;10,334.46,67.28,16.17,6.99">Proceedings of the Twenty-Fourth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>the Twenty-Fourth Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2015">2015. 2016</date>
			<biblScope unit="page" from="500" to="319" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,85.22,221.46,6.99;10,334.46,94.18,221.45,6.99;10,334.46,103.15,86.96,6.99" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,541.81,85.22,14.11,6.99;10,334.46,94.18,206.97,6.99">Significant words language models for contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.46,103.15,68.62,6.99">Voorhees and Ellis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,112.12,221.46,6.99;10,334.46,121.08,221.45,6.99;10,334.46,130.05,99.95,6.99" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,452.22,112.12,103.70,6.99;10,334.46,121.08,124.14,6.99">Venue recommendation and web search based on anchor text</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,480.10,121.08,75.81,6.99;10,334.46,130.05,73.04,6.99">23rd Text REtrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,139.01,221.45,6.99;10,334.46,147.98,221.45,6.99;10,334.46,156.95,221.45,6.99;10,334.46,165.91,221.45,6.99;10,334.46,174.88,105.81,6.99" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,397.73,147.98,154.82,6.99">On the reusability of open test collections</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiseleva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,345.23,156.95,210.69,6.99;10,334.46,165.91,221.45,6.99;10,334.46,174.88,22.60,6.99">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="827" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,183.85,221.45,6.99;10,334.46,192.81,221.45,6.99;10,334.46,201.78,221.46,6.99;10,334.46,210.75,123.75,6.99" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,400.83,192.81,155.09,6.99;10,334.46,201.78,75.46,6.99">Test collection building and maintenance in dynamic domains</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiseleva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.64,201.78,122.28,6.99;10,334.46,210.75,73.29,6.99">15th Dutch-Belgian Information Retrieval Workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,219.71,221.45,6.99;10,334.46,228.68,221.46,6.99;10,334.46,237.64,221.46,6.99;10,334.46,246.61,138.87,6.99" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,403.28,228.68,152.64,6.99;10,334.46,237.64,146.49,6.99">An easter egg hunting approach to test collection building in dynamic domains</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiseleva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,502.52,237.64,53.40,6.99;10,334.46,246.61,71.85,6.99">Proceedings of NTCIR-EVIA 2016</title>
		<meeting>NTCIR-EVIA 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,255.58,221.45,6.99;10,334.46,264.54,221.45,6.99;10,334.46,273.51,15.52,6.99" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,497.98,255.58,57.94,6.99;10,334.46,264.54,127.08,6.99">Neural endorsement based contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">O</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,484.70,264.54,71.21,6.99">Voorhees and Ellis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,282.48,221.45,6.99;10,334.46,291.44,221.45,6.99;10,334.46,300.41,125.54,6.99" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,471.94,282.48,83.97,6.99;10,334.46,291.44,221.45,6.99;10,334.46,300.41,20.79,6.99">Recommending pointsof-interest via weighted kNN, rated rocchio, and borda count fusion</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kalamatianos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,373.04,300.41,68.62,6.99">Voorhees and Ellis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,309.38,221.46,6.99;10,334.46,318.34,221.45,6.99;10,334.46,327.31,221.45,6.99;10,334.46,336.27,15.52,6.99" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,549.58,309.38,6.34,6.99;10,334.46,318.34,221.45,6.99;10,334.46,327.31,138.59,6.99">A context based recommender system through collaborative filtering and word embedding techniques</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ramazani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ensan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,488.94,327.31,66.97,6.99">Voorhees and Ellis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,345.24,221.45,6.99;10,334.46,354.21,221.45,6.99;10,334.46,363.17,50.94,6.99" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,488.99,345.24,66.93,6.99;10,334.46,354.21,171.12,6.99">Word embeddings and global preference for contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamontagne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Khoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,522.72,354.21,33.19,6.99;10,334.46,363.17,32.60,6.99">Voorhees and Ellis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,372.14,221.45,6.99;10,334.46,381.11,221.45,6.99;10,334.46,390.07,221.45,6.99;10,334.46,399.04,104.73,6.99" xml:id="b16">
	<analytic>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,500.64,372.14,55.27,6.99;10,334.46,381.11,217.57,6.99">Proceedings of the Twenty-Fifth Text REtrieval Conference (TREC 2016)</title>
		<meeting>the Twenty-Fifth Text REtrieval Conference (TREC 2016)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="500" to="321" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,334.46,408.01,221.46,6.99;10,334.46,416.97,221.46,6.99;10,334.46,425.94,221.45,6.99;10,334.46,434.91,86.96,6.99" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,491.56,408.01,64.36,6.99;10,334.46,416.97,221.46,6.99;10,334.46,425.94,202.75,6.99">Beijing university of posts and telecommunications (BUPT) at TREC 2016: A rating model based on tags for contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.46,434.91,68.62,6.99">Voorhees and Ellis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
