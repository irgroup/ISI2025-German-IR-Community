<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.43,96.74,285.14,15.11">TREC 2016 Total Recall Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.39,129.22,101.33,10.48"><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.63,129.22,100.96,10.48"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.35,129.22,78.25,10.48"><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.43,96.74,285.14,15.11">TREC 2016 Total Recall Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8A3B4EB3EC3CBE37BCA097827FB953D4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The primary purpose of the Total Recall Track is to evaluate, through controlled simulation, methods designed to achieve very high recall -as close as practicable to 100% -with a human assessor in the loop. Motivating applications include, among others, electronic discovery in legal proceedings <ref type="bibr" coords="1,401.78,256.13,9.96,8.74" target="#b2">[3]</ref>, systematic review in evidencebased medicine <ref type="bibr" coords="1,126.04,268.08,9.96,8.74" target="#b5">[6]</ref>, and the creation of fully labeled test collections for information retrieval ("IR") evaluation <ref type="bibr" coords="1,542.05,268.08,9.96,8.74" target="#b4">[5]</ref>. A secondary, but no less important, purpose is to develop a sandboxed virtual test environment within which IR systems may be tested, while preventing the disclosure of sensitive test data to participants. At the same time, the test environment also operates as a "black box," affording participants confidence that their proprietary systems cannot easily be reverse engineered.</p><p>The task to be solved in the Total Recall Track is the following:</p><p>Given a simple topic description -like those typically used for ad-hoc and Web search -identify the documents in a corpus, one at a time, such that, as nearly as possible, all relevant documents are identified before all non-relevant documents. Immediately after each document is identified, its groundtruth relevance or non-relevance is disclosed.</p><p>Datasets, topics, and automated relevance assessments were all provided by a Web server supplied by the Track.</p><p>Participants were required to implement either a fully automated ("automatic") or semi-automated ("manual") process to download the datasets and topics, and to submit documents for assessment to the Web server, which rendered a relevance assessment for each submitted document in real time. Thus, participants were tasked with identifying documents for review, while the Web server simulated the role of a human-in-the-loop assessor operating in real time. Rank-based and set-based evaluation measures were calculated based on the order in which documents were presented to the Web server for assessment, as well as the set of documents that were presented to the Web server at the time a participant "called their shot," or declared that a "reasonable" result had been achieved. Particular emphasis was placed on achieving high recall while reviewing the minimum possible number of documents. The Total Recall Track debuted at TREC 2015 <ref type="bibr" coords="1,285.67,510.50,9.96,8.74" target="#b6">[7]</ref>. The TREC 2016 track was operationally identical to the TREC 2015 Track, differing only in the following respects:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• This year, participants were required to "call their shot" to indicate when they believed that as many of the relevant documents as reasonably possible had been identified with proportionate effort;</p><p>• The TREC 2015 At-Home collections (as well as the TREC 2015 Practice collections) were available for testing and development;</p><p>• 34 new topics were developed for the TREC 2015 Jeb Bush dataset for the 2016 At-Home task;</p><p>• Six topics and a new Rod Blagojevich/Pat Quinn dataset, as well as four topics and a new collection of Twitter tweets <ref type="bibr" coords="1,148.65,637.18,10.52,8.74" target="#b0">[1]</ref> were introduced for the 2016 Sandbox task.</p><p>Testing and development, as well as At-Home participation were done using the open Web: Participants ran their own systems and connected to the Web server at a public address. The Practice collections were available for several weeks prior to the At-Home collections; the At-Home collections were available for official runs throughout June, July, and August, 2016 (and continue to be available for unofficial runs). Sandbox runs were conducted in September 2016, entirely on a Web-isolated platform hosting the data collections. To participate in the Sandbox task, participants were required to encapsulate -as a VirtualBox virtual machine -a fully autonomous solution that would contact the Web server and conduct the task without human intervention. The only feedback available to Sandbox participants consisted of summary evaluation measures showing the number of relevant documents identified, as a function of the total number of documents identified to the Web server for review.</p><p>To aid participants in the Practice, At-Home, and Sandbox tasks, as well as to provide a baseline for comparison, a Baseline Model Implementation ("BMI") was made available. <ref type="foot" coords="2,327.29,106.10,3.97,6.12" target="#foot_0">1</ref> BMI was run on all of the collections, and summary results were supplied to participants for their own runs, as well as for the BMI runs. The system architecture for the Track is detailed in a separate 2015 Notebook Draft paper titled Total Recall Track Tools Architecture Overview. <ref type="foot" coords="2,549.42,130.01,3.97,6.12" target="#foot_1">2</ref>The TREC 2016 Total Recall Track attracted five participants, including two industrial groups that submitted manual At-Home runs, one academic group that submitted only automatic At-Home runs, and two academic groups that submitted both automatic At-Home and Sandbox runs.</p><p>The 2016 At-Home collection, athome4, consisted of 34 topics and the same dataset of 290,000 Jeb Bush emails that was used in the TREC 2015 athome1 collection (see <ref type="bibr" coords="2,304.19,191.36,10.30,8.74" target="#b6">[7]</ref>). The topics were composed by the Track coordinators, and relevance assessments were rendered by NIST assessors, with guidance and quality assurance provided by the Track coordinators. Documents were selected for assessment using a combination of interactive search and judging <ref type="bibr" coords="2,56.69,227.19,12.09,8.77" target="#b3">[4]</ref> and machine-learning techniques similar to those used for the TREC 2002 Filtering Track <ref type="bibr" coords="2,467.52,227.22,9.96,8.74" target="#b7">[8]</ref>.</p><p>The Sandbox collections consisted of two datasets and 10 topics. The Illinois collection consisted of 2.1M email messages from the administration of former Illinois Governors Blagojevich and Quinn, which were provided by the Illinois State Archive. In collaboration with the University of Illinois, six topics were identified and assessed by archive and university personnel. Documents were selected for review using a combination of interactive search and judging and machine learning as described above. The Twitter collection consisted of 800,000 tweets, with crowdsourced relevance assessments, for four topics <ref type="bibr" coords="2,283.31,298.95,9.96,8.74" target="#b0">[1]</ref>.</p><p>The principal tool for comparing runs was a gain curve. A gain curve plots recall (i.e., the proportion of all relevant documents submitted to the Web server for review) as a function of effort (i.e., the total number of documents submitted to the Web server for review). A run that achieves higher recall with less effort demonstrates superior effectiveness, especially at high recall levels. The traditional recall-precision curve conveys similar information, plotting precision (i.e., the fraction of documents submitted to the Web server that are relevant) as a function of recall (i.e., the proportion of all relevant documents submitted to the Web server for review). While gain curves and recall-precision curves convey similar information, they are influenced differently by prevalence or richness (i.e., the proportion of documents in the collection that are relevant), and convey different impressions when averaged over topics with different richness. In general, Total-Recall applications tolerate a fair amount of fixed overhead in exchange for high recall; this tradeoff is more readily apparent in a gain curve.</p><p>A gain curve or recall-precision curve is blind to the important consideration of when to stop a retrieval effort. In general, the density of relevant documents diminishes as effort increases, and at some point, the benefit of identifying more relevant documents no longer justifies the review effort required to find them. This year, participants were required to "call their shot," or to indicate when they thought a "reasonable" result had been achieved; that is, to specify the point at which they would recommend terminating the review process because further effort would be "disproportionate." They were not actually required to stop at this point, but they had to indicate, contemporaneously, when they would have chosen to stop had they been required to do so. For this point, we report traditional set-based measures, such as recall, precision, and F 1 .</p><p>To evaluate the appropriateness of various possible stopping points, in 2015, the Total Recall Track coordinators introduced a new parametric measure: recall @ aR + b, for various values of a and b. Recall @ aR + b was defined to be the recall achieved when aR + b documents had been submitted to the Web server, where R is the number of relevant documents in the collection. In its simplest form, recall @aR + b [a = 1; b = 0] is equivalent to R-precision, which has been used since TREC 1 as an evaluation measure for relevance ranking. R-precision might equally well be called R-recall, as precision and recall are, by definition, equal when R documents have been reviewed. The parameters a and b allow us to explore the recall that might be achieved when a times as many documents, plus and additional b documents are reviewed. The parameter a admits that it may be reasonable to review more than one document for every relevant one that is identified; the parameter b admits that it may be reasonable to review a fixed number of additional documents, over and above the number that are relevant. For example, if there are 100 relevant documents in the collection, it may be reasonable to review 200 documents (a = 2), plus an additional 100 documents (b = 100), for a total of 300 documents, in order to achieve high recall. In this Track Overview paper, we report all combinations of a ∈ {1, 2, 4} and b ∈ {0, 100, 1000}.</p><p>To address limitations of recall measures based on binary relevance, assessors for the athome4 and Illinois collections were asked to identify, among those documents that they assessed to be relevant, those they deemed to be "important" (i.e., "key"). An alternative version of recall was computed with respect to this set of documents; the corresponding gain curves and aR + b results are shown for this alternative version of recall, as well as for traditional recall.</p><p>To address the question of how well the systems identified different facets of relevance (see <ref type="bibr" coords="3,479.35,95.72,10.30,8.74" target="#b1">[2]</ref>), the athome4 assessors were also asked to group relevant documents into folders corresponding to meaningful subcategories they identified. In addition to overall recall for each topic, recall for each facet or subcategory was computed separately, so as to assess the diversity of coverage of each topic among the submitted results.</p><p>Finally, to address the issues of assessor (dis)agreement and the completeness of the documents presented to the assessors, a stratified sample of 50 documents for each of the athome4 topics was independently assessed by three secondary NIST assessors. Alternative versions of recall were computed for each of the secondary assessors, as well as for the "majority-of-three" assessments for the documents in the sample.</p><p>In calculating effort and precision, the measures described above consider only the number of documents submitted to the Web server for assessment. For manual runs, however, participants were permitted to look at the documents, and hence to conduct their own assessments. Participants were required to track and report the number of documents that they reviewed, and were required to submit the documents they reviewed contemporaneously to the server. However, not all participants followed the instructions to submit all documents they reviewed to the server. Therefore, the reader should consult the participants' descriptions of their methods; these descriptions should be considered when comparing manual runs to one another, or to automatic runs. It is not obvious whether (or how) this additional -and sometimes unaccounted for -effort is (or should be) reflected in the gain curves, and recall @ aR + b measures; therefore, the coordinators have chosen not to try.</p><p>Results for the TREC 2016 Total Recall Track are consistent with those of the 2015 Track, showing that a number of methods achieved results with very high recall and precision, on all collections, according to the standards set by previous TREC tasks. This observation should be interpreted in light of the fact that runs were afforded an unprecedented amount of relevance feedback, allowing them to receive authoritative relevance assessments throughout the process.</p><p>Overall, no run at TREC 2015 or TREC 2016 -whether manual or automatic -consistently achieved higher recall at lower effort than BMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Test Collections</head><p>Each test collection consisted of a corpus of English-language documents, a set of topics, and a complete set of relevance assessments for each topic. For the 2016 Practice runs, all of the Practice and At-Home collections from the TREC 2015 Total Recall Track were made available to participants.</p><p>For the TREC 2016 At-Home runs, four variants of the athome4 collection were available:</p><p>• athome4 : The (redacted) Jeb Bush Emails,<ref type="foot" coords="3,273.12,479.33,3.97,6.12" target="#foot_2">3</ref> consisting of 290,099 emails from Jeb Bush's eight-year tenure as Governor of Florida. We used 34 issues associated with his governorship as topics for the athome4 test collection, shown in Table <ref type="table" coords="3,197.73,504.82,3.87,8.74">1</ref>. For each topic, the server supplied a short topic title, consisting of one-to-three words.</p><p>• athome4desc: The same dataset and topics as athome4, but the server supplied the title as well as a short description of the topic, rather than just the title alone.</p><p>• athome4subset: A subset of athome4 with 12 randomly selected topics. This collection was provided for participants who lacked the resources to complete all 34 topics. Because all participants submitted results for either athome4 or athome4desc, athome4subset results are not reported here.</p><p>• athome4descsubset: A subset of athome4desc with 12 randomly selected topics, with both the title and a short description of the topic. This collection was likewise provided for participants who lacked the resources to complete all 34 topics. Because all participants submitted results for either athome4 or athome4desc, athome4descsubset results are not reported here.</p><p>For the Sandbox runs, we used two new datasets:</p><p>• Illinois: The Jeb Bush administration's involvement in a trademark dispute between Bacardi and the U.S. Patent and Trademark Office.</p><p>Table <ref type="table" coords="4,87.22,737.71,3.87,8.74">1</ref>: Topics and Topic Descriptions for the Athome4 Collection. The 12 subset topics are marked with a (*).</p><p>• Twitter : 800,000 tweets with crowdsourced relevance assessments, for four topics, supplied by Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participant Submissions</head><p>To assist participants in completing the At-Home and Sandbox tasks, as well as to provide a baseline for comparison, a Baseline Model Implementation ("BMI") was supplied to Track participants. <ref type="foot" coords="5,405.88,125.00,3.97,6.12" target="#foot_3">4</ref> The only change from the 2015 version of BMI was the inclusion of a default rule to call your shot: A "reasonable" result was deemed to have been achieved when m relevant and n non-relevant documents had been reviewed, where n &gt; a • m + b , and a = 0.5 and b = 1000, were predetermined constants. In general, the constant a determines how many non-relevant documents are to be reviewed in the course of finding each relevant document, while b represents fixed overhead, independent of the number of relevant documents. Two commercial teams (eDiscoveryTeam and catres) used manual processes; three academic teams (IMS, SFSU, and UW) used fully automated processes. All teams did the full athome4 collection; only two teams (SFSU and UW) submitted Sandbox runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">At-Home Task</head><p>Gain curves for the At-Home task are shown in Figure <ref type="figure" coords="5,299.78,297.36,3.87,8.74" target="#fig_1">1</ref>; aR + b and "call-your-shot" results are shown in Figure <ref type="figure" coords="5,56.69,309.32,3.87,8.74">2</ref>. The gain curves plot recall (averaged over all topics) as a function of the number of documents submitted. Several of the methods -all derivatives of BMI -yielded essentially the same curve, which is superior to all other submissions. The two manual efforts (eDiscoveryTeam and catres) fall somewhat below. The first nine columns of Figure <ref type="figure" coords="5,87.86,345.18,4.98,8.74">2</ref> show the same information in tabular form: recall when aR + b documents have been submitted, averaged over all topics. BMI and sfsu yield comparable results; sfsu may have a tiny edge. The last column shows recall achieved when the system "calls its shot." The BMI-derived runs achieve recall on the order of 0.95; the manual runs, on the order of 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sandbox Task</head><p>Figures <ref type="figure" coords="5,92.49,427.33,4.98,8.74" target="#fig_2">3</ref> and<ref type="figure" coords="5,121.02,427.33,4.98,8.74">4</ref> show gain curves, aR + b, and call-your-shot results for the Illinois collection. Only uw and sfsu participated in the Sandbox task, both achieving results comparable to BMI. Figures <ref type="figure" coords="5,433.17,439.28,4.98,8.74">5</ref> and<ref type="figure" coords="5,461.08,439.28,4.98,8.74">6</ref> show results for the same systems on the Twitter collection; notably, uw.knee calls its shot at lower recall (0.801), compared to other collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Alternative Relevance I: "Important" or "Key" Documents</head><p>Figures <ref type="figure" coords="5,91.46,509.47,4.98,8.74">7</ref> and<ref type="figure" coords="5,117.93,509.47,4.98,8.74">8</ref> show the results when only "important" or "key" documents are considered relevant for the purpose of evaluating recall. The calculations of the number of documents submitted and R remain unchanged. Comparison with the results that consider all relevant documents (Figures <ref type="figure" coords="5,328.00,533.38,4.98,8.74" target="#fig_1">1</ref> and<ref type="figure" coords="5,355.40,533.38,4.43,8.74">2</ref>) shows an insubstantial difference: recall for "important" or "key" documents appears to be slightly higher, particularly at lower levels of effort. Figures <ref type="figure" coords="5,530.92,545.34,4.98,8.74" target="#fig_4">9</ref> and<ref type="figure" coords="5,56.69,557.29,9.96,8.74" target="#fig_1">10</ref> show a similar effect for the Illinois Test Collection, as compared to figures 3 and 4. No "important" or "key" relevance assessments were available for the Twitter Test Collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Alternative Relevance II: Facet or Subtopic Recall</head><p>For the Jeb Bush Test Collection, assessors were asked to categorize relevant documents into subfolders of their own choosing, reflecting meaningful facets of relevance. A total of 348 folders were created (10.2 per topic, on average). Figures <ref type="figure" coords="5,92.71,639.43,9.96,8.74" target="#fig_1">11</ref> and<ref type="figure" coords="5,126.69,639.43,9.96,8.74" target="#fig_1">12</ref> show recall, macro-averaged over the 348 subtopics, as a function of effort. Comparison with recall averaged over the 34 topics as a whole (Figures <ref type="figure" coords="5,293.16,651.39,4.98,8.74" target="#fig_1">1</ref> and<ref type="figure" coords="5,320.84,651.39,4.43,8.74">2</ref>) shows no substantial difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Alternative Relevance III: Secondary Assessor and Majority-of-Three Assessments</head><p>For each topic in the Jeb Bush Test Collection, 50 documents were chosen using non-uniform sampling. Three assessors independently assessed each of the 50 documents for relevance. Recall over the entire dataset was computed using the Horvitz-Thompson estimator, as well as for four alternative versions of relevance: separately for each of the three secondary assessors, as well as a majority-of-three assessment for the three secondary assessors. The results for all of these versions of relevance is shown in Figures 13 through 20. Recall with respect to the majority vote appears somewhat lower than with respect to the original NIST assessments, as shown by comparing Figures <ref type="figure" coords="6,56.69,163.92,9.96,8.74" target="#fig_2">13</ref> and<ref type="figure" coords="6,89.44,163.92,9.96,8.74" target="#fig_1">14</ref> to Figures <ref type="figure" coords="6,150.42,163.92,4.98,8.74" target="#fig_1">1</ref> and<ref type="figure" coords="6,178.20,163.92,3.87,8.74">2</ref>. Results for recall with respect to each individual secondary assessor is substantially lower; the best results achieve recall on the order of 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In 2015 and 2016, a number of participants derived their systems from BMI. Other participants used manual processes involving some combination of ad-hoc search, human document review, and commercial macnine-learning tools. None of the manual participants were able to show consistently superior results to the fully automated method, BMI. This year, SFSU appeared to have a small edge in performance on the Jeb Bush Test Collection, but that edge did not manifest itself on either the Illinois or Twitter Test Collections. It is worth noting that the SFSU submission was more than ten times slower than BMI, taking more than one week to run on the Illinois collection, whereas BMI only took several hours. Similar run-time disparities were noticed in 2015: The WaterlooClarke submission, which appeared to have an edge on the At-Home Collections, took about one week to process the Kaine collection, whereas BMI took four hours.</p><p>Although the Track coordinators were not aware of any method that was superior to BMI, they were somewhat surprised that none has emerged in the two years of the Total Recall Track. One hypothesis is that uncertainty in human relevance determinations limits the ability to measure further improvements over those achieved by BMI. Proponents of manual review processes have suggested that limitations of recall may mask the inability of automated systems to find important documents, or to find documents representing uncommon facets or subtopics. To address these concerns, the Total Recall Track had the NIST assessors identify documents they felt were "important" or "key," as well as place relevant documents into subfolders reflecting the different facets or aspects of relevance. If "important" documents were being missed by the systems, or rare subtopics were underrepresented, we would have expected to see reduced recall, according to the alternate recall measures based on these criteria. In fact, we see remarkably consistent results among the different recall measures suggesting that the systems are robust in identifying "key" documents and different components of relevance.</p><p>A limitation of any Cranfield-style test <ref type="bibr" coords="6,245.00,469.74,10.52,8.74" target="#b8">[9]</ref> is the completeness and reliability of the relevance assessments. For the Jeb Bush and Illinois Test Collections, the documents were selected for review using a combination of interactive search and judging and machine-learning techniques. While these methods are state of the art, it has been suggested that they are biased in favor of similar methods. One way to investigate this issue is to use independently labeled collections. In 2015, the Kaine and MIMIC II Collections were both independently labeled; in 2016, the Twitter Collection was likewise independently labeled. The similarity of results using these independently labeled collections suggests that bias in the selection of documents is not a major factor in the results presented here.</p><p>A second way to investigate the issue of document-selection bias, and also assessor reliability, is to use independent assessments of a non-uniform random sample of documents to calculate recall using the Horvitz-Thompson estimator, which yields an unbiased estimate. When a single independent assessor is used to determine relevance, the recall results are substantially lower than those using the official relevance assessments. This result is perhaps not surprising, as a small number of false-positive assessments in the gold standard can result in substantially underestimated recall. Put another way, if the assessor has 70% precision, a perfect system (with 100% recall and 100% precision) would achieve only 70% recall, as measured with respect to the assessor's judgments. This observation is borne out by the fact that recall rises substantially when the majority-vote-of-three assessors is used to determine relevance, rather than a single assessor.</p><p>Brent West, Joanne Kaczmarek, and their colleagues at the Illinois State Archive and the Univerity of Illinois, for the time and effort they spent assessing these documents.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Recall @ R R+100 R+1000 2R 2R+100 2R+1000 4R 4R+100   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Recall @ R R+100 R+1000 2R 2R+100 2R+1000 4R 4R+100 4R+1000 Reasonable BMI .83</p><p>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,56.69,459.44,498.62,8.74;8,56.69,471.39,249.67,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Gain Curves Showing Recall (Averaged Over 34 Topics) as a Function of the Number of Submitted Documents, for the Athome4 (Jeb Bush) Test Collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,56.69,489.32,498.62,8.74;9,56.69,501.28,321.77,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Gain Curves Showing Recall (Averaged Over Six Topics) as a Function of the Number of Submitted Documents, for the Illinois (Rod Blagojevich/Pat Quinn) Test Collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,56.69,486.33,498.61,8.74;12,56.69,498.29,452.14,8.74"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Gain Curves Showing "Important" or "Key" Document Recall (Averaged Over Six Topics) as a Function of the Number of Submitted Documents, for the Illinois (Rod Blagojevich/ Pat Quinn) Test Collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="15,56.69,459.44,498.61,8.74;15,56.69,471.39,474.11,8.74"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Gain Curves Showing Recall According to the First of Three Secondary Assessors (Averaged Over 34 Topics) as a Function of the Number of Submitted Documents, for the Athome4 (Jeb Bush) Test Collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="16,56.69,459.44,498.61,8.74;16,56.69,471.39,474.11,8.74"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Gain Curves Showing Recall According to the Second of Three Secondary Assessors (Averaged Over 34 Topics) as a Function of the Number of Submitted Documents, for the Athome4 (Jeb Bush) Test Collection.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,71.94,712.98,180.18,6.64"><p>http://plg.uwaterloo.ca/∼gvcormac/trecvm/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,71.94,721.91,233.86,6.99"><p>cormack.uwaterloo.ca/total-recall/overview/totalrecallarch.pdf.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,71.94,730.44,287.92,6.64"><p>https://web.archive.org/web/20160221072908/http://jebemails.com/home</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,71.94,670.87,142.33,6.99"><p>http://cormack.uwaterloo.ca/trecvm/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We are grateful to the <rs type="institution">Illinois State Archive</rs> and the <rs type="institution">University of Illinois</rs> for affording us access to the <rs type="institution">Rod Blagojevich/Pat Quinn</rs> dataset for the purposes of the Sandbox evaluation. A special thanks goes to <rs type="person">David Joens</rs>,</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,72.19,126.57,483.11,8.74;7,72.19,138.53,51.91,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,336.14,126.57,214.65,8.74">Estimating topical volume in social media streams</title>
		<author>
			<persName coords=""><forename type="first">Praveen</forename><surname>Bommannavar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anand</forename><surname>Rajaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,84.64,138.53,35.45,8.74">SAC &apos;16</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.19,158.45,483.11,8.74;7,72.19,170.41,139.27,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,273.83,158.45,281.48,8.74;7,72.19,170.41,63.05,8.74">Multi-faceted recall of continuous active learning for technologyassisted review</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,156.50,170.41,28.26,8.74">SIGIR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.19,190.33,483.11,8.74;7,72.19,202.29,127.09,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,271.98,190.33,155.38,8.74">The Grossman-Cormack Glossary of</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,429.74,190.33,125.56,8.74;7,72.19,202.29,72.92,8.74">Technology-Assisted Review. Fed. Cts. L. Rev</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.19,222.21,483.12,8.74;7,72.19,234.17,119.48,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,403.00,222.21,152.31,8.74;7,72.19,234.17,43.68,8.74">Efficient construction of large test collections</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,136.72,234.17,28.26,8.74">SIGIR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.19,254.09,483.12,8.74;7,72.19,266.05,253.74,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,354.06,254.09,201.25,8.74;7,72.19,266.05,113.31,8.74">The implications of Rule 26(g) on the use of technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Maura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,198.84,266.05,72.92,8.74">Fed. Cts. L. Rev</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.19,285.98,483.12,8.74;7,72.19,297.93,121.40,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,247.68,285.98,256.51,8.74">Cochrane handbook for systematic reviews of interventions</title>
		<author>
			<persName coords=""><forename type="first">Julian Pt</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sally</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Wiley Online Library</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.19,317.86,483.12,8.74;7,72.19,329.81,192.51,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,447.92,317.86,107.39,8.74;7,72.19,329.81,63.92,8.74">TREC 2015 Total Recall Track overview</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,157.47,329.81,75.72,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.19,349.74,457.26,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,238.47,349.74,214.61,8.74">Building a filtering test collection for TREC 2002</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,474.49,349.74,50.33,8.74">SIGIR 2003</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.19,369.66,483.11,8.74;7,72.19,381.62,245.75,8.74;8,115.18,518.86,18.40,8.74;8,334.37,512.89,37.77,8.74;8,149.19,524.84,7.33,8.74;8,172.14,524.84,388.84,8.74;8,113.40,536.80,45.83,8.74;8,180.78,536.80,12.73,8.74;8,225.25,536.80,12.73,8.74;8,262.03,536.80,12.73,8.74;8,298.81,536.80,12.73,8.74;8,347.31,536.77,14.64,8.77;8,387.53,536.80,12.73,8.74;8,424.31,536.80,12.73,8.74;8,472.61,536.77,14.64,8.77;8,527.67,536.80,17.71,8.74;8,89.68,548.72,60.74,8.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,160.69,369.66,219.17,8.74">The philosophy of information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno>BMI .68 .79 .93 .86 .89 .96 .93 .93 .97 .945 BMI-Desc</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,402.15,369.66,153.16,8.74;7,72.19,381.62,104.45,8.74;8,115.18,518.86,18.40,8.74;8,334.37,512.89,37.77,8.74;8,149.19,524.84,7.33,8.74;8,172.14,524.84,388.84,8.74">Run Recall @ R R+100 R+1000 2R 2R+100 2R+1000 4R 4R+100 4R+1000 Reasonable</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="143" to="170" />
		</imprint>
	</monogr>
	<note>Evaluation of cross-language information retrieval systems</note>
</biblStruct>

<biblStruct coords="8,151.70,714.13,308.60,8.74;11,416.45,229.10,51.66,18.23;11,416.45,243.24,78.81,18.23;11,416.45,257.39,17.77,18.23;11,416.45,271.53,44.93,18.23;11,416.45,285.67,53.37,18.23;11,416.45,299.81,53.37,18.23;11,416.45,313.96,40.18,18.23;11,416.45,328.10,54.08,18.23;11,416.45,342.24,47.95,18.23;11,416.45,356.38,46.27,18.23;11,416.45,370.53,75.62,18.23;11,416.45,384.67,109.20,18.23" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,151.70,714.13,308.60,8.74;11,416.45,229.10,51.66,18.23;11,416.45,243.24,78.81,18.23;11,416.45,257.39,17.77,18.23;11,416.45,271.53,44.93,18.23;11,416.45,285.67,53.37,18.23;11,416.45,299.81,53.37,18.23;11,416.45,313.96,22.96,18.23">Figure 2: Recall @ aR+b for the Athome4 (Jeb Bush) Test Collection. UW-Target UW-Target-Desc BMI BMI-Desc SFSU-Run1 SFSU-Run2 IMS</title>
		<idno>ASR IMS-BM25S IMS-BM25 IMS-ASRE catres-manual1 eDiscoveryTeam-Run1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.69,459.44,498.61,8.74;11,56.69,471.39,376.73,8.74;11,115.18,518.86,18.40,8.74;11,334.37,512.89,37.77,8.74;11,149.19,524.84,7.33,8.74;11,172.14,524.84,388.84,8.74;11,113.40,536.80,45.83,8.74;11,180.78,536.80,12.73,8.74;11,225.25,536.80,12.73,8.74;11,262.03,536.80,12.73,8.74;11,298.81,536.80,12.73,8.74;11,348.26,536.80,12.73,8.74;11,387.54,536.80,12.73,8.74;11,424.32,536.80,12.73,8.74;11,472.61,536.77,14.64,8.77;11,527.67,536.80,17.71,8.74;11,89.68,548.75,69.54,8.74;11,180.78,548.75,12.73,8.74;11,225.25,548.75,12.73,8.74;11,262.03,548.75,12.73,8.74;11,298.81,548.75,12.73,8.74;11,348.26,548.75,12.73,8.74;11,386.58,548.72,14.64,8.77;11,424.31,548.75,12.73,8.74;11,472.61,548.72,14.64,8.77;11,527.67,548.75,17.71,8.74;11,107.64,560.71,51.58,8.74;11,180.78,560.71,12.73,8.74;11,225.25,560.71,12.73,8.74;11,262.03,560.71,12.73,8.74;11,298.81,560.71,12.73,8.74;11,348.26,560.71,12.73,8.74;11,387.54,560.71,12.73,8.74;11,424.32,560.71,12.73,8.74;11,473.57,560.71,12.73,8.74;11,527.67,560.71,17.71,8.74" xml:id="b10">
	<analytic>
		<idno>BMI .74 .84 .95 .89 .91 .97 .95 .95 .99 .967 BMI-Desc .78 .87 .96 .92 .95 .97 .98 .98 .99 .964 catres .62 .75 .85 .80 .84 .89 .90 .90 .91 .835</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,100.23,459.44,455.08,8.74;11,56.69,471.39,376.73,8.74;11,115.18,518.86,18.40,8.74;11,334.37,512.89,37.77,8.74;11,149.19,524.84,7.33,8.74;11,172.14,524.84,388.84,8.74">Gain Curves Showing &quot;Important&quot; or &quot;Key&quot; Document Recall (Averaged Over 34 Topics) as a Function of the Number of Submitted Documents, for the Athome4 (Jeb Bush) Test Collection. Run Recall @ R R+100 R+1000 2R 2R+100 2R+1000 4R 4R+100 4R+1000 Reasonable</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,77.30,714.13,457.40,8.74;13,416.45,226.11,51.66,18.23;13,416.45,240.26,78.81,18.23;13,416.45,254.40,17.77,18.23;13,416.45,268.54,44.93,18.23;13,416.45,282.68,53.37,18.23;13,416.45,296.82,53.37,18.23;13,416.45,310.97,40.18,18.23;13,416.45,325.11,54.08,18.23;13,416.45,339.25,47.95,18.23;13,416.45,353.39,46.27,18.23;13,416.45,367.54,75.62,18.23;13,416.45,381.68,109.20,18.23" xml:id="b11">
	<monogr>
		<idno>IMS-ASR IMS-BM25S IMS-BM25 IMS-ASRE catres-manual1 eDiscoveryTeam-Run1</idno>
		<title level="m" coord="11,77.30,714.13,334.14,8.74">Figure 8: &quot;Important&quot; or &quot;Key&quot; Document Recall @ aR+b for the Athome</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Jeb Bush) Test Collection. UW-Target UW-Target-Desc BMI BMI-Desc SFSU-Run1 SFSU-Run2</note>
</biblStruct>

<biblStruct coords="13,56.69,456.45,498.61,8.74;13,56.69,468.40,418.38,8.74;13,115.18,509.90,18.40,8.74;13,334.37,503.92,37.77,8.74;13,149.19,515.87,7.33,8.74;13,172.14,515.87,388.84,8.74;13,113.40,527.83,45.83,8.74;13,180.78,527.83,12.73,8.74;13,225.25,527.83,12.73,8.74;13,262.03,527.83,12.73,8.74;13,298.81,527.83,12.73,8.74;13,347.31,527.80,14.64,8.77;13,387.53,527.83,12.73,8.74;13,424.31,527.83,12.73,8.74;13,472.61,527.80,14.64,8.77;13,527.67,527.83,17.71,8.74;13,89.68,539.75,70.50,8.77;13,180.78,539.78,12.73,8.74;13,225.25,539.78,12.73,8.74;13,262.03,539.78,12.73,8.74;13,298.81,539.78,12.73,8.74;13,347.31,539.75,14.64,8.77;13,386.58,539.75,14.64,8.77;13,423.36,539.75,14.64,8.77;13,472.61,539.75,14.64,8.77;13,527.67,539.78,17.71,8.74;13,107.64,551.74,51.58,8.74;13,180.78,551.74,12.73,8.74;13,225.25,551.74,12.73,8.74;13,262.03,551.74,12.73,8.74;13,298.81,551.74,12.73,8.74;13,348.26,551.74,12.73,8.74;13,387.54,551.74,12.73,8.74;13,424.32,551.74,12.73,8.74;13,473.57,551.74,12.73,8.74;13,527.67,551.74,17.71,8.74;13,62.67,563.70,96.55,8.74;13,180.78,563.70,12.73,8.74;13,225.25,563.70,12.73,8.74;13,262.03,563.70,12.73,8.74;13,298.81,563.70,12.73,8.74;13,348.26,563.70,12.73,8.74;13,387.54,563.70,12.73,8.74;13,424.32,563.70,12.73,8.74;13,473.57,563.70,12.73,8.74;13,527.67,563.70,17.71,8.74;13,95.72,575.65,63.50,8.74" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="13,56.69,456.45,498.61,8.74;13,56.69,468.40,418.38,8.74;13,115.18,509.90,18.40,8.74;13,334.37,503.92,37.77,8.74;13,149.19,515.87,7.33,8.74;13,172.14,515.87,202.49,8.74">Figure 11: Gain Curves Showing Facet or Subtopic Recall (Macro-Averaged Over 348 Subtopics of 34 Topics) as a Function of the Number of Submitted Documents, for the Athome4 (Jeb Bush) Test Collection. Run Recall @ R R+100 R+1000 2R 2R+100 2</title>
		<idno>R+1000 4R 4R+100 4R+1000 Reasonable BMI .67 .77 .93 .86 .89 .96 .94 .95 .97 .946 BMI-Desc .69 .78 .92 .88 .90 .96 .95 .96 .97 .949 catres .52 .60 .75 .71 .74 .82 .82 .83 .86 .706 eDiscoveryTeam .65 .71 .81 .78 .80 .84 .84 .85 .87 .735 ims base .17</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,56.69,705.16,498.61,8.74;13,56.69,717.12,119.94,8.74;14,416.45,226.11,51.66,18.23;14,416.45,240.26,78.81,18.23;14,416.45,254.40,17.77,18.23;14,416.45,268.54,44.93,18.23;14,416.45,282.68,53.37,18.23;14,416.45,296.82,53.37,18.23;14,416.45,310.97,40.18,18.23;14,416.45,325.11,54.08,18.23;14,416.45,339.25,47.95,18.23;14,416.45,353.39,46.27,18.23;14,416.45,367.54,75.62,18.23;14,416.45,381.68,109.20,18.23" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,105.51,705.16,449.79,8.74;13,56.69,717.12,115.71,8.74">Facet or Subtopic Recall (Macro-Averaged Over 348 Subtopics of 34 Topics) @ aR+b for the Athome4 (Jeb Bush) Test Collection</title>
		<idno>SFSU-Run2 IMS-ASR IMS-BM25S IMS-BM25 IMS-ASRE catres-manual1 eDiscoveryTeam-Run1</idno>
	</analytic>
	<monogr>
		<title level="j" coord="13,56.69,705.16,28.12,8.74">Figure</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">UW-Target UW-Target-Desc BMI BMI-Desc SFSU-Run1</note>
</biblStruct>

<biblStruct coords="14,56.69,456.45,498.61,8.74;14,56.69,468.40,498.61,8.74;14,107.02,509.90,18.40,8.74;14,326.21,503.92,37.77,8.74;14,141.03,515.87,7.33,8.74;14,163.97,515.87,388.84,8.74;14,105.23,527.83,45.83,8.74;14,172.62,527.83,12.73,8.74;14,217.09,527.83,12.73,8.74;14,253.87,527.83,12.73,8.74;14,290.65,527.83,12.73,8.74;14,340.10,527.83,12.73,8.74;14,379.37,527.83,12.73,8.74;14,416.15,527.83,12.73,8.74;14,464.45,527.80,14.64,8.77;14,519.51,527.83,17.71,8.74;14,81.52,539.78,69.54,8.74;14,172.62,539.78,12.73,8.74;14,217.09,539.78,12.73,8.74;14,252.91,539.75,14.64,8.77;14,290.65,539.78,12.73,8.74;14,340.10,539.78,12.73,8.74;14,379.37,539.78,12.73,8.74;14,415.19,539.75,14.64,8.77;14,464.45,539.75,14.64,8.77;14,519.51,539.78,17.71,8.74;14,99.48,551.74,51.58,8.74;14,172.62,551.74,12.73,8.74;14,217.09,551.74,12.73,8.74;14,253.87,551.74,12.73,8.74;14,290.65,551.74,12.73,8.74;14,340.10,551.74,12.73,8.74;14,379.37,551.74,12.73,8.74;14,416.15,551.74,12.73,8.74;14,465.40,551.74,12.73,8.74;14,519.51,551.74,17.71,8.74;14,78.58,563.67,73.43,8.77;14,172.62,563.70,12.73,8.74;14,217.09,563.70,12.73,8.74;14,253.87,563.70,12.73,8.74;14,290.65,563.70,12.73,8.74;14,340.10,563.70,12.73,8.74;14,379.37,563.70,12.73,8.74;14,416.15,563.70,12.73,8.74;14,465.40,563.70,12.73,8.74;14,519.51,563.70,17.71,8.74;14,87.56,575.65,63.50,8.74;14,172.62,575.65,12.73,8.74;14,217.09,575.65,12.73,8.74;14,253.87,575.65,12.73,8.74;14,290.65,575.65,12.73,8.74;14,340.10,575.65,12.73,8.74;14,379.37,575.65,12.73,8.74;14,416.15,575.65,12.73,8.74;14,465.40,575.65,12.73,8.74;14,519.51,575.65,17.71,8.74;14,91.21,587.61,59.84,8.74;14,172.62,587.61,12.73,8.74;14,217.09,587.61,12.73,8.74;14,253.87,587.61,12.73,8.74;14,290.65,587.61,12.73,8.74;14,340.10,587.61,12.73,8.74;14,379.37,587.61,12.73,8.74;14,416.15,587.61,12.73,8.74;14,465.40,587.61,12.73,8.74;14,519.51,587.61,17.71,8.74;14,93.68,599.56,57.38,8.74;14,172.62,599.56,12.73,8.74;14,217.09,599.56,12.73,8.74;14,253.87,599.56,12.73,8.74;14,290.65,599.56,12.73,8.74;14,340.10,599.56,12.73,8.74;14,379.37,599.56,12.73,8.74;14,416.15,599.56,12.73,8.74;14,465.40,599.56,12.73,8.74;14,519.51,599.56,17.71,8.74;14,74.55,611.52,76.50,8.74;14,172.62,611.52,12.73,8.74;14,217.09,611.52,12.73,8.74;14,253.87,611.52,12.73,8.74;14,290.65,611.52,12.73,8.74;14,340.10,611.52,12.73,8.74;14,379.37,611.52,12.73,8.74;14,416.15,611.52,12.73,8.74;14,465.40,611.52,12.73,8.74;14,519.51,611.52,17.71,8.74;14,85.04,623.47,66.02,8.74;14,172.62,623.47,12.73,8.74;14,217.09,623.47,12.73,8.74;14,253.87,623.47,12.73,8.74;14,290.65,623.47,12.73,8.74;14,340.10,623.47,12.73,8.74;14,378.42,623.44,14.64,8.77;14,415.19,623.44,14.64,8.77;14,465.40,623.47,12.73,8.74;14,519.51,623.47,17.71,8.74;14,66.24,635.43,84.82,8.74;14,171.66,635.40,14.64,8.77;14,216.13,635.40,14.65,8.77;14,252.92,635.40,14.64,8.77;14,289.69,635.40,14.64,8.77;14,339.15,635.40,14.64,8.77;14,379.37,635.43,12.73,8.74;14,416.15,635.43,12.73,8.74;14,465.40,635.43,12.73,8.74;14,519.51,635.43,17.71,8.74;14,69.06,647.38,82.00,8.74;14,172.62,647.38,12.73,8.74;14,217.09,647.38,12.73,8.74;14,253.87,647.38,12.73,8.74;14,290.65,647.38,12.73,8.74;14,340.10,647.38,12.73,8.74;14,379.37,647.38,12.73,8.74;14,416.15,647.38,12.73,8.74;14,465.40,647.38,12.73,8.74;14,519.51,647.38,17.71,8.74;14,62.67,659.34,88.39,8.74;14,172.62,659.34,12.73,8.74;14,217.09,659.34,12.73,8.74;14,253.87,659.34,12.73,8.74;14,290.65,659.34,12.73,8.74;14,340.10,659.34,12.73,8.74;14,379.37,659.34,12.73,8.74;14,416.15,659.34,12.73,8.74;14,465.40,659.34,12.73,8.74;14,519.51,659.34,17.71,8.74;14,90.15,671.29,60.91,8.74;14,172.62,671.29,12.73,8.74;14,217.09,671.29,12.73,8.74;14,253.87,671.29,12.73,8.74;14,290.65,671.29,12.73,8.74;14,340.10,671.29,12.73,8.74;14,379.37,671.29,12.73,8.74;14,416.15,671.29,12.73,8.74;14,465.40,671.29,12.73,8.74;14,519.51,671.29,17.71,8.74;14,83.76,683.25,67.30,8.74;14,172.62,683.25,12.73,8.74;14,217.09,683.25,12.73,8.74;14,253.87,683.25,12.73,8.74;14,290.65,683.25,12.73,8.74;14,340.10,683.25,12.73,8.74;14,379.37,683.25,12.73,8.74;14,416.15,683.25,12.73,8.74;14,465.40,683.25,12.73,8.74;14,519.51,683.25,17.71,8.74;14,56.69,705.16,498.62,8.74;14,56.69,717.12,46.49,8.74;15,115.18,518.86,18.40,8.74;15,334.37,512.89,37.77,8.74;15,149.19,524.84,7.33,8.74;15,172.14,524.84,388.84,8.74;15,113.40,536.80,45.83,8.74;15,179.83,536.77,14.64,8.77;15,224.29,536.77,14.65,8.77;15,262.03,536.80,12.73,8.74;15,297.86,536.77,14.64,8.77;15,347.31,536.77,14.64,8.77;15,387.53,536.80,12.73,8.74;15,424.31,536.80,12.73,8.74;15,472.61,536.77,14.64,8.77;15,527.67,536.80,17.71,8.74;15,89.68,548.75,61.06,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,487.63,468.40,63.44,8.74">Test Collection</title>
		<idno>BMI .47 .55 .71 .62 .64 .74 .70 .70 .76 .733 BMI-Desc</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,107.02,509.90,18.40,8.74;14,326.21,503.92,37.77,8.74;14,141.03,515.87,7.33,8.74;14,163.97,515.87,30.03,8.74">Run Recall @ R R+100</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>Figure 13: Gain Curves Showing Recall According to the Majority Vote of the Three Secondary Assessors (Averaged Over 34 Topics) as a Function of the Number of Submitted Documents, for the Athome4 (Jeb Bush. Recall @ aR+b for the Majority Vote of Three Secondary Assessors, for the Athome4 (Jeb Bush. Test Collection. Run Recall @ R R+100 R+1000 2R 2R+100 2R+1000 4R 4R+100 4R+1000 Reasonable</note>
</biblStruct>

<biblStruct coords="15,56.69,714.13,498.61,8.74;16,115.18,518.86,18.40,8.74;16,334.37,512.89,37.77,8.74;16,149.19,524.84,7.33,8.74;16,172.14,524.84,388.84,8.74;16,113.40,536.80,45.83,8.74;16,180.78,536.80,12.73,8.74;16,224.30,536.77,14.64,8.77;16,262.03,536.80,12.73,8.74;16,298.81,536.80,12.73,8.74;16,348.26,536.80,12.73,8.74;16,387.53,536.80,12.73,8.74;16,424.31,536.80,12.73,8.74;16,473.57,536.80,12.73,8.74;16,527.67,536.80,17.71,8.74;16,89.68,548.75,69.54,8.74;16,180.78,548.75,12.73,8.74;16,224.30,548.72,14.64,8.77;16,262.03,548.75,12.73,8.74;16,298.81,548.75,12.73,8.74;16,347.31,548.72,14.64,8.77;16,386.58,548.72,14.64,8.77;16,423.36,548.72,14.64,8.77;16,472.61,548.72,14.64,8.77;16,527.67,548.75,17.71,8.74;16,107.64,560.71,51.58,8.74;16,180.78,560.71,12.73,8.74;16,225.25,560.71,12.73,8.74;16,262.03,560.71,12.73,8.74;16,298.81,560.71,12.73,8.74;16,348.26,560.71,12.73,8.74;16,387.54,560.71,12.73,8.74;16,424.32,560.71,12.73,8.74;16,473.57,560.71,12.73,8.74;16,527.67,560.71,17.71,8.74;16,62.67,572.63,97.51,8.77;16,180.78,572.66,12.73,8.74;16,225.25,572.66,12.73,8.74;16,261.08,572.63,14.64,8.77;16,298.81,572.66,12.73,8.74;16,348.26,572.66,12.73,8.74;16,387.53,572.66,12.73,8.74;16,424.31,572.66,12.73,8.74;16,473.57,572.66,12.73,8.74;16,527.67,572.66,17.71,8.74;16,95.72,584.62,63.50,8.74;16,180.78,584.62,12.73,8.74;16,225.25,584.62,12.73,8.74;16,262.03,584.62,12.73,8.74;16,298.81,584.62,12.73,8.74;16,348.26,584.62,12.73,8.74;16,387.54,584.62,12.73,8.74;16,424.32,584.62,12.73,8.74;16,473.57,584.62,12.73,8.74;16,527.67,584.62,17.71,8.74;16,99.38,596.57,59.85,8.74;16,180.78,596.57,12.73,8.74;16,225.25,596.57,12.73,8.74;16,262.03,596.57,12.73,8.74;16,298.81,596.57,12.73,8.74;16,348.26,596.57,12.73,8.74;16,387.54,596.57,12.73,8.74;16,424.32,596.57,12.73,8.74;16,473.57,596.57,12.73,8.74;16,527.67,596.57,17.71,8.74;16,101.84,608.53,57.38,8.74;16,180.78,608.53,12.73,8.74;16,225.25,608.53,12.73,8.74;16,262.03,608.53,12.73,8.74;16,298.81,608.53,12.73,8.74;16,348.26,608.53,12.73,8.74;16,387.54,608.53,12.73,8.74;16,424.32,608.53,12.73,8.74;16,473.57,608.53,12.73,8.74;16,527.67,608.53,17.71,8.74;16,82.72,620.48,76.50,8.74;16,180.78,620.48,12.73,8.74;16,225.25,620.48,12.73,8.74;16,262.03,620.48,12.73,8.74;16,298.81,620.48,12.73,8.74;16,348.26,620.48,12.73,8.74;16,387.54,620.48,12.73,8.74;16,424.32,620.48,12.73,8.74;16,473.57,620.48,12.73,8.74;16,527.67,620.48,17.71,8.74;16,93.21,632.44,66.02,8.74;16,180.78,632.44,12.73,8.74;16,225.25,632.44,12.73,8.74;16,262.03,632.44,12.73,8.74;16,298.81,632.44,12.73,8.74;16,348.26,632.44,12.73,8.74;16,386.58,632.41,14.64,8.77;16,424.31,632.44,12.73,8.74;16,473.57,632.44,12.73,8.74;16,527.67,632.44,17.71,8.74;16,74.40,644.39,84.82,8.74;16,179.83,644.36,14.64,8.77;16,225.25,644.39,12.73,8.74;16,261.08,644.36,14.64,8.77;16,297.85,644.36,14.64,8.77;16,348.26,644.39,12.73,8.74;16,387.53,644.39,12.73,8.74;16,424.31,644.39,12.73,8.74;16,473.57,644.39,12.73,8.74;16,527.67,644.39,17.71,8.74;16,77.23,656.35,82.00,8.74;16,180.78,656.35,12.73,8.74;16,225.25,656.35,12.73,8.74;16,261.08,656.32,14.64,8.77;16,298.81,656.35,12.73,8.74;16,348.26,656.35,12.73,8.74;16,387.53,656.35,12.73,8.74;16,424.31,656.35,12.73,8.74;16,473.57,656.35,12.73,8.74;16,527.67,656.35,17.71,8.74;16,70.83,668.30,88.39,8.74;16,180.78,668.30,12.73,8.74;16,225.25,668.30,12.73,8.74;16,262.03,668.30,12.73,8.74;16,298.81,668.30,12.73,8.74;16,348.26,668.30,12.73,8.74;16,387.54,668.30,12.73,8.74;16,424.32,668.30,12.73,8.74;16,473.57,668.30,12.73,8.74;16,527.67,668.30,17.71,8.74;16,98.32,680.26,60.91,8.74;16,180.78,680.26,12.73,8.74;16,225.25,680.26,12.73,8.74;16,262.03,680.26,12.73,8.74;16,298.81,680.26,12.73,8.74;16,348.26,680.26,12.73,8.74;16,387.54,680.26,12.73,8.74;16,424.32,680.26,12.73,8.74;16,473.57,680.26,12.73,8.74;16,527.67,680.26,17.71,8.74;16,91.92,692.21,67.30,8.74;16,180.78,692.21,12.73,8.74;16,225.25,692.21,12.73,8.74;16,262.03,692.21,12.73,8.74;16,298.81,692.21,12.73,8.74;16,348.26,692.21,12.73,8.74;16,387.54,692.21,12.73,8.74;16,424.32,692.21,12.73,8.74;16,473.57,692.21,12.73,8.74;16,527.67,692.21,17.71,8.74;16,56.69,714.13,498.61,8.74;17,416.45,229.10,51.66,18.23;17,416.45,243.24,78.81,18.23;17,416.45,257.39,17.77,18.23;17,416.45,271.53,44.93,18.23;17,416.45,285.67,53.37,18.23;17,416.45,299.81,53.37,18.23;17,416.45,313.96,40.18,18.23;17,416.45,328.10,54.08,18.23;17,416.45,342.24,47.95,18.23;17,416.45,356.38,46.27,18.23;17,416.45,370.53,75.62,18.23;17,416.45,384.67,109.20,18.23" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,62.67,572.63,97.51,8.77;16,180.78,572.66,12.73,8.74;16,225.25,572.66,12.73,8.74;16,261.08,572.63,14.64,8.77;16,298.81,572.66,12.73,8.74;16,348.26,572.66,12.73,8.74;16,387.53,572.66,12.73,8.74;16,424.31,572.66,12.73,8.74;16,473.57,572.66,12.73,8.74;16,527.67,572.66,17.71,8.74;16,95.72,584.62,37.46,8.74">eDiscoveryTeam .51 .57 .63 .61 .61 .64 .64 .64 .65 .579 ims base</title>
		<idno>UW-Target UW-Target-Desc BMI BMI-Desc SFSU-Run1 SFSU-Run2 IMS-ASR IMS-BM25S IMS-BM25 IMS-ASRE catres-manual1 eDiscoveryTeam-Run1</idno>
	</analytic>
	<monogr>
		<title level="m" coord="15,487.25,714.13,68.05,8.74;16,115.18,518.86,18.40,8.74;16,334.37,512.89,37.77,8.74;16,149.19,524.84,7.33,8.74;16,172.14,524.84,30.03,8.74">Test Collection. Run Recall @ R R+100</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>Figure. Recall @ aR+b for the Second of Three Secondary Assessors, for the Athome4 (Jeb Bush. Test Collection</note>
</biblStruct>

<biblStruct coords="17,56.69,459.44,498.61,8.74" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="17,56.69,459.44,412.39,8.74">Figure 19: Gain Curves Showing Recall According to the Third of Three Secondary Assessors</title>
		<imprint>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,56.69,471.39,474.11,8.74;17,115.18,518.86,18.40,8.74;17,334.37,512.89,37.77,8.74;17,149.19,524.84,7.33,8.74;17,172.14,524.84,388.84,8.74;17,113.40,536.80,45.83,8.74;17,180.78,536.80,12.73,8.74;17,225.25,536.80,12.73,8.74;17,262.03,536.80,12.73,8.74;17,298.81,536.80,12.73,8.74;17,348.26,536.80,12.73,8.74;17,387.54,536.80,12.73,8.74;17,424.32,536.80,12.73,8.74;17,472.61,536.77,14.64,8.77;17,527.67,536.80,17.71,8.74;17,89.68,548.75,69.54,8.74;17,180.78,548.75,12.73,8.74;17,225.25,548.75,12.73,8.74;17,262.03,548.75,12.73,8.74;17,298.81,548.75,12.73,8.74;17,348.26,548.75,12.73,8.74;17,387.54,548.75,12.73,8.74;17,424.32,548.75,12.73,8.74;17,472.61,548.72,14.64,8.77;17,527.67,548.75,17.71,8.74;17,107.64,560.71,51.58,8.74;17,180.78,560.71,12.73,8.74;17,225.25,560.71,12.73,8.74;17,262.03,560.71,12.73,8.74;17,298.81,560.71,12.73,8.74;17,348.26,560.71,12.73,8.74;17,387.54,560.71,12.73,8.74;17,424.32,560.71,12.73,8.74;17,473.57,560.71,12.73,8.74;17,527.67,560.71,17.71,8.74;17,62.67,572.63,97.51,8.77;17,180.78,572.66,12.73,8.74;17,225.25,572.66,12.73,8.74;17,262.03,572.66,12.73,8.74;17,298.81,572.66,12.73,8.74;17,348.26,572.66,12.73,8.74;17,387.53,572.66,12.73,8.74;17,424.31,572.66,12.73,8.74;17,473.57,572.66,12.73,8.74;17,527.67,572.66,17.71,8.74;17,95.72,584.62,63.50,8.74;17,180.78,584.62,12.73,8.74;17,225.25,584.62,12.73,8.74;17,262.03,584.62,12.73,8.74;17,298.81,584.62,12.73,8.74;17,348.26,584.62,12.73,8.74;17,387.54,584.62,12.73,8.74;17,424.32,584.62,12.73,8.74;17,473.57,584.62,12.73,8.74;17,527.67,584.62,17.71,8.74;17,99.38,596.57,59.85,8.74;17,180.78,596.57,12.73,8.74;17,225.25,596.57,12.73,8.74;17,262.03,596.57,12.73,8.74;17,298.81,596.57,12.73,8.74;17,348.26,596.57,12.73,8.74;17,387.54,596.57,12.73,8.74;17,424.32,596.57,12.73,8.74;17,473.57,596.57,12.73,8.74;17,527.67,596.57,17.71,8.74;17,101.84,608.53,57.38,8.74;17,180.78,608.53,12.73,8.74;17,225.25,608.53,12.73,8.74;17,262.03,608.53,12.73,8.74;17,298.81,608.53,12.73,8.74;17,348.26,608.53,12.73,8.74;17,387.54,608.53,12.73,8.74;17,424.32,608.53,12.73,8.74;17,473.57,608.53,12.73,8.74;17,527.67,608.53,17.71,8.74;17,82.72,620.48,76.50,8.74;17,180.78,620.48,12.73,8.74;17,225.25,620.48,12.73,8.74;17,262.03,620.48,12.73,8.74;17,298.81,620.48,12.73,8.74;17,348.26,620.48,12.73,8.74;17,387.54,620.48,12.73,8.74;17,424.32,620.48,12.73,8.74;17,473.57,620.48,12.73,8.74;17,527.67,620.48,17.71,8.74;17,93.21,632.44,66.02,8.74;17,180.78,632.44,12.73,8.74;17,225.25,632.44,12.73,8.74;17,262.03,632.44,12.73,8.74;17,298.81,632.44,12.73,8.74;17,348.26,632.44,12.73,8.74;17,386.58,632.41,14.64,8.77;17,423.35,632.41,14.64,8.77;17,473.57,632.44,12.73,8.74;17,527.67,632.44,17.71,8.74;17,77.99,644.39,81.24,8.74;17,180.78,644.39,12.73,8.74;17,224.30,644.36,14.64,8.77;17,261.07,644.36,14.64,8.77;17,297.86,644.36,14.64,8.77;17,347.31,644.36,14.64,8.77;17,387.53,644.39,12.73,8.74;17,424.31,644.39,12.73,8.74;17,473.57,644.39,12.73,8.74;17,527.67,644.39,17.71,8.74;17,77.23,656.35,82.00,8.74;17,179.83,656.32,14.64,8.77;17,225.25,656.35,12.73,8.74;17,262.03,656.35,12.73,8.74;17,298.81,656.35,12.73,8.74;17,348.27,656.35,12.73,8.74;17,387.54,656.35,12.73,8.74;17,424.32,656.35,12.73,8.74;17,473.57,656.35,12.73,8.74;17,527.67,656.35,17.71,8.74;17,70.83,668.30,88.39,8.74;17,180.78,668.30,12.73,8.74;17,225.25,668.30,12.73,8.74;17,262.03,668.30,12.73,8.74;17,298.81,668.30,12.73,8.74;17,348.26,668.30,12.73,8.74;17,387.54,668.30,12.73,8.74;17,424.32,668.30,12.73,8.74;17,473.57,668.30,12.73,8.74;17,527.67,668.30,17.71,8.74;17,98.32,680.26,60.91,8.74;17,180.78,680.26,12.73,8.74;17,225.25,680.26,12.73,8.74;17,262.03,680.26,12.73,8.74;17,298.81,680.26,12.73,8.74;17,348.26,680.26,12.73,8.74;17,387.54,680.26,12.73,8.74;17,424.32,680.26,12.73,8.74;17,473.57,680.26,12.73,8.74;17,527.67,680.26,17.71,8.74;17,91.92,692.21,67.30,8.74;17,180.78,692.21,12.73,8.74;17,225.25,692.21,12.73,8.74;17,262.03,692.21,12.73,8.74;17,298.81,692.21,12.73,8.74;17,348.26,692.21,12.73,8.74;17,387.54,692.21,12.73,8.74;17,424.32,692.21,12.73,8.74;17,473.57,692.21,12.73,8.74;17,527.67,692.21,17.71,8.74;17,56.69,714.13,498.61,8.74" xml:id="b17">
	<monogr>
		<idno>R+1000 2R 2R+100 2R+1000 4R 4R+100 4R+1000 Reasonable BMI .42 .51 .65 .57 .61 .69 .66 .67 .74 .699 BMI-Desc .44 .53 .65 .58 .61 .71 .70 .70 .74 .668 catres .37 .46 .59 .54 .57 .63 .61 .62 .65 .599 eDiscoveryTeam .50 .54 .56 .55 .56 .59 .61 .61 .61 .587 ims base .11 .12 .21 .19 .19 .22 .22 .23 .24 .187 ims exp .15 .18 .24 .20 .22 .27 .26 .26 .30 .458 ims rot .16 .19 .25 .21 .22 .27 .27 .28 .32 .654 ims smooth .25 .28 .42 .34 .36 .44 .40 .42 .46 .404 sfsu run1 .45 .54 .68 .58 .62 .72 .69 .71 .73 .728 sfsu run2exp .45 .55 .69 .60 .63 .73 .66 .70 .74 .735 uw.desc.knee .45 .56 .65 .58 .62 .69 .67 .68 .69 .692 uw.desc.target .05 .06 .06 .08 .08 .12 .13 .13 .18 .655 uw.knee .40 .49 .60 .53 .59 .66 .62 .63 .66 .692 uw.target .03 .03 .08 .10 .11 .13 .14 .14 .17 .712</idno>
		<title level="m" coord="17,462.39,471.39,68.41,8.74;17,115.18,518.86,18.40,8.74;17,334.37,512.89,37.77,8.74;17,149.19,524.84,7.33,8.74;17,172.14,524.84,30.03,8.74">Test Collection. Run Recall @ R R+100</title>
		<imprint/>
	</monogr>
	<note>as a Function of the Number of Submitted Documents, for the Athome4 (Jeb Bush. Figure 20: Recall @ aR+b for the Third of Three Secondary Assessors, for the Athome4 (Jeb Bush. Test Collection</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
