<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,170.16,72.44,269.37,16.59;1,130.32,92.36,348.97,16.59">Searching and Filtering Tweets: CSIRO at the TREC 2012 Microblog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.52,140.70,81.38,11.06"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Engineering Laboratory CSIRO ICT Centre</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.80,140.70,35.57,11.06"><forename type="first">Jie</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Engineering Laboratory CSIRO ICT Centre</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,339.24,140.70,69.82,11.06"><forename type="first">Paul</forename><surname>Thomas</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Engineering Laboratory CSIRO ICT Centre</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,170.16,72.44,269.37,16.59;1,130.32,92.36,348.97,16.59">Searching and Filtering Tweets: CSIRO at the TREC 2012 Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D0FE5A7D8F283B1584D25246054259D1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report on the participation of the CSIRO 1 team in the TREC 2012 Microblog Track. We participated with four automatic runs for the adhoc search task and four automatic runs for the filtering task. In the adhoc search task, we experiment with different pre-processing and query expansion techniques. Our most important finding is highlighting the value of systematic pre-processing of tweets and its impact on improving the effectiveness of search. In the filtering task, we apply different feature extraction and classification techniques. We demonstrate the potential of using SVM classifiers for filtering tweets for a given topic.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">MICROBLOG TRACK</head><p>The Microblog Track was introduced in 2011 to TREC to encourage the information retrieval community to explore techniques for searching and filtering information in microblogs, specifically Twitter. The search task is designed as a typical search scenario where users are interested to see recent relevant tweets for their time-stamped queries. The filtering task on the other hand has double-timestamped queries to specify a time span that started from the last time that the user queried for this topic, till when the latest tweet was tweeted. It assumes that the user has already seen the old tweets prior to the previous tweet-time. An example query for both tasks is shown in Figure <ref type="figure" coords="1,191.88,481.98,3.56,8.66">1</ref>. In this figure, &lt;query-time&gt; and &lt;querytweettime&gt; can be used interchangeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic MB010</head><p>Query with one time-stamp: &lt;title&gt; Egyptian protesters attack museum &lt;/title&gt; &lt;querytime&gt; Sat Jan 29 20:06:35 +0000 2011 &lt;/querytime&gt; &lt;querytweettime&gt; 31443107291598848 &lt;/querytweettime&gt; Query with double time-stamp: &lt;title&gt; Egyptian protesters attack museum &lt;/title&gt; &lt;querytime&gt; Sat Jan 29 20:06:35 +0000 2011 &lt;/querytime&gt; &lt;querytweettime&gt; 30354903104749568 &lt;/querytweettime&gt; &lt;querynewesttweet&gt; 31443107291598848 &lt;/querynewesttweet&gt; Figure <ref type="figure" coords="1,86.52,612.44,4.11,8.52">1</ref>: A query that shows the difference between adhoc (above) and filtering (below) tasks. Tweetid is used to imply tweeting time, with bigger tweetid indicating a more recent tweet.</p><p>Last year, a tweet corpus called Tweets2011 was developed for this track which included two weeks' worth of sampled tweets -from 24th of January till 8th of February-comprising approximately 16 million tweets <ref type="bibr" coords="1,473.76,241.14,9.12,8.66" target="#b6">[6]</ref>. The same corpus was used for both of the adhoc and filtering tasks in 2012. In 2011 fifty queries, and in 2012 a new set of 60 queries accompanied the tweet corpus for the adhoc task. The filtering task was based on the 2011 queries. These queries however were double-timestamped (as shown in Figure <ref type="figure" coords="1,498.00,293.46,3.57,8.66">1</ref>), from which a subset of 39 queries were used for testing and the rest for training. Since we did not participate last year, we report our experiments using both sets of queries, where applicable, for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">REAL-TIME ADHOC TASK</head><p>As mentioned earlier, the goal of the adhoc search task is given a query accompanied by a timestamp to find all the recent relevant tweets. It is therefore different from traditional adhoc search for its emphasis on recency of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tweet Corpus Processing</head><p>Previous reports in TREC 2011 reported mixed results with different tweet pre-processing techniques, mostly claiming negative effects. However, most of these works only reported the outcome of applying more than one technique and did not explore what the effect of each individual step was. Hence, to fully explore the effect of pre-processing on retrieval effectiveness, we performed a number of preprocessing steps that led to five different variations of the original set of downloaded tweets, as listed in Table <ref type="table" coords="1,520.08,522.54,4.60,8.66" target="#tab_0">1</ref> and Table 2. Given the requirements of the task for not including retweets and non-English tweets, we first removed tweets with null content, and retweets (tweets starting with RT) for all these datasets. We then removed non-English tweets using an off-the-shelf tool called langid.py <ref type="bibr" coords="1,495.36,574.86,9.12,8.66" target="#b3">[3]</ref>. A dataset that only used this basic level of pre-processing is called D0. These basic text processing steps reduced the size of the collection dramatically, with D0 only containing 5, 371, 776 tweets from the original crawl of 15, 414, 586 (July 2011).</p><p>Other text processing strategies we investigated were a shallow lexical normalisation, mention removal, link removal, emotion removal, and short-tweet elimination (Table <ref type="table" coords="1,528.48,648.18,3.57,8.66" target="#tab_0">1</ref>). We hypothesised that such elements in tweets may not be useful for an adhoc search task, for example because sentiment of a tweet may not be important in identifying topical relevance of the tweet. For lexical normalisation, we only considered words that were emphasised by repeating one or more letters. If a letter was repeated more than three times -we do not know any legitimate English word that contains more than three repetitions of a letter-it was normalised to one instance of that letter. This process can introduce some errors, for example the word sweet is often emphasised in Twitter by repeating the letter e, which will be reduced to swet in our process. We leave further improvement on this for future work.</p><p>Emotional expressions using smilies, emoticons, or some of the Internet slang which convey emotions such as lol or xoxo could also increase noise in a tweet. We investigated the effect of removing these words from the tweet as well. Mentions represent username in the form of @username, for example @CSIROnews or @DaFemaleBiebzy11. Some of these mentions reveal the name or nature of the Twitter user, and some are random names people came up with. In our experiments we show the effect of keeping or removing these elements from the tweets. <ref type="foot" coords="2,156.72,223.13,3.65,5.37" target="#foot_2">2</ref>Statistics on the size of each of the datasets created based on the above steps can be found in Table <ref type="table" coords="2,235.20,245.58,3.56,8.66" target="#tab_2">3</ref>. These preprocessing steps could potentially eliminate some of the relevant tweets from the dataset and consequently the index. We calculated the number of relevant tweets remained in the datasets, and the number (and percentage) of relevant tweets missing from each dataset. Our original crawl already misses 6% of the relevant tweets in 2011 track and 4% in 2012, which means we could never retrieve those tweets even with a perfect system. Language detection almost doubles the percentage of the missing relevant tweets (10% for 2011 and 9% for 2012). This suggests that the tool we used makes mistakes in tagging English tweets as non-English. Other steps cause more elimination of relevants but their deteriorating effect is in the order of one or two percent. Our evaluation results are therefore directly affected by losses in pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indexing and Retrieval</head><p>We used Indri search engine <ref type="bibr" coords="2,167.88,437.46,14.20,8.66" target="#b11">[11]</ref> with its default settings for indexing and retrieval in all our experiments. We compiled a set of stopwords suitable for Twitter content ourselves. <ref type="foot" coords="2,278.16,456.89,3.65,5.37" target="#foot_3">3</ref> It includes formal English stopwords such as is, am, informal English stopwordes such as aint, gonna, and Twitter specific stopwords such as RT that indicates a retweet. Note we had already removed re-tweets prior to indexing and RT was only added to the list because it can also occur inside a tweet. For example in this tweet Certainly Is! :( RT @NSWRFS: Today's #bushfire at Wyee -a reminder that fire season is well and truly here. #wyeefire http://instagr.am/p/QW3oJ5kH_W/ RT comes in the middle of the text and therefore this tweet is not removed from the original corpus.</p><p>We also included a list of common swear words, assuming they do not convey any information useful for the TREC tasks. We used the Krovetz stemmer <ref type="bibr" coords="2,211.20,641.94,9.64,8.66" target="#b2">[2]</ref> in all our experiments. We included hashtags as a searchable field via Indri's parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hashtag Expansion (run csiroQEll112). Hashtags in a</head><p>tweet can act as explicit markers of topics, as noted for example by Diaz-Aviles et al. <ref type="bibr" coords="2,417.36,104.70,9.12,8.66" target="#b1">[1]</ref>. Motivated by this observation, we used hashtags for a simple form of pseudo-relevance feedback. A first round of retrieval used the query as-is to return a set of results R1; a second round of retrieval used the same query but added in hashtags from R1 to produce R2. This second set formed our submission.</p><p>Tweets were indexed using Indri, with hashtags in a separate field. Set R1 was retrieved from indri using this index, with stemming and stopping as above.</p><p>Each hashtag h in each of the top k tweets in R1 was weighted in the conventional manner <ref type="bibr" coords="2,467.88,209.22,9.12,8.66" target="#b9">[9]</ref>:</p><formula xml:id="formula_0" coords="2,316.80,234.42,234.52,23.96">w h = top k tweets tf k1((1 -b) + b dl avgdl ) + tf log N -n + 0.5 n + 0.5</formula><p>where tf is the term frequency of hashtag h in each tweet, dl is the length of each tweet, avgdl is the mean length of the top tweets, N is the number of tweets, and n is the number of tweets with hashtag h. The top-weighted H hashtags were selected and added to the original query with weight β.</p><p>Re-running this query gave our final R2.</p><p>Experiments with the 2011 queries and relevance judgements suggested best performance with k = 10 (ten tweets used for feedback), H = 3 (up to three hashtags added), and β = 0.25 (hashtags have one quarter the weight of the original query). The usual values k1 = 1.2 and b = 0.75 were used in weighting terms.</p><p>In total, we added hashtags to 49 of the 60 queries (in 11 cases there were no hashtags in the top-10 tweets). Table <ref type="table" coords="2,332.76,412.98,4.60,8.66" target="#tab_3">4</ref> gives some examples. In many cases, hashtags add terms that are relevant to the topic but not in the original query (e.g. MB057, MB066, MB069, MB078); however in some cases the connection is tenuous, or relates to a different topic (e.g. MB059, MB092) and in several cases the hashtags simply recapitulate the query text (e.g. MB099).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Expansion (run csiroQEt112</head><p>). Run csiroQEt112 used pseudo-relevance feedback as above, but candidates were drawn from all terms in the tweet (not just hashtags). All queries were expanded with three terms drawn from the top-10 tweets, and expanded terms were added to the original query with multiplier β = 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Recognition (run csiroNE112).</head><p>We used a Twitterspecific named-entity recogniser <ref type="bibr" coords="2,451.92,572.94,9.64,8.66" target="#b7">[7]</ref> to find interesting entities such as names of movie, locations, or people in the queries. Run csiroNE112 used pseudo-relevance feedback over named entities (only), in the same manner as our other runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Evaluation</head><p>We evaluate our approaches to the adhoc search task in two stages. We first evaluate the effect of tweet corpus preprocessing in vanilla runs (no query expansion), and then assess the effect of query expansion. Table <ref type="table" coords="2,495.84,679.50,4.60,8.66" target="#tab_4">5</ref> shows the effect of different pre-processing approaches on tweets before indexing. For simplicity, these runs are named after their corresponding datasets explained in Table <ref type="table" coords="2,491.76,710.94,3.56,8.66" target="#tab_1">2</ref>. We observed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What How</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lang. Detection</head><p>Removed all non-English tweets using langid.py.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emotion Removal</head><p>Removed all the common words that convey feelings such as lol, haha, or xoxo, and emoticons such as :-), ☼, or , and Japanese emoticons such as (&gt;_&lt;).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical Normalisation</head><p>If a character or was repeated four or more times in a tweet, it was reduced to one. For example, yesssssssss was turned to yes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention Removal</head><p>All the Twitter username mentions (i.e. @username) were removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Removal</head><p>All the web-links (e.g. http://t.co/sOMqSVXq) in the tweets were removed. Short-tweet Elimination Any tweet that that contained less than four words, after removing punctuations, was eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retweet Elimination</head><p>Any tweet started with RT followed by a username was eliminated. that apart from D0, the rest of these datasets led to similar results in terms of three measures: mean average precision (MAP), precision at cut-off 30 (P@30), and recall of preferences (R-Pref). Note these runs are based on the entire corpus, therefore they use future evidence and cannot be categorised as real-time runs. TREC 2011 relied on P@30 to rank the submissions. We ran a statistical significance test (paired t-test) over 2011 results to compare these baseline runs. D0 was significantly (p-value&lt;0.05) inferior to all other runs for all the metrics. D4 was significantly (p-value&lt;0.05) better than all other runs in terms of MAP score.</p><p>In Table <ref type="table" coords="4,100.80,418.02,3.56,8.66" target="#tab_5">6</ref>, we compare a baseline run based on dataset D4 with official results of best-performing real-time run from last year (isiFDL) from University of Southern California <ref type="bibr" coords="4,281.88,438.90,9.12,8.66" target="#b4">[4]</ref>, highest performing non-real-time run (ri) from Kobe University <ref type="bibr" coords="4,83.76,459.78,9.12,8.66">[5]</ref>, median and best results to show where a solely pre-processing based baseline would stand if we had participated in TREC 2011. These numbers are reported for relevance judgements based on all-relevant tweets. Note that best is a synthetic run composed of the top score for each query from all submissions Our simple run (D4) is well above median of the submitted runs for last year. Comparing it to the official ranking of all the participated teams <ref type="bibr" coords="4,164.28,543.54,9.12,8.66" target="#b6">[6]</ref>, surprisingly, its MAP score places this run at third rank, and P@30 would rank it 14th. This simple comparison gives us confidence that our preprocessing steps are useful.</p><p>In all the following experiments including our submitted runs we used an index based on D4 dataset, which means all the non-English tweets were removed, and within tweets words indicating emotions, emoticons, and also links to web pages were omitted. We submitted a real-time run(csiroR112) in which we used the same method of creating D4 but we created a separate index for each topic removing all the tweets with a timestamp after the tweettime. This way, we did not include any future evidence in our run.</p><p>We compared our query expansion strategies with a baseline run in using topics from the 2011 and 2012 tracks (Table <ref type="table" coords="4,69.24,700.38,3.57,8.66" target="#tab_6">7</ref>). These results are based on all-relevant judgements. A query expansion strategy (csiroQEt112) that used all the query terms (hashtags and non-hashtags) performs better than all our other runs for TREC 2011 on all the metrics, and performs best based on P@30 and R-Prec in TREC 2012.</p><p>For TREC 2012 queries, csiroQEll112 marginally wins over csiroNE112.</p><p>Table <ref type="table" coords="4,351.72,244.38,4.60,8.66" target="#tab_7">8</ref> shows a comparison of our submitted runs with TREC official results released for the best, median, and worst synthetic aggregated runs submitted for this task. These results are based on highly relevant judgements. Our best run in terms of MAP score was csiroQEt112 which was based on pseudo-relevance feedback query expansion using all the terms in the tweets. This run was near median for MAP and above median using R-Prec measure. Using hashtags for query expansion (csiroQEll112) led to our highest score using P@30, but it was poor in MAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">REAL-TIME FILTERING TASK</head><p>An important aspect of Twitter is its near real-time stream of tweets, which makes it a candidate application for information filtering. Traditionally, filtering techniques develop user profiles that distinguish between relevant and irrelevant texts in an incoming stream <ref type="bibr" coords="4,433.08,416.70,9.12,8.66" target="#b8">[8]</ref>. The 2012 Microblog Track introduced an adaptive filtering task where at the beginning only a small number of positive or relevant tweets are available -only one for this task-and the system should gradually learn to create a more accurate profile for a given topic.</p><p>We submitted four runs for this task, of which one was based on traditional pseudo-relevance feedback and three were based on two different types of classifiers: support vector machines (SVM) and logistic regression. We used the same pre-processed corpus (D4) that we used as baseline for the adhoc runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevance Feedback for Filtering</head><p>We submitted one run based on traditional relevance feedback as a baseline. That is, we wanted to know how well a plain retrieval approach would work compared to a more sophisticated machine learning approach. We first retrieved an initial ranked list for each query, and then to simulate judgements by a user, we expanded the queries using the provided judgements of the top 5 retrieved tweets. Obviously, if there was no relevant tweet retrieved in the original retrieved tweets, the query remained unexpanded. If the query was expanded, we then retrieved a new set of tweets that satisfied the time limits given in each query. Only the top 10 newly retrieved tweets were considered as final relevant tweets to construct a run. This run is called csiro-QERF111. Although in adaptive filtering this process of getting feedback from the user continues, we did not iterate  this process any further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification-based Filtering</head><p>Our other three runs cast the filtering task as a binary classification problem. Given a topic, we use the entire corpus prior to the querytweettime as training data, and build a classifier using positive and negative examples. The classifier is thereafter applied on each tweet from querytweettime to querynewesttweet to determine whether or not the tweet is relevant to the topic. However, a main difficulty lies in that, we only have one positive example -querytweet -available for training at the beginning. Thus, we focused on gradually enriching positive examples for training a classifier. For each topic, we first retrieved an initial ranked list of tweets using Indri, from which we examined the top 5 tweets and those tweets judged as relevant were used as positive examples.</p><p>Besides that, another 10 tweets posted before querytweettime were taken as negative examples. Together positive and negative examples comprised the labelled training data for building a classifier for a given query. This process is illustrated in Figure <ref type="figure" coords="5,132.24,500.34,3.56,8.66" target="#fig_0">2</ref>. Using the training data, we trained a classifier based on SVM or logistic regression. When applied on new tweets, the classifier not only generated a retrieval decision about whether each tweet is relevant to the given topic, but also returned the probability of being classified to be positive as the relevance score. Details of feature extraction and the corresponding submitted runs are given below.</p><p>Feature Extraction. In order to train the classifier, we extracted three types of features for each tweet: n-grams generated on the text of the tweet, hashtags that the tweet uses, and whether or not the tweet contains terms from the query text. Using these features, tweets were converted into feature vectors upon which the classifier was built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tweet Classification</head><p>We submitted three runs using the classifiers. They are detailed as follows:</p><p>• csiroSVMqe111: An initial ranked list was retrieved using Indri with expanded queries similar to the baseline run (csiroQERF111). The top 5 judgements were used as positive examples to train an SVM classifier. The probability of being classified as positive was returned as the score.</p><p>• csiroshuq111: An initial ranked list was retrieved using Indri, from which the top 5 judgements were used as positive examples to train an SVM classifier. The probability of being classified to be positive was returned as the score.</p><p>• csirolrhuq111: It used the same approach to construct the training data as above, except that it used logistic regression as the classifier. The score returned for each tweet was the probability of being relevant to a specific topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filtering Evaluation</head><p>The filtering task is evaluated using two main metrics: F0.5 and T 11SU <ref type="bibr" coords="5,366.96,356.10,9.12,8.66" target="#b8">[8]</ref>. F0.5 is defined based on F β measure</p><formula xml:id="formula_1" coords="5,360.72,376.13,58.60,10.71">F β = (1 + β 2 ).</formula><p>precision.recall (β 2 .precision) + recall , where β = 0.5. T11SU or scaled linear utility measure is defined as</p><formula xml:id="formula_2" coords="5,343.44,425.46,185.92,20.54">T 11SU = max(T 11N U, M inN U ) -M inN U 1 -M inN U ,</formula><p>where</p><formula xml:id="formula_3" coords="5,384.36,465.30,103.96,20.54">T 11N U = 2 × T P -F P 2 × N .</formula><p>T P or true positive rate is the number of relevant tweets retrieved, F P or false positive is the total number of irrelevant tweets retrieved, N is total number of relevant documents, and M inN U is the minimum negative utility that a user would tolerate (e.g. -0.5). Two other measures, precision and recall, are also reported to help understand F -measure.</p><p>Table <ref type="table" coords="5,352.32,564.42,4.60,8.66" target="#tab_8">9</ref> reports the effectiveness of our submitted runs compared to the best, median, and worst results provided by TREC based on all the submissions. Except for csiroL-RHUQ111 that is based on a logistic regression classifier, the other three runs are above median for both main measures (F0.5 and T 11SS). Among our four submitted runs, csiroQERF111, relevance-feedback run, achieves the best F -measure at 0.2631 mainly because of its high precision at 0.4781. csiroQERF111 also achieves T 11SS at 0.3448, which ranks seventh according to the sorted T 11SS scores among all the filtering runs provided by TREC, as reported in the Overview paper <ref type="bibr" coords="5,412.92,679.50,13.37,8.66" target="#b10">[10]</ref>. On the other hand, the effectiveness of the other three runs, based on tweet classification, varies for different topics, with some queries benefiting greatly from a classifier-based approach and the others To gain a better understanding on how our classificationbased runs rely on the initial retrieval using Indri, we compare precision at cut-off 5 (P@5) of an initial ranked list (no feedback) and precision gained by csiroSHUQ111. We observe that when the original run retrieves more than two relevant tweets in the top 5 results (two positive examples), the classifier tends to achieve a high precision. We also calculate Pearson's correlation of per-query precision for these two runs. For no feedback run, correlation between P@5 and precision was 0.4249 (p = 0.0078), and for csiroSHUQ111, correlation between P@5 and F0.5 was 0.5935 (p &lt; 0.0001). This suggests a weak positive correlation between the number of positive examples and effectiveness of the classifier for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>We reported our experiments in the TREC Microblog Track for both search and filtering tasks. In the adhoc task we experimented methods of processing tweets before indexing to reduce non-content words or words such as expressions of emotions or links to web pages. Our initial experiments show that these simple heuristics indeed lead to a promising baseline for this task. However, given the very short length of tweets, care should be taken to apply these heuristics because of their potential of removing relevant tweet from the index. It also worth noting that an application can determine what pre-processing would be required.</p><p>We also reported our experiments on the filtering task, where given a query and one relevant tweet we are to retrieve recent tweets relevant to that query. We investigated using a simple relevance feedback baseline, and two different classifiers (SVM and logistic regression). The effectiveness of our runs were all above median for the main evaluation metrics except for logistic regression which performed poorly. In the future we intend to explore how adaptive filtering can be more effectively implemented using tweet classifiers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,53.76,249.68,239.04,8.52;5,53.76,260.12,77.58,8.52;5,53.86,155.09,240.71,80.37"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of constructing training data for classification.</figDesc><graphic coords="5,53.86,155.09,240.71,80.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,103.92,167.84,401.91,133.24"><head>Table 1 :</head><label>1</label><figDesc>Tweet pre-processing heuristics.</figDesc><table coords="3,103.92,238.38,401.91,62.70"><row><cell>Dataset Lang. Detect. Emotion Removal Lexical Norm. Mention Removal Link Removal</cell></row><row><cell>D0</cell></row><row><cell>D1</cell></row><row><cell>D2</cell></row><row><cell>D3</cell></row><row><cell>D4</cell></row><row><cell>D5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.40,313.16,502.66,164.28"><head>Table 2 :</head><label>2</label><figDesc>Specification of the pre-processed datasets indexed for the baseline runs. A in a column means the corresponding dataset was pre-processed for the specified text processing heuristic.</figDesc><table coords="3,158.88,394.14,291.94,83.30"><row><cell></cell><cell></cell><cell></cell><cell cols="2">TREC 2011</cell><cell></cell><cell cols="2">TREC 2012</cell></row><row><cell>Dataset</cell><cell># tweets</cell><cell cols="2">#rs #mrels</cell><cell cols="3">(%) #rels #mrels</cell><cell>(%)</cell></row><row><cell cols="3">Original 15 414 586 2784</cell><cell>181</cell><cell>(6.1)</cell><cell>6018</cell><cell>268</cell><cell>(4.3)</cell></row><row><cell>D0</cell><cell cols="2">5 371 776 2666</cell><cell>299</cell><cell>(10.1)</cell><cell>5695</cell><cell>591</cell><cell>(9.4)</cell></row><row><cell>D1</cell><cell cols="2">4 304 454 2621</cell><cell>344</cell><cell>(11.6)</cell><cell>5624</cell><cell>662</cell><cell>(10.5)</cell></row><row><cell>D2</cell><cell cols="2">4 342 660 2633</cell><cell>332</cell><cell>(11.2)</cell><cell>5661</cell><cell>625</cell><cell>(9.9)</cell></row><row><cell>D3</cell><cell cols="2">4 340 852 2633</cell><cell>332</cell><cell>(11.2)</cell><cell>5662</cell><cell>624</cell><cell>(9.9)</cell></row><row><cell>D4</cell><cell cols="2">4 439 881 2622</cell><cell>343</cell><cell>(11.6)</cell><cell>5631</cell><cell>655</cell><cell>(10.4)</cell></row><row><cell>D5</cell><cell cols="2">4 473 471 2634</cell><cell>331</cell><cell>(11.2)</cell><cell>5666</cell><cell>620</cell><cell>(9.9)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.40,489.20,502.85,185.28"><head>Table 3 :</head><label>3</label><figDesc>Size of the original dataset downloaded from Twitter, and processed datasets, with the total number of relevant tweets (#rels) they include, the number of relevant tweets missing (#mrels) and the percentage of the missing relevants over total number of existing relevants in brackets. Total number of existing relevants was 2965 for TREC 2011, and 6286 for TREC 2012.</figDesc><table coords="3,155.28,592.14,299.22,82.34"><row><cell>Topic Original query</cell><cell>Hashtags added</cell></row><row><cell>MB057 chicago blizzard</cell><cell>#blizzard, #chicago, #smomg</cell></row><row><cell>MB059 glen beck</cell><cell>#mostannoyingtvpeople</cell></row><row><cell cols="2">MB066 journalists treatment in eygpt #eygpt, #media, #jan25</cell></row><row><cell>MB069 high taxes</cell><cell>#taxsupport, #sotu</cell></row><row><cell>MB078 mcdonalds food</cell><cell>#ronald, #mcdonald, #healthy</cell></row><row><cell>MB092 stock market tutorial</cell><cell>#photoshop, #tutorial</cell></row><row><cell>MB099 superbowl commercials</cell><cell>#superbowl</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,124.08,686.84,361.62,8.52"><head>Table 4 :</head><label>4</label><figDesc>Examples of hashtag-based query expansion. Hashtags are stemmed.</figDesc><table coords="4,56.76,55.14,237.01,71.90"><row><cell></cell><cell>TREC 2011</cell><cell>TREC 2012</cell></row><row><cell>Dataset</cell><cell>MAP P@30 R-Prec</cell><cell>MAP P@30 R-Prec</cell></row><row><cell>D0</cell><cell cols="2">0.2888 0.3408 0.3459 0.1925 0.2949 0.2423</cell></row><row><cell>D1</cell><cell cols="2">0.3024 0.3619 0.3570 0.2056 0.3085 0.2645</cell></row><row><cell>D2</cell><cell cols="2">0.2949 0.3565 0.3555 0.2006 0.3023 0.2540</cell></row><row><cell>D3</cell><cell cols="2">0.2950 0.3565 0.3555 0.2007 0.3023 0.2540</cell></row><row><cell>D4</cell><cell cols="2">0.3032 0.3592 0.3570 0.2062 0.3090 0.2613</cell></row><row><cell>D5</cell><cell cols="2">0.2955 0.3558 0.3537 0.2009 0.3073 0.2557</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,53.40,138.92,239.58,89.88"><head>Table 5 :</head><label>5</label><figDesc>Effect of dataset pre-processing for two sets of queries (2011 and 2012). Tweets and queries were stopped and stemmed.</figDesc><table coords="4,62.88,192.78,220.87,36.02"><row><cell></cell><cell></cell><cell></cell><cell>Run</cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell>D4</cell><cell>isiFDL</cell><cell>ri</cell><cell>Best</cell><cell>Median</cell></row><row><cell>P@30</cell><cell>0.3592</cell><cell>0.4551</cell><cell cols="2">0.4265 0.6116</cell><cell>0.2575</cell></row><row><cell>MAP</cell><cell>0.3032</cell><cell>0.1923</cell><cell cols="2">0.2227 0.5127</cell><cell>0.1426</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,53.40,240.68,240.90,29.40"><head>Table 6 :</head><label>6</label><figDesc>Comparison of one of our baseline runs with top-performing real-time and non-real-time runs, median and best in TREC 2011.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,316.44,54.91,239.65,107.53"><head>Table 7 :</head><label>7</label><figDesc>Effect of a vanilla run on D4 index (both realtime and non-real-time), and query expansion methods based on the submitted runs for two sets of queries (2011 and 2012).</figDesc><table coords="4,322.32,54.91,228.18,55.07"><row><cell></cell><cell>TREC 2011</cell><cell>TREC 2012</cell></row><row><cell>Run</cell><cell>MAP P@30 R-Prec</cell><cell>MAP P@30 R-Prec</cell></row><row><cell>D4</cell><cell>0.3032 0.3592 0.3570</cell><cell>0.2062 0.3090 0.2613</cell></row><row><cell>csiroR112</cell><cell>0.2860 0.3469 0.3319</cell><cell>0.1542 0.1324 0.1675</cell></row><row><cell>csiroQEll112</cell><cell>0.2943 0.3463 0.3521</cell><cell>0.1616 0.1393 0.1767</cell></row><row><cell>csiroQEt112</cell><cell>0.3108 0.3639 0.3603</cell><cell>0.1537 0.1445 0.1896</cell></row><row><cell>csiroNE112</cell><cell>0.2945 0.3408 0.3476</cell><cell>0.1605 0.1363 0.1770</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,53.40,54.18,502.69,75.74"><head>Table 8 :</head><label>8</label><figDesc>Comparison of CSIRO TREC 2012 adhoc search submitted runs with official results of best, median and worst submissions.</figDesc><table coords="5,116.28,54.18,376.96,45.02"><row><cell></cell><cell></cell><cell cols="2">Submitted Runs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell cols="4">csiroR112 csiroQEll112 csiroQEt112 csiroNE112</cell><cell>Best</cell><cell cols="2">Median Worst</cell></row><row><cell>P30</cell><cell>0.1542</cell><cell>0.1616</cell><cell>0.1537</cell><cell>0.1605</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MAP</cell><cell>0.1324</cell><cell>0.1393</cell><cell>0.1445</cell><cell>0.1363</cell><cell>0.4144</cell><cell>0.1486</cell><cell>0.0099</cell></row><row><cell>R-Prec</cell><cell>0.1675</cell><cell>0.1767</cell><cell>0.1896</cell><cell>0.1770</cell><cell>0.4473</cell><cell>0.1869</cell><cell>0.0131</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,53.40,54.18,504.18,196.82"><head>Table 9 :</head><label>9</label><figDesc>Comparison of CSIRO TREC 2012 filtering submitted runs with mean of the official best, median, and worst results reported for all the test queries by all participating teams. the results returned by Indri, but it does not help with the accuracy. csirolrhuq111 using logistic regression performs worse than two SVM-based runs and only leads to high recall value compared to the median.</figDesc><table coords="6,53.76,54.18,475.96,165.38"><row><cell></cell><cell></cell><cell cols="2">Submitted Runs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell cols="4">csiroQERF111 csiroSVMQE111 csiroLRHUQ111 csiroSHUQ111</cell><cell>Best</cell><cell cols="2">Median Worst</cell></row><row><cell>F0.5</cell><cell>0.2631</cell><cell>0.1580</cell><cell>0.0809</cell><cell>0.1594</cell><cell>0.6074</cell><cell>0.1491</cell><cell>0.0000</cell></row><row><cell>T11SU</cell><cell>0.3448</cell><cell>0.3227</cell><cell>0.0980</cell><cell>0.2971</cell><cell>0.5967</cell><cell>0.2076</cell><cell>0.0000</cell></row><row><cell>Precision</cell><cell>0.4781</cell><cell>0.1953</cell><cell>0.0821</cell><cell>0.2688</cell><cell>0.9224</cell><cell>0.1767</cell><cell>0.0000</cell></row><row><cell>Recall</cell><cell>0.1427</cell><cell>0.2217</cell><cell>0.3431</cell><cell>0.2294</cell><cell>0.9463</cell><cell>0.3343</cell><cell>0.0002</cell></row><row><cell cols="3">not. Our csiroshuq111 run that is based on an SVM clas-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">sifier achieves precision at 0.2688, recall at 0.2294, and F -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">measure at 0.1594, which outperforms the other two runs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">csiroSVMqe111 uses query expansion to perform relevance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>feedback based on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,58.44,671.58,235.99,8.66;1,53.76,680.46,23.95,8.66;1,53.76,714.08,22.39,7.17"><p>Commonwealth Scientific and Industrial Research OrganisationTREC,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2012" xml:id="foot_1" coords="1,78.12,714.08,51.26,7.17"><p>November 2012   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="2,58.44,682.62,237.51,8.66;2,53.76,692.46,129.40,7.62"><p>Our pre-processing script is available at https://github. com/skarimi/SKTwitter-Tools.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="2,58.44,701.94,237.51,8.66;2,53.76,711.78,126.87,7.62"><p>Our list of stop-words can be found in: https://github. com/skarimi/SKTwitter-Tools</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,58.25,651.59,96.71,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.60,669.06,181.59,8.66;6,72.60,679.50,206.64,8.66;6,72.60,689.94,201.15,8.66;6,71.88,700.38,201.28,8.26;6,72.36,710.94,20.80,8.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,72.60,679.50,206.64,8.66;6,72.60,689.94,166.43,8.66">Exploiting social #-tagging behavior in Twitter for information filtering and recommendation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Diaz-Aviles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Siehndel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">D</forename><surname>Naini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,258.24,689.94,15.51,8.26;6,71.88,700.38,197.26,8.26">The Twentieth Text REtrieval Conference Proceedings</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,169.02,195.02,8.66;6,335.64,179.58,168.67,8.66;6,335.64,190.02,219.98,8.26;6,335.64,200.46,212.44,8.66;6,335.64,210.90,87.52,8.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,385.08,169.02,145.58,8.66;6,335.64,179.58,27.54,8.66">Viewing morphology as an inference process</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,381.72,179.58,122.59,8.26;6,335.64,190.02,219.98,8.26;6,335.64,200.46,146.02,8.26">Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 16th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,222.42,205.14,8.66;6,335.64,232.86,220.33,8.66;6,335.16,243.30,222.04,8.66;6,335.64,253.74,50.44,8.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,462.58,222.42,78.20,8.66;6,335.64,232.86,108.15,8.66">py: An off-the-shelf language identification tool</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Langid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,461.88,232.86,94.09,8.26;6,335.16,243.30,114.36,8.26">Proceedings of the ACL 2012 System Demonstrations</title>
		<meeting>the ACL 2012 System Demonstrations<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,265.26,195.28,8.66;6,335.64,275.70,201.67,8.66;6,334.92,286.14,119.92,8.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,431.40,265.26,99.52,8.66;6,335.64,275.70,62.43,8.66">USC/ISI at TREC 2011: Microblog track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,416.40,275.70,120.91,8.26;6,334.92,286.14,91.78,8.26">The Twentieth Text REtrieval Conference Proceedings</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,297.54,193.51,8.66;6,335.64,308.10,212.67,8.66;6,335.64,318.54,212.23,8.66;6,334.92,328.98,119.92,8.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,382.92,308.10,165.39,8.66;6,335.64,318.54,73.19,8.66">TREC 2011 microblog track experiments at Kobe university</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miyanishi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Okamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,426.96,318.54,120.91,8.26;6,334.92,328.98,91.78,8.26">The Twentieth Text REtrieval Conference Proceedings</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,340.38,197.30,8.66;6,335.28,350.94,210.30,8.66;6,335.28,361.38,161.32,8.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,468.84,340.38,64.10,8.66;6,335.28,350.94,112.59,8.66">Overview of the TREC 2011 microblog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,466.20,350.94,79.38,8.26;6,335.28,361.38,133.18,8.26">The Twentieth Text REtrieval Conference Proceedings</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,372.78,214.03,8.66;6,335.64,383.22,210.76,8.66;6,335.64,393.78,208.39,8.66;6,335.16,404.22,190.83,8.66;6,335.16,414.66,135.40,8.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,521.28,372.78,28.39,8.66;6,335.64,383.22,206.71,8.66">Named entity recognition in tweets: An experimental study</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,347.04,393.78,196.99,8.26;6,335.16,404.22,161.52,8.26">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,426.06,187.48,8.66;6,335.64,436.62,214.87,8.66;6,334.92,447.06,119.92,8.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,456.48,426.06,66.64,8.66;6,335.64,436.62,81.32,8.66">The TREC 2002 filtering track report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,435.36,436.62,115.15,8.26;6,334.92,447.06,91.78,8.26">The Eleventh Text REtrieval Conference Proceedings</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,458.46,195.15,8.66;6,335.28,468.90,220.62,8.66;6,335.04,479.46,220.73,8.66;6,335.64,489.90,93.76,8.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,548.52,468.90,7.38,8.66;6,335.04,479.46,197.58,8.26">of Foundations and Trends in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,463.08,458.46,67.71,8.26;6,335.28,468.90,166.20,8.26">The Probabilistic Relevance Framework: BM25 and Beyond</title>
		<meeting><address><addrLine>Delft</addrLine></address></meeting>
		<imprint>
			<publisher>now Publishers</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,501.30,197.30,8.66;6,335.28,511.74,202.02,8.66;6,334.92,522.30,182.08,8.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,468.84,501.30,64.10,8.66;6,335.28,511.74,112.59,8.66">Overview of the TREC-2012 microblog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,466.20,511.74,71.10,8.26;6,334.92,522.30,153.94,8.26">The Twenty-First Text REtrieval Conference Proceedings</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,533.70,217.72,8.66;6,335.64,544.14,195.12,8.66;6,335.64,554.58,208.75,8.66;6,334.92,565.14,169.72,8.66" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,335.64,544.14,195.12,8.66;6,335.64,554.58,62.70,8.66">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,416.76,554.58,127.63,8.26;6,334.92,565.14,141.82,8.26">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
