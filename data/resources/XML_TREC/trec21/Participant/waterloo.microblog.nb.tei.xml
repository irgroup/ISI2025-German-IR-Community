<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.64,72.35,388.44,16.84;1,186.32,92.27,237.07,16.84">Frequent Itemset Mining for Query Expansion in Microblog Ad-hoc Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.21,137.97,99.35,11.06"><forename type="first">Younos</forename><surname>Aboulnaga</surname></persName>
							<email>yaboulna@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<addrLine>200 University Avenue</addrLine>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.30,137.97,107.07,11.06"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<email>claclark@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<addrLine>200 University Avenue</addrLine>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.64,72.35,388.44,16.84;1,186.32,92.27,237.07,16.84">Frequent Itemset Mining for Query Expansion in Microblog Ad-hoc Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5E0C84ACE0934E9C09E46D1947852855</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The high volume of Tweets arriving every second and the requirement to index them in real time emphasize the importance of the computational complexity of algorithms used to process them. In this paper, we investigate the use of Frequent Itemsets Mining to quickly discover patterns that can later be used for query expansion. Frequent Itemsets Mining (FIM) has been highly adopted to mine data streams because of its computational simplicity and the possibility to parallelize some of its steps. Initial experiments using the TREC 2011 Microblogs track queries showed that it is possible to improve the performance of BM25, however this was not the case with the 2012 queries. Our analysis of the difference in performance provides insight about how to make best use of FIM for microblog search.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The short length of Tweets is a challenge to many of the algorithms developed for searching other corpora, but it also makes it possible to apply other algorithms developed for other forms of data. The computational simplicity of Frequent Itemsets Mining (FIM) makes it an attractive option in situations where the processing time is important, such as real-time ad-hoc search. It is also straightforward to model Tweets as Itemsets, since the mean number of terms in a Tweet is small. In the TREC 2012 Microblogs track's dataset, Tweets written in Latin characters contained 9.637 terms on average. Besides the algorithmic convenience, itemsets also fit the Bag of Words model which neglects the order of terms. In this paper we use itemsets mined from the whole dataset to expand queries, and experiment with different techniques for selecting expansion terms.</p><p>The rest of the paper is organized as follows. Initially we describe the indexing process in section 2, and we explain our choice of baseline in section 3. Then we cover the Frequent Itemsets Mining process in section 4. In section 5 we give the details of how we use the mined itemsets for query expansion, and in section 6 we report the performance of our methods on the TREC topics. Finally, we conclude by section 7 where we also highlight our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">INDEX</head><p>We use Lucene 3.6<ref type="foot" coords="1,404.26,227.76,3.65,5.24" target="#foot_0">1</ref> as an inverted index. Our tokenizer keeps only Latin characters (Unicode code points less than 'u024F'), numbers, and the characters {'@', '#', ' '}. All URLs are replaced by the token 'URL', and runs of the same character is reduced to only 3 repetitions (for example, 'coooool' is replaced by 'coool'). Hashtags are stored twice, with and without the the '#' character. All tokens are stored both in their original form, and after applying Lucene's implementation of the Porter stemmer. We use the original form to select the subset of documents to score, and the stemmed form while scoring. This avoids the disadvantage that stemming retrieves irrelevant documents, while taking advantage of it to count words about the same concept towards the same term frequency. While words sharing the same stem can mean totally different concepts in the context of different documents, it is very likely that they have the same meaning within one document, and this meaning is probably the one desired if the document is retrieved using the unstemmed query term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BASELINE</head><p>The baseline on which we improve uses the BM25 ranking function used in <ref type="bibr" coords="1,384.60,465.48,9.20,7.86" target="#b2">[2]</ref>:</p><formula xml:id="formula_0" coords="1,393.84,497.74,162.08,32.86">BM 25 d = t qt.idft, idft = log( N -df t +0.5 df t +0.5 )<label>(1)</label></formula><p>This is analogous to the Okapi BM25 equation with k1 set to 0 to annihilate the effect of term frequency, so a Tweet with one occurrence of a query term is as good as one with many. The short length of Tweets restricts the term frequency to numbers close to unity, and there is no reason to believe that a Tweet with two or three occurrences is more relevant than a Tweet with one occurrence. Actually Tweets with many occurrences are likely to be spam, or totally uninformative. Moreover, setting k1 to 0 also annihilates the effect of document length normalization, favouring long Tweets that make most use of the allowed length. It is noteworthy that we had arrived at this formula by looking for the best values for k1 and b using a grid search. We varied the values of b from 0 to 1 in increments of 0.1 and the value of k1 from 0 to 100, and tested the performance of Okapi BM25 on the TREC 2011 Microblog track qrels. There are other values that performed nearly as well or even slightly better, but the intuitive explanation of the selected values make them less likely to be over fitting the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FREQUENT ITEMSETS MINING</head><p>The FIM algorithm we use is a slightly modified version of Mahout's 2 implementation of PFP <ref type="bibr" coords="2,194.01,127.21,9.20,7.86" target="#b5">[5]</ref>, a Map/Reduce adaptation of the FP-Growth algorithm <ref type="bibr" coords="2,198.28,137.67,9.20,7.86" target="#b3">[3]</ref>. The FP-Growth algorithm is scalable to a large number of vocabulary terms (items), because it skips the candidate generation step which is the bottle neck of Apriori <ref type="bibr" coords="2,171.77,169.05,9.72,7.86" target="#b1">[1]</ref> like algorithms. By taking advantage of the parallelization capabilities of Map/Reduce, it is possible to speed up the mining process almost arbitrarily because the overhead of adding more nodes is subtle, as reported in <ref type="bibr" coords="2,103.07,210.89,9.20,7.86" target="#b5">[5]</ref>. In our experiments we used only one machine, a quad-core AMD Phenom PC with 8 GB of RAM. Even on such modest machine, we could always get the results of mining a window of any length in less time than the window length. The reported runs do not respect the real time requirement because the Frequent Itemsets used for expansion are mined from the whole dataset. To simulate using the itemsets available at a certain point of time we first need to adapt the mining algorithm for a streaming environment. Parallel FIM algorithms that work on streams are mostly Apriori like because the FP-Tree structure is hard to update, but this is not our main area of interest.</p><p>The PFP algorithm avoids running out of memory by keeping only a certain amount of itemsets for each item, selecting the ones with the highest support. We modify it by using measures that go beyond merely counting and take into account the semantic relations between terms. The results reported use itemsets mined with support as the measure in intermediate stages, but finally when selecting which itemsets to keep for each term we use the Normalized Mutual Information (NMI) of the whole itemset I with the head term h:</p><formula xml:id="formula_1" coords="2,106.30,465.29,186.61,19.31">N M I(I, h) = t∈I p(t,h) ln p(t,h) p(t)p(h) -t∈I p(t,h) ln p(t,h)<label>(2)</label></formula><p>The probabilities required for calculating NMI are easily estimated from the FP-Tree. The FP-Tree is a prefix tree in which the count at each node represent the number of times the term in this node occurs in itemsets along with items in higher levels. The structure also keeps links between nodes of the same term. Therefore to estimate the joint probability of two terms it is easy to get all paths between them, then sum the counts at lower level nodes. This is an estimate not an exact calculation because subtrees for which the root's count is less than the support are pruned. The tree is also kept within a certain size limit by increasing the support kept in the tree every time it reaches the size limit. If support is used to select the final set of itemsets then this increase of support in intermediate stages only prunes itemsets that wouldn't be selected for the final result anyway, according to <ref type="bibr" coords="2,141.33,652.30,9.20,7.86" target="#b5">[5]</ref>. To avoid ending up with patterns including only terms that have very high support, such as stop words, we remove the top 1 percentile of terms.</p><p>Mutual Information is a better measure than support for the final selection since it is a measure of association. Fre-2 http://mahout.apache.org/ quent Itemsets Mining is usually a preliminary stage for associate rules mining which uses many measures other than support, such as confidence, lift and Pearson's correlation to name a few. We chose Normalized Mutual Information because of its suitability for use with natural languages since it is robust against low frequency data. It is also a measure whose value is bounded even for itemsets with different lengths. In our work in progress we are empirically evaluating the properties of different other measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">QUERY EXPANSION</head><p>To use the mined frequent itemsets for query expansion, they are first indexed just like Tweets. The itemsets corpus is searched with the original query, using BM25 for ranking. A few terms are selected from the itemsets result set, using one of a variety of methods. In later subsections, we describe the two best performing expansion term selection techniques, as well as a "control" Pseudo Relevance Feedback technique. The number of expansion terms that worked best with the TREC 2011 qrels is 10 expansion terms for each query term. The weight of the expansion terms are set so that their total weight is equal to the total weight of the original query, thus reducing the effect of concept drift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Terms from most relevant itemsets</head><p>Expansion terms are added from patterns with the highest BM25 scores, until the required number of distinct expansion terms are added. This is the most straightforward method yet it is the most effective one. Other methods were attempts to improve on this simple BM25 relevance ranking, but none of them could achieve better performance. By using BM25 for retrieving relevant itemsets we balance between adding expansion terms that increase diversity and ones that increase specificity of the query. More diversity is achieved by using itemsets that satisfy the disjunction of the original query, but this causes severe concept drift. On the other hand, using a conjunction of the original query leads to filtering the results to very specific Tweets -the ones from which the itemsets were mined -leading to a result set with at least minimum support repetitions of very similar Tweets. The IDF weighting of terms in BM25 strikes a good balance for selecting itemsets, just like it does for Tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Terms from clusters of itemsets</head><p>We attempted a variety of methods to select expansion terms by using a matrix of itemset to term features, where each row represents an itemset and each column represents a term. Instead of using binary indicators of presence of a term in an itemset, we use a value similar to the one used in <ref type="bibr" coords="2,316.81,575.27,9.20,7.86" target="#b8">[8]</ref>. The value in cells of a column is the change in the IDF of the column's term when calculated on the corpus of relevant itemsets from that calculated on the whole corpus of Tweets. The value is also scaled by the smoothed probability of the term in the Tweets corpus. This value gives more weight to terms whose IDF has decreased in the relevant itemsets corpus compared to that in the Tweets corpus, but scales this weight by the term's probability. This value is actually the negative of the KL-divergence between the probability of the term in the Tweets corpus and the relevant itemsets corpus. The performance of several algorithms improved by using this elaborate value. It is given by the following equation:</p><formula xml:id="formula_2" coords="3,105.09,71.51,187.81,12.90">-KLD = (ln Nc df t ,c -ln Nr df t,r ) * pt,c<label>(3)</label></formula><p>This itemset to terms matrix was used as an input to many algorithms for selecting the important terms. We attempted Singular Value Decomposition (SVD) as in <ref type="bibr" coords="3,224.38,114.63,9.20,7.86" target="#b8">[8]</ref>, Markov chain word translation model <ref type="bibr" coords="3,148.06,125.09,9.20,7.86" target="#b4">[4]</ref>, PageRank and clustering. Terms chosen by clustering itemsets caused the biggest improvement in performance, and it is also the easiest method to explain. It simply tries to expand the query by terms from different possible concepts which are assumed to be represented by clusters of itemsets in the relevant result set.</p><p>We use the XMeans <ref type="bibr" coords="3,145.70,187.86,9.71,7.86" target="#b6">[6]</ref> implementation in Weka 3 to cluster itemsets according to the matrix given above. The matrix is standardized to avoid any unexpected behaviour because of great variation in the values in the matrix. Then expansion terms are selected by taking the term closest to each cluster centroid in turn, then the second closest and so on. The distance of a term from a cluster centroid is the mean of the distances of itemsets in which it is a member.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Terms for clusters of Tweets</head><p>This is the same as expansion using terms from clusters of itemsets, but using Tweets relevant to the original query instead of itemsets; that is, using Pseudo Relevance Feedback (PRF). This is meant as a "control" run to assess whether itemsets mining is useful. Better performance is sometimes achieved using PRF, however the expansion terms from the itemsets makes more sense than those from the relevant Tweets. Tables <ref type="table" coords="3,121.69,365.70,4.61,7.86">1</ref> and<ref type="table" coords="3,149.16,365.70,4.61,7.86" target="#tab_1">2</ref> show the expansion terms of the first 5 topics using both methods. The topics' queries are given in appendix A. Terms from the original query are marked by an asterisk in the expanded query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>Table <ref type="table" coords="3,89.83,433.95,4.61,7.86" target="#tab_0">3</ref> shows Precision at 30 (P@30), Mean Average Precision (MAP) and Reciprocal Precision (R-Prec) for the base line (BM25) and for the different expansion methods, nFromTop described in section 5.1, clusterFIS described in section 5.2 and clusterTwts described in section 5.3. Performance is reported on both the TREC 2011 and 2012 topics, using all relevant qrels for both sets of topics, as well as only highly relevant qrels for the 2012 topics. The effect of query expansion on the 2011 topics is positive, but it has virtually no effect on the 2012 topics. Out of the 60 topics of 2012, only topic no. 84 got improved and topics no. 87, 90 and 99 got worse. However on the 49 topic of 2011, our baseline is in the highest 10 percentile and the query expansion beats the best run reported in 2011. Following, we explore the possible causes for such a different performance.</p><p>One reason could be that the expansion with frequent itemsets work well on certain types of queries, like those which include a named entity for example. We performed an Analysis of Variance (ANOVA) of the performance across different categories of topics, based on the three categorizations in <ref type="bibr" coords="3,89.74,643.16,9.20,7.86" target="#b7">[7]</ref>. One-way ANOVA tests of the three categorizations don't show a significant variance of performance across different categories, neither do two-and three-way tests. Even though the categories are unbalanced, the normality of residuals was verified by QQ-plots and the homogeneity of variance was verified by the Levene test.</p><p>3 http://www.cs.waikato.ac.nz/ml/weka/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head><p>Expanded Query MB051 @britishexpat, government*, #fed, debt, #expats, cuts*, budgets, #government, fed, british*, @sion israel, #british, expats, #debt, #cuts MB052 outbreak, probes, bedbug*, contribute, @i kill termites, abating, upends, salts, h1n1, infestation, swine, @healthqd, paralysis, obesity, heart, epidemic*, #obesity, @n1hc, specialists, proportions, willpower, #heart MB053 superclasico, rcl, #condo, river*, riverboat, viking, web5, web3, web1, #superclasico, #michigan, #fishing, #nature, overlooking, supercl Gsico, fishing, #boat, news1, #park, plunges, athens, #lincoln, msc, itineraries, @kathsellshomes, condo, cruises*, @cruiseschedules, lincoln, getaways, boat*, park, nature, nile, @rcaribbeancruis MB054 19.99, oakley, 27.99, twitenna, of, @imalwayzsmacked, preisvergleich, #the, #2chmatome, #preisvergleich, steepandcheap, cigarmonster, the*, #and, #premier, @soccerts, @trends internet, 24.99, 2chmatome, #of, and, daily*, hooded, robusto, #twitenna, 39.99 MB055 shark, shari, dealcenter, blues, @andrew-pain1974, with, maqui, ezinearticles4u, #you, watchers, of, captin, @rayhattersley, #rhythm, almond, #lift, #white, rhythm, #blues, #science, loss*, you, #eating, #beauty, ambria, #with, weight*, diets, white, @puwisdom, science, #and, @pulistbook, lift, dingle, @miracleweight, @tweettraffic4u, #diets, oranges, #of, @aase25, and*, berries*, #no, navigator, @pubooks, eating, beauty, @music mattters</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: Expansion using Frequent Itemsets</head><p>Another reason could be that the weights given to the expansion terms were too low for them to have any effect. However, increasing the weights in further experiments, with the same setting of the official runs, caused severe concept drift. This leads us to believe that the actual reason is that the expansion terms are not really relevant to the query. Part of the reason behind this might be the way users employ hashtags, because most of the expansion terms are actually hashtags. The problem with hashtags is the versatility of their use across different topics and even languages. For example, the hashtag "#fishing" is used in many CKJ Tweets, and the hashtags "#obesity" and "#fit" are used in many Tweets not pertaining to the topic query. This can be overcome by selecting expansion terms that have stronger association with query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION AND FUTURE WORK</head><p>We have described our use of Frequent Itemsets for query expansion in the TREC 2012 microblog ad-hoc search task. We have seen promising results using the TREC 2011 topics, but the expansion had no effect on the TREC 2012 topics. We believe that the approach is promising and intend to improve it by using different measures for selecting the expansion terms from the mined Frequent Itemsets, and by mining Itemsets that yield strong association rules. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,59.68,56.49,505.42,331.31"><head>Table 3 :</head><label>3</label><figDesc>Performance on the 2012 and 2011 topics Topic Expanded Query MB051 cut, government*, you, fannie, cameron, on, of, stake, tax, for, down, eat, are, cuts*, out, british*, cutting, the, about, seek, s, to, i, just, a, be, spending, from, and, it, is, in, if MB052 of, bedbug*, that, health, its, are, my, this, the, t, by, to, i, new, a, spreading, from, an, and, it, is, in, epidemic*, all MB053 river*, with, travel, go, on, of, that, for, down, cruise, you, results, want, take, but, best, the, about, s, m, to, i, new, a, cruising, at, #cruise, and, cruises*, is, in, boat*, get MB054 your, go, of, stories, are, my, you, out, via, the*, every, today, to, i, a, gets, from, top, really, and, it, is, daily* MB055 benefits, with, acai, their, diet, on, of, wt, health, plans, ber, for, first, how, are, loss*, you, this, mt, lose, weight*, best, the, include, t, s, to, behind, new, a, precisely, really, workouts, and*, facts, berry, berries*, it, is, in, has, juice, will, what, effective, healthy, pills</figDesc><table coords="4,59.68,56.49,505.42,62.16"><row><cell>Method</cell><cell>P@30</cell><cell cols="4">2012 High Rel MAP R-Prec N &gt; median N ≥ median P@30</cell><cell>2012 All Rel MAP R-Prec</cell><cell>P@30</cell><cell>2011 All Rel MAP R-Prec</cell></row><row><cell>BM25</cell><cell cols="2">0.1944 0.1623 0.1926</cell><cell>20</cell><cell>43</cell><cell cols="2">0.3814 0.2442 0.2982</cell><cell>0.4177 0.3511 0.3968</cell></row><row><cell cols="3">nFromTop 0.1949 0.1657 0.1922</cell><cell>20</cell><cell>43</cell><cell cols="2">0.3814 0.2490 0.3010</cell><cell>0.4525 0.3764 0.4130</cell></row><row><cell cols="3">clusterFIS 0.1955 0.1646 0.1930</cell><cell>20</cell><cell>43</cell><cell cols="2">0.3819 0.2467 0.2991</cell><cell>0.4204 0.3501 0.3925</cell></row><row><cell cols="3">clusterTwt 0.1831 0.1620 0.1824</cell><cell>17</cell><cell>41</cell><cell cols="2">0.3650 0.2454 0.2933</cell><cell>0.4381 0.3596 0.4029</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,400.69,239.10,18.35"><head>Table 2 :</head><label>2</label><figDesc>Expansion using Pseudo Relevance Feedback</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,321.42,711.19,103.82,7.86"><p>http://lucene.apache.org/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="4,53.80,560.74,103.29,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,67.99,573.20,220.44,7.86;4,67.99,583.66,214.93,7.86;4,67.99,594.12,218.88,7.86;4,67.99,604.59,215.28,7.86;4,67.99,615.05,187.65,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,181.04,573.20,107.39,7.86;4,67.99,583.66,139.55,7.86">Fast algorithms for mining association rules in large databases</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,226.25,583.66,56.66,7.86;4,67.99,594.12,218.88,7.86;4,67.99,604.59,68.56,7.86">Proceedings of the 20th International Conference on Very Large Data Bases, VLDB &apos;94</title>
		<meeting>the 20th International Conference on Very Large Data Bases, VLDB &apos;94<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,67.99,626.50,197.48,7.86;4,67.99,636.96,221.46,7.86;4,67.99,647.42,212.34,7.86;4,67.99,657.89,183.44,7.86;4,67.99,668.35,190.06,7.86;4,67.99,678.81,64.47,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,234.19,636.96,55.26,7.86;4,67.99,647.42,101.56,7.86">Clarity at the trec 2011 microblog track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ohare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lanagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Phelan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,187.94,647.42,92.39,7.86;4,67.99,657.89,183.44,7.86;4,67.99,668.35,78.25,7.86">Text Retrieval Evaluation Conference (TREC)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">November 2011</date>
		</imprint>
	</monogr>
	<note>Proceedings of the 20th TREC Conference</note>
</biblStruct>

<biblStruct coords="4,67.99,690.26,212.68,7.86;4,67.99,700.72,194.26,7.86;4,67.99,711.19,89.20,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,179.84,690.26,100.84,7.86;4,67.99,700.72,116.63,7.86">Mining frequent patterns without candidate generation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,191.73,700.72,66.35,7.86">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,170.37,211.90,7.86;4,331.01,180.83,211.37,7.86;4,331.01,191.29,172.85,7.86;4,331.01,201.75,217.21,7.86;4,331.01,212.21,224.65,7.86;4,331.01,222.67,175.66,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,429.73,170.37,113.18,7.86;4,331.01,180.83,211.37,7.86;4,331.01,191.29,32.04,7.86">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,381.16,191.29,122.69,7.86;4,331.01,201.75,217.21,7.86;4,331.01,212.21,195.96,7.86">Proceedings of the 24th annual international ACM SIGIR conference on research and development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th annual international ACM SIGIR conference on research and development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,234.13,224.92,7.86;4,331.01,244.59,215.78,7.86;4,331.01,255.05,176.69,7.86;4,331.01,265.51,205.47,7.86;4,331.01,275.97,137.84,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,331.01,244.59,199.68,7.86">Pfp: parallel fp-growth for query recommendation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,331.01,255.05,176.69,7.86;4,331.01,265.51,138.97,7.86">Proceedings of the 2008 ACM conference on Recommender Systems, RecSys &apos;08</title>
		<meeting>the 2008 ACM conference on Recommender Systems, RecSys &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,287.43,199.39,7.86;4,331.01,297.89,203.40,7.86;4,331.01,308.35,224.91,7.86;4,331.01,318.81,205.46,7.86;4,331.01,329.27,197.55,7.86;4,331.01,339.73,105.07,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,447.37,287.43,83.03,7.86;4,331.01,297.89,203.40,7.86;4,331.01,308.35,29.21,7.86">X-means: Extending k-means with efficient estimation of the number of clusters</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,378.21,308.35,177.70,7.86;4,331.01,318.81,176.77,7.86">Proceedings of the Seventeenth International Conference on Machine Learning, ICML &apos;00</title>
		<meeting>the Seventeenth International Conference on Machine Learning, ICML &apos;00<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="727" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,351.19,203.36,7.86;4,331.01,361.65,205.76,7.86;4,331.01,372.11,222.50,7.86;4,331.01,382.57,204.95,7.86;4,331.01,393.03,49.13,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,448.29,361.65,88.48,7.86;4,331.01,372.11,75.74,7.86">Evaluating Real-Time Search over Tweets</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mccullough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,425.73,372.11,127.78,7.86;4,331.01,382.57,165.16,7.86">Proceedings of the International Conference on Weblogs and Social Media</title>
		<meeting>the International Conference on Weblogs and Social Media</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>ICWSM 2012</note>
</biblStruct>

<biblStruct coords="4,331.00,404.49,219.09,7.86;4,331.01,414.95,217.32,7.86;4,331.01,425.41,186.83,7.86;4,331.01,435.87,206.40,7.86;4,331.01,446.33,184.51,7.86;4,331.01,456.80,193.28,7.86;4,331.01,467.26,205.53,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,514.78,404.49,35.31,7.86;4,331.01,414.95,217.32,7.86;4,331.01,425.41,171.21,7.86">On selecting a good expansion set in pseudo-relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bhole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,331.01,435.87,206.40,7.86;4,331.01,446.33,184.51,7.86;4,331.01,456.80,164.60,7.86">Proceedings of the 2nd International Conference on Theory of Information Retrieval: Advances in Information Retrieval Theory, ICTIR &apos;09</title>
		<meeting>the 2nd International Conference on Theory of Information Retrieval: Advances in Information Retrieval Theory, ICTIR &apos;09<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="104" to="115" />
		</imprint>
	</monogr>
	<note>a term is known by the company it keeps</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
