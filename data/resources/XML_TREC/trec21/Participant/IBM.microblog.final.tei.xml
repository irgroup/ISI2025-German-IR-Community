<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.67,81.12,268.66,12.62">IBM at TREC 2012: Microblog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,129.06,138.60,52.43,10.52"><forename type="first">Myle</forename><surname>Ott</surname></persName>
							<email>myleott@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.53,138.60,94.06,10.52"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
							<email>vittorio@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.40,138.60,93.13,10.52"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,404.79,138.60,78.15,10.52"><forename type="first">Radu</forename><surname>Florian</surname></persName>
							<email>raduf@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.67,81.12,268.66,12.62">IBM at TREC 2012: Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C2D45477D704C83F8CA2F0BC3C93DB3B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes IBM Research's approach to the real-time ad-hoc retrieval task of the TREC 2012 Microblog Track. As this was our first time participating in the Microblog Track, our primary goal was to build a strong baseline system on which to later improve.</p><p>In particular, our system implements a Learning-to-Rank framework on top of a full-dependence Markov Random Field (MRF) retrieval model <ref type="bibr" coords="1,193.63,410.66,83.35,8.74;1,93.82,422.61,22.14,8.74" target="#b3">(Metzler and Croft, 2005)</ref>. We chose LambdaMART <ref type="bibr" coords="1,251.45,422.61,20.42,8.74;1,93.82,434.57,90.61,8.74" target="#b0">(Ganjisaffar et al., 2011)</ref> as our Learning-to-Rank learner, and trained it using over 50 features, including tweet, query and userspecific features. Our cross-validation results on the 2011 Track queries show that our system performs comparably to the top-performing systems in the TREC 2011 Microblog Track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Microblogs, such as those found on Twitter, are an increasingly popular medium for online content sharing. However, microblogs can pose numerous challenges to standard search and Information Retrieval (IR) techniques, largely due to the short and informal nature of their contents. Accordingly, there is a growing need for IR systems that can address the unique challenges associated with searching microblog collections.</p><p>Unfortunately, restrictions on sharing of microblog data has made it challenging to evaluate or compare the performance of IR systems on microblogs. This changed in 2011 with the introduction of the TREC Microblog Track, where researchers were allowed to crawl and evaluate their systems on a shared microblog corpus, Tweets11, containing nearly 16 million tweets.</p><p>The Microblog Track has been continued for TREC 2012, and in the following sections we will describe our efforts to build a strong baseline retrieval system for this year's real-time ad-hoc retrieval task. Note that while this year's Track also includes a new real-time filtering pilot task, since this was our first year participating in the Track, we chose to focus our efforts exclusively on the ad-hoc retrieval task.</p><p>The rest of this paper is organized as follows. First, in Section 2, we describe our process for collecting and preprocessing the data. Then, we describe our ad-hoc retrieval approach in Section 3. Finally, in Section 4, we present and discuss our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>In this section, we describe our handling of the various data utilized by our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tweets</head><p>For TREC 2012, the organizers chose to use the same Tweets11 corpus used for TREC 2011. This corpus originally contained a random sample of approximately 16 million tweets from a two week time period spanning January 24, 2011 to February 8th, 2011. The corpus was made to be as realistic as possible, and therefore no spam removal was performed prior to release.</p><p>The dataset was then distributed to participants in one of two ways. For those participants with greater access to the Twitter API, the corpus could be downloaded in the original (and richer) JSON format. For most participants, however, a more basic version of the corpus was obtained by crawling Twitter and extracting the tweets from the HTML. This was made possible through use of a custom crawler released by the TREC organizers last year. <ref type="foot" coords="2,202.21,382.55,4.23,6.99" target="#foot_0">1</ref>Unfortunately, changes to Twitter's HTML in early 2012 broke the original crawler, forcing teams that joined the Microblog Track in 2012 to first fix the crawler. Fortunately, the changes to Twitter's HTML included embedding the richer JSON tweet representation into each page's HTML, making it possible for all teams to download the JSON version of the corpus.<ref type="foot" coords="2,287.18,491.04,4.23,6.99" target="#foot_1">2</ref> </p><p>In addition to providing richer meta-data for each tweet, extracting the embedded JSON reduced the size of each block of 10,000 tweets from over 150MB to less than 5MB. More detailed corpus statistics appear in Table <ref type="table" coords="2,261.39,560.82,4.24,9.57" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Queries</head><p>For the 2011 task, the TREC organizers created 50 topics (queries), representing realistic information needs that people might have on Twitter. To evaluate how well the participating systems did at returning relevant results for these queries, TREC pooled the result sets and had human judges assess tweet relevance. This assessment was done on a fourpoint scale, where -2 indicates spam, 0 indicates not-relevant, 1 indicates relevant, and 2 indicates highly-relevant.</p><p>For the 2012 task, a new set of 60 topics (queries) were chosen, allowing participants to use the 2011 queries and relevance judgments to improve their systems. In particular, we chose to use the relevance judgments to train a Learningto-Rank system, which we describe further in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-processing</head><p>Once we completed downloading the corpus, we applied several pre-processing steps to the data, described here:</p><p>1. Removal of non-English tweets: The TREC evaluation guidelines define non-English tweets as de facto irrelevant. Therefore, we ran language identification on each tweet,<ref type="foot" coords="2,387.14,410.18,4.23,6.99" target="#foot_2">3</ref> and deleted from our collection all tweets that were assigned a 0-probability of being English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Removal of retweets: The TREC evaluation guidelines also define all retweets as de facto irrelevant. Therefore, per the Twitter Field Guide,<ref type="foot" coords="2,442.55,501.48,4.23,6.99" target="#foot_3">4</ref> we deleted from our collection all tweets that contained retweeted status in the status portion in their JSON. Among the remaining tweets, we additionally deleted any text that followed an RT token (as well as the RT token itself), since such text typically corresponds to quoted (retweeted) material.</p><p>3. Conversion to ASCII: Many tweets contain unusual or non-standard characters, which can be problematic for down-stream processing. To address these issues, we used a combination of BeautifulSoup<ref type="foot" coords="3,272.24,87.11,4.23,6.99" target="#foot_4">5</ref> and Unidecode<ref type="foot" coords="3,143.52,100.65,4.23,6.99" target="#foot_5">6</ref> to convert and transliterate all tweets to ASCII.</p><p>4. Tokenization: Next, we tokenized each tweet using a Twitter-specific tokenizer, tweetmotif<ref type="foot" coords="3,144.92,163.46,4.23,6.99" target="#foot_6">7</ref> , which properly handles most @-mentions, URLs and emoticons. Note that we modified the tokenizer to additionally recognize the heart (&lt;3) emoticon.</p><p>5. Removal of empty tweets: After completing all of the other pre-processing, we deleted any empty tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Indexing</head><p>After pre-processing, we used the Indri search engine<ref type="foot" coords="3,105.84,306.73,4.23,6.99" target="#foot_7">8</ref>  <ref type="bibr" coords="3,114.12,308.68,122.82,9.57" target="#b2">(Metzler and Croft, 2004)</ref> to index our corpus for fast retrieval. However, since each query has a query time associated with it, and systems are not permitted to return tweets that are broadcast after that time, we additionally chose to build one index per query instead of a single, large index. Each query index contained all and only those tweets broadcast between the query tweet time and the chronologically previous query's tweet time. The first index contained all tweets from the beginning of the corpus until the first query's tweet time. Additionally, we utilized Indri's built-in Porter stemmer at index time, in order to increase the engine's recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Given an information need, q, issued at a specific time, t, the TREC 2012 real-time ad-hoc retrieval task is to return an ordered list of 10, 000 tweets, ordered according to two factors: (a) relevance of the tweet to the query q; and (b) chronological closeness (recentness) of the tweet's broadcast time to the query time t. In particular, tweets that are more relevant to q and more recent w.r.t. t should be ordered higher in the list than tweets that are less relevant and less recent. The balance between relevance and recency is captured in the task evaluation, which includes thresholding the returned results and re-sorting the remaining results (reverse chronologically) before computing each metric. The metrics for this year's task are: P@30, MAP and AUC (ROC).</p><p>Since this was our first year participating in the Microblog Track, we largely ignored these task-specific considerations, and instead focused on building a strong baseline retrieval system on which to later improve. To this end, we implemented a standard Learning-to-Rank framework. In particular, for each query, q, issued at time, t, we retrieved a result set using the following process:</p><p>1. Retrieve a large, high-recall result set, baseline, using the Indri search engine<ref type="foot" coords="3,354.41,340.45,4.23,6.99" target="#foot_8">9</ref>  <ref type="bibr" coords="3,362.59,342.41,120.73,9.57" target="#b2">(Metzler and Croft, 2004)</ref>, which was a popular choice at TREC 2011. In particular, following <ref type="bibr" coords="3,407.64,369.50,110.94,9.57" target="#b1">Metzler and Cai (2011)</ref>, we utilize the full-dependence variant of Indri's MRF retrieval model <ref type="bibr" coords="3,443.57,396.60,96.43,9.57;3,335.02,410.15,26.06,9.57" target="#b3">(Metzler and Croft, 2005)</ref> to retrieve 20,000 results for each query, and additionally use Indri's built-in Porter stemming to increase recall.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features</head><p>We trained our ranking models using three kinds of features, given in Figures <ref type="figure" coords="4,212.46,576.27,8.49,9.57">1,</ref><ref type="figure" coords="4,225.35,576.27,4.24,9.57">2</ref>, and 3. Note that features prefixed by "FUTURE " rely on "future" information and are therefore excluded, per the task guidelines, except where indicated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-validation</head><p>Parameters for LambdaMART were tuned based on cross-validation experiments on the TREC 2011 queries and relevance judgments. However, in order to abide by the real-time constraint, our cross-validation procedure was performed in an online manner. In particular, for each 2012 test query, q 2012 , issued at time t 2012 , we tuned the parameters of a new ranking model by 5-fold cross-validation using all 2011 queries that had issue times prior to t 2012 . One complication of this procedure is that the learned ranking models were often built using very little training data, e.g., if q 2012 was issued early in the corpus timeline. To address this problem, we introduced a function, chosen in an ad-hoc manner and without tuning, to weight and interpolate the LambdaMART ranking and the original Indri ranking:</p><formula xml:id="formula_0" coords="4,313.20,271.93,209.82,40.78">weight(LTR) = 0.8 * 1 -exp 2 -|Q train | 20 weight(Indri) = 1 -weight(LTR),</formula><p>where |Q train | corresponds to the number of queries used for training the model. This function has the effect of down-weighting the Lamb-daMART ranking when the model is trained on only a few queries, and up-weighting its ranking when the model is trained on many queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Our team submitted three runs for evaluation, the results of which are given in Table <ref type="table" coords="4,505.66,449.94,4.24,9.57" target="#tab_2">2</ref>. The details of these three systems are given below:</p><p>• IBMBaseline: The exact ranking as given by Indri, up to rank 10,000.</p><p>• IBMLTR: The interpolated Lamb-daMART and Indri ranking, as described in Section 3, using only those features not prefixed with "FUTURE " in Figures <ref type="figure" coords="4,519.19,560.92,8.49,9.57">1,</ref><ref type="figure" coords="4,531.52,560.92,4.24,9.57">2</ref>, and 3, i.e., only those features that do not constitute "future information" according to the task guidelines.</p><p>• IBMLTRFuture:</p><p>The same as IBMLTR, but additionally includes features prefixed with "FUTURE " in Figures <ref type="figure" coords="4,373.74,663.99,20.61,9.57">1, 2,</ref> and<ref type="figure" coords="4,419.20,663.99,4.24,9.57" target="#fig_0">3</ref>.</p><p>The results suggest that Learning-to-Rank (LTR) does improve performance over the baseline retrieval model given by Indri. Indeed, the LTR runs outperform the Indri baseline in all but one case (MAP, Highly Relevant, LTR w/o future features). We also observe that the "future" features are very helpful and improve LTR performance in all but one case (P@30, Relevant). We suspect that this is primarily due to the inclusion of the features num retweets and user num followers, though this should be explored further in future work through a feature ablation study. Future work may additionally include features that address the vocabulary mismatch problem, potentially using topic models, as discussed in <ref type="bibr" coords="5,164.42,371.14,98.42,9.57" target="#b4">Ramage et al. (2010)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,133.62,511.65,103.55,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: User features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,121.50,82.08,127.80,108.81"><head>Table 1 :</head><label>1</label><figDesc>Corpus statistics.</figDesc><table coords="2,121.50,99.23,127.80,91.66"><row><cell cols="2">Number of tweets: 14.1m</cell></row><row><cell>Vocabulary size:</cell><cell>812k</cell></row><row><cell>Unique hashtags:</cell><cell>220k</cell></row><row><cell>English tweets:</cell><cell>62%</cell></row><row><cell>Retweets:</cell><cell>10%</cell></row><row><cell>Replies:</cell><cell>20%</cell></row><row><cell>Compressed size:</cell><cell>7GB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.00,82.08,468.00,96.33"><head>Table 2 :</head><label>2</label><figDesc>Results. Bold entries in each row correspond to best performing run. "Relevant" and "Highly Relevant" refer to the relevance cutoff used for evaluation.</figDesc><table coords="5,125.05,109.21,361.90,69.19"><row><cell>Metric</cell><cell cols="3">IBMBaseline IBMLTR IBMLTRFuture</cell></row><row><cell>P@30 (Relevant)</cell><cell>0.3808</cell><cell>0.4136</cell><cell>0.4090</cell></row><row><cell>P@30 (Highly Relevant)</cell><cell>0.2028</cell><cell>0.2237</cell><cell>0.2254</cell></row><row><cell>MAP (Relevant)</cell><cell>0.2620</cell><cell>0.2630</cell><cell>0.2731</cell></row><row><cell>MAP (Highly Relevant)</cell><cell>0.1968</cell><cell>0.1932</cell><cell>0.2018</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.59,648.13,66.29,7.86;2,171.70,648.78,127.10,7.47;2,72.00,659.74,97.70,7.47"><p>Available here: https://github.com/lintool/ twitter-corpus-tools.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,88.59,670.04,210.21,7.86;2,72.00,681.00,226.80,8.12;2,72.00,691.96,226.80,8.12;2,72.00,702.92,226.80,8.12;2,72.00,714.52,196.55,7.47"><p>Code to extract the embedded JSON was first made available by spacelis here: https://github.com/ spacelis/twitter-corpus-tools; this code was later improved by our team and made available here: https: //github.com/myleott/twitter-corpus-tools.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,329.79,670.05,210.21,7.86;2,313.20,681.01,226.80,8.12;2,313.20,692.61,21.39,7.47"><p>We used Nakatani Shuyo's Twitter-specific language ID code, available here: https://github.com/shuyo/ ldig.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,329.79,703.56,136.51,7.47;2,313.20,714.52,75.81,7.47"><p>https://dev.twitter.com/docs/ platform-objects</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,88.59,670.72,145.93,7.47;3,72.00,681.67,65.90,7.47"><p>http://www.crummy.com/software/ BeautifulSoup/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,88.59,692.62,174.17,7.47"><p>http://pypi.python.org/pypi/Unidecode</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="3,88.59,703.57,178.88,7.47"><p>https://github.com/brendano/tweetmotif</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="3,88.59,714.52,160.05,7.47"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="3,329.79,703.57,160.05,7.47"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="3,329.79,714.52,160.05,7.47"><p>http://code.google.com/p/jforests/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,72.00,424.66,226.80,8.74;5,82.91,436.62,215.89,8.74;5,82.91,448.57,215.89,8.74;5,82.91,460.53,215.89,8.74;5,82.91,472.48,215.89,8.74;5,82.91,484.44,215.89,8.74;5,82.91,496.39,52.86,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,146.32,436.62,152.48,8.74;5,82.91,448.57,194.18,8.74">Bagging gradient-boosted trees for high precision, low variance ranking models</title>
		<author>
			<persName coords=""><forename type="first">Yasser</forename><surname>Ganjisaffar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,82.91,460.53,215.89,8.74;5,82.91,472.48,215.89,8.74;5,82.91,484.44,80.42,8.74">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information, SIGIR &apos;11</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,509.19,226.79,8.74;5,82.91,521.15,215.89,8.74;5,82.91,533.10,145.62,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,208.87,509.19,89.92,8.74;5,82.91,521.15,68.24,8.74">Usc/isi at trec 2011: Microblog track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,174.83,521.15,123.97,8.74;5,82.91,533.10,78.92,8.74">Proceedings of the Text REtrieval Conference</title>
		<meeting>the Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,545.90,226.80,8.74;5,82.91,557.86,215.89,8.74;5,82.91,569.81,215.89,8.74;5,82.91,581.77,90.82,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,233.45,545.90,65.34,8.74;5,82.91,557.86,215.89,8.74;5,82.91,569.81,215.89,8.74;5,82.91,581.77,19.53,8.74">Combining the language model and inference network approaches to retrieval. Information processing &amp; management</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="735" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,594.57,226.80,8.74;5,82.91,606.52,215.89,8.74;5,82.91,618.48,215.90,8.74;5,82.91,630.43,215.90,8.74;5,82.91,642.39,156.78,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,220.95,594.57,77.85,8.74;5,82.91,606.52,144.90,8.74">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,249.14,606.52,49.65,8.74;5,82.91,618.48,215.90,8.74;5,82.91,630.43,215.90,8.74;5,82.91,642.39,55.19,8.74">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,655.19,226.80,8.74;5,82.91,667.14,215.89,8.74;5,82.91,679.10,215.89,8.74;5,82.91,691.05,142.54,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,273.86,655.19,24.93,8.74;5,82.91,667.14,169.71,8.74">Characterizing microblogs with topic models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liebling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,273.70,667.14,25.10,8.74;5,82.91,679.10,215.89,8.74;5,82.91,691.05,24.18,8.74">International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
