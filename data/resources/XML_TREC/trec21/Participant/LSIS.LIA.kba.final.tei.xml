<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.51,80.99,336.99,12.90">LSIS/LIA at TREC 2012 Knowledge Base Acceleration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,157.54,133.85,92.92,10.75"><forename type="first">Ludovic</forename><surname>Bonnefoy</surname></persName>
							<email>ludovic.bonnefoy@etd.univ-avignon.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">LIA -University of Avignon</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.99,133.85,82.91,10.75"><forename type="first">Vincent</forename><surname>Bouvier</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">LSIS -Aix-Marseille University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,435.74,133.85,69.27,10.75"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">LSIS -Aix-Marseille University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.51,80.99,336.99,12.90">LSIS/LIA at TREC 2012 Knowledge Base Acceleration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5C94A9A61AE9AA991B92E269144C5769</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our joint participation in the TREC 2012 KBA task. The system is broken down as follows : first name variations of the entity topics are searched then documents containing at least one of them are retrieved. Finally documents go through two classifiers to categorize them as garbage, neutrals, relevant or centrals. This system got good results (3rd of 11) however first analyses tends to show that ranking is just a little bit better than random.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC 2012 has seen a new track named Knowledge Base Acceleration (KBA) started. This new task requires to focus on new challenges such as finding good ways of managing large corpora along a timeline.</p><p>KBA is about retrieving information as well as assessing the importance of them in order to eventually feed databases with new correct statements, or even broadcast news in real time. To do so, a stream has to be continuously monitored along the time to detect changes (i.e., new upcoming things about a topic). This stream is actually a set of documents that could be fed of blogs, microblogs, news websites and so on. Hence, the corpus has three categories of documents:</p><p>News: come from public news websites; Socials: come from blogs, forums; Links: come from bitly database; Timestamp as well as metadata are provided depending on the category of the document. As a first task this year, the purpose is to find documents about known entities and classify those documents into four different classes :</p><p>Garbage: not relevant, e.g. spam.</p><p>Neutral: Not relevant, i.e. no information could be deduced about the entity, e.g., entity name used in product name, or only pertains to community of target such that no information could be learned about entity.</p><p>Relevant: Relates indirectly, e.g., tangential with substantive implications, or topics or events of likely impact on entity.</p><p>Central: Relates directly to target such that you would cite it in the wikipedia article for this entity, e.g. entity is a central figure in topics/events.</p><p>Next section we detail preprocessing steps we did concerning the corpus and the topics. Second part presents the core of our method which relies on two classification steps and high-level features. Finally we present and discuss results and bring some perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preprocessings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpus</head><p>The first challenge is how to deal with such a tremendous corpus and how to do it efficiently. We decided to index the corpus offline and, when running our system, to retrieve candidate documents by mean of a search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Indexation</head><p>We first had to deal with how to index the corpus with respect to the KBA rules to treat the corpus as a stream. The corpus is already divided into folders (one folder per hour) so we naturally indexed each single folder separately.</p><p>To index the corpus, we used the state-of-the-art information retrieval platform Terrier 1 . Within a document it is possible to access either the RAW version of the data (e.g., body as HTML document) or to access the CLEANSED version (e.g., body as plain text). We decided not to deal with document structures so we only used the cleansed version for every data. Moreover only cleansed documents were evaluated by assessors.</p><p>For the indexation process we indexed documents' titles as well as their bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Retrieval</head><p>We adopted a recall oriented approach for the document retrieval step : we wanted to retrieve all documents containing at least one mention of a given topic entity. The only restriction was that documents must contain the named entity without word permutations, missing words, etc. (to avoid to get documents about "Barak Obama" when looking for 'Aharon Barak"). In order to find variants we parsed the whole Wikipedia corpus to find every links that pointed to the topic document from another document. We also consider the bold text contains in the first paragraph of a Wikipedia's article. Variants are weighted according to :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The topics</head><formula xml:id="formula_0" coords="2,334.55,389.29,158.00,32.32">w(v i ) = 1 if v i is the topic entity tf (v i ) v j ∈V tf (v j ) otherwise</formula><p>Example 2.1 -Sample of found variants for Boris Berzovsky the businessman and the pianist :</p><formula xml:id="formula_1" coords="2,313.20,470.13,255.27,143.17">--------------------------------------- Boris_Berezovsky_(businessman) --------------------------------------- boris berezovsky 1,000000 boris abramovich berezovsky 1,000000 --------------------------------------- Boris_Berezovsky_(pianist) --------------------------------------- boris berezovsky 1,000000 boris vadimovich berezovsky 1,000000</formula><p>3 Classification</p><p>The output of the document retrieval step is a set of all documents containing at least one mention of one the entity's variants. According to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features</head><p>A lot of approaches already exists in order to filter documents in a stream according to a topic. However, most of them are topic specific ie each topic need is own classifier and so need an associated training set. This is a huge drawback because they are not easily adaptable to new topics.</p><p>In this work we were looking for features which capture topic independent phenomena which denote relevancy or centrallity (in the way it is defined for this task).</p><p>Intuitively, we came with three groups of features : time related features, document content related features and related entities features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Time related features</head><p>Since we are monitoring topic activities on a chronological stream of documents, we thought interesting to look at the peaks that could arise suddenly after querying a certain amount of indexes. A scale has been determined arbitrarily that seems reasonable enough in term of time monitoring scale:</p><p>Daily report : Results are aggregated over 24hours so that there is enough time for documents to appear when a topic is making the "buzz". The number of document found within a 24 hour scale is used as a feature for the algorithm.</p><p>6 previous days statistics : Statistics are gathered on a sliding window over a week (i.e., a queue of 7 days), where daily report represent the current day and other statistics are computed from the 6 previous days such as:</p><p>Number of mentions in previous days title: counts how many mentions there are in titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of mentions in previous documents:</head><p>counts how many mentions there are in documents' bodies.</p><p>The average number of documents: The average number of documents where the whole week is considered.</p><p>σ of the amount of documents: The standard deviation computed from the number of documents found over a week. This features is helpful to detect peaks since the standard deviation will change brutally when the distribution of documents changes suddenly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Document content related features</head><p>Even though those features are really interesting to observe it is not enough to assess whether a document is relevant or not, since it represents a set of documents and not a specific document. Moreover it may have some noise in the peak itself. So we decided to add features concerning the topic's mentions for each single document:</p><p>Mentions distribution: How the different variants of the mention are distributed along the document. We computed then the amount of mentions from 0% to 100% using a 10% step. In addition we add a specific feature for the mentions in title.</p><p>Tf-Idf: The document score given by a TF IDF computed by Terrier with variants as queries;</p><p>Cosine Similarity: A cosine similarity is computed between 1gram and 2gram words distribution of the document and the 1gram and 2gram words distribution created from the Wikipedia topic's page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Related entities features</head><p>When dealing with monitoring particular topics, it's interesting to keep track of different relations the topic has with a particular entity (e.g., a person, an event,...). Related entities have been extracted from Wikipedia during the dump parsing using two different information:</p><p>• The page is linked by the topic's page;</p><p>• All named entities found in the topic's page</p><p>Relations : Count the amount of relations in document's title and body for each related entity; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs</head><p>Our team submitted 6 runs falling in one of the two different scoring methods used. Each classifier returns a score between 0 and 1 for a given document.</p><p>A score of 0 or 1 denotes a total confidence in the class associated to a document by a classifier. Concerning the first classifier, a score below 0.5 signifies that it has assigned the garbage/neutral class to the document, a score higher corresponds to the relevant/central class. The second classifier gives a score between near 0 and near 1 where near 0 is relevant and near 1 is central.</p><p>"Yes" runs : for the 3 runs falling in this category we returned only documents classified as Relevant/Central by the first classifier. Then documents are scored according to :</p><formula xml:id="formula_2" coords="4,123.93,644.76,144.77,10.63">score(d i ) = s(d i , c 1 ) × s(d i , c 2 )</formula><p>where the score of the document d i is given by a product of the scores given respectively from classifiers c 1 and c 2 having therefore s(d 1 , c 1 ) &gt; 0.5.</p><p>"All" runs: for these 3 runs all documents found are returned. Documents are scored according to :</p><formula xml:id="formula_3" coords="4,335.02,119.62,208.46,28.18">score(d i ) = s(d i , c 1 ) if s(d i , c 1 ) &lt; 0.5 0.5 + s(d i ,c 1 )×s(d i ,c 2 ) 2 otherwise</formula><p>The 6 submitted runs are :  <ref type="table" coords="5,113.24,285.58,5.45,9.46" target="#tab_1">2</ref> we can see that proportions of central documents compare to documents with a mention is 0.232. This shows that our ranking method performs just a little bit better than random. This is obvious when looking at RF-All for central and relevant judgements : the precision for a cutoff of 200 is 0.567 and random would have done 0.648.</p><formula xml:id="formula_4" coords="4,313.20,180.97,13.94,9.81">All</formula><p>In this work, we tried to find what could make a document relevant considering the time. Our first and strong assumption was to say:</p><p>"for each topic, a document is more likely to be relevant if a huge amount of document appears at the same time."</p><p>It seems to describe well the behavior of micro blogs such as tweeter where once a news about someone or something is released, many tweets appear about it, where almost no tweet where mentioning the topic beforehand.</p><p>Figure <ref type="figure" coords="5,104.51,556.98,5.45,9.46" target="#fig_0">1</ref> gives gini variables importances for each features on the training data for Random Forests. It shows that the number of documents in the queue (i.e., in a week) do not really affect the decisions made by the classifier. However, this may be explain because the way we did it is not relevant. Investigations have to be done about it before strong conclusions can be made.</p><p>Still considering the gini variables, we found that it is highly valuable to look at relations. Indeed when a relation is already known the document is almost automatically accepted. Thus by keeping track of new relations we could probably improve the classifier precision. Moreover a relation between entities may help to determine which of the homonyms the document is about even though finding relations is an hard task and therefore only a few are found.</p><p>Example 5.1 -example of disambiguation using relations between entities :</p><p>Say for instance we know that "Boris Berezovsky" (BB) (the pianist) plays at a specific concert hall. The extracted relation is:</p><formula xml:id="formula_5" coords="5,329.17,234.51,194.87,9.81">Boris Berezovsky → PLAYS ← concert hall</formula><p>where "Boris Berezovsky" is the relation entity 1 (RE 1 ), "concert hall" is the relation entity 2 (RE 2 ) and "PLAYS" is the link l between the two elements.</p><p>Let considers that in a future document appearing 2 month later the same relation appears. Since we know from a previous relation that Boris Berezovsky played at this concert hall, so it's more likely the pianist (and not the businessman) that plays back there 2 month later. So this feature might become a criteria that really helps disambiguating homonyms topics.</p><p>In addition, similarity between topics wikipedia's pages and the found documents has been also revealed by the decision tree classifier. This means that somehow a document that is about a specific topic may be similar to a document that has general overview of the topic. It could be interesting to measure the similarity between documents that appears the same day. Thus documents that echo the same information could be aggregated or at least have a heavier weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This task is a very challenging task as expected and therefore very interesting. This first year allows us to comprehend what is behind KBA. From now on we have learned a lot concerning the way we have to treat the stream corpus as well as the topics. We also know, since relations might be a key point, that having a system that we can rely on to find relation would probably improve our final results.</p><p>Moreover there are plenty of data we have not used (e.g, metadata, category of documents, the source,...) although it might be also important to consider a document for instance depending on the reliability of the source.</p><p>In this report we presented features that can be used to capture relevancy mainly independently of the entity evaluated. One of the natural perspective is to figure out how to combine them with topic specific approach in order to see how they can contribute to improve their results.</p><p>There are still many improvements that can be done, and since this is only the first years, it will make the following years quite promising.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,72.00,570.22,465.49,9.79"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Gini variable importance for features on training data for Garbage/Neutral vs. Relevant/Central</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,313.20,685.45,226.80,36.91"><head>Table 2 :</head><label>2</label><figDesc>document being either garbage, neutral, relevant or central, we decided to rely on a supervised approach. Such approach is made possible by the train corpus (from October to December 2011) and associated relevance judgments provided by organizers. KBA corpus statistics</figDesc><table coords="3,79.33,166.01,212.14,31.43"><row><cell></cell><cell cols="4">garbage neutral relevant central</cell></row><row><cell>contains mention</cell><cell>7991</cell><cell>3862</cell><cell>13971</cell><cell>7806</cell></row><row><cell>zero mention</cell><cell>15367</cell><cell>163</cell><cell>61</cell><cell>0</cell></row></table><note coords="2,470.26,685.45,69.74,9.81;2,313.20,699.35,226.80,9.46;2,313.20,712.90,226.80,9.46;3,72.00,75.86,59.82,9.46"><p>table 1, ≈ 33% of documents containing an entity mention are either garbage or neutral. In order to determine what could make a</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,313.20,180.97,226.80,541.39"><head>Table 3 :</head><label>3</label><figDesc>Results for central judgments for F1 and SU measures</figDesc><table coords="4,313.20,180.97,226.80,541.39"><row><cell cols="3">: The two first runs are the result of Ran-</cell></row><row><cell cols="3">dom Committee classifiers that uses the Weka</cell></row><row><cell>framework;</cell><cell></cell></row><row><cell cols="3">RF : The next two other runs are the result of</cell></row><row><cell cols="3">Random Forest classifiers that uses the Weka</cell></row><row><cell>framework;</cell><cell></cell></row><row><cell cols="3">SRF : The last two other runs are the result of Ran-</cell></row><row><cell cols="3">dom Forest classifiers that uses the Salford Uni-</cell></row><row><cell>versity System;</cell><cell></cell></row><row><cell cols="3">For each one run is for classifying Garbage/Neutral</cell></row><row><cell cols="3">over Central/Relevant, where the other is for seper-</cell></row><row><cell>ating Central from relevant.</cell><cell></cell></row><row><cell>5 Results</cell><cell></cell></row><row><cell></cell><cell>F1</cell><cell>SU</cell></row><row><cell>bests</cell><cell cols="2">.359 .410</cell></row><row><cell>RF-Yes (200)</cell><cell cols="2">.342 .278</cell></row><row><cell>RF-All (600)</cell><cell cols="2">.330 .279</cell></row><row><cell cols="3">SRF-All (400) .326 .216</cell></row><row><cell cols="3">SRF-Yes (200) .322 .228</cell></row><row><cell>All-All (450)</cell><cell cols="2">.318 .188</cell></row><row><cell>All-Yes (50)</cell><cell cols="2">.306 .193</cell></row><row><cell>medians</cell><cell cols="2">.289 .220</cell></row><row><cell>means</cell><cell cols="2">.220 .311</cell></row><row><cell cols="3">Table 3 and 4 show performances of our runs against</cell></row><row><cell cols="3">official judgments. At first look, our results are quite</cell></row><row><cell cols="3">good. For central judgements: all out runs are above</cell></row><row><cell cols="3">the median; our best run get to the third place. Con-</cell></row><row><cell cols="3">cerning relevant/central evaluation, 4 of them are</cell></row><row><cell cols="3">still above the median and far better than the mean</cell></row><row><cell>scores.</cell><cell></cell></row><row><cell cols="3">However, if we look for instance to our best run for</cell></row><row><cell cols="3">central evaluation (RF-Yes) with a cutoff of 0, the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,72.00,223.02,226.80,72.01"><head>Table 4 :</head><label>4</label><figDesc>Results for central and relevant judgments for F1 and SU measures precision is equal to 0.276. Now, if we look back at Table</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
