<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.55,83.76,328.62,15.48">The University of Amsterdam at TREC 2012</title>
				<funder ref="#_6guqc3K">
					<orgName type="full">Netherlands eScience Center</orgName>
				</funder>
				<funder ref="#_aASZ4jF">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_RaJBUSx">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder ref="#_ZxszmTJ #_Q87Vmfh #_m9Wcj2h #_BQnak26 #_Qbe8ezZ #_YtTqZqr">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_KYYCDJ2">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_TbuUjfK">
					<orgName type="full">ESF</orgName>
				</funder>
				<funder ref="#_YHaDDAM">
					<orgName type="full">CLARIN-nl program</orgName>
				</funder>
				<funder>
					<orgName type="full">Royal Dutch Academy of Sciences (KNAW)</orgName>
				</funder>
				<funder>
					<orgName type="full">Elite Network Shifts</orgName>
				</funder>
				<funder>
					<orgName type="full">CIP ICT-PSP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.26,116.28,98.42,10.75"><forename type="first">Richard</forename><surname>Berendsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.62,116.28,58.77,10.75"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.33,116.28,60.12,10.75"><forename type="first">Daan</forename><surname>Odijk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.49,130.23,90.19,10.75"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.62,130.23,97.60,10.75"><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.55,83.76,328.62,15.48">The University of Amsterdam at TREC 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">617B43D6D9D8CEEF9829076E54592597</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the participation of the University of Amsterdam's ILPS group in the Knowledge Base Acceleration and Microblog tracks at TREC 2012.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year the Information and Language Processing Systems (ILPS) group of the University of Amsterdam participated in the Microblog and the Knowledge Base Acceleration (KBA) tracks. In this paper, we describe our participation for each of these tracks, in two largely independent sections: Section 2 on our KBA track participation and Section 3 on our work in the Microblog track. We detail the runs we submitted, present the results of the submitted runs, and, where possible, provide an initial analysis of these results. We conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Knowledge Base Acceleration</head><p>Recent advances have enabled a precise manner of analysis, where phrases occurring in documents are automatically linked to entries in a knowledge base <ref type="bibr" coords="1,214.48,483.68,15.27,8.64" target="#b10">[11]</ref>. This process is commonly known as entity linking. Entity linking facilitates advanced forms of searching and browsing in various domains and contexts. It can be used, for instance, to anchor the textual resources in background knowledge; authors or readers of a piece of text may find entity links to supply useful pointers <ref type="bibr" coords="1,160.73,555.41,10.79,8.64" target="#b5">[6,</ref><ref type="bibr" coords="1,175.55,555.41,7.19,8.64" target="#b8">9]</ref>. Another application can be found in search engines, where it is increasingly common to link queries to entities and present entity-specific overviews <ref type="bibr" coords="1,96.69,591.27,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="1,109.97,591.27,7.19,8.64" target="#b7">8]</ref>. This year's single task at TREC KBA concerned entity linking on a stream of data. That is, given a target entity from a knowledge base and an incoming stream consisting of textual content such as web pages, news items, and social media content, generate a score for each item based on how pertinent it is to the target entity.</p><p>Typical solutions to entity linking operate on static collections of documents <ref type="bibr" coords="1,147.30,686.91,10.58,8.64" target="#b8">[9]</ref>. In this year's TREC participation, we adapt our entity linking system to be able to operate in this dynamic setting. To this end, we have implemented an incremental learning algorithm that updates at each time step, thereby improving performance at each point in time. Our algorithm also caters for different levels of aboutness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning to Link Entities</head><p>We base our submissions mainly on the approach detailed in <ref type="bibr" coords="1,316.81,291.51,10.58,8.64" target="#b8">[9]</ref>. This approach is a learning to rerank approach to improve precision on a recall-oriented baseline. In this section we first introduce our general machine learning framework and the features we use. We then zoom in on how we adapt these to account for the incremental nature of the document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Machine Learning</head><p>For entity linking on document streams we build upon a twostep process that has been shown to perform well on generic entity linking, i.e., exhaustively identifying all possible links in a piece of text <ref type="bibr" coords="1,386.77,435.86,10.58,8.64" target="#b8">[9]</ref>. The goal of the first step is to provide a filter and identify whether or not a document d contains a mention of the entity of interest, e. To this end, we consider all surface forms for e and determine whether the document contains any of these surface forms. In the second step we determine the level of centrality of e in d by applying supervised machine learning, using the set of features listed in Section 2.1.2. Using a machine learning framework allows us to encode different levels of relevance into our learning algorithm. Filtering the documents to be used as input for the machine learning algorithm reduces the number of feature vectors that need to be created in the second step, decreasing the end-to-end runtime.</p><p>We cast the second step as a binary classification problem and use the classifier's confidence to rank each document with respect to an entity. We employ random forests (RF) as our classification algorithm <ref type="bibr" coords="1,439.22,627.14,10.58,8.64" target="#b1">[2]</ref>. RF is an ensemble-based decision tree classifier based on bagging, in which a learning algorithm is applied multiple times to a subset of the instances and the results averaged. In this case, for each iteration a bootstrap sample is taken and a full tree is constructed. For each node of the tree, m features are randomly selected to obtain the best split. This process reduces overfitting by averaging classifiers that are trained on different subsets of the data with the same underlying distribution. RF is also relatively insensitive to parameter settings, resistant to overfitting, and easily parallelizable. We set m to the square root of the number of features <ref type="bibr" coords="2,158.97,326.05,10.58,8.64" target="#b4">[5]</ref>. Besides m, RF has one additional parameter: the number of iterations which we set to k = 1500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Features</head><p>Table <ref type="table" coords="2,78.44,394.88,4.98,8.64" target="#tab_0">1</ref> shows the features that we use. They are calculated on two distinct parts of each document, f d : the title and the body. All feature values are normalized by the length of this part of the document, if applicable. Moreover, for the features TF, POS f , POS l , and SPR, we extract separate feature values for each of the different representations of the entity, r e . These representations are obtained from the Wikipedia article associated with e and include (i) the title, (ii) the text of the incoming anchors, (iii) the text of any incoming redirect links, and (iv) the union of these three. So, for instance for the feature TF, we actually have eight feature values, one for each combination of representation and document field. This yields a total of 48 features.</p><p>The SIM feature indicates the similarity between the entity and the document and is determined using the KLdivergence (KL-div) between the language models of the entity and the document:</p><formula xml:id="formula_0" coords="2,88.08,605.98,204.83,26.09">KL-div(θ||θ ) = ∑ t∈V P(t|θ) log P(t|θ) P(t|θ ) ,<label>(1)</label></formula><p>where t ∈ V denotes a term in the shared vocabulary; each language model is estimated using a multinomial distribution over unigrams. To avoid zero frequency issues, we apply additive smoothing: Table <ref type="table" coords="2,342.84,234.63,3.88,8.64">2</ref>: Results for TREC KBA. For each run the optimal cutoff is indicated; this is determined by considering the highest value for the F-measure for that run.</p><formula xml:id="formula_1" coords="2,111.74,699.42,181.16,23.95">P(t|θ d ) = δ + n(t, d) δ + ∑ t n(t , d) ,<label>(2)</label></formula><p>and set δ = 1. Here, n(t, d) denotes the count of term t in d. Note that KL-div is an asymmetric measure, hence we calculate it twice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Incremental Learning</head><p>As we have a set of training documents, we experiment with a number of incremental learning variants. Each incremental learning algorithm can be cast as a sequential prediction problem, where for each time step i = 1, 2, . . .:</p><p>1. An unlabeled document d i arrives.</p><p>2. We perform a prediction ŷi , with confidence c i based on e and the current model m i .</p><p>3. We assume the predicted labels are correct and add a subset of documents as training material with assumed label y i according to some constraint.</p><p>4. Update the model m i+1 .</p><p>We vary the constraints for step 3 in our experimental conditions below. In all cases, we consider a semi-supervised scenario, were we start with set of training material with annotated labels and try to learn from newly seen material. Here we start with a model m 1 based on the 2011 training data and update the model m i+1 at each step using the predictions made on documents arriving at step i. We define each day to be a time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Runs</head><p>We preprocess the TREC KBA document collection as follows. We tokenize each document, lowercase all characters, and remove all diacritics. We implement our algorithm on Hadoop; the code can be found on GitHub. UvAIncLearnT50 Same as UvAIncLearnT25, with k = 50.</p><p>UvAIncLearnLow Incremental learning; We consider the confidence of the machine learning algorithm and include d i only if the confidence of the machine learning algorithm is above a certain threshold for each class.</p><p>UvAIncLearnHigh Same as UvAIncLearnLow, with a higher threshold.</p><p>Note that we discovered a bug in our official submission files and the performance of our submitted runs is therefore not indicative of the performance of our system. Furthermore, we trained a single classifier for all entities, whereas our approach calls for training machine learning models on a perentity basis. In the next section, we report on the results of the repaired version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>We found that the threshold-based runs (UvAIncLearnLow and UvAIncLearnHigh) did not perform significantly different from the top-k runs (UvAIncLearnT25 and UvAIn-cLearnT50). We therefore do not include the former in our analysis, but discuss different, bug-fixed variants of the latter instead (see above).</p><p>Table <ref type="table" coords="3,86.99,674.96,4.98,8.64">2</ref> shows the main results for our TREC KBA participation. We observe that our machine learning approach improves precision over the baseline, resulting at an improved highest F-measure. Incremental learning has a positive effect Table <ref type="table" coords="3,341.15,244.64,3.88,8.64">4</ref>: Results per genre for TREC KBA. For each run the optimal cutoff is indicated; this is determined by considering the highest value for the F-measure for that run.</p><p>on performance in terms of F-measure. For the "central" relevance level, this is mainly due to a higher precision; recall at the highest F-measure is lower. Figure <ref type="figure" coords="3,480.11,323.60,4.98,8.64">1</ref> shows the performance of the IncLearnT10 run at varying cutoff levels. We observe that precision remains quite constant over all cutoff levels, while recall increases as expected. This results in a highest F-measure at a rather low cutoff of 191. Overall, precision is fairly low, which is mainly due to the fact that we approach the task as a ranking problem (instead of classification). As such, we do not "classify" many documents as being non-relevant. Table <ref type="table" coords="3,351.03,431.20,4.98,8.64" target="#tab_2">3</ref> shows the results of our runs as measured using rank-based evaluation measures. We observe a similar pattern as for precision, recall, F-measure, and SU, with our machine learning approach improving on most rank-based measures. For most measures, incremental learning is not able to improve on the static machine learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Per Genre</head><p>Next, we consider the performance for each genre (news, social, linking) separately. That is, we split up the "central" assessments based on the genre associated with each document. Table <ref type="table" coords="3,367.56,576.99,4.98,8.64">4</ref> shows the results. We observe that we obtain the best scores on all four measures in the news genre. Interestingly, in the news and linking genre, incremental learning does not improve precision for the optimal cutoff, where it does improve in the social genre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Per Day</head><p>Figure <ref type="figure" coords="3,344.77,674.96,4.98,8.64">2</ref> shows the performance of each run in terms of MAP on each day in the test set, determined using the minimum relevance level "central." This plot shows that no run outperforms any other overall. We also observe a pattern of peaks that seems to be consistent with the number of documents in the collection for that particular day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Microblog</head><p>This year's TREC Microblog track consists of two tasks: real-time ad-hoc and filtering. We participated only in the real-time ad-hoc task. Due to technical problems we could only submit one official run for this track (UvAFilter, see below), but in this section we also discuss the two runs that we originally planned to submit (UvAtrain2011 and UvAtrain-Gen). We discuss the following three runs in this section:</p><p>UvAFilter A baseline approach that filters out tweets based on a set of heuristics.</p><p>UvAFilterExp Baseline retrieval run with query expansion based on UvAFilter.</p><p>UvAtrain2011 A learning to rank approach trained on the TREC 2011 Microblog track topics.</p><p>UvAtrainGen The same learning to rank approach as UvA-trainGen, but trained on a generated pseudo test collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>For all our runs we use a preprocessed version of the corpus. First, we discard non-English tweets using a language identification method for microblogs <ref type="bibr" coords="4,449.36,639.09,10.58,8.64" target="#b2">[3]</ref>. We then remove exact duplicates and keep the oldest of each duplicate set, and finally, we discard retweets, unless there are added comments.</p><p>For each tweet we remove punctuation and stopwords using a collection-based stopword list. After preprocessing our collection has just over four million tweets, roughly 25% of the raw collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">UvAFilter</head><p>Based on observations from last year's Microblog track, we follow a heuristics-based approach for our baseline run.</p><p>Starting from a retrieved set of tweets, constructed using Indri<ref type="foot" coords="5,73.17,110.20,3.69,6.39" target="#foot_1">2</ref> and queries rewritten using a Markov random field model for term dependencies <ref type="bibr" coords="5,175.63,124.08,15.27,8.64" target="#b9">[10]</ref>, we only keep tweets in our final result list if they match the following criteria: A tweet should (i) contain a URL, (ii) not contain a mention to another user ("@user . . . "), and (iii) does not refer to first or second person pronouns ("you," "me," "I," . . . ). The intuition behind the three filtering criteria is that a tweet only becomes "interesting" once it refers to more information besides the 140 characters of the tweets itself (the URL criterium), if it is not part of a conversation or directed as another user (the mention criterium and the second person pronouns), and finally, if it is not talking about personal opinions or experiences (the first person pronouns).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">UvAFilterExp</head><p>A potential problem with query expansion is topic drift and the inclusion of non-informative terms from highly ranked documents. In this run we try to counter this by using the strictly filtered UvAFilter run as our input set of tweets for query expansion. We use a naive expansion technique that takes the top K tweets for each topic and return only those terms that appear more than N times in this set of tweets. We apply this method on our preprocessed collection of tweets, which does not contain stopwords.</p><p>We use the following settings: K = 20 and N = 4. As an example, Table <ref type="table" coords="5,118.51,424.17,4.98,8.64" target="#tab_4">5</ref> shows three example topics that improve over UvAFilter and one topic that drops in performance. For each topic we show the original query and the selected expansion terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">UvAtrain2011</head><p>For microblog search, learning to rank (LTR) is a natural approach to take into account features such as the existence of a link, a hashtag, the recency of a tweet, the authority of the user, and so on. Training an LTR system requires good quality training material, the more the better. A natural training set are the TREC 2011 Microblog track topics. Our LTR run UvAtrain2011 run trains on these topics. We describe the details of this run below.</p><p>As features we use:</p><p>Query-document features Five Indri 2 runs; Five Terrier<ref type="foot" coords="5,288.72,623.38,3.69,6.39" target="#foot_2">3</ref> runs; The number of rankers that retrieved a tweet; The {min, max, avg, median} reciprocal rank of a tweet;</p><p>The recency of a tweet.</p><p>Query features Query clarity, as described in <ref type="bibr" coords="5,241.38,682.46,10.58,8.64" target="#b3">[4]</ref>.</p><p>Tweet features Presence of a link, number of user mentions, tweet length, capitalization <ref type="bibr" coords="5,476.05,69.23,15.27,8.64" target="#b13">[14]</ref>, density <ref type="bibr" coords="5,531.33,69.23,10.58,8.64" target="#b6">[7]</ref>, is the message a direct message.</p><p>The parameters of Indri and Terrier retrieval runs were first tuned on the TREC 2011 microblog topics.</p><p>Learning to rank We linearly normalized all features. We used a pairwise SVM based learner <ref type="bibr" coords="5,463.10,151.29,15.27,8.64" target="#b12">[13]</ref>, and set it to learn a linear model, optimizing the ROC area under curve. The regularization parameter λ was set to 0.1, and 100,000 iterations were performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">UvAtrainGen</head><p>Training material for microblog search systems is expensive, and non-trivial to obtain. Therefore we explore the possibility of mining a collection of tweets for training material.</p><p>We use hashtags for this, and are able to generate a pseudo test collection for microblog search. This pseudo test collection consists of a set of timestamped queries, with for each query a set of relevant tweets with a publication date prior to the timestamp. In the next paragraphs we describe how this pseudo test collection was generated. The UvAtrainGen run uses the same LTR pipeline as the UvAtrain2011 run, but it is trained on a generated pseudo test collection. Another difference is that the Indri and Terrier retrieval runs (which are used as features for the LTR run) are now tuned on the pseudo test collection as well.</p><p>Selecting hashtags We selected hashtags that appeared in at least fifty tweets in the TREC 2011 Microblog track collection.</p><p>Generating timestamps For each hashtag, the publication dates of its associated tweets form a time series. We set the timestamp for the hashtag to the time of the first peak in this series. We discard hashtags that have less than fifty hashtags before the timestamp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keeping interesting tweets</head><p>We rank all tweets in our collection that contain one or more hashtags by their interestingness. We think of a tweet as interesting if it could be relevant to a query. We estimate interestingness by learning from tweets that we know were relevant to some query: the union of all relevant tweets for the TREC 2011 Microblog track queries. After ranking the tweets, we keep the top fifty percent of them. We discard hashtags that have less than fifty interesting tweets.</p><p>Generating a query For each hashtag, we compare the set of associated tweets (T h ) with the collection of all tweets (T ). We rank terms by a log-likelihood ratio test statistic. Terms of which the term frequency in T h is most significantly different from the term frequency in T are ranked at the top. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">UvA Microblog results</head><p>The results for our microblog runs are listed in Table <ref type="table" coords="6,268.69,462.71,3.74,8.64" target="#tab_5">6</ref>. We find similar patterns for both relevance levels in that the two learning to rank runs (UvAtrain2011 and UvAtrainGen) perform almost identical. This shows that automatically constructing training data for this task is feasible and that a large set of human-annotated queries and tweets is not required.</p><p>The second finding is that our naive filtering baseline is a good choice when it comes to query expansion. The UvAFilter run itself shows strong performance on early precision (P10), which leads to an improved set of tweets from which to select expansion terms. This in turn results in improved performance of the UvAFilterExp run over both its baseline and thelearning to rank runs on most metrics. We should note, however, that differences are small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we have described the participation of the University of Amsterdam's ILPS group at TREC 2012. We have participated in two tracks, Knowledge Base Acceleration (KBA) and Microblog. For the KBA track, we find that our machine learning-based approaches outperform the baseline. For the Microblog track our main observations are (i) that automatic construction of training data for a learning to rank approach is equally helpful as using a humanannotated dataset, (ii) that strict heuristics-based filtering leads to improved early precision, and (iii) that using this filtered run as basis for query expansion leads to best overall performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,53.80,244.97,502.11,8.64;4,53.80,256.93,37.75,8.64"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Performance changes over a varying cutoff for the TREC KBA IncLearnT10 run with minimum relevance level "central."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.80,58.95,239.10,213.13"><head>Table 1 :</head><label>1</label><figDesc>TF(r e , f d ) Term frequency. POS f (r e , f d ) Position of first occurrence. POS l (r e , f d ) Position of last occurrence. SPR(r e , f d ) Distance between the first and last occurrence. SIM( f d , a e ) Asymmetric similarity between f d and a e . SIM(a e , f d ) Asymmetric similarity between a e and f d . List of features we use. f d refers to the document fields, r e to the representations of e, and a e to the Wikipedia article associated with e; more details can be found in Section 2.1.2.</figDesc><table coords="2,59.78,142.63,227.15,68.73"><row><cell>CTA(e, f d ) Number of times all context is found in</cell></row><row><cell>the document (e.g., "music group" for Ba-</cell></row><row><cell>sic Element (music group).</cell></row><row><cell>CTS(e, f d ) Number of times some context is found</cell></row><row><cell>in the document (e.g., "music" for Ba-</cell></row><row><cell>sic Element (music group).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,316.81,682.90,239.10,22.52"><head>Table 3 :</head><label>3</label><figDesc>Results for TREC KBA, measured by rank based evaluation metrics.</figDesc><table coords="3,53.80,59.22,239.10,253.57"><row><cell>Run</cell><cell>R-Prec</cell><cell>MAP</cell><cell>MRR</cell><cell>P@5</cell><cell>P@10</cell></row><row><cell cols="3">Minimum relevance level "central"</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell cols="5">0.4514 0.4484 0.5766 0.4828 0.5069</cell></row><row><cell>Learning</cell><cell cols="5">0.4445 0.4533 0.7031 0.6207 0.5897</cell></row><row><cell>IncLearnT1</cell><cell cols="5">0.4363 0.4362 0.6449 0.5379 0.5310</cell></row><row><cell>IncLearnT5</cell><cell cols="5">0.4372 0.4411 0.6515 0.5586 0.5103</cell></row><row><cell cols="6">IncLearnT10 0.4317 0.4366 0.5999 0.5724 0.5586</cell></row><row><cell cols="6">UvAbaseline Run based on simple lexical matching, i.e.,</cell></row><row><cell cols="6">only applying the first, recall-oriented step from Sec-</cell></row><row><cell cols="6">tion 2.1.1. This considers all surface forms for e and de-</cell></row><row><cell cols="6">termines whether each document contains any of these</cell></row><row><cell cols="6">surface forms: more frequent matches yield a higher</cell></row><row><cell>score.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">UvALearning Run that builds upon UvAbaseline and ap-</cell></row><row><cell cols="6">plies machine learning using the 2011 training docu-</cell></row><row><cell cols="4">ments to perform predictions.</cell><cell></cell><cell></cell></row></table><note coords="2,487.40,682.90,3.69,6.39;2,494.64,684.83,61.27,8.64;2,316.81,696.78,110.40,8.64;3,53.80,324.80,238.93,9.03;3,73.72,336.97,219.18,8.82;3,73.72,348.78,56.04,8.96"><p><p><p><p>1 </p>All our submitted runs are automatic runs.</p>UvAIncLearnT25 Incremental learning; add the top-k most probable documents for each class at time step i,</p>where k = 25.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,53.80,59.27,502.11,286.82"><head>Table 5 :</head><label>5</label><figDesc>s obesity campaign first, green, lady, weight, ap, loss, major, insurance, rate, role, response, plays, atlanta, rising, oprah, childhood, crescent Example topics and their selected query expansion terms. Topics 55, 86, and 109 improved over the baseline on most metrics, topic 65 dropped in performance.</figDesc><table coords="6,65.87,59.27,401.22,286.82"><row><cell cols="2">Topic Query</cell><cell></cell><cell></cell><cell>Expansion terms</cell></row><row><cell>55</cell><cell cols="3">berries and weight loss</cell><cell>been, fat, lose, amazon, berry, calories, acai, diets</cell></row><row><cell>86</cell><cell cols="3">Joanna Yeates murder</cell><cell>man, jo, charged, accused, remanded, tabak, vincent</cell></row><row><cell>109</cell><cell>Gasland</cell><cell></cell><cell></cell><cell>gas, oscar</cell></row><row><cell cols="3">65 Michelle Obama'Run MAP</cell><cell>P30</cell><cell>P20</cell><cell>P10</cell></row><row><cell cols="5">Depth: 1000, minimum relevance level 1</cell></row><row><cell cols="2">UvAFilter</cell><cell cols="3">0.1658 0.3384 0.3839 0.4627</cell></row><row><cell cols="5">UvAFilterExp 0.1852 0.3627 0.3992 0.4644</cell></row><row><cell cols="5">UvAtrain2011 0.2153 0.3514 0.3847 0.4390</cell></row><row><cell cols="2">UvAtrainGen</cell><cell cols="3">0.2057 0.3475 0.3890 0.4288</cell></row><row><cell cols="5">Depth: 1000, minimum relevance level 2</cell></row><row><cell cols="2">UvAFilter</cell><cell cols="3">0.1385 0.1774 0.2085 0.2780</cell></row><row><cell cols="5">UvAFilterExp 0.1561 0.1932 0.2161 0.2644</cell></row><row><cell cols="5">UvAtrain2011 0.1524 0.1831 0.2076 0.2508</cell></row><row><cell cols="2">UvAtrainGen</cell><cell cols="3">0.1501 0.1836 0.2110 0.2542</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,53.80,362.30,239.10,60.91"><head>Table 6 :</head><label>6</label><figDesc>Results for the TREC 2012 Microblog track tealtime ad-hoc task.The top ten terms from this list form the query for the hashtag h<ref type="bibr" coords="6,75.94,414.57,16.60,8.64" target="#b11">[12]</ref> </figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,331.16,712.12,154.19,6.91"><p>See https://github.com/ejmeij/trec-kba.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,68.14,703.06,109.75,5.63"><p>http://www.lemurproject.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,68.14,712.87,77.23,5.63"><p>http://terrier.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>5 Acknowledgments This research was partially supported by the <rs type="funder">European Union</rs>'s <rs type="programName">ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme</rs>, <rs type="funder">CIP ICT-PSP</rs> under grant agreement nr 250430, the <rs type="funder">European Community</rs>'s <rs type="programName">Seventh Framework Programme</rs> (<rs type="grantNumber">FP7/2007-2013</rs>) under grant agreements nr 258191 (<rs type="programName">PROMISE Network of Excellence</rs>) and <rs type="grantNumber">288024</rs> (<rs type="projectName">LiMoSINe</rs> project), the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project nrs <rs type="grantNumber">612.061.814</rs>, <rs type="grantNumber">612.061.815</rs>, <rs type="grantNumber">640.004.-802</rs>, <rs type="grantNumber">727.011.005</rs>, <rs type="grantNumber">612.001.116</rs>, <rs type="grantNumber">HOR-11-10</rs>, the <rs type="institution">Center for Creation, Content and Technology (CCCT)</rs>, the <rs type="projectName">BI</rs>-<rs type="projectName">LAND</rs> project funded by the <rs type="funder">CLARIN-nl program</rs>, the <rs type="programName">Dutch national program</rs> COMMIT, the <rs type="funder">ESF</rs> <rs type="programName">Research Network Program ELIAS</rs>, the <rs type="funder">Elite Network Shifts</rs> project funded by the <rs type="funder">Royal Dutch Academy of Sciences (KNAW)</rs>, and the <rs type="funder">Netherlands eScience Center</rs> under project number <rs type="grantNumber">027.012.105</rs>. 6 References</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aASZ4jF">
					<orgName type="program" subtype="full">ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_RaJBUSx">
					<idno type="grant-number">FP7/2007-2013</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funded-project" xml:id="_KYYCDJ2">
					<idno type="grant-number">288024</idno>
					<orgName type="project" subtype="full">LiMoSINe</orgName>
					<orgName type="program" subtype="full">PROMISE Network of Excellence</orgName>
				</org>
				<org type="funding" xml:id="_ZxszmTJ">
					<idno type="grant-number">612.061.814</idno>
				</org>
				<org type="funding" xml:id="_Q87Vmfh">
					<idno type="grant-number">612.061.815</idno>
				</org>
				<org type="funding" xml:id="_m9Wcj2h">
					<idno type="grant-number">640.004.-802</idno>
				</org>
				<org type="funding" xml:id="_BQnak26">
					<idno type="grant-number">727.011.005</idno>
				</org>
				<org type="funding" xml:id="_Qbe8ezZ">
					<idno type="grant-number">612.001.116</idno>
				</org>
				<org type="funded-project" xml:id="_YtTqZqr">
					<idno type="grant-number">HOR-11-10</idno>
					<orgName type="project" subtype="full">BI</orgName>
				</org>
				<org type="funded-project" xml:id="_YHaDDAM">
					<orgName type="project" subtype="full">LAND</orgName>
					<orgName type="program" subtype="full">Dutch national program</orgName>
				</org>
				<org type="funding" xml:id="_TbuUjfK">
					<orgName type="program" subtype="full">Research Network Program ELIAS</orgName>
				</org>
				<org type="funding" xml:id="_6guqc3K">
					<idno type="grant-number">027.012.105</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,333.41,581.88,211.41,8.64;6,328.43,593.66,222.66,8.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,522.37,581.88,22.45,8.64;6,328.43,593.84,164.79,8.64">Topic pages: An alternative to the ten blue links</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cucerzan</surname></persName>
		</author>
		<idno>ICSC &apos;10</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.41,612.73,186.60,8.82;6,328.43,624.68,88.28,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,416.41,612.91,61.67,8.64">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,485.30,612.73,34.71,8.59;6,328.43,624.68,34.69,8.59">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.41,643.93,206.50,8.64;6,328.43,655.89,203.41,8.64;6,328.43,667.84,191.91,8.64;6,328.43,679.62,198.94,8.82" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,328.43,655.89,203.41,8.64;6,328.43,667.84,191.91,8.64;6,328.43,679.62,144.66,8.59">Microblog language identification: overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsagkias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.41,698.87,174.02,8.64;6,328.43,710.65,206.70,8.82;7,65.42,57.10,214.43,8.59;7,65.42,69.05,185.68,8.82;7,65.42,81.19,103.76,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,328.43,710.82,113.84,8.64">Quantifying query ambiguity</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,460.87,710.65,74.26,8.59;7,65.42,57.10,214.43,8.59;7,65.42,69.05,82.51,8.59">Proceedings of the second international conference on Human Language Technology Research</title>
		<meeting>the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.40,101.11,213.09,8.64;7,65.42,112.89,187.90,8.82" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m" coord="7,65.42,112.89,143.73,8.59">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.40,132.99,218.29,8.64;7,65.42,144.95,210.65,8.64;7,65.42,156.90,199.72,8.64;7,65.42,168.68,87.70,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,150.17,144.95,125.90,8.64;7,65.42,156.90,199.72,8.64;7,65.42,168.86,26.39,8.64">Generating links to background knowledge: a case study using narrative radiology reports</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sevenster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Van Ommering</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,109.95,168.68,39.22,8.59">CIKM &apos;11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.40,188.78,205.00,8.64;7,65.42,200.74,222.93,8.64;7,65.42,212.69,181.24,8.64;7,65.42,224.65,217.05,8.64;7,65.42,236.42,199.81,8.82;7,65.42,248.56,37.36,8.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,149.80,212.69,96.86,8.64;7,65.42,224.65,217.05,8.64;7,65.42,236.60,105.67,8.64">SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B.-K</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="442" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.40,268.48,199.25,8.64;7,65.42,280.44,211.03,8.64;7,65.42,292.21,209.12,8.82;7,65.42,304.17,215.85,8.59;7,65.42,316.13,105.89,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,150.26,280.44,126.19,8.64;7,65.42,292.39,184.01,8.64">Mapping queries to the Linking Open Data cloud: A case study using DBpedia</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hollink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Huurnink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,257.75,292.21,16.79,8.59;7,65.42,304.17,215.85,8.59;7,65.42,316.13,36.87,8.59">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="418" to="433" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.40,336.23,199.34,8.64;7,65.42,348.01,212.23,8.82" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,65.42,348.18,146.99,8.64">Adding semantics to microblog posts</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<idno>WSDM &apos;12</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,368.11,188.23,8.64;7,65.42,380.07,183.43,8.64;7,65.42,391.84,202.38,8.59;7,65.42,403.80,207.02,8.59;7,65.42,415.75,153.98,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,223.74,368.11,39.87,8.64;7,65.42,380.07,167.23,8.64">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,65.42,391.84,202.38,8.59;7,65.42,403.80,207.02,8.59;7,65.42,415.75,85.07,8.59">Proceedings of the 28th annual international ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th annual international ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,435.86,184.71,8.64;7,65.42,447.81,202.69,8.64;7,65.42,459.59,43.17,8.59" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,226.17,435.86,33.91,8.64;7,65.42,447.81,186.21,8.64">Wikify!: Linking documents to encyclopedic knowledge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,65.42,459.59,39.23,8.59">CIKM &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,479.69,186.22,8.64;7,65.42,491.47,224.62,8.82;7,65.42,503.42,183.33,8.82;7,65.42,515.56,172.13,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,216.77,479.69,44.83,8.64;7,65.42,491.65,132.30,8.64">Comparing corpora using frequency profiling</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rayson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garside</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,215.77,491.47,74.26,8.59;7,65.42,503.42,133.65,8.59">Proceedings of the workshop on Comparing Corpora</title>
		<meeting>the workshop on Comparing Corpora</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,535.48,214.34,8.64;7,65.42,547.44,219.69,8.64;7,65.42,559.21,145.15,8.82" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,65.42,547.44,214.91,8.64">Pegasos: Primal estimated sub-gradient solver for svm</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,76.20,559.21,38.12,8.59">ICML &apos;12</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,579.32,206.25,8.64;7,65.42,591.09,217.34,8.82;7,65.42,603.05,185.04,8.82;7,65.42,615.18,221.67,8.64;7,65.42,627.14,122.60,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,238.45,579.32,43.18,8.64;7,65.42,591.27,139.85,8.64">Credibility improves topical blog post retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,223.16,591.09,59.60,8.59;7,65.42,603.05,109.76,8.82">Proceedings of ACL-08: HLT, page 923931</title>
		<meeting>ACL-08: HLT, page 923931<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
