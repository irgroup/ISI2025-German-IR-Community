<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.17,115.90,325.01,12.90;1,202.73,133.83,209.90,12.90">UT Austin in the TREC 2012 Crowdsourcing Track&apos;s Image Relevance Assessment Task</title>
				<funder ref="#_pEvfCBj">
					<orgName type="full">DARPA Young Faculty</orgName>
				</funder>
				<funder>
					<orgName type="full">Yahoo! Faculty Research and Engagement award</orgName>
				</funder>
				<funder>
					<orgName type="full">Temple Fellowship</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,235.17,172.22,43.45,8.64"><forename type="first">Hyun</forename><surname>Joon</surname></persName>
							<email>hyunjoon@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.29,172.22,60.89,8.64"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.17,115.90,325.01,12.90;1,202.73,133.83,209.90,12.90">UT Austin in the TREC 2012 Crowdsourcing Track&apos;s Image Relevance Assessment Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">26B7A10589545DD1F1C43C3F5CFF50E1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our submission to the Image Relevance Assessment Task (IRAT) at the 2012 Text REtrieval Conference (TREC) Crowdsourcing Track. Four aspects distinguish our approach: 1) an interface for cohesive, efficient topicbased relevance judging and reporting judgment confidence; 2) a variant of Welinder and Perona's method for online crowdsourcing <ref type="bibr" coords="1,349.29,303.03,14.94,7.77" target="#b16">[17]</ref> (inferring quality of the judgments and judges during data collection in order to dynamically optimize data collection); 3) a completely unsupervised approach using no labeled data for either training or tuning; and 4) automatic generation of individualized error reports for each crowd worker, supporting transparent assessment and education of workers. Our system was built start-to-finish in two weeks, and we collected approximately 44,000 labels for about $40 US.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Internet-based crowdsourcing is transforming traditional labor practices for many common data processing tasks <ref type="bibr" coords="1,241.74,440.90,11.38,8.64" target="#b0">[1,</ref><ref type="bibr" coords="1,253.13,440.90,11.38,8.64" target="#b10">11]</ref>. To date, statistical quality assurances to crowdsourcing have predominantly focused on offline label aggregation approaches, where labels are first obtained and only aggregated afterward. While this permits a simple staged approach, with minimal coupling between label collection and aggregation stages it precludes dynamic optimization of labeling effort, meaning the same labeling effort is expended on all examples without regard to varying example difficulty or variable skill of the workforce. In contrast, Welinder and Perona proposed an online approach which integrates label collection with aggregation, dynamically evaluating both aggregated labels and workers as labels are collected <ref type="bibr" coords="1,293.09,536.54,15.27,8.64" target="#b16">[17]</ref>. While under some assumptions the online approach offers no benefit over offline <ref type="bibr" coords="1,287.83,548.50,10.58,8.64" target="#b7">[8]</ref>, in general the greater flexibility of the online approach provides additional opportunities for optimization. We describe a variant of Welinder and Perona's approach which we apply to the IRAT task of collecting image relevance judgments. This enables us to reduce effort on simple examples, dynamically request additional labels for uncertain examples, and identify trusted workers.</p><p>As in Welinder and Perona's approach, we adopt a completely unsupervised method. While supervised methods tend to outperform unsupervised methods for a variety of tasks, including aggregation of noisy crowdsourcing labels, gold data required for supervised training is typically assumed to be expensive to create. This is particularly true in a crowdsourcing scenario, where supervised approaches require constant creation of new gold labels to avoid common forms of online fraud based on worker memorization and sharing of gold answers (when gold is reused across runs instead of being renewed).</p><p>To collect labels via Amazon's Mechanical Turk service, we created a cohesive and cost-effective relevance judging interface which groups together a large set of images needing to be judged for the same search topic. As part of this interface, we also required that workers self-report judgment confidence; while we use this confidence data in our data collection process, primarily the confidence data still awaits further analysis.</p><p>To educate and retain trusted workers, we also experiment with a communication strategy which provides crowd workers with individualized error reports, as well as bonuses tied to a metric in these reports. The individualized error reports include a variety of performance measures and an illustrative visual plot. Again, substantive analysis of these reports and corresponding bonusing policy remains forthcoming.</p><p>The question of appropriate payment for crowdsourcing tasks remains complicated and unresolved. To date, the question has been framed predominantly in terms of how to optimize production (e.g., quality and speed relative to cost). While this simple framing is incomplete, even it presents a host of interacting concerns. A variety of crowdsourcing studies have explored such concerns <ref type="bibr" coords="2,298.96,311.52,15.18,8.64" target="#b9">[10,</ref><ref type="bibr" coords="2,314.14,311.52,7.59,8.64" target="#b2">3]</ref>, and a far larger body of work in other fields (e.g., psychology, economics, business) has considered the general question of appropriate payment beyond the crowdsourcing context. Beyond optimizing production, however, the questions raises other, broader considerations one must wrestle with, spanning ethics, economic sustainability, and legal regulation <ref type="bibr" coords="2,383.96,359.34,11.56,8.64" target="#b6">[7,</ref><ref type="bibr" coords="2,395.52,359.34,11.56,8.64" target="#b13">14,</ref><ref type="bibr" coords="2,407.08,359.34,11.56,8.64" target="#b17">18,</ref><ref type="bibr" coords="2,418.65,359.34,7.71,8.64" target="#b0">1,</ref><ref type="bibr" coords="2,426.35,359.34,7.71,8.64" target="#b3">4]</ref>. Like Irani et al., we do resolve these broader concerns but believe it important to acknowledge them to promote community awareness and provoke ongoing discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image Relevance Assessment Task (IRAT)</head><p>IRAT represents one of the two shared tasks in the 2012 TREC Crowdsourcing Track. The task requires collection of topical, binary relevance judgments for approximately 20K topic-image pairs spanning a total of 89 topics. 69 of these topics are taken from the ImageCLEF 2009 evaluation campaign, with 200 Belga images per topic needing to be judged. The remaining 20 topics come from the ImageCLEF 2012 evaluation campaign, with 300 MIRFLICKR images per topic to be judged. In sum, a total of 69*200 + 20*300 = 19,800 topic-image pairs required judging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relevance Judging Interface</head><p>Figure <ref type="figure" coords="2,164.05,572.75,4.98,8.64" target="#fig_0">1</ref> shows our judging interface for an example topic and image set. We implemented this interface as an external Human Intelligence Task (HIT) hosted on our own webserver, allowing us greater design flexibility than possible using Amazon's predefined internal HIT widgets, with the additional benefit of avoiding fraudulent robot workers built for Amazon's standard interface widgets. Each HIT includes up to 100 images (25 rows of 4 columns) to be judged for a given topic. The topic is described in the top of the HIT page, and each image is displayed at the entry of the table which is composed of 4 columns by 25 rows. Several years ago, we collected crowd judgments for the TREC 2010 Relevance Feedback Track <ref type="bibr" coords="3,201.44,510.15,11.34,8.64" target="#b1">[2,</ref><ref type="bibr" coords="3,212.79,510.15,11.34,8.64" target="#b14">15,</ref><ref type="bibr" coords="3,224.13,510.15,7.56,8.64" target="#b4">5]</ref>. For that task, we designed the relevance judging interface to display 5 documents at a time for a given topic, and we paid roughly a $0.01 per judgment. If we had reused this same design here for our 20K topic-image pairs, collecting only a single judgment per example would have cost roughly $200. For this task, however, we expected that judging image relevance would be easier, faster, and more fun for workers than making text-based relevance judgments, and many image thumbnails can easily be placed on the same page. As a result, our revised HIT design further optimized production for this task by using 100 images per page with $0.05 offered per HIT, or $0.01 per 20 judgments ( 1 20 th the cost of the original design). While we cannot yet comment on judgment accuracy, we can report HITs were still accepted and completed quickly, with satisfied workers to the best of our knowledge.</p><p>Because many images could be easily accommodated on a single, cohesive task page, judges could also see a broad spectrum of images in making relative decisions on relevance, and we expected (but have not yet evaluated) that this would reduce judging effort similar to prior studies showing pair-wise preference judging to be easier than making item-at-a-time absolute relevance judgments. Images could also be judged in any arbitrary order (e.g., one could simply judge images left-to-right, or first find and mark relevant images and then return to mark each remaining image as nonrelevant). In addition to the common crowdsourcing question of how to determine optimal pay (according to some external performance criteria <ref type="bibr" coords="4,364.88,191.04,16.60,8.64" target="#b9">[10]</ref> along with other considerations <ref type="bibr" coords="4,168.85,203.00,14.94,8.64" target="#b13">[14,</ref><ref type="bibr" coords="4,183.79,203.00,7.47,8.64" target="#b3">4]</ref>), it would be interesting to study how exposing judges to more or fewer images in parallel impacts satisfaction, effort, relative judgments, and the judges' own self-developed criteria for deciding topical relevance. We also wonder how workers' different judgment ordering or other assessment practices affect performance metrics.</p><p>In regard to design, we intentionally have no default setting for the radio selection of relevant vs. non-relevant in order to avoid any bias in default behavior and so that equal effort is required for selecting either option <ref type="bibr" coords="4,342.55,275.30,10.58,8.64" target="#b8">[9]</ref>. However, even such a simple design decision opens a variety of questions for further study. For example, with no default option, how does the extra effort involved affect task completion time, worker satisfaction (enjoyment, recruitment, and retention), task pricing, and/or quality of the actual relevance judgments. As a point of contrast, we note CrowdFlower's interface for content moderation (another binary judgment task) sets the dominant class as the default selection, thereby requiring workers to click only on examples of the less frequent class.</p><p>Raykar et al. <ref type="bibr" coords="4,203.82,359.56,16.60,8.64" target="#b11">[12]</ref> measured worker performance via sensitivity (true positive rate) and specificity (1-false positive rate). Following this, Raykar and Yu <ref type="bibr" coords="4,423.52,371.51,16.60,8.64" target="#b12">[13]</ref> proposed the following single "spammer" metric as a function of sensitivity and specificity: A novel aspect of our data collection in general is having workers self-report confidence of each judgment, which obviously has potential to further inform automatic analysis and aggregation of collected labels. This poses a variety of similar questions for further study. For example, collecting these confidence scores requires twice as much clicking for the worker, which introduces similar tradeoffs. How do we balance the tradeoff between simply collecting more judgments without confidence scores vs. collecting fewer judgments and requiring confidence self-reporting? Perhaps most critically, how informative and useful are the self-reported confidence scores once collected? While we make use of these confidence scores in our iterative data collection algorithm, this analysis remains for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluating Workers and Establishing Trust</head><formula xml:id="formula_0" coords="4,202.54,564.86,204.09,23.42">spammer(w) = recall(r) + specif icity(s) -1 √ 2</formula><p>Figure <ref type="figure" coords="4,163.88,596.66,4.98,8.64">2</ref> presents a visualization of this metric with additional explanation. We adopt this as our primary metric for evaluating worker performance, with a parameter minimum threshold for establishing trust. Because our approach is completely unsupervised, workers are evaluated with respect to "pseudo-gold" produced by aggregating judgments of other workers. A worker must also complete a minimum number of judgments on pseudo-gold to become trusted, a simple surrogate for a confidence interval Raykar and Yu's spammer metric for evaluating annotator performance (copied from <ref type="bibr" coords="5,460.43,319.08,13.44,7.77" target="#b12">[13]</ref>). As similarly noted by Ipeirotis <ref type="bibr" coords="5,245.71,330.04,9.52,7.77" target="#b5">[6]</ref>, it is important to distinguish a worker's labeling errors arising from consistent (correctable) biases vs. errors representing unrecoverable noise. In general, we desire worker labels to be strongly correlated to true labels (either directly or inversely; in the latter case we merely flip worker predictions). In contrast, uncorrelated labels offer little value. Accuracy as a metric is also less meaningful when classes (e.g., binary relevant and non-relevant classes) are significantly imbalanced. For example, if only 10% of images are relevant, 90% accuracy can be achieved by simply labeling all images as non-relevant. In contrast with accuracy, Raykar's spammer metric is similar in spirit (though different in implementation) to measuring average recall across classes in order to limit the influence of the dominant class. A perfect worker would be located at [0,1], a perfectly inverse worker at [1,0], and constant workers (all examples assigned to the same class) at [0,0] and <ref type="bibr" coords="5,279.52,439.63,9.71,7.77" target="#b0">[1,</ref><ref type="bibr" coords="5,289.23,439.63,6.47,7.77" target="#b0">1]</ref>, respectively. The spammer score is defined as the distance from the diagonal y = x to a worker's score (1 -s, r), indicating degree of correlation between the worker's labels and the true class assignments.</p><p>to bound our measurement error in estimating the worker's spammer score based on his observed judgments.</p><p>The spammer score and trust also had two other consequences for a worker. Firstly, workers achieving a spammer score of at least 0.80 received a bonus explicitly and proportionately tied to that score. For example, a worker making 1,000 relevance judgments with spammer score of 0.80 earned a bonus of $4 (1, 000 × $0.005 × 0.80). Secondly, only trusted workers received personalized error reports, as shall be later discussed. If both the Jaccard agreement between collected labels and the average judgment confidence exceed parameter thresholds, the aggregate label is then accepted into the "pseudo-gold" set and no additional judgments are requested. However, if the label remains ambiguous and we have not exceeded our per-example labeling budget, we then iteratively: 1) collect an additional label via an open call to known, trusted workers; 2) perform the same test as above; and 3) repeat until the aggregate label is accepted or the labeling budget is exceeded. Trusted workers are identified using a random subset of pseudo-gold examples. For each pseudo-gold example selected, 2k additional labels are requested via an open call to any non-trusted workers. We then evaluate each worker's individual performance on the set of labeled examples given the pseudo-gold labels. If the worker judges a minimum number of examples and achieves a spammer score exceeding a parameter threshold, the worker is then promoted into the set of trusted workers. A labeling effort tradeoff exists between exploration to identify trusted workers (i.e., collecting more labels on pseudo-gold examples), and exploitation to collect more labels for ambiguous examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Online Crowdsourcing Algorithm</head><p>collect labels in a gradual and incremental way. The algorithm is initialized with three empty sets: pseudo-gold labels P , ambiguous examples A, and trusted workers W . The process starts by obtaining an even number (2k) of independent labels L e = {l e 1 , l e 2 , ..., l e 2k } from crowd workers. Next, it measures the level of agreement J(L e ) between obtained labels using the Jaccard similarity coefficient J computed between labels sets (the size of the intersection over the size of their union, as in <ref type="bibr" coords="7,445.76,179.30,14.94,8.64" target="#b15">[16]</ref>). In addition, we leverage self-reported judgment confidence scores reported by workers C e = {c e 1 , c e 2 , ..., c e 2k }. If these two measures (J(L e ), avg(C e )) exceed predefined thresholds, the example e's labels are aggregated by simple majority voting and added to the pseudo-gold set P . Otherwise, the example e is considered ambiguous and more judgments are collected.</p><p>Specifically, we ask our pool of trusted workers for an additional label for the example. We then again test for agreement and promote the example to pseudo-gold if sufficient agreement is observed, and if not, continue to iterate until either agreement is reached or we exceed a annotation budget limit, in which case we give up on the example and output it in the ambiguous set.</p><p>The second part of the online algorithm shown in Figure <ref type="figure" coords="7,378.03,311.43,4.15,8.64">3</ref>(b) is worker evaluation. We measure the performance of a new worker w over the set of pseudo-ground truth labels which is constructed in the previous partition of examples. Thus, this process does not work in the first partition since no pseudo-ground truth is ready. When a pseudoground truth set P is not empty, it measures each new worker w over pseudo-ground truth labels in P . If a worker's performance exceeds a given threshold α per , this worker is qualified for a trusted worker and added into the trusted worker pool W . This process iterates until a set of example partitions are empty or the number of obtained labels for each example exceeds a predefined threshold. Finally, it produces a set of pseudoground truth labels P m along with a pool of trusted workers W m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Personalized Error Reports and Worker Education</head><p>Another unique aspect of our approach was the automatic generation of individualized error reports for each crowd worker. Figure <ref type="figure" coords="7,319.08,488.86,4.98,8.64" target="#fig_4">4</ref> shows an example personalized error report which shows worker accuracy, precision, recall, specificity, and LAM (logistic average mis-classification), along with a ROC plot including Raykar and Yu's spammer score <ref type="bibr" coords="7,159.04,524.72,15.27,8.64" target="#b12">[13]</ref>. Error reports were implemented as an interactive Google Web App Component hosted on our webserver, which workers could inspect and explore to better understand how they were being evaluated and which of their judgments were automatically determined to be incorrect. Only trusted workers were sent these reports, as they were the workers we most wanted to train and retain, and with whom we hoped to foster relationships. We also wished to minimize risks of cheating by other workers, since gold answers disclosed in the error report were sometimes reused later.</p><p>These error reports raise a variety of research questions for which additional analysis is still needed. For example, the current reports are extremely dense with very technical information; how much of this is clearly understood by the workers, and how could we both better pare down what information is included and better convey this information to laymen? One worker wrote, "What's the meaning of this plot?" Another worker wrote, "This one is useful to figure out what I've done." How much attention do workers give such reports and what parts of them are most useful? Finally, how do such error reports actually impact the bottom line (task performance, education and improvement, enjoyment, retention, etc.)?</p><p>Taking a step back, these error reports might be seen as a very simple form of educational assessment with workers as online students. Whereas crowd work today (particularly micro-work) predominantly involves simple tasks and/or using workers' existing skill sets rather than further developing workers' skills, we forsee a not-todistant future in which online education and crowd work converge in exciting ways to deliver more scalable education and integrate vocational practice into educational curriculum <ref type="bibr" coords="8,182.10,240.02,10.58,8.64" target="#b0">[1]</ref>. After all, crowd work inherently involves training (learning) and assessment, as workers often need to acquire new skills to perform new tasks, before or in the midst of performing the actual work. Workers may also polish and refine existing skills while completing more familiar tasks. Requesters must continually engage in quality assurance. Such a training-assessment cycle of work offers potentially exciting synergies with online, education by-doing. For example, DuoLingo (duolingo.com) explores this direction for foreign language learning. This idea can be generalized much further; for example, content generation tasks could be designed to better assess and enhance writing skills. Tracking and mining of work history could support personalized instruction and feedback, as well as recommending new tasks and learning modules. As workers master new skills and are assessed, badges or credentials could document this proficiency so that others can recognize and utilize this enhanced skill set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>The set of all examples (topic-image pairs) was partitioned into four subsets of increasing size; 5%, 15%, 30% and 50%. In order for an aggregate label to be accepted by the system as pseudo-gold, we used a Jaccard similarity coefficient of J &gt;= 0.67 and average judgment-confidence score of 4.0. With regard to worker evaluation, if a worker's spammer score s(w) &gt;= 0.5 over at least 100 judgments, the worker was promoted to trusted status (the threshold 0.5 indicates that a sum of recall r and specificity s should be approximately 1.7. The proposed algorithm obtains the different number of labels for each example until the predefined threshold is achieved. Figure <ref type="figure" coords="8,341.91,644.48,4.98,8.64" target="#fig_6">6</ref> shows the distribution of labels collected per example, showing 80% of examples were accepted as pseudo-gold after only two judgments. Only 1% of examples required more than three judgments before being accepted as pseudo-gold.</p><p>We did not explicitly request worker feedback given our short timeframe but did receive some anecdotal comments which are mildly suggestive of worker satisfaction: "I like this one", "Good job", "Great". In addition to the worker comments on the error report mentioned earlier, another worker asked, "How do I get a bonus?", suggesting the bonusing policy did attract interest but still wasn't sufficiently clear.</p><p>Finally, we give a breakdown of the total cost of our data collection. As discussed earlier, each of our HITs includes 100 images, and we pay $0.05 per HIT (excluding Amazon's 10% overhead rate, which raises actual cost to $0.055 per HIT). $22 was paid for the label collection (44, 000 labels/100 × 0.05) , and $5 was paid for worker evaluation to identify trusted workers (10, 000 labels/100 × 0.05). Finally, we awarded a bonus to 4 trusted workers totaling $10 based on our bonusing policy (judgments per worker w × $0.005 × spammer score s(w)). In total, $22 + $5 + $10 = $37.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Our approach was characterized by four factors: 1) an interface for cohesive, efficient topic-based relevance judging and reporting judgment confidence; 2) a variant of Welinder and Perona's method for online crowdsourcing <ref type="bibr" coords="9,340.71,353.00,15.49,8.64" target="#b16">[17]</ref>; 3) a completely unsupervised approach; and 4) automatic generation of individualized error reports for each crowd worker. Our prototype system was built start-to-finish in two weeks, approximately 44,000 labels were collected for about $40 US, and the experience helped us to identify many interesting research questions which will be fruitful for further study.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,418.40,345.82,8.12;3,134.77,429.71,345.82,7.77;3,134.77,440.67,345.83,7.77;3,134.77,451.63,345.82,7.77;3,134.77,462.59,345.82,7.77;3,134.77,473.55,340.63,7.77;3,134.77,115.83,360.01,287.84"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Our judging interface for the image relevance assessment task. Each crowdsourcing task includes up to 100 images which need to be judged for a given search topic, whose description appears at top. Each judgment requires selecting the relevant or non-relevant option via a radio button, as well as reporting a likert-scale confidence score for each judgment via a drop-down list. Confidence options range from 5 (very confident) to 1 (very unsure). Both the radio button and drop-down list are initialized without a selection to avoid any possible bias in default selection.</figDesc><graphic coords="3,134.77,115.83,360.01,287.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,318.73,345.83,8.12;5,134.77,330.04,345.82,7.77;5,134.77,341.00,345.82,7.77;5,134.77,351.95,345.82,7.77;5,134.77,362.91,345.82,7.77;5,134.77,373.87,345.82,7.77;5,134.77,384.83,345.82,7.77;5,134.77,395.79,345.82,7.77;5,134.77,406.75,345.83,7.77;5,134.77,417.71,345.82,7.77;5,134.77,428.67,345.82,7.77;5,134.77,439.63,345.82,7.77;5,134.77,450.30,345.82,8.06;5,134.77,461.54,212.23,7.77"><head></head><label></label><figDesc>Fig.2. Raykar and Yu's spammer metric for evaluating annotator performance (copied from<ref type="bibr" coords="5,460.43,319.08,13.44,7.77" target="#b12">[13]</ref>). As similarly noted by Ipeirotis<ref type="bibr" coords="5,245.71,330.04,9.52,7.77" target="#b5">[6]</ref>, it is important to distinguish a worker's labeling errors arising from consistent (correctable) biases vs. errors representing unrecoverable noise. In general, we desire worker labels to be strongly correlated to true labels (either directly or inversely; in the latter case we merely flip worker predictions). In contrast, uncorrelated labels offer little value. Accuracy as a metric is also less meaningful when classes (e.g., binary relevant and non-relevant classes) are significantly imbalanced. For example, if only 10% of images are relevant, 90% accuracy can be achieved by simply labeling all images as non-relevant. In contrast with accuracy, Raykar's spammer metric is similar in spirit (though different in implementation) to measuring average recall across classes in order to limit the influence of the dominant class. A perfect worker would be located at [0,1], a perfectly inverse worker at<ref type="bibr" coords="5,332.73,428.67,9.71,7.77" target="#b0">[1,</ref>0], and constant workers (all examples assigned to the same class) at [0,0] and<ref type="bibr" coords="5,279.52,439.63,9.71,7.77" target="#b0">[1,</ref><ref type="bibr" coords="5,289.23,439.63,6.47,7.77" target="#b0">1]</ref>, respectively. The spammer score is defined as the distance from the diagonal y = x to a worker's score (1 -s, r), indicating degree of correlation between the worker's labels and the true class assignments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,632.53,345.82,8.64;5,134.77,644.48,345.82,8.64;5,134.77,656.12,345.82,8.96"><head>Figure 3 Fig. 3 .</head><label>33</label><figDesc>Figure 3 depicts our online data collection algorithm via a flow diagram. Our algorithm has two parts, as shown in the diagram: (a) label collection and (b) worker evaluation. We partition the set of examples E into several subsets of increasing size in order to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,149.71,523.77,330.88,8.64;8,134.77,535.73,345.82,8.64;8,134.77,547.68,345.82,8.64;8,134.77,559.64,345.82,8.64;8,134.77,571.59,345.82,8.64;8,134.77,583.55,345.82,8.64;8,134.77,595.50,345.82,8.64;8,134.77,607.46,345.83,9.33;8,134.77,619.41,88.70,8.64"><head>Figure 5 (</head><label>5</label><figDesc>a) shows the variations of number of pseudo-ground truth labels vs. ambiguous examples over four partitions of the example set in the experiments. The ratios of ambiguous examples across the partitions are very similar regardless of the size of examples in the each step. It varies from 18.7% to 21.5%, which indicates that each step does not affect the ratio of ambiguous examples over steps in this experiment. Next Figure 5 (b) presents the increase of the number of trusted workers over the partitions of the example set. The number of trusted workers increased over the steps based on the threshold (α spam = 0.5). The ratios of trusted workers vs. all workers across the steps vary from 21% ~28%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,134.77,470.24,345.82,8.12;11,134.77,481.54,345.83,7.77;11,134.77,492.50,345.82,7.77;11,134.77,503.46,345.83,7.77;11,134.77,514.42,345.82,7.77;11,134.77,525.38,345.83,7.77;11,134.77,536.34,345.82,7.77;11,134.77,547.30,345.83,7.77;11,134.77,558.26,345.82,7.77;11,134.77,569.21,345.82,7.77;11,134.77,580.17,345.82,7.77;11,134.77,591.13,136.18,7.77;11,163.68,191.80,288.01,263.70"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. A screenshot of an example individualized error report of a given worker's performance (the worker's ID is intentionally occluded in the screenshot). Such error reports are automatically generated for each worker and output as an interactive Google Web App Component that is hosted on our webserver. The top-left panel shows a set of numerical metrics characterizing the worker's performance. The top-right panel shows a ROC plot (recall vs. 1-specificity) visualization. The bottom panel provides a listing of the worker's specific mistakes (based on pseudo-gold aggregate labels. This listing promotes transparent performance assessment and allows workers to report any errors they find. Requesters can easily browse performance across individual workers as well as aggregate statistics. Workers are notified of the URL to their individual error reports via the Mechanical Turk API, though to avoid subsequent cheating on disclosed gold, only trusted workers are notified. Additional security measures are needed to ensure each worker's error report is only accessible to the given worker.</figDesc><graphic coords="11,163.68,191.80,288.01,263.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,134.77,294.60,345.83,8.12;12,134.77,305.91,345.82,7.77;12,134.77,316.87,345.82,7.77;12,134.77,327.83,345.82,7.77;12,134.77,338.79,345.83,7.77;12,134.77,349.75,345.83,7.77"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The stacked bar plot in Figure (a) (left) shows the relative ratio of examples requiring only the minimal two judgments to achieve "pseudo-gold" status vs. "ambiguous" examples requiring more than two judgments to resolve ambiguity in each stage of data collection. Similarly, line plots in Figure (b) (right) show the relative ratio of trusted workers vs. all workers per stage. Since pseudo-gold from Stage 1 is used to identify trusted workers for Stage 2, no trusted workers exist in Stage 1 (total workers for Stage 1 are only omitted due to lack of trusted workers for contrast).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="12,134.77,580.57,345.83,8.12;12,134.77,591.88,345.83,7.77;12,134.77,602.84,345.82,7.77;12,134.77,613.80,345.82,7.77;12,134.77,624.76,267.43,7.77"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The distribution of judgments collected per example. For 80% of examples, two judgments suffice to establish the aggregate label. Other examples were labeled iteratively until either accepted as pseudo-gold (i.e., achieving high agreement between judges and judgment confidence scores) or until the per-example maximum label budget of 5 was reached. Only 1% of examples required more than three judgments before being accepted as pseudo-gold.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9">Acknowledgments</head><p>We thank <rs type="institution">Amazon.com</rs> for their continuing sponsorship of the <rs type="institution">TREC Crowdsourcing Track</rs>, supporting our use of <rs type="affiliation">Mechanical Turk</rs>. The authors' recent industrial experiences at Intelius and CrowdFlower, respectively, also informed our approach. This work was partially supported by <rs type="funder">DARPA Young Faculty</rs> Award No. <rs type="grantNumber">N66001-12-1-4256</rs>, a <rs type="funder">Yahoo! Faculty Research and Engagement award</rs>, and a <rs type="funder">Temple Fellowship</rs>. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of their funding agencies.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pEvfCBj">
					<idno type="grant-number">N66001-12-1-4256</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.61,580.73,337.98,7.77;9,150.95,591.53,329.64,7.93;9,150.95,602.49,315.09,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,181.00,591.69,88.90,7.77">The future of crowd work</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">N</forename><surname>Aniket Kittur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Horton</surname></persName>
		</author>
		<idno>ID: 2190946</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,285.75,591.53,194.85,7.73;9,150.95,602.49,48.37,7.73">ACM Conference on Computer Supported Cooperative Work (CSCW)</title>
		<imprint>
			<date type="published" when="2013-02">February 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,613.43,337.98,7.77;9,150.95,624.23,329.64,7.93;9,150.95,635.35,20.17,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,304.25,613.43,176.34,7.77;9,150.95,624.39,63.41,7.77">Overview of the TREC 2010 Relevance Feedback Track (Notebook)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,235.29,624.23,241.24,7.73">The Nineteenth Text Retrieval Conference (TREC 2010) Notebook</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,646.13,337.98,7.77;9,150.95,656.92,243.83,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,305.92,646.13,174.67,7.77;9,150.95,657.08,25.76,7.77">Whats the right price? pricing tasks for finishing on time</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Faridani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,193.28,656.92,174.99,7.73">Proc. of AAAI Workshop on Human Computation</title>
		<meeting>of AAAI Workshop on Human Computation</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,119.80,337.98,7.93;10,150.95,130.76,160.91,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,270.51,119.96,187.09,7.77">Amazon Mechanical Turk: Gold mine or coal mine?</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,460.67,119.80,19.92,7.73;10,150.95,130.76,77.25,7.73">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="413" to="420" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,141.88,337.98,7.77;10,150.95,152.68,329.64,7.93;10,150.95,163.64,199.00,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,240.02,141.88,240.57,7.77;10,150.95,152.84,15.42,7.77">Crowdsourcing Document Relevance Assessment with Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,182.59,152.68,298.00,7.73;10,150.95,163.64,114.32,7.73">Proceedings of the NAACL-HLT Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL-HLT Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,174.76,337.98,7.77;10,150.95,185.55,319.10,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,287.76,174.76,177.30,7.77">Quality management on amazon mechanical turk</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.95,185.55,243.21,7.73">Proceedings of the ACM SIGKDD workshop on human computation</title>
		<meeting>the ACM SIGKDD workshop on human computation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,196.67,337.98,7.77;10,150.95,207.47,329.64,7.93;10,150.95,218.43,95.14,7.93" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,258.46,196.67,222.13,7.77;10,150.95,207.63,45.61,7.77">Turkopticon: Interrupting worker invisibility in amazon mechanical turk</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,213.45,207.47,267.14,7.73;10,150.95,218.43,68.90,7.73">Proceeding of the Annual ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>eeding of the Annual ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,229.55,337.98,7.77;10,150.95,240.35,173.00,7.93" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,270.18,229.55,210.41,7.77;10,150.95,240.51,26.81,7.77">Budget-optimal task allocation for reliable crowdsourcing systems</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1110.3564</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,251.31,337.98,7.93;10,150.95,262.27,329.64,7.73;10,150.95,273.39,78.70,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,257.77,251.47,174.92,7.77">Crowdsourcing user studies with mechanical turk</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,448.13,251.31,32.46,7.73;10,150.95,262.27,325.81,7.73">Proceeding of the 26th Annual ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>eeding of the 26th Annual ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,284.18,338.35,7.93;10,150.95,295.14,169.10,7.93" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,239.39,284.34,181.33,7.77">Financial incentives and the performance of crowds</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,427.17,284.18,53.42,7.73;10,150.95,295.14,85.22,7.73">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,306.26,338.35,7.77;10,150.95,317.06,329.64,7.93;10,150.95,328.18,89.56,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,251.32,306.26,226.24,7.77">Human computation: a survey and taxonomy of a growing field</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,160.19,317.06,294.52,7.73">Proceedings of the 2011 annual conference on Human factors in computing systems</title>
		<meeting>the 2011 annual conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1403" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,339.14,338.35,7.77;10,150.95,349.94,252.72,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,427.86,339.14,52.73,7.77;10,150.95,350.10,23.92,7.77">Learning from crowds</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,182.09,349.94,138.96,7.73">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,361.06,338.35,7.77;10,150.95,371.85,267.14,7.93" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,245.67,361.06,234.92,7.77;10,150.95,372.02,47.94,7.77">Eliminating spammers and ranking annotators for crowdsourced labeling tasks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,205.49,371.85,138.96,7.73">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="491" to="518" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,382.81,338.35,7.93;10,150.95,393.77,235.70,7.93" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,285.76,382.81,194.83,7.93;10,150.95,393.77,38.90,7.73">Ethics and tactics of professional crowdwork. XRDS: Crossroads</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,195.98,393.77,115.81,7.73">The ACM Magazine for Students</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,404.73,338.35,7.93;10,150.95,415.69,329.64,7.73;10,150.95,426.81,20.17,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,236.31,404.89,195.32,7.77">Semi-supervised consensus labeling for crowdsourcing</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,448.13,404.73,32.46,7.73;10,150.95,415.69,325.86,7.73">Proceedings of the ACM SIGIR 2nd Workshop on Crowdsourcing for Information Retrieval (CIR)</title>
		<meeting>the ACM SIGIR 2nd Workshop on Crowdsourcing for Information Retrieval (CIR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,437.77,338.35,7.77;10,150.95,448.57,245.96,7.93" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,200.81,437.77,279.78,7.77;10,150.95,448.73,14.15,7.77">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,171.86,448.57,140.42,7.73">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="697" to="716" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,459.69,338.35,7.77;10,150.95,470.48,329.64,7.93;10,150.95,481.60,47.07,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,255.61,459.69,224.98,7.77;10,150.95,470.65,53.40,7.77">Online crowdsourcing: Rating annotators and obtaining costeffective labels</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,222.83,470.48,229.84,7.73">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,492.40,338.35,7.93;10,150.95,503.36,329.64,7.73;10,150.95,514.32,103.05,7.93" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,250.19,492.56,195.53,7.77">Look before you leap: Legal pitfalls of crowdsourcing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,464.56,492.40,16.03,7.73;10,150.95,503.36,329.64,7.73;10,150.95,514.32,76.46,7.73">Proceedings of the 74th Annual Meeting of the American Society for Information Science and Technology (ASIS&amp;T)</title>
		<meeting>the 74th Annual Meeting of the American Society for Information Science and Technology (ASIS&amp;T)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
