<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.80,143.92,430.66,15.12;1,239.02,165.84,132.20,15.12">ISTI@TREC Microblog track 2012: real-time filtering through supervised learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.36,199.74,85.13,10.48"><forename type="first">Giacomo</forename><surname>Berardi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Istituto di Scienza e Tecnologie dell&apos;Informazione Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.57,199.74,65.33,10.48"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Istituto di Scienza e Tecnologie dell&apos;Informazione Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.61,199.74,101.28,10.48"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Istituto di Scienza e Tecnologie dell&apos;Informazione Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.80,143.92,430.66,15.12;1,239.02,165.84,132.20,15.12">ISTI@TREC Microblog track 2012: real-time filtering through supervised learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF0BBDB64B34A6B8E0F4F5C28D5CEC7F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our approach to the microblog filtering task is based on learning a relevance classifier from an initial training set of relevant and non relevant tweets, generated by using a simple retrieval method. The classifier is then retrained using the (simulated) user feedback collected during the training process, in order to improve its accuracy as the filtering process goes on. In the official runs the system scored low effectiveness values, suffering a strong imbalance toward recall.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Microblogging is a form of personal content sharing that derives from blogging, and it has a focus on concision, space and time locality, and social network-style interactions <ref type="bibr" coords="1,479.93,446.90,10.91,9.57" target="#b2">[3]</ref>. This is the second year that microblogging is the subject of a TREC track. For this edition two tasks have been proposed to the participants: Real-time Adhoc search (equivalent to the task in the 2011 track) and Real-time Filtering<ref type="foot" coords="1,337.62,485.59,4.23,6.99" target="#foot_0">1</ref> . Twitter<ref type="foot" coords="1,388.26,485.59,4.23,6.99" target="#foot_1">2</ref> is currently the dominant platform for microblogging, and it has been selected as the source of data for the microblog track.</p><p>Twitter enforces a strong policy on short messages, allowing only a maximum length of 140 characters <ref type="foot" coords="1,158.10,539.79,4.23,6.99" target="#foot_2">3</ref> . In addition to simple text, a tweet can contain three types of references:</p><p>• URLs to external content. External links are typically used to extend a tweet with other media, e.g., an image shot by the user with her mobile phone, or to point to a web content the tweet comments upon.</p><p>• Mention of names of other Twitter users, e.g., "@aesuli ". This is a way to credit the mentioned user for the content of the tweet or a way to establish an exchange of public messages other user may be interested to read and join the discussion.</p><p>• hashtags: a hashtag is defined by any string prefixed with a #, e.g., "#whyalwaysme", "#iwould ". The string can be a single word, an acronym, or multiple words joined together, and usually identifies the subject topic of the tweet (e.g., "#SPIRE2011 ") or expresses a comment about it (e.g., "#epicwin").</p><p>In this edition of the track we have participated to the Real-time Filtering task. The methodology for this task is the same of the adaptive filtering described in <ref type="bibr" coords="2,449.27,241.78,10.91,9.57" target="#b6">[7]</ref>. In the case of tweets, it simulates a scenario in which the system receives feedback from the user, about the relevance of tweets to a topic, so it can recalibrate the filter and improve the results. For each topic a time range is defined, the tweets are checked sequentially, starting from the oldest, in chronological order. No information is given to the system, unless it decides to classifies as relevant a tweet, then it can know the true relevance of it. No training data is available to the system, relevance judgment of a tweet can be accessed only in the filtering phase, after the system has added it to the results.</p><p>In our participation we have explored the use of a classification system in which a supervised learning model is retrained with a new example (tweet), each time this is classified as relevant to the filtering topic. In Section 2 we describe the Tweets2011 corpus; we have reused the data which was already downloaded for the 2011 microblog track. In Section 3 we describe the system developed for the real-time filtering task; we have combined a retrieval module, similar to the one used in the 2011 microblog track, and a supervised learning module. In Section 4 we show the evaluations of two runs of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Tweets2011 collection</head><p>The microblog track of this year reuses the Tweets2011 Corpus <ref type="bibr" coords="2,390.06,490.19,11.52,9.57" target="#b3">[4]</ref> of the previous year. We have thus reused the data we downloaded last year using the downloader provided by the organizers. Table <ref type="table" coords="2,175.20,517.29,5.45,9.57" target="#tab_0">1</ref> reports some statistics on the full corpus and on the various subsets we have defined in order to build our indexes, which are described in Section 3.</p><p>We have indexed only the tweets in English, filtering out non-English tweets. We have used a language recognition system that we have implemented along the lines of <ref type="bibr" coords="2,491.71,557.94,10.91,9.57" target="#b1">[2]</ref>, in order to recognize English tweets. The English set, the only part we have actually indexed, resulted to be roughly a quarter of the entire corpus. We have then identified two subsets of the English set: a Hashtag set that contains only English tweets that have hashtags in them, and a Link set that contains only English tweets with URLs in them. The two subsets show a similar ratio of tweets with at least one hashtag/URL with respect to tweets without hashtag/URL (1:5.6 for hashtags, 1:6. 3 The adaptive filtering system</p><p>Our system, named CipCipPy<ref type="foot" coords="3,233.11,275.88,4.23,6.99" target="#foot_3">4</ref> , is an indexing and retrieval system based on the Whoosh<ref type="foot" coords="3,515.72,275.88,4.23,6.99" target="#foot_4">5</ref> IR library, written in Python <ref type="bibr" coords="3,233.49,291.38,10.91,9.57" target="#b0">[1]</ref>. In this edition of the Microblog track we have extended it with to filtering task. The source code of CipCipPy is available for download at http: //hlt.isti.cnr.it/cipcippy/.</p><p>The adaptive filtering task requires to decide, under real time processing constraints, if a tweet is relevant for a given query. We tackled it as a binary classification task, exploring the use of a supervised learning classification approach to the problem. We used a Naive Bayes classifier in order to have a quick training and classification time.</p><p>The Naive Bayes classifier requires a training set to learn on, composed by tweets that are examples of relevant or non-relevant content with respect to the topic. Given that at the beginning of the retrieval process there are no examples available, our system initially filters relevant tweets, while bootstrapping a training set, using the adhoc retrieval module of CipCipPy <ref type="bibr" coords="3,152.97,440.42,11.52,9.57" target="#b0">[1]</ref> (see Section 3.1).</p><p>Once the training set reaches a reasonable size (the size is a parameter of the system) that allows to train the classifier the bootstrap phase ends. The Naive Bayes classifier is then trained and it is used in the classification phase. During this phase the classifier is used to filter the tweet stream, and it is retrained any time a filtered tweet is considered to be relevant, adding such tweet to the training set with the proper relevance label assigned according to the true relevance judgment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ad Hoc retrieval module</head><p>We have built indexes of the English set for the first retrieval phase. There is an index specific to each topic, i.e, containing only the tweets prior to oldest known relevant tweet for the topic (pre-topic index ), according to the relevance judgements produced by the 2011 evaluation. These indexes, built in this way, do not contain future evidence (e.g. document frequencies computed on the tweets to be filtered in real-time). We have not indexed retweets, since the guidelines and discussions in the mailing list of the track stated that retweets would be considered not relevant by default.</p><p>We have performed a retrieval phase in order to obtain an initial training set of relevant and non relevant examples. We have used the simple IDF as the weighting function in order to compute the retrieval score of the tweets. Including the term frequency component into the weighting could be not well suited for the task, given the short and relatively compact distribution of the text lengths.</p><p>An initial set of p relevant examples for each topic have been retrieved searching on the index of the English set. The queries have been formulated, from the original topic title, as the disjunction of the conjunction of every possible pair of terms appearing in the topic title, plus each single term. The top-p retrieved tweets have been selected to be positive examples in the training set.</p><p>Non relevant examples have been retrieved from the same index. In order to obtain a varied set of tweets that are non relevant to the topic, we have issued a query that is the negation of the disjunction of all the terms in the topic title. The top-n tweets in the ranking have been selected as negative examples for the initial training set.</p><p>Although the fact of searching for positive training examples in a set of tweets that by definition are likely to not contain relevant tweet may seem counterintuitive, note that we leave the determination of the p and n values to a set of parameter validation experiments on the validation topics (see Section 4).</p><p>During the initial phase of the filtering process, the query used to retrieve the positive examples from the pre-topic indexes is used to filter tweet, until the classification module is ready to be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Machine learning based filtering</head><p>The ad hoc retrieval module works on a part of the tweet corpus that is antecedent to the time span that is relevant to the filtering process. Only its query formulation contributes directly to the filtering process. The actual filtering system is composed by two modules, the bootstrap module and the classification module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bootstrap module</head><p>The bootstrap module carries on the initial part of filtering process while the classifier is not yet trained, at the same time it contributes, along with the ad hoc retrieval module, to the definition of the initial training set for the classifiers. As described in the previous section, this module considers relevant any tweet that matches the query used to retrieve the positive examples from the pre-topic indexes.</p><p>Following the guidelines on the simulation of relevance feedback, for all the tweets considered relevant by the bootstrap module the relevance judgments are checked. They are then added to the training set, with their real relevance category. This process is repeated until b truly relevant tweets are found, i.e., until a minum number of positive examples are identified. The bootstrap module then stops and the filtering task is continued by the classification module, trained on the train set obtained by merging the training sets generated by the ad hoc retrieval and the bootstrap module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification module</head><p>The classification module classifies one tweet at the time, following their temporal order. When the classifier classifies a tweet as relevant, the real relevance is checked and the tweet is added to the training set with its true relevance class. After the addition the classifier is re-trained with the new training set, obtaining a new, updated classifier.</p><p>In our system we have used the Scikit-learn <ref type="bibr" coords="5,311.61,172.24,11.52,9.57" target="#b4">[5]</ref> implementation of the Multinomial Naive Bayes classifier.</p><p>Obviously, the method can be instantiated with almost any learning algorithm, just taking into account the required balance between the potential accuracy of the resulting classifier and the cost, in time, necessary to train it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Features</head><p>As features for the Naive Bayes classifier we have used the bag of words of the tweet status from the English set, plus some more tweet-specific features:</p><p>Linked page titles Words composing the title of the pages linked by a tweet (i.e., the text included in the &lt;title&gt; tag of the linked html Web page) have been added to the bag of words. The information contained in the linked page can contribute to the relevance of a tweet with respect to a query. For example, a tweet can be composed by a sentence expressing an opinion and a URL that links to a news, and the tweet becomes relevant because the content linked by the URL gives a proper, relevant, context to the opinion. On the opposite side, the whole content of a Web page can be misleading, due to the possibility of including unrelated information (e.g., navigational information, advertisement). For this reason we chose to include only the title of the Web page.</p><p>Hash tag splitting As we did in <ref type="bibr" coords="5,257.93,442.26,10.91,9.57" target="#b0">[1]</ref>, in addition to the words of the tweet, we have used a hashtag splitter to split the compound words representing the hashtags in common English words. We have improved the Viterbi-based splitting model feeding it with a dataset larger than the one used in <ref type="bibr" coords="5,186.90,482.91,10.91,9.57" target="#b0">[1]</ref>. We have used the Google N-grams collection <ref type="foot" coords="5,413.51,480.96,4.23,6.99" target="#foot_5">6</ref> , taking the frequency of words from the English One Million collection of Google books from years 1999 to 2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named entities</head><p>We used a named entity recognizer specifically devised for tweets <ref type="bibr" coords="5,505.91,525.28,10.91,9.57" target="#b5">[6]</ref>, to mark the presence in tweets of named entities of various types. In addition to the classic named entities i.e. "Person", "Location", "Organization", this system extracts other entities such as "Music Band", "Product", "Movie", "Sports Team" and "TV show". The rational behind this feature is that a tweet that does not contain the same type of named entities that (statistically) characterize the positive examples (i.e., the relevant tweets met during the filtering process until the actual time of the examined tweet) is likely a not relevant tweet for the topic under examination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>For the official runs we have optimized the system parameters (p, n, b) by performing a validation phase using the topics indicated in the track guidelines (MB1, MB6, MB11, MB16, MB21, MB26, MB31, MB36, MB41, and MB46). The optimization has been based on the measure of F 1 (this choice was made before knowing the official evaluation measures, which included F 0.5 instead of F 1 ). We have explored all the combinations of values {0, 5, 10, 15, 20, 50, 100} for the three parameters. We have achieved the best validation scores for the configuration p = 0, n = 100, b = 10. This means that considering as positive examples even few relevant tweets retrieved prior to the oldest known relevant tweet has a negative impact on performance. Adding negative examples has instead a positive effect on the filtering process.</p><p>We have submitted two runs, one using external information (i.e., features from link titles and named entities), and one not using it. They are called nemisExt and nemisNotExt respectively. In Table <ref type="table" coords="6,196.32,302.39,5.45,9.57" target="#tab_1">2</ref> evaluations are showed, using the official evaluation measures; the runs are compared with best and median results, averaged on the topics. The obtained results are well below the median results of the track, except for recall. The system in fact resulted to be strongly imbalanced toward judging tweets as relevant. As expected, the use of external information improved the recall, though in this poor performace situation it is hard to determine the significance of the improvement. The extreme imbalance between recall and precision is mainly generated by the query formulation we adopted in the bootstrapping phase. Moreover, making an a posteriori analysis of the system components we also identified a design error, dictated by efficiency goals in the choice of the learning algorithm. The Naive Bayes classifier uses prior knowledge on the probability of positive cases to take its classification decisions. By limiting, in the validation experiments, the value of n to a maximum value of 100, we have limited the prior probability of positive cases to a minimum value of min b+p n = 5 100 , when such probability is much lower (on average on topics, about 1 113000 ). The fact that the p = 10 configuration has been selected instead of the p = 5 one is likely due to the fact that five (positive examples) is a really low absolute value, and doubling that value produces an improvement that easily counterweights any other negative effect related to prior probabilities. Although one can think about exploring the use of higher values for the n parameters, e.g., n = 113000, this is not a flexible solution with respect to possible fluctuations in the class prior distribution with respect to different topics and also requires a validation set that is representative of the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future work</head><p>Our system did not perform well on the task, for two reasons: (i) the poor performance of the ad hoc retrieval system used in the bootstrapping phase, (ii) the choice of a supervised learning algorithm that relies on the prior probabilities from the training set. With respect to the first issue we plan to investigate the performance of the system when using one of the top-performing retrieval systems from the ad hoc retrieval task for the bootstrap phase. With respect to the second issue we plan to test the use other learning algorithms, such as SVMs, which do not use the prior probability of the class, in the classification phase.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,311.89,639.23,59.42,9.57"><head>Table 1 :</head><label>1</label><figDesc>Some statistics from the corpus and the subsets we have selected for indexing. The total number of tweets is divided in effective tweets (http status 200), retweets (http status 302) and null tweets (http status 301/403/404). The hashtags column indicates the number of unique hashtags.</figDesc><table coords="2,311.89,639.23,59.42,9.57"><row><cell>6 for URLs).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,172.05,394.10,266.15,90.35"><head>Table 2 :</head><label>2</label><figDesc>Evaluations of the runs</figDesc><table coords="6,172.05,394.10,266.15,67.35"><row><cell></cell><cell cols="2">Precision Recall F 0.5</cell><cell>Utility</cell></row><row><cell>nemisExt</cell><cell>0.0293</cell><cell cols="2">0.4433 0.0343 0.0140</cell></row><row><cell>nemisNotExt</cell><cell>0.0315</cell><cell cols="2">0.4232 0.0370 0.0140</cell></row><row><cell>Median Average</cell><cell>0.1766</cell><cell cols="2">0.3343 0.1491 0.2076</cell></row><row><cell>Best Average</cell><cell>0.9224</cell><cell cols="2">0.9462 0.6073 0.5967</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,106.39,613.08,282.93,7.47"><p>https://sites.google.com/site/microblogtrack/2012-guidelines</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,106.39,624.04,84.73,7.47"><p>http://twitter.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,106.39,634.35,414.07,7.86;1,89.80,645.31,430.65,7.86;1,89.80,656.27,147.25,7.86"><p>Twitter text length limitation derives from the original possibility of sending/receiving tweets on cell phones via SMS, which have a maximum length of 160 characters, with 20 characters reserved by Twitter for the id of the author of the tweet.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,106.39,647.41,414.07,7.86;3,89.80,658.37,37.89,7.86"><p>"Cip Cip" is the Italian word for the sound of birds, while "Py" identifies the Python programming language.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,106.39,669.97,216.54,7.47"><p>https://bitbucket.org/mchaput/whoosh/wiki/Home</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,106.39,641.20,170.46,7.86"><p>http://books.google.com/ngrams/datasets</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,106.77,302.21,413.68,9.57;7,106.77,315.76,413.68,9.57;7,106.77,329.30,368.39,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,401.50,302.21,118.95,9.57;7,106.77,315.76,390.57,9.57">ISTI@ TREC Microblog track 2011: exploring the use of hashtag segmentation and text quality ranking</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Berardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,106.77,329.30,334.94,9.57">Proceedings of the Twentieth Text REtrieval Conference (TREC 2011)</title>
		<meeting>the Twentieth Text REtrieval Conference (TREC 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.77,350.73,413.69,9.57;7,106.77,364.28,413.68,9.57;7,106.77,377.83,99.46,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,269.12,350.73,160.49,9.57">N-Gram-based text categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,466.07,350.73,54.38,9.57;7,106.77,364.28,409.14,9.57">Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</title>
		<meeting>SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.77,399.25,413.69,9.57;7,106.77,412.80,413.68,9.57;7,106.77,426.35,413.69,9.57;7,106.77,439.90,197.31,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,303.11,399.25,217.34,9.57;7,106.77,412.80,109.69,9.57">Why we twitter: understanding microblogging usage and communities</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,242.49,412.80,277.96,9.57;7,106.77,426.35,379.79,9.57">Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 workshop on Web mining and social network analysis, WebKDD/SNA-KDD &apos;07</title>
		<meeting>the 9th WebKDD and 1st SNA-KDD 2007 workshop on Web mining and social network analysis, WebKDD/SNA-KDD &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.77,461.32,413.68,9.57;7,106.77,474.87,413.69,9.57;7,106.77,488.42,413.69,9.57;7,106.77,501.97,219.12,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,505.90,461.32,14.55,9.57;7,106.77,474.87,156.64,9.57">On building a reusable twitter corpus</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mccullough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,285.00,474.87,235.46,9.57;7,106.77,488.42,377.66,9.57">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;12</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1113" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.77,523.39,413.68,9.57;7,106.77,536.94,413.68,9.57;7,106.77,550.49,413.68,9.57;7,106.77,564.04,285.55,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,314.70,550.49,199.09,9.57">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,106.77,564.04,182.42,9.57">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.77,585.46,413.69,9.57;7,106.77,599.01,413.69,9.57;7,106.77,612.56,195.63,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,272.39,585.46,248.07,9.57;7,106.77,599.01,23.79,9.57">Named entity recognition in tweets: An experimental study</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,151.33,599.01,369.12,9.57;7,106.77,612.56,47.63,9.57">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.77,633.98,413.69,9.57;7,106.77,647.53,413.69,9.57;7,106.77,661.08,413.68,9.57;7,106.77,674.63,57.28,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,262.54,633.98,233.30,9.57">Building a filtering test collection for trec 2002</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,106.77,647.53,413.69,9.57;7,106.77,661.08,226.36,9.57">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR &apos;03</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
