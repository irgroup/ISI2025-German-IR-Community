<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,76.85,72.35,456.01,16.84">Using Stream Features for Instant Document Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.91,118.05,79.74,11.06"><forename type="first">Andreas</forename><surname>Bauer</surname></persName>
							<email>andreas.bauer@extern.ur.de</email>
							<affiliation key="aff0">
								<orgName type="department">Media Informatics Group</orgName>
								<orgName type="institution">University of Regensburg</orgName>
								<address>
									<settlement>Regensburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.16,118.05,77.54,11.06"><forename type="first">Christian</forename><surname>Wolff</surname></persName>
							<email>christian.wolff@ur.de</email>
							<affiliation key="aff1">
								<orgName type="department">Media Informatics Group</orgName>
								<orgName type="institution">University of Regensburg</orgName>
								<address>
									<settlement>Regensburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,76.85,72.35,456.01,16.84">Using Stream Features for Instant Document Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">215164B879ACC666A231EDFA22722509</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Search process information filtering</term>
					<term>event processing</term>
					<term>web2.0</term>
					<term>text streams</term>
					<term>real-time search</term>
					<term>tf/idf</term>
					<term>okapi</term>
					<term>stream features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we discuss how event processing technologies can be employed for real-time text stream processing and information filtering in the context of the TREC 2012 microblog task. After introducing basic characteristics of stream and event processing, the technical architecture of our text stream analysis engine is presented. Employing wellknown term weighting schemes from document-centric text retrieval for temporally dynamic text streams is discussed next, giving details of the ESPER Event Processing Agents (EPAs) we have implemented for this task. Finally, we describe our experimental setup, give details on the TREC microblog runs as well as the result thereafter with our system including some extensions and give a short interpretation of the evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Due to the rapid growth in user-generated digital content, data stream processing has received increasing scholarly attention. Most of this content is textual, thus investigating how to effectively rank real-time text streams is an interesting research question.</p><p>In this paper we present an event-based approach to realtime information filtering as well as our results created for the microblog real-time filtering task for TREC 2012. In addition, we discuss improved results, which we have achieved</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">EVENT-BASED INFORMATION FIL-TERING</head><p>The basic idea of splitting and mapping text streams onto semantically distinct event types has already been presented in <ref type="bibr" coords="1,328.31,308.89,9.20,7.86" target="#b0">[1]</ref>. In short, the approach here is to feed an incoming tweet into a network of event processing agents that immediately execute the analysis and provide an instant ranking of the tweet. This is possible because modern event processing engines like Esper<ref type="foot" coords="1,409.18,348.96,3.65,5.24" target="#foot_0">1</ref> , Tibco BusinessEvents<ref type="foot" coords="1,508.98,348.96,3.65,5.24" target="#foot_1">2</ref> or Drools Fusion<ref type="foot" coords="1,344.20,359.42,3.65,5.24" target="#foot_2">3</ref> support high-speed processing of events.</p><p>More recently a streaming version for the big data framework Hadoop has been released <ref type="foot" coords="1,442.51,380.35,3.65,5.24" target="#foot_3">4</ref> and Twitter has published its real-time streaming system Storm<ref type="foot" coords="1,467.64,390.81,3.65,5.24" target="#foot_4">5</ref> . In addition, the S4 distributes streaming platform, originally published by Yahoo, is now an incubator project supported by the Apache Foundation as well. <ref type="foot" coords="1,398.24,422.19,3.65,5.24" target="#foot_5">6</ref> . All these developments show that event processing is still needed and is consider as a viable and elementary part of the efficient processing of large amounts of data. Big Data and event processing are no mutually exclusive concepts but rather complementary, where event processing addresses interesting goals in analysing large amount of data by offering features like sliding windows or pattern matching.</p><p>It is quite obvious that streaming and event processing offer major opportunities for analysing text streams in realtime and that the industry is not only focusing on increasing the speed of analysing large amount of data. While the event processing paradigm as proposed by David Luckham <ref type="bibr" coords="1,521.08,549.48,14.00,7.86" target="#b8">[9]</ref> originally focussed on business applications, the applicability for information retrieval tasks has been recognized in more recent work <ref type="bibr" coords="1,353.94,580.87,75.50,7.86">[3, p.10][10, p. 42</ref>]. We have used Esper as our event processing engine of choice, because it is open source, offers good online support and allows for a straightforward integration of high volume text stream analysis. Some of the advantages of an event processing approach are:</p><p>1. Clear semantics: Due to the real-time scenario each digital utterance has to be considered in its temporal context. The sheer amount of information constantly generated raises the probability of an information being lost, overlooked or ignored increases as time passes by. The event metaphor is well suited for this type of informational scenario because it stresses the temporal aspect.</p><p>2. Interoperability: From a design point of view the mapping of text onto event types allows for easy combination of events from different sources. E.g. if Facebook status updates and Tweets are mapped onto the same basic event type like Token Event, Location Event, or Sentiment Event, cross data stream analysis can easily be performed, because the same event processing agents (EPAs) as well as the same event processing network (EPN) can be used.</p><p>3. Technical integration: Many current big data or largescale analysis systems rely on exploiting the temporal nature of information. Thus, they are designed to work with events that can be analysed in a temporal manner by providing temporal meta-information, e.g. detection time, creation time or expiry time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TECHNICAL ARCHITECTURE</head><p>Figure <ref type="figure" coords="2,92.92,366.29,4.61,7.86" target="#fig_0">1</ref> shows the overall technical architecture of the system we have used for our experiments. Mircoblogs are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Language detection</head><p>After the conversion of raw Tweets into T weetEvents, the latter are fed into the event processing network using the ESPER event processing engine. First, each tweet is sent to an event processing agent that splits up the text of the tweet and maps it onto semantically distinct event types. Then these events are processed by several other Event Processing Agents, that calculate stream statistics like average text length, count of distinct tokens, term counts, etc.</p><p>Important to notice are the SearchP rof ileEP As that were created for every TREC topic. Within these EPAs the filtering was done which was based on different ranking schemes. Important to fulfil the real-time requirement the amount of Tweets that should be examined was decrease by a first filtering step that prevent many Tweets from entering the real filtering stage. This first filtering step has major influence on the recall of the system, because it acts as a gatekeeper to the filtering processes. Hence a strict filter condition increases precision, but lowers recall for instance. For the runs we present in this paper the primary filtering rule is shown in condition 1</p><formula xml:id="formula_0" coords="2,325.89,344.66,230.03,7.86">F ilter = {x|qm ≥ 1 ∨ stl = 1 ∨ (qm &gt; 0 ∧ f m &gt; 1)} (1)</formula><p>With qm being a term match of a topic term and a term in the Tweet, stl being a search profile with only one search term and f m being a feedback match, i.e. a feedback term was encountered in an incoming Tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">USING STREAM FEATURES FOR RANKING</head><p>The stream features that were described in the previous section are now processed using different ranking algorithms. For the TREC 2012 runs, we were only able to provide two algorithms, one based on OkapiBM25 and the other a standard Vector Space Model-based approach. For each schema we have provided one run with and without relevance feedback. In later publications we will present the application of stream based features using BursT, TF/ICF and incremental TF/IDF. Furthermore we will present the results of plugging stream based TF/IDF values into weighting schemes like ATC and LTU <ref type="bibr" coords="2,397.90,551.78,31.68,7.86">[5, p. 4</ref>]. Here, we would like to find out whether stream based features calculated by applying sliding time windows onto the data streams offer reasonable results. This is relevant because this approach emphasizes the temporal sensitivity of events, i.e. a streamed approach reflects better the continuous nature of a text stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Components of a Document Ranking Scheme</head><p>[16, p. 517f] state that there are three relevant components within a term weighting scheme: Term frequency describes what a document is about. The second component is a factor that reflects the distribution of terms in the document collection as a whole while the third factor takes into account the length of a document in order to avoid over-or under representing terms in the weighting scheme (document length normalization). All of these components were taken into account and adjusted for the real-time filtering scenario. In the following subsections we discuss how concepts like term, document, document collection and relationships have to be adjusted for the microblog scenario and its streaming and real-time aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Determining term frequency in text streams</head><p>[12, p. 2] have showed that 85% of all Tweets contain terms only once. Hence the traditional term frequency measures that rely on the term count within a document fail, because for almost every token within a tweet the tf value would be the same. To overcome this problem we have used stream statistics in order to derive a term frequency replacement value that reflects the importance of a term. As mentioned above we use sliding time windows: With these windows we build a windows into the past that reflect the last k seconds, minutes or events of the data stream. We think that this approach reflects best the temporal notion of information streams.</p><p>For the runs we submitted to TREC 2011 we have used a sliding time window of 120 seconds to calculate ad-hoc statistics for each event stream. Assuming we could use approx. 4.5 out of 12 million downloaded Tweets distributed equally over 16 days, we get an average event frequency of approx. 300k events per day. Assuming a constant event arrival rate, this means that only 4 events arrive per second.</p><p>In the set-up for the TREC runs we have set the event arrival rate to 500 events per second, because otherwise a run would have taken understandably 16 days, if we had kept the original arrival gaps. Extrapolating the time window of 120 seconds, which we used for the TREC runs, this corresponds to a real world window of approx. 4 hours. This value was chosen arbitrarily and will be subject of further investigations. In general, we can assume that the smaller a time window is the better it reflects the most recent changes.</p><p>For the post-TREC runs -runs we created after the deadline of TREC -we changed the values to 1200 events per second. The best results, which will be presented in the last section5.1.1, were yielded with a time window of 240 seconds. If we extrapolate this to real time this would correspond to a sliding time window of almost one day.</p><p>We use the sliding time windows to build dynamic statistics for different events type of the stream: We have built the top-k window for hashtags and tokens event stream. For the presented runs we only used hashtags and tokens for calculating a local term weighting factor, because hashtags are ""derived"" from regular tokens and hence can be used for the boolean matching step that is conducted in order to determine the terms that should be weighted.</p><p>There are more special semantics to Twitter like retweets(rtusername), mentions (@username) and or embedded links, that could be used for filtering the stream, but this is still subject to further investigation.</p><p>Listing 1 8 shows how to construct the top-k ranking for retweets. Each insert into statement can be considered as a separate EPA. Esper offers the possibility to subscribe to such statements and then Java code can be executed on the events that are being processed by the EPA. 8 1 shows how the top-k retweets are generated. i n s e r t into r t c o u n t s e l e c t i s t r e a m token , cnt , ' RtCount ' as type , current timestamp ( ) as t s from r t c o u n t r a w s where c n t &gt; 0 ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stream Weight</head><p>i n s e r t into topKrt s e l e c t token , c n t from r t c o u n t output e v e r y v a r o u t p u t s e c order by c n t desc l i m i t t o p k ; For the TREC runs the term frequency tf is then being calculated using the following formula:</p><formula xml:id="formula_1" coords="3,348.29,530.49,207.63,26.84">tfw,t = 1 + n i=1 T Wi,t * 1 log 2 (rankt + K)<label>(2)</label></formula><p>with T Wi,t being an event stream specific weighting value.</p><p>For the runs we use the values shown in table 1. The values are heuristically chosen, based on observations of Twitter, e.g. <ref type="bibr" coords="3,351.61,594.62,14.32,7.86" target="#b18">[19]</ref> or <ref type="bibr" coords="3,380.26,594.62,9.20,7.86" target="#b1">[2]</ref>.</p><p>Discussion on real time filtering corpora.</p><p>We want to mention that we compressed the text stream for our runs from a real world 16 day period to a continuous, one hour data stream, because there is no separate high-volume microblog corpus available for the TREC 2012 filtering task.</p><p>This might cause discussion on how representative the results are in comparison to the real world Twitter stream, because in our scenario real world events of 16 days are simulated to happen within one hour. Hence the real Twitter stream might offer a different density and distribution of terms than the TREC corpus. This will be subject to further research and discussion, but we think that our approach is valid, because experiments with smaller time windows in the post-TREC runs showed also comparable results. Due to space limitation we will postpone this discussion to a later paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Determining document collection features in text streams</head><p>For determining the document collection based features we employ sliding time windows as well. Sliding time windows were also used for the BursT weighting scheme <ref type="bibr" coords="4,276.62,182.66,12.22,7.86" target="#b6">[7]</ref>, which underpins the viability of our approach. Again, we use the time window to calculate a streamed inverse document frequency(sIDF ) value that will be use as the document collection value. The formula is the same as proposed in <ref type="bibr" coords="4,65.06,234.97,9.71,7.86" target="#b5">[6]</ref> and explained in <ref type="bibr" coords="4,149.74,234.97,42.73,7.86">[14, p. 504</ref>], but it is based on the document count N and term count n in the sliding time window w :</p><formula xml:id="formula_2" coords="4,133.59,277.95,159.31,19.74">sIDFw,t = log 2 Nw nw<label>(3)</label></formula><p>The final score is then simply calculated by plugging the features into the chosen ranking method. For the TREC runs we used a Vector Space Model and OkapiBM25. For the post-TREC runs we modified OkapiBM25 slightly as well as we used the ATC method <ref type="bibr" coords="4,182.34,344.36,12.81,7.86" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pseudo Relevance Feedback for Query Expansion and External Evidence</head><p>For the TREC runs no external evidence was used, i.e. no data from Wikipedia or from a search engines were used nor URLs contained in a Tweet were resolved in order to adjust the search profiles. A search profile had the following structure shown in listing 2 and Tweets were only allowed to be considered if they had a tweet id between querytweetime and querynewesttweet.</p><p>Listing 2: Sample search profile TREC filtering trec &lt;top&gt; &lt;num&gt; MB049 &lt;/num&gt; &lt; t i t l e&gt; carbon monoxide law &lt;/ t i t l e&gt; &lt;q u e r y t i m e&gt; Tue Feb 01 22 : 4 4 : 2 3 +0000 2011 &lt;/ q u e r y t i m e&gt; &lt;q u e r y t w e e t t i m e&gt; 32005451423948800 &lt;/ q u e r y t w e e t t i m e&gt; &lt;q u e r y n e w e s t t w e e t&gt; 32569981321347074 &lt;/ q u e r y n e w e s t t w e e t&gt; &lt;/ top&gt; It has been shown ( <ref type="bibr" coords="4,143.39,627.50,13.63,7.86" target="#b10">[11]</ref>, <ref type="bibr" coords="4,163.86,627.50,9.98,7.86" target="#b7">[8]</ref>) that expanding a URL contained in a Tweet can improve the retrieval and ranking performance of algorithms. This approach was not applied here, as the delay between arrival and judgement of a tweet should be kept as small as possible. Hence the resolution of a URL, its parsing and analysis would have introduced an additional time gap that was not acceptable. In an end-user scenario where a real user does not require immediate estimation of new Tweets, this constraint may be loosened and external evidence from a given url might be included. As mentioned above, in this experiment we focus on the immediately available textual information only.</p><p>In two TREC-runs pseudo-relevance feedback was included as follows: The system starts without any additional query terms. While the system is running, new Tweets arrive. If the arriving Tweet is considered as being relevant according to the judgements provided by the TREC board, the terms of this Tweet are incorporated into the search profile . The top 5 tokens are added dynamically to the search profile.</p><p>But this approach had the drawback that search profiles got dominated by general terms<ref type="foot" coords="4,441.77,181.40,3.65,5.24" target="#foot_6">9</ref> that diluted the search profile. But despite of this one Okapi run submitted to TREC was ranked as #13 out of 69 submitted runs <ref type="bibr" coords="4,493.82,204.09,15.25,7.86" target="#b17">[18]</ref>.</p><p>The relevance feedback mechanism was improved for the post-TREC runs. The positive and negative feedback Tweets were saved in the search profile. During the filtering phase the relevance feedback terms were queried in that way the all -no ranking or selection process -negative feedback terms were subtracted from the positive ones. Only these terms -weighted by factor α -were used in addition to the original query terms. This substraction method was the reason for the remarkable increase of the system. In further research we will try to describe in detail why this worked and if it only worked by chance for this scenario.</p><p>Without the new feedback method the result stayed around a T 11SU around .33, but with the it went up to .47. We also tried a Rocchio based feedback and incorporated decay factor to re-weight the terms in the feedback set. Both did not yield results comparable to the subtraction method. The result stayed around a T 11SU value of .33.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relevance Decision</head><p>The relevance decision and thus the setting of the decision threshold are the two main aspects that sustainably influence the performance of an information filtering system.</p><p>The relevance decision for the TREC run was as follows. The guidelines of the microblog filtering task asked to provide a retrieval decision for every retrieved Tweet. To do so we simply built an ideal document by adding missing search terms to the Tweet under inspection and calculated the score for this Tweet. So we had two scores that could be use to generate a ratio. For the TREC runs we used 0.5 as the threshold, i.e. every Tweet scoring more than 0.5 was marked as relevant.</p><p>For the POST TREC runs we used a different approach. Here we exploited a further stream characteristic . We calculated the average score of the positive marked Tweets in the sliding time window. If an incoming Tweet exceeded the average of the positive feedback samples than it was marked relevant. This approach increased all performance measures verifiably.</p><p>In order to verify this we did a post-hoc evaluation of our runs and determined the best T 11SU value by increasing the threshold step by step. This retrospective approach yielded a fixed threshold and for e.g. ReverseOkapi the best T 11SU value was at around .46. While precision was almost equal the dynamic average approach yielded by far better recall values. This is obvious as a dynamic threshold always reflect the current situation of the stream and hence adapts well to changing situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RUN RESULTS</head><p>In this section we present the results for the TREC 2012 and the post-TREC runs. Furthermore we provide a comparison to an incrementalcorpus approach based on Lucene 10 , i.e. the arriving Tweets were constantly added to the Lucene index and score with the custom Lucene scoring as well as with an custom OkapiBM25 implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup and Data</head><p>The data for the experiments is the TREC 2011 microblog corpus 11 . It is one of the last microblog corpora that is still freely available. Due to its copyright rules Twitter does not allow third parties to provide closed sets of Tweets for research or similar purposes. For example, the Edinburgh Twitter Corpus <ref type="bibr" coords="5,118.92,262.18,14.32,7.86" target="#b12">[13]</ref> was a scientifically edited corpus but is not available any more due to the aforementioned restrictions.</p><p>The TREC 2011 corpus is not a Twitter corpus available for free download either. TREC only offers the id of Tweets that are used for the TREC conference. The Tweets can be downloaded with a tool that crawls either the HTML page of the Tweet with the given ID or downloads a JSON version of the corresponding Tweet. The latter makes use of Twitter API calls which are usually very restricted (e.g. 150 per hour) which dramatically slows down the download process. We have used the HTML version as this allowed us to download the data in a reasonable amount of time.</p><p>In total, there are 16 million Tweet ids available. But Twitter is constantly moving its data and that is why it is not assured that each Tweet ID provided by TREC can be downloaded, what in turn has the effect that every research group obtains a different corpus depending on the time Twitter was crawled. We have downloaded the data from Twitter in a time period from October 10 to 19, 2011.</p><p>In total we were able to download approx. 12 million Tweets. For our runs, only English Tweets were considered. For this purpose we used the language detection library developed by <ref type="bibr" coords="5,100.08,502.78,13.49,7.86" target="#b16">[17]</ref>.</p><p>We also removed Tweets containing more than four question marks (' ???' ) because there were many Tweets that contained only question marks because of their encoding 12 . Out of the approx. 16 million available ids that were provided by the TREC board we could use approx. 4.5 million. The proceedings of the TREC conference 13 show that this is average of Tweets that could be effectively used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Evaluation</head><p>In total, we have submitted four runs to TREC 2012: Two runs based on OkapiBM25 and two using a Vector Space Model approach, each with and without relevance 10 https://lucene.apache.org/core/ 11 Access for academic purposes can be requested here http: //trec.nist.gov/data/Tweets/ 12 Foremost Asian languages were not correctly retrieved by the Crawler 13 The TREC proceedings will be probably available in the first quarter 2013 http://trec.nist.gov/proceedings/ proceedings.html feedback. After the TREC deadline we continued experimenting. These results are also shown here. We compare our results with the best run from the TREC microblog filtering track in terms of T11SU, f-measure, precision and recall. The runs presented in this section were evaluated against the relevance assessment provided by the TREC board 14 . The qrel 15 value 2 was mapped onto value 1 to get to the binary case. In total 60129 Tweets had been assessed, out of which 116 were considered very bad (-2), 57048 not relevant (0), 2404 relevant (1) and 561 highly relevant <ref type="bibr" coords="5,498.60,151.78,11.51,7.86" target="#b1">(2)</ref>.</p><p>The used evaluation measure are described in <ref type="bibr" coords="5,519.86,162.24,13.49,7.86" target="#b14">[15]</ref>. All results are sorted by their T11SU value, which is a utility oriented evaluation measure <ref type="bibr" coords="5,426.81,183.17,35.65,7.86">[4, p. 3]</ref> and which is the standard evaluation measure in the filtering tasks of TREC.</p><p>Only after submitting the runs to TREC, we have found out that the Vector Space Model (VSM) results got corrupted which became apparent due to their poor performance. The reasons for this are under investigation. The okapiv1 and okapiv2rel performed quite well, while okapiv1 being our best run. This run made it to number 13 out of 69 submitted runs.</p><p>In the TREC runs the ones without relevance feedback did better than the ones with relevance feedback. This is due to the naive feedback approach used for the TREC runs 4.2, which diluted the search profile. So the search profiles without feedback stayed more concise and hence performed better.</p><p>The naive feedback approach was changed for the post-TREC runs (reverse okapi,atc 2) and this improved the result significantly. Besides changing the feedback mechanism we experimented with different window sizes. Increasing the time window from 10 to 120 seconds also showed better performance 16 .</p><p>Furthermore the weight for hashtags while determining the local term weight was increased from 1.5 to 12. The ratio between tokens and hashtags is quite skewed, so the hashtag weight would not contribute significantly if we kept the weight so low.</p><p>The aforementioned adjustment of the feedback mechanism and the dynamic threshold helped to increase the precision of the post TREC runs. Also the experimentation with different ranking schemes showed interesting result. We tried two versions of Okapi: one with regular document length normalization 17 and one with reverse normalization 18 . The goal of the latter was to improve the ranking for longer Tweets. This yielded remarkably better result than regular Okapi. Furthermore we tried a classic Vector Space Model approach, as well as an approach (RSV) only based on the document collection features, i.e. only the idf values. Finally in order to show the effectiveness of the event based approach we did some comparison runs based on Lucene. We used Lucene for indexing and retrieving incremental term statistics (document and token count) and calculated a cosine similarity measure as well as a custom 14 http://trec.nist.gov/data/microblog/11/ microblog11-qrels access restricted; registration required 15 Relevance assessment value provided by the TREC board 16 Due to space reasons we only show the values for the adjusted relevance feedback and changed window size 17 norm = doclength averagedoclength 18 norm = averagedoclength doclength OkapiBM25 one. Both performed considerably worse than the event based approach using sliding windows.</p><p>Table <ref type="table" coords="6,89.56,78.56,4.61,7.86" target="#tab_1">3</ref> shows the concrete numbers of for the experiments conducted after the TREC submission deadline. Table <ref type="table" coords="6,69.57,99.48,4.61,7.86" target="#tab_2">4</ref> shows the runs without using the subtraction based relevance feedback mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND OUTLOOK</head><p>Few results submitted to TREC come close to the best value for a specific task, but many of the results are above the median. For the the post-TREC runs the performance was increased. This was foremost due to the adjustment of the window size and the improvement of the relevance feedback mechanism. Both results can be interpreted as a confirmation of the viability of the general approach employing an event processing engine for microblog stream processing. Besides error correction we will focus on the analysis of additional measures of comparison. We will also compare different strategies of construction the temporal corpus, where we will investigate a dynamic adjustment of the window size depending on evaluation metrics like precision, recall or f-measure. Additionally we want investigate how to incorporate the context of the search terms in order to improve the retrieval quality and increase recall. Finally we will contrast sliding windows with incremental approaches and investigate how to efficiently set the decision threshold in order to maximize the performance of the system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run id Precision</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,97.20,604.73,152.17,7.89;2,53.80,625.67,239.10,7.86;2,53.80,636.14,220.38,7.86;2,64.58,652.76,110.76,7.86;2,64.58,669.89,154.85,7.86;2,64.58,687.01,87.12,7.86;2,151.69,685.25,3.65,5.24;2,155.84,687.01,2.56,7.86;2,54.25,700.45,3.65,5.24;2,58.40,702.22,234.51,8.12;2,53.80,711.83,84.58,7.47;2,59.32,395.06,225.00,195.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture Overview imported via a JSON interface and fed into the text preprocessing component. which includes the following steps: 1. Conversion to lower case 2. Stemming with the Porter stemmer 3. Stop word removal 7 . 7 Stop word list used: ftp://ftp.cs.cornell.edu/pub/ smart/english.stop</figDesc><graphic coords="2,59.32,395.06,225.00,195.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,316.88,66.45,225.27,287.62"><head>Table 1 :</head><label>1</label><figDesc>Stream weights for TREC 2012 runs</figDesc><table coords="3,316.88,66.45,225.27,287.62"><row><cell>Token Stream</cell><cell>1.0</cell></row><row><cell>Hashtag Stream</cell><cell>1.5</cell></row><row><cell cols="2">Listing 1: Building an EPA in Esper</cell></row><row><cell>create window</cell><cell></cell></row><row><cell>r t c o u n t</cell><cell></cell></row><row><cell cols="2">. win : time ( c o n s t s t a t s w i n d o w s e c )</cell></row><row><cell>. s t d : unique ( token )</cell><cell></cell></row><row><cell>as ( token S t r i n g</cell><cell></cell></row><row><cell>, c n t Long</cell><cell></cell></row><row><cell>, type S t r i n g</cell><cell></cell></row><row><cell>, t s Long ) ;</cell><cell></cell></row><row><cell>i n s e r t into r t c o u n t r a w</cell><cell></cell></row><row><cell>s e l e c t</cell><cell></cell></row><row><cell>i s t r e a m token</cell><cell></cell></row><row><cell>, count (  *  ) as c n t</cell><cell></cell></row><row><cell>, ' RtCount ' as type</cell><cell></cell></row><row><cell cols="2">, current timestamp ( ) as t s</cell></row><row><cell>from RetweetEvent</cell><cell></cell></row><row><cell>. win : time (</cell><cell></cell></row><row><cell cols="2">c o n s t s t a t s w i n d o w s e c</cell></row><row><cell>)</cell><cell></cell></row><row><cell>group by token</cell><cell></cell></row><row><cell>having count (  *  ) &gt; 0</cell><cell></cell></row><row><cell cols="2">output e v e r y v a r o u t p u t s e c ;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,59.18,121.89,518.89,518.14"><head>Table 3 :</head><label>3</label><figDesc>Recall F-Measure@.5 T11SU Post TReC runs using relevance feedback -summary</figDesc><table coords="7,409.14,121.89,89.18,7.89"><row><cell>Decision Threshold</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,174.08,643.02,261.57,7.86"><head>Table 4 :</head><label>4</label><figDesc>Post TReC runs without relevance feedback -summary</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,321.42,624.31,112.77,7.47"><p>http://www.espertech.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,321.42,634.64,221.33,7.47;1,316.81,643.61,226.53,7.47;1,316.81,652.58,14.10,7.47"><p>http://www.tibco.com/products/event-processing/ complex-event-processing/businessevents/default. jsp</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,321.42,662.91,221.33,7.47"><p>https://www.jboss.org/drools/drools-fusion.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="1,321.42,673.24,225.53,7.47;1,316.81,682.20,18.80,7.47"><p>http://hadoop.apache.org/docs/r0.15.2/streaming. html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="1,321.42,692.53,226.03,7.47;1,316.81,701.50,161.74,7.47"><p>http://engineering.twitter.com/2011/08/storm-iscoming-more-details-and-plans.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="1,317.27,709.42,3.65,5.24;1,321.42,711.83,145.66,7.47"><p><ref type="bibr" coords="1,317.27,709.42,3.65,5.24" target="#b5">6</ref> http://incubator.apache.org/s4/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6" coords="4,321.42,702.22,234.51,7.86;4,316.81,711.19,165.37,7.86"><p>In [? , p. 8] general terms are defined as occurring in positive as well as in negative documents</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,72.59,372.26,220.32,7.86;6,72.59,382.72,220.31,7.86;6,72.59,393.18,20.96,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,218.05,372.26,74.85,7.86;6,72.59,382.72,193.32,7.86">Event based classification of Web 2.0 text streams. arXiv.org, cs.IR</title>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Wolff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-04">April 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,410.21,220.32,7.86;6,72.59,420.68,220.32,7.86;6,72.59,431.14,220.31,7.86;6,72.59,441.60,220.31,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,128.19,410.21,164.73,7.86;6,72.59,420.68,39.15,7.86">Hashtag retrieval in a microblogging environment</title>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,133.70,420.68,159.21,7.86;6,72.59,431.14,220.31,7.86;6,72.59,441.60,145.68,7.86">SIGIR &apos;10: Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,458.63,220.31,7.86;6,72.59,469.09,95.08,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Opher</forename><surname>Etzion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Niblett</surname></persName>
		</author>
		<title level="m" coord="6,212.45,458.63,80.46,7.86;6,72.59,469.09,66.30,7.86">Event Processing in Action. Manning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,486.13,220.32,7.86;6,72.59,496.59,72.89,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D A</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<title level="m" coord="6,183.63,486.13,109.28,7.86;6,72.59,496.59,43.94,7.86">The TREC-8 filtering track final report</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,513.63,220.31,7.86;6,72.59,524.09,220.31,7.86;6,72.59,534.55,220.31,7.86;6,72.59,545.01,220.32,7.86;6,72.59,555.47,220.32,7.86;6,72.59,565.93,20.96,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,238.38,513.63,54.53,7.86;6,72.59,524.09,220.31,7.86;6,72.59,534.55,94.45,7.86">Meta-scoring: automatically evaluating term weighting schemes in IR without precision-recall</title>
		<author>
			<persName coords=""><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A G</forename><surname>Falusos</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,178.95,534.55,113.95,7.86;6,72.59,545.01,220.32,7.86;6,72.59,555.47,163.39,7.86">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="83" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,582.97,220.31,7.86;6,72.59,593.43,220.32,7.86;6,72.59,603.89,128.34,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,123.41,582.97,169.50,7.86;6,72.59,593.43,144.22,7.86">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName coords=""><forename type="first">K S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,224.55,593.43,68.35,7.86;6,72.59,603.89,38.69,7.86">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="493" to="502" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,620.92,220.32,7.86;6,72.59,631.38,220.32,7.86;6,72.59,641.85,220.32,7.86;6,72.59,652.31,58.78,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,217.62,620.92,75.29,7.86;6,72.59,631.38,220.32,7.86;6,72.59,641.85,19.27,7.86">BursT: A Dynamic Term Weighting Scheme for Mining Microblogging Messages</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">F</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,99.32,641.85,164.44,7.86">Advances in Neural Networks-ISNN 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,669.34,220.32,7.86;6,72.59,679.80,220.32,7.86;6,72.59,690.26,220.32,7.86;6,72.59,700.72,220.31,7.86;6,72.59,711.19,155.84,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,278.71,669.34,14.21,7.86;6,72.59,679.80,220.32,7.86;6,72.59,690.26,37.81,7.86">Exploiting real-time information retrieval in the microblogosphere</title>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Runwei</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,140.11,690.26,152.80,7.86;6,72.59,700.72,216.50,7.86">JCDL &apos;12: Proceedings of the 12th ACM/IEEE-CS joint conference on Digital Libraries</title>
		<imprint>
			<publisher>ACM Request Permissions</publisher>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,57.64,220.32,7.86;6,335.61,68.10,53.77,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,415.30,57.64,89.44,7.86">The Power of Events</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Luckham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,85.73,220.32,7.86;6,335.61,96.19,220.31,7.86;6,335.61,106.65,41.15,7.86" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mani</forename><surname>Chandy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roy</forename><surname>Schulte</surname></persName>
		</author>
		<title level="m" coord="6,484.66,85.73,71.26,7.86;6,335.61,96.19,174.77,7.86">Event Processing. Designing IT Systems for Agile Companies</title>
		<imprint>
			<publisher>McGraw Hill</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,124.29,220.32,7.86;6,335.61,134.75,220.32,7.86;6,335.61,145.21,220.31,7.86;6,335.61,155.67,176.59,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,394.91,134.75,161.01,7.86;6,335.61,145.21,165.84,7.86">Incorporating query expansion and quality indicators in searching microblog posts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Massoudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,508.32,145.21,47.59,7.86;6,335.61,155.67,85.82,7.86">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="362" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,173.30,220.32,7.86;6,335.61,183.76,220.31,7.86;6,335.61,194.22,189.21,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="6,335.61,183.76,220.31,7.86;6,335.61,194.22,104.46,7.86">Bad News Travel Fast: A Content-based Analysis of Interestingness on Twitter</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naveed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gottron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kunegis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Alhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>websci11.org</note>
</biblStruct>

<biblStruct coords="6,335.60,211.86,220.32,7.86;6,335.61,222.32,220.31,7.86;6,335.61,232.78,220.31,7.86;6,335.61,243.24,220.31,7.86;6,335.61,253.70,188.26,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,335.61,222.32,123.03,7.86">The Edinburgh Twitter corpus</title>
		<author>
			<persName coords=""><forename type="first">Saša</forename><surname>Petrović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,478.72,222.32,77.19,7.86;6,335.61,232.78,220.31,7.86;6,335.61,243.24,220.31,7.86;6,335.61,253.70,138.31,7.86">WSA &apos;10: Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,271.34,220.31,7.86;6,335.61,281.80,220.32,7.86;6,335.61,292.26,148.68,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,400.57,271.34,155.35,7.86;6,335.61,281.80,168.58,7.86">Understanding inverse document frequency: on theoretical arguments for IDF</title>
		<author>
			<persName coords=""><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,514.19,281.80,41.73,7.86;6,335.61,292.26,58.81,7.86">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="503" to="520" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,309.89,220.32,7.86;6,335.61,320.35,75.98,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="6,454.13,309.89,101.79,7.86;6,335.61,320.35,47.03,7.86">The TREC 2002 filtering track report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,337.99,220.31,7.86;6,335.61,348.45,220.31,7.86;6,335.61,358.91,139.18,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,445.11,337.99,110.81,7.86;6,335.61,348.45,106.92,7.86">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,451.67,348.45,104.25,7.86;6,335.61,358.91,48.94,7.86">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,376.54,220.31,7.86;6,335.61,387.00,219.82,8.12;6,335.61,398.11,49.54,7.47" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="6,408.83,376.54,143.35,7.86">Language detection library for java</title>
		<author>
			<persName coords=""><forename type="first">Nakatani</forename><surname>Shuyo</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/language-detection/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,415.10,220.31,7.86;6,335.61,425.56,213.24,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="6,490.68,415.10,65.24,7.86;6,335.61,425.56,115.23,7.86">Overview of the TREC-2012 Microblog Track</title>
		<author>
			<persName coords=""><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In trec.nist.gov. NIST</note>
</biblStruct>

<biblStruct coords="6,335.60,443.19,220.32,7.86;6,335.61,453.65,220.32,7.86;6,335.61,464.11,220.31,7.86;6,335.61,474.57,220.32,7.86;6,335.61,485.03,85.41,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="6,358.14,453.65,131.78,7.86">Topical semantics of twitter links</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uri</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junghoo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,508.29,453.65,47.63,7.86;6,335.61,464.11,220.31,7.86;6,335.61,474.57,123.82,7.86">WSDM &apos;11: Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<imprint>
			<date type="published" when="2011-02">February 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
