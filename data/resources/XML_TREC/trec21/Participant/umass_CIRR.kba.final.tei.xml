<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,78.42,80.99,455.16,12.90;1,126.70,98.92,358.61,12.90">Bi-directional Linkability From Wikipedia to Documents and Back Again: UMass at TREC 2012 Knowledge Base Acceleration Track</title>
				<funder ref="#_F6CFrej">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_E6ErHnT">
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_euFjcau">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.51,150.53,72.97,10.75"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
							<email>jdalton@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.29,150.53,61.43,10.75"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
							<email>dietz@cs.umass.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,78.42,80.99,455.16,12.90;1,126.70,98.92,358.61,12.90">Bi-directional Linkability From Wikipedia to Documents and Back Again: UMass at TREC 2012 Knowledge Base Acceleration Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B3919D91292035F6B5BA725ACA15AE06</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This notebook details the participation of the University of Massachusetts Amherst in the Cumulative Citation Recommendation task (CCR) of the TREC 2012 Knowledge Base Acceleration Track. UMass' objective is to introduce a single model for Knowledge Base Entity Linking and Knowledge Base Acceleration stream filtering using bi-directional linkability between knowledge base (KB) entries and mentions of the entities in documents.</p><p>Our system focuses on estimating linkability between documents and Knowledge Base entities which measures compatibility in two directions: (1) from a KB entity to documents and (2) from mentions of entities in documents to their KB entries. The entity to document direction, is modeled as a retrieval task where the goal is to identify the most relevant documents for an entity in the evaluation time range. However, if the entity is ambiguous, the retrieved documents may contain documents that are relevant to other entities with the same or similar name. To address this, we want to leverage information from the document to disambiguate the entity. We observe that this problem, from mention to KB entity, is very similar to the TAC Knowledge Base Population Entity Linking Task <ref type="bibr" coords="1,143.53,544.38,58.94,8.64" target="#b1">(Ji et al., 2011)</ref>. The major goal of our participation is to explore how these two directions, from KB to documents and back can be combined.</p><p>For KBA, the goal is to identify documents from a stream that are central for a given entity in Wikipedia. Some participants viewed this as a classification problem and trained supervised classifiers for each entity. Instead, our approach to the problem is based upon document ranking. We combine probabilistic information retrieval and then combine the results with TAC entity linking for re-ranking and filtering.</p><p>Our ranking approach has three stages: First, documents are retrieved from the stream corpus using an entity query model. Second, potential mentions of the target entity are identified in the retrieved documents. Third, links between the document mentions and the target entity are established or dismissed, giving rise to a filtered (or reranked) list of results that mention the target entity. Our initial system leverages the bi-directional relevance as a simple form of re-ranking of retrieved documents, but we envision tighter integration in the future.</p><p>The baseline retrieval run generates a query from the Wikipedia KB entry, including the name, name variants, and linked entities. We also incorporate contextual evidence from the document stream by using the documents in the training time period as relevance feedback documents. We use Latent Concept Expansion <ref type="bibr" coords="1,488.07,391.99,51.92,8.64;1,313.20,403.94,49.75,8.64" target="#b5">(Metzler and Croft, 2007)</ref> to estimate important contextual words and NER concepts.</p><p>Our experiments show that incorporating entity context from query expansion methods provides significant gains both in precision and recall over the baseline, with all of our experimental runs outperforming the median. Our best performing run incorporates linkability evidence from the TAC Entity Linking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our method to estimate bi-directional linkability uses graphical latent variable models that combine probabilistic retrieval and extraction models. In each direction, we first generate a high recall set of candidates using the Markov Random Field retrieval model <ref type="bibr" coords="1,517.31,582.01,18.15,8.64;1,313.20,593.96,84.67,8.64" target="#b4">(Metzler and Croft, 2005)</ref> to construct a query model that includes a model of entity context. The retrieval model includes name variations, surrounding words and NER spans which are identified from text associated with the target entity. We experiment with various methods for estimating the model of an entity, using Latent Concept Expansion (LCE) to incorporate across-document evidence from the corpus. The result is a focused set of candidate documents and knowledge base entries, ranked by the likelihood of referring to the same entity. This set is either used directly, or acts as input to more advanced inference methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus Processing</head><p>Our retrieval models are implemented using Galago 1 , an open source retrieval engine which is part of the Lemur toolkit. Galago supports indexing of large scale data in a distributed cluster environment with a MapReduce-like framework called TupleFlow.</p><p>Both the KBA stream corpus and the Wikipedia knowledge base are indexed to efficiently support bi-directional linking queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">KBA Stream Corpus</head><p>We used the "cleansed" documents with NER information from the KBA stream corpus. These documents are indexed with Galago, stripping out HTML tags. No stemming or stopword removal is performed. In order to filter the stream by time stamp and source type (e.g. linking, social, news), we index this information in Galago fields. Further NER information is preserved in the documents, to be used in relevance feedback queries.</p><p>For efficiency we create a separate index shard for each month. Indexing each shard took between four and eight hours on a cluster of fifty nodes. Per-shard collection statistics are given in Table <ref type="table" coords="2,182.32,378.29,3.74,8.64" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Wikipedia Knowledge Base</head><p>For entity linking, we use a Freebase Wikipedia Extraction (WEX) dump of English Wikipedia from January 2012 which provides the Wikipedia page in machinereadable XML format and relational data in tabular format. The Freebase dump contains 5,841,791 entries. We filter out non-article entries, such as category pages. The resulting index contains 3,811,076 documents and over 60 billion words.</p><p>The goal is to create an index with fields for: anchor text (internal as well as from the Web), Wikipedia categories, Freebase names, Freebase types, redirects, article titles, and full-text for each article. Most of this information is contained in the WEX dump. We also incorporate external web anchor text to Wikipedia entries using the the Google Cross-Wiki dictionary (Spitkovsky and Chang, 2012), which contains 3 billion links and 297 million associations from 175 million unique anchor text strings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KB Entities to Documents</head><p>For each target entity from Wikipedia, the first step is to retrieve a high recall set of stream documents. First, name variants and potentially disambiguating context is 1 http://www.lemurproject.org/galago.php extracted from the target's Wikipedia article. We leverage the stream corpus to re-weight disambiguating context. From these ingredients, we build a retrieval query against the stream corpus.</p><p>The goal is to identify:</p><p>• the target entity's name,</p><p>• name variants by which the entity is referred to,</p><p>• contextual words,</p><p>• related named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Extracting Name Variants and Disambiguating Context</head><p>The canonical name of the target entity is taken from the title of the Wikipedia article. Name variants for the Wikipedia entry are gathered from the title, redirects, Freebase names, disambiguation links, and incoming anchor text. Related named entities are taken from titles of in-and outlinks of the target's Wikipedia page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Modeling using Latent Concept Expansion</head><p>We estimate disambiguating context from the corpus and training document evidence using Latent Concept Expansion (LCE). LCE is a query expansion technique for estimating contextual evidence built upon the Markov Random Field retrieval framework. We use it to model dependencies between related entities by including NER name spans as types of concepts. LCE estimates the salience of an entity span from documents that are relevant to the target entity. The intuition is that the salience of words and named entities increases the more often they occur in documents relevant to the target entity.</p><p>For relevance feedback, we use the set of relevant documents from the pre-cutoff sample documents. In one experimental run we also add post-cutoff documents using pseudo-relevance feedback. We now discuss the individual components of the expansion model. The most probable k unigrams (after removing stopwords) are used as disambiguating contextual words with weights φ CW . We now discuss how we estimate the NER φ NER weights. The first source of spans are the named entities mentioned in the inlink and outlink structure. In addition, we use NER spans that frequently occur in the relevant documents. After sets of entity spans from the KB entry and external documents are combined, the top k weighted named entities are selected as context. We use both NER spans from the Wikipedia entry and external spans from relevant documents because they capture different aspects of relevance for the entity. The corpus may be biased towards one event in time. The link information from Wikipedia captures long-term hand-constructed relationships, but they may not be timely. The current method for determining entity context importance is a first step. In the future, we plan to experiment with more advanced  techniques for combining and weighting the evidence from the local Wikipedia document with the external evidence from the document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Retrieving Relevant Stream Documents</head><p>The entity model, M E we use for retrieving stream documents consists of four concept types in K: the entity name T , name variations N V , context words CW , and context NER spans N ER. For each entity we compute the score for an entity, E and a document D as follows.</p><p>The query model scores the documents in the collection using a log-linear weighted combination of the matches of the concepts K and ranks the documents using this score.</p><formula xml:id="formula_0" coords="3,115.19,636.68,183.61,20.14">sc(E, D) = t∈T λ t k∈K φ k ψ(k, D)<label>(1)</label></formula><p>The potential function ψ(k, D) is a real valued score for a concept in a document, which itself may be a submodel. In M E we use a sequential dependence model to estimate ψ(k, D) for all concept types, except CW which consists only of unigrams. For each scoring com-ponent we use the matching function in Equation <ref type="formula" coords="3,515.02,240.31,3.74,8.64" target="#formula_1">2</ref>. We set the λ T and λ NV parameters for both the name and name variants to be constant. For the context word and NER span weights, we estimate the latent φ k parameter weights using relevance model weighting <ref type="bibr" coords="3,481.72,289.43,58.28,8.64;3,313.20,301.38,47.46,8.64" target="#b3">(Lavrenko and Croft, 2001)</ref>.</p><p>The score of a concept in document is the log of the probability of a concept, k, given a document D with Dirichlet smoothing, i.e.,</p><formula xml:id="formula_1" coords="3,364.49,357.88,175.52,27.71">f (k, D) = log tf k,D + µ tf k,C |C| |D| + µ<label>(2)</label></formula><p>where tf k,D is the number of occurrences of the concept in the document, tf k,C is the number of occurrences in the collection, |D| is the number of terms in document, |C| is the number of words in the collection, and µ is the smoothing parameter that is set empirically. The query model is run using the Galago search engine to score all of the stream documents. The query is given in Galago's query syntax in Figure <ref type="figure" coords="3,459.38,482.18,3.74,8.64" target="#fig_0">1</ref>. The result of retrieval is a linkability score in the direction of Wikipedia entity to documents which can be used as-is or re-ranked further. The source code for our KBA system including Galago configuration will be available online<ref type="foot" coords="3,495.38,528.33,3.49,6.05" target="#foot_0">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Entity Mentions to Wikipedia</head><p>We estimate linkability in the opposite direction from document mention to Wikipedia entity using a linking model developed for the TAC KBP Entity Linking task. For details of the TAC system, including features please see our TAC 2012 notebook paper <ref type="bibr" coords="3,462.57,621.19,77.43,8.64;3,313.20,633.15,21.44,8.64" target="#b0">(Dietz and Dalton, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Identifying Mentions of the Target Entity</head><p>For each candidate document retrieved for a target entity, we extract potential mentions of that entity. Across the set of all mentions, we identify the mention with the highest similarity to target entity by searching for the target entity name and name variants. Matches are identified with string matches ignoring case and punctuation, preferring exact matches and high confidence name variants over partial name matches. If no matching entity mentions are found, a dummy empty mention is created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Re-ranking Mentions to Match the Target Entity</head><p>Next, each of the canonical entity mentions is linked against Wikipedia entries -which is the direction evaluated in the entity linking task of the TAC KBP competition. We train a supervised discriminative ranking model with TAC entity linking data from years 2010 and 2011. It incorporates features based on string similarity of names, similarity of term vectors, and name confidence based on ambiguity of anchor texts. A full list of features and a complete description of the entity linking system is provided in our TAC KBP notebook paper.</p><p>6 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>We now describe the parameter setting used for the model. For scoring with Equation 2 we use the default smoothing value, µ = 2000. It is important to note that we only used background term statistics from the training time range. For the free parameters in our Sequential Dependence (SD) sub-models we estimate the parameters using training data from the TAC KBP 2010 entity linking data, resulting in settings λ T D = 0.29, λ O D = .21, and λ U D = 0.50. These parameters place greater emphasis on the ordered window and term proximity, which is logical since the queries consist largely of names. We use the LCE context model to retrieve and rank the stream documents. We manually set the concept type weights as:</p><formula xml:id="formula_2" coords="4,313.20,329.71,199.94,9.65">λ T = 0.3, λ N V = 0.3, λ CW = 0.2, λ N ER = 0.2.</formula><p>These parameter setting are similar to the default LCE settings, which provides half the weight to the original query and half to expansion terms. The result of running the query is an unnormalized log probability. To produce a score in the 1 to 1000 confidence range, we normalize the score by the sum of the retrieved document scores. We scale this approximation of the posterior probability by 1000. We experimented with other normalization techniques because we hypothesized that this would affect the optimal cutoff, but as we see in Figure <ref type="figure" coords="4,367.22,461.61,4.98,8.64" target="#fig_2">3</ref> the cutoff appears to have little impact on the evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Run comparison</head><p>In this section, we compare the runs submitted to the CCR task of the TREC 2012 KBA Track. The runs we submitted are variations of the models described previously. The descriptions of the runs follow:</p><p>1. NV Full Stream -This a baseline run using the entity name and name variations only, scoring the full stream (TTR + ETR) documents , with λ T = 0.5, λ N V = 0.5 λ CW = 0, λ N ER = 0. The highest scoring 6000 documents are returned by the run.</p><p>(submitted run ID:FS_NV_6000) 2. NV -This run uses entity name and name variations only, scoring the post-cutoff (ETR) documents , with λ T = 0.5, λ N V = 0.5 λ CW = 0, λ N ER = 0. The highest scoring 1500 documents are returned by the run. (submitted run ID: PC_NV_150050) 3. LCE10 -This run employs explicit relevance feed-back on the TTR documents using Latent Concept Expansion to estimate related context words (CW) and NER names (NER) using 10 context words and 10 NER spans each from Wikipedia and the training documents. The parameter setting used are: </p><formula xml:id="formula_3" coords="5,91.92,134.93,199.27,9.65">λ T = 0.3, λ N V = 0.3, λ CW = 0.2, λ N ER = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Further Analysis</head><p>We examine the query-by-query performance of the our top performing run, LCE10+TAC model in Figure <ref type="figure" coords="5,330.97,265.62,4.98,8.64">2</ref> and Figure <ref type="figure" coords="5,391.51,265.62,4.98,8.64" target="#fig_2">3</ref> and how it compares with other teams. The results show that for our optimal cutoff over 68.9% of our queries are above the median. However, if our overall best average cutoff is used, 55.2% of queries are above the median. Our best performing queries are Basic_Element_(music_group), Jim_Steyer, Nassim_Nicholas_Taleb, and James_McCartney.</p><p>The worst performing queries in order are Basic_Element_(company), Boris_Berezovsky_(businessman), Satoshi_Ishii, Darren_Rowse, and William_D._Cohan. All the cutoff values correlate highly, with 750 and cutoff 0 both perform comparably despite retrieving very different numbers of results. Choosing a particular cutoff value to evaluate is difficult. The reasons for the similar effectiveness across cutoffs is unclear, but we hypothesize that it may be caused by the evaluation process where only judged negative documents are counted as false positive examples.</p><p>In retrospect, the performance of our runs would have improved if more documents (&gt;1500) were retrieved. The NV Full Stream run that retrieves six thousand documents over both the ETR and TTR periods outperforms same method that retrieves only fifteen hundred documents on the evaluation time range. Instead of returning thousands of potentially relevant documents, we focus on ranking a smaller set of highly relevant results. In the future, rank based evaluation metrics may be used to further characterize the behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In our submissions to KBA we experimented with bidirectional linkability between Wikipedia and documents to estimate centrality. We attempted to combine evidence from both directions: from an entity to documents and back. Our goal was to utilize the TAC entity linking to reduce noise and improve precision for ambiguous entities.</p><p>We present a single model that uses graphical latent variable models with probabilistic retrieval to generate a focused set of candidates, rank the results, and combine evidence from cross-document entity context using relevance feedback. Our experiments show that incorporating evidence from mention to entity using a TAC linker is a promising area and may improve over models that only use evidence in one direction from entities to documents.</p><p>One area for future work is modeling temporal change for the entity. We do not explicitly model the temporal change in the stream structure of the KBA corpus. However, we note that there is significant recent work using temporal relevance feedback <ref type="bibr" coords="6,193.04,219.15,86.82,8.64" target="#b2">(Keikha et al., 2011)</ref> that could be built upon for this task.</p><p>The current use of bi-directional information in our model is limited. The TAC entity linking model is used mainly as a re-ranker. Combining these two tasks is challenging because the linkability evidence is not symmetric, with different sources of evidence in either direction. We intend to explore ways of modeling context and combining these two linkability directions further in our future submissions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,391.98,226.80,7.77;3,72.00,402.94,226.80,7.77;3,72.00,413.90,226.80,7.77;3,72.00,424.86,26.15,7.77"><head>#Figure 1 :</head><label>1</label><figDesc>Figure 1: LCE query for retrieving relevant stream documents in Galago query syntax. The query includes the entity name (T), name variants (NV), context words (CW), and NER spans (NER).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.00,264.75,226.80,7.77;4,72.00,275.71,226.81,7.77;4,72.00,286.67,29.13,7.77"><head></head><label></label><figDesc>Figure 2: F-Score performance over queries at different cutoff thresholds. Queries are sorted by difficulty in terms of median F-score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,313.20,263.84,226.80,7.77;4,313.20,274.79,28.14,7.77"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Difference in F-score to the median performance over queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,124.99,74.28,362.02,142.62"><head>Table 1 :</head><label>1</label><figDesc>KBA Galago Shard Statistics</figDesc><table coords="3,124.99,74.28,362.02,121.81"><row><cell>Month</cell><cell cols="4">Documents Collection Length Index Size (GB) Total Size (GB)</cell></row><row><cell>October 2011</cell><cell>36,547,282</cell><cell>54,33,597,431</cell><cell>22</cell><cell>245</cell></row><row><cell>November 2011</cell><cell>55,434,234</cell><cell>14,529,421,474</cell><cell>55</cell><cell>673</cell></row><row><cell>December 2011</cell><cell>62,773,692</cell><cell>16,058,713,120</cell><cell>62</cell><cell>739</cell></row><row><cell>January 2012</cell><cell>60,799,418</cell><cell>16,983,265,272</cell><cell>64</cell><cell>781</cell></row><row><cell>February 2012</cell><cell>58,147,836</cell><cell>18,488,791,637</cell><cell>67</cell><cell>833</cell></row><row><cell>March 2012</cell><cell>50,857,928</cell><cell>19,388,982,395</cell><cell>67</cell><cell>871</cell></row><row><cell>April 2012</cell><cell>33,796,674</cell><cell>14,217,201,526</cell><cell>51</cell><cell>835</cell></row><row><cell>May 2012</cell><cell>395,732</cell><cell>447,158,725</cell><cell>1</cell><cell>21</cell></row><row><cell>Total</cell><cell>358,752,796</cell><cell>100,113,534,149</cell><cell>389</cell><cell>4998</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.00,74.28,468.00,647.87"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Best F-Score of the runs. Best result appears in boldface.</figDesc><table coords="5,291.19,134.93,7.61,8.74"><row><cell>2.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,327.55,715.56,162.59,5.61"><p>http://www.github.com/CIIR/TrecKBA</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgment</head><p>This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs>, in part under subcontract #<rs type="grantNumber">19-000208</rs> from <rs type="affiliation">SRI International</rs>, prime contractor to DARPA contract #<rs type="grantNumber">HR0011-12-C-0016</rs>, and in part by <rs type="funder">NSF</rs> grant #<rs type="grantNumber">IIS-0910884</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_E6ErHnT">
					<idno type="grant-number">19-000208</idno>
				</org>
				<org type="funding" xml:id="_euFjcau">
					<idno type="grant-number">HR0011-12-C-0016</idno>
				</org>
				<org type="funding" xml:id="_F6CFrej">
					<idno type="grant-number">IIS-0910884</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,72.00,505.19,226.80,7.77;6,81.96,516.15,216.84,7.77;6,81.96,526.96,216.84,7.93;6,81.96,537.91,43.12,7.72" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,233.57,505.19,65.24,7.77;6,81.96,516.15,216.84,7.77;6,81.96,527.11,24.20,7.77">Across-Document neighborhood expansion: UMass at TAC KBP 2012 entity linking</title>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,130.25,526.96,168.55,7.72">Proceedings of the Text Analysis Conference</title>
		<meeting>the Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>TAC KBP</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,549.21,226.80,7.77;6,81.96,560.02,216.84,7.93;6,81.96,570.98,75.87,7.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,263.81,549.21,35.00,7.77;6,81.96,560.17,182.26,7.77">Overview of the TAC2011 knowledge base population track</title>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,284.36,560.02,14.44,7.72;6,81.96,570.98,71.94,7.72">Text Analysis Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,582.28,226.80,7.77;6,81.96,593.09,216.84,7.93;6,81.96,604.04,93.50,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,240.09,582.28,58.71,7.77;6,81.96,593.24,118.00,7.77">Temper: A temporal relevance feedback method</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Keikha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,209.07,593.09,89.74,7.72;6,81.96,604.04,31.48,7.72">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="436" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,615.34,226.80,7.77;6,81.96,626.15,216.84,7.93;6,81.96,637.11,216.84,7.72;6,81.96,648.07,216.84,7.93;6,81.96,659.18,82.92,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,238.64,615.34,60.16,7.77;6,81.96,626.30,58.42,7.77">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,157.08,626.15,141.72,7.72;6,81.96,637.11,216.84,7.72;6,81.96,648.07,130.93,7.93">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;01</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,670.33,226.80,7.77;6,81.96,681.13,216.84,7.93;6,81.96,692.09,216.84,7.72;6,81.96,703.05,216.84,7.93;6,81.96,714.16,160.39,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,243.74,670.33,55.05,7.77;6,81.96,681.29,142.64,7.77">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,244.45,681.13,54.35,7.72;6,81.96,692.09,216.84,7.72;6,81.96,703.05,213.29,7.93">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,76.13,226.80,7.77;6,323.16,86.93,216.84,7.93;6,323.16,97.89,216.84,7.72;6,323.16,108.85,216.84,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,446.75,76.13,93.25,7.77;6,323.16,87.08,99.91,7.77">Latent concept expansion using markov random fields</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,439.97,86.93,100.04,7.72;6,323.16,97.89,216.84,7.72;6,323.16,108.85,130.56,7.72">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,119.94,226.80,7.77;6,323.16,130.74,216.84,7.93;6,323.16,141.70,173.48,7.72" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,507.11,119.94,32.89,7.77;6,323.16,130.89,179.87,7.77">A Cross-Lingual dictionary for english wikipedia concepts</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,522.07,130.74,17.93,7.72;6,323.16,141.70,169.70,7.72">Conference on Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
