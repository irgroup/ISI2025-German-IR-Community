<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.85,75.38,298.62,10.54">Effective Structured Query Formulation for Session Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.25,94.97,56.79,9.06"><forename type="first">Dongyi</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street</addrLine>
									<postCode>20057</postCode>
									<settlement>Washington</settlement>
									<region>NW, DC</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,285.72,94.97,41.04,9.06"><forename type="first">Hui</forename><surname>Yang</surname></persName>
							<email>huiyang@cs.georgetown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street</addrLine>
									<postCode>20057</postCode>
									<settlement>Washington</settlement>
									<region>NW, DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,421.66,94.97,64.36,9.06"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street</addrLine>
									<postCode>20057</postCode>
									<settlement>Washington</settlement>
									<region>NW, DC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.85,75.38,298.62,10.54">Effective Structured Query Formulation for Session Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F7C17069F2961B8289835A1999C84CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we emphasize on formulating effective structured queries for session search. For a given query, phrase-like text nuggets are identified and formulated into Lemur queries to feed into the Lemur search engine. Nuggets are substrings in q n , similar to phrases but not necessarily as semantically coherent as phrases. We assume that a valid nugget appears frequently in top returned snippets for q n . In this work, the longest sequences of words consisting of frequent bigrams within the top returned snippets are identified as nuggets and are used to formulate a new query. By formulating structured query using the nuggets, we greatly boost the search accuracy than just using q n . We experiment both strict and relaxed forms of structured query formulation. The strict form of query formulation achieves an improvement of 13.5% and the relaxed form achieves an improvement of 17.8% on nDCG@10 on TREC 2011 query sets. We further combine the nuggets generated from all queries q 1 , … , q n-1 , q n , to formulate one structured session query for the entire session. Nuggets from each query are weighed by various weighting schemes to indicate their relations to the current query and their potential contributions to the retrieval performance. We experiment three weighting schemes, uniform (all queries share the same weight), previous vs. current (previous queries q 1 , … , q n-1 share the same weight while q n uses a different and higher weight), and distance-based (the weights are distributed based on how far a query's position in the session is from the current query). We find that previous vs. current achieves the best search accuracy. For retrieval, we first retrieve a large pool of documents for q n . We then employ a re-ranking model that considers document similarity between clicked documents and documents in the pool as well as dwell time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction TREC 2012 Session track features sequences of queries q 1 , q 2 , … , q n-1 , q n , with only current query q n being the subject for retrieval. Four subtasks progressively include four types of session information. They are: RL1: a subtask only use the current query q n ; RL2: a subtask uses the previous queries q 1 , q 2 , … , q n-1 and the current query q n ; RL3: a subtask provides top retrieved documents for previous queries; and RL4: additional information about which top results are clicked by users. In this research, we emphasize on formulating effective structured queries for search tasks within a session. In RL1, we attempt to find text nuggets in the current query to generate structured queries. In RL2, we merge the nuggets extracted from all queries to build a structured session query. In RL3 and RL4, we employ anchor texts in the top 10 search results to expand the structured session query. We remove duplicated queries from the query sequence. Dwell time for clicked documents is employed for document re-ranking in RL4. In the following sections, we present our methods for query formulation, query expansion, duplicate removal and document re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structured Query Formulation</head><p>In a query, several words often bundle together as a phrase to express a coherent meaning. We identify phrase-like text nuggets and formulate them into Lemur queries for retrieval . Nuggets are substrings in q n , similar to phrases but not necessarily as semantically coherent as phrases. We observe that a valid nugget appears frequently in the top returned snippets for q n . We then use nuggets to formulate new structured queries in the Lemur query language. Particularly, we look or nuggets in the top k snippets returned by Lemur with the original current query q n . The nuggets are identified by two methods, strict and relaxed, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Strict Method</head><p>First, we send the original query q n into Lemur and retrieve the top k snippets over an index built for ClueWeb CatB. Then all snippets are concatenated as a reference document R. The original query is represented as a word list q = w 1 w 2 ...w n . For every bigram in q n , we count its occurrences in the reference document R. Frequent bigrams are marked "candidate" if its normalized occurrence exceeds a threshold.</p><formula xml:id="formula_0" coords="2,217.22,140.36,293.33,24.43">count(𝑤 ! 𝑤 !!! ; 𝑅) min(count 𝑤 ! ; 𝑅 , count(𝑤 !!! ; 𝑅)) ≥ 𝜃 (1)</formula><p>where count(x; R) denotes the occurrence of x in the reference document R, w i and w i+1 are adjacent words in the query, θ is the threshold, which is trained to be 0.97 according to the TREC 2011 session data.</p><p>For example, in TREC 2012 session 53 query "servering spinal cord consequenses", we find bigram "spinal cord" as candidates.</p><p>The bigrams could connect to form longer n-grams. For instance, in TREC 2011 session 11 query "hawaii real estate average resale value house or condo news", we discover that "hawaii real" and "real estate" are both marked "candidate", and they can be merged into a longer sequence "hawaii real estate". On the contrary, "estate average", is not a candidate, hence we cannot form "hawaii real estate average" and "hawaii real estate" is the longest sequence and is recognized as a nugget. Formally, we define:</p><formula xml:id="formula_1" coords="2,90.28,292.76,420.27,46.51">𝑛𝑢𝑔𝑔𝑒𝑡 = #1(𝑤 ! ˽𝑤 !!! ˽ … ˽𝑤 !!! ) (2) such that 𝑤 ! 𝑤 !!! is connected for each 𝑗 ∈ 𝑖, 𝑖 + 𝑘 -1 𝑤 !!! 𝑤 ! and 𝑤 !!! 𝑤 !!!!! are not connected (3)</formula><p>where #1 is the ordered window operator with size 1 in Lemur query language which means the words in the bracket are all adjacent, "˽" denotes the space. Consequently the query is broken down into nuggets and single words. All serve as the elements to build up a structured query using the Lemur query language #combine(𝑛𝑢𝑔𝑔𝑒𝑡</p><formula xml:id="formula_2" coords="2,260.11,375.08,246.57,10.27">! ˽𝑛𝑢𝑔𝑔𝑒𝑡 ! ˽… 𝑛𝑢𝑔𝑔𝑒𝑡 ! ˽𝑤 ! ˽𝑤 ! ˽…𝑤 ! )<label>(4</label></formula><p>) where we suppose there are m nuggets and r single words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relaxed Method</head><p>#1 is a strict structure operator and may produce many false negatives in retrieval. We therefore introduce another relaxed method for query formulation. We relax the constraints based on the intuition that distance between two words reflects the associativeness of them. Particularly, we first retrieve the top k snippets as in Section 2.1. For RL2, every word's position in the snippet is marked as shown in Figure <ref type="figure" coords="2,455.80,447.13,3.73,8.64">1</ref>. We then estimate the position of word w i as</p><formula xml:id="formula_3" coords="2,239.39,469.16,271.16,33.79">𝑥 𝑤 ! = 1 𝑘 • 𝑥 ! (𝑤 ! ; 𝑆 ! ) ! count(𝑤 ! ; 𝑆 ! ) ! !!! (5)</formula><p>where k is the number of snippets, S t is the t th snippet, x j (w i ; S t ) is the index of the j th instance of w i in S t , count(w i ; S t ) is the occurrence of w i in S t . The estimated postion is averaged over all the snippets. For every bigram in the query, the distance between their estimated postions is calculated. We predict the window size (X in #X) of a nugget by learned through a decision tree:</p><formula xml:id="formula_4" coords="2,170.01,550.52,340.54,35.95">𝑛𝑢𝑔𝑔𝑒𝑡 = #1(𝑤 ! 𝑤 !!! ) 𝑥 𝑤 ! -𝑥 𝑤 !!! ≤ 5 #2(𝑤 ! 𝑤 !!! ) 5 &lt; 𝑥 𝑤 ! -𝑥 𝑤 !!! ≤ 10 ∅ 𝑥 𝑤 ! -𝑥 𝑤 !!! &gt; 10 (6)</formula><p>The structured query is then formulated as in eq (4).</p><p>For TREC 2012 session 53 query "servering spinal cord consequenses", we obtain the structured query "#2(spinal cord) servering consequenses".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Expansion</head><p>To include more information to formulate effective queries, we expand the queries by using top search results for previous queries and click information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query expansion with previous queries (RL2)</head><p>For RL2, we first obtain a set of nuggets and single words from every query q k = {nugget ik , w jk } by the approach presented in Section 2. We then merge these nuggets to form an expanded query:</p><formula xml:id="formula_5" coords="3,146.84,135.80,363.71,67.31">#weight( 𝜆 ! #combine(𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽𝑛𝑢𝑔𝑔𝑒𝑡 !" ˽… 𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽𝑤 !! ˽𝑤 !" ˽…𝑤 !! ) 𝜆 ! #combine(𝑛𝑢𝑔𝑔𝑒𝑡 !" ˽𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽… 𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽𝑤 !" ˽𝑤 !! ˽…𝑤 !! ) … 𝜆 ! #combine(𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽… 𝑛𝑢𝑔𝑔𝑒𝑡 !" ˽𝑤 !! ˽𝑤 !! ˽…𝑤 !" ) )<label>(7)</label></formula><p>where λ k denotes the weight of query q k . Note that the last #combine is for the current query q n . Three weighting schemes are designed to determine the weight λ k , namely uniform, previous vs. current, and distance-based.</p><p>• uniform. All queries are assigned the same weight, i.e., λ k = 1.</p><p>• previous vs. current. All previous queries share the same weight while the current query uses a complementary and higher weight. Particularly, we define:</p><formula xml:id="formula_6" coords="3,230.49,275.24,280.06,23.71">𝜆 ! = 𝜆 ! 𝑘 = 1, 2, … , 𝑛 -1 1 -𝜆 ! 𝑘 = 𝑛 (8)</formula><p>where λ p is trained to be 0.4 on TREC 2011 session track data.</p><p>• distance-based. The weights are distributed based on how far a query's position in the session is from the current query. We use a reciprocal decay function to model it.</p><formula xml:id="formula_7" coords="3,229.46,336.92,281.09,34.99">𝜆 ! = 𝜆 ! 𝑛 -𝑘 𝑘 = 1, 2, … , 𝑛 -1 1 -𝜆 ! 𝑘 = 𝑛 (9)</formula><p>where λ p is trained to be 0.4 based on TREC 2011 session track data, k is the position of a query..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query expansion with previous search results (RL3 and RL4)</head><p>Anchor texts pointing to a page often provide valuable human-created description to this page <ref type="bibr" coords="3,480.34,401.05,10.59,8.64">[1]</ref>, which enable us to expand the query by the words in the anchor texts. The anchor log is extracted by the harvestlinks in Lemur toolkit. The format is shown in Figure <ref type="figure" coords="3,342.17,424.09,3.87,8.64">2</ref>: The first column is the page linked by an anchor. The second column is the page this anchor lies in. The third column is the anchor text that describes the page in the first column. We collect the anchor texts for all previous search results and sort them by frequency in decreasing order. The top 5 frequent anchor texts are appended to the expanded query generated for RL2 with a weight proportional to their frequencies.</p><p>#weight(</p><formula xml:id="formula_8" coords="3,149.53,505.16,362.76,67.31">𝜆 ! #combine(𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽𝑛𝑢𝑔𝑔𝑒𝑡 !" ˽… 𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽𝑤 !! ˽𝑤 !" ˽…𝑤 !! ) 𝜆 ! #combine(𝑛𝑢𝑔𝑔𝑒𝑡 !" ˽𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽… 𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽𝑤 !" ˽𝑤 !! ˽…𝑤 !! ) … 𝜆 ! #combine(𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽𝑛𝑢𝑔𝑔𝑒𝑡 !! ˽… 𝑛𝑢𝑔𝑔𝑒𝑡 !" ˽𝑤 !! ˽𝑤 !! ˽…𝑤 !" ) 𝛽𝜔 ! #combine 𝑒 ! 𝛽𝜔 ! #combine 𝑒 ! … 𝛽𝜔 ! #combine(𝑒 ! ) )<label>(1)</label></formula><p>where e i (i = 1 ... 5) is the top 5 anchor texts, ω i (i = 1 ... 5) denotes the corresponding frequency of the anchor texts normalized by the maximum frequency, β is a factor to adjust the intervention of the anchor texts, which is trained to be 0.1 on the TREC 2011 session data. For example, in TREC 2012 session 53, the anchor texts with top frequency are "type of paralysi", "quadriplegia paraplegia", "paraplegia", "spinal cord injury", and "quadriplegic tetraplegic", so the final structured query is "#weight(1.0 #1(spinal cord) 0.6 consequenses 0.4 paralysis 1.0 servering 0.380723 #combine(type of paralysi) 0.004819 #combine(quadriplegia paraplegia) 0.004819 paraplegia 0.004819 #combine(spinal cord injury) 0.00241 #combine(quadriplegic tetraplegic) )", where the underlined part is from anchor texts. This expansion is applied to both RL3 and RL4. In RL4, only the anchor texts in clicked documents are extracted to expand the final structured session query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Duplicated Queries</head><p>The trace of how a user modifies queries in a session may suggest the intention of the user so that it can be exploited to study the real information need of the user. We notice that sometimes the user repeats a previous query and makes a duplicated query. We thus refine the final structured session query as follows.</p><p>• If there exists a previous query that is the same as the current query q n , we only use the current query to generate the structured session query. • If several previous queries are duplicated but they are all different from q n , we remove these queries when formulating the structured session query. We also consider a special situation as follows. If one substring is the abbreviation of another, we also consider these two queries duplicates. For example, the only difference between queries "History of DSEC" and "History of dupont science essay contest" is "DSEC" and "dupont science essay contest", in which the former is the abbreviation of the latter, so they are considered duplicates. To detect abbreviations, we scan the query string and split a word into letters if this word is entirely uppercase. In the example above, the first query is transformed to "History of D S E C". When comparing two queries, two words in corresponding positions are considered the same if one of them contains only one capital letter and they start with the same letter. Also in the above example, "dupont" and "D" are considered the same. We process duplicated queries in RL3 and RL4. 5 Document Re-ranking Users intend to stay in a page that he is interested for longer time <ref type="bibr" coords="4,370.02,361.93,10.92,8.64">[3,</ref><ref type="bibr" coords="4,380.94,361.93,7.28,8.64">4,</ref><ref type="bibr" coords="4,388.22,361.93,7.28,8.64">5]</ref>. We use dwell time, which is defined as the elapsed time that user stays in the page, to re-rank the search results in RL3. The click information provided in RL4 is associated with a start time t s and an end time t e . The dwell time Δt can be derived by t e -t s . In a session, we retrieve all clicked pages c i with their dwell time Δt i . For each returned document d j for the structured query, the cosine similarity to c i is computed. We calculate the score of d j by</p><formula xml:id="formula_9" coords="4,241.59,438.68,268.96,20.83">𝑠 𝑑 ! = Sim 𝑑 ! , 𝑐 ! ⋅ Δ𝑡 ! !<label>(2)</label></formula><p>where Sim(d j , c i ) is the cosine similarity between d j and c i . We rank d j by s(d j ) in decreasing order as the final search results. The re-ranking method is applied in run gurelaxphr for RL4. In our experiments, the raw dwell time used in our method strongly bias the document weights towards those with long dwell time, which corresponds to satisfing visits receive much higher weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments 6.1 Dataset and Evaluation Metrics</head><p>We employ the Lemur search engine<ref type="foot" coords="4,239.08,544.87,3.24,5.69" target="#foot_0">1</ref> as the basis and use ClueWeb09 Category B (CatB) as the document collection. The index is built on CatB, and the anchor log is acquired by applying harvestlinks on ClueWeb09 Category A (CatA) since the official previous search results are from CatA. Previous research demonstrates that ClueWeb09 collection involves many spam documents. We filter out spam documents based on Waterloo "GroupX" spam ranking score<ref type="foot" coords="4,359.08,590.95,3.24,5.69" target="#foot_1">2</ref> less than 70[2].  </p><p>where c(w;d) denotes the occurrences of term w in document d, p(w|C) is the collection language model, µ is the parameter. The parameter µ is tuned based on the 2011 session data. We do not use the topic descriptions provided by NIST. nDCG@10 is the main metric to evaluate the retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results on TREC 2011</head><p>For RL1, where only the current query q n is available, we generate the structured query from q n by the approach described in 2 and send it into Lemur. The Dirichlet parameter µ and the number of pseudo relevance feedback k are tested on TREC 2011 session data. The documents retrieved by directly searching q n serve as the baseline. Table <ref type="table" coords="5,220.80,403.45,4.92,8.64" target="#tab_2">1</ref> shows the nDCG@10 results for RL1 on TREC 2011. By formulating structured query using nuggets, we greatly boost the search accuracy than baseline by 13.50%. The relaxed form achieves even better search accuracy of 0.3979 (+17.79%).</p><p>For RL2, we apply query expansion with the previous queries explained in Section 0. We observe that the strict method performs much better, because the window size in relaxed method is hard to optimize for multiple queries. Table <ref type="table" coords="5,191.10,463.93,4.92,8.64" target="#tab_3">2</ref> presents the nDCG@10 for RL2 on TREC 2011 session data. We find that previous vs. current gives the best search accuracy. It is worth noting that distance-based scheme performs even worse than uniform scheme, which implies that the modification of user intention is complex and we cannot assume that the early query has less importance in the entire session. For RL3 and RL4, we combine several methods, including anchor texts, removing duplicated queries and re-ranking by dwell time. Table <ref type="table" coords="5,222.17,524.41,4.92,8.64" target="#tab_4">3</ref> displays the nDCG@10 for RL3 and RL4 on 2011 session track data. It illustrates that removing duplicated queries significantly improves the performance. However, neither reranking nor only involving clicked document contributes to the results. The reason may lie in that we treat the time too roughly. Bold fonts in Table <ref type="table" coords="5,256.67,558.97,4.92,8.64" target="#tab_2">1</ref> to Table <ref type="table" coords="5,299.73,558.97,4.92,8.64" target="#tab_4">3</ref> indicate a performance better than the 2011 best run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results on TREC 2012</head><p>We submit three runs to TREC 2012 session track. The run names, methods and parameters are listed in Table <ref type="table" coords="5,115.54,596.41,3.73,8.64">4</ref>, where µ is the Dirichlet smoothing parameter and k is the number of pseudo relevance feedback. The evaluation results of nDCG@10 and Average Precision (AP) by TREC are presented in Table <ref type="table" coords="5,499.47,611.05,4.92,8.64">5</ref> and Table <ref type="table" coords="5,115.82,622.57,3.73,8.64">6</ref>. They show similar trends as what we observe on the TREC 2011 data, but in a much lower range; which may imply that our query formulation methods may overfit on TREC 2011 session data. We also realize that we fail to well handle the typo in the queries like "consequenses" in TREC 2012 session 53. Nonetheless, using previous queries and eliminating duplicates keep demonstrating significant improvement in search accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.28,74.40,429.20,41.08"><head>The word indexing in a snippet built from TREC 2012 session 53 query "servering spinal cord consequenses", where bold words "spinal cord" positioned at 16 and 17 appear in the query.</head><label></label><figDesc>cubs sport injuries with girls back of knee injuries school sports injuries bladder assistance for spinal cord injuries reigning ...</figDesc><table coords="2,96.66,84.24,403.70,20.68"><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6 7 8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15 16 17</cell><cell>18</cell><cell>19</cell></row><row><cell></cell><cell cols="2">Figure 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,90.28,74.80,429.66,69.37"><head>Table 1 . The nDCG@10 for RL1 using 2011 session track data. Dirichlet smoothing method is used. µ = 4000, k = 10 for strict method and µ = 4000, k = 20 for relaxed method. Methods are compared to the baseline -original query. A significant improvement over the baseline is indicated with a † at p&lt;0.05 level and a ‡ at p&lt;0.005 level. The best run and median run in TREC 2011 are listed for comparison</head><label>1</label><figDesc></figDesc><table coords="4,90.28,116.40,427.75,27.78"><row><cell>Method</cell><cell>original query</cell><cell>strict</cell><cell>relaxed</cell><cell>Best run in 2011</cell><cell>Median run in 2011</cell></row><row><cell>nDCG@10</cell><cell>0.3378</cell><cell>0.3834</cell><cell>0.3979</cell><cell>0.3789</cell><cell>0.3232</cell></row><row><cell>%chg</cell><cell></cell><cell>+13.50%  †</cell><cell>+17.79%  ‡</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,90.28,74.80,430.05,69.37"><head>Table 2 . The nDCG@10 for RL2 using 2011 session track data. Dirichlet smoothing method and strict method are used. µ = 4000, k = 5 for uniform, µ = 4500, k = 5 for previous vs. current and distance-based. Methods are compared to the baseline -original query. A significant improvement over the baseline is indicated with a † at p&lt;0.05 level and a ‡ at p&lt;0.005 level. The best run and median run in TREC 2011 are listed for comparison.</head><label>2</label><figDesc></figDesc><table coords="5,90.28,116.40,430.05,27.78"><row><cell>Sheme</cell><cell>original query</cell><cell>uniform</cell><cell>previous vs. current</cell><cell cols="2">distance-based Best run in 2011</cell><cell>Media run in 2011</cell></row><row><cell>nDCG@10</cell><cell>0.3378</cell><cell>0.4475</cell><cell>0.4626</cell><cell>0.4431</cell><cell>0.4281</cell><cell>0.3215</cell></row><row><cell>%chg</cell><cell></cell><cell>32.47%  ‡</cell><cell>36.94%  ‡</cell><cell>31.17%  ‡</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,90.28,150.40,431.91,165.10"><head>Table 3 . The nDCG@10 for RL3 and RL4 using 2011 session track data. All runs use strict method and the configuration of µ = 4500, k = 5. Methods are compared to the baseline -original query. A significant improvement over the baseline is indicated with a † at p&lt;0.05 level and a ‡ at p&lt;0.005 level. The best run and median run in TREC 2011 are listed for comparison</head><label>3</label><figDesc></figDesc><table coords="5,90.28,192.00,431.91,115.50"><row><cell>Baseline = 0.3378</cell><cell></cell><cell></cell><cell>anchor text</cell><cell></cell><cell cols="2">nDCG@10 in 2011</cell></row><row><cell></cell><cell cols="2">all documents</cell><cell cols="2">clicked documents (RL4 only)</cell><cell>Best run</cell><cell>Median run</cell></row><row><cell>Method</cell><cell>nDCG@10</cell><cell>%chg</cell><cell>nDCG@10</cell><cell>%chg</cell><cell>RL3</cell><cell>RL3</cell></row><row><cell>all queries</cell><cell>0.4695</cell><cell>38.99%  ‡</cell><cell>0.4680</cell><cell>38.54%  ‡</cell><cell>0.4307</cell><cell>0.3259</cell></row><row><cell>remove duplicated queries</cell><cell>0.4836</cell><cell>43.16%  ‡</cell><cell>0.4542</cell><cell>34.46%  ‡</cell><cell>RL4</cell><cell>RL4</cell></row><row><cell>re-rank by dwell time (RL4 only)</cell><cell>0.4435</cell><cell>31.29%</cell><cell>N/A</cell><cell></cell><cell>0.4540</cell><cell>0.3354</cell></row><row><cell cols="7">Language model with Bayesian smoothing using Dirichlet priors is applied when performing search by</cell></row><row><cell cols="7">Lemur. The language model is a multinomial distribution, for which the conjugate prior for Bayesian</cell></row><row><cell cols="2">analysis is the Dirichlet distribution:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>𝑝</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="5,240.43,302.84,4.50,6.19;5,245.56,298.76,35.39,8.75;5,283.85,291.08,79.90,8.75;5,306.67,305.48,48.62,8.75;5,299.09,309.32,5.69,6.19"><p><p>! (𝑤|𝑑) = 𝑐 𝑤; 𝑑 + 𝜇𝑝(𝑤|𝐶)</p>𝑐 𝑤; 𝑑 + 𝜇 !</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,94.78,704.00,133.74,6.95"><p>http://www.lemurproject.org/, version 5.0</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,94.78,713.36,143.00,6.95"><p>http://durum0.uwaterloo.ca/clueweb09spam/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We attempt to generate structured Lemur query for the entire session by involving the previous queries with the rules to eliminate duplicated queries. The evaluation results show that search accuracy can be significantly increased. Although, due to overfitting to TREC 2011 and typos in TREC 2012 queries, the nugget finding method does not work well as what we expect. We still believe it is a promising approach since it greatly boosts the performance on TREC 2011 data. We will investigate the overfitting issue and refine our approach as the future work. </p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
