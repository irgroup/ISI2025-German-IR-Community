<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.34,137.98,401.31,15.12;1,184.40,159.90,243.20,15.12">The University of Illinois&apos; Graduate School of Library and Information Science at TREC 2012</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,113.29,193.80,57.37,10.48"><forename type="first">Miles</forename><surname>Efron</surname></persName>
							<email>mefron@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Library and Information Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>Urbana-Champaign 501 E. Daniel St</addrLine>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.85,193.80,64.45,10.48"><forename type="first">Jana</forename><surname>Deisner</surname></persName>
							<email>jdeisner@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Library and Information Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>Urbana-Champaign 501 E. Daniel St</addrLine>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.39,193.80,89.24,10.48"><forename type="first">Peter</forename><surname>Organisciak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Library and Information Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>Urbana-Champaign 501 E. Daniel St</addrLine>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.83,193.80,84.45,10.48"><forename type="first">Garrick</forename><surname>Sherman</surname></persName>
							<email>gsherma2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Library and Information Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>Urbana-Champaign 501 E. Daniel St</addrLine>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,446.20,193.80,52.50,10.48"><forename type="first">Ana</forename><surname>Lucic</surname></persName>
							<email>alucic2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Library and Information Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>Urbana-Champaign 501 E. Daniel St</addrLine>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.34,137.98,401.31,15.12;1,184.40,159.90,243.20,15.12">The University of Illinois&apos; Graduate School of Library and Information Science at TREC 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A20C15420133C50AF44AE112A85F050</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Illinois' Graduate School of Library and Information Science (uiucGSLIS) participated in TREC's microblog and knowledge base acceleration (KBA) tracks in 2012. Our high-level goals were two-fold:</p><p>• Microblog: Test a novel method for document ranking during real-time search.</p><p>• KBA: Compare methods of topic representation-particularly longitudinal adaptation of topic representation-for the KBA task.</p><p>Our document ranking in the microblog track is based on a behavioral metaphor. Given a query Q, we decompose Q into a set of imaginary saved searches S. Given an incoming document stream D = D 1 , D 2 , . . . , D N , we ask: what is the probability that a document D is read, given the user's query and a rational allocation of attention over his saved searches? Our KBA runs relied on the track's inherent temporality to induce and maintain expressive entity profiles during the Cumulative Citation Recommendation (CCR) task. Time influenced our approach in two ways. First, an initial entity model was built by analyzing that entity's Wikipedia edit history prior to the corpus start date. Each feature's initial probability was a function of its persistence over the edit history. Second, as our system iterated over the stream's nine-month window, an entity's model adapted in light of recently seen documents. Entities were represented as weighted feature sets using the Indri query language. Our main result hinged on the method for model adaptation over time. In adapting for temporal change, we used a monthly "memoryless" updating procedure, balancing a conservative approach to updating with more aggressive adaptation. Thus, at the end of each month of filtering, an entity's features were updated based on the month's stream. This update mixed the previous month's model with the current model; but all previous months were ignored. No new features were added; rather, we only re-estimated each feature's probability within the model. This approach allowed a helpful degree of adaptability, while hedging against model drift. Of particular interest was the effect of model updating mechanisms. We used two very different approaches to adaptation. On the supplied training data, a Markov model-based technique strongly outperformed a simple linear interpolation. However, initial results suggest that in the long run, the formalism behind updating was less important than the more basic matter of assuring adequate flexibility by some means while retaining the model's pertinence to the entity it described.</p><p>Our system used the Indri search engine and API<ref type="foot" coords="2,347.52,191.44,4.23,6.99" target="#foot_0">1</ref> for core indexing and data manipulation. Our retrieval models varied from task to task, as described below. Very little pre-processing was used in our experiments. We did not stem documents. We did no stopping at index time, though stoplists (described below) were used during construction of some baseline pseudo-relevance feedback models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Microblog Track</head><p>We submitted runs for only one of the microblog track's two tasks: real-time ad hoc retrieval. This task required teams to retrieve documents posted to the Twitter microblog service<ref type="foot" coords="2,123.71,331.46,4.23,6.99" target="#foot_1">2</ref> in response to a query issued at a particular time. Track organizers created 50 test topics and accumulated relevance judgments in a fashion similar to the standard TREC pooling method. Details of this process are available in the track overview paper.</p><p>We downloaded a version of the tweets2011 corpus on May 25-26, 2012 using the HTML scraping tools provided by the track organizers. This crawl yielded 11,908,900 indexable tweets with an HTTP status code of 200. These comprised our experimental corpus.</p><p>Our main goal was to capitalize on the essentially temporal character of people's interactions with Twitter. With this in mind, we formulated a retrieval model appropriate for inherently temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Saved-Query Model for Real-Time Search</head><p>Our tweet ranking model is based on a behavioral metaphor. Assuming that a person has an information need I articulated by the query Q = (q 1 , . . . , q n(Q) ), we imagine Q as a set of saved searches S = (s 1 , s 2 , . . . , s |S| ), where |S| is the number of unique terms in Q. This fictional user then monitors incoming documents by checking tweets retrieved by his saved searches. However, a user only has a finite amount of attention to allocate to monitoring his saved search results. Thus, we score a document D on the probability that a user would read D given a rational allocation of attention over his |S| saved searches. We denote this as P (D|S): the probability that D is read given a set of saved searches S: P (D|S) = s∈S P (D|s)P (s|S).</p><p>(</p><p>It is important to stress that the random variable D has a binary outcome: a document is either read or it is not. This is in opposition to any sort of generative model. Microblog documents are scored according to Eq. 1 in our runs.</p><p>We can read the summation in Eq. 1 as being over "the probability that D is read given that we are scanning the results of saved search s times the probability that we choose to scan the results of s" for each s ∈ S.</p><p>To estimate P (s|S) we assume that the searcher allocates his attention to each saved search s according to P (s|Q), under the intuition that the query model expresses the searcher's topical priorities. Initially, then, we simply have the maximum likelihood estimate:</p><formula xml:id="formula_1" coords="3,265.32,290.20,254.88,24.43">P0 (s|S) = n(s, Q) n(Q)<label>(2)</label></formula><p>If we have a set of (pseudo-) relevant documents R = (D R1 , D R2 , . . . , D Rk ) we have:</p><formula xml:id="formula_2" coords="3,264.35,346.31,255.85,24.43">PR (s|S) = n(s, R) n(R)<label>(3)</label></formula><p>where n(R) is the number of tokens in all k feedback documents, and n(s, R) is the frequency of s in all documents comprising R. Given a mixing parameter α ∈ [0, 1], we have the feedback distribution:</p><formula xml:id="formula_3" coords="3,216.56,432.88,298.99,12.33">PF (s|S) = α P0 (s|S) + (1 -α) PR (s|S). (<label>4</label></formula><formula xml:id="formula_4" coords="3,515.55,435.64,4.65,9.57">)</formula><p>Throughout the following discussion we set k = 20 and α = 0.5. We also restrict the number of terms added to the collection of saved searches via feedback to the the 20 terms with highest PR (s|S). The more difficult task lies in estimating the user's probability of reading a particular document retrieved by the saved search s. The key to our approach here lies in the problem of information overload.</p><p>Creating a saved search for a very common word such as the will of course lead to high recall-many relevant documents contain this term. But over time, the results of this search would be unmanageable. We assume that users have finite time to allocate to reading the results of their searches. If a search accumulates many documents, it carries a risk that the user will fail to read a relevant document due to the scarcity of attention. As the frequency of s in the document stream increases, the probability of reading a given document in its collected results decreases. Intuitively, P (D|s) monotonically decreases with the frequency of s.</p><p>To capture this intuition, we imagine the following. At time t the user reviews the results obtained by the saved search s. This consists of n s documents. While one could conceivably make various assumptions about the ordering of these n s documents for presentation to the user, we assume no particular order (i.e. a simple set-based retrieval is at work). Below we discuss the implications of this choice. The user begins to read these n s documents. After each one, he chooses either to continue onto the next document or to quit. In the interest of simplicity, we assume that λ, the probability of quitting, is constant. This leads to an exponential distribution with rate parameter λ. The probability of reading at least m documents is thus e -λm .</p><p>Let us consider a particular document D among all n s retrieved documents. We wish to know the probability that the user will read D (the event D). If D is first in the user's "in-box" we have P (D) = e -λ1 . If λ is small, this is a very likely event. But if D is buried deep among the n s documents, requiring the user to read, say, 1000 documents before he finds it, the probability of reading d is very small.</p><p>Let E(r D ) be the expected rank of document D in the results accumulated for s at time t. The probability of D being read given a result set of n s documents is:</p><formula xml:id="formula_5" coords="4,258.69,323.80,94.62,12.06">P (D|s) = e -λ•E(r D ) .</formula><p>(5)</p><p>If we imagine that the order of documents in the results for s follows no particular ordering (i.e. we imagine a simple set-based search), then E(r D ) = 1 2 s n . This amounts to having no influence of term frequency in our document scoring. However, we could easily introduce a local term frequency factor into the ranking by assuming that E(r D ) follows some function that is increasing on the frequency of s in D. Alternatively, we might imagine a temporal ordering, with newer documents promoted via and expectation using a distribution favoring recently published information. However, we leave that for future work.</p><p>In this work, we assumed a uniform distribution for document ranks in saved search results, which means that we relied only on binary features. As in the binary independence model, we effectively treated each query term's occurrence in a document as a Bernoulli trial.</p><p>The choice of λ in Eq. 5 governs the importance of an IDF-like factor in document ranking. Setting λ = 0 leads document frequency to play no role in the utility of a word. On the other hand, large values of λ lead to a very strong penalty for common words.</p><p>In our work, based on ad hoc tuning using training queries, we set λ = 0.001 in our experiments.</p><p>In many ways, the "attention model" given in Eq. 1 is analogous to simple TF-IDF ranking. Its first factor is a TF weight, the second factor expresses the frequency of each term in the query (or feedback documents), and the final term is similar to IDF. However, our model offers several points of flexibility:</p><p>1. Tunable IDF influence via selection of λ.</p><p>2. Arbitrary functional form for IDF influence (e.g. Weibull distribution for quit probability instead of simple exponential).</p><p>3. Ability to account for arbitrary document features (query-dependent or independent) by the expectation in Eq. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Submitted Runs</head><p>We submitted four runs, all based on the model described in the previous section, with varying levels of additional sophistication. Except for uiucGSLIS01, none of our runs relied on external or future evidence. In uiucGSLIS01, we used external evidence in the form of the vocabulary from the TREC AP 88-89 corpus to assess the degree to which a tweet's language conformed to conventional English. In almost every sense, like our other runs, uiucGSLIS01 ignored future evidence. However, the weights in the learned model were trained on the topics from the 2011 microblog track. Some of these topics have query times posted after query times for the 2012 topics, so in a strict sense we were learning from future data in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">uiucGSLIS04</head><p>This run used the retrieval model described in Section 2.1. It used no feedback and was intended to serve as a simple baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">uiucGSLIS02</head><p>This run used the retrieval model described in Section 2.1. We used pseudo-relevance feedback (20 documents, 20 terms, with the feedback model interpolated with the original query via a mixing parameter of 0.5). An important point is that our relevance feedback method used no stoplist (unlike the relevance model described below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">uiucGSLIS03</head><p>This run used the retrieval model described in Section 2.1. We used pseudo-relevance feedback (20 documents, 20 terms, with the feedback model interpolated with the original query via a mixing parameter of 0.5). Additionally, each document's score in this run was multiplied by an exponential factor calculated in the method described in <ref type="bibr" coords="5,275.52,562.98,10.91,9.57" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">uiucGSLIS01</head><p>This run was intended to show our most competitive effort. The ranking model described above was put into a larger learning to rank framework. The features used in our ranking were largely due to observations from successes of last year's microblog participants. The weights shown in Table <ref type="table" coords="6,243.74,286.18,5.45,9.57" target="#tab_0">1</ref> we learned by a simple coordinate ascent, using the 2011 microblog topics for training, with MAP as the objective function. The main surprise in these weights in the influence given to document recency. The recency score was a simple exponential decay function of the age of the document with respect to the query; the rate parameter on the exponential was λ = 0.01, with time measured in days. Previous research has shown that many of the 2011 topics were in essence "recency queries," with a preponderance of relevant documents posted near query time. This led to the strong temporal influence seen here. We found, however, that the 2012 topics were much less sensitive to time. The row in Table <ref type="table" coords="6,197.28,622.45,5.45,9.57" target="#tab_1">2</ref> labeled median shows the median effectiveness scores aggregated over all official runs. The rows labeled QL and Rel Model were not submitted, but are shown as a baseline. The QL row gives results using the standard query likelihood model; this is intended as a point of comparison with uiucGSLIS04. The row labeled Rel Model shows results obtained from applying feedback via relevance models with feedback configurations as in uiucGSLIS02, with a strong stoplist applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Microblog Ad Hoc Empirical Results</head><p>Not surprisingly, uiucGSLIS01-a many-featured model-performed well. However, its advantage over uiucGSLIS02 and 03 is actually slim. In comparison to uiucGSLIS04 we can see that the lion's share of effectiveness in these runs is due to the influence of relevance feedback. Except for uiucGSLIS04, all of our runs outperform the population median.</p><p>Comparing uiucGSLIS with QL shows that the attention model ranking method is performing near the quality of a well-tested language modeling approach. More intensive parameterization of the attention model may well fill this gap, especially insofar as the difference in effectiveness between these runs is not statistically significant. Additionally, relaxing our reliance on binary feature representation may help improve the settings of uiucGSLIS04.</p><p>We were gratified by the comparison of uiucGSLIS02 and the Rel Model run. This comparison showed feedback in the attention model outperforming the more established feedback method. Additionally, while obtaining strong scores with relevance models required the use of an aggressive stoplist, feedback in our approach does not use a stoplist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TREC 2012 KBA Track</head><p>The Cumulative Citation Recommendation (CCR) task in this year's knowledge base acceleration (KBA) track challenged teams with the following scenario: given a Wikipedia entity E represented by a Wikipedia node E, monitor an incoming document stream, signaling to the editors of E when an "edit-worthy" document is seen. Track organizers identified 29 target entities. Teams monitored a stream of timestamped documents-mostly news articles and blog posts-spanning approximately a nine-month window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We used only the "cleansed" subset of the KBA document stream. The cleansed subset was generated by track organizers by limiting documents to those with a "reasonable" chance of being in English. We chose to work with the cleansed data for two reasons: 1) Its relatively small volume reduced engineering challenges, and 2) all relevance judgments were limited to documents in the cleansed subset. Track organizers also made named entity (NER) data for each document available to participants. However, NER information was not central to our interests so we did not use this information.</p><p>Relevance assessments over the 29 target entities took on one of four integer values corresponding to four categories. We itemize them in Table <ref type="table" coords="7,385.03,613.56,4.24,9.57" target="#tab_2">3</ref>. The rightmost column of Table <ref type="table" coords="7,122.01,627.11,5.45,9.57" target="#tab_2">3</ref> stems from our interpretation of what constituted relevance. We limited relevant judgments to central documents. In our discussion below, we also report results using NDCG, which used the raw assessment scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Motivation and Approach</head><p>Because this was the first year that KBA ran at TREC, the best way to consider the task was an open question. For instance, participants could think of the system as a binary classification problem. Alternatively, we could approach it as a document ranking problem. We chose the latter approach, aiming to score each document with a real-valued number instead of a binary +/-decision. This outcome, we reasoned, would allow a Wikipedia editor to browse documents in decreasing order of predicted relevance, while also admitting a simple thresholding if she wished to see only those documents judged to be either "relevant," "central" or both. Lastly, all system parameters were set by experimentation on the supplied training data. No rigorous optimization was done.</p><p>The main focus of our work was entity representation. We explored the ability of the tasks' inherent temporality to give information that would allow the profile for E to evolve over time. We also experimented with the value of the edit history of the Wikipedia page for E in learning a topic representation.</p><p>Each entity E i was represented by an indri query Q i . At the outset of a run, we began with the initial query Q 0 i . At any time t in the iteration over the document stream, we have the current query Q t i . We will return to our methods for estimating Q 0 i and Q t i , but first there are a few details to note.</p><p>Taking a very conservative approach, we only returned documents that contained an exact match on the canonical entity name (i.e. the name represented in the Wikipedia URL) for E or a very close match. For instance, the query for entity Aharon Barak contained the filtering statement #scoreif(#1(Aharon Barak)). However, this stricture ignored disambiguation information. So, the entities Basic Element (company) and Basic Element (music group) both filtered on #scoreif(#1(Basic Element)). Similarly, the URL Frederick M. Lawrence contained the filter #scoreif(#syn(#1(Frederick Lawrence) #1(Frederick M Lawrence))).</p><p>Our queries also combined several types of information. First, documents were promoted based on the log of their inbound link count plus 1. This factor was implemented as a document prior (the log-counts were divided by the sum of all log-counts) to achieve a prior distribution over documents. Second, the indri query described below were applied to both the text of documents and their resolved URLs. The weight of evidence in URL text was 2.0 versus 1.0 in the document text. Lastly, in each indri query, the learned model was weighted 0.35 while the name of the entity was weighted 0.65.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Initial Query Model Estimation</head><p>For an entity E we created an initial query Q 0 by analyzing the edit history of E. We represent this history by the sequence of documents H = E 1 , E 2 , . . . , E k where E k is the last version of E's Wikipedia entry before the start of the KBA document stream. Iterating over this history we extracted a set of features and weights used to represent E.</p><p>Extracted features were any wikiText that appeared as a link using the [[.]] notation. These features were treated as quoted phrases.</p><p>For each feature f i we estimated P (f i |H). To estimate these probabilities we simply calculated:</p><formula xml:id="formula_6" coords="9,238.63,332.12,281.57,29.64">P (f i |H) = 1 k E∈H Pml (f i |E).<label>(6)</label></formula><p>As a point of comparison, we built a set of queries whose weights ignored edit history in favor a estimating probabilities only from the most recent version E k . However, this condition did not inform any of our official runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Query Model Adaptation</head><p>A key factor in KBA is the change that information about an entity E can undergo over time. We reasoned that over a nine-month window, the initial query model Q 0 would become "stale." To remedy this, we desired some sort of adaptation over our models. However, we did not assume any explicit feedback. Likewise, we speculated that it would be risky to let incoming documents influence the model too much, in which case the risk of topic drift would become large. Our query model adaptation utilized a month-by-month, memoryless updating procedure. That is, at the beginning to the KBA process we used Q 0 to score incoming documents. After one month, the system gathered the highest scoring n = 20 documents from the initial month, updated the model based on these results (as described below) and then processed the next month's documents. After each month, the most recently accumulated documents were used to update the model.</p><p>To avoid query drift, we took a conservative approach and allowed models to change only in a re-weighting of the originally observed features. i.e. No features were added to the model, although as described below, some features' weights went to zero.</p><p>The model was memoryless in the sense that at time t (for t &gt; 0), the parameters of the model were fully specified by the results from month t -1 and t. i.e. Months t -2, t -3... exerted no influence. This choice was made in order to allow the model to avoid stagnation. Thus at time t &gt; 0 we had the model</p><formula xml:id="formula_7" coords="10,265.30,187.83,254.90,13.13">Q t = φ(Q t-1 , R t ) (7)</formula><p>where R t is the set of pseudo-relevant documents retrieved during month t -1 by query Q t-1 . The two runs described below vary only in their specification of the updating function φ(•, •).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Submitted Runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">gslis adaptive</head><p>This run generated a query Q t based on the previous month's query Q t-1 and the pseudorelevant documents R via a simple linear combination:</p><formula xml:id="formula_8" coords="10,203.52,346.62,316.68,13.39">P a (f |Q t ) = αP (f |Q t-1 ) + (1 -α) Pml (f |R)<label>(8)</label></formula><p>for a mixing parameter α ∈ [0, 1]. We simply set α = 0.5..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">gslis mult</head><p>This run generated a query Q t based on the previous month's query Q t-1 and the pseudorelevant documents R using a log-linear model:</p><formula xml:id="formula_9" coords="10,161.06,456.93,359.15,24.43">P m (f |Q t ) = [λ • 1 |Q| + P (f |Q t-1 )] • [(1 -λ • 1 |Q| ) + Pml (f |R)]<label>(9)</label></formula><p>for a smoothing parameter λ = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">KBA Empirical Results</head><p>Table <ref type="table" coords="10,120.59,543.31,5.45,9.57" target="#tab_3">4</ref> summarizes the outcome of our runs in comparison to two participant-wide aggregatesmean and median F1. Two results are clear from these data:</p><p>1. Neither of our updating approaches (gslis adaptive or gslis mult) saw much advantage over the other.</p><p>2. Our approaches, though significantly stronger than the group-wise mean with respect to F1 operate with effectiveness that is nearly identical to the group-wise median, at least with respect to F1.</p><p>Figures <ref type="figure" coords="11,148.19,98.55,5.45,9.57" target="#fig_3">1</ref> and<ref type="figure" coords="11,179.96,98.55,5.45,9.57" target="#fig_7">2</ref> show the performance of several models measured at each month in the KBA corpus. The upper-left panel compares our official runs to a "toy" (i.e. straw man") approach. The toy system simply weights occurrences of the exact entity title at 2.0 and individual words from the title at 1.0, using this weighted feature set as a simple indri query.</p><p>A few points of interest emerge from Figures <ref type="figure" coords="11,339.12,166.30,5.45,9.57" target="#fig_3">1</ref> and<ref type="figure" coords="11,372.66,166.30,4.24,9.57" target="#fig_7">2</ref>. First, our two methods of updating models largely track in parallel, but do occasionally (e.g. at month 5) give different results. Second, our approaches are only slightly superior to the toy system. But as we can see from the horizontal red line in the figure, the straw man actually performed near the TREC median. Lastly, performance declines over time. Though this appears to be a problem with query drift, we might keep in mind that the toy system's model is in fact static. Instead of suffering problematic query drift, it seems to be that case that our performance declines because the KBA problem becomes more difficult (at least with respect to these metrics) over time. In fact, during conversations with track organizers, it was suggested that new sources of documents entered the corpus in the later months, and that these were in some sense noisier than the initially dominant documents. Thus the performance decline may be an artifact of the data. Two points are worth elaborating regarding these data. First, our system was designed explicitly as a ranking system (i.e. a document routing platform). We are thus eager to compare our performance using other teams' results using rank-based measures. Secondly, the success of the "toy" system (c.f. Figure <ref type="figure" coords="11,309.06,518.38,4.85,9.57" target="#fig_3">1</ref>) suggests that the community has a good deal of research to do if we hope to perform KBA successfully.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,91.80,504.09,428.40,9.57;12,91.80,517.63,61.51,9.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: F1 Observed on a Month-by-Month Basis. Red line is a the mean F1 of a "straw man" model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="13,91.80,504.09,428.40,9.57;13,91.80,517.63,176.30,9.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mean Average Precision Observed on a Month-by-Month Basis. Red line is the mean MAP of a "straw man" model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,154.36,105.13,303.29,143.32"><head>Table 1 :</head><label>1</label><figDesc>Feature weights in ranking model for uiucGSLIS1 run.</figDesc><table coords="6,159.98,130.08,35.94,9.57"><row><cell>Feature</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,144.91,462.65,322.18,129.77"><head>Table 2 :</head><label>2</label><figDesc>Initial Report of Microblog Ad Hoc Experimental Results.</figDesc><table coords="6,211.44,487.60,189.12,104.81"><row><cell>Run</cell><cell>MAP Rprec</cell><cell>P30</cell></row><row><cell>median</cell><cell cols="2">0.1487 0.1869 0.1808</cell></row><row><cell cols="3">uiucGSLIS01 0.1829 0.2080 0.2186</cell></row><row><cell cols="3">uiucGSLIS02 0.1751 0.2020 0.1972</cell></row><row><cell cols="3">uiucGSLIS03 0.1717 0.1959 0.1983</cell></row><row><cell cols="3">uiucGSLIS04 0.1259 0.1482 0.1599</cell></row><row><cell>QL</cell><cell cols="2">0.1333 0.1602 0.1627</cell></row><row><cell>Rel Model</cell><cell cols="2">0.1558 0.1912 0.1684</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,91.80,143.71,428.40,100.55"><head>Table 3 :</head><label>3</label><figDesc>TREC KBA Document-Entity Annotations with Associated Relevance Interpretations.</figDesc><table coords="8,188.50,180.10,235.00,64.16"><row><cell>Score Label</cell><cell>Our interpretation</cell></row><row><cell cols="2">-1 garbage Not relevant</cell></row><row><cell cols="2">0 neutral Not relevant</cell></row><row><cell cols="2">1 relevant Not relevant in our evaluations</cell></row><row><cell>2 central</cell><cell>Relevant in our evaluations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,91.80,346.95,428.40,100.55"><head>Table 4 :</head><label>4</label><figDesc>uiucGSLIS KBA Experimental Results. Statistics are calculated using only "central" documents for relevance .</figDesc><table coords="11,234.50,383.33,143.00,64.16"><row><cell>Run</cell><cell>F1</cell><cell>SU</cell></row><row><cell cols="3">TREC median 0.289 0.333</cell></row><row><cell>TREC mean</cell><cell cols="2">0.220 0.311</cell></row><row><cell>gslis adaptive</cell><cell cols="2">0.284 0.339</cell></row><row><cell>gslis mult</cell><cell cols="2">0.284 0.337</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.39,595.24,108.27,7.47"><p>http://lemurproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.39,606.20,84.73,7.47"><p>http://twitter.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,108.77,590.65,411.43,9.57;11,108.77,604.20,411.43,9.57;11,108.77,617.75,24.85,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,163.76,590.65,356.44,9.57;11,108.77,604.20,37.96,9.57">Query-specific recency ranking: Survival analysis for improved microblog retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,168.48,604.20,277.58,9.57">SIGIR 2012 Workshop on Time-aware Information Access</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>#TAIA2012</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
