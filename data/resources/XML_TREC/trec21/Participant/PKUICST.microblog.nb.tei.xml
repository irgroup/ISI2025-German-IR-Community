<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.13,72.23,339.47,16.98">PKUICST at TREC 2012 Microblog Track</title>
				<funder ref="#_TA3Tzxk">
					<orgName type="full">National Natural science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,115.55,125.01,59.44,11.32"><forename type="first">Feng</forename><surname>Liang</surname></persName>
							<email>liangfeng@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.27,125.01,74.96,11.32"><forename type="first">Runwei</forename><surname>Qiang</surname></persName>
							<email>qiangrw@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.51,125.01,68.87,11.32"><forename type="first">Yihong</forename><surname>Hong</surname></persName>
							<email>hongyihong@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.66,125.01,39.53,11.32"><forename type="first">Yue</forename><surname>Fei</surname></persName>
							<email>feiyue@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,419.46,125.01,66.78,11.32;1,486.24,118.64,1.54,8.37"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
							<email>yangjw@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.13,72.23,339.47,16.98">PKUICST at TREC 2012 Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A06F04726A4AB387D5EEF366F3899904</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the PKUICST's entry into the TREC 2012 Microblog track. In this year of microblog track, we participate in both the Real-time Adhoc Task and Real-time Filtering Task. In the Real-time Adhoc Task, we designed and conducted a series of experiments based on different retrieval models, namely Real-time Tweet Ranking (RTR) model and learning to rank framework. In the Real-time Filtering Task, we adopted various strategies to determine the filtering threshold. Official results demonstrate that our approach obtains convincing performances and more unofficial runs lead to some further conclusions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The popularity of microblog has significantly increased information seeking behaviors in the microblogging environments. To explore the search behavior and boost the search performance in the real-time environment, TREC introduced a novel pilot track named microblog track last year. In this year of microblog track, two tasks are introduced, namely Real-time Adhoc Task and Real-time Filtering Task, whereby a user's information need is represented by a query at a specific time. In the real-time adhoc task, the user wishes to see the most recent but relevant information to the query. However, different from last year, participants are required to return top 10,000 tweets prior to the query time per topic according to their relevance score. Hence, systems should favor relevant and highly informative tweets about the query topic, which makes this task skin to ad-hoc search on Twitter. In this task, we adopt both RTR model and state-of-art learning to rank framework to improve the retrieval effectiveness.</p><p>The real-time filtering task aims at deciding if subsequently posted tweets are relevant for a query entered at a particular point in time. In this task, the user is interested in new relevant tweets, thus to keep up to date about a developing topic. The topics used for the real-time filtering task are the same as last year, which provides a way to use supervised methodology. Hence, we train different models from training data and try various strategies to determine the filtering threshold which is of vital importance in the adaptive filtering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">REAL-TIME ADHOC TASK</head><p>In this section, we describe our approach for the Real-time Adhoc Task in detail. * Corresponding author. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Overview</head><p>Our system contains two major steps: initial search using RTR Model <ref type="bibr" coords="1,367.80,343.07,9.21,7.86" target="#b7">[7]</ref>, which is based on statistic language model and re-ranking the initial result sets with Ranking SVM <ref type="bibr" coords="1,543.64,353.54,9.21,7.86" target="#b5">[5]</ref>. The architecture of our system is shown in Figure <ref type="figure" coords="1,516.13,364.00,3.58,7.86">1</ref>. Corpus and query are processed in parallel.</p><p>Our system does some necessary text preprocessing to the corpus and query, such as stemming and stopwords elimination. The links within tweets are very informative as they are always aimed at tracking breaking news stories, recommending interesting video clips and brand marketing <ref type="bibr" coords="1,316.81,437.22,9.21,7.86" target="#b3">[3]</ref>. Thus, we crawl all the links and extract topic information from the pages we get <ref type="bibr" coords="1,424.21,447.68,10.52,7.86" target="#b7">[7]</ref>, forming a new corpus called TopicInfo Corpus. Another way to use topic information is directly replacing the links in the original tweet and generate a new DE(Document Expansion) Corpus. After the retrieval step with the help of RTR Model, the system produces top 20,000 scoring candidate tweets. Then we collect features for these candidate tweets, such as semantic features, tweetrelated features and temporal features, then use learning to rank framework to re-rank the candidate tweets. Lastly, we choose the top 10,000 relevant tweets as the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preprocessing</head><p>Tweet11 corpus was obtained using a donation of the unique identifiers of a sample of tweets from Twitter <ref type="bibr" coords="1,539.04,583.19,13.51,7.86" target="#b10">[10]</ref>. We crawled the HTML version copy of the corpus with the provided tools.Table <ref type="table" coords="1,402.73,604.11,4.61,7.86" target="#tab_0">1</ref> shows basic statistics of our HTML version acquisition on June 23, 2011. Given the corpus and topic set, we do the following preprocessings.</p><p>• Corpus status update: For the sake of fairness, organizers re-crawled the Tweet11 corpus at the beginning of this year's track, and offered a list of valid IDs for corpus update. We filter all the invalid IDs to generate the Tweet12 corpus.   <ref type="table" coords="2,76.21,546.46,3.58,7.86" target="#tab_2">2</ref>.</p><p>• Non-English filter: We filter out all tweets that have words encoded with non-ASCII code.</p><p>• Simple retweet elimination: We eliminate tweets that begin with 'RT' with the consideration that these tweets are simple retweets without any other additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Retrieval Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Real-Time Tweet Ranking Model</head><p>Given a real-time search problem, the ideal system should consider: 1) build a dynamic dataset for each query to avoid using the future resources; 2) use expansion techniques to enrich the representation of both query and document; 3) make a tradeoff between relevance and recentness. To solve these challenges, Feng et al. <ref type="bibr" coords="2,408.41,68.10,9.72,7.86" target="#b7">[7]</ref> propose a Real-time Tweet Ranking (RTR) model, which highlights the following aspects: 1) describe a two-stage pseudo-relevance feedback query expansion to estimate a query language model. 2) propose two ways to expand document with the shortened URL's information to enrich the representation of document. 3) suggest several temporal re-ranking functions and two representations of temporal profile to evaluate the temporal aspect of documents.</p><p>To rank tweets for a given topic, RTR model is to estimate the probability of generating a query Q given the content D and timestamp t of the tweet as follows:</p><formula xml:id="formula_0" coords="2,370.25,199.28,185.68,19.75">P (Q|D, t) = P (t|Q, D) • P (Q|D) P (t|D)<label>(1)</label></formula><p>Assuming that P (Q|D) ∝ Score(Q, D) which can be calculated using Kullback-Leibler retrieval model <ref type="bibr" coords="2,502.55,238.25,13.51,7.86" target="#b14">[14]</ref>, and that P (t|D) can be assumed as a constant because it is queryindependent, the ranking formula can be rewritten as follows:</p><formula xml:id="formula_1" coords="2,327.52,286.75,228.40,46.88">P (Q|D, t) ∝ P (t|Q, D) • P (Q|D) ∝ P (t|Q, D) • Score(Q, D) = P (t|Q, D) • ∑ w∈V P (w| θQ) • log P (w| θD)(2)</formula><p>With the ranking formula, the retrieval task is reduced to three subtasks, i.e. the estimation of query model θQ, the estimation of document model θD and the temporal re-ranking component P (t|Q, D), respectively. Considering that this year's task doesn't require participants to rank returned tweets by timestamp, we just implement the estimation of query model and the estimation of document model.</p><p>For the estimation of query model, RTR model adopts a two-stage pseudo-relevance feedback query expansion as follows: 1) in the first stage, a single tweet is picked up to generate topical words using the maximum likelihood estimator. 2) in the second stage, a group pseudo-relevant tweets are used to distill the relevant content by implementing the model-based feedback approach <ref type="bibr" coords="2,478.19,489.48,13.50,7.86" target="#b15">[15]</ref>.</p><p>It is important to point out that the single tweet (i.e. issue tweet), which is generated in the first stage query expansion can be used to calculate another score with both original tweets and topic information for further semantic representation. Overall, the estimation of query model can be represented as:</p><formula xml:id="formula_2" coords="2,340.71,567.00,215.22,11.60">P (w| θQ ) = (1 -α) • P (w| θQ) + α • P (w| θP RF 1 ) (3)</formula><p>For the estimation of document model, RTR model presents two ways to utilize the external resource, i.e. TopicInfo corpus. One is to merge the original tweet T and topic information I if exists to form a new document and estimate the document language model using Dirichlet Smoothing <ref type="bibr" coords="2,530.72,628.32,14.33,7.86" target="#b14">[14]</ref> as follows:</p><formula xml:id="formula_3" coords="2,372.81,652.48,183.12,20.26">P (w| θD) = c(w, D) + µP (w|C) |D| + µ (4)</formula><p>Another approach is to smooth the original document model using linear incorporation with the topic information language model estimated based on TopicInfo corpus, and each model is smoothed using Dirichlet method as well. The doc-   </p><formula xml:id="formula_4" coords="3,65.08,142.60,231.37,80.12">( , ) M M N q t   11 ( , ) MQ q t  1 2 ( , ) MQ q t  1 1 ( , ) MQ M Q N q t   ... ... ... 11 ( , ) M q t  1 2 ( , ) M q t  Feature Selection Vectors 11 ( , ) M S q t  1 2 ( , ) M S q t  1 1 ( , ) M M N S q t   ... 11 ( , ) MQ S q t  1 2 ( , ) MQ S q t  1 1 ( , ) MQ M Q N S q t   ... ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Learning to Rank Framework</head><p>Learning to rank is a data-driven approach which integrates a bag of features in the model effectively <ref type="bibr" coords="3,257.50,480.07,9.21,7.86" target="#b2">[2]</ref>. Our system adopts the same framework that Duan et al <ref type="bibr" coords="3,263.77,490.53,9.73,7.86" target="#b2">[2]</ref> proposed except that we select different features for the learning algorithm. The basic learning to rank framework is shown in Figure <ref type="figure" coords="3,93.56,521.91,3.58,7.86" target="#fig_0">2</ref>.</p><p>In order to train an effective model, adequate training data and useful feature set are required. The candidate tweets is produced by the RTR model described in section 2.3.1. Our training set is generated from official result set of TREC'11 Microblog Track, the relevance distribution in the result set is listed in Table <ref type="table" coords="3,177.42,584.68,4.61,7.86" target="#tab_4">3</ref>.Our system trains Ranking SVM <ref type="bibr" coords="3,77.32,595.14,9.72,7.86" target="#b5">[5]</ref> as learning to rank model.</p><p>Three major types of features are used in our model: semantic features, tweet related features and temporal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic features</head><p>Semantic features refer to the features that describe the relevance between the query and tweets, such as the Kullback-Leilbler divergence between query model and document model. Using different query or document model can generate different features that may reflect different aspects of querydocument similarity. For example, using TopicInfo Corpus, we may get the relevance between the tweet link and user's query while using Origin Corpus, we can get the content relevance between the query and the tweet text.</p><p>In our approaches, we propose four semantic features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Feature</head><p>Recentness is also an important aspect in the microblogosphere. In TREC'11 Microblog Track, participants are required to produce a ranked list of tweets from the latest timestamp to the earliest. Thus, tweets posted an hour ago are often more worthy than those were updated a day before. So we use the normalized time difference between the time when the query issued and the time when the tweet published as temporal feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Result Analysis</head><p>In this section, we analyze the results of our approaches. In this year's track, all submitted runs were pooled to depth 100 (while in last year the pooling depth is 30) according to the retrieval scores indicated in each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Analysis of Official Runs</head><p>Table <ref type="table" coords="3,352.47,441.20,4.61,7.86" target="#tab_6">4</ref> show the performance values of our submitted four runs. The primary evaluation measures for this year's task are still P@30 (Precision at 30), MAP (Mean Average Precision) and R-Prec(R-Precision). Our training metric in learning to rank framework is MAP.  From the evaluation result, we can see that training on allrel topics is better than training on highrel topics. Compared with PKUICST2, PKUICST1 achieves 4.64% and 5.00% further increases in P@30 and MAP, respectively. As we known, the official evaluation used only highly relevant tweets as relevant, however we didn't gain any improvements by training on highrel topics, further investigation is needed for this issue. Origin candidate is even better than the DE candidate according to the official evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Analysis of Unofficial Runs</head><p>In addition to the submitted runs, we also do some complementary experiments on TREC2011 data for comparison. These experiments aim at comparing the selection of candidate tweet sets, semantic features and the effectiveness of temporal feature. All models in the experiments adopt learning to rank algorithm and apply repeated random subsampling validation. The metric used in our learning algorithm is MAP, which is one of the major evaluation measures in TREC'11 microblog track.</p><p>The models we compare adopt different candidates or feature sets. The model description is shown in Table <ref type="table" coords="4,264.86,377.90,3.58,7.86" target="#tab_7">5</ref>. The second column describes the corpus used in the RTR Model to generate the candidate tweet set. With the optimum parameters C (trade-off between training errors) in SV M rank <ref type="bibr" coords="4,53.80,419.74,9.20,7.86">[6]</ref>, we re-rank the candidate tweets and generate the final results. The performance of each run is shown in Table <ref type="table" coords="4,263.27,531.54,3.58,7.86" target="#tab_8">6</ref>. The performance of DEBase and OrgBase is basically the same, so is OrgBase and OrgTime. DEIssueTitle gains about 2.3% improvements in MAP score compared with DEBase.</p><p>According to these experiments, we conclude that:</p><p>• When feature set are determined, candidate set influence a little in the unofficial runs.</p><p>• Temporal feature may not be effective in the current framework.</p><p>• More semantic features can improve the MAP score to some extent.</p><p>The conclusion may not be the same with the ones from the official runs, as the unofficial runs are all tested on the TREC2011 Tweet Corpus. As now the evalation tool for TREC2012 is published, we'll do more experiments for further conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">REAL-TIME FILTERING PILOT TASK</head><p>This section describes our approach for Real-time Filtering Pilot Task. Filtering differs from searching in that documents arrive sequentially over time. The Real-time filtering task aims at simulating online time-critical tweet filtering applications, which means that potentially relevant tweet must be presented immediately to the user.  Figure <ref type="figure" coords="4,354.39,549.20,4.61,7.86" target="#fig_1">3</ref> shows the architecture of our filtering system. In our filtering system, we do the same preprocessing as the Real-time Adhoc Task for the corpus at first. The corpus index is still built with the help of the Lemur IR toolkit 1 . The Filtering Model and the Relevance Feedback Model are the core parts of our system. For each tweet, we use the Filtering Model to estimate its relevance score with respect to the current query and generate the filtering result according to the relationship between the relevance score and threshold obtained in the training phase. We vary the decision threshold to get the best evaluation result based on P@30 evaluation metric. The Relevance Feedback Model aims at expanding the keywords of the query and it can be regarded as an evolvement of topic. To achieve the goal of filtering task, we try different filtering models and feedback algorithms which will be discussed in detail next. When the filtering action is done, new tweets from the foreground corpus will be added to the background corpus. Thus we can update the index dynamically with the time <ref type="bibr" coords="5,234.79,89.02,9.21,7.86" target="#b8">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Real-Time Tweet Filtering Model</head><p>The filtering model we used in the filtering model will be introduced in this section. For each model, we train the decision thresholds to make evaluation result best based on P@30 evaluation metric in the training stage except the SVM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Baseline Models</head><p>In the Filtering Model, several traditional retrieval models such as Boolean Model, Language Model and Vector Space Model are applied as the scoring method <ref type="bibr" coords="5,213.51,214.42,12.81,7.86" target="#b9">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boolean Model</head><p>The Boolean Retrieval Model was used by the earliest search engines and is still in use today. And Boolean Retrieval System achieves its goal by judging whether the document contains the keywords of query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Model</head><p>Language Model described in the Adhoc task is also applied to the filtering task while we still use the Kullback-Leibler divergence as the relevance score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vector Space Model</head><p>Vector space model (VSM) is an algebraic model for representing text documents as vectors of identifiers. We express the tweet and the query as vector.</p><p>-→ Ti = (w1i, w2i, w3i,</p><formula xml:id="formula_5" coords="5,116.88,402.16,112.93,30.73">• • • wni) -→ Qi = (w1q, w2q, w3q, • • • wnq)</formula><p>The T F IDF weighting scheme is adopted as the term weight and the Cosine Similarity Metric is used to evaluate the relevance between tweets and query. The Cosine Similarity Metric is defined as Eq.8.</p><formula xml:id="formula_6" coords="5,120.18,482.09,172.74,28.85">Sim = cos θ = -→ Ti • -→ Q -→ Ti • -→ Q (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Two-Stage Filtering Model Combined VSM and Improved Boolean Model</head><p>We propose an efficient but simple combination model in the filtering task. Figure <ref type="figure" coords="5,157.24,554.50,4.61,7.86" target="#fig_3">4</ref>  For each tweet, we calculate its relevant score based on the Improved Boolean Model if it survives in the Vector Space Model, which depends on the Cosine Similarity Metric. Here, we use an improved Boolean Model instead of the traditional Boolean Model. Since each tweet contains no more than 140 words, the tweet is likely to be more relevant to the query if it contains a high proportion of keywords. Thus we define its relevance score as Eq.9.</p><formula xml:id="formula_7" coords="5,111.90,684.44,177.09,20.26">Sim(T, Q) = |{t|t ∈ (T ∩ Q)}| |Q| (<label>9</label></formula><formula xml:id="formula_8" coords="5,288.99,690.24,3.92,7.86">)</formula><p>where T and Q denote the term set of the tweet and query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">SVM-based Model</head><p>Support Vector Machine (SVM) is a robust machine learning methodology which has been shown to yield state-of-theart performance for text classification <ref type="bibr" coords="5,474.20,281.05,9.21,7.86" target="#b4">[4]</ref>. D. Sculley et al <ref type="bibr" coords="5,316.81,291.51,14.32,7.86" target="#b13">[13]</ref> also demonstrates that online SVMs do indeed provide good performance for online spam filtering. Thus we try to combine SVM in our approach to gain a more robust performance in tweet filtering task. In our experiment, libSVM tool developed by Chih-Chung Chang and Chih-Jen Lin <ref type="bibr" coords="5,546.20,333.36,9.73,7.86" target="#b1">[1]</ref> is used as our core SVM solver.</p><p>Generating machine learning features from text could be done in a variety of ways, especially when the text may include hyper-content and meta-content such as tweet link and hash tag. All the tweet related features mentioned in section 2.3.2 are used in our algorithm. The score generated by the language model described in section 3.2.1 is used as semantic feature. In addition, the query words' average inverse document frequency and the score generated by the Boolean model which is described in Eq.9 are both candidate features.</p><p>The SVM tradeoff parameter C must be tuned to obtain the optimal performance. The filtering task provides us 10 topics whose related tweets are classified as high relevant, minimally relevant and non-relevant. To tune our system parameters, five-fold cross validation was used in our experiment to determine the optimum parameter C of SVM.</p><p>With the optimum parameter, we classify the query documents pair of the remaining 39 topics as 3 levels: 3(highly relevant), 2(minimally relevant) and 1(non-relevant). These label info can help us filter tweets. Our relevance judging strategy is described as follows: (1) if a tweet is labeled by SVM as minimally relevant or highly relevant, we output yes for this tweet. (2) Else we will judge the tweet by the score calculated from the language model. if the score is higher than the static threshold tuned using the training set, then system outputs yes. (3) Otherwise, outputs no.</p><p>To conclude, we consider SVM as a high performance classifier that may merely miss relevant tweets, and these missing tweets are expected to be judged correctly by static score threshold method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relevance Feedback Model</head><p>The Relevance Feedback Model aims at expanding the keywords of the query and it can be regarded as an evolvement of topic. And we apply two relevance feedback models </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Rocchio Algorithm</head><p>The Rocchio algorithm <ref type="bibr" coords="6,158.60,176.99,14.33,7.86" target="#b12">[12]</ref> is based on a method of relevance feedback found in information retrieval systems which stemmed from the SMART Information Retrieval System. And the Rocchio feedback approach is developed with the help of Vector Space Model. The algorithm is based on the assumption that most users have a general conception of which documents should be denoted as relevant or nonrelevant. Therefore, the user's search query is revised to include an arbitrary percentage of relevant and non-relevant documents as a means of increasing the search engine's recall, and possibly the precision as well.</p><p>The Rocchio Feedback Algorithm is described as Eq.10.</p><p>-</p><formula xml:id="formula_9" coords="6,77.10,302.73,211.72,30.31">→ Qm = α -→ Q0 + β ∑ T j ∈T R -→ Tj |TR| + γ ∑ T l ∈T N R -→ T k |TNR| (<label>10</label></formula><formula xml:id="formula_10" coords="6,288.82,318.59,4.09,7.86">)</formula><p>where TR denotes the set of relevant tweets while TNR denotes the set of non-relevant tweets, and Qm represents the feature vector after m tweets. We set γ as zero since we can only use the non-relevant tweets as background dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Iteration Algorithm</head><p>The Iteration Algorithm used in the filtering task is similar to the Rocchio Algorithm. It's also a method of relevance feedback and developed with the help of Vector Space Model. In the feedback stage, terms with low weight will be discarded. The Iteration Algorithm is described as Eq.11.</p><p>-→ Qm = α ---→ Qm-1 + β ---→ Tm-1 <ref type="bibr" coords="6,276.55,462.35,16.36,7.86" target="#b11">(11)</ref> where α and β are the tuning parameters and the sum of them equals 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Result Analysis</head><p>Table <ref type="table" coords="6,87.17,520.27,4.61,7.86" target="#tab_10">7</ref> show the performance values of our submitted four runs. The primary evaluation measures for the filtering task are T11SU <ref type="bibr" coords="6,99.31,541.19,13.50,7.86" target="#b11">[11]</ref>, F-score (beta=0.5), P-score (Precision) and R-score (Recall). Our training metric is based on the P30 evaluation metric.</p><p>Run PKUICSTF1 uses the two-stage filtering model that combines the VSM and improved Boolean Model. And the Rocchio Algorithm is denoted to Run PKUICSTF1 as the relevace feedback model. Run PKUICSTF3 uses the language model only with a static threshold set as -6. PKUIC-STF2 and PKUICSTF4 both apply another two-stage strategy which include SVM classifier and Language Model. The difference between two runs is that Run PKUICSTF4 took the external link resources into consideration and the parameters of these two runs are different.</p><p>We'll do more experiments for further conclusions after the publication of evalation tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we present our system for TREC'12 Microblog Track. For the real-time search task, we adopt Realtime Tweet Ranking (RTR) model to rank the tweets to the given topic, and meanwhile the RTR model provides candidate tweets to Learning to Rank framework for the further ranking process. For the real-time filtering pilot task, we compare different baseline models and propose the two-stage filtering model which combines VSM model and Boolean model. In addition, we apply two relevance feedback models to improve the filtering results. Many studies remain for the future work. One of the most interesting directions is to improve learning to rank/filter framework for better results. Moreover, we also interested in the unified methodology of how to determine a decision threshold for different topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,84.63,249.87,177.44,7.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learning to rank framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,343.00,526.19,186.73,7.89"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: FIltering System Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,165.07,554.50,127.84,7.86;5,53.80,564.97,239.11,7.86;5,53.80,575.43,239.11,8.34;5,53.80,585.89,114.60,7.86"><head></head><label></label><figDesc>describes a Two-Stage Filtering Model which combines Vector Space Model and Improved Boolean Model. The two score thresholds tc and t b are obtained in the training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,350.44,216.27,171.85,7.89"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two-Stage Filtering Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,322.19,216.23,241.42,81.71"><head>Table 1 : Summary statistics of Tweet corpus</head><label>1</label><figDesc></figDesc><table coords="1,322.19,226.81,241.42,71.13"><row><cell cols="2">Html Code Status</cell><cell cols="2">Tweets in 2011 Tweets in 2012</cell></row><row><cell>200</cell><cell>OK</cell><cell>13,839,083</cell><cell>8,084,724</cell></row><row><cell>302</cell><cell>Found</cell><cell>1,106,999</cell><cell>815,794</cell></row><row><cell>403</cell><cell>Not Found</cell><cell>284,225</cell><cell>817,273</cell></row><row><cell>404</cell><cell>Forbidden</cell><cell>844,494</cell><cell>868,667</cell></row><row><cell>Null</cell><cell>Null</cell><cell>67,011</cell><cell>67,011</cell></row><row><cell>Searchable</cell><cell></cell><cell>14,946,082</cell><cell>8,900,518</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,61.43,66.15,231.48,477.71"><head>Table 2 : Summary statistics of TopicInfo corpus</head><label>2</label><figDesc></figDesc><table coords="2,76.21,66.15,216.70,477.71"><row><cell cols="2">Tweet2012 Corpus</cell><cell>Original Query</cell></row><row><cell cols="2">Preprocessing</cell><cell></cell></row><row><cell>Origin</cell><cell>DE</cell><cell>Preprocessing</cell></row><row><cell>Corpus</cell><cell>Corpus</cell><cell></cell></row><row><cell>Candidate</cell><cell>TopicInfo</cell><cell></cell></row><row><cell>Corpus</cell><cell>Corpus</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Query</cell></row><row><cell cols="2">Incre Index</cell><cell></cell></row><row><cell cols="3">Real-Time Tweet Ranking Model</cell></row><row><cell cols="3">Top 20,000 Candidate Tweets</cell></row><row><cell></cell><cell>Ranking SVM</cell><cell></cell></row><row><cell cols="3">Top 10,000 Final Tweets</cell></row><row><cell cols="3">Figure 1: System Architecture</cell></row><row><cell cols="2">Html Code Status</cell><cell>Tweets in 2011</cell></row><row><cell>200</cell><cell>OK</cell><cell>1,225,947</cell></row><row><cell>302</cell><cell>Found</cell><cell>688</cell></row><row><cell>403</cell><cell>Not Found</cell><cell>5,050</cell></row><row><cell>404</cell><cell>Forbidden</cell><cell>92,378</cell></row><row><cell>Null</cell><cell>Null</cell><cell>265,468</cell></row><row><cell>Searchable</cell><cell></cell><cell>1,226,635</cell></row><row><cell cols="3">(i.e. TopicInfo corpus) contained in Tweet11 corpus</cell></row><row><cell cols="3">and extract their topic information for our document</cell></row><row><cell cols="3">expansion process in early December, 2011.Note that</cell></row><row><cell cols="3">web pages might be deleted as time elapsed, we have</cell></row><row><cell cols="3">only crawled a portion of the external URL set. Sum-</cell></row><row><cell cols="3">mary statistics of TopicInfo corpus is present in Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,53.80,277.70,240.54,169.19"><head>Table 3 : Training Data Relevance Distribution</head><label>3</label><figDesc></figDesc><table coords="3,53.80,288.28,240.54,79.69"><row><cell cols="3">Category TREC11 Labeled Tweets Crawled</cell></row><row><cell>Minimally Relevant</cell><cell>2306</cell><cell>2075</cell></row><row><cell>Highly Relevant</cell><cell>558</cell><cell>511</cell></row><row><cell>Non Relevant</cell><cell>37900</cell><cell>33803</cell></row><row><cell>Total</cell><cell>40764</cell><cell>36389</cell></row><row><cell>ument model is present as follows:</cell><cell></cell><cell></cell></row></table><note coords="3,87.58,373.67,171.53,10.74;3,281.14,376.04,11.77,7.86;3,105.29,399.35,43.61,10.22;3,152.66,395.91,87.55,7.86;3,179.22,407.80,32.96,8.37;3,281.15,401.71,11.77,7.86;3,98.23,430.08,62.86,10.23;3,164.84,426.64,82.43,7.86;3,190.61,438.53,29.93,8.37;3,281.15,432.45,11.77,7.86"><p>P (w| θD) = (1 -λ) • P (w| θT ) + λ • P (w| θI ) (5) P (w| θT ) = c(w, T ) + µT P (w|CT ) |T | + µT (6) thickP (w| θI ) = c(w, I) + µI P (w|CI ) |I| + µI (7)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="3,316.81,511.51,254.20,207.54"><head>Table 4 : Performance of our submitted runs</head><label>4</label><figDesc>The difference between PKUICST1 and PKUICST2 is that they train in different training set. The former trains on the 49 allrel topics while the latter trains on the 33 highrel topics. All ranking SVM models use all the semantic features and tweet related features. PKUICST4 doesn't use the OrgTitleScore and IssueTitleScore features as it doesn't use any external resources.</figDesc><table coords="3,316.81,520.36,239.12,135.93"><row><cell>Run ID</cell><cell>P@30</cell><cell cols="2">MAP R-Prec</cell></row><row><cell>PKUICST1</cell><cell>0.2164</cell><cell cols="2">0.1639 0.2176</cell></row><row><cell>PKUICST2</cell><cell>0.2068</cell><cell>0.1561</cell><cell>0.2120</cell></row><row><cell>PKUICST3</cell><cell>0.2113</cell><cell>0.1686</cell><cell>0.2174</cell></row><row><cell cols="3">PKUICST4 0.2333 0.2263</cell><cell>0.2174</cell></row><row><cell cols="4">PKUICST3 only uses RTR model with the DE Corpus</cell></row><row><cell cols="4">and cuts the top 10,000 tweets from the candidate tweet</cell></row><row><cell cols="4">set. Except PKUICST3, other runs all adopt learning to</cell></row><row><cell cols="4">rank framework. PKUICST1 and PKUICST2's candidate</cell></row><row><cell cols="4">tweet sets are both generated with the DE Corpus, while</cell></row><row><cell cols="4">PKUICST4's candidate tweet set is generated with the Ori-</cell></row><row><cell>gin Corpus.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="4,64.80,63.80,480.11,71.25"><head>Table 5 : Model Description in unofficial runs</head><label>5</label><figDesc></figDesc><table coords="4,64.80,74.38,480.11,60.67"><row><cell cols="3">Model Name Candidate Set Feature Set</cell></row><row><cell>OrgBase</cell><cell cols="2">Origin Corpus OrgTweetScore, Tweet Related Features</cell></row><row><cell>OrgTime</cell><cell cols="2">Origin Corpus OrgTweetScore, Tweet Related Features, Temporal Feature</cell></row><row><cell>DEBase</cell><cell>DE Corpus</cell><cell>OrgTweetScore, Tweet Related Features</cell></row><row><cell cols="2">DEIssueTitle DE Corpus</cell><cell>OrgTweetScore, OrgTitleScore, IssueTweetScore, IssueTitleScore, Tweet Related Fea-</cell></row><row><cell></cell><cell></cell><cell>tures</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="4,82.59,456.29,181.53,59.05"><head>Table 6 : Performance of unofficial runs</head><label>6</label><figDesc></figDesc><table coords="4,86.79,465.13,173.14,50.21"><row><cell>Model Name</cell><cell>P@30</cell><cell cols="2">MAP R-Prec</cell></row><row><cell>OrgBase</cell><cell>0.4623</cell><cell>0.2914</cell><cell>0.3317</cell></row><row><cell>OrgTime</cell><cell>0.4532</cell><cell>0.2902</cell><cell>0.3314</cell></row><row><cell>DEBase</cell><cell>0.4654</cell><cell>0.2921</cell><cell>0.3319</cell></row><row><cell>DEIssueTitle</cell><cell cols="3">0.4567 0.2989 0.3355</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="6,53.80,63.80,246.21,89.85"><head>Table 7 : Performace of submitted runs</head><label>7</label><figDesc></figDesc><table coords="6,53.80,72.64,246.21,81.01"><row><cell>Run ID</cell><cell cols="4">T11SU F(beta=0.5) Precision Recall</cell></row><row><cell cols="3">PKUICSTF1 0.3424 0.2722</cell><cell>0.3963</cell><cell>0.2300</cell></row><row><cell cols="2">PKUICSTF2 0.3244</cell><cell>0.2525</cell><cell>0.3701</cell><cell>0.2809</cell></row><row><cell cols="2">PKUICSTF3 0.3233</cell><cell>0.2556</cell><cell>0.3857</cell><cell>0.2272</cell></row><row><cell cols="2">PKUICSTF4 0.3341</cell><cell>0.2629</cell><cell>0.3766</cell><cell>0.2936</cell></row><row><cell cols="2">in our filtering system.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,321.42,711.19,163.23,7.86"><p>http://www.lemurproject.org/lemur.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">ACKNOWLEDGMENTS</head><p>The work reported in this paper was supported by the <rs type="funder">National Natural science Foundation of China</rs> Grant <rs type="grantNumber">60875033</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TA3Tzxk">
					<idno type="grant-number">60875033</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.29,287.01,96.81,10.53" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,303.72,206.05,7.86;6,335.61,314.18,169.42,7.86;6,335.61,324.64,211.17,7.86;6,335.61,335.10,162.34,7.86;6,335.61,345.56,169.62,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,493.07,303.72,48.58,7.86;6,335.61,314.18,140.56,7.86">LIBSVM: A library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j" coord="6,483.59,314.18,21.44,7.86;6,335.61,324.64,206.96,7.86">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,357.02,213.08,7.86;6,335.61,367.48,210.76,7.86;6,335.61,377.94,187.99,7.86;6,335.61,388.40,214.44,7.86;6,335.61,398.86,92.13,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,421.00,367.48,125.37,7.86;6,335.61,377.94,66.70,7.86">An empirical study on learning to rank of tweets</title>
		<author>
			<persName coords=""><forename type="first">Yajuan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,407.48,388.40,33.81,7.86">COLING</title>
		<editor>
			<persName><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<imprint>
			<publisher>Tsinghua University Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="295" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,410.32,198.62,7.86;6,335.61,420.78,207.20,7.86;6,335.61,431.24,200.15,7.86;6,335.61,441.70,189.77,7.86;6,335.61,452.16,207.26,7.86;6,335.61,462.62,202.26,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,411.07,420.78,131.75,7.86;6,335.61,431.24,63.24,7.86">Micro-blogging as online word of mouth branding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mimi</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kate</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abdur</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chowdury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,525.90,452.16,16.97,7.86;6,335.61,462.62,74.81,7.86">CHI Extended Abstracts</title>
		<editor>
			<persName><forename type="first">Dan</forename><forename type="middle">R</forename><surname>Olsen</surname><genName>Jr</genName></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Richard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ken</forename><surname>Arthur</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Hinckley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Morris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Saul</forename><surname>Hudson</surname></persName>
		</editor>
		<editor>
			<persName><surname>Greenberg</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3859" to="3864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,474.08,209.49,7.86;6,335.61,484.54,187.17,7.86;6,335.61,495.00,164.99,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,417.42,474.08,127.67,7.86;6,335.61,484.54,187.17,7.86;6,335.61,495.00,30.31,7.86">Text categorization with suport vector machines: Learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,384.26,495.00,23.51,7.86">ECML</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,506.46,211.12,7.86;6,335.61,516.92,198.16,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,417.42,506.46,129.30,7.86;6,335.61,516.92,69.03,7.86">Optimizing search engines using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,423.29,516.92,17.65,7.86">KDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,528.38,200.62,7.86;6,335.61,538.84,145.50,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,417.42,528.38,118.80,7.86;6,335.61,538.84,16.37,7.86">Training linear svms in linear time</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,370.63,538.84,17.65,7.86">KDD</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,550.29,185.37,7.86;6,335.61,560.75,193.25,7.86;6,335.61,571.22,200.38,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,335.61,560.75,193.25,7.86;6,335.61,571.22,67.95,7.86">Exploiting real-time information retrieval in the microblogosphere</title>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Runwei</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,422.36,571.22,21.34,7.86">JCDL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,582.67,184.75,7.86;6,335.61,593.13,205.09,7.86;6,335.61,603.59,220.31,7.86;6,335.61,614.05,220.30,7.86;6,335.61,624.52,220.31,7.86;6,335.61,634.98,175.66,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,335.61,593.13,205.09,7.86;6,335.61,603.59,156.38,7.86">Smoothing techniques for adaptive online language models: topic tracking in tweet streams</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,510.06,603.59,45.86,7.86;6,335.61,614.05,220.30,7.86;6,335.61,624.52,149.60,7.86">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="422" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,646.43,209.39,7.86;6,335.61,656.89,218.74,7.86;6,335.61,667.35,184.50,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,404.90,656.89,145.90,7.86">Introduction to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,678.81,220.32,7.86;6,335.61,689.27,196.95,7.86;6,335.61,699.73,174.63,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,374.47,689.27,158.09,7.86;6,335.61,699.73,21.13,7.86">Overview of the TREC-2011 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craigand</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,375.51,699.73,106.44,7.86">Proceedings of TREC 2011</title>
		<meeting>TREC 2011</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,711.19,219.32,7.86;7,72.59,57.64,183.83,7.86;7,72.59,68.10,91.91,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,488.16,711.19,66.76,7.86;7,72.59,57.64,81.31,7.86">The TREC 2002 filtering track report</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.38,57.64,84.04,7.86;7,72.59,68.10,61.71,7.86">TEXT RETRIEVAL CONFERENCE</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,79.56,196.14,7.86;7,72.59,90.02,59.62,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="7,130.74,79.56,137.99,7.86;7,72.59,90.02,32.04,7.86">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,101.48,220.31,7.86;7,72.59,111.94,202.09,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,211.35,101.48,81.55,7.86;7,72.59,111.94,68.56,7.86">Relaxed online svms for spam filtering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Wachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,159.09,111.94,23.87,7.86">SIGIR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,123.39,191.05,7.86;7,72.59,133.86,205.70,7.86;7,72.59,144.32,181.74,7.86;7,72.59,154.78,82.29,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,220.89,123.39,42.75,7.86;7,72.59,133.86,205.70,7.86;7,72.59,144.32,81.91,7.86">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,161.12,144.32,89.57,7.86">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,166.23,213.69,7.86;7,72.59,176.69,189.96,7.86;7,72.59,187.16,218.32,7.86;7,72.59,197.62,20.96,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,235.60,166.23,50.68,7.86;7,72.59,176.69,189.96,7.86;7,72.59,187.16,81.91,7.86">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.62,187.16,22.40,7.86">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
