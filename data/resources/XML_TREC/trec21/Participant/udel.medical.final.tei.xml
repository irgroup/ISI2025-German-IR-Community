<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,75.68,112.05,460.69,15.11;1,186.59,133.96,238.81,15.11">Exploring Evidence Aggregation Methods and External Expansion Sources for Medical Record Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,218.04,166.45,72.70,10.48"><forename type="first">Dongqing</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.40,166.45,76.57,10.48"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,75.68,112.05,460.69,15.11;1,186.59,133.96,238.81,15.11">Exploring Evidence Aggregation Methods and External Expansion Sources for Medical Record Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">699E4A5A19CCFE669C34E3DFF4AA96DD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes and analyzes experiments we performed for the Medical Records track in the 2012 Text REtrieval Conference (TREC). We mainly investigated three research problems:</p><p>1. Evidence Aggregation: In last year's track there were two different methods in general for obtaining a visit ranking out of reports (smaller document units), i.e., (A) using reports as indexing and retrieval units and then converting a report ranking into a visit ranking, and (B) using visits as indexing and retrieval units by concatenating reports at the very first stage and then obtain a visit ranking directly.</p><p>Method A avoids the potential problem of varying visit document length, while Method B naturally aggregates evidence scatter over multiple reports and easily obtains a visit ranking. It is unclear which method is better based on all reported results. Thus, we compared the two approaches, tried various score aggregation methods for (A), and combined both approaches in a way that further improved the system performance.</p><p>2. Expansion Sources: We tested a variety of external collections (ranging from general web datasets to domain-specific thesauri, and from Megabyte datasets to Terabyte datasets) for query expansion, compared their effectiveness, and obtained useful insights into the data.</p><p>3. Retrieval Models: We tested several statistical IR models (proven to be effective on news and web collections) on this medical collection, and explored ways to combine these models to address different aspects of task. For instance, we used MRF model to model term proximity since most medical concepts are phrases. We also used a mixture of relevance models to obtain various relevant expansion terms covered by different expansion collections respectively, which is expect to significantly alleviate the vocabulary mismatch between medical terminologies.</p><p>For TREC submissions, we tested systems that combined multiple IR models, leveraged diverse expansion sources, and used various evidence aggregation methods. We implemented all the retrieval models in the Indri<ref type="foot" coords="1,93.34,558.94,3.97,6.12" target="#foot_0">1</ref> retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Features</head><p>In this section, we describe the retrieval models and evidence aggregation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval Model</head><p>In this section, we briefly describe the three retrieval models, namely the query likelihood language model, MRF model, and mixture of relevance models, which serve as the underpinning of our retrieval systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Model</head><p>Our baseline model is the query likelihood language model as formulated below:</p><formula xml:id="formula_0" coords="2,191.68,207.30,344.08,32.63">score(D, Q) = log P (Q|D) = n ∑ i=1 log tf qi,D + µ tfq i ,C |C| |D| + µ , (<label>1</label></formula><formula xml:id="formula_1" coords="2,535.76,220.01,4.24,8.74">)</formula><p>where q i is the ith term in query Q, n is the total number of terms in Q, |D| and |C| are the document and collection lengths in words respectively, tf qi,D and tf qi,C are the document and collection term frequencies of q i respectively, and µ is the Dirichlet smoothing parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Markov Random Field Model</head><p>Medical queries usually contain phases that describe conditions, symptoms, drug names, treatments, etc. These query terms are likely to occur in close proximity to each other in relevant documents. Thus, we use the Markov random field (MRF) model proposed by Metzler and Croft <ref type="bibr" coords="2,383.56,331.55,10.52,8.74" target="#b8">[9]</ref> to model term dependencies. We use their sequential dependence model in particular. Following Metzler and Croft <ref type="bibr" coords="2,442.18,343.50,9.97,8.74" target="#b8">[9]</ref>, we set the feature weights (λ T , λ O , λ U ) to (0.8, 0.1, 0.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture of Relevance Models</head><p>Queries specified by the search users can have a "vocabulary mismatch" with the content in a medical report since there are many different ways to express a medical concept (e.g., "hearing loss", "hearing impairment", "difficult of hearing", and even "deafness" are all semantically related, but they only have one common term at the most). The consequence is that the system may have a relatively low recall if there is a "vocabulary mismatch". We can alleviate this problem and improve our baseline retrieval model by expanding the query with additional "related" terms. These related terms (also called expansion terms) can be derived from a relevance model θ Q , which is usually built upon top-ranked k documents for the query in the target collection (i.e., the same collection used for retrieval). Thus, in this paper we derive expansion terms based on their weights p which are estimated by:</p><formula xml:id="formula_2" coords="2,197.04,504.41,342.96,30.32">p i = k ∑ j=1 exp{ tf ei,Dj |D j | + log |C| df ei,C + score(D j , Q)},<label>(2)</label></formula><p>where score(D j , Q) is the query likelihood score for the top jth feedback document in the initial retrieval set ranked by Equation <ref type="formula" coords="2,161.25,557.97,3.88,8.74" target="#formula_0">1</ref>, tf ei,Dj is the term frequency of e i in document D j , df ei,C is the document frequency of e i in collection C, and |D j | and |C| are document and collection lengths in words respectively. This formula estimates the importance of term e i based on its term frequency, inverse document frequency, and feedback document scores. m terms with highest scores p are selected as expansion terms, and they form our estimated relevance model θQ . Note that we also normalize p so that we have an estimated probability P (w| θQ ) for each word w.</p><p>Relevance modeling can be further improved upon by leveraging information in other document collections. Specifically, following Diaz and Metzler <ref type="bibr" coords="2,275.31,643.09,9.97,8.74" target="#b1">[2]</ref>, we can form relevance models for two or more additional collections, then expand the query using those models.</p><p>To achieve better performance, we linearly interpolate the mixture of relevance models with the maximum likelihood (ML) query estimate by formulating the equation:</p><formula xml:id="formula_3" coords="2,210.81,695.15,329.19,30.20">P (w|θ Q ) = λ Q #(w, Q) |Q| + ∑ C λ C P (w| θQ,C ),<label>(3)</label></formula><p>where the first part is the weighted ML query estimate for word w and the second part represents the mixture of relevance models. In particular, P (w| θQ,C ) is the probability of w in the estimated relevance model θ built upon top-ranked documents in expansion collection C. λ's are collection weights and</p><formula xml:id="formula_4" coords="3,444.50,90.93,76.56,19.28">λ Q + ∑ C λ C = 1.</formula><p>For TREC submissions, we set λ's to (0.7, 0.1, 0.1, 0.1) and use top 10 terms from top 50 feedback documents. We implement Equation 3 using Indri in the same way as our previous work <ref type="bibr" coords="3,405.46,122.98,14.62,8.74" target="#b11">[12]</ref>. We denoted this model as MRM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Combined Model</head><p>We linearly combine MRF and MRM to get our third retrieval model. The scoring function looks like</p><formula xml:id="formula_5" coords="3,214.80,181.74,325.20,30.20">P (w|θ Q ) = λ Q • MRF + ∑ C λ C P (w| θQ,C ),<label>(4)</label></formula><p>which is similar to Equation <ref type="formula" coords="3,201.46,222.36,3.88,8.74" target="#formula_3">3</ref>. The difference is that we replace the ML query estimate with MRF. The new retrieval model is expected to benefit from term dependence modeling as well as query expansion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-level Evidence</head><p>In the aforementioned retrieval models, "document" (i.e., D) is a broad term that could indicate different granularities of a visit: the text of all reports associated with the visit, a single report from the visit, or just one field within a single report from the visit. Thus, in this section we describe how we leverage evidence from each of these to come up with a final document score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Field Level Evidence</head><p>The main fields in a report are the doctor's notes and the fields that contain diagnosis codes. Here we describe how we leverage ICD-9 codes in the language model, and how we remove some extraneous information and extract useful information from doctor's notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Expansion:</head><p>We expand ICD codes in the "admit diagnosis" and "discharge diagnosis" fields with their corresponding descriptions<ref type="foot" coords="3,213.17,577.07,3.97,6.12" target="#foot_1">2</ref> to introduce additional useful words into the documents. We refer this feature as ICD in the following sections.</p><p>Negation Removal: The "report text" filed contains clinical narratives. One distinct feature of clinical narratives is that negation phrases are frequently used to claim the absence of certain conditions or symptoms <ref type="bibr" coords="3,72.00,626.47,9.97,8.74" target="#b0">[1]</ref>, such as "cannot tell", "not clear", "without evidence", etc. Negations may cause retrieval false positives. Thus, we use NegEx<ref type="foot" coords="3,163.51,636.85,3.97,6.12" target="#foot_2">3</ref>  <ref type="bibr" coords="3,172.13,638.42,9.97,8.74" target="#b3">[4]</ref>, an open-source clinical negation detection tool, to remove all negated portions of the sentences from the medical records before indexing. We refer this feature as NEG in the following sections.</p><p>Age/Gender Filtering: We use simple regular expressions to search for age/gender indication words and phrases in both the"report text" filed and the topics. We use the extracted age and gender information to filter from the retrieval set visits that do not meet the inclusion criteria specified in the topics. We refer this feature as AGF in the following sections.</p><p>Previous work <ref type="bibr" coords="4,151.63,99.07,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="4,165.16,99.07,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="4,175.93,99.07,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="4,186.70,99.07,12.73,8.74" target="#b11">12]</ref> have shown that the above three medical features are quite useful. Thus, they would be the default features for our systems unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report Level Evidence</head><p>Evidence in a visit may mainly exist in only a small proportion of all the associated reports. This allows us to rely on the strongest evidence of a visit to estimate its relevance. Thus, we use reports as the initial retrieval units (i.e., building an index for reports and applying the retrieval model to each report), and then transform a report ranking into a visit ranking based on the strongest report-level evidence, which is equivalent to using the following report score merging method for ranking visits:</p><formula xml:id="formula_6" coords="4,192.18,211.61,343.58,12.69">score RbM (V, Q) = f RbM ({score(r V 1 , Q), score(r V 2 , Q), ...}), (<label>5</label></formula><formula xml:id="formula_7" coords="4,535.76,213.68,4.24,8.74">)</formula><p>where r V j is a report associated with visit V based on the report-to-visit mapping, score(r V j , Q) is the language modeling score of the report with respect to query Q, and f RbM is the function for aggregating the scores. We will try MAX, SUM, and ANZ for f RbM in Section 3. We name this evidence aggregation strategy Retrieval-before-Merging (RbM). The merging process involved in RbM corresponds to "merging I" in Figure <ref type="figure" coords="4,126.98,282.46,3.88,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visit Level Evidence</head><p>Evidence may also spread across multiple reports, especially when the information need is a complex one. Thus, our second strategy to aggregate evidence is to first merge reports from a single visit field by field into a visit document and then construct an index for visit documents. With this strategy, the language model built on a merged document can naturally combine the evidence scattered across multiple reports. Furthermore, this strategy can directly lead to a ranking of visits which are the desired retrieval units. We call this second evidence aggregation strategy Merging-before-Retrieval (MbR). The merging process involved in MbR corresponds to "merging II" in Figure <ref type="figure" coords="4,316.91,388.06,3.88,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-level Evidence</head><p>RbM and MbR as described above are two different strategies for aggregating evidence and ranking visits. RbM and MbR complement each other in that the former can naturally aggregate evidence spreading across multiple reports (which would be challenging to do at the report-level) while the latter can leverage the strongest evidence (which may become less apparent after reports merging in MbR) to estimate relevance. This leads to our third evidence aggregation method in which we take advantage of both RbM and MbR by merging their visit rankings, as demonstrated by "merging III" in Figure <ref type="figure" coords="4,415.47,481.71,3.88,8.74" target="#fig_0">1</ref>. We call third strategy as Visit-Ranking-Merging (VRM). The merging method (i.e., "Merging III" in Figure <ref type="figure" coords="4,437.10,493.67,4.43,8.74" target="#fig_0">1</ref>) is defined by:</p><formula xml:id="formula_8" coords="4,192.71,514.62,347.29,9.27">score VRM (V, Q) = f VRM (score RbM (V, Q), score MbR (V, Q)),<label>(6)</label></formula><p>where score RbM (V ) and score MbR (V ) are the language modeling scores for visit V with respect to query Q in the two visit rankings obtained by RbM and MbR respectively, f VRM is the function for score aggregation, and score VRM (V, Q) is the final score of visit V in the merged ranking. We will try different methods for f VRM such as CombMNZ, CombSUM, and CombMAX in Section 3 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experimental Setup</head><p>We explore the best system feature settings for 2012 TREC submission by performing cross-validation on the test collection of 2011 medical records track. Once we find the best settings as will be described in Section 3, we train the systems on the whole 2011 test collection and generate runs for 2012 topics. We use the Indri retrieval system for indexing and retrieving. In particular, we use the Porter stemmer to stem words in both reports and queries, and use a simple standard medical stoplist <ref type="bibr" coords="4,446.15,665.38,10.52,8.74" target="#b4">[5]</ref> for stopping words in queries only. Then we conduct 5-fold cross-validation and use top 1000 retrieved visits for each query to evaluate our system under different settings. In each iteration, we train our system on 28 queries to obtain the best parameter setting for MAP by sweeping over the range of [1000, 20000] at a step size of 1000 for the Dirichlet smoothing parameter (i.e., µ in Equation <ref type="formula" coords="4,293.18,713.20,3.88,8.74" target="#formula_0">1</ref>), and then generate a ranking for each of the remaining 7 queries based on the trained system. When complete, we have full rankings for all 35 topics as a test set. We evaluate the system based on the MAP, bpref, P10, and Rprec over all 35 topics.</p><p>We train our systems on MAP though bpref is the primary evaluation metric for 2011 medical track. There are two reasons: 1) training on MAP is most commonly used in IR to improve retrieval performance; 2) we find that training on MAP improves the retrieval performance on other metrics as well while training on bpref does not improve the overall performance. Thus, MAP and bpref will both be the primary evaluation measures in this work. In fact, MAP correlates well with bpref as we will show in the next section.</p><p>To access the statistical significance of differences in the performance of two systems, we perform onetailed paired t-test for MAP (since we train systems on MAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>This section describes experiments, presents the evaluation results, and discusses the research findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Retrieval before Merging</head><p>As mentioned in Section 2.2, we have several options for choosing the score merging function f RbM in Equation 5 (i.e., "merging I" in Figure <ref type="figure" coords="5,221.66,283.27,4.43,8.74" target="#fig_0">1</ref>) for RbM. Now we describe them formally below:</p><formula xml:id="formula_9" coords="5,72.00,301.92,329.47,110.92">MAX: score RbM (V, Q) = max({score(r V j , Q)}) SUM: score RbM (V, Q) = ∑ j score(r V j , Q) ANZ: score RbM (V, Q) = ∑ j score(r V j , Q) |{score(r V j , Q) ̸ = 0}|</formula><p>where again score(r V j , Q) is the language modeling score of the report r V j (associated with visit V ) with respect to query Q. ANZ stands for "Averaging over Non-Zeros", meaning we only consider reports containing at least one query term. MAX, SUM, and ANZ are similar to CombMAX, CombSUM, and CombANZ proposed by Fox and Shaw <ref type="bibr" coords="5,195.70,460.00,14.62,8.74" target="#b10">[11]</ref>. However, CombMAX, CombSUM and CombANZ were used for merging multiple retrieval runs.</p><p>Table <ref type="table" coords="5,114.51,483.92,4.98,8.74" target="#tab_0">1</ref> shows that MAX is superior to SUM and ANZ. This confirms our assumption that we can rely on the strongest evidence (i.e, the most relevant report) of a visit to estimate the relevance of that visit. Thus, we will use MAX for score merging in RbM by default in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAX (selected</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evidence Aggregation</head><p>Similarly, we also have several options for choosing the score merging function f VRM in Equation <ref type="formula" coords="5,506.65,623.53,4.98,8.74" target="#formula_8">6</ref>(i.e.," merging III" in Figure <ref type="figure" coords="5,174.38,635.48,4.43,8.74" target="#fig_0">1</ref>) for VRM, such as CombMNZ, CombANZ, and CombMAX <ref type="bibr" coords="5,449.70,635.48,14.62,8.74" target="#b10">[11]</ref>. In our case, we are only merging two rankings. Thus, these merging methods are specified as follows:</p><formula xml:id="formula_10" coords="5,72.00,666.09,368.19,53.88">CombMNZ: score VRM (V, Q) = N V • [score RbM (V, Q) + score MbR (V, Q)] CombSUM: score VRM (V, Q) = score RbM (V, Q) + score MbR (V, Q) CombMAX: score VRM (V, Q) = max(score RbM (V, Q), score MbR (V, Q)) CombANZ: score VRM (V, Q) = score RbM (V, Q) + score MbR (V, Q) N V</formula><p>where score VRM (V, Q) is the merged score for visit V , and score RbM (V, Q) and score MbR (V, Q) are the scores for V in two different visit rankings as demonstrated in Figure <ref type="figure" coords="6,341.03,162.96,3.88,8.74" target="#fig_0">1</ref>, and N V is the number of rankings that have V in the top 1200 retrieved visits (We cut off the merged rank list at rank 1000 to get the final ranking). Note that score MbR/RbM (V, Q) = 0 if V does not appear in the top 1200 retrieved. We compare the performance of these merging methods using the primary evaluation measures in Table <ref type="table" coords="6,390.48,198.82,3.88,8.74" target="#tab_1">2</ref>. As we can see, CombMNZ and CombSUM achieve comparable performance, and are better than CombMAX and CombANZ. Thus, we can infer that a good aggregation strategy for "merge III" should favor visits that appear in both rankings. We use CombMAX and CombANZ as the merging methods for VRM. Next, we compare the three evidence aggregation strategies as described in Section 2.2. Table <ref type="table" coords="6,506.01,353.44,4.98,8.74" target="#tab_2">3</ref> shows that VRM is significantly better than MbR and RbM on MAP, which means that merging visit rankings as the top-level evidence aggregation strategy boosts the retrieval performance significantly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method MAP bpref</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System MAP bpref P10 Rprec</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Selection of Expansion Collections</head><p>We test several expansion collections. In addition to the medical records that are the target of retrieval, we leverage information in several other large, widely-available collections: ImageCLEF 2009 Medical Image Retrieval Task dataset <ref type="bibr" coords="6,174.81,554.52,14.62,8.74" target="#b9">[10]</ref>, TREC 2007 Genomics Track dataset <ref type="bibr" coords="6,363.08,554.52,9.97,8.74" target="#b5">[6]</ref>, TREC 2009 ClueWeb09 Category B dataset (excluding Wikipedia pages), a Wikipedia dataset (containing those excluded Wikipedia pages), and the 2012 Medical Subject Headings (MeSH). Table <ref type="table" coords="6,303.04,578.43,4.98,8.74" target="#tab_4">4</ref> provides detailed information about these datasets.</p><p>In particular, we use MeSH for expansion in the way as described in <ref type="bibr" coords="6,386.07,590.39,14.62,8.74" target="#b12">[13]</ref>. Moreover, the CLEF dataset consists of 74,902 medical images. We crawled 5,704 full-text CLEF articles associated with these images as the actual external collection used in this work. We choose these collections because there are existing topics and relevance judgments for analysis and because we want to compare the effects of different sources on retrieval performance.</p><p>For simplicity, we use aggregation strategy MbR (without any medical features described in Section ) and retrieval model MRM with one expansion collection at a time to explore the expansion effectiveness of each collection as show in Table <ref type="table" coords="6,214.63,674.07,3.88,8.74" target="#tab_5">5</ref>.</p><p>As we can see in Table <ref type="table" coords="6,197.19,686.03,3.88,8.74" target="#tab_5">5</ref>, ImageCLEF and Wikipedia have comparable improvement over the baseline, though the former is more medical-related, much smaller, and less noisy than the latter. The same situation applies to Genomics and ClueWeb09. However, Genomics and ClueWeb09 are much larger than ImageCLEF and Wikipedia respectively, and Genomics and ClueWeb09 both have significant improvement over the baseline. Genomics is also significantly better than Wikipedia. Thus, we can infer that expansion effectiveness depends on both the quality (i.e., content similarity to the target collection) and size of the expansion collection. MeSH expansion is different from general expansion in that it relies on a controlled vocabulary from which expansion terms derived are not as diversified as those from a general expansion collection. For instance, for the query "hearing loss", it is difficult for MeSH to provide related expansion terms such as "cochlear", "noise", "auditory", and "binaural" (top-ranked terms from Genomics), "cerumen", "canals", and "tympanic" (from Medical), "vestibular", "ear", and "stape" (from ImageCLEF). Some of these terms do appear in the MeSH trees at upper levels, however, it is hard to find a link to them, i.e., discriminating them from other unrelated tree nodes. Simply including all visited concepts along the path is likely to cause query drift. Moreover, these terms normally appear in phrase concepts having different meanings than individual terms.</p><p>MeSH expansion is quite restrictive, yet is comparable to top performing single expansions and is significantly better than the baseline and ImageCLEF. This is most likely because our MeSH expansion emphasizes modeling term proximity which is a big advantage of any medical thesaurus-based expansion over general expansion. Another merit of MeSH expansion is that, if used properly, it rarely includes bad expansion terms, while we have no control of the quality of each expansion term from general expansions.</p><p>Based on Table <ref type="table" coords="7,156.20,636.25,3.88,8.74" target="#tab_5">5</ref>, we choose Genomics, Medical, MeSH, and ClueWeb09 (i.e., the top-performing expansion collections) for our MRM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">TREC Submissions and Results</head><p>Base on all the previous investigations, we select and combine multiple features for our TREC submissions as shown in Table <ref type="table" coords="7,139.76,706.43,3.88,8.74" target="#tab_6">6</ref>. The settings for udelMRF and udelMED are for evaluating the impact of MRF and MeSH  Table <ref type="table" coords="8,225.49,305.51,3.88,8.74">7</ref>: Pairwise one-tail paired t-test on infAP respectively. Table <ref type="table" coords="8,159.06,338.99,4.98,8.74" target="#tab_6">6</ref> also shows the evaluation scores averaged over 47 official topics. We pick udelSUM, the system with the highest MAP score, for further analysis. Figure <ref type="figure" coords="8,378.31,350.94,4.98,8.74" target="#fig_2">2</ref> shows the comparison of infNDCG and P10 scores with TREC results (combining both automatic and manual runs). As we can see, system udelSUM is above TREC medians for the majority of topics. We observe similar results for the other three runs.</p><p>Table <ref type="table" coords="8,114.86,398.76,4.98,8.74">7</ref> shows the results of pairwise one-tail paired t-test on infAP for our four submitted runs. The significance scores indicate that MRF and MeSH are both very effective system features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>For 2012 Medical Records track, we investigated various evidence aggregation methods, explored different query expansion collections, and combined multiple statistical IR models. Our systems perform well compared with the aggregated TREC results. In particular, we found the following to be very effective: 1) external expansion using diverse sources, 2) models that incorporate term proximity information, 3) evidence aggregation at both report and visit levels. For future work, we plan to investigate why our systems did not do well on a few hard topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,168.63,401.37,274.78,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Merging results from two different retrieval methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,82.12,189.90,210.56,7.89;8,82.12,200.86,73.22,7.89;8,319.28,189.90,210.56,7.89;8,319.28,200.86,46.58,7.89"><head></head><label></label><figDesc>(a) udelSUM is below TREC medians for 3/47 topics on infNDCG. (b) udelSUM is below TREC medians for 4/47 topics on P10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,213.65,221.83,184.71,8.74;8,82.12,72.14,210.52,110.26"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison with TREC results.</figDesc><graphic coords="8,82.12,72.14,210.52,110.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,181.12,529.86,249.78,43.41"><head>Table 1 :</head><label>1</label><figDesc>Comparison of score merging methods for RbM.</figDesc><table coords="5,217.01,529.86,177.99,21.10"><row><cell></cell><cell></cell><cell>) SUM ANZ</cell></row><row><cell>MAP</cell><cell>0.416</cell><cell>0.110 0.317</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,180.15,269.70,251.71,66.92"><head>Table 2 :</head><label>2</label><figDesc>Comparison of score merging methods for VRM.</figDesc><table coords="6,219.86,269.70,169.99,44.61"><row><cell cols="2">CombMNZ (selected) 0.446</cell><cell>0.564</cell></row><row><cell>CombSUM (selected)</cell><cell>0.446</cell><cell>0.563</cell></row><row><cell>CombMAX</cell><cell>0.427</cell><cell>0.559</cell></row><row><cell>CombANZ</cell><cell>0.356</cell><cell>0.510</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,412.37,468.01,67.10"><head>Table 3 :</head><label>3</label><figDesc>Comparison of evidence aggregation methods.</figDesc><table coords="6,208.66,412.37,191.02,32.74"><row><cell>MbR</cell><cell>0.393</cell><cell>0.530 0.565</cell><cell>0.403</cell></row><row><cell>RbM</cell><cell>0.416</cell><cell>0.551 0.594</cell><cell>0.434</cell></row><row><cell>VRM</cell><cell cols="2">0.446 △ 0.563 0.635</cell><cell>0.456</cell></row></table><note coords="6,315.73,457.20,224.28,10.31;6,72.00,470.73,230.75,8.74"><p><p>△ </p>indicates that the MAP difference between VRM and MbR/RbM is statistically significant (p &lt; 0.05).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,191.37,73.45,229.27,60.32"><head>Collection # documents vocabulary size avg doc length</head><label></label><figDesc></figDesc><table coords="7,191.37,81.25,210.24,52.52"><row><cell>Medical*</cell><cell>100,866</cell><cell>10 5</cell><cell>423</cell></row><row><cell>MeSH</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>ImageCLEF</cell><cell>5,704</cell><cell>10 5</cell><cell>6,495</cell></row><row><cell>Genomics</cell><cell>162,259</cell><cell>10 7</cell><cell>6,595</cell></row><row><cell>Wikipedia</cell><cell>5,957,529</cell><cell>10 6</cell><cell>1,305</cell></row><row><cell>ClueWeb09</cell><cell>44,262,894</cell><cell>10 7</cell><cell>756</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,191.50,146.83,229.02,97.62"><head>Table 4 :</head><label>4</label><figDesc>Collection Statistics.</figDesc><table coords="7,191.50,170.72,229.02,73.74"><row><cell>System</cell><cell>MAP</cell><cell cols="3">Significance bpref P10</cell></row><row><cell>Baseline (B )</cell><cell>0.353</cell><cell></cell><cell>0.469</cell><cell>0.506</cell></row><row><cell cols="2">ImageCLEF (I ) 0.371 (+5.1%)</cell><cell></cell><cell>0.492</cell><cell>0.544</cell></row><row><cell cols="2">Wikipedia (W ) 0.376 (+6.5%)</cell><cell></cell><cell>0.500</cell><cell>0.550</cell></row><row><cell cols="2">ClueWeb09 (C ) 0.390 (+11%)</cell><cell>&gt;{B}</cell><cell>0.513</cell><cell>0.556</cell></row><row><cell>MeSH (S )</cell><cell>0.391 (+11%)</cell><cell>&gt;{B, I}</cell><cell>0.496</cell><cell>0.547</cell></row><row><cell>Medical (M )</cell><cell>0.393 (+11%)</cell><cell>&gt;{B}</cell><cell>0.520</cell><cell>0.535</cell></row><row><cell>Genomics (G)</cell><cell>0.395 (+12%)</cell><cell>&gt;{B, W}</cell><cell>0.524</cell><cell>0.553</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,72.00,257.61,468.03,116.70"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of query expansion. "X &gt; S" means the MAP difference between system X and any system specified in set S is statistically significant. The statistical significance is determined using one-tailed paried t-test on queries and p-value &lt; 0.05.</figDesc><table coords="7,79.94,305.55,452.12,68.76"><row><cell></cell><cell></cell><cell>Features</cell><cell></cell><cell></cell><cell></cell><cell>Scores</cell><cell></cell></row><row><cell>runID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>udelMRF udelMED</cell><cell>√</cell><cell>√ √</cell><cell>√</cell><cell>CombMNZ CombMNZ</cell><cell>0.408 0.280 0.398 0.269</cell><cell>0.572 0.564</cell><cell>0.418 0.594 0.415 0.604 0.410 0.590</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,173.79,387.59,264.41,8.74"><head>Table 6 :</head><label>6</label><figDesc>Feature settings and results for TREC submissions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,715.13,143.89,6.64"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,87.24,705.63,198.91,6.64"><p>https://drchrono.com/public_billing_code_search</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,87.24,715.13,131.19,6.64"><p>http://code.google.com/p/negex/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,92.48,580.03,447.66,8.74;8,92.48,591.98,425.25,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,442.86,580.03,97.28,8.74;8,92.48,591.98,153.04,8.74">Evaluation of negation phrases in narrative clinical reports</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Bridewell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Buchanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,254.11,591.98,142.57,8.74">Proceedings of AMIA Symposium</title>
		<meeting>AMIA Symposium</meeting>
		<imprint>
			<date type="published" when="2001-01">Jan. 2001</date>
			<biblScope unit="page" from="105" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,611.91,447.58,8.74;8,92.48,623.86,447.53,8.74;8,92.48,635.82,313.95,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,200.97,611.91,321.19,8.74">Improving the estimation of relevance models using large external corpora</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,92.48,623.86,447.53,8.74;8,92.48,635.82,89.44,8.74">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,655.74,447.63,8.74;8,92.48,667.70,381.68,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,378.55,655.74,161.56,8.74;8,92.48,667.70,109.59,8.74">Cohort shepherd: Discovering cohort traits from hospital visits</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,222.54,667.70,221.00,8.74">Proceedings of The 20th Text REtrieval Conference</title>
		<meeting>The 20th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,687.62,447.62,8.74;8,92.48,699.58,447.52,8.74;8,92.48,711.53,89.12,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,381.21,687.62,158.89,8.74;8,92.48,699.58,289.20,8.74">Context: An algorithm for determining negation, experiencer, and temporal status from clinical reports</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Harkema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">N</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Thornblade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,389.42,699.58,146.18,8.74">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="839" to="851" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,75.16,447.55,8.74;9,92.48,87.12,83.63,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,141.72,75.16,351.15,8.74">Information Retrieval: A Health and Biomedical Perspective. Health Informatics</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>third edition</note>
</biblStruct>

<biblStruct coords="9,92.48,107.04,447.57,8.74;9,92.48,119.00,57.06,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,359.41,107.04,161.14,8.74">TREC 2007 genomics track overview</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ruslen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,92.48,119.00,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,138.92,447.60,8.74;9,92.48,150.88,251.62,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,304.71,138.92,213.82,8.74">Cengage Learning at TREC 2011 medical track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Provalov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,92.48,150.88,221.00,8.74">Proceedings of The 20th Text REtrieval Conference</title>
		<meeting>The 20th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,170.80,447.63,8.74;9,92.48,182.76,447.52,8.74;9,92.48,194.71,76.77,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,443.17,170.80,96.94,8.74;9,92.48,182.76,249.95,8.74">University of Glasgow at medical records track 2011: Experiments with Terrier</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bouamrane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,365.82,182.76,174.18,8.74;9,92.48,194.71,46.15,8.74">Proceedings of The 20th Text REtrieval Conference</title>
		<meeting>The 20th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,214.64,447.52,8.74;9,92.48,226.59,447.54,8.74;9,92.48,238.55,108.71,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,229.65,214.64,236.06,8.74">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,477.69,214.64,62.31,8.74;9,92.48,226.59,447.54,8.74;9,92.48,238.55,34.57,8.74">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">472</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,258.48,447.64,8.74;9,92.48,270.43,434.84,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,124.93,270.43,250.99,8.74">Overview of the CLEF 2009 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bakke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,384.50,270.43,39.08,8.74">In CLEF</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="84" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,290.36,447.54,8.74;9,92.48,302.31,142.00,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,209.58,290.36,142.70,8.74">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,372.40,290.36,167.62,8.74;9,92.48,302.31,42.54,8.74">The Second Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,322.24,447.52,8.74;9,92.48,334.19,186.69,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,214.26,322.24,240.69,8.74">Using multiple external collections for query expansion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,478.20,322.24,61.80,8.74;9,92.48,334.19,156.06,8.74">Proceedings of The 20th Text REtrieval Conference</title>
		<meeting>The 20th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,354.12,447.59,8.74;9,92.48,366.07,441.32,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,210.69,354.12,325.40,8.74">Improving health records search using multiple query expansion collections</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,104.93,366.07,398.18,8.74">Proceedings of the 2012 IEEE International Conference on Bioinformatics and Biomedicine</title>
		<meeting>the 2012 IEEE International Conference on Bioinformatics and Biomedicine</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
