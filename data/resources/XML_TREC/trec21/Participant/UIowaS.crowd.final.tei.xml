<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.00,131.79,403.67,12.30">Using Hybrid Methods for Relevance Assessment in TREC Crowd&apos;12</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,208.32,156.46,72.89,8.72"><forename type="first">Christopher</forename><surname>Harris</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Program</orgName>
								<orgName type="institution">The University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.24,156.46,76.12,8.72"><forename type="first">Padmini</forename><surname>Srinivasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Program</orgName>
								<orgName type="institution">The University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.00,131.79,403.67,12.30">Using Hybrid Methods for Relevance Assessment in TREC Crowd&apos;12</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8881E217962CB9569D8FAECFF6087522</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Iowa (UIowaS) submitted three runs to the TRAT subtask of the 2012 TREC Crowdsourcing track. The task objective was to evaluate approaches to crowdsourcing high quality relevance judgments for a text document collection. We used this as an opportunity to examine three hybrid (combination of human-based and machine-based) approaches while simultaneously limiting time and cost. We create a training set from topics, which were previously assessed for relevance on the same document set, and use this training set to build strategies. We apply machine approaches, including clustering, to order documents for each topic, and then ask crowdworkers to provide relevance judgments for a subset of documents. One of our runs provides the best logistic average misclassification (LAM) rates of all submitted TRAT runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Approach 1.1 Overview</head><p>The TRAT (Text Relevance Assessing Task) objective was to explore new techniques to examine the ability to obtain relevance judgments on 18,260 documents using any method that did not make explicit use of the qrels for the test topics (i.e., TREC-8 topic IDs 401-450). Our approach also had the unstated objective of minimizing task time and cost. Available to us were the TREC-7 topics (topic IDs 351-400), which make use of the same data collection, the TREC-7 and TREC-8 ad hoc track submitted runs from previous years, and the TREC-7 ad hoc track qrels. Our TRAT strategy exploits information about document ranks in these previously submitted runs.</p><p>To make a training set, we randomly select ten of the fifty TREC-7 ad hoc track topics. For each of these topics, we evaluate the information from the 102 submitted TREC-7 ad hoc track runs, along with the TREC-7 ad hoc track qrels. The use of previous years' TREC submissions has been explored in other contexts (e.g., <ref type="bibr" coords="1,446.52,486.94,10.58,8.72" target="#b3">[4,</ref><ref type="bibr" coords="1,459.84,486.94,6.91,8.72" target="#b5">6]</ref>). We examine two types of counts, each of which represent different characteristics of the TREC-7 ad hoc track submitted runs, and then determine the appropriate weighting strategy that combines these two counts. This weighting is determined through our training runs. For each topic, we apply these weighing strategies to the TREC-8 ad hoc track submitted runs and calculate a score for each document. We subsequently rank all documents in decreasing order by this score.</p><p>From this point forward, the methods used for each of our three runs differ. For our first run (UIowaS01r), we take the list of test documents in rank score order and break them into three distinct groups: (1) definitely relevant, (2) possibly relevant and (3) definitely not relevant. We determine a fixed size for the first group of documents, which are all marked relevant. The second group is assigned to the crowd in descending rank score order for assessment. We use the crowd to determine a relevance threshold score for each topic. All documents lower in rank score than this relevance threshold are marked not relevant.</p><p>In our second run (UIowaS02r), we cluster the document collection using k-means clustering on the document headline and text fields. An appropriate k is determined (in our training runs). We order documents in rank score order within each cluster and divide them into the same three groups as we did with our first run. The overall top ranking documents across all clusters are marked as relevant. For the remaining unmarked documents, we calculate a mean rank score per cluster. The cluster with the highest mean rank score is given to the crowd to assess first. We provide the documents from this cluster to the crowd in decreasing rank score order until a relevance threshold is reached. The remaining documents in that cluster become our third group and are marked as non-relevant.</p><p>Our third run (UIowaS03r), we use the same clustering technique as we did in the second run, ranking the clusters according to the highest rank score for relevant documents for each topic, and evaluating each cluster in mean rank score order. After we mark the top 10 documents overall as relevant, we mark all documents that fall below the 40 th percentile in each cluster as not relevant. For the remaining documents, which comprise the top 60% of each cluster, we randomly sample documents from the selected cluster until a relevance threshold is reached. Once that threshold is reached, the remaining documents in that cluster are marked as non-relevant and we evaluate the cluster that has the next-highest mean rank score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Training</head><p>Ten topics from the TREC-7 ad hoc collection (topic IDs between 351 and 400) were randomly-selected and used to construct a training set. The topic IDs randomly selected were: 351, 358, 364, 369, 374, 375, 379, 388, 395, and 396.</p><p>Using these topics, we gathered the submission files. These 102 submission files represented the submitted TREC-7 ad hoc runs using a variety of methods and from different research groups, containing the topic ID, retrieved document name and a binary relevance score. A total of 14,307 unique documents referred to in these submission runs for these 10 topics.</p><p>Using these submission documents, we compute two scores for each topic:</p><p>• A simple count, C S , indicates the count of submitted runs (out of 102) that included a given document.</p><p>• A Borda count, C B , takes into account the rank in each submitted run for a given document.</p><p>This represents an approach similar to the one used in <ref type="bibr" coords="2,289.32,390.94,10.37,8.72" target="#b0">[1]</ref>. This Borda count is calculated as (n-r), where n is the number of documents retrieved for a topic in a single submission file, and r represents the document's rank within the list (i.e., the top-ranking document in a list of 1000 documents will receive a score of (1000-1) = 999). We then sum the borda count for all All TREC-7 submissions provide 1000 or fewer documents per topic, so for each of our 102 submissions, the Borda count is in the range (0, 999).</p><p>We use both counts since they represent different properties of each training document. C S measures the number of submissions that include that document for a topic, but does not consider its rank; C B examines the documents rank but does not consider how many of the 102 submitted runs the document appears. For example, for a given topic, if a document exists in all 102 lists, it would receive a C S of 102. However, if that document was ranked at the bottom of each list, the document is not likely to be relevant. Conversely, if a document was listed in only 10 of the 102 lists, but ranked at or near the top of each, C B would be relatively high. The count ratio coefficient, α, represented by a value in the range (0,1), is the relative balance between these two counts for a data collection. Using these two counts (C S and C B ) and applying the count ratio coefficient, α, we calculate a weighted rank coefficient, C(d) W , for each document using these two separate counts for each individual document, d. A document will have a different weighted rank coefficient for each topic examined.</p><formula xml:id="formula_0" coords="2,224.40,595.91,137.58,24.71">C(d) = α C(d) + (1 -α) C(d)</formula><p>A merged listing of documents was created ranked by C(d) W , from highest to lowest for each topic.</p><p>We then experimented with various values of α, from 0 to 1, in increments of 0.05. A relevant document score at α, S , was determined for each topic:</p><formula xml:id="formula_1" coords="2,240.72,677.99,99.42,39.83">S = ∑ rel(n) * C(n) ∑ rel(n)</formula><p>Where rel(n) is the binary relevance for document n and ( ) is the weighted sum for document n for a given α for one topic. S indicates the weighted rank of all relevant documents for a single topic for a given α; we obtain the average S across all ten training topics. If we rank our list by C(d) in decreasing order and the resulting S is large (i.e., documents appearing at the top are relevant), it indicates the selected α bunches the relevant documents closer to the top of our list. The effect of different values of α on average relative document rank is provided graphically in Figure <ref type="figure" coords="3,158.88,177.94,3.67,8.72">2</ref>. Empirically, we determined that α = 0.8 provided the highest S across all training set topics. We therefore use this value for calculating our document score. Table <ref type="table" coords="3,392.04,191.02,4.84,8.72" target="#tab_0">1</ref> provides additional information about each topic used in our training set.  For the 10 topics in this training set, we observe that 6.0% of all documents are considered relevant by the TREC assessors. On average, 7 of the top 10 documents (as ranked by C(d) in decreasing order) for each topic are found to be relevant. For these 10 topics, the mean lowest ranked relevant document position was 750. The mean rank for relevant documents for these 10 topics was 201.68.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Run UIowaS01r: Single Ranked List Method</head><p>For each topic in our training set, we ranked each document by C(d) in decreasing order. We divided this single ranked list into batches. Our objective was to assign these documents into one of three groups:</p><p>1. All documents are relevant 2. Documents are possibly relevant 3. All documents are non-relevant</p><p>We make a manual examination of our documents, beginning with those with the highest C(d) . We determine the most appropriate size for our first group is 10 documents; that is, we mark the top 10 documents as relevant.</p><p>Next, we examine the distribution of the remaining documents. These unmarked documents make up the second and third group of documents. We empirically determine that the most appropriate batch size is 20. These are the documents we submit to the crowd for relevance assessment. To determine the threshold between the second and third document groups, we establish a rule: We submit the document batches to the crowd in descending rank order. If the crowd judges 2 consecutive batches (40 consecutive documents) as not relevant, this marks our relevance threshold and the start of our third group. We mark all the batches below this relevance threshold as not relevant.</p><p>Table <ref type="table" coords="4,95.16,405.22,4.84,8.72" target="#tab_2">2</ref> shows the potential merits with regard to time and cost on our training set: we are able to achieve a 92.3% recall by examining only 26.6% of documents per topic. By starting with p batches for each topic, we are able to assign the initial batches of documents in parallel, reducing crowd assessment time. We simulate crowd assessment in our training runs as we have access to the qrels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Runs UIowaS02r and UIowaS03r: k-means Clustering Method using Document Ranks</head><p>We used another machine-based method to divide the 14,307 documents into the three groups (relevant, maybe relevant, and non-relevant) by topic. In this method, which we use for our second and third runs, we cluster documents based on the similarity in text. We cluster using k-means clustering, a method used to partition n documents into k clusters in which each document belongs to the cluster with the nearest mean. Using Weka 3.6, we clustered using the headline and text (body) sections of each document, placing an equal weight on each of these two sections. Our initial step was to determine a value for the number of clusters, k. To do so, we take the following approach for each topic:</p><p>First, we rank the documents by average C(d) in decreasing order. We mark the 10 documents with highest overall weighted counts across all clusters as relevant. Note that this relevancy determination is independent of our clustering approach. Next, examine the average C(d) for each cluster. Our goal is to have a few clusters that contain a majority of the relevant documents and other clusters that contain very few or no relevant documents. Therefore, we examine the variance of average cluster C(d) for each value of k. With a fixed number of relevant documents, a large variance implies more variation, in terms of relevant documents, across clusters. We evaluated ranges of k between 5 and 25 and empirically determine that k=18 provides the highest average cluster variance and we use this value for k.</p><p>For run UIowaS02r, we begin with the cluster with the largest average C(d) . Within this cluster, we ask the crowd to assess relevance in batches of 20 documents, stopping once the crowd indicates 2 consecutive batches (40 documents) are all non-relevant. We mark the remaining documents in that cluster (in order of C(d) ) as nonrelevant. We then move to the cluster with the next-largest average C(d) and repeat the process for that cluster.</p><p>For run UIowaS03r, we also begin with the cluster with the largest average C(d) . We mark all documents that fall below the 40 th percentile in each cluster as not relevant. We choose to evaluate the top 60% of documents as an empirical evaluation of our training set found very few relevant documents appeared in the bottom 40%. For the remaining unmarked documents, which comprise the top 60% of each cluster, we randomly sample documents from the selected cluster and provide them to the crowd for assessment. We continue with documents from this same cluster until two consecutive batches (40 documents) are found to have no relevant documents. Once that threshold is reached, the remaining documents in that cluster are marked as non-relevant. We then move to the cluster with the next-largest average C(d) and repeat the process for that cluster.</p><p>Table <ref type="table" coords="5,95.04,561.10,4.84,8.72" target="#tab_3">3</ref> (next page) provides the cluster sizes and some basic information on the test set averages obtained for each of the 18 clusters across all topics. This information was used for Runs UIowaS02r and UIowaS03r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The logistic average misclassification rate (LAM) is used as the evaluation metric. LAM is defined as</p><formula xml:id="formula_2" coords="5,228.36,639.59,161.44,38.99">= !"#$% &amp; ' !"#$% (( )) + !"#$% ((*)) 2 ,</formula><p>where fnr is the smoothed false negative rate and the fpr is the smoothed false positive rate.</p><formula xml:id="formula_3" coords="5,249.48,699.23,96.69,39.11">(*) = |./| + 0.5 |./| + |34| + 1 ( ) = |.4| + 0.5 |.4| + |3/| + 1</formula><p>The logit function (and its inverse) are defined as:</p><formula xml:id="formula_4" coords="6,254.52,154.67,86.41,74.51">!"#$% (*) = log * 1 -* !"#$% &amp; = 7 8 1 -7 8</formula><p>Thus, lower values of LAM are desirable. We provide our results for run UIowaS01r (Table <ref type="table" coords="6,450.36,233.38,3.49,8.72" target="#tab_4">4</ref>), run UIowaS02r (Table <ref type="table" coords="6,98.16,246.22,4.07,8.72">5</ref>) and run UIowaS03r (Table <ref type="table" coords="6,216.72,246.22,3.53,8.72">6</ref>). We achieve our best results (and the best results for all TRAT submissions overall) with our second run, which combines ensemble methods (scoring submissions from a number of different techniques), human computation methods (crowdsourcing relevance judgments) and machine methods (clustering). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Judgment Cost and Time</head><p>We performed all of our relevance judgments using Amazon Mechanical Turk. We paid $0.20 for each batch of 20 document assessments, or $0.01 per assessment, which is a customary rate for a binary relevance assessment. Of the 112 different crowdworkers who performed judgments on our three runs, we had 31 crowdworkers (27.7%) evaluate more than a single batch. Table <ref type="table" coords="8,197.28,162.10,4.84,8.72" target="#tab_5">7</ref> examines the cost for each of our three runs. The cost for UIowaS02r is slightly higher per relevant document found than for UIowaS01r, but all are cheaper than UIowaS03r, which uses random sampling. Overall, since we expected to outsource roughly 26% of the documents, the results in Table <ref type="table" coords="8,147.12,323.50,3.67,8.72" target="#tab_5">7</ref>, particularly column indicating the percent of documents assessed, shows we overestimated the number of documents we believed the assessors would evaluate from our training set. Roughly 17% of documents were sent to the crowd for assessment. This is likely due a different distribution of relevant documents from our training set or that our crowd assessors were excessively conservative in their assessment of relevant documents.</p><p>In Table <ref type="table" coords="8,105.24,384.70,3.67,8.72" target="#tab_6">8</ref>, we evaluate the time taken for each of our runs. From Table <ref type="table" coords="8,119.16,531.46,3.67,8.72" target="#tab_6">8</ref>, we see that the time taken for our second run, with more assessments, takes the longest to complete. The tradeoff between time taken and the improvement in the LAM rate for our runs indicates the method used has an impact. The methods that apply our ranking approach (runs UIowaS01r and UIowaS02r) are more efficient than the run that uses random samples (UIowaS03r).</p><p>These time and cost numbers do not count the real cost of obtaining the submitted runs for our test set, since these represent important inputs to our process. The run that is least dependent on submitted run information (UIowaS03r) had the highest LAM rate, indicating the power of the ensemble method to reduce LAM rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations of our Methods</head><p>We identify some of the limitations of our methods. They are as follows:</p><p>1. There is a difference between the test and training sets. The distribution of the data between the test and training set topics might be different. A preliminary assessment of the test topics indicates their underlying distribution is quite different for 4 of the 10 topics.</p><p>2. Reliance on the crowd to set relevance thresholds. The crowd determines which documents are relevant and also when we stop our assessment. One careless crowdworker could grab two consecutive batches for a topic, mark them all non-relevant, and we would subsequently miss many important relevance judgments for that topic that appear lower in our ranked list. 3. Lack of anti-spam crowdsourcing techniques. Due to time constraints on our part, we did not integrate any of the voting techniques, honey pots, or other types of crowdsourcing quality checks that are now commonplace, which affected the precision of our results. This is easily addressed through the application of voting mechanisms and incentives, as has been discussed in <ref type="bibr" coords="9,354.84,197.98,10.70,8.72" target="#b1">[2,</ref><ref type="bibr" coords="9,368.40,197.98,7.22,8.72" target="#b2">3,</ref><ref type="bibr" coords="9,378.48,197.98,3.53,8.72" target="#b4">5</ref>]. We also did not limit the number of batches a single crowdworker could assess, which may potentially bias our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Summary</head><p>For the TREC Crowd'12 TRAT task, we used a hybrid method for each of our three runs; however the hybrid method used differs slightly for each run. Central to each method is a calculation of a weighted score for each document for each topic. We rank our list of documents using this score. We then set out to break the list into three groups of documents for that topic -definitely relevant, possibly relevant, definitely non-relevant. We take the top 10 ranked documents as our definitely relevant group. The possibly relevant and definitely non-relevant groups were submitted to the crowd for relevance assessment.</p><p>For run UIowaS01r, we score our documents and then create a single ranked list. We send documents to the crowd in decreasing rank score order. The crowd continued to assess these documents and if there was at least 1 relevant document out of 40 consecutive documents, we continued to send documents to the crowd. Our preliminary assessment using training data from the TREC 7 ad hoc track showed we could assess 92.3% of relevant documents only using 26.6% of the document set. In our test set, we used only 17% of the document set, but we did not find as many relevant documents. Thus, our method is dependent on test and training sets have a very similar distribution.</p><p>For runs UIowasS02r and UIowaS03r, we performed k-means clustering of all the documents. Using 18 clusters and the ranked list of each document for each topic, we applied two slightly different methods of obtaining documents form these clusters to send to the crowd for assessment. Run UIowaS02r results, which made better use of our ranking methods, performed better than our third run, UIowaS03r, which instead used a random sampling method. All three of our runs were above the mean LAM rate for all submitted runs. Although significance testing was not performed, our best results we obtained using a combination of methods, which illustrates the merits of this hybrid approach.</p><p>We believe much of our strong performance is due to an ensemble method. We use an ensemble method to establish ranking and weights, machine approaches (clustering), and a human computation approach (crowd-based relevance assessment). We derive our weights using the submissions for 129 training runs from many different participants.</p><p>As with crowdsourcing in general, the use of a number of different submission approaches provides allows us to obtain ensemble-like results, which reduce the risk of applying a single method. However, we note that distributional differences between our training and test results may present biases that skew the document weights upon which we rely, undermining many aspects of our experiment. In most experimental settings, submission run data is not typically available, limiting our ability to extrapolate these results to real-world scenarios.</p><p>Although not a stated TRAT task objective, we also examine how the methods used for our three runs vary in terms of quality (in terms of LAM) relative to the time and cost needed. Our first run (UIowaS01r) requires the fewest number of documents to be evaluated (and the lowest time and costs); our second run (UIowaS02r) requires the most. Although further analysis needs to be performed, our initial observation is that the slight extra time and cost required for UIowaS02r results in a much larger improvement in the LAM rate.</p><p>Last, we highlight some limitations of our approach. Among these are a dependency on a similar distribution of relevant documents between our training and our test sets, a heavy reliance on the crowd to determine when we stop our assessment for a given topic, and a lack of employing anti-spam crowdsourcing techniques, which could permit sloppy or malevolent behavior to occur in our assessment tasks. These represent directions of future work in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,143.16,426.89,309.59,12.63;3,446.40,427.13,6.35,12.63;3,446.40,426.89,6.11,12.63"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Relation between the document rank for 10 training topics and α α α α</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,128.04,450.41,335.35,12.87"><head>Table 1 . Overview of the 10 training topics using a ranked list approach at</head><label>1</label><figDesc></figDesc><table /><note coords="3,439.20,450.41,6.11,12.63;3,438.96,450.65,6.35,12.63;3,438.96,450.41,24.43,12.63"><p>α α α α = 0.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,71.28,240.97,454.36,455.45"><head>8 topic ID # of unique documents for that topic ID # of relevant documents for that topic ID Percent relevant overall # of relevant docs in top 10 ranked docs</head><label></label><figDesc></figDesc><table coords="3,78.12,240.97,447.52,455.45"><row><cell></cell><cell></cell><cell>205.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Average Document Rank</cell><cell>201.0 202.0 203.0 204.0</cell><cell cols="3">Average Relevant Document Rank</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>200.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Count Ratio Coefficient, α , α , α , α</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Position of highest ranked relevant document</cell><cell>Position of lowest ranked relevant document</cell><cell>Average rank of relevant documents</cell></row><row><cell>351</cell><cell cols="2">1029</cell><cell>48</cell><cell>4.66</cell><cell>9</cell><cell>1</cell><cell>723</cell><cell>128.61</cell></row><row><cell>358</cell><cell cols="2">1121</cell><cell>51</cell><cell>4.55</cell><cell>6</cell><cell>3</cell><cell>609</cell><cell>122.33</cell></row><row><cell>364</cell><cell cols="2">1513</cell><cell>35</cell><cell>2.31</cell><cell>9</cell><cell>1</cell><cell>182</cell><cell>40.46</cell></row><row><cell>369</cell><cell cols="2">1706</cell><cell>13</cell><cell>0.76</cell><cell>5</cell><cell>1</cell><cell>102</cell><cell>176.43</cell></row><row><cell>374</cell><cell cols="2">1107</cell><cell>203</cell><cell>18.34</cell><cell>5</cell><cell>1</cell><cell>886</cell><cell>352.11</cell></row><row><cell>375</cell><cell cols="2">1140</cell><cell>80</cell><cell>7.02</cell><cell>7</cell><cell>1</cell><cell>719</cell><cell>250.92</cell></row><row><cell>379</cell><cell cols="2">2196</cell><cell>16</cell><cell>0.73</cell><cell>4</cell><cell>1</cell><cell>660</cell><cell>32.38</cell></row><row><cell>388</cell><cell cols="2">1467</cell><cell>51</cell><cell>3.48</cell><cell>7</cell><cell>2</cell><cell>1111</cell><cell>253.84</cell></row><row><cell>395</cell><cell cols="2">1492</cell><cell>213</cell><cell>14.28</cell><cell>6</cell><cell>1</cell><cell>1483</cell><cell>431.06</cell></row><row><cell>396</cell><cell cols="2">1536</cell><cell>59</cell><cell>3.84</cell><cell>9</cell><cell>1</cell><cell>1022</cell><cell>228.70</cell></row><row><cell>ALL</cell><cell cols="2">Sum: 14307</cell><cell>Sum: 769</cell><cell>Avg: 6.00</cell><cell>Avg: 6.7</cell><cell>Avg: 1</cell><cell>Avg: 750</cell><cell>Avg: 201.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,87.12,466.78,416.09,231.44"><head>Table 2 . Values obtained using the simulation and fixed batch sizes. topic ID # of unique documents for that topic ID # of relevant documents for that topic ID Lowest ranked document examined # of relevant documents found using this approach Percent of documents examined</head><label>2</label><figDesc>For our training set, we send 190 batches for crowd assessment. At a cost of $0.20 per batch of 20 documents, this involves a total cost to us of $40.70 for this run, including the 10% Amazon Mechanical Turk fees.</figDesc><table coords="4,458.40,489.70,44.81,42.32"><row><cell>Percent of</cell></row><row><cell>relevant</cell></row><row><cell>documents</cell></row><row><cell>found</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.60,294.82,414.35,190.52"><head>Table 3 . Number of batches evaluated for each Topic ID. Note each run had an initial top 10 ranked documents judged.</head><label>3</label><figDesc></figDesc><table coords="6,173.04,330.46,253.54,154.88"><row><cell>.Topic ID</cell><cell>UIowaS01r</cell><cell>UIowaS02r</cell><cell>UIowaS03r</cell></row><row><cell>411</cell><cell>6</cell><cell>7</cell><cell>6</cell></row><row><cell>416</cell><cell>11</cell><cell>15</cell><cell>16</cell></row><row><cell>417</cell><cell>15</cell><cell>19</cell><cell>12</cell></row><row><cell>420</cell><cell>10</cell><cell>15</cell><cell>16</cell></row><row><cell>427</cell><cell>8</cell><cell>14</cell><cell>16</cell></row><row><cell>432</cell><cell>7</cell><cell>14</cell><cell>10</cell></row><row><cell>438</cell><cell>25</cell><cell>29</cell><cell>24</cell></row><row><cell>445</cell><cell>22</cell><cell>27</cell><cell>24</cell></row><row><cell>446</cell><cell>22</cell><cell>30</cell><cell>28</cell></row><row><cell>447</cell><cell>5</cell><cell>6</cell><cell>5</cell></row><row><cell>ALL</cell><cell>Sum: 133</cell><cell>Sum: 176</cell><cell>Sum: 157</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,72.36,512.50,445.55,198.56"><head>Table 4 . Results per topic ID for UIowaS01r. Test Topic ID Documents in Collection Marked as Relevant (TP + FP) Correctly Identified (TP) All Relevant (TP + FN)</head><label>4</label><figDesc></figDesc><table coords="6,386.52,540.94,131.39,20.00"><row><cell>Run1 Results</cell><cell>TRAT Task</cell></row><row><cell>(LAM)</cell><cell>Mean (LAM)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,82.68,184.78,439.07,86.12"><head>Table 7 . Analysis of cost for each of our runs.</head><label>7</label><figDesc></figDesc><table coords="8,82.68,207.82,439.07,63.08"><row><cell>Run</cell><cell>Documents Assessed</cell><cell>Percent of Docs Assessed</cell><cell>Cost (including AMT service fee)</cell><cell>LAM</cell><cell>Cost per Relevant Document</cell></row><row><cell>UIowaS01r</cell><cell>2660</cell><cell>14.6</cell><cell>$29.26</cell><cell>0.069</cell><cell>$0.072</cell></row><row><cell>UIowaS02r</cell><cell>3520</cell><cell>19.3</cell><cell>$38.72</cell><cell>0.047</cell><cell>$0.082</cell></row><row><cell>UIowaS03r</cell><cell>3140</cell><cell>17.2</cell><cell>$34.54</cell><cell>0.066</cell><cell>$0.092</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,82.68,407.50,439.91,97.04"><head>Table 8 . Analysis of time taken for each of our runs.</head><label>8</label><figDesc></figDesc><table coords="8,82.68,430.42,439.91,74.12"><row><cell>Run</cell><cell># Docs Assessed</cell><cell>Task Time taken (hours)</cell><cell>Time taken per batch of 20 (min)</cell><cell>LAM</cell><cell>Time per Relevant Document (min)</cell></row><row><cell>UIowaS01r</cell><cell>2660</cell><cell>42</cell><cell>9.8</cell><cell>0.069</cell><cell>6.161</cell></row><row><cell>UIowaS02r</cell><cell>3520</cell><cell>51</cell><cell>10.3</cell><cell>0.047</cell><cell>6.442</cell></row><row><cell>UIowaS03r</cell><cell>3140</cell><cell>47</cell><cell>10.1</cell><cell>0.066</cell><cell>7.540</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALL</head><p>Sum: 18260 Sum: 523 Sum: 409 Sum: 637 Avg: 0.069 Avg: 0.187 We acknowledge there are a number of crowdsourcing techniques that would likely improve our results, such as spam detection, incentives, or having overlapping assessments made by different crowdworkers and applying a voting method. We believe that not having multiple assessments negatively impacted worker accuracy.</p><p>Like other groups, we struggled with several of the topics, particularly topic 432 ("Do police departments use 'profiling' to stop motorists?"). This may be due to the difficulty of crowdworkers who reside outside of the United States to understand the nature of this information need, since profiling is not a universally-known concept of police methods. Other information needs that have greater concept transferability did better, such as topic 416 ("What is the status of The Three Gorges Project?"), which clearly refers to a massive construction project in China that is well-documented in the press, and is well-known globally.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,73.75,211.06,451.89,8.72;10,87.60,224.02,438.14,8.72;10,87.60,236.98,18.02,8.72" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,348.60,211.06,177.04,8.72;10,87.60,224.02,109.65,8.72">University of Iowa at TREC 2008 Legal and Relevance Feedback Tracks</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Almquist</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mejova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ha-Thuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<idno>TREC&apos;08</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,215.40,224.02,201.53,8.72">Proceedings of the 17th Text Retrieval Conference</title>
		<meeting>the 17th Text Retrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.75,255.58,451.83,8.72;10,87.60,268.42,393.14,8.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,229.32,255.58,296.26,8.72;10,87.60,268.42,41.58,8.72">Can we get rid of TREC assessors? Using Mechanical Turk for relevance assessment</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,146.40,268.42,285.64,8.72">Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation</title>
		<meeting>the SIGIR 2009 Workshop on the Future of IR Evaluation</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="15" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.75,287.14,451.89,8.72;10,87.60,299.98,203.06,8.72" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
		<title level="m" coord="10,245.76,287.14,279.88,8.72;10,87.60,299.98,104.23,8.72">Using crowdsourcing for TREC relevance assessment. Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1053" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.75,318.82,451.89,8.72;10,87.60,331.66,364.10,8.72" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,433.56,318.82,92.08,8.72;10,87.60,331.66,34.63,8.72">IBM&apos;s PIQUANT II in trec 2004</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<idno>TREC&apos;04</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,129.12,331.66,195.41,8.72">Proceedings of the 13th Text Retrieval Conference</title>
		<meeting>the 13th Text Retrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.75,350.38,451.90,8.72;10,87.60,363.22,437.92,8.72;10,87.60,376.06,342.02,8.72" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,374.28,350.38,151.37,8.72;10,87.60,363.22,196.88,8.72">Quality through flow and immersion: gamifying crowdsourced relevance assessments</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,305.52,363.22,220.00,8.72;10,87.60,376.06,256.49,8.72">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="871" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.75,394.90,451.75,8.72;10,87.60,407.74,438.04,8.72;10,87.60,420.58,167.30,8.72" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,231.84,394.90,290.31,8.72">Information retrieval system evaluation: effort, sensitivity, and reliability</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,98.28,407.74,427.36,8.72;10,87.60,420.58,81.89,8.72">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
