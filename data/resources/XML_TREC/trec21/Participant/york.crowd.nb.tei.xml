<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,118.15,160.71,375.70,15.12">York University at TREC 2012: CrowdSourcing Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.27,194.56,48.98,8.74"><forename type="first">Qinmin</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval and Knowledge Managment Lab</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.14,194.56,30.73,8.74"><forename type="first">Zhi</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval and Knowledge Managment Lab</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.76,194.56,63.65,8.74"><forename type="first">Xiangji</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval and Knowledge Managment Lab</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,380.30,194.56,40.96,8.74"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
							<email>yezheng@yorku.ca</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval and Knowledge Managment Lab</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,118.15,160.71,375.70,15.12">York University at TREC 2012: CrowdSourcing Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F2903213C0071DF4D755860402439213</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CrowdSourcing</term>
					<term>Information Retrieval</term>
					<term>CrowdFlower</term>
					<term>BM25</term>
					<term>DFR</term>
					<term>Language Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The objective of this work is to address the challenges in managing and analyzing crowdsourcing in the information retrieval field. In particular, we would like to answer the following questions: (1) how to control the quality of the workers when crowdsourcing? (2) How to design the interface such that the workers are willing to participate in and are driven to give useful feedback information? (3) How to make use the crowdsourcing information in the IR systems? The crowdsourcing system called CrowdFlower is employed and four classic information retrieval models are applied in our proposed approaches. Our experimental results show that the IR systems refine the results crowdsourcing by minimizing the manual work and the cost is much less.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first year that our York University group participates in the TREC 2012 CrowdSourcing Track. We focus on the text relevance assessing task (TRAT). TRAT is one of the two tasks in the TREC 2012 Crowdsourcing Track, in which the other one is the image relevance assessing task (IRAT). The goal of TRAT is to evaluate approaches to text relevance assessing.</p><p>Different with the other tracks such as genomics and blog, we simulate playing the relevance assessing role of NIST in the TREC 8 ad-hoc track <ref type="bibr" coords="1,318.85,559.74,129.26,8.74" target="#b3">[Voorhees and Harman, 1999]</ref> with a subset of the TREC 8 topics. 10 topics from TREC 8 ad-hoc are randomly selected for use in the <ref type="bibr" coords="1,490.86,571.69,31.14,8.74">TRAT,</ref><ref type="bibr" coords="1,90.00,583.65,65.00,8.74">which are 411,</ref><ref type="bibr" coords="1,159.66,583.65,17.71,8.74">416,</ref><ref type="bibr" coords="1,182.04,583.65,17.71,8.74">417,</ref><ref type="bibr" coords="1,204.42,583.65,17.71,8.74">420,</ref><ref type="bibr" coords="1,226.80,583.65,17.71,8.74">427,</ref><ref type="bibr" coords="1,249.17,583.65,17.71,8.74">432,</ref><ref type="bibr" coords="1,271.55,583.65,17.71,8.74">438,</ref><ref type="bibr" coords="1,293.93,583.65,17.71,8.74">445,</ref><ref type="bibr" coords="1,316.30,583.65,17.71,8.74">446,</ref><ref type="bibr" coords="1,338.68,583.65,14.95,8.74">447</ref> provided officially by the NIST. The title, description and narrative of a topic define the relevance and non-relevance to itself when a document is judged. The documents are partially selected from the TREC 8 ad-hoc which uses the Text Research Collection Volumes 4 (May 1996) and 5 (April 1997) minus the Congressional Record (CR) <ref type="bibr" coords="1,148.97,631.47,126.20,8.74" target="#b3">[Voorhees and Harman, 1999]</ref>. In the TART, there are 18,260 topic-document pairs to be judged under 10 topics.</p><p>An assumption is made in the TART that we are required to utilize crowdsourcing to do the relevance assessing, but can use any approach as long as we follow the task's guidelines 1 . Therefore, we propose a crowdsourcing system in Figure with four approaches as the submitted four runs, which obtain the benefits of both crowdsourcing and the traditional information retrieval (IR) models. The differences among these four approaches are that they adopt four relevance feedback methods as interactive feedback, tf-idf feedback, modified pseudo feedback and proximity feedback. The crowdsourcing system we use is CrowdFlower and the traditional IR models are BM25, DFR and language model (LM). There are 47.25 dolloars costed in our experiments and in total around 5 hours for both worker training and real jobs. More details are presented in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Approaches</head><p>Here we present our approaches of (1) how to utilize crowdsourcing; (2) how to apply the IR systems;</p><p>(3) how to make use of the crowdsourcing information into the IR systems.</p><p>The motivation of using the IR systems into the whole crowdsourcing procedure is that the cost will be very high if we manually ask the workers to judge the given 18,260 topic-document pairs directly. Additionally, the accuracy will be less if there are more pairs judged by workers, since the quality control is a big challenge in crowdsourcing. Hence, we refine the number of pairs judged by the workers through retrieving the documents by the IR systems.</p><p>As we can see in Figure <ref type="figure" coords="2,211.36,297.64,3.87,8.74" target="#fig_0">1</ref>, the IR systems output two rounds results, in which the first round is to obtain the candidate documents for crowdsourcing, and the second round is to provide evidences for the final decisions of the given pairs. A key step of this figure is how to design the crowdsourcing page and get the useful information from the workers. More details will be discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>(1) Queries (2) TREC 8 Data sets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>A list of relevance judgements for the topic-document pairs, which includes the topic, the document number, the binary judgement (0 for non-relevant and 1 for relevant, the probabilities of relevance and the run tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>(1)Preliminary results: the IR systems output the first-round retrieved results, which are ready for crowdsourcing (2) Crowdsourcing results: design the page in the crowdsourcing system called CrowdFlower</p><p>The relevance documents for each topic are manually judged by the workers The feedback information is obtained from the designed page (3) Evidence results: get the feedback information back to the IR systems again</p><p>The second-round retrieved results are obtained (4) Binary judgements are made based on the second-round retrieved results for the given topic-document pairs, the probabilities are computed as well for each pair </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CrowdFlower</head><p>Crowdsourcing is an online practice, which describes the act of outsourcing work to a large group of people of a community as a crowd.</p><p>It is an open call for contributions from the crowd to complete a task in exchange for social recognition, micro-payments and so on. Nowadays, crowdsourcing has attracted growing attentions as a valuable solution to harness human abilities from a large population of workers <ref type="bibr" coords="2,138.29,713.91,53.89,8.74" target="#b1">[Howe, 2008]</ref>. The crowdsourcing of relevance judgements enables the evaluation of the IR systems on the large-scale data sets.</p><p>CrowdFlower uses crowdsourcing techniques to provide a wide range of enterprise solutions which process or create large amounts of data. CrowdFlower has over 50 labor channel partners, among them Amazon Mechanical Turk and TrialPay; their network is composed of over 2 million Contributors from all over the world. We present our crowdsourcing stage in Figure <ref type="figure" coords="3,426.02,159.69,3.87,8.74">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>The top five common retrieved documents</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>Manually judged relevant documents and the corresponding supportive keywords</p><p>Crowdsourcing; CrowdFlower System (1) For 10 topics, there are 10*5 records which will be judged by the workers (2) Design the page which shows the topic document pair to the workers and ask the feedback from the workers (3) Clean the worker's feedback (4) Get the relevant documents judged by the workers and the corresponding keywords and then put them into the IR systems again We design our page to catch the feedback information from the workers, including the instructions given for each task, the topic and the document candidate presented for workers' reading. After this general information, the judgements and supported keywords are required for the worker to complete this job.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quality Control</head><p>CrowdFlower stands apart from these individual networks because they offer the quality control, called Gold Standard Data, which has workers perform pre-completed tasks to determine their accuracy and trustworthiness. So we adopt the TREC 7 topics as the training topics to find the valued workers. For each task, we also ask 10 workers to complete the same task as a peer review method. Hence, we present the sample without quality control in Table <ref type="table" coords="3,397.26,504.54,4.98,8.74" target="#tab_0">1</ref> and the worker information is also listed in Table <ref type="table" coords="3,185.64,516.49,3.87,8.74">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IR Systems</head><p>Here we first present how the IR systems obtain the preliminary results and the evidence results with different IR models in Figure <ref type="figure" coords="3,242.87,709.83,3.87,8.74">3</ref>. In the preliminary step, the IR systems find the most likely relevant documents as the candidates for the manually judgements in the crowdsourcing stage. In order to improve the relevance possibilities of the retrieved results, four IR models of BM25, DFR, BM25 DFR and LM are adopted. Note that the candidate for crowdsourcing are the top five common documents in all four retrieved lists.</p><p>In the evidence step, the IR systems treat the manually feedback information in four ways: (1) directly make use of the manually judged documents and the keywords provided by the workers in the IR systems; (2) use the manually judged documents and then adopt the proximity feedback method proposed by <ref type="bibr" coords="4,184.03,333.18,80.40,8.74" target="#b2">[Miao et al., 2012]</ref> to get the weighted feedback terms; (3) use the manually judged documents and then apply the standard pseudo feedback method <ref type="bibr" coords="4,408.08,345.14,66.66,8.74" target="#b0">[He et al., 2004]</ref>; (4) count the TF-IDF of the terms in the manually judged documents and extract the top 20 terms as the feedback terms. These are also our four runs submitted to the track for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>(1) Queries (2) TREC 8 Data sets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output: two lists of retrieved documents</head><p>Preliminary Results:</p><p>(1) Customize the queries and the TREC 8 collection into the IR systems (2) Apply an IR model: { Output the retrieval list where documents are searched without no judgements } (3) Four IR models are applied: BM25, DFR, BM25 DFR, LM (4) For each topic { Extract the top five common retrieved documents from four retrieved lists Note that these fie retrieved documents have to be retrieved by all the four models} (5)The top five documents for each topic are ready for crowdsourcing Evidence Results:</p><p>(1) Make use of the manually judged documents and keywords as the relevance feedback into the IR systems { Output four retrieved lists again under four IR models of BM25, DFR, BM25 DFR and LM} (2) Make use of the manually judged document without keywords { Adopt the proximity feedback method proposed by <ref type="bibr" coords="4,346.73,567.35,63.20,6.12" target="#b2">[Miao et al., 2012]</ref> Then get the weighted feedback terms as the relevance feedback Output four retrieved lists again under four IR models of BM25, DFR, BM25 DFR and LM} (3) Make use of the manually judged document without keywords { Adopt the pseudo feedback method <ref type="bibr" coords="4,292.09,599.24,55.27,6.12" target="#b0">[He et al., 2004]</ref> Then get the pseudo feedback terms as the relevance feedback Output four retrieved lists again under four IR models of BM25, DFR, BM25 DFR and LM} (4) Make use of the manually judged document without keywords { Calculating the TF-IDF of each term in the relevant documents Extract the top 20 terms as the relevance feedback Output four retrieved lists again under four IR models of BM25, DFR, BM25 DFR and LM}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: IR Systems</head><p>We report our experimental results here. All runs' binary judgements are evaluated using the LAM measure and treated the adjudicated judgements as truth. The submitted probability of relevance figures are evaluated by the AUC measure and also treated the adjudicated binary judgments as truth.  Here we present our work in the TART task of the TREC 2012 Crowdsourcing Track. One of our major motivations is to refine the crowdsourcing jobs through adopting the traditional IR systems.</p><p>The crowdsourcing system called CrowdFlower is employed and four classic information retrieval models are applied in our proposed approaches as BM25, DFR, BM DFR and LM. Another four feedback methods are also presented as the submitted four runs, where the feedback terms are respectively given by the crowdsouricng workers, by the proximity based feedback method, by the standard pseudo feedback method and TF-IDF.</p><p>Our experimental results show that the LAM results is much better than the AUC results. The main reason is that the way that we compute the probabilities is very simple. This is our ongoing work in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,182.27,589.13,247.46,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A Whole Procedure of the Proposed Approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,238.34,340.70,135.32,8.74"><head>Figure</head><label></label><figDesc>Figure 2: Crowdsourcing Stage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,106.51,516.49,398.98,121.46"><head>Table 1 :</head><label>1</label><figDesc>. The Sample CrowdSourcing Results</figDesc><table coords="3,106.51,536.47,398.98,70.27"><row><cell>unit id</cell><cell>id</cell><cell>judgement</cell><cell>keyword1</cell><cell>keyword2</cell><cell>keywords3</cell><cell>others</cell></row><row><cell>195005932</cell><cell>563650952</cell><cell>Relevant</cell><cell>National Assembly</cell><cell>women priests</cell><cell>young women</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Religious Women</cell><cell></cell><cell></cell><cell></cell></row><row><cell>195005932</cell><cell>563662790</cell><cell>Relevant</cell><cell>LUTHERANS ...</cell><cell>Times Staff</cell><cell>RUSSELL CHANDLER</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>BLESSING OF ...</cell><cell>Writers</cell><cell>JOHN DART</cell><cell></cell></row><row><cell>195005932</cell><cell>563662882</cell><cell>Relevant</cell><cell>woman</cell><cell>religion</cell><cell>clergy</cell><cell></cell></row><row><cell>195005932</cell><cell>563686956</cell><cell>Relevant</cell><cell>9</cell><cell>7</cell><cell>8</cell><cell>waw</cell></row><row><cell>195005932</cell><cell>563690807</cell><cell>Relevant</cell><cell>44</cell><cell>kjlk</cell><cell>h</cell><cell></cell></row><row><cell>195005932</cell><cell>563694956</cell><cell>Irrelevant</cell><cell>f</cell><cell>k</cell><cell>j</cell><cell>l</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,90.00,438.59,380.81,53.58"><head>Table 3 :</head><label>3</label><figDesc>Performance of Four Runs Compared to the Official Median Value 6 Conclusions and Future Work</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,722.13,210.75,6.99"><p>//www.mansci.uwaterloo.ca/ msmucker/trec2012TRAT/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,90.00,659.97,432.00,6.99;5,99.96,669.44,391.57,6.99" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,298.09,659.97,223.91,6.99;5,99.96,669.44,151.44,6.99">Pseudo Relevance Feedback Based on Iterative Probabilistic One-Class SVMs in Web Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,269.31,669.44,196.11,6.99">Proceeding of Pacific-Rim Conference on Multimedia</title>
		<meeting>eeding of Pacific-Rim Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,90.00,684.88,272.68,6.99" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Crown Publishing Group</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct coords="5,90.00,700.32,432.00,6.99;5,99.96,709.79,77.20,6.99" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,277.95,700.32,198.35,6.99">Proximity-based rocchio&apos;s model for pseudo relevance</title>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><forename type="middle">Xiangji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,495.41,700.32,22.16,6.99">SIGIR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,90.00,725.23,422.79,6.99" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,240.33,725.23,206.04,6.99">Overview of the eighth text retrieval conference (trec-8)</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,464.06,725.23,21.30,6.99">TREC</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
