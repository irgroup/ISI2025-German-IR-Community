<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.62,74.60,328.45,10.80">(Not Too) Personalized Learning to Rank for Contextual Suggestion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,112.22,99.61,60.50,9.50"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
							<email>andrew@cs.georgetown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.34,99.61,57.51,9.50"><forename type="first">Dave</forename><surname>Deboer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.61,99.61,41.27,9.50"><forename type="first">Hui</forename><surname>Yang</surname></persName>
							<email>huiyang@cs.georgetown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.17,99.61,64.32,9.50"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.27,99.61,56.77,9.50"><forename type="first">Steve</forename><surname>Kunath</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">Georgetown University Washington</orgName>
								<address>
									<postCode>20057</postCode>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,438.79,99.61,57.53,9.50"><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.62,74.60,328.45,10.80">(Not Too) Personalized Learning to Rank for Contextual Suggestion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D6D1EC55C80A4EE09D27929276DCF37</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we emphasize how to merge and re-rank contextual suggestions from the open Web based on a user"s personal interests. We retrieve relevant results from the open Web by identifying context-independent queries, combining them with location information, and issuing the combined queries to multiple Web search engines. Our learning to rank model utilizes three types of profiles (a general profile, a city profile, and a personal profile) to re-rank and merge the results retrieved from the Web. We find that the learning model generates better results when the user profiles" weights are biased heavily towards major personal interests. The detections of major, minor and negative personal interests are done by statistical analysis across users, examples, and context-independent query types. For user interests detected by query types, we call the interests "macro-level interests", while for user interest detected by examples, we call them "micro-level interests". In our experiments, we find that "micro-level interests" effectively avoid favoring too much towards rare query types such as spa and game, and thus yields more balanced rankings. Finally, for the top ranked suggestions for each user and context, we generate result descriptions by learning to rank favorable Yelp comments and using a natural language generation algorithm to generate positive comments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work, we emphasize how to merge and re-rank contextual suggestions from the open Web based on a user"s personal interests. We first identify context-independent queries from the given Toronto examples based on mapping to a two-level Yelp-like ontology. The context-independent queries combined with new location information are sent to multiple Web search engines, such as Google, Yelp, Yellow Pages, and Bing, to crawl and build a large pool of potential contextual suggestions. A learning to rank model is then used to merge and re-rank those potential suggestions from the pool for each query, context, and user profile combination based on a heterogeneous set of features. A resource merging algorithm is used to merge the results from all queries within each context and user pair. Finally, for the top ranked suggestions, besides extracting the top rated Yelp comments as their descriptions, we design a natural language generation algorithm to generate positive comments for the suggestions.</p><p>The learning to rank model utilizes three types of profiles: a general profile based on the training suggestions given in the Toronto examples, a city profile containing well-known attractions for each city, and a personal profile based on a user"s personal interests. Since the pool of potential contextual suggestions is crawled from major search engines, a problem that we face is that many suggestions are local stores that seem "not well-known" and therefore are "not attractive" to visitors who may want to see the major attractions in a new place. Our use of general profiles and city profiles resolves this issue by improving the availability and ranking of famous attractions, thereby providing more balanced recommendation to a visitor.</p><p>Moreover, the learning model makes distinctions among major personal interests, minor personal interests, and negative personal interests for all personal profiles. We find that the learning model generates better results when the weights are biased heavily towards major personal interests. The detections of major, minor and negative personal interests are done by statistical analysis across users, examples, and context-independent query types.</p><p>For user interests detected by query types, we call the interests "macro-level interests", while for user interest detected by examples, we call them "micro-level interests". In our experiments, we find that "micro-level interests" effectively avoid favoring too much towards rare query types such as spa and game, and thus yields more balanced rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: System Architecture</head><p>Our system consists of three primary components: a query generator that utilizes provided examples and crawled pages to formulate queries; a query ranking component that produces a ranked result set for each context and user pair, and a description generator that produces descriptions for our final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Query Formulation</head><p>As the overall task is to provide entertainment suggestions based on user preferences for the provided examples, we generated the queries using information from those examples. The example data consisted of a title, a description, and a url to the example website.</p><p>We implemented various methods, including using phrases from the title, phrases from the description, and phrases from the website. We also utilized Web resources, such as categories assigned at Yelp for the provided examples. Among these, the Yelp categories seemed to perform the best as to generating a good query from an example. We observed two shortcomings in utilizing the Yelp categories:</p><p>First, the Yelp categories could be very specific, such as "Sushi Bar" for examples where we would also want to consider the query "Restaurant". This problem was solved using a mapping from Yelp categories to more general categories. The second problem was that not all of the examples had Yelp pages, and so we did not have a Yelp category for those examples. To automatically assign categories to examples we generated representative documents for each category and then used the probabilistic retrieval strategy BM25 <ref type="bibr" coords="2,218.91,432.71,11.86,8.96" target="#b3">[4]</ref> with the example title and description as the query to determine which representative document was most relevant to the query. The category whose document was found to be most relevant was then assigned to the example. The representative documents were generated by querying Google using the category, taking the words from the snippets that Google displays, and retrieving any Wikipedia articles that appear. From the snippets and Wikipedia articles we obtained word counts, and from the list of 100 most frequent words we manually extracted words for the document, weighted by their counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Profile Creation</head><p>Our ranking algorithm uses three types of profiles: general, personal, and city; the general and personal profiles consist of a list of categories with weights for each category, while city profiles are ranked lists of well-known attractions in a city. We tested two different types of profiles: one with the more specific Yelp categories, and one with the more general categories. The profiles were created as follows:</p><p>1. General Profile: This profile was created by counting each training example for a given category and using that count as the weight (normalized by the total number of examples). The purpose of the profile was to bias the result categories to obtain a distribution of categories equal to that of the example set. 2. Personal Profile: A personal profile was created for each of the user profiles given. These profiles were meant to reflect user interests and were used to bias our algorithm towards suggestions that the user was likely to favor. We had two different methods for creating these profiles: macro-level interests were detected by query type, and micro-level interests were detected by examples. The algorithm for each was the same, but with a subtle difference at step 2: a.</p><p>Step 1: To determine a category score, we first built a matrix with each cell being the count of positive reviews for a given profile for a given category b.</p><p>Step 2: For Macro, these counts were normalized by the total number of that category in the training examples. For Micro, these counts were normalized by the total number of examples.</p><p>c.</p><p>Step 3: From this we determined the average user count and the average category count. d.</p><p>Step 4: We assigned a weight to each cell in the matrix. If the cell was greater than both the profile average and the category average, it received the highest weight (w1). If it was higher than the row average only, it was given second highest weight (w2). If it was not higher than either, but still non-zero, it was semi-preferred (w3). If it was zero, then it was not preferred (w4). e.</p><p>Step 5: We multiplied this weight by the category probability (Pr(Example|Category)), and then by the profile probability (Pr(Positive Rating|Profile and Category)). The weights we used were w1 = 2.0, w2 = 0,7, w3 = 0.3, and w4 = 0.0 with the micro method. 3. City Profile: The purpose of the city profile is to include well-known attractions that a city might be known for. For example, Central Park is a good suggestion for anyone visiting New York. We crawled each city"s page on aviewoncities.com, wikitravel.org, and Wikipedia to find results that were mentioned on the page. We labeled each of these results as a well-known attraction. To create a ranked list of attractions for each city, each city"s well-known attractions were ranked by the attraction"s rank on aviewoncities.com and the number of Web sites the attraction was found on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Crawling &amp; Database Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Crawling &amp; Database Design</head><p>We used a relational database to store information about resources found on the Web. Our schema consisted primarily of tables for locations, users, and user rankings of locations. Location records stored information about potential locations for a contextual suggestion and allowed us to store a variety of associated metadata including precise geographical attributes and operating hours. Tables relating to user information stored information concerning a user's preference and links to actual location information. We evaluated Web sites that provided a geographical search capability and constructed a rough scheme of possible metadata fields for each entity including operating hours, location, and review. We designed crawlers to query Web search engines and crawl through the results, extracting as much metadata information as possible, and then inserting the associated data into our database system for later analysis and indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Crawling Methodology</head><p>We settled on five Web search engines that were relevant to our task: Google, Google Places, Bing, Yelp, and Yellow Pages. We issued a query to each search engine for every category, location pair and crawled everything listed in the first 5 pages of results. We used the Nokogiri library (http://nokogiri.org/) to extract information from each search engine"s result pages. Each tool extracted the result"s title, URL, city, state, zip, address, telephone number, snippet, reviews, and hours of operation from the result"s page and stored it in our database. Some result pages did not include all of this information; in those cases we extracted only the available information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Filtering</head><p>To help avoid returning irrelevant resources as results, we filtered 3rd party pages (e.g. Wikipedia) by removing any result whose URL that was not rated in Yelp or Google Places. We also removed "under construction" and "coming soon" pages by removing pages that did not contain at least one link to another page on the same domain. To help avoid returning duplicate results, we combined results from different search engines that had the same title, URL, or phone number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Context</head><p>We handled the location aspect of the context by including the location in the query issued to our search engines. We specified the desired location in the search engine"s location field, where applicable (e.g., Yelp). For engines that do not have a separate location field, we appended the location to the query. Each resource"s hours of operations were used to handle the time and day aspects of the context. For resources in our list of famous sites, we assumed that theaters, operas, and concert venues were open in the evenings, and we assumed everything else was open in the mornings and afternoon. In cases where no hours of operation were available for a resource and the resource was not a famous site, we assumed the resource was closed at the specified time and did not return it.</p><p>We handled the season aspect of the context by ignoring a category, boosting a category, or doing nothing. This approach is based on the ideas that some types of activities are undesirable in the winter (i.e., outside activities) and that some types of activities are more desirable during the warmer seasons (i.e., restaurants and bars that may have outside seating). We did not account for locations with different weather patterns, such as locations where outside activities would be comfortable all year. The table below shows the action take for each season, category pair. We did not alter the ranking for season, category pairs not shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seasons</head><p>Action Categories winter ignore amusement parks, botanical gardens, cultural districts, farmers market, landmarks, landmarks and historical buildings, mini golf, tours, walking tours, zoos, shopping, shopping centres, and parks spring, summer, and fall boost cafes, restaurants, bars, dim sum, dive bars, irish, mediterranean, mexican, pubs, tea rooms, and vegetarian</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Resource Merging</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning to Rank</head><p>After unwanted resources were filtered and duplicate resources were combined, we used learning to rank to create a result set for each context. To do so, we used SVMRank <ref type="bibr" coords="4,245.43,273.33,11.69,8.96" target="#b2">[3]</ref> to rank a list of resources for each query/category. We biased this ranking towards Google"s ranking by giving Google"s ranking a weight of 0.4 and our ranking a weight of 0.6, so that popular results and famous sites were weighted more heavily. We trained SVMRank using the results from the TREC example context (Toronto). We ranked the example results by their popularity, which was computed as the ratio of final 1"s to final -1"s in the profiles. We also experimented with computing popularity using only initial ratings and using a combination of both ratings.</p><p>For each example result we used the following features, which were based on those used in <ref type="bibr" coords="4,427.44,342.33,11.04,8.96" target="#b0">[1]</ref>: the average rating across all search engines; the total number of reviews across all search engines; the mean reciprocal rank across all search engines; the average percentage of query terms appearing in the title and snippet across all search engines; the maximum percentage percentage of query terms appearing in the title and snippet; a boolean indicating whether the sentiment expressed in the example"s reviews is mostly positive or negative, determined by using SentiWordNet <ref type="bibr" coords="4,408.91,388.29,11.60,8.96" target="#b1">[2]</ref> to look up terms; the sum of the IDF of query terms found in reviews; the sum of the IDF of query terms found in snippets; the total size in words of the reviews; the average size of the title in words across all search engines; the average size of the snippet in words across all search engines; the number of slashes in the URL; the length of the URL in characters; the sum of the BM25 similarity between the query and example"s titles; the sum of the BM25 similarity between the query and example"s snippets; and a boolean for each search engine indicating whether the example was found by the search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Personalizing the Learning to Rank Results</head><p>Once we had a ranked list of results for each category, we built the merged result set 10 results at a time. Our goal was to include a variety of results from categories that the user liked. For example, if a user gave high scores to only spas, restaurants, and bars, each set of 10 results returned for the user should contain only a mixture of spas, restaurants, and shopping centers. To do this we calculated a score for each example result for each user as described in Profile Creation. We combined these scores into per-user category scores by summing the scores for every resource in each category. Categories with a non-positive score were ignored. In each set of 10 results, the number of results from a category was proportional to the category"s score with regard to the other categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Boosting Well-Known Attractions</head><p>We used city profiles to boost well-known attractions to the top of each set of 10 results that we returned. To do so, we added well-known attractions from each category which had a positive user profile score. The well-known attractions which were added to the result set were ranked by their ranking in the city profile. To avoid overwhelming the result set with well-known attractions for locations that have many of them (e.g., New York), we placed constraints on how many well-known attractions could appear in each set of 10 results. This approach allowed us to heavily weight well-known attractions in the first 20 results while still keeping other types of resources. This approach allowed us to add well-known attractions that were from categories that the user liked while keeping the user"s result set from being entirely determined by the user profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Result Set Size</head><p>For some contexts it was difficult to produce 50 results given the constraints imposed by the user"s profile, the season, and the desired time of day. In cases where it was difficult to compile 50 results that satisfied all our constraints, we slowly relaxed constraints until we had enough results. We first relaxed the season and time of day constraints. That is, we did not filter the results based on the season or based on the result"s operating hours. If we still did not have 50 results after relaxing those constraints, we removed the user"s profile constraint and replaced it with a general profile constraint. The general profile was computed from all profiles as described in Profile Creation. We also experimented with smoothing the user profile scores using the general profile scores and a city profile score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Automatic Natural Language Description Generation</head><p>Our goal was to use descriptions that talked about the resource favorably, explained what the resource was, and gave the reader some idea of what could be expected at the resource. We chose each result"s description from the favorable reviews on Yelp and Google Places. We used SVMRank <ref type="bibr" coords="5,239.88,269.73,11.69,8.96" target="#b2">[3]</ref> to rank the favorable reviews and selected the highest-ranked review as the result"s description. We manually ranked descriptions from a test context not included in contexts.txt (Pittsburgh, PA) to create a training set.</p><p>The features we used were the review"s: word count, character count, percentage of characters that are capital letters, percentage of characters that are numbers, percentage of characters that are not alphanumeric, percentage of words that come from the category"s language model, BM25 similarity between the description and the category, and a boolean indicating the sentiment of the review as determined by SentiWordNet <ref type="bibr" coords="5,281.52,350.25,10.81,8.96" target="#b1">[2]</ref>.</p><p>In order to ensure that each resource"s description included a reference to the resource"s category, we added a sentence to the beginning of the description in the format. This sentence was generated from the resource"s name, a list of favorable adjectives, and the resource"s category. The structure of the sentence varied with the resource"s category, so that the sentence was well-formed for each category. For example, for the "music venue" category sentences were constructed with this structure: "(result name) is (a favorable adjective) (the result"s category)," such as "Lincoln Center is a good music venue." Each category was mapped either to this rule or to another rule, which was: "(result name) is (a favorable adjective) place for (the result"s category)."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Results</head><p>We submitted two runs for judgment in this track: GUINIT and GUFINAL. Both used judgments to find the micro-level interests as outlined above. GUINIT used the initial judgments of a person in the sample profile based on the description of a suggestion. GUFINAL used the final judgments which were given after the person visited the website of the suggestion. We chose these runs because they contained more famous attractions than runs that used macro-level interests and runs that used different user profile weights. Suggestions were judged based on four criteria: geographic appropriateness, temporal appropriateness, interestingness of the website, and interestingness of the description. Geographic and temporal judgments were made by NIST assessors, while description and website interestingness judgments were made by the original pool of students who judged the example suggestions. Judgments were made on a 0 to 2 scale, with 0 being not appropriate/interesting, and 1 being partially appropriate/interesting, and 2 being completely appropriate/interesting. NIST then calculated metrics of MRR and P@5 for different combinations of judgments, with a suggestion only being relevant if it received a 2 for all judgments in the combination. NIST also provided best, worst, and median results for all runs for P@5 of Website, Place and Time, and Website, Place and Time combinations. Based on these results we see that our approach achieved some of the best results for the geotemporal judgments. As seen in Figure <ref type="figure" coords="5,397.16,626.30,3.76,8.96">3</ref>, our mean scores when taken over the profiles were all above the median, and many times were the best. Figure <ref type="figure" coords="5,401.67,637.60,4.98,9.05">2</ref> shows that our system"s Website recommendation performance is not as clear. We did do well, with most of our results being above the median, but we were not as consistently at the top of the ratings. However, upon inspection it seems to show that GUFINAL did better for website judgments, which would be expected because its personal profile scores were calculated based on people"s rating of suggestion sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We aggregated results from many Web search engines to produce results. This approach allowed us to take advantage of structured contextual information on the Web and thus worked well for the context evaluation. Utilizing multiple search engines also gives enough varied results that we were usually able to find appropriate results for the contexts. We cannot yet tell how well it performed for the profile evaluation, but our hope is that the mixture of personal, general, and city profiles allowed us to personalize the results for each user while avoiding "overfitting" the personal profiles by utilizing general profiles and city-specific profiles. By incorporating general profiles we were able to model general users interests in cases where not enough information was available to produce a full result set for the user profile. Finally, we were able to provide balanced rankings by using micro-level interests, which avoid favoring rare categories (e.g., spas) as macro-level interests do. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,185.30,209.15,32.19,8.10;6,409.87,209.15,32.19,8.10;6,96.90,217.75,204.75,124.50"><head>Figure 2 Figure 3 Figure 4 Figure 5 Figure 5</head><label>23455</label><figDesc>Figure 2Figure3</figDesc><graphic coords="6,96.90,217.75,204.75,124.50" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,57.77,614.78,500.23,8.96;6,72.02,626.30,328.93,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,215.21,614.78,259.55,8.96">Learning to Aggregate Vertical Results into Web Search Results</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,492.82,614.78,65.17,8.96;6,72.02,626.30,299.19,8.96">Proc of the 20th ACM Conference on Information and Knowledge Management (CIKM&apos;11)</title>
		<meeting>of the 20th ACM Conference on Information and Knowledge Management (CIKM&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,57.77,637.70,499.93,8.96;6,72.02,649.12,422.98,9.06" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,251.66,637.70,306.03,8.96;6,72.02,649.22,79.44,8.96">SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,169.34,649.12,295.72,9.06">Proc. Of the 7th Conf. on Language Resources and Evaluation (LREC&quot;10)</title>
		<meeting>Of the 7th Conf. on Language Resources and Evaluation (LREC&quot;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,57.77,660.74,500.04,8.96;6,72.02,672.26,101.17,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,127.94,660.74,157.75,8.96">Training Linear SVMs in Linear Time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,306.39,660.74,251.41,8.96;6,72.02,672.26,71.12,8.96">Proc. of the ACM Conf. on Knowledge Discovery and Data Mining (KDD&quot;06)</title>
		<meeting>of the ACM Conf. on Knowledge Discovery and Data Mining (KDD&quot;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,57.77,683.68,500.22,9.06;6,72.02,695.19,159.68,9.06" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,385.95,683.68,68.05,9.05">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,484.92,683.68,73.06,9.06;6,72.02,695.19,129.94,9.06">Proc. of the Third Text REtrieval Conf. (TREC&quot;94)</title>
		<meeting>of the Third Text REtrieval Conf. (TREC&quot;94)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
