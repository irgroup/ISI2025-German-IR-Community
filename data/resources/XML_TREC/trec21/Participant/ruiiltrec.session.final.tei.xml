<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.74,61.03,290.02,13.91">Rutgers at the TREC 2012 Session Track</title>
				<funder ref="#_jxgNrSz">
					<orgName type="full">IMLS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.63,95.74,49.08,10.54"><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.62,95.74,62.84,10.54"><forename type="first">Michael</forename><surname>Cole</surname></persName>
							<email>m.cole@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.60,95.74,44.99,10.54"><forename type="first">Eun</forename><surname>Baik</surname></persName>
							<email>eunjungbaik@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.92,95.74,88.32,10.54"><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
							<email>belkin@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.74,61.03,290.02,13.91">Rutgers at the TREC 2012 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">823632261AF1694F8F093E2FED709ECB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At Rutgers, we approached the Session Track task as an issue of personalization, based on both the behaviors exhibited by the searcher during the course of an information seeking episode, and a classification of the task that led the person to engage in information-seeking behavior. Our general approach is described in detail at the Web site of our project (http://comminfo.rutgers.edu/imls/poodle) and in the papers available there. In the TREC 2011 Session Track, we tested preliminary results of predictive models of document usefulness using recursive partitioning models learned from user studies of task session information behaviors. In this year's TREC Session Track, we tested predictive models of document usefulness based on user behaviors by using logistic regression. This was combined with predictive models of task type derived from a multinomial logistic regression model learned from the 2012 Session Track data.</p><p>After an overview of our approach we provide details of how we actually did things, our results, and our conclusions about the results. The Session Track tasks were addressed by first classifying the track task sessions into four types of tasks, using the scheme and method described in section 2. This classification was based on the Session topic descriptions and narratives. Task classification was performed both manually and automatically. We distinguish the two in our results submissions as RutgersHu (human) and RutgersM (machine). The task classifications were used in our experimental runs, where the document usefulness prediction model depended on the identified search task type along with the observed search behaviors. Since the Session Track data did not allow us to incorporate evidence from behaviors on content pages, we used only data associated with SERPs and various temporal characteristics, such as dwell time on content pages, and time between queries (section 3 describes the models and data in detail). The predicted useful documents were then used to modify the last query but one in each search session using the useful documents to supply terms in a standard relevance feedback mode using the Lemur system in remote mode (section 4 describes our methods in detail). Unlike our work in the 2011 Session Track, this year we used only positive feedback in the query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The task classification scheme and method</head><p>There are several ways to conceptualize search tasks. <ref type="bibr" coords="1,314.83,639.34,92.99,10.54" target="#b1">Li &amp; Belkin (2008)</ref> proposed a holistic faceted approach which features fifteen essential facets. <ref type="bibr" coords="1,289.55,655.18,77.97,10.54" target="#b2">Liu et al. (2010)</ref> and <ref type="bibr" coords="1,390.84,655.18,141.98,10.54;1,56.64,671.02,31.99,10.54" target="#b4">Liu, Belkin, Cole &amp; Gwizdka (2011)</ref> identified several additional facets to extend Li &amp; Bekin's classification scheme. <ref type="bibr" coords="1,481.50,671.02,67.00,10.54;1,56.64,686.86,57.66,10.54" target="#b3">Liu, Belkin &amp; Cole (2012)</ref> present the task facets controlled during our user experiments on behavior-based prediction of document usefulness and some results using those predictive models on TREC Session Track 2011 data. The specific task facets controlled were the Product, Goal, Complexity, Level, and Naming of the search tasks. The other task facets identified in <ref type="bibr" coords="2,356.55,59.74,92.99,10.54" target="#b1">Li &amp; Belkin (2008)</ref> were not manipulated, including Source of task; Task doer; Time (length) Process; Goal (quantity); Interdependence; and Urgency. We generated specific predictive models of document usefulness for each task type, and then compared the differences among these specific models to examine the task facet effect on the specific models of document usefulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Tasks classification for TREC</head><p>Our results identified two task facets that influenced the predictors and predictive rules: "Product" and "Goal quality". These two facets were used to classify the TREC 2012 Session Track topics. The "Product" facet has three values: intellectual, factual and image. Given the nature of the data in the Session Track, we used just the intellectual and factual values to provide the controls in the search tasks. The difference between Intellectual and Factual tasks is that Intellectual tasks produce new ideas or findings (e.g. learn about a topic or make decision based on information collected), while Factual tasks only involve locating facts, data and other information items.</p><p>The "Goal quality" facet has two values: specific goal(s) and amorphous goal(s). Goal quality is very similar to the dimension that <ref type="bibr" coords="2,196.95,305.02,138.29,10.54">Ingwersen &amp; JÃ¤rvelin (2005)</ref> proposed as 'well-defined' and 'ill-defined' information need. Tasks with specific goals have well-defined information needs, while in tasks with amorphous goals, the information need is ill-defined. Tasks with an amorphous goal might require users to redefine the topic or identify specific aspects of the subject themselves. Note that we simplify here in terms of facet values; for goal quality, in particular, the values we have specified are more properly viewed as the poles of a dimension of specificity or clarity in the searcher's understanding of the goal, than as a binary distinction. Using these two facets, we would have four types of tasks, as shown in Table <ref type="table" coords="2,133.63,415.90,4.50,10.54" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Manual classification of tasks types for TREC</head><p>The task types of 98 sessions were manually classified by two doctoral students independently according to the classification scheme introduced above. An initial classification was produced after the two coders compared notes and discussed to reach an agreement. A third coder (faculty) confirmed and made minor revisions to the discussion results, which were agreed upon by all three coders. The final manual classification of task types for TREC 2012 Session Track is presented in Table <ref type="table" coords="2,475.89,695.26,4.50,10.54" target="#tab_2">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Automatic classification of tasks types for TREC</head><p>In our previous studies, we generated predictive models of task type using behavioral measures during the search sessions and after the search sessions are completed. In the automatic classification of task types for TREC 2012 Session Track, we simply applied the predictive models of task types as learned from our previous user experiment. The task descriptions in that user experiment were presented in <ref type="bibr" coords="3,535.15,270.94,19.66,10.54;3,56.64,287.02,113.98,10.54" target="#b3">Liu, Belkin, and Cole (2012)</ref>, and the four task types can be labeled as the four task types we described in Table <ref type="table" coords="3,86.96,302.62,4.50,10.54" target="#tab_0">1</ref>. Task type classification 1. The predictive models of task types are listed below:</p><formula xml:id="formula_0" coords="3,67.32,327.28,477.14,259.78">!""#$!%&amp;'()* = !" !"#$%$&amp;'&amp;()#!"#$%&amp; !"#$%$&amp;'()#*()+,- = -1.91 -0.061 * !"#$. !"#$%#$. !"#$$. !"#$!% + 0.051 * !"#$%&amp;. !"#$%#$ -0.004 * !"#$. !"#$%&amp;'("). !"#$ + 0.277 * !"#$%&amp;. !"#$ + 0.719 * !"#$%&amp;. !"#$. !"#. !"!"# !""#$!%&amp;'()* = !" !"#$%$&amp;'&amp;()#*()+,- !"#$%$&amp;'()#*()+,- = -0.769 -0.255 * !"#$. !"#$%#$. !"#$$. !"#$!% -0.021 * !"#$%&amp;. !"#$%#$ + 0.005 * !"#$. !"#$%&amp;'("). !"#$ + 0.132 * !"#$%&amp;. !"#$ + 0.662 * !"#$%&amp;. !"#$. !"#. !"#$% !""#$!%&amp;'()* = !" !"#$%$&amp;'&amp;()#*()+,- !"#$%$&amp;'()#*()+,- = 0.2403 -0.073 * !"#$. !"#$%#$. !"#$$. !"#$!% + 0.08 * !"#$%&amp;. !!"#$"# -0.002 * !"#$. !"#$%&amp;'("). !"#$ + 0.021 * !"#$%&amp;. !"#$ -0.661 * !"#$%&amp;. !"#$. !"#. !"#$% odds for type A =1 -!""#$!%&amp;'()* -!""#$!%&amp;'()* -!""#$!%&amp;'()*</formula><p>These predictive models provided an automatic classification of task types for TREC 2012 Session Track as presented in Table <ref type="table" coords="3,192.24,616.30,4.50,10.54">3</ref>. The results show that only two types of tasks were identified by our model: Type A and Type C. Among 98 sessions, 60 were predicted as type A, and 38 were predicted as type C. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The prediction models of document usefulness</head><p>In this section we provide a description of how we arrived at the models that were used for prediction of document usefulness, and a specification of the models themselves.</p><p>Our group implemented implicit relevance feedback to personalize search result content. In particular, we used several prediction models generated from our previous studies to predict the usefulness of the returned documents. This was accomplished through analysis of users' interactions during their search sessions, considering task type as a contextual factor.</p><p>RL1 is our baseline run, which used Pseudo Relevance Feedback on the last query issued by users in each We used the default parameters for the Indri Retrieval System, as follows:</p><p>Parameters for Pseudo Relevance Feedback (RL1)</p><p>int fbDocs = _param.get( "fbDocs" , 10 ); int fbTerms = _param.get( "fbTerms" , 10 ); double fbOrigWt = _param.get( "fbOrigWeight", 0.5 ); double mu = _param.get( "fbMu", 0 );</p><p>In RL4, we considered all user interactions available in the log, and used those that were also variables in the prediction models. In Liu, Belkin, Cole and Gwizdka (2011), we examined multiple user interactions on both content pages and search result pages, with respect to document usefulness and task type, and generated several prediction models of document usefulness. Our results demonstrated that combining multiple behaviors on content pages and search result pages can improve the prediction of useful documents. In addition, the specific prediction models for each type of task demonstrated improved prediction results.</p><p>User behavioral measures in our prediction models include:</p><p>â¢ dwell time on content pages,</p><p>â¢ number of times a page has been visited in one search episode (visit_id),</p><p>â¢ number of mouse clicks and number of keyboard activities on content pages,</p><p>â¢ the total number of content pages visited during that query interval (content_count);</p><p>â¢ the total dwell time on content pages during that query interval (content_sum);</p><p>â¢ the total dwell time on SERPs during that query interval (serp_sum), and</p><p>â¢ the average dwell time on each SERP during that query interval (serp_mean).</p><p>Among these behavioral measures, users' interactions on content pages (i.e. number of mouse movements and keyboard activities) are not available in the interaction log of Session Track. Therefore, our submission for RL4 was based on only the available variables in the Session data.</p><p>The specific models we used are as follows.</p><p>Type A: Known-item search</p><formula xml:id="formula_1" coords="5,56.64,178.00,475.70,176.48">!" ! 1 -! = -1.59 + 0.03 * !"#$$%&amp;'# + 0.07 * !"#"$. !" Type B: Known-subject search !" ! 1 -! = -2.41 + 0.08 * !"!""#$%! + 0.84 * !"#"$. !" Type C: Interpretive search !" ! !!! = -0.76 + 0.06 * !"#$$%&amp;'# + 0.07 content_count -0.005 * content_sum Type D: Exploratory search !" ! 1 -! = -1.73 + 0.05 * !"#$$%&amp;'# + 0.76 * !"#"$. !" -0.04 * !!"#. !"#$ + 0.02 * !"#$. !"#</formula><p>These models predict document usefulness. From those documents we selected the most informative terms and used them for query expansion. For both of the RL4 runs, the expanded query terms from the prediction models were added to the penultimate (last-1) queries. The reason for this is that the logs do not contain the user interactions that followed the last query, and so our behavior-based models could not be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Queries and runs</head><p>RL1 is our baseline run, which used Pseudo Relevance Feedback on the last queries users issued in each session. We used the default parameters in Indri Retrieval System, as described in section 3.</p><p>For RL4 in our two submissions, we performed Positive Relevance Feedback using the document usefulness prediction results. This was accomplished by taking the predicted useful documents and calculating the term frequency for each term in the corpus of useful documents for that task session. Each term frequency was then discounted by the prior expectation of the appearance of the term, using the Brown corpus as the English language frequency reference. The terms were stopped using the SMART project stopwords but not stemmed. The top 25 terms in the resulting ranking were used to expand the last-1 query in the session. If the session contained no clicked documents or had only nonuseful documents clicked, then we did not conduct any relevance feedback, which means no change to users' original queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Average performance of models over all performance measures</head><p>We start by comparing the average of the RutgersHu (human-assigned task types) runs with the RutgersM (machine-assigned task types) against one another and the baseline run (RL1). On average, both methods performed better on the mean of all the measures than the baseline run. The RutgersHu method performed somewhat better than RutgerM.</p><p>Mean of all measures: RL1 =0.1329 RutgersHu=0.1711 RutgersM=0.1701</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model performance by evaluation measures</head><p>We then compared the improvement of our models (RutgersHu and RutgersM) with our Baseline (RL1) on all the measures. Table <ref type="table" coords="6,217.59,237.34,6.00,10.54" target="#tab_4">4</ref> shows that both the models performed better than the baseline for err and normalized. In the case of normalized DCG, our models performed better for the top ten (44.10% and 30.42%), but worse for nDCG over all ranks by RutgersHu model (-1.21%). In terms of average precision (ap), our models performed better for the top ten (38.5% and 38.56%), but did not improve much for average precision over all ranks (3.11% and 12.64%). The Human and Machine models had similar performance gains/losses against the baseline run for all performance measures. This provides evidence that the task type assignment model disagreements with the human-assigned task types do not result in materially less effective performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to 2011 Session Track performance</head><p>Recall that our work this year extended the positive feedback models used in last year's Session Track, in particular they were a test of the document usefulness prediction models that were specialized to task type. As shown in Table <ref type="table" coords="6,176.62,660.46,4.50,10.54" target="#tab_5">5</ref>, compared to the 2011 Session Track, the baseline run performance was about the same for many measures, except for ndcg@10 and average precision where this year (2012)'s result were quite a bit better. Generally our models this year performed about the same as the runs last year, including average precision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison by task type</head><p>Since we used task-specific prediction models of document usefulness for implicit relevance feedback, we are interested in the retrieval performance in each type of task. Table <ref type="table" coords="7,407.15,265.66,6.00,10.54" target="#tab_6">6</ref> compares retrieval performance between the baseline and our manual model and the improvement of our manual model over the baseline by task type. First of all, it is shown that our manual model improved over the baseline on nearly all the measures of the retrieval performance, except nDCG and average precision (ap). This is similar to our general results.</p><p>Secondly, we compared the retrieval performance by the baseline model (RL1) among different types of tasks. The results show that type D (Exploratory search) tasks achieved best performance improvement over the baseline, followed by type A (Known-item search) and type B (Known-subject search). Type C (Interpretive search) had the worst performance in the baseline model, which used pseudo relevance feedback technique to refine users' queries. This result indicates that pseudo relevance feedback may not be a good technique for Interpretive search (type C) tasks.</p><p>Thirdly, we found the amount of improvement was different for different types of tasks. Our model achieved the greatest improvements over the baseline for task type C, Interpretive search (Intellectual search with specific goals), on some measures (err, err@10, nerr, nerr@10), and the improvement was even greater than 100%. Our model also achieved much improvement over the baseline in task type D (about 50%), and in task type B (about 30%). As we just pointed out, type D tasks also had the best performance in the baseline run (RL1), and our behavioral model improved the retrieval results even more, and the absolute retrieval performance was best in type D tasks among all tasks. Comparatively, our model achieved least improvement for task type A, Known-item search (Factual search with specific goals), with improvement less than 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our baseline run used Indri's default pseudo-relevance feedback on the penultimate query in each session. The experimental runs used our document usefulness prediction models that were developed from our PoODLE user studies of information behaviors when users engage different types of tasks. This year we applied the logistic regression predictive models on the new interaction sessions, which had much more diversity in task types as compared to last year. We expected to see improvements in the performance of the models over baseline and different retrieval performance in different types of tasks.</p><p>Generally, however the performance improvements over baseline were rather similar to last year's results. The baseline performance was a bit better this year than last. The 2011 Session Track baseline run was rather low when compared to the other systems. This year the baseline run was at the median of the runs in the Session Track. Last year the specific model did not really face a classification problem as the interaction sessions were overwhelmingly of a single type, and we found that our model generally improved our baseline. This year there was greater diversity and the performance of the models was also shown to improve the baseline, as measured by nearly all the measures of retrieval performance, except nDCG and Average Precision on all ranks. This is encouraging from the perspective that the performance improvement of our document usefulness prediction models is confirmed with a new, more diverse interaction data set. Even though the baseline performance was better this year we saw roughly the same absolute gains as last year, so this is an encouraging sign that our technique will provide a performance boost to any general retrieval engine. It is important to note that the usefulness prediction models were used as input to a simple relevance feedback technique. The models can be used in more sophisticated ways that could result in greater improvements in overall system performance.</p><p>More interestingly, we found a task type effect on the retrieval performance by implementing our predictive models. As shown in our results, our model achieved most improvement over the baseline in task type C, the Interpretive Search, Intellectual search with specific goals; while our model achieved least improvement over the baseline in task type A, the Known-item Search, Factual search with specific goals. But we also notice that the baseline for Interpretive search (task type C) was much lower than that in other task types. This may indicate that the pseudo relevance feedback could not work well for Interpretive search, and our model was able to improve the retrieval performance greatly. However, our model did not have much improvement over the baseline in Known-item search (task type A). This may indicate that our model could work as well as pseudo relevance feedback in Known-item search type of tasks. We also found that even though the baseline for task type D, Exploratory search, was the best compared with other types of tasks, our model was still able to improve the baseline by about 50%. Therefore, it is very important to implement our model rather than pseudo relevance feedback (the baseline), especially for tasks that are not Known-item search type of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our results have shown that the task-specific document usefulness prediction models which were developed from radically different search sessions than those represented in the TREC Session Track, nevertheless led to consistently improved performance over a reasonable baseline that did not take account of session-level information. We also found our model was able to improve retrieval performance over pseudo relevance feedback in task types that are not Known-item search. Since the current search systems work best for Known-item search, our results leads us to believe that it is very important to detect the type of tasks users are engaging in when using search systems, and the models we have developed could be used for personalization of retrieval with more practical value.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,56.64,441.58,330.04,107.26"><head>Table 1 . Task type classification Goal Quality (specific vs. amorphous) Specific Amorphous Product (factual vs. intellectual) Factual Type A: Known-item search Factual</head><label>1</label><figDesc></figDesc><table coords="2,262.32,524.38,86.65,24.46"><row><cell>tasks with</cell></row><row><cell>specific goal(s)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,193.68,496.78,361.30,107.74"><head>Type B: Known-subject search Factual</head><label></label><figDesc></figDesc><table coords="2,193.68,510.70,361.30,93.82"><row><cell></cell><cell>tasks with amorphous</cell></row><row><cell></cell><cell>goal(s)</cell></row><row><cell>Intellectual Type C: Interpretive</cell><cell>Type D: Exploratory search</cell></row><row><cell>search</cell><cell>Intellectual tasks with amorphous</cell></row><row><cell>Intellectual tasks with</cell><cell>goal(s)</cell></row><row><cell>specific goal(s)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,56.64,59.50,491.43,136.30"><head>Table 2 .</head><label>2</label><figDesc>Manual classification of tasks types for TREC</figDesc><table coords="3,56.64,86.14,491.43,109.66"><row><cell>Task</cell><cell>Task type</cell><cell>Goal(quality)</cell><cell>Product</cell><cell>number of</cell><cell>number of</cell></row><row><cell>type</cell><cell></cell><cell></cell><cell></cell><cell>topics</cell><cell>sessions</cell></row><row><cell>A</cell><cell cols="2">Known-item search Specific</cell><cell>Factual</cell><cell>19</cell><cell>40</cell></row><row><cell>B</cell><cell>Known-subject</cell><cell>Amorphous</cell><cell>Factual</cell><cell>10</cell><cell>18</cell></row><row><cell></cell><cell>search</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C</cell><cell cols="2">Interpretive search Specific</cell><cell>Intellectual</cell><cell>9</cell><cell>17</cell></row><row><cell>D</cell><cell cols="2">Exploratory search Amorphous</cell><cell>Intellectual</cell><cell>10</cell><cell>23</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell>48</cell><cell>98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,591.36,314.09,23.94,21.43"><head>Chan Delete Table 3 .</head><label>Delete3</label><figDesc>Automatic classification of tasks types for TREC</figDesc><table coords="4,35.04,86.14,477.99,95.98"><row><cell cols="2">Task type Task type</cell><cell>Goal(quality)</cell><cell>Product</cell><cell>number of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sessions</cell></row><row><cell>A</cell><cell>Known-item search</cell><cell>Specific</cell><cell>Factual</cell><cell>60</cell></row><row><cell>B</cell><cell>Known-subject search</cell><cell>Amorphous</cell><cell>Factual</cell><cell>0</cell></row><row><cell>C</cell><cell>Interpretive search</cell><cell>Specific</cell><cell>Intellectual</cell><cell>38</cell></row><row><cell>D</cell><cell>Exploratory search</cell><cell>Amorphous</cell><cell>Intellectual</cell><cell>0</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell>98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,58.20,373.90,502.13,213.10"><head>Table 4 .</head><label>4</label><figDesc>Performance by measure (all interaction sessions)</figDesc><table coords="6,58.20,400.30,502.13,186.70"><row><cell></cell><cell>RL1</cell><cell>RutgersHu</cell><cell>RutgersHu (absolute improvemen t over RL1)</cell><cell>RutgersHu (percentage improveme nt over RL1)</cell><cell>RutgersM</cell><cell>RutgersM (absolute improvemen t over RL1)</cell><cell>RutgersM (percentage improveme nt over RL1)</cell></row><row><cell>err</cell><cell>0.0968</cell><cell>0.1246</cell><cell>0.0278</cell><cell>28.74%</cell><cell>0.1112</cell><cell>0.0144</cell><cell>14.91%</cell></row><row><cell>err@10</cell><cell>0.0830</cell><cell>0.1137</cell><cell>0.0307</cell><cell>37.00%</cell><cell>0.0997</cell><cell>0.0167</cell><cell>20.15%</cell></row><row><cell>nerr</cell><cell>0.1605</cell><cell>0.2148</cell><cell>0.0543</cell><cell>33.84%</cell><cell>0.1983</cell><cell>0.0378</cell><cell>23.53%</cell></row><row><cell cols="2">nerr@10 0.1351</cell><cell>0.1947</cell><cell>0.0596</cell><cell>44.10%</cell><cell>0.1762</cell><cell>0.0411</cell><cell>30.42%</cell></row><row><cell>ndcg</cell><cell>0.1947</cell><cell>0.1923</cell><cell>-0.0024</cell><cell>-1.21%</cell><cell>0.2489</cell><cell>0.0542</cell><cell>27.86%</cell></row><row><cell cols="2">ndcg@10 0.1074</cell><cell>0.1606</cell><cell>0.0533</cell><cell>49.65%</cell><cell>0.1505</cell><cell>0.0431</cell><cell>40.19%</cell></row><row><cell>ap</cell><cell>0.0799</cell><cell>0.0824</cell><cell>0.0025</cell><cell>3.11%</cell><cell>0.0900</cell><cell>0.0101</cell><cell>12.64%</cell></row><row><cell>ap@10</cell><cell>0.2062</cell><cell>0.2856</cell><cell>0.0794</cell><cell>38.50%</cell><cell>0.2857</cell><cell>0.0795</cell><cell>38.56%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,51.36,59.50,496.27,162.94"><head>Table 5 .</head><label>5</label><figDesc>Comparison of retrieval performance between this year (2012) and last year(2011)    </figDesc><table coords="7,51.36,85.90,496.27,136.54"><row><cell>RL1</cell><cell>2011</cell><cell cols="2">2012 absolute</cell><cell>absolute</cell><cell>absolute</cell><cell>absolute</cell></row><row><cell>(baseline)</cell><cell></cell><cell></cell><cell>improvement by</cell><cell>improvement by</cell><cell>improvement by</cell><cell>improvement by</cell></row><row><cell></cell><cell></cell><cell></cell><cell>rgpos (2011)</cell><cell>rspos (2011)</cell><cell>RutgersHu (2012)</cell><cell>RutgersM (2012)</cell></row><row><cell>err</cell><cell cols="2">0.1135 0.0968</cell><cell>0.0324</cell><cell>0.0597</cell><cell>0.0278</cell><cell>0.0144</cell></row><row><cell>err@10</cell><cell cols="2">0.0990 0.0830</cell><cell>0.041</cell><cell>0.0673</cell><cell>0.0307</cell><cell>0.0167</cell></row><row><cell>nerr</cell><cell cols="2">0.1730 0.1605</cell><cell>0.0565</cell><cell>0.0896</cell><cell>0.0543</cell><cell>0.0378</cell></row><row><cell>nerr@10</cell><cell cols="2">0.1482 0.1351</cell><cell>0.0717</cell><cell>0.1039</cell><cell>0.0596</cell><cell>0.0411</cell></row><row><cell>ndcg</cell><cell cols="2">0.2824 0.1947</cell><cell>-0.0922</cell><cell>-0.0780</cell><cell>-0.0024</cell><cell>0.0542</cell></row><row><cell cols="3">ndcg@10 0.1069 0.1074</cell><cell>0.0469</cell><cell>0.0739</cell><cell>0.0533</cell><cell>0.0431</cell></row><row><cell>ap</cell><cell cols="2">0.0731 0.0799</cell><cell>-0.0023</cell><cell>-0.0031</cell><cell>0.0025</cell><cell>0.0101</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,56.64,323.02,497.32,231.83"><head>Table 6 .</head><label>6</label><figDesc>Comparison of retrieval performance between baseline and our manual model by task type</figDesc><table coords="7,56.64,349.17,497.32,205.68"><row><cell></cell><cell cols="2">A (N=40)</cell><cell></cell><cell>B (N=18)</cell><cell></cell><cell></cell><cell cols="2">C (N=17)</cell><cell></cell><cell>D (N=23)</cell><cell></cell></row><row><cell></cell><cell>RL1</cell><cell>Hu.</cell><cell>impro</cell><cell>RL1</cell><cell>Hu.</cell><cell>impro</cell><cell>RL1</cell><cell>Hu.</cell><cell>improv</cell><cell>RL1</cell><cell>Hu.</cell><cell>impro</cell></row><row><cell></cell><cell></cell><cell>RL4</cell><cell>veme</cell><cell></cell><cell>RL4</cell><cell>veme</cell><cell></cell><cell>RL4</cell><cell>ement</cell><cell></cell><cell>RL4</cell><cell>veme</cell></row><row><cell></cell><cell></cell><cell></cell><cell>nt</cell><cell></cell><cell></cell><cell>nt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nt</cell></row><row><cell>err</cell><cell>0.100</cell><cell cols="2">0.101 1%</cell><cell>0.081</cell><cell>0.097</cell><cell>20%</cell><cell cols="2">0.058 0.100</cell><cell>74%</cell><cell>0.134</cell><cell>0.209</cell><cell>56%</cell></row><row><cell>err@10</cell><cell>0.086</cell><cell cols="2">0.089 3%</cell><cell>0.068</cell><cell>0.086</cell><cell>26%</cell><cell cols="2">0.041 0.089</cell><cell>117%</cell><cell>0.122</cell><cell>0.201</cell><cell>65%</cell></row><row><cell>nerr</cell><cell>0.164</cell><cell cols="2">0.172 5%</cell><cell>0.160</cell><cell>0.212</cell><cell>32%</cell><cell cols="2">0.102 0.163</cell><cell>59%</cell><cell>0.200</cell><cell>0.336</cell><cell>68%</cell></row><row><cell>nerr@10</cell><cell>0.137</cell><cell cols="2">0.151 10%</cell><cell>0.135</cell><cell>0.189</cell><cell>40%</cell><cell cols="2">0.073 0.142</cell><cell>94%</cell><cell>0.179</cell><cell>0.320</cell><cell>78%</cell></row><row><cell>ndcg</cell><cell>0.219</cell><cell cols="3">0.192 -12% 0.155</cell><cell>0.182</cell><cell>17%</cell><cell cols="2">0.134 0.123</cell><cell>-9%</cell><cell>0.230</cell><cell>0.254</cell><cell>10%</cell></row><row><cell cols="2">ndcg@10 0.109</cell><cell cols="2">0.130 20%</cell><cell>0.097</cell><cell>0.156</cell><cell>62%</cell><cell cols="2">0.043 0.102</cell><cell>139%</cell><cell>0.164</cell><cell>0.265</cell><cell>62%</cell></row><row><cell>ap</cell><cell>0.091</cell><cell cols="3">0.074 -18% 0.055</cell><cell>0.079</cell><cell>43%</cell><cell cols="2">0.042 0.046</cell><cell>9%</cell><cell>0.109</cell><cell>0.128</cell><cell>18%</cell></row><row><cell>ap@10</cell><cell>0.225</cell><cell cols="2">0.235 4%</cell><cell>0.194</cell><cell>0.267</cell><cell>37%</cell><cell cols="2">0.100 0.247</cell><cell>147%</cell><cell>0.264</cell><cell>0.423</cell><cell>60%</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>of all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>measures</cell><cell>0.141</cell><cell cols="3">0.143 1.6% 0.118</cell><cell>0.159</cell><cell>35%</cell><cell cols="2">0.074 0.126</cell><cell>79%</cell><cell>0.175</cell><cell>0.267</cell><cell>52%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>The research that provided the data for the user models in this work was funded by <rs type="funder">IMLS</rs> grant <rs type="grantNumber">LG-06-07-0105-07</rs>. We thank all of the members of the PoODLE research team, without whose efforts this work could not have been accomplished.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jxgNrSz">
					<idno type="grant-number">LG-06-07-0105-07</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,71.04,393.27,65.25,12.22;9,56.64,417.17,497.80,9.48;9,56.64,431.57,350.00,9.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,429.11,417.17,125.33,9.48;9,56.64,431.57,70.47,9.48">Rutgers&apos; TREC-6 interactive track experience</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>References Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Carballo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Rieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,134.50,431.57,225.11,9.48">Proceedings of the Sixth Text REtrieval Conference</title>
		<meeting>the Sixth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="597" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,56.64,449.09,486.49,9.48;9,56.64,463.73,347.96,9.48;9,56.64,478.37,193.95,9.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,186.74,449.09,294.44,9.48">A faceted approach to conceptualizing tasks in information seeking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<idno type="DOI">=10.1016/j.ipm.2008.07.005</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ipm.2008.07.005" />
	</analytic>
	<monogr>
		<title level="j" coord="9,488.51,449.09,54.62,9.48;9,56.64,463.73,38.12,9.48">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1822" to="1837" />
			<date type="published" when="2008-11">2008. November 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,56.64,495.89,494.05,9.48;9,56.64,510.29,448.37,9.48;9,56.64,524.93,485.79,9.48;9,56.64,539.33,186.65,9.48" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,326.58,495.89,224.11,9.48;9,56.64,510.29,82.70,9.48">Analysis and evaluation of query reformulations in different task types</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gwizdka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,158.37,510.29,346.65,9.48;9,56.64,524.93,101.58,9.48">Proceedings of the 73rd ASIS&amp;T Annual Meeting on Navigating Streams in an Information Ecosystem</title>
		<meeting>the 73rd ASIS&amp;T Annual Meeting on Navigating Streams in an Information Ecosystem<address><addrLine>Silver Springs, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>American Society for Information Science</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,56.64,557.09,497.60,9.48;9,56.64,571.49,473.68,9.48;9,56.64,586.13,322.19,9.48" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,235.91,557.09,318.33,9.48;9,56.64,571.49,36.10,9.48">Personalization of Search Results Using Interaction Behaviors in Search Sessions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,111.95,571.49,418.37,9.48;9,56.64,586.13,145.00,9.48">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;12)</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,56.64,603.82,455.93,10.54;9,56.64,619.66,494.90,10.54;9,56.64,635.50,465.90,10.54;9,56.64,651.10,54.33,10.54" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,305.96,603.82,206.61,10.54;9,56.64,619.66,118.14,10.54">Personalization of Information Retrieval in Different Types of Tasks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gwizdka</surname></persName>
		</author>
		<ptr target="http://select.cs.cmu.edu/meetings/enir2011/papers/liu-belkin-cole-gwizdka.pd" />
	</analytic>
	<monogr>
		<title level="m" coord="9,261.92,619.66,284.45,10.54">Workshop on Enriching Information Retrieval (ENIR 2011)</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07-28">2011. July 28, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
