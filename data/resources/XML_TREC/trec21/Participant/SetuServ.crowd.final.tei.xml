<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,73.26,66.40,465.95,10.54;1,228.40,82.00,155.68,10.54;1,208.26,108.16,195.96,10.54">Skierarchy: Extending the Power of Crowdsourcing Using a Hierarchy of Domain Experts, Crowd and Machine Learning TREC 2012 Crowdsourcing Track Paper</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.76,122.15,78.47,9.48"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
						</author>
						<author>
							<persName coords="1,265.77,122.15,74.18,9.48;1,339.99,119.65,1.74,6.11"><forename type="first">Sanga</forename><surname>Peerreddy</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,364.86,122.15,69.28,9.48;1,434.22,119.65,1.74,6.11"><forename type="first">Prateek</forename><surname>Singhal</surname></persName>
							<email>prateek.singhal@setuserv.com</email>
						</author>
						<title level="a" type="main" coord="1,73.26,66.40,465.95,10.54;1,228.40,82.00,155.68,10.54;1,208.26,108.16,195.96,10.54">Skierarchy: Extending the Power of Crowdsourcing Using a Hierarchy of Domain Experts, Crowd and Machine Learning TREC 2012 Crowdsourcing Track Paper</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5616ABC691F666AB1129DFAE6940F239</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last few years, crowdsourcing has emerged as an effective solution for large-scale 'micro-tasks'. Usually, the micro-tasks that are accomplished using crowdsourcing tend to be those that computers cannot solve very effectively, but are fairly trivial for humans with no specialized training. In this work, we aim to extend the capability of crowdsourcing to tasks that are complex even from a human perspective.</p><p>Towards this objective, we present a novel hierarchical approach involving a small number of domain experts at the top of the hierarchy, a large crowd with generic skills at the intermediate level, and a Machine Learning system serving as a personal assistant to the crowd, at the bottom level. We call this approach Skierarchy, short for Hierarchy of Skills.</p><p>To test the efficacy of the Skierarchy approach, we deployed the model on the TREC 2012 TRAT task, a task we believe is fairly complex compared to typical micro-tasks. In this paper, we present illustrative experiments to demonstrate the utility of each of the layers of our hierarchy. Our experiments on TRAT as well as IRAT show that using an interactive process between the experts and the crowd could significantly reduce the need for redundancy among the crowd, while also enabling a crowd with generic skills to perform tasks that are reserved for specialists. Further, we found from our TRAT experience that both the crowd and the Machine Learning system improve their performance over time as they gain experience on specialized tasks.</p><p>1 CrowdFlower.com has a real-time app that performs image moderation using crowdsourcing at http://crowdflower.com/rtfm/ 4 www.art.sy recommends works of art to art-lovers.</p><p>5 www.lexmachina.com creates a structured knowledge base from unstructured legal dockets.</p><p>6 www.esparklearning.com recommends learning aids such as apps and videos to students based on their learning needs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>Crowdsourcing has recently emerged as a scalable and cost-effective technological alternative to performing tasks that are either too prohibitively expensive to be performed by expert humans or too complex for machines. In this process, a large-scale project is broken down into several repeatable, independent "micro-tasks" that are then posted on the Internet for users across the world to solve on a pay-per-task basis. This process has been highly successful on many tasks such as digitization of books <ref type="bibr" coords="1,248.89,507.27,89.55,8.64" target="#b7">(von Ahn L. M., 2008)</ref>, adult content moderation 1 , etc.</p><p>With the rising success of crowdsourcing, considerable research effort has been invested on improving the workflow processes in order to optimize quality and cost effectiveness of the output. Several novel techniques have been developed to address this issue, such as pre-filtering the crowd by subjecting them to qualification tests, using redundancy to improve accuracy, inserting data with known ground truth to filter out spammers, etc. <ref type="bibr" coords="1,480.54,563.19,59.67,8.64;1,72.24,574.71,21.49,8.64" target="#b1">(Aniket Kittur, 2008)</ref>.</p><p>One question about crowdsourcing that is still not very well researched upon is the degree of complexity of tasks that a crowdsourcing-based solution is capable of solving effectively. Most of the success stories of crowdsourcing are tasks that are hard for a computer to solve, but fairly trivial for an untrained human (e.g.: image annotation tasks, key-word search tasks, finding business locations etc. that are typically found on Amazon Mechanical Turk). It is not clear if crowdsourcing can be extended towards solving more complex problems that data analysts encounter in highly specialized domains such as legal, medical and educational informatics. Some research is just beginning to emerge on breaking complex tasks in specific applications into simpler micro-tasks that are suitable for crowdsourcing <ref type="bibr" coords="1,134.17,676.71,68.15,8.64" target="#b2">(Bernstein, 2010)</ref>.</p><p>In this work, we are primarily interested in exploring the feasibility of extending crowdsourcing-like solutions to sophisticated data analytics problems in specialized domains. Our main contribution is a novel hierarchical approach called "Skierarchy" that utilizes highly specialized domain experts at the top of the hierarchy, who train and supervise the crowd with generic skills sitting at the intermediate level, and a Machine Learning system at the bottom of the hierarchy acting as a personal assistant to the crowd. Although using experts in combination with people with generic skills is a successful technique in other domains (e.g.: Doctors working with Nurse Practitioners in the medical domain), it has not been sufficiently experimented with in the crowdsourcing setting. Also, although some work on combining crowdsourcing with Machine Learning is beginning to appear (Alexander J. <ref type="bibr" coords="2,484.29,146.31,51.62,8.64" target="#b0">Quinn, 2010)</ref>, we believe ours is the first effort that proposes using Machine Learning as personal assistant to the crowd.</p><p>The Text Relevance Assessing Task (TRAT) is an example of a moderately complex task that is typically performed by domain experts<ref type="foot" coords="2,146.64,188.61,3.24,5.69" target="#foot_0">2</ref> , and therefore it forms an ideal testing ground to validate our Skierarchy approach. In the rest of the paper, we first describe the Skierarchy approach, and its application to TRAT and IRAT. Then, we present our internal experiments to demonstrate the effectiveness of various layers in the hierarchy. We then report performance numbers of our system on the official evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Skierarchy Approach</head><p>The Skierarchy approach aims to extend the capabilities of crowdsourcing to solving sophisticated domain specific data analytics tasks that are currently performed by domain experts. Our approach retains the best practices of crowdsourcing technologies but adds additional layers in the workflow in order to accomplish this goal, which we describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and Supervision of the crowds using Domain Experts</head><p>Although crowdsourcing is very cost effective for many data annotation tasks, it has not yet been able to replace highly expensive domain experts in many verticals such as legal, medical and educational informatics. For example, recommendation systems such as Pandora<ref type="foot" coords="2,243.36,368.61,3.24,5.69" target="#foot_1">3</ref> , Art.sy 4 , Lex Machina 5 and eSparkLearning 6 employ a large number of domain experts to curate their respective domain-specific data. We believe the main hindrance to crowdsourcing these tasks is the relative inability of the crowd to understand and interpret domain specific language/data. Clearly, domain experts are indispensible for these tasks. However, we can use them most effectively by moving them to a supervisory role, to design the micro-tasks, and train and support the less skilled crowd. Following this idea, we propose a hierarchical solution where we use a small number of domain experts to train and supervise a large crowd, so that we still retain the ability to scale at low costs.</p><p>The responsibility of domain experts includes (a) breaking down the tasks into generalizable micro-tasks, so that the tasks become relatively "de-skilled", and therefore more amenable to crowdsourcing, (b) training and supervising the crowd actively in performing these micro-tasks, and (c) solving a small number of micro-tasks themselves, that the crowd finds too difficult or confusing. This approach enables us not only to extend the capability of crowdsourcing to more complex domain-specific tasks by letting the domain experts focus on the difficult problems, and shifting the simpler ones to the crowd, but also in improving the quality of the crowd's output over time through training and supervision. Over time, we expect that members of the crowd become experts at specific tasks that are domain independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Assisting the crowd using an Automatic Machine Learning system</head><p>The performance of the crowd is expected to improve under the supervision of domain experts. However, since the domain experts do not micro-manage the crowd on every micro-task, we use Machine Learning as an additional layer in the hierarchy to improve the performance of the crowd. Unlike traditional settings where Machine Learning either replaces humans wholly or partially, we use Machine Learning as a personal assistant to the crowd rather than as an alternative. Our main idea is based on the observation that modern Machine Learning models, although not on par with humans in terms of performance on most tasks, are still rich enough to assist humans by: (a) Emitting prediction scores for each micro-task, which can be used as a surrogate measure for its difficulty. The difficulty measure may in turn be used to (i) allocate larger chunk of time to the more difficult items, (ii) assign the more difficult items to the better performers, and (iii) measure the throughput of each member in the crowd normalized by difficulty of the micro-tasks he/she has solved. The schematic diagram in Figure <ref type="figure" coords="3,210.22,437.61,5.47,9.13" target="#fig_1">1</ref> presents a unified view of the Skierarchy model with its three layers comprising domain experts, crowd and machines. As the figure indicates, the top of the pyramid consists of the domain experts who offer the highest quality but the least cost-effectiveness and scale, while the base comprises Machine Learning that offers lowest quality but the highest scale and cost effectiveness. The crowd is in the intermediate layer offering mid-level quality, costs and scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">"Open--Crowd" vs. "Closed Crowd"</head><p>The gold standard of crowdsourcing industry is to employ what we call "open-crowd", a set of anonymous people from around the world who perform the micro-tasks on a pay-per-task basis, through a web-based platform. This has been a very attractive setting since open-crowd is very elastic (i.e., the size of the crowd can be increased or decreased at a moment's notice), scalable, cheap and available round the clock.</p><p>In contrast to the standard setting, we used the setting of what we refer to as "closed-crowd" in our experiments, where we hire employees in India with generic college background and good reading comprehension skills in English, and bring to them to the office space to work for us full-time on micro-tasks. This setting was necessitated for our initial experiments because our Skierarchy approach requires close interaction between the domain experts and the crowd, although we believe it could be extended in the future to the open-crowd setting as well. Although closed-crowd setting is less elastic, we found several advantages of this setting in our experiments, some of which we list below:</p><p>Fast prototype cycle: Closed crowd is ideal for experimenting and developing a prototype process -it makes the feedback loops short and quick. This is not different from the setting of a manufacturing firm that outsources its mature processes to third parties but performs its own manufacturing during the prototyping stage. Also, it is easier to collect plenty of data about the right way to design a micro-task by observing the crowd in the initial stages. Training, Knowledge sharing and Continuity: We found face-to-face training and feedback to be very effective in bringing the crowd up-to-speed on a complex micro-task. In addition, we allowed the crowd to share tips based on their experience with their peers during the training phase, which they found very useful in improving their skills. The other benefit of training and employing the same crowd for a long period is that the crowd improves over time, and provides higher quality as time progresses.</p><p>Privacy concerns: Although many companies have a pressing need to analyze terabytes of internal data that they generate through their transaction records, customer support logs, etc., they are reluctant to employ open-crowd systems due to the privacy and security concerns inherent in the data. We believe a closed-crowd setting alleviates these concerns to a great extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Text Relevance Assessment Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting and tools</head><p>We employed 5 people with college degrees in India as our "closed-crowd" 7 , and used 1 person with good experience in information analytics, as our domain expert 8 . The crowd was screened at the time of hiring to make sure that they had reasonable reading comprehension skills in English. The expert spent only 1/3 rd the time as each member of the crowd in the entire annotation process. We used the Logistic Classifier from Stanford CoreNLP 9 as our Machine Learning system. We wrote our own parsers to parse the SGML format of the TREC corpus to extract the subset of 18,600 documents needed for our experiments. For UI, we wrote scripts in Excel using VBA, to support user highlighting as well as Machine highlighting. The UI, a snapshot of which is shown in Figure <ref type="figure" coords="4,329.37,534.57,4.01,9.13" target="#fig_2">2</ref>, also has radio buttons to enter ratings, which get automatically recorded into a spreadsheet. Additionally, we inserted items with known ground truth at random, and built real-time alerts as part of our UI in cases where the crowd's annotations deviated from the ground truth. The documents are displayed one at a time to the user along with the topic description on the side. The time a user spent on each document is recorded as the time between two clicks of the "Next Document" button.</p><p>We used annotations on a scale of 1-5 where 5 represents highest relevance and 1 for highest non-relevance. A rating of 3 was assumed to mean that the annotator considered the document as either too confusing or too difficult to judge for relevance.</p><p>7 See Section 3.3 above for the meaning of "closed-crowd".</p><p>8 Please note that our original plan was to hire a high school English teacher but we hired an information analyst instead due to time constraints. We believe that an English teacher would have done an equally good job as a domain expert. 9 http://nlp.stanford.edu/software/corenlp.shtml As we will describe below in more detail, we used an iterative approach to annotations as opposed to a parallel approach (Greg Little, 2010). We used 2 iterations to perform the annotations, which in-effect means each document received at-most two annotations (and at-least one annotation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Description of the Skierarchy Workflow process</head><p>Step 1: Training the Crowd Using the Expert: For each topic, we first used the averaged ranking of TREC8 submission runs as our starting point. The crowd was first asked to annotate the top 20 documents in the ranked list independently, and also highlight passages of text that they deem relevant to the topic. The expert then examined the documents that had disagreements between them and assigned a final rating based on his own judgment. A meeting was held with the crowd in which the expert walked over each document in the disagreement set, examined their passage highlights to understand their reasoning, and explained to them the reasons behind his own judgment. This was considered as an initial training period for the crowd. Upon the completion of the initial training period, the crowd completed annotations for ~300 documents. These documents were checked randomly by the expert to ensure high quality.</p><p>Step 2: Building a Machine Learning Model, and a Curated Keyword Set: The annotation data was then used to train the Machine Learning algorithm, which in this case is a Logistic Regression based binary classifier. For the purposes of training the binary classifiers, we mapped the ratings of 4 and 5 to positive class and the rest to the negative class. The logistic classifier also trains on the passages highlighted by the crowd, treating them simply as additional documents. We also use the 20 words with the highest weights for the positive class, as learned by the classifier and filter them manually using the expert, to generate a curated list of keywords for each topic. The Machine Learning model is then run on the remaining documents on that topic to generate prediction scores in the range [0,1], where a score of `1' indicates that the machine thinks that the document is absolutely relevant to the topic, and a score of `0' indicates the machine's belief that the document is absolutely non-relevant to the topic.</p><p>Step 3: Annotations with the Help of both Machine Learning and the Expert: The documents are then divided into several buckets by binning them based on the Machine Learning scores, and each bucket is provided to a distinct annotator. Typically, the annotators with superior performance (judged during the training period for the topic) were given the buckets with higher scores and the annotators with moderate performance were given the buckets with the lowest scores. The rationale is that the documents with higher scores tend to contain larger proportion of relevant documents, so the annotator needs to be careful to avoid misses, while the ones with lower scores tend to be mostly non-relevant, so a moderate performer will not hurt too much. When the documents are displayed to the annotators, we also highlight any words that match the list of curated keywords for that topic. In this case, the expert annotator only examines the documents assigned a rating of 3 and assigns a final rating. The expert also occasionally inspects a few documents at random to ensure that the crowd is on track.</p><p>Step 4: Automatic Error Correction with Machine Learning: Once the entire set of documents is annotated, we retrain the Machine Learning model using the complete annotated data and generate predictions for all the documents using 10 fold cross-validation. Next, we do a second pass of annotations with the crowd focusing on documents where the Machine Learning scores and annotations are in disagreement. For example, the crowd reexamines the documents with scores 1 and 2 (likely non-relevant) sorted in the descending order by the Machine Learning scores, and documents rated 4 and 5 (likely relevant) sorted in the ascending order of Machine Learning scores. These are the documents that may contain likely annotation errors. We made sure that each documents in this round is assigned to annotator that is different from its original annotator. Also the new annotator has no access to the original annotation to prevent potential biases. We also generate a new set of curated keywords based on the updated ML model and use them for keyword highlighting in this stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Validation Experiments</head><p>In this section, we aim to test whether some of the hypotheses we made in the Skierarchy approach hold true in the TRAT task. Note that we performed these experiments after the official submission since we were not allowed to access TREC ground truth before the submission. We present each hypothesis in the form of a question, and then proceed to answer empirically through our internal experiments. We have presented our results with respect to selfcurated judgments to account for ambiguity in the topic definitions in TRAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does machine assistance boost productivity and quality of the crowd?</head><p>In order to quantify the improvement in productivity and quality as a result of using Machine Learning output, we performed three independent rounds of annotations with the following ML assistance strategies:</p><p>• Top 10 keywords: ML assists by highlighting top 10 keywords learned from training data in a given document, • Curated keywords: ML suggested keywords are curated by the expert for each topic, and are highlighted in a given document, • Curated keywords and prioritization: In addition to curation of keywords and highlighting them, the documents are presented to the crowd in the decreasing order of ML confidence score. Crowd was instructed to expect a higher percentage of relevant documents at the top than at the bottom of the list.</p><p>Above three ML assistance strategies were deployed on 3 sets containing 35 documents each to create 9 simulations. Documents in each set were chosen at random so that the mean and range of ML confidence scores were the same across the 3 sets. Each simulation (i.e. set and ML assistance combination) was routed through 3 annotators according to the following table so that we could control for the crowd bias and productivity levels to some extent. Crowd productivity and quality were measured in each simulation in terms of time per annotation and accuracy respectively. The measurements showed that the crowd productivity increased by 11% when curated key words were highlighted instead of the top 10 keywords. Productivity further increased by 36% when the documents were prioritized by the ML confidence score. In this experiment, gain in quality was marginal (~2%) as the crowd already had high accuracy levels (95%) leaving little room for improvement with keyword-curation or prioritization. In settings when such high accuracy levels are not possible (e.g., when no training or coaching are provided), keywordcuration and prioritization may be able to help with quality as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML Assistance Strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does Expert in the loop improve quality and productivity?</head><p>Expert can add value to the crowd through a) training/coaching, which improves their capability, and b) by completing annotations themselves where the crowd is likely to commit mistakes. Current crowdsourcing techniques focus on achieving the latter through redundancy across the crowd. Here, we aim to compare redundancy with the expert to redundancy across the crowd.</p><p>To test this, we picked a set of 50 random documents for a fairly complex topic. Four annotators and the expert annotated these documents independently on a scale of 1 to 5. Quality is evaluated against TREC relevance judgments, some of which we corrected to comply with the topic descriptions 10 . Following table summarizes the results of our analysis:</p><p>Number of annotations by category and annotator Ann1 Ann2 Ann3 Ann4 Majority Expert TREC Confusing (rating of 3)</p><formula xml:id="formula_0" coords="6,105.60,508.47,371.75,32.64">5 7 8 4 7 0 0 False Positive 1 0 1 2 0 0 1 False Negative 2 2 2 1 2 0 3</formula><p>As can be seen from above table, majority opinion did not help clarify the confusing annotations. This was partly because of ambiguity in the topic description and narrative and high level of comprehension needed to understand some articles. In such cases, most annotators tend to err simultaneously, rendering majority voting ineffective. However, having expert in the loop resulted in resolution of all these errors. Also, the expert was able to annotate tasks within half as much time as the crowd took. This efficiency gain partly offsets the higher cost incurred in hiring experts. In this experiment, we also noticed that the crowd is fairly good at telling when they are not confident. The expert's time can be best spent in such situations. In our TREC annotations, approximately 70% of the expert time was spent in analyzing documents that were confusing to the crowd or documents where the crowd strongly disagreed with ML-based suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What type of performance management can improve quality in crowdsourcing?</head><p>Quality is hard to measure and manage with annotations for two reasons -a) ambiguity in topics &amp; documents leaves significant room for interpretation and subjectivity b) expected outputs are not known upfront, making quality measurable only post-hoc. Accounting for these factors in the performance management process is the key to deliver high quality annotations.</p><p>We developed a performance management process where we ask the crowd to flag the ambiguous tasks, and we hold them accountable only for the quality of the annotations that they were confident about (i.e. non-ambiguous tasks). Further, we also asked each worker to provide a short description that justifies their annotations, and highlight the supporting text in the document. We then asked a second worker to read the description and highlighted text from the first worker, and confirm the annotation. Annotations from the first and second workers were audited by an expert at random. The documents that were marked as ambiguous by the crowd are also routed to expert. This process is illustrated below -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Process Flow</head><p>To test how well this process works, we implemented this process on a set of 165 documents that were confusing to the machine. Quality is evaluated against TREC relevance judgments. Following As can be seen from above table, this process resulted in highly accurate annotations with an overall redundancy of just 1.5 (the 2 nd annotator was able to do tasks twice faster using the supporting text provided by the 1 st annotator although it could have led to some bias). This process also helped us develop 18 "edge" cases. Expert annotated these cases, and used them to develop additional guidelines. This in turn helped the crowd get better over time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Our Results on TRAT</head><p>Our relevance annotations were found to be highly accurate as evaluated with respect to those of adjudicated annotations. Our performance was on par with that of of NIST's own professional assessors (TREC 8 QRels). As shown in the following graph, comparison of the quality metrics across the first and last bars show that we achieved high levels of quality and saved quite a bit of crowd effort if we had used ML annotations for documents with ML score of &lt;2% (i.e. where machine was confident about non-relevance of the document).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>On an average, our crowd took 67 seconds per annotation. Our crowd prioritized their time effectively, spending more time when quality was required (e.g. during training period and while developing training data for M), as shown in the following table: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUC Score</head><p>In our first pass, we had only one person from the crowd annotate each document. We then identified likely errors using ML confidence score, and had a second person from the crowd annotate the documents again. This resulted in an overall redundancy of 1.5. Additionally, the expert annotated 16% of documents that were confusing to the crowd, taking the redundancy to 1.66.</p><p>5 Image Relevance assessment task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>For IRAT, we used the same crowd that we used for TRAT, but significantly enhanced the workflow and UI to adapt to the requirements of IRAT. In the UI, we displayed all the images for a topic together, saving sequential traversing time which otherwise would have been comparable to the assessment time for the Image task. As shown in Figure <ref type="figure" coords="9,117.01,330.57,4.01,9.13">3</ref>, the UI has radio buttons for each image to enter ratings, which get automatically recorded into a spreadsheet along with the time when the user clicked the radio button. Although ML was not used to assist crowd, we created rule-based "double-check" alerts in the UI using the textual information listed in the caption. These alerts helped retain the due diligence from the crowd.</p><p>Given that the low level of ambiguity in IRAT, we used annotations on a less granular scale of 1-3, where 3 represents highest relevance and 1 for highest non-relevance. A rating of 2 was assumed to mean that the annotator considered the document as either too confusing or too difficult to judge for relevance.</p><p>We used a combination of parallel and iterative approaches to do annotations as opposed to using only one of them (Greg Little, 2010). Our expert's direct interaction with the crowd enabled us to conduct quick experiments to deduce the optimal mix of iterative and parallel approaches, by evaluating the quality and cost tradeoffs. We used 2 annotators to perform annotations in parallel, and another annotator to annotate the images where the first two disagreed, which in-effect meant that each document received at-most three annotations (and at-least two annotations). Further the expert also spent 5% of the time of a single annotator in training, supervising and annotating the confusing images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Description of the Skierarchy Process</head><p>Step1: Training the Crowd using the Expert: The expert trained the crowd on 5 topics and provided a list of web based resources that they can refer to (e.g. how to optimally use Google image search and Wikipedia resources). The crowd was also trained to use the UI designed for IRAT effectively, e.g., focusing on reading captions, maintaining consistency of annotations between similar images etc.</p><p>Step2: Parallel Annotations coupled with expert auditing and feedback: Post training, we preloaded all the images in our UI along with captions for a given topic. The images whose captions had certain important keywords were highlighted in yellow so that the crowd could prioritize annotating these images. Two annotators were asked to provide annotations in parallel, and the disagreements were tracked during the annotation process. The expert used the disagreement rate between annotators and their deviation from known ground truth, to coach the crowd so that they could continuously learn, improve and remain diligent.</p><p>Step3: Reassessment of Complex / Disagreed annotations to ensure quality: The disagreements between two annotators were sent to the 3 rd annotator, and a majority opinion was taken as the final annotation. If each of the 3 annotators gave a different answer, it was sent to the expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Validation experiments</head><p>In this section, we aim to test whether some of our hypotheses related to advantages of 'closed crowd' hold true in the IRAT task. We believe that IRAT is a relatively simple task for the crowd compared to TRAT, and having studied value -add of our 'Skierarchy' approach in TRAT, we did not reassess it here. We tested the following hypothesis and optimized our process based on the learnings from this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does the 'fast prototyping cycle' enabled by a closed crowd environment improve productivity without compromising on quality?</head><p>Our ability to conduct quick experiments in our 'closed crowd' setting has helped us arrive at the optimal redundancy for our process, while also providing us with the flexibility to adapt our process dynamically. For example, we were able to reduce our average redundancy from a typical industry standard of 5 annotations per image to 2.2 per image without compromising on quality. This was based on our experiment in which we introduced captions with our images and collected 2 independent reviews on 200 images from a topic. We observed that 88% times the labels agreed and they were correct. Observing this high accuracy, we decided to take a third opinion only on the remaining 12% of the articles, limiting the redundancy to maximum of 3, and a mode of 2 reviews. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Our Results on IRAT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>A closely related area to Crowdsourcing is Human Computation that concerns with solving tasks using both humans and computers in an interactive manner. The general approach of this field is to break down the tasks into distinct tasks that humans can solve and those that computers can solve, so that the entire problem is solved using a symbiotic interaction of humans and computers. Digitization of scanned books using ReCAPCHA (von Ahn L. M., 2008), Protein folding structure discovery using the FoldIt (Christopher B Eiben, 2012), Gamification of micro-tasks (von Ahn L. K., 2006), etc. are some examples of this technique. Note that our novel approach is related to the idea of Human Computation but is still distinct from the latter since, in our case the tasks are not broken down between computers and humans. Computers do the same tasks as humans, but humans are placed in the driver's seat with computers serving as assistants.</p><p>In the area of crowdsourcing, some work combining it with Machine Learning is beginning to emerge. Companies such as SpeakerText.com are using humans to correct the machine predictions of speech transcriptions. In <ref type="bibr" coords="11,505.80,156.39,34.39,8.64;11,72.24,167.91,57.90,8.64" target="#b5">(Vamshi Ambati, 2010)</ref>, the authors proposed using Active Machine Learning approach combined with crowdsourcing to enable automatic translation for low-resource language pairs. A paper that comes closest to our work is that of (Alexander J. <ref type="bibr" coords="11,131.33,190.95,52.84,8.64" target="#b0">Quinn, 2010)</ref>, in which the authors discuss a process called CrowdFlow that facilitates interaction between humans and machines in solving a micro-task. They present an example problem of human detection in photographs where the humans correct the bounding boxes predicted by the machine to deliver the final output. We believe ours is the first set of experiments that ties together Machine Learning as a personal assistant to humans in multiple ways including prioritization of data by difficulty, automatic highlighting of key-words, and human error correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Typical sources for error in crowdsourcing can be decomposed as follows: 1) Lack of competency and lack of calibration/clarification 2) Lack of due diligence 3) Subjectivity 4) Human error due to "flow bias"<ref type="foot" coords="11,468.24,309.09,6.60,5.69" target="#foot_2">11</ref> and genuine human error. Current crowdsourcing approaches rely on micro tasking and qualification to address the first source, and ground truth and redundancy to address the next three sources of errors. While these techniques are helpful for simpler and intuitive tasks, complex tasks require pulling additional levers to control for quality. In addition to micro tasking and qualification, skill enhancement of crowd through training, expert coaching/feedback and appropriate incentives can address the first two sources of errors effectively. While redundancy is a helpful tool, it can be applied more creatively -using it where it matters the most and sometimes, using it across crowd and experts. Crowd is often good at telling where they could be going wrong; this coupled with error detection using ML and certain simple rules can help improve the quality. Also, as we proved in our experiments, for "edge" cases, redundancy with expert is much more valuable than redundancy across the crowd.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.24,368.55,52.35,9.16;3,172.08,368.55,42.13,8.64;3,90.24,380.07,123.97,8.64;3,90.24,391.59,118.31,8.64;3,72.24,403.35,352.40,9.16;3,72.24,414.87,390.06,9.16"><head></head><label></label><figDesc>(b) Offering annotation suggestions to the annotator to reduce his/her cognitive load, (c) Directing the annotator's attention by highlighting relevant sections in the data, and (d) Detecting potential human errors through their prediction mechanism and alerting the crowd.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,273.72,370.30,218.00,8.24"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Skierarchy model: Hierarchy of Skills</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,242.16,478.05,127.48,8.24;4,72.24,296.25,467.25,176.20"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: TRAT UI Screenshot</figDesc><graphic coords="4,72.24,296.25,467.25,176.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,239.52,725.06,124.36,8.24;9,72.24,500.30,459.00,218.46"><head></head><label></label><figDesc>Figure 3: IRAT UI Screenshot</figDesc><graphic coords="9,72.24,500.30,459.00,218.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,72.24,145.89,464.17,242.58"><head></head><label></label><figDesc>Our quality metrics were higher than the median scores across all topics as shown in the following table. (Our LAM score was 62.5% lower than the median and our AUC score was 17% higher than the median)</figDesc><table coords="8,77.42,145.89,423.04,242.58"><row><cell></cell><cell></cell><cell></cell><cell>LAM</cell><cell cols="2">AUC</cell><cell cols="2">MAP RMSE</cell><cell cols="2">MAP Tau</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SetuServ</cell><cell></cell><cell>0.07</cell><cell cols="2">0.91</cell><cell>0.02</cell><cell></cell><cell></cell><cell>0.93</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">NIST (TREC 8 QRels)</cell><cell></cell><cell>N/A</cell><cell cols="2">N/A</cell><cell>0.03</cell><cell></cell><cell></cell><cell>0.96</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">UIowaS02r</cell><cell></cell><cell>0.05</cell><cell cols="2">N/A</cell><cell>0.08</cell><cell></cell><cell></cell><cell>0.77</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">INFLB2012</cell><cell></cell><cell>0.13</cell><cell cols="2">N/A</cell><cell>0.09</cell><cell></cell><cell></cell><cell>0.72</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NEUEIo3</cell><cell></cell><cell></cell><cell>0.18</cell><cell cols="2">0.75</cell><cell>0.11</cell><cell></cell><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">yorku12cs03</cell><cell></cell><cell>0.22</cell><cell cols="2">0.48</cell><cell>0.11</cell><cell></cell><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BUPTPRISZHS</cell><cell></cell><cell>0.23</cell><cell cols="2">N/A</cell><cell>0.18</cell><cell></cell><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">OrcVBW16Conf</cell><cell></cell><cell>0.26</cell><cell cols="2">0.81</cell><cell>0.20</cell><cell></cell><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Topic</cell><cell>411</cell><cell>416</cell><cell>417</cell><cell>420</cell><cell>427</cell><cell></cell><cell>432</cell><cell>438</cell><cell>445</cell><cell>446</cell><cell cols="2">447 Average</cell></row><row><cell></cell><cell>Median</cell><cell>0.15</cell><cell>0.16</cell><cell>0.2</cell><cell>0.17</cell><cell>0.18</cell><cell></cell><cell>0.27</cell><cell>0.26</cell><cell>0.19</cell><cell>0.21</cell><cell>0.08</cell><cell>0.19</cell></row><row><cell>LAM</cell><cell>SetuServ</cell><cell>0.078</cell><cell>0.027</cell><cell>0.082</cell><cell>0.069</cell><cell>0.038</cell><cell cols="2">0.163</cell><cell>0.069</cell><cell>0.052</cell><cell>0.116</cell><cell>0.007</cell><cell>0.07</cell></row><row><cell></cell><cell>Median</cell><cell>0.86</cell><cell>0.85</cell><cell>0.75</cell><cell>0.71</cell><cell>0.73</cell><cell></cell><cell>0.71</cell><cell>0.78</cell><cell>0.83</cell><cell>0.82</cell><cell>0.76</cell><cell>0.780</cell></row><row><cell>AUC</cell><cell>SetuServ</cell><cell>0.968</cell><cell>0.965</cell><cell>0.89</cell><cell>0.96</cell><cell>0.956</cell><cell cols="2">0.712</cell><cell>0.951</cell><cell>0.935</cell><cell>0.834</cell><cell>0.968</cell><cell>0.914</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,72.24,543.03,464.47,108.96"><head></head><label></label><figDesc>Our relevance annotations were found to be highly accurate on IRAT as well -</figDesc><table coords="10,72.24,566.55,464.47,85.44"><row><cell></cell><cell>LAM</cell><cell>AUC</cell></row><row><cell>SetuServ</cell><cell>0.092</cell><cell>0.873</cell></row><row><cell>UT Austin</cell><cell>0.227</cell><cell>0.529</cell></row><row><cell cols="3">The average redundancy was 2.2 per image. Cost per label (including this redundancy) was $0.02. The average time</cell></row><row><cell cols="2">per each label was nearly 6 seconds.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,80.02,645.90,457.36,7.80;2,72.24,657.18,466.08,7.80;2,72.24,667.50,51.71,7.80"><p>In fact, the ground truth "qrels" for TREC topics were curated by full-time information analysts at LDC. Besides, TREC topics tend to be very nuanced with their inclusion and exclusion rules compared to broader subject categories such as politics, sports or entertainment.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,80.02,680.46,197.90,7.80"><p>www.pandora.com is a music recommendation engine.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_2" coords="11,84.47,711.66,449.07,7.80;11,72.24,722.70,229.16,7.80"><p>This is our nomenclature for the type of error that is caused by the "momentum" of the annotator, due to which the annotator tends to be biased towards one of the ratings more than the rest.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,72.24,466.62,448.60,7.80;11,72.24,478.62,195.43,7.80" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,196.56,466.62,324.29,7.80;11,72.24,478.62,36.42,7.80">CrowdFlow: Integrating Machine Learning with Mechanical Turk for Speed-Cost-Quality Flexibility</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="11,72.24,500.46,333.40,7.80" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,174.55,500.46,186.07,7.80">Crowdsourcing User Studies With Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Aniket Kittur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM CHI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,72.24,522.30,289.94,7.80" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Bernstein</surname></persName>
		</author>
		<title level="m" coord="11,162.20,522.30,195.73,7.80">Soylent: A Word Processor with a Crowd Inside. UIST</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,72.24,544.14,447.77,7.80;11,72.24,556.14,131.70,7.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,197.71,544.14,318.78,7.80">Increased Diels-Alderase activity through backbone remodeling guided by Foldit players</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Christopher B Eiben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,72.24,556.14,79.21,7.80">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="190" to="192" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,72.24,577.98,404.17,7.80" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,165.54,577.98,227.05,7.80">Exploring Iterative and Parallel Human Computation Processes</title>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Human Computation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,72.24,599.82,463.34,7.80;11,72.24,611.82,93.21,7.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,183.05,599.82,214.17,7.80">Active learning and crowd-sourcing for machine translation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">V</forename><surname>Vamshi Ambati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,402.91,599.82,132.67,7.80;11,72.24,611.82,25.90,7.80">Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2169" to="2174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,72.24,633.66,332.68,7.80" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,157.52,633.66,202.62,7.80">Verbosity: A Game for Collecting Common-Sense Facts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">K</forename><surname>Von Ahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ACM CHI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,72.24,655.50,456.86,7.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,159.02,655.50,288.31,7.80">reCAPTCHA: Human-Based Character Recognition via Web Security Measures</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>Von Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,453.65,655.50,27.43,7.80">Science</title>
		<imprint>
			<biblScope unit="page" from="1465" to="1468" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
