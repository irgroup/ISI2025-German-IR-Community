<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.23,80.99,399.54,12.90">THE HLTCOE APPROACH TO THE TREC 2012 KBA TRACK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.65,133.77,76.70,10.75"><forename type="first">Brian</forename><surname>Kjersten</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HLTCOE Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.05,133.77,77.90,10.75"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">HLTCOE Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.23,80.99,399.54,12.90">THE HLTCOE APPROACH TO THE TREC 2012 KBA TRACK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">447A22C765F172ED885985C64F3C3EC9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our team submitted runs for the TREC KBA Cumulative Citation Recommendation task. This task involves labeling over 300 million documents for whether they are relevant and/or central to particular entities already in a database. For this task, we used an SVM classifier that uses unigrams and named entities as binary features. In this paper, we describe our work for the 2012 evaluation and the results we obtained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the TREC KBA track 1 is to help people edit and add information about named-entities to existing databases such as Wikipedia. The defined task for the first year of KBA is to identify whether documents include facts that are relevant to certain entities that already exist in a database. We were provided with a large collection of documents consisting of blogs and news articles from between October 7, 2011 to May 2, 2012. A subset of 15,815 of the 2011 documents are labeled for whether or not they are relevant or central to any of 29 entities.</p><p>This task is interesting in a number of ways. One is the collection which contains over 300 million documents which contain fine-grained (i.e., subhour) timestamps for a period of 8 consecutive months. Another is that some of the entities of interest were selected to be especially difficult to disambiguate. Another interesting feature is that there is an option of labeling documents for whether they 1 http://trec-kba.org/ are central to an entity, rather than merely mentioning the entity. We opted to focus on labeling centrality rather than relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Entity Search as Topic Classification</head><p>The HLTCOE team approached the task by thinking of relevance and centrality labeling as a classical document classification task such as was studied at the TREC Filltering Track <ref type="bibr" coords="1,429.72,360.29,110.28,9.46;1,313.20,373.84,23.48,9.46" target="#b2">(Robertson and Soboroff, 2002)</ref>. Support Vector Machines (SVMs) are known to be well-performing text classifiers, and they support high-dimension representations such as bags of words. We trained a linear SVM classifier for each of the 29 entities of interest. To generate a model for each entity, we used all of the the labeled training data from the time period October 7 to December 31, 2011. If a document was labeled as "central" for an entity we included it as a positive training example for the SVM. If a document was not labeled as "central" (for example, it had a label of "relevant" or "junk"), we included it as a negative training example.</p><p>The documents that were pre-labeled for training were selected because they were the ones that were most likely to be about the entities. Therefore, we were able to assume that most of the documents that were not labeled are not central to any of the entities. In addition to the labeled examples, we added 11,007 unlabeled documents randomly selected from the same 2011 time span to use as additional, presumptive negative training examples. This served as a generic source of negative documents that are likely not topically close to the entities of interest. We also want a large source of negative documents to help the classifier avoid focusing on that words that are particular to the training epoch (e.g., Thanksgiving or December). The same 11,007 presumed negative examples were used for each entity.</p><p>As preprocessing, we downcased words and removed punctuation within words. This meant that the word "can't" was normalized to "cant" and "high-performing" was normalized to "highperforming". We removed a set of 319 stopwords. To filter out non-English documents, we ignored any word that contained a character that was neither ASCII nor part of the Latin subset of Unicode. We also ignored all documents that contained fewer than 6 word types from our stop word list. This was intended as a means to avoid non-English documents, and it had the added benefit of skipping very short English documents.</p><p>All of our SVM experiments were performed with the SVM Light toolkit <ref type="bibr" coords="2,178.40,320.22,75.17,9.46" target="#b1">(Joachims, 1998)</ref>. We exclusively used binary features, representing whether or not a term was present in a document, regardless of its frequency. The features we used were the unigrams in each article and the named entities labeled in each document; we used the automaticallylabeled named entities that were provided as annotations with the corpus. Each of these features was assigned an integer-valued feature number f in the order it was seen.</p><p>This leads to 61,735,102 unigram features and 59,534,101 entity name features, for a total of 121,269,203 features. Since out of the box, the SVM Light tool can handle only 100 million features, we needed to map these to a smaller feature set. For features that were seen after the first 90 million, we reassigned their feature numbers using an multiplicative hashing method.</p><p>The hashing algorithm we used was to multiply the feature number by an irrational number (we chose 1/ (π)) , subtract the next lowest integer, and map it to the range of 10 million to 90 million to get a new feature number:</p><formula xml:id="formula_0" coords="2,131.78,640.97,101.20,25.95">λ = f 1 (π) - 1 (π)</formula><p>f new = λ90, 000, 000 + (1 -λ)10, 000, 000</p><p>This reduces the number of unique features by creating collisions between existing features. We protect the first 10 million features from collision, and some of the remaining, rarer features will have conflations, which we thought unlikely to have a significant deleterious effect. <ref type="foot" coords="2,426.53,114.46,3.99,6.91" target="#foot_0">2</ref>The test data was the corpus from January 1 to May 2, 2012. At test time, we ran svm classify on all of the evaluation data for all of our models (i.e., one model for each entity). SVM Light produces confidence labels, which are less than 0 if a document is given a negative label, and are greater than 0 if a document is given a positive label. We inspected predictions on training data (i.e., data before Jan. 1), and we observed that the majority of these scores lie between -1.0 and 1.0. To convert these to the [0, 1000] range that is required for the TREC KBA task, we multiplied by 500 and added 500. If a document had a score greater than 1000, we forced its score to 1000. If a document had a score less than 50, we did not include it in the submission. This ensures that a neutral document is mapped to 500, and documents with scores greater than one or higher are mapped to 1000.</p><p>To produce runs we ran computing jobs for different evaluation epochs in parallel using the Sun Grid Engine (SGE) platform.</p><p>We submitted two runs: hltcoe-wordNER and hltcoe-wordNER500. The later is simply a filtering of the first run, which removed documents with scores below 500 ("neutral").<ref type="foot" coords="2,441.37,453.46,3.99,6.91" target="#foot_1">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head><p>We first examine the distribution of output scores (Figure <ref type="figure" coords="2,347.99,514.28,3.94,9.46" target="#fig_0">1</ref>). Remembering that a score of 500 is neutral, and higher scores are positive, we can observe that very few documents are labeled as positive. This is as expected, because most documents in the vast collection are not about the entities we are interested in. More interestingly, there is a smooth drop-off in the proportions of documents with scores as the scores increase. This will affect the way we interpret our other results. To see how our system performs using traditional metrics, see Figure <ref type="figure" coords="3,189.70,389.89,4.09,9.46">2</ref>. The figure shows the arithmetic mean of Precision, Recall, F-Score, and Scaled Utility across the 29 entities as cutoff value is adjusted. Surprisingly, Precision peaks near a cutoff value of 500, which corresponds to the boundary between the positive label and the negative label. This might be because it is unusual for a document to obtain very high scores, and high-scoring documents have greater variance due to sparsity. Interestingly, the F-Score continues to grow considerably after Precision drops off, suggesting that if Recall is just as important as Precision, we should use cutoff values much lower than 500.</p><p>While there are insights to be gained by using the cutoff value as the x-axis, in an IR context it is also informative to look at the ranks of the documents themselves. Figure <ref type="figure" coords="3,161.11,612.37,5.45,9.46">3</ref> shows the same performance metrics from the perspective of rank. In this graph, the x-axis is shifted in proportion to how numerous documents are at each score level. Here, it is clear that Precision peaks early on, indicating that the true positives are some of the highest scoring.</p><p>An analysis of the scores for each individual entity is shown in Table <ref type="table" coords="3,173.68,712.90,4.09,9.46" target="#tab_0">1</ref>. This shows the scores at the cutoff value which maximizes the F-Score. For most entities, the F-Score is maximized near a cutoff of 50 or 100. This is because of the high recall, as noted above. Some entities have a corresponding F-Score of 0, because none of the relevant documents appeared with a score above 50. It is not clear whether this method has particular difficulty when two entities share a name. None of the Boris Berezovsky (pianist) documents were correctly labeled, but Basic Element (company) was the best performing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We have demonstrated that a Support Vector Machine classifier using bag-of-words and bag-ofentity-names is a tractable method to label very large collections of documents as central to an entity or not, and its performance is competitive with other approaches to the TREC KBA track. There are many ways that this approach can be extended in the future, including additional features based on syntactic relations or relationships between document entities. It might also be informative to compare SVMs with other standard approaches. We also envisioned dynamically adapting classifiers over time, which Figure <ref type="figure" coords="4,102.57,353.03,4.24,9.46">2</ref>: The performance of our system (hltcoe-wordNER) as a function of cutoff value. Note that Precision peaks near a cutoff=500, which corresponds to the boundary between a positive label and a negative label according to the support vector machine.</p><p>Figure <ref type="figure" coords="4,103.77,660.39,4.24,9.46">3</ref>: Since most documents were negative (and the negative class prior is quite large), the SVM produced few documents scores above 500. For this reason, it is useful to view the performance with score ranks along the x-axis. Here it is clear that the Precision is maximized for high-ranking documents, but that the F-Score continues to grow as Recall increases. might improve performance as some entities appear in the document stream in bursts due to discrete events, however, time did not permit exploration of adaptive filtering on this data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,330.37,468.00,9.46;3,72.00,343.92,222.99,9.46;3,135.92,72.00,340.17,241.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The distribution of document scores produced by our SVM system. Since most documents are not central, few documents earn a score of at least 500.</figDesc><graphic coords="3,135.92,72.00,340.17,241.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,135.92,81.50,340.16,255.12"><head></head><label></label><figDesc></figDesc><graphic coords="4,135.92,81.50,340.16,255.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,135.92,416.56,340.16,227.42"><head></head><label></label><figDesc></figDesc><graphic coords="4,135.92,416.56,340.16,227.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,97.69,73.53,416.62,329.66"><head>Table 1 :</head><label>1</label><figDesc>The cutoff values that maximize F-Score for each entity, and the corresponding scores.</figDesc><table coords="5,138.61,73.53,334.79,307.45"><row><cell>Entity</cell><cell cols="5">Cutoff Precision Recall F-Score Scaled Utility</cell></row><row><cell>Aharon Barak</cell><cell>100</cell><cell>0.27</cell><cell>0.13</cell><cell>0.17</cell><cell>0.31</cell></row><row><cell>Alex Kapranos</cell><cell>100</cell><cell>0.23</cell><cell>0.2</cell><cell>0.21</cell><cell>0.24</cell></row><row><cell>Alexander McCall Smith</cell><cell>100</cell><cell>0.16</cell><cell>0.45</cell><cell>0.24</cell><cell>0</cell></row><row><cell>Annie Laurie Gaylor</cell><cell>100</cell><cell>0.49</cell><cell>0.33</cell><cell>0.39</cell><cell>0.44</cell></row><row><cell>Basic Element (company)</cell><cell>200</cell><cell>0.88</cell><cell>0.5</cell><cell>0.64</cell><cell>0.64</cell></row><row><cell>Basic Element (music group)</cell><cell>100</cell><cell>0.95</cell><cell>0.22</cell><cell>0.36</cell><cell>0.48</cell></row><row><cell>Bill Coen</cell><cell>100</cell><cell>0.33</cell><cell>0.04</cell><cell>0.07</cell><cell>0.33</cell></row><row><cell>Boris Berezovsky (businessman)</cell><cell>100</cell><cell>0.33</cell><cell>0.27</cell><cell>0.3</cell><cell>0.33</cell></row><row><cell>Boris Berezovsky (pianist)</cell><cell>950</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.33</cell></row><row><cell>Charlie Savage</cell><cell>400</cell><cell>0.61</cell><cell>0.23</cell><cell>0.33</cell><cell>0.44</cell></row><row><cell>Darren Rowse</cell><cell>150</cell><cell>0.18</cell><cell>0.35</cell><cell>0.24</cell><cell>0.04</cell></row><row><cell>Douglas Carswell</cell><cell>100</cell><cell>0.13</cell><cell>0.32</cell><cell>0.19</cell><cell>0</cell></row><row><cell>Frederick M. Lawrence</cell><cell>950</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.33</cell></row><row><cell>Ikuhisa Minowa</cell><cell>50</cell><cell>0.45</cell><cell>0.66</cell><cell>0.53</cell><cell>0.5</cell></row><row><cell>James McCartney</cell><cell>50</cell><cell>0.14</cell><cell>0.3</cell><cell>0.19</cell><cell>0</cell></row><row><cell>Jim Steyer</cell><cell>100</cell><cell>0.72</cell><cell>0.39</cell><cell>0.51</cell><cell>0.55</cell></row><row><cell>Lisa Bloom</cell><cell>250</cell><cell>0.48</cell><cell>0.22</cell><cell>0.3</cell><cell>0.4</cell></row><row><cell>Lovebug Starski</cell><cell>150</cell><cell>0.33</cell><cell>0.11</cell><cell>0.17</cell><cell>0.33</cell></row><row><cell>Mario Garnero</cell><cell>100</cell><cell>0.96</cell><cell>0.36</cell><cell>0.53</cell><cell>0.57</cell></row><row><cell>Masaru Emoto</cell><cell>150</cell><cell>0.33</cell><cell>0.14</cell><cell>0.2</cell><cell>0.33</cell></row><row><cell>Nassim Nicholas Taleb</cell><cell>100</cell><cell>0.4</cell><cell>0.51</cell><cell>0.45</cell><cell>0.42</cell></row><row><cell>Rodrigo Pimentel</cell><cell>250</cell><cell>0.75</cell><cell>0.23</cell><cell>0.35</cell><cell>0.46</cell></row><row><cell>Roustam Tariko</cell><cell>100</cell><cell>0.33</cell><cell>0.41</cell><cell>0.36</cell><cell>0.33</cell></row><row><cell>Ruth Rendell</cell><cell>50</cell><cell>0.32</cell><cell>0.29</cell><cell>0.3</cell><cell>0.32</cell></row><row><cell>Satoshi Ishii</cell><cell>100</cell><cell>0.59</cell><cell>0.39</cell><cell>0.47</cell><cell>0.5</cell></row><row><cell>Vladimir Potanin</cell><cell>250</cell><cell>0.62</cell><cell>0.25</cell><cell>0.36</cell><cell>0.45</cell></row><row><cell>William Cohen</cell><cell>100</cell><cell>0.1</cell><cell>0.14</cell><cell>0.12</cell><cell>0</cell></row><row><cell>William D. Cohan</cell><cell>250</cell><cell>0.47</cell><cell>0.49</cell><cell>0.48</cell><cell>0.48</cell></row><row><cell>William H. Gates, Sr</cell><cell>100</cell><cell>0.1</cell><cell>0.04</cell><cell>0.05</cell><cell>0.25</cell></row><row><cell>Macro-average</cell><cell>0</cell><cell>0.32</cell><cell>0.29</cell><cell>0.27</cell><cell>0.27</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,329.34,643.51,210.67,7.77;2,313.20,653.47,226.80,7.77;2,313.20,663.44,226.80,7.77;2,313.20,673.40,77.73,7.77"><p>One of the authors was introduced to hashing termids in this fashion by Chris Buckley, who used a hybrid hashed/unhashed dictionary for the Terabyte track in TREC 2004.<ref type="bibr" coords="2,332.69,673.40,58.24,7.77" target="#b0">(Buckley, 2005)</ref></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,325.85,682.41,2.99,5.18;2,329.34,684.27,210.66,7.77;2,313.20,694.09,226.80,7.93;2,313.20,704.20,226.80,7.77;2,313.20,714.16,29.01,7.77"><p>  3  We did not intend to imply a ranking of documents in hltcoe-wordNER500, however, we neglected to transform all scores in this run to a fixed value, which would have made this explicit.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,72.00,522.39,226.80,8.64;5,82.91,533.18,215.89,8.81;5,82.91,544.14,181.55,8.58" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,164.24,522.39,134.56,8.64;5,82.91,533.35,119.93,8.64">Looking at Limits and Tradeoffs: Sabir Research at TREC 2005</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,223.62,533.18,75.18,8.58;5,82.91,544.14,123.16,8.58">Proceedings of the 14th Text Retrieval Conference</title>
		<meeting>the 14th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005. TREC 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,564.23,226.80,8.64;5,82.91,575.19,215.89,8.64;5,82.91,585.98,215.89,8.81;5,82.91,596.94,169.35,8.81" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,164.68,564.23,134.12,8.64;5,82.91,575.19,215.89,8.64;5,82.91,586.15,18.21,8.64">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,121.24,585.98,177.56,8.58;5,82.91,596.94,30.60,8.58">European Conference on Machine Learning (ECML)</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,617.03,226.80,8.64;5,82.91,627.82,215.89,8.81;5,82.91,638.78,181.55,8.58" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,254.82,617.03,43.98,8.64;5,82.91,627.99,114.46,8.64">The TREC 2002 Filtering Track Report</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,221.78,627.82,77.02,8.58;5,82.91,638.78,123.16,8.58">Proceedings of the 11th Text Retrieval Conference</title>
		<meeting>the 11th Text Retrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
