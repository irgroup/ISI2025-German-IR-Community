<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,150.20,468.01,15.12">Northeastern University Runs at the TREC12 Crowdsourcing Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-02-06">February 6, 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.30,183.98,77.62,10.48"><forename type="first">Maryam</forename><surname>Bashir</surname></persName>
							<email>maryam@ccs.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.01,183.98,75.52,10.48"><forename type="first">Jesse</forename><surname>Anderton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.17,183.98,32.24,10.48"><forename type="first">Jie</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.24,183.98,130.68,10.48"><forename type="first">Matthew</forename><surname>Ekstrand-Abueg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,186.08,197.92,80.06,10.48"><forename type="first">Peter</forename><forename type="middle">B</forename><surname>Golbus</surname></persName>
							<email>pgolbus@ccs.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.65,197.92,59.80,10.48"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.73,197.92,81.18,10.48"><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,150.20,468.01,15.12">Northeastern University Runs at the TREC12 Crowdsourcing Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-02-06">February 6, 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">F3066BC7C5BCE6E322EA03FAC5BA7D34</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of the TREC 2012 Crowdsourcing Track was to evaluate approaches to crowdsourcing high quality relevance judgments for images and text documents. This paper describes our submission to the Text Relevance Assessing Task. We explored three different approaches for obtaining relevance judgments. Our first two approaches are based on collecting a limited number of preference judgments from Amazon Mechanical Turk workers. These preferences are then extended to relevance judgments through the use of expectation maximization and the Elo rating system. Our third approach is based on our Nugget-based evaluation paradigm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We have participated in the Text Relevance Assessing Task (TRAT), one of the two TREC 2012 Crowdsourcing Track tasks. The goal of the TREC Crowdsourcing Track was to evaluate approaches to crowdsourcing high quality relevance judgments for text documents and images. We used the following approaches for obtaining relevance judgments:</p><p>• pair-based ELO The Elo rating system is a method for calculating the relative rating of players in two player games like Chess <ref type="bibr" coords="1,218.13,532.37,9.96,8.74" target="#b4">[5]</ref>. We have used this system for obtaining relevance ranking of documents instead of players by treating a preference judgment between two documents as a match between two players. The Elo rating system updates the score of a document when a certain number of preference comparisons for that document have been made. The score of every document is updated based on the score of the documents to which it was compared. In this work, we demonstrate that the Elo rating system can accurately rank documents using only O(n) preference judgments.</p><p>• pair-based EM In this context, the Expectation Maximization algorithm <ref type="bibr" coords="1,422.49,617.23,10.52,8.74" target="#b6">[7]</ref> is a means of estimating "true" labels from crowd workers as latent variables in a model of worker quality. We convert the collected preferences into binary relevance judgments, and use EM to compute probabilities of relevance for each document.</p><p>• nugget-based Relevant nuggets-atomic pieces of factual information-are extracted by assessors directly from documents. The nuggets are used to find relevant documents, using a technique based on our nugget-based evaluation framework <ref type="bibr" coords="1,267.96,702.10,9.96,8.74" target="#b8">[9]</ref>. This does not use the document pair preference assessments obtained from Mechanical Turk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Preference Judgments vs. Relevance Judgments</head><p>Traditionally, assessors are asked to give absolute relevance grades to each document with respect to some topic. However, studies have shown that assessors can give more reliable judgments if they are asked which of a pair of documents they prefer, i.e. "is document A better than document B?" rather than "how relevant is document A?" <ref type="bibr" coords="2,137.45,167.59,9.96,8.74" target="#b3">[4]</ref>. Another advantage of using preferences is that many popular learning-to-rank algorithms such as RankBoost <ref type="bibr" coords="2,157.30,179.54,10.52,8.74" target="#b5">[6]</ref> and RankNet <ref type="bibr" coords="2,231.79,179.54,10.52,8.74" target="#b2">[3]</ref> are trained on preferences. Since preferences are often not available, these algorithms need to infer preferences from the absolute relevance judgments collected from assessors. Some information is lost during this process, leading to many ties between documents. This suggests that preferences can be used to improve the training of learning-to-rank algorithms that use such a pairwise approach.</p><p>The use of preference judgments on document pairs, as opposed to absolute judgments on documents, for IR evaluations creates new challenges. There are n 2 unique pairs of documents for a list of n documents, which means that the number of judgments we need to collect increases to O(n 2 ). Since collecting judgments is very costly, we need a mechanism for collecting these preferences efficiently. If the collected preferences were transitive, we could reduce the cost by collecting preference judgments for only O(n lg n) pairs of documents and using them as the comparator for a sorting algorithm. This would produce a sorted list of documents. Unfortunately, preference judgments are known to be not perfectly transitive. We overcome this problem by using the Elo ranking system to produce ranked lists of documents from incomplete preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Relevant Nuggets vs Relevance Judgments</head><p>In a separate run, we propose a new method for relevance assessment based on relevant information, not relevant documents. Once the relevant information is collected, any document can be assessed/inferred for relevance <ref type="bibr" coords="2,114.16,398.95,9.96,8.74" target="#b8">[9]</ref>. The problem with document judgments and with its variants is that the information relevant to a topic is encoded by documents, and in the presence of large topic sets or large and/or dynamic collections, it is difficult or impossible to find and judge all relevant documents. Our thesis is that while the number of documents potentially relevant to a topic can be enormous, the amount of information relevant to a topic, the nuggets, is far, far smaller.</p><p>The fundamental thesis of this idea is that finding relevant information immediately leads to relevant documents, which in turn lead to more relevant information. Additionally, documents marked non-relevant are useful for determining non-relevant information; in particular, the TREC assessment philosophy that a document is relevant if it contains any relevant information fits this setup well: any nuggets found in a document marked non-relevant must be non-relevant, up to reasonable assessor disagreement and label noise.</p><p>The rest of paper is organized as follows: Section 2 describes the collection of preference judgments on document pairs, providing details on our HIT design, experimental setup, and results. Section 3 describes the pair-based runs that use the Elo rating system and EM approaches. Section 4 describes our work using the Nugget-based evaluation paradigm. The final section, Section 6, concludes the paper with a description of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document-Pair assessment collection with Mechanical Turk</head><p>In this section we describe our experimental design for the collection of preference judgements from crowd workers and our methodology for obtaining complete relevance judgments from only O(n) preference judgements. This section is organized as follows: Section 2.1 gives the details of our Amazon Mechanical Turk setup for crowdsourcing preference judgements, section 2.1.1 describes our HIT design, section 3.1 and section 5 give the methodology and results of using the Elo rating method for extracting relevance judgements from incomplete preference pairs, and section 3.2 gives the methodology and results of using EM for extracting relevance judgements from incomplete preference pairs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Amazon Mechanical Turk HITs</head><p>Amazon Mechanical Turk (AMT) is a crowdsourcing Internet marketplace that gives developers the ability to use human intelligence for tasks that computers are currently unable to do <ref type="bibr" coords="3,411.51,400.27,9.96,8.74" target="#b0">[1]</ref>. A person or organization, termed the "requestor," creates a task definition in the marketplace which workers may then carry out in return for payment. A task definition is called a Human Intelligence Task (HIT). When a worker submits work for a given HIT, the quality of the submission can be automatically assessed and the submission can be either approved, leading to payment, or rejected. There are around 200,000 registered workers of AMT from almost 100 countries. More details on how to use this service can be found in the developer documentation <ref type="bibr" coords="3,526.72,460.04,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">HIT Design</head><p>The HITs we created to collect preference judgements from crowd workers had the following design. After accepting the assignment, workers were shown the instructions seen in Figure <ref type="figure" coords="3,418.49,521.36,3.87,8.74" target="#fig_0">1</ref>. In these instructions, we explained that documents should be preferred strictly based on whether they provide information about the query, description, and narrative for a particular topic: that a well-written discussion of a related topic should not be preferred to a poorly-written document which is exactly on topic.</p><p>After dismissing the instructions, the worker is shown the interface presented in Figure <ref type="figure" coords="3,474.91,569.89,3.87,8.74">2</ref>. The "Title" field of a TREC topic is displayed on top, along with its description and narrative. This information describes in detail what constitutes a relevant document for this query. Below the query information is a series of buttons, which allow workers to record their preferences. Two documents are displayed side-by-side below the buttons. The leftmost and rightmost buttons are labeled "This One," with an arrow pointing to the left or right document, respectively. These buttons allow users to choose a winning document. Between these buttons are two buttons for recording ties, labeled "They're Equally Good" and "They're Equally Bad." Each HIT consisted of 20 preference pairs for the same topic, and had a time limit of 30 minutes. Workers were paid $0.15 for each approved HIT. The order in which the preferences were displayed, as well as the order of the documents for each particular preference, was randomized. Document pairs were selected for judgement in the following manner. First, we calculated a prior relevance score for each document using BM25. This produced an initial ranking of the documents for each topic. For each document below the top six, we selected five documents to be compared against, uniformly at random, Figure <ref type="figure" coords="4,161.66,353.40,3.87,8.74">2</ref>: Preference pair selection interface with topic keywords and description from the set of documents with higher BM25 scores. For the top six documents, we collected complete pairwise preferences. We collected four worker assessments for each preference pair which we selected for judgement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Quality Control</head><p>The workers we employed have no particular training in assessing document relevance, so we need a means of verifying the quality of their work. There have been many studies on assessing worker quality for crowdsourcing platforms such as AMT <ref type="bibr" coords="4,215.79,515.88,9.96,8.74" target="#b6">[7]</ref>. We used trap questions in our study to ensure that workers are making a reasonable effort, instead of clicking randomly.</p><p>A trap question is a question inserted into the HIT which looks the same as any other, but for which a correct answer is known ahead of time. We asked five IR graduate students to create our trap questions by pairing documents which appeared to be highly relevant with documents which appeared to be highly non-relevant. We then mixed five of these trap questions, selected at random, into each HIT. As a result, each assignment consisted of five trap questions and fifteen "real" questions. A worker's submission was accepted if at least two of the five trap questions were answered correctly, and rejected otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Data</head><p>There were a total of 18,260 documents from TREC 8 ad-hoc, which used the Text Research Collection Volumes 4 (May 1996) and 5 (April 1997) minus the Congressional Record (CR). 10 topics were selected randomly from TREC topics. Participants of the crowdsourcing track were required to simulate the role of NIST assessors for the 10 TREC 8 ad-hoc topics.</p><p>3 Pair-based runs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Elo Rating System</head><p>The Elo rating system is a method for calculating the relative rating of players in two player games <ref type="bibr" coords="5,526.72,154.48,9.96,8.74" target="#b4">[5]</ref>. The rating system assigns each player a rating score, with a higher number indicating a better player. Each player's rating is updated after he or she has played a certain number of matches, increasing or decreasing in value depending on whether the player won or lost each match, and on the ratings of both players competing in each match-beating a highly rated player increases one's rating more than beating a player with a low rating, while losing to a player with a low rating decreases one's score more than losing to a player with a high rating. These scores are used in two ways: 1) players are ranked by their scores, 2) the scores can be used to compete the likelihood of each player beating every other player. If the matches are selected intelligently, this can be accomplished even if only O(n) matches are played.</p><p>For our problem, we treat each document as a player and each preference judgment between two documents as a match between players. All documents enter the "tournament" rated equally. After each document "plays" a certain number of matches, we update each document's rating according to equation 3. After all the matches are played, we can rank the documents by their final score. This list can be thresholded to produce absolute relevance judgments. We can also use the scores to compute transitive preference judgments.</p><p>For our preliminary experiments, we select O(n) matches stochastically. We wish to sample pairs in such a way that we create a bias towards relevant documents. In this way, relevant documents will play more matches than non-relevant documents, giving them more opportunities to improving their ratings and move up the list. We begin by ranking documents for each topic using the BM25 retrieval method. The first five documents all "play" one another. Each remaining document in the list plays against 5 randomly selected documents with higher ranks. In our experiments, every document plays at least 5 matches. On average, every document plays 11 matches. We sort all documents based on their ratings after all O(n) matches have been played.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Mathematical Details of The Elo Rating System</head><p>The Elo rating system measures the skill level of players in relative terms. Each player's rating depends on the rating of his or her opponents, as well as the of wins or losses. The expected score for each player in a match can be estimated from the ratings of the players. If player A has rating R A and player B has rating R B , then the expected score of player A is calculated as follows:</p><formula xml:id="formula_0" coords="5,262.63,517.74,277.37,25.20">E A = 1 1 + 10 R B -R A 200<label>(1)</label></formula><p>Similarly the expected score for player B is:</p><formula xml:id="formula_1" coords="5,262.45,572.50,277.56,25.20">E B = 1 1 + 10 R A -R B 200<label>(2)</label></formula><p>Player's ratings are updated by comparing their actual score to their expected score. When a player performs worse than expected, then his or her rating is decreased. If player A has an expected score, E A , and an actual score, S A , then his or her rating will be updated by following formula:</p><formula xml:id="formula_2" coords="5,251.45,653.78,288.55,10.62">R A = R A + K(S A -E A )<label>(3)</label></formula><p>This update can be made after every match, or after several matches according to the situation.</p><p>ELO Experimental Setup. We have set K equal to 20 in Elo rating update equation 3 and each document has an initial uniform rating of 1000. Every document pair selected for comparison was judged by 5 crowd workers. For each of the 10 topics, participants of TREC Crowdsourcing track were required to provide both binary relevance judgments and probabilities of relevance for each document. Probabilities were produced by normalizing the Elo scores. We then label the n most probable documents as relevant, where n was chosen by manual inspection.</p><p>3.1.2 TREC Runs "NEUElo2/3/4/5"</p><p>We have submitted four different runs to the TREC Crowdsourcing track using the Elo rating algorithm for preference judgments. Due to time constraints we were not able to collect five judgments for every document pair for our submitted runs to TREC. The following is a description of each of the runs created using the Elo rating algorithm:</p><p>NEUElo2: This run was created using a non-uniform number of judgments for each document pair, i.e. the number of judgments for each document pair could be from 1 to 5. The threshold for binary relevance was the top 20 documents for each topic.</p><p>NEUElo3: This run was the same as NEUElo3 except that we also used the judgment from our trap questions. Since the trap questions were created by experts, we added the documents preferred in the trap questions to the top of each ranked list. We also chose a different number of relevant documents for topic, again by manual inspection.</p><p>NEUElo4: This run was the same as NEUElo3 except that we ignored all the document pairs that had only 1 judgment. This was done to avoid random judgments.</p><p>NEUElo5: This run was the same as NEUElo4 except that the threshold for binary relevance was the top 20 documents for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Expectation Maximization run "NEUEM1"</head><p>Expectation Maximization (EM) is an iterative method for finding the maximum likelihood estimate of the parameters of a probability distribution using a model incorporating unobserved latent variables. In our model, we treat the observed preference judgments from Crowdsource workers as being drawn from a distribution parameterized by the "quality" of each worker. The relevance grade of each document is treated as a latent variable. For the purposes of this task, the end result is a probability distribution over each latent variable, i.e. the probability that each document is relevant. Our approach is similar to that of Hosseini et al. <ref type="bibr" coords="6,85.83,563.13,9.96,8.74" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The EM Algorithm</head><p>Assume that there are N documents and M crowd workers. Let G represent the number of relevance grades. We assign each worker j a G × G latent confusion matrix, C j . Each element c j lg is the probability that worker j assigns a label l to a document whose true relevance is g. In our model, each document has one of two relevance grades, relevant and non-relevant; i.e. g ∈ {r, n}. Hence C j is a 2 × 2 confusion matrix, C j = c rr c rn c nr c nn . Let Pr(R i = g) denote the probability that document i has the true relevance grade g, p g</p><p>is the prior probability distribution over relevance grades, and X j il is an indicator variable indicating whether worker j has assigned labeled l to document i. There are five steps in the EM algorithm:</p><p>Step 0: Pre-processing</p><p>We wish to learn the relevance grades of each document. However, we have collected preferences between pairs of documents. Before we can use the EM algorithm, we must first convert these preferences into grades. We do so in the following way: assume that worker j prefers document A to document B.</p><p>Then we interpret this as worker j labeling document A as relevant, and document B as non-relevant.</p><p>Step 1: Initialization</p><p>We assume that all of the crowd workers are honestly attempting to give correct answers. Therefore, we initialize each workers confusion matrix to:</p><p>C j = 0.9 0.1 0.1 0.9</p><p>Furthermore, we assume that all documents are equally likely to be relevant and non-relevant. Therefore, we initialize p r = 0.5 and p n = 0.5.</p><p>Step 2: Estimate the probability of relevance In this step, we combine the confusion matrix and relevance labels from each worker to estimate the probability that each document is relevant. All labels are assumed to be i.i.d.</p><formula xml:id="formula_3" coords="7,179.44,335.77,360.56,55.92">Pr(R i = r | C j , ∀j ∈ M ) = p r × M j=1 G l=0 c j lr X j il p r × M j=1 G l=0 c j lr X j il + M j=1 G l=0 c j ln X j il<label>(4)</label></formula><formula xml:id="formula_4" coords="7,178.84,400.79,361.16,55.92">Pr(R i = n | C j , ∀j ∈ M ) = p r × M j=1 G l=0 c j ln X j il p r × M j=1 G l=0 c j lr X j il + M j=1 G l=0 c j ln X j il<label>(5)</label></formula><p>Step 3: Estimate the maximum likelihood Using the each documents' estimated probability of relevance calculated in step 2, we produce an updated estimate of each worker's confusion matrix.</p><formula xml:id="formula_5" coords="7,246.20,511.59,293.80,52.35">c j lg = N i=1 Pr(R i = g) × X j il G k=0 N i=i Pr(R i = g) × X j ik<label>(6)</label></formula><p>Step 4: Iterate Step 2 and 3 until convergance We repeatedly calculated worker confusion matrices and probabilities of document relevance until the results converged. We consider this process to have converged when, for each document i, the difference between Pr(R i = g) at iteration t -1 and at iteration t for all documents i and relevance grades g is less than or equal to 0.01.</p><p>After applying the steps above, we created ranked lists by sorted the documents by their probability of relevance, i.e. Pr(R i = r). Additionally, all documents whose probability of relevance is greater than 0.5 is marked as relevant. Separate from pair assessments runs using mechanical turk, we produced a run using our nugget technology <ref type="bibr" coords="8,104.94,135.93,9.96,8.74" target="#b8">[9]</ref>. We use a framework of mutual, iterative reinforcement between nuggets and documents, most similar to Active Learning <ref type="bibr" coords="8,212.31,159.84,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="8,230.74,159.84,7.75,8.74" target="#b1">2]</ref> mechanisms. The human feedback (relevance assessment) is given iteratively on the documents/data; thus, while the assessor judges the document (as it is standard IR procedure), our "back-end" system infers the relevance of unjudged documents as well as the relevance of text fragments, or nuggets, from within the documents. Figure <ref type="figure" coords="8,118.67,244.46,4.98,8.74" target="#fig_1">3</ref> illustrates this framework, with MATCHING being the reinforcing procedure from documents to nuggets and vice-versa:</p><p>• the iterative loop selects documents based on the current notion of relevance (as expressed by current set of nuggets GREL); documents are judged and put into the set of documents QREL;</p><p>• for relevant documents, nuggets are extracted and added to GREL</p><p>• all nugget scores are updated based on the relevant and non-relevant documents judged so far and how well each nugget matches them This framework uses four components that can be designed somewhat independently. Our implementa-Figure <ref type="figure" coords="8,104.32,651.86,3.87,8.74">4</ref>: The nugget extraction interface, on query "Three Gorges Project", showing a document to be judged (left) and the list of candidate nuggets (right) tions for SELECTION, MATCHING, and EXTRACTION are presented in <ref type="bibr" coords="8,401.20,690.15,9.96,8.74" target="#b8">[9]</ref>, noting that for each of these we settled on these methods after trying various ideas. While ours work well, each can be independently replaced by more suited techniques for specific tasks.</p><p>A fifth component, not explored in this paper, is the JUDGE model. Here we assume the judge follows the traditional NIST assessor, simply labeling binary documents as relevant (contains something of some relevance) or non-relevant; a more sophisticated judge can review and modify the nuggets extracted, categorize documents, use graded relevance, annotate important keywords, or classify the query.</p><p>The supervised aspect of the nugget extraction process is shown in Figure <ref type="figure" coords="9,420.86,159.84,3.87,8.74">4</ref>. Here the user views the next document to be judged as well as the current relevance beliefs of all extracted nuggets. One aspect of the process added to our previous work is the ability to mark nuggets as relevant and nonrelevant. This is achieved through the buttons next to each nugget on the right side of the interface. While not necessary, if a user judges a nugget, we can integrate this information into our update system.</p><p>Every nugget has a score that is a combination of the relevance judgments of all matching judged documents. A matching relevant document increases the score, and a nonrelevant document decreases the score. Therefore, if a nugget is marked relevant, we set its score to the maximum of all unjudged nuggets, and if a nugget is marked nonrelevant, it is given the minimum score of all unjudged nuggets. As described in our previous paper, these scores are then used to update the relevance belief of unjudged documents, and to then select the next documents for judgment. Finally, when the process stops (usually up to the assessor), to obtain binary relevance judgments from document scores, we use a threshold of 0.8.</p><p>This resulted in a low amount of manual work per query, as a small percentage of documents were judged and the relevance of the rest were extrapolated. The entire nugget extraction process took about one hour per query and was performed by a grad student. While only one person was used, the methodology could utilize judgments from multiple workers to reduce user error. We performed our experiments with one person to demonstrate the small amount of manual work required to infer relevance grades for the entire set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Following the conventions of the TREC Crowdsourcing track, we evaluate binary preference judgements using both Logistic Average Misclassification (LAM) and the area under the ROC curve (AUC). The evaluation results of our runs submitted to TREC are given in Table <ref type="table" coords="9,327.76,429.63,3.87,8.74" target="#tab_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper we have described our work based on three approaches for obtaining high quality relevance judgements. The first two approaches are based on crowd sourcing relevance judgements using preferences, whereas the third approach is based on nuggets assessments. Elo runs use partial preference judgements obtained from crowd workers as input to a popular rating system that outputs a rank list of documents with their relevance scores. Our preliminary results submitted to TREC crowd sourcing track are close to average results submitted to TREC by various systems for most of the topics. We later have experimented with some modifications to out basic Elo rating system, and collected more crowdsourced pair assessments, which resulted in significant improvement to our reported TREC 2012 results. For our future work, we plan to design more intelligent matches between documents, with a goal to obtain high quality judgements from small number of preference judgements.</p><p>The EM run is a straight forward application of Expectation Maximization algorithm on crowdsourced assessments, as other researchers have done, in order to account for worker quality. We use it as a baseline comparison, but we think it can be most useful as an intermediary processing stage of crowd workers assessments, before running other methods like Elo. One of the Elo enhancements mentioned above is to run it over EM-processed pair assessments.</p><p>The nuggets run is very different than the others, to the point that "crowdsourcing" might not be an appropriate name: the focus here is not on cheap weak judgments that can be averaged somehow into a good assessment, but rather on very few judgments of high quality (on nuggets) and some expert feedback on documents. We note that the nuggets run did best among the runs we submitted; while not a lot better, the human effort put in to it, even at expert level, is significantly less than either crowdsourcing or TREC assessing efforts. Unfortunately, later analysis showed that our "expert" grad student performing the assessments disagreed significantly, perhaps mistakenly, with the existing TREC qrel assessments, thus certainly impacting the run performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,160.27,335.36,291.47,8.74;3,189.00,108.86,234.01,211.39"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Instructions for preference judgements for crowd workers</figDesc><graphic coords="3,189.00,108.86,234.01,211.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,306.00,184.59,234.00,8.74;8,306.00,196.55,234.00,8.74;8,306.00,208.50,171.16,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall design of assessment process: Iteratively, documents are selected and assessed, and nuggets are extracted and [re]weighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,72.00,108.86,467.99,229.42"><head></head><label></label><figDesc></figDesc><graphic coords="4,72.00,108.86,467.99,229.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,85.81,429.63,440.38,215.06"><head>Table 1 :</head><label>1</label><figDesc>. Evaluation results of runs submitted to TREC 2012 Crowd Sourcing track.</figDesc><table coords="9,85.81,451.89,440.38,170.93"><row><cell>Topic</cell><cell cols="2">NEUElo2</cell><cell cols="2">NEUElo3</cell><cell cols="2">NEUElo4</cell><cell>NEUElo5</cell><cell>NEUEM1</cell><cell>NEUNuggets12</cell></row><row><cell>ID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">AUC LAM AUC LAM AUC LAM AUC LAM AUC LAM AUC LAM</cell></row><row><cell>411</cell><cell cols="3">0.665 0.293 0.69</cell><cell>0.11</cell><cell cols="3">0.693 0.145 0.692 0.171 0.650 0.383 0.860 0.121</cell></row><row><cell>416</cell><cell cols="7">0.735 0.265 0.745 0.209 0.854 0.183 0.855 0.162 0.623 0.432 0.868 0.220</cell></row><row><cell>417</cell><cell cols="7">0.678 0.261 0.695 0.195 0.716 0.182 0.716 0.156 0.728 0.196 0.686 0.132</cell></row><row><cell>420</cell><cell cols="7">0.579 0.285 0.598 0.134 0.642 0.153 0.639 0.151 0.524 0.464 0.865 0.157</cell></row><row><cell>427</cell><cell cols="7">0.747 0.146 0.756 0.134 0.788 0.122 0.788 0.12</cell><cell>0.605 0.260 0.798 0.191</cell></row><row><cell>432</cell><cell cols="2">0.598 0.38</cell><cell cols="4">0.598 0.256 0.631 0.38</cell><cell>0.631 0.38</cell><cell>0.624 0.405 0.580 0.375</cell></row><row><cell>438</cell><cell cols="4">0.578 0.272 0.583 0.38</cell><cell>0.62</cell><cell cols="2">0.319 0.617 0.311 0.578 0.154 0.762 0.447</cell></row><row><cell>445</cell><cell cols="4">0.775 0.232 0.783 0.12</cell><cell cols="3">0.706 0.144 0.706 0.212 0.691 0.452 0.891 0.121</cell></row><row><cell>446</cell><cell cols="3">0.683 0.392 0.72</cell><cell cols="4">0.227 0.729 0.238 0.711 0.156 0.656 0.460 0.707 0.274</cell></row><row><cell>447</cell><cell>0.78</cell><cell cols="6">0.265 0.953 0.012 0.992 0.051 0.953 0.082 0.652 0.186 0.464 0.081</cell></row><row><cell>All</cell><cell cols="7">0.682 0.279 0.712 0.178 0.737 0.192 0.731 0.19</cell><cell>0.633 0.339 0.748 0.213</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="7,96.21,698.37,257.69,12.62"><p>Nuggets-based run "NEUNugget12"</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,92.48,405.55,223.35,9.02" xml:id="b0">
	<monogr>
		<ptr target="http://www.mturk.com" />
		<title level="m" coord="10,92.48,405.55,107.29,8.74">Amazon mechanical turk</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,424.69,447.52,8.74;10,92.48,436.64,116.12,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,253.97,424.69,183.37,8.74">Online choice of active learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Luz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,445.85,424.69,89.91,8.74">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2004-12">December 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,455.78,447.52,8.74;10,92.48,467.73,447.52,8.74;10,92.48,479.69,255.65,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,489.31,455.78,50.69,8.74;10,92.48,467.73,118.74,8.74">Learning to rank using gradient descent</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,232.83,467.73,307.17,8.74;10,92.48,479.69,41.09,8.74">Proceedings of the 22nd international conference on Machine learning, ICML &apos;05</title>
		<meeting>the 22nd international conference on Machine learning, ICML &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,498.82,447.52,8.74;10,92.48,510.78,53.68,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,401.51,498.82,58.25,8.74">Here or there</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,484.20,498.82,22.21,8.74">ECIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,529.92,447.52,8.74;10,92.48,541.87,49.26,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,185.08,529.92,203.49,8.74">The Rating of Chess Players, Past and Present</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sloan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Arco Publishing</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,561.01,447.52,8.74;10,92.48,572.96,247.18,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,333.98,561.01,206.02,8.74;10,92.48,572.96,46.52,8.74">An efficient boosting algorithm for combining preferences</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,147.65,572.96,90.49,8.74">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="933" to="969" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,592.10,447.52,8.74;10,92.48,604.05,447.52,8.74;10,92.48,616.01,447.52,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,382.84,592.10,157.16,8.74;10,92.48,604.05,206.94,8.74">On aggregating labels from multiple crowd workers to infer relevance of documents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Milić-Frayling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vinay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,326.31,604.05,213.69,8.74;10,92.48,616.01,190.73,8.74">Proceedings of the 34th European conference on Advances in Information Retrieval, ECIR&apos;12</title>
		<meeting>the 34th European conference on Advances in Information Retrieval, ECIR&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="182" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,635.15,447.52,8.74;10,92.48,647.10,447.52,8.74;10,92.48,659.06,447.52,8.74;10,92.48,671.01,149.19,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,350.43,635.15,189.57,8.74;10,92.48,647.10,231.67,8.74">Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Milic-Frayling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,349.18,647.10,190.81,8.74;10,92.48,659.06,374.03,8.74">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR &apos;11</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,690.15,447.52,8.74;10,92.48,702.10,447.52,8.74;10,92.48,714.06,371.76,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,360.56,690.15,179.43,8.74;10,92.48,702.10,235.37,8.74">Constructing test collections by inferring document relevance via extracted relevant information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ekstrand-Abueg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,348.91,702.10,191.09,8.74;10,92.48,714.06,177.92,8.74">Proceedings of the 21st ACM Conference on Information and Knowledge Management</title>
		<meeting>the 21st ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012-10">October 2012</date>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,92.48,112.02,447.52,8.74;11,92.48,123.98,304.58,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,293.25,112.02,87.43,8.74">Query by committee</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,402.48,112.02,137.52,8.74;11,92.48,123.98,274.68,8.74">Proceedings of the Fifth Annual AC¡ Conference on Computational Learning Theory, COLT &apos;92</title>
		<meeting>the Fifth Annual AC¡ Conference on Computational Learning Theory, COLT &apos;92</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
