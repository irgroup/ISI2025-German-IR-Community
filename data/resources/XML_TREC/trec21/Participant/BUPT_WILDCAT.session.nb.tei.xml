<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.54,82.83,286.41,14.26">BUPT_PRIS at TREC 2012 Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,164.26,107.14,67.88,10.69"><forename type="first">Chuang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.34,107.14,68.51,10.69"><forename type="first">Xiaotian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.79,107.14,58.77,10.69"><forename type="first">Songlin</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.52,107.14,44.30,10.69"><forename type="first">Runze</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.54,82.83,286.41,14.26">BUPT_PRIS at TREC 2012 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BDB3557915C4DC346F1EEB6E0911DB4F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce our experiments carried out at TREC 2012 session track. Based on the work of our group in TREC 2011 session track, we propose several methods to improve the retrieval performance by considering the user behavior information over the session, which includes use query expansion based on meta data, query expansion based on click order, optimization based on history ranked lists and so on. The results show that some methods can really improve the search performance and some methods need to be optimized.</p><p>User behavior model can do the query expansion and term weighting by considering the users' behavior in the session <ref type="bibr" coords="2,188.40,735.53,11.14,9.40" target="#b0">[2]</ref>. The detail process is shown as follows. Assume q i = (t 1 , t 2 , … t n ) is the i th query in one search session, and t j is the j th term of q i .S i</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The TREC Session track ran for the third time in this year, and its goal for this year is to test whether systems can improve their performance for a given query by using previous queries and user interactions with the retrieval system (including clicks on ranked results, dwell times, etc.) <ref type="bibr" coords="1,493.15,421.46,11.14,9.40">[1]</ref>. Based the sessions, there are four tasks in TREC 2011 session track: run the retrieval system: RL1: only using the current query. RL2: using the current query and the set of past queries in the session. RL3: using the current query, the set of past queries in the session and the ranked lists of URLs RL4: using the current query, the set of past queries in the session, the ranked lists of URLs, the clicked URLs and the time spent on the clicked documents. RL1 retrieval effectiveness is viewed as the basic standard. By comparing RL1 with the effectiveness of RL2, RL3, RL4, I can evaluate whether the retrieval system can use previous queries and user interactions to improve the search performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experiment setup</head><p>In our experiment, we choose Category B comprising 50 million documents as the search dataset. Indri search is the search engine for the search process in our experiment. I ndri search service for ClueWeb09 collection is available on the web. The service enables the user to submit the queries and obtain top documents returned by Indri search engine. Query expansion and term weighting can be applied in Indri search. Spam Rankings data provided by University of Waterloo include the spam scores of the web pages in ClueWeb09 collection. In our experiment, the web pages with spam score less than 40 are viewed as spam and filtered out from the final search results. We use spam ranking filter to do this represents the set of history queries of the i th user behavior. Thus, S 1 = { q i }, 𝑆 2 = 𝑆 1 ∪ q 2 , and S 𝑖-1 = 𝑆 𝑖-2 ∪ q i-1 = (𝑡 1 ′ , 𝑡 2 ′ , … , 𝑡 𝑚 ′ ) . The query expansion and term weighting is realized in the following process.</p><p>1. The weight of term t j is set to 1 n for the new query q i = (t 1 , t 2 , … t n ).And</p><formula xml:id="formula_0" coords="3,443.42,125.57,38.21,19.46">1 n = 1 n i=1</formula><p>.</p><p>2. (e 1 , e 2 , … e m ) is the weight vector of the terms for the history query set 𝑆 𝑖-1 = (𝑡 1 ′ , 𝑡 2 ′ , … , 𝑡 𝑚 ′ ).</p><p>And e i = 1 m i=1 . 3. The query set is expanded as S 𝑖 = 𝑆 𝑖 -1 ∪ q i and the normalized expanded term weights :</p><formula xml:id="formula_1" coords="3,237.02,202.66,121.78,36.36">d e i + (1 -d) 1 n = 1 n i=1 m i=1</formula><p>d is the attenuation factor，and d &lt; 0.5.I choose 0.4 as the attenuation factor in my project. 4. Assume there are k terms appearing both in the new query and in the previous query:</p><formula xml:id="formula_2" coords="3,177.24,277.11,259.41,13.41">S i-1 ∩ q i = 𝑡 1 ′ , 𝑡 2 ′ , … 𝑡 𝑘 ′ = t 1 , t 2 , … t k k ≤ m and k ≤ n</formula><p>The query set 𝑆 𝑖 is expanded as</p><formula xml:id="formula_3" coords="3,249.62,293.31,222.28,13.43">∶ 𝑆 𝑖 = S i-1 ∪ {q i } = (𝑡 1 ′ … 𝑡 𝑘 ′ , 𝑡 𝑘 +1 ′ … 𝑡 𝑚 ′ , t k+1 … t n )</formula><p>Finally, the term weighs (𝑒 1 ′ , 𝑒 2 ′ , … , 𝑒 𝑚 ′ ) are assigned as the following functions show</p><formula xml:id="formula_4" coords="3,234.14,327.62,142.01,52.23">𝑒 𝑖 ′ = de i + 1 -d 1 n i ∈ 1, k de i i ∈ k + 1, m 1 -d 1 n i ∈ m + 1, n</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Anchor log model</head><p>University of Essex developed a method for extracting useful terms and phrases to expand the reformulated query in Session Track 2010 <ref type="bibr" coords="3,273.77,457.10,10.87,9.40" target="#b1">[3]</ref>. Based on that, we modified it to adapt to the new requirements.</p><p>We retrieve the anchor texts of the documents in ranked lists for past queries. And we extract the top ten terms to expand the query terms. The weight of original query terms is set to 0.7 and the terms from anchor log has the weight value of 0.3. After the stop word filtering, query is expanded in the following form: #combine( 0.7#combine r c 0.3#combine e 1 e 2 … e 10 ) r c is the current query and e j is the j th anchor log expansion term. Finally, submit the expanded query to Indri search to get the new search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.Optimization based on history ranked lists</head><p>In Session Track 2010, University of Lugano proposed a method of generating optimized ranked list of current query by the rank of documents in ranked list of current query and only one past query <ref type="bibr" coords="3,117.07,713.18,11.14,9.40" target="#b2">[4]</ref>. Based on it we developed one improved method to do n-1 iterative procedures for n-1 past queries and the current query. Finally we can get the scores of optimized result lists and sorting them in ascending order: Assume the returned ranked lists for the past queries are RL 1 , RL 2 , RL 3 …RL n , where RL n is the ranked list of the last past query. The ranked list we need to calculate for the current query is RL n+1 . We can re-rank the documents in ranked list of RL n+1 by considering the ranked lists of RL 1 ,RL 2 …RL n. The final ranked list current query is denoted as FinalRL.</p><p>If there are some past queries: There are ranked lists RL1, RL2, RL3…RLn for past query we need to calculate.</p><p>For any document in RL1 and RL2 denoted as document i:</p><formula xml:id="formula_5" coords="4,108.07,170.06,395.39,40.75">If document i of RL2 appears in RL1, score[i] = 1/rl2[i] + 0.2(1/rl2[i] -1/rl1[i]); If document i of RL2 does not appear in RL1, score[i] = 1/rl2[i]; If document i of RL1 does not appear in RL2, score[i] = -1.</formula><p>We get the ranked list TEMP1-2 according to the score in descending order.</p><p>Then we do the iteration until we get the score of the ranked list for current query. Finally, we can get the ranked list of FinalRL according to the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.Query expansion based on meta data</head><p>This method uses the meta tags in the documents of ranked lists for past queries to do the query expansion. We collect the terms in "keyword" and "description" data in meta tag. Then we extract the top 10 terms with highest frequency to expand the query. The query becomes #combine( (1 -d)#combine r c d#combine e 1 e 2 … e 10 ) r c is the current query and e j is the j th meta data expansion term. d is the attenuation factor which is between 0 and 1. We use the data of TREC 2011 session track to test which value assigned to d can achieve the best performance. By using session data and the relevance judgments for TREC 2011 session track, we find that when d = 0.5 the search results achieve the highest relevance score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.Query expansion based on click order</head><p>We think the click order can reflect the attraction of the documents titles to the user. In this model we use the clicked titles to query expansion by considering the click order. 1. The term weight of current query is set to w current , so (1-w current ) is assigned to the expanded terms.</p><p>2. Assume that there are n history queries. RL1, RL2...RLn are the ranked lists of the history queries.</p><p>Assume the user click m titles, denoted as (RL, click order k, title), such as: In our experiment, we set w current = 0.5.</p><formula xml:id="formula_6" coords="4,110.95,666.36,42.27,24.90">1, 1, title1 1, 2, title2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.6.User behavior model considering the attention time</head><p>The attention time of the clicked documents can reflect the usefulness of the information in the document as conceived by the user. Songhua Xu etc <ref type="bibr" coords="5,321.97,254.35,12.57,9.40" target="#b3">[5]</ref> proposed the attention time prediction algorithm in 2008. Based on that, we build the model using the dwelling time of the clicked documents to calculate the documents relevance level and re-rank the documents.</p><p>1. For the k th Clicked document C ik in session i ,t inter represents the dwelling time interval. t offset represents the time offset. t att denotes the attention time on the document.</p><formula xml:id="formula_7" coords="5,108.07,337.26,270.54,73.72">t inter (C ik ) = t end -t start * d c t offset (C ik ) = 2exp (-d * rank (C ik )) 1+exp (-d * rank (C ik )) t att (C ik ) = t inter (C ik )t offset (C ik ) rank(C ik )</formula><p>is the rank number of the k th Clicked document in session i. We set the control parameter d c = 0.1 to make the interval small, and d= 0.2 controlling the drop off.</p><p>2. The j th document attention time in prediction is calculated :</p><formula xml:id="formula_8" coords="5,241.70,461.90,144.11,11.96">t predict = sim d ij , C ik * t att (C ik )</formula><p>And re-rank the documents considering the prediction attention time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.7.Query expansion based on clicked titles and snippets</head><p>This method uses the titles and snippets for the clicked web pages for past queries to do the query expansion. We collect the terms of titles and snippets data in clicked web pages. Then we extract the top 30 terms to expand the query. The terms of current query have weight 0.6, and the terms extracted from titles and snippets have weight 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>This year we submit three runs for the four tasks (RL1, RL2, RL3, RL4). Table2 shows the relevance scores of our runs with ndcg@10 and nerr@10. Different from Session 2011, relevance for TREC 2012 session track is defined against the entire topic and not against different subtopics. By analyzing the results, we have the following findings: 1. For wildcat1 and wildcat2, the relevance score of RL2 is less than that of RL1. We use user behavior model in RL2, which improved the search performance efficiently for 2011 session track, so user behavior model does not work well for the session data of this year. For the session data of this year, many topics contain more than one session. We think this change may be one reason for the bad performance of user behavior model. We may need to do some adjustment to make this method fit the session data of this year better. 2. For all the three runs scores of RL3 and RL4 are higher than those of RL1 and RL2. This indicates that by considering the previous interactions in session the system can improve the search performance. 3. wildcat1.RL3 and wildcat3.RL3, where we use anchor log model and query expansion based on meta data, achieve high scores. This indicates that anchor log data and meta data of the previous ranked lists are useful for system to predict the intention of the users. 4. wildcat2.RL1 and wildcat2.RL2, where we use PageRank score to re-rank the search results, perform badly. This situation may indicate that re-ranking the results by only considering PageRank scores are not appropriate for the tasks of session track. The relevance between the results and the user intention also should be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The scores of Query expansion based on clicked titles and snippets in wildcat2.RL4 and</head><p>Query expansion based on click order wildcat3.RL4 in are lower than the scores of RL3. This indicates that we do not use the click data very efficiently. These two methods can improve search performance, but they need to be optimized in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,110.95,697.60,20.88,9.49;4,110.95,713.18,45.67,10.05;4,110.95,728.66,7.80,9.40;4,134.35,728.66,267.96,9.40;4,110.95,744.53,194.40,10.05;4,110.95,760.01,189.98,10.05;5,110.95,76.44,184.59,10.05;5,110.95,91.92,295.75,9.40;5,110.95,107.76,362.04,10.06;5,110.95,123.24,369.46,9.40;5,110.95,138.74,366.58,9.40;5,110.95,154.58,246.41,9.40"><head></head><label></label><figDesc>…… 1, m, title m a) Assign the weight to the current query and the expanded terms : weight of RL k = (1-w current )*(k/1+2+3+4+…n) weight_RL1=(1-w current ) *(1/1+2+3+4+…n) weight_RL2=(1-w current )*(2/1+2+3+4+…n) b) Assign the weight of expanded terms to the terms in clicked titles: weight_RL1_title k = weight_RL1 * (m+1-k/1+2+3+4+…m), where k is the click order the weight of title1 in RL1: weight_RL1_title1= weight_RL1 * (m+1-1/1+2+3+4+…m) the weight of title2in RL1: weight_RL1_title2= weight_RL1 * (m+1-2/1+2+3+4+…m) By doing this in turn we can get the weight of all the titles.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,104.83,620.25,304.82,9.40" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,201.72,620.25,203.58,9.40">Research and application of user search behavior</title>
		<author>
			<persName coords=""><forename type="first">Hongtao</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,104.83,635.76,400.36,9.40;6,90.05,651.60,148.72,9.40" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,377.39,635.76,127.80,9.40;6,90.05,651.60,58.71,9.40">Autoadapt at the Session track in TREC 2010</title>
		<author>
			<persName coords=""><forename type="first">M-Dyaa</forename><surname>Albakour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinzhong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Fasli</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of Essex</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.07,667.08,396.64,9.40;6,90.05,682.58,90.07,9.40" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,331.68,667.08,168.26,9.40">University of Lugano at TREC 2010</title>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Keikha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Parvaz</forename><surname>Mahdabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shima</forename><surname>Gerani</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of Lugano</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,104.83,698.42,382.18,9.40;6,90.05,713.90,411.66,9.40;6,90.05,729.38,49.11,9.40" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,337.84,698.42,149.17,9.40;6,90.05,713.90,170.68,9.40">A User-Oriented Webpage Ranking Algorithm Based on User Attention Time</title>
		<author>
			<persName coords=""><forename type="first">Songhua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lau</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Zhejiang University, Yale University, The University of Hong Kong</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
