<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.92,71.87,411.88,16.84">CWI at TREC 2012, KBA track and Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,122.18,116.90,72.84,11.06"><forename type="first">Samur</forename><surname>Araujo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<postBox>PO Box 5031</postBox>
									<postCode>2600 GA</postCode>
									<settlement>Delft</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,174.00,176.07,105.70,11.06"><forename type="first">Corrado</forename><surname>Bosscarino</surname></persName>
							<email>corrado@cwi.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<postBox>PO Box 5031</postBox>
									<postCode>2600 GA</postCode>
									<settlement>Delft</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.92,71.87,411.88,16.84">CWI at TREC 2012, KBA track and Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">77F0E9288BDA0A05169678585D19D09A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in two tracks: Knowledge Base Acceleration (KBA) Track and Session Track. In the KBA track, we focused on experimenting with different approaches as it is the first time the track is launched. We experimented with supervised and unsupervised retrieval models. Our supervised approach models include language models and a string-learning system. Our unsupervised approaches include using: 1)DBpedia labels and 2) Google-Cross-Lingual Dictionary (GCLD). While the approach that uses GCLD targets the central and relvant bins, all the rest target the central bin. The GCLD and the string-learning system have outperformed the others in their respective targeted bins. The goal of the Session track submission is to evaluate whether and how a logic framework for representing user interactions with an IR system can be used for improving the approximation of the relevant term distribution that another system that is supposed to have access to the session information will then calculate.</p><p>the documents in the stream corpora. Three out of the seven runs used a Hadoop cluster provide by Sara.nl to process the stream corpora. The other 4 runs used a federated access to the same corpora distributed among 7 workstations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We have participated in two tracks this year: the KBA track and the session track. The paper is orgaized as follows. We specify our submission to the session track in Section 2. In Section 3 we will discuss our approaches, experiments and results with respect to the KBA track. We conclude the paper with our observations and findings in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SESSION TRACK</head><p>We consider a TREC search session to mainly provide evidence of a learning process. A task, as specified in the narrative, requires a user to interact with a search system for learning about the topic and about the modes of interaction with that particular system that can lead to task accomplishment.</p><p>The goal of this submission is to evaluate whether and how a logic framework for representing user interactions with an IR system can be used for improving the approximation of the relevant term distribution that another system that is supposed to have access to the session information will then calculate.</p><p>We distinguish two separate models. One is a retrieval model that ranks documents in response to the current query. Another model takes care of representing the effect of interactions to how the underlying model ought to rank documents. We use query expansion to connect the output of the interaction model to the input of the IR model: the interaction model provides therefore a set of expansion terms for the query that the user issued at the last step of the recorded session.</p><p>We use a variation of Probabilistic Dynamic Epistemic Logic (PDEL) <ref type="bibr" coords="1,316.81,398.94,10.45,7.77" target="#b8">[8]</ref> to represent user interactions as sentences interpreted on a Kripke structure. Within this approach, actions may change the model in more complex ways than state elimination only, hence the resulting probability update generally differs from Bayesian update. The underlying IR model is a classic language model implemented on Indri with standard parameters, emphasising the contribution of the interaction model to the overall performance.</p><p>The probabilities at the exit states of the model represent, for each agent, a subjective probabilistic statement: we hypothesize that a mixture of these probability metrics outperform current IR models with relevance feedback. We evaluated this hypothesis on the session track data.</p><p>We briefly introduce the model and some background literature. It is outside the scope of this document to work out all the technical details of this approach, which is only presented here as a motivation for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval Model</head><p>Besides the most basic way to incorporate new evidence into an existing probabilistic model, that is conditional probability, there are some alternatives such as using Dempster-Shafer theory <ref type="bibr" coords="1,535.43,630.72,10.45,7.77">[5]</ref> or cross-entropy <ref type="bibr" coords="1,367.74,640.76,9.52,7.77" target="#b4">[4]</ref>. A model of a retrieval situation with PDEL contains two separate parts, one epistemic model that accomodates the deterministic information about the interactions and one pure probabilistic model.</p><p>A general case of such a mixed retrieval model is a relevance model as in <ref type="bibr" coords="1,336.31,701.43,10.45,7.77" target="#b3">[3]</ref> with a PDEL model built on top. Next to a frequentist or bayesian interpretation we introduce then an alternative interpreta-tion of the relevance model in <ref type="bibr" coords="2,160.23,57.92,9.52,7.77" target="#b3">[3]</ref>, that is the probability distribution over a vocabulary conditioned on the observation of a sample from the relevant population. Both the interpretations that Lavrenko puts forth in <ref type="bibr" coords="2,84.94,88.05,10.45,7.77" target="#b3">[3]</ref> assume a generative process that repeatedly samples terms from the relevant population. In the logical interpretation we extend the generative process by specifying that observations are performed in a certain order, with a well defined structure as determined by the protocol used to create the dataset.</p><p>According to this view, terms from clicked document can be distinguished from query terms and the end system can explore the model parameters θ more effectively in an attempt of estimating the relevant parameter setting θR. We still probabilistically evaluate an uncertainty about relevance, but now we take into account deterministic information such as the observed interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Adding interactions to a relevance model</head><p>Assuming that probabilistic statements about individual cases (for example the probability of observing a certain term) depend on the knowledge state of an agent who estimates this probability <ref type="bibr" coords="2,266.88,255.82,9.52,7.77" target="#b2">[2]</ref>, the main issue that we address in the experiments is: how can we modify existing IR models such as in <ref type="bibr" coords="2,174.86,275.90,10.45,7.77" target="#b3">[3]</ref> in order to accomodate a formalisation of these epistemic states as "observed" (with uncertainty modeled in PDEL) during a search session? We want to formalise the intuition that observing a term as a query, in a document from a result list or in a clicked document should result in different representations in the total model. We make the simplifying assumption that we can keep the representation of documents and queries, the notion of relevance as well as the distribution over documents and queries as it is in the standard account <ref type="bibr" coords="2,84.16,376.74,9.52,7.77" target="#b3">[3]</ref>. We only assume that, next to query's and document's transforming functions D(x) and Q(x) for every x in the full representation space S, there is a similar function A(x) that transforms elements of S into well formed sentences of the PDEL language LP EL. This transforming function represents another possible reduction of the full representation space: next to documents and queries, a suitable subset of the space S represents actions that a user might perform during a search session. Since we still want the transforming functions, Q, D and now the freshly introduced A to be functionally similar to each other, we get the additional requirement that queries and documents must be regular sentences φ of the PDEL language as well. In this case the probability of observing an interaction expressed with the formula φ, becomes:</p><formula xml:id="formula_0" coords="2,53.80,530.33,289.31,32.71">P (φ) = Z Θ n Y a=1 0 @ X x ia δ(da, xi a )P ia,θ (xi a ) 1 A m Y b=n+1 0 @ X x i b P i b ,θ (xi b ) 1 A p(θ)dθ</formula><p>which is the same as <ref type="bibr" coords="2,134.41,570.04,36.87,7.77">[3, eq.3.4]</ref>, where the argument of the first product operator are the dimensions of the full representation space that are retained by A.</p><p>Under this assumption a modification of the generative process that produces an alternative distribution P (•|r) for the relevant population, provided that it outputs term probabilities in the usual way, still admits standard ranking approaches such as PRP or KL divergence based methods.</p><p>But, even if we make plausible that only the relevance model needs to be modified, what additional structures from <ref type="bibr" coords="2,232.25,681.35,10.45,7.77" target="#b8">[8]</ref> do we need? The task here is to do something similar to <ref type="bibr" coords="2,216.16,691.39,36.70,7.77">[3, eq.3.9]</ref>, where an initial estimate P (θ) was updated with the observation that r by means of Bayes' rule, as p(θ|r) = P (r1, r2, . . . , rm|θ)p(θ) P (r1, r2, . . . , rm) .</p><p>In our case, however, we need a different update strategy that makes sense of the structured observations. We do not want to simply assume that we sample r = r1, r2, . . . , rm from the relevant population. We want also to take into account that r was the product of a certain user strategy. We also might have some knowledge of the cognitive process behind this strategy or of the presence of at least two agents, a human user who has been instructed to search (and that we know the rules in terms of a narrative or an experimental protocol) and a retrieval system which can be similar to the system that we use for our experiments. This rich structure of session records is not accounted for in a classic retrieval model.</p><p>We expect that the potential performance enhancement upon exploiting this structured information about a search process pays back for the added complexity that we must introduce in the relevance model. The major source of this additional complexity will appear to be the dependence between probability distributions and epistemic states: whereas, see for example <ref type="bibr" coords="2,473.44,279.93,36.06,7.77">[3, eq.5.5]</ref>, in the standard generative model we need only one distribution over each representation, queries or documents, when we include actions, distributions will prolify, since we will generate one distribution for each agent in each possible epistemic state. This complexity, however, only reflects the complexity of the structure that comes with our observation. Standard relevance models are a distribution over the vocabulary, conditioned on the observation of a string: it is not surprising that if we want to introduce a probabilistic update upon observation of what we assume is a much more complicated process, the complexity of the formalism will inevitably increase.</p><p>At a minimum we need a probabilistic epistemic language LP EL to express sentences about what agents believe, as on that depends their probability assignments. We also need a semantic to interpret these sentences and an update frame that, once combined with the prior model, outputs a new epistemic model with posterior probabilities. Hopefully the posterior distribution p(θ|φ) so obtained is a better estimation of the unknown parameter θR than that from <ref type="bibr" coords="2,546.20,461.11,9.71,7.77;2,316.81,471.15,23.75,7.77">[3, eq.3.9]</ref>.</p><p>In text retrieval, the set of primitive proposition that agents are supposed to reason about are of the type 'the relative frequency of term a in the relevant population is f (a)'. We assume that there is a search process that, after a certain number of steps which depend on a user's skills, will generate the relevant population SR; an optimally skilled user is someone who successfully pursues this path.</p><p>In this case we do not need to represent sessions' interactions because the final query will generate the best possible ranking. However, while we do not have these idealised optimal users, the search skills of our users still improve during a search session.</p><p>We are interested in the probabilities that users assign at different stages of this learning process: this is the relevance feedback that our users indirectly provide to the system. We want also to provide a uniform account of actions as if they were documents and queries.</p><p>In more formal terms, we are interested in the probability of the denotation of a formula in the language LP EL that represents the result of the observed learning process. These are probabilities over the full event space E, that is a σ-algebra over the full representation space S: E ⊆ ℘(S)</p><p>We propose an epistemic alternative to the pure probabilistic update <ref type="bibr" coords="3,53.80,57.92,32.95,7.77">[3, eq.3.9</ref>] by first giving an epistemic interpretation of eq. 3.9, next to its frequentist and bayesian versions. Eq. 3.9 defines an update function P (•|r) for an agent who observes the statement that r, which is a true statement so that P (r|r) = 1. We can represent a prior epistemic status as a network of states that are singled out by the truth values of different propositions at those states: accessibility relations connect states into an oriented graph representing what an agent believes to be true. In a typical IR application, for example, we might think of states as labeled by different configurations of a distribution parameter θ; whereas in standard epistemic logic there is a focus on factual knowledge, the world that is actually the case, in IR the focus will be on relevance.</p><p>Applying eq. 3.9 according to this interpretation amounts then to define a distribution µ over the set S of all possible epistemic states, such that a sum of µ over all states s ∈ S is defined to be 1, and a discrete update function that maps a distribution before the observation that r to a distribution after that observation, as:</p><formula xml:id="formula_1" coords="3,96.59,262.14,152.33,26.51">µ (s) = ( µ(s) • " 1 + P (¬r) P (r) " , if s r 0, if s r</formula><p>where P (r) = P {s∈S|s|=r} µ(s) is the confidence in the observation that r. The update from eq. 3.9 amounts then to eliminating all the epistemic states where the observed r does not hold and renormalising the distribution over the remaining states in the same proportions as before the observation. This very same feature of updating with conditional probabilities was already questioned in other logical approaches to IR such as Logical Imaging <ref type="bibr" coords="3,115.10,379.98,10.45,7.77" target="#b1">[1]</ref> and its quantum IR version <ref type="bibr" coords="3,225.66,379.98,9.52,7.77" target="#b9">[9]</ref>. In the imaging approach renormalisation after an update was performed by considering that the probability mass taken away from ¬r states should be assigned only to states similar to the removed states, under some suitable similarity measure.</p><p>The alternative update based on PDEL considers the uncertainty in the learning process or observation probabilities <ref type="bibr" coords="3,227.59,450.69,10.45,7.77" target="#b8">[8]</ref> so that we cannot say that r is definitely a token of relevance. In our approach we also agree that renormalisation should not be uniform as in standard update, and in our contribution we aim at deriving from user models and observed interactions a set of occurrence probabilites for events, and use them to perform the update more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TREC evaluation</head><p>The retrieval model introduced in the previous section extends conditional probability with an account of how a user's interactions with a system changes the probability model. Information about interactions, in our case the session information, which belongs to each of the 4 TREC tasks, determine the space upon which the final system computes a standard conditional probability.</p><p>At the first step, before any interaction, the PDEL model contains only an initial distribution on the parameter space. This model accounts for the prior probability, one of the three kind of probabilities that we want to model.</p><p>To a second type of probability belong the occurrence probabilities for events. While in a real life situations <ref type="bibr" coords="3,198.28,671.30,10.45,7.77" target="#b6">[6]</ref> this probability reflects our knowledge of how information needs arise, in the TREC setting this probabilities model our knowledge of the experimental setting, that is represented in the procedure used to create the dataset. The main component of this experimental protocol is a narrative, a set of sentences designed to guide the user in finding relevant documents for the topic.</p><p>Occurrence probabilities for events are taken into account by means of a set Φ of preconditions and a function PRE that assigns to each element of Φ a probability distribution over the set of events E. The PRE function determines the states of the updated model, allowing only states where a positive probability is assigned to the event at a state of the prior model. For example an updated model S will contain a state (s , e) iff there is a state s = θm labeled with a certain parameter θm in the prior model S and there is an event e ∈ E such that PRE(s, e) &gt; 0. In the TREC application we consider only cooperative users: we only allow query events that are relevant to the global narrative, given the knowledge represented at each state.</p><p>For each distribution θs at state s we consider the difference under Kolmogorov-Smirnov test between θs and all the distributions θ , . . . , θ m obtained by considering each sentence of the narrative to be a sample from the relevant population. The cardinality of the event set is equal to the cardinality of the set of narrative sentences. We then compare the effect of updating each distribution pair with the observed query: we define relevance as the property of reducing the gap between each distribution pair compared with a uninformative sentence represented by a sample from the background distribution. Therefore the PRE function will be positive if the difference exceeds a treshold that depends on the background probability.</p><p>We can interpret the preconditions in terms of the relevance model in <ref type="bibr" coords="3,326.84,360.45,43.07,7.77">[3, eq. 3.8]</ref> if we consider that each state is a point of the integration range Θ. The integral can be splitted in subintervals and multiplied by a constant before normalisation: some intervals of possible parameters can be eliminated if the constant is zero.</p><p>A third component of the updated model is the set of uncertainty relations between events, which models the observation probabilities. At the first step, before any query refinement, we take our best guess for these probabilities to be the effect, again compared to that of a sample of the background probabilities, of updating a pair of narrative distributions with the observed query. Notice that the model does not depend on a choice of the similarity test: we can use any estimation of the similarity between the ranked lists that an IR system would produce by exchanging the two probability distributions. The rationale is that if a system cannot produce different ranked lists when different prior models are updated with the a relevant sample, either the query is not discriminative enough or the collection does not contain enough information to effectively handle the query: in both cases this is a metric for the uncertainty about which distribution is the most appropriate to generate the relevant population.</p><p>The normalised product of these three probabilities yields the prior probabilities of the relevant model at the following step, that is the probabilistic uncertainties between the new states, now labeled with the content of the previous state (a distribution parameter) and the action taken (a query interpreted in one of the possible senses according to the narrative).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">RL1 task: prior distribution</head><p>The prior model has n states, each labeled with one distribution parameter and probability Pt syst,s = 1 n . That is, from the test system perspective, each parameter is a priori an equally probable generator for the relevant population. We estimate the distributions at each state using the narrative: we use each sentence as a query and we retrieve kp documents. We tested this approach on 2011 data and we found out that retrieving more than 25 documents ensures that the estimated distributions are not too similar to each other. Each retrieved list determines a distribution over the entire session vocabulary, that is over all the terms that have nonzero probability at least in one state distribution. In order to avoid zero probabilities in a state distribution, we mix the estimated distribution with the background distribution under a mixing parameter λ = 0.3. We take the top ep most frequent terms at each state as expansion for the current query at RL1. In TREC 2012 we limited the size of the distribution to kp = 3 and of the query expansions to ep = 5. A weighting schema assigns to the current query a weight equal to the cardinality of the state set, while state weights are inverserly proportional to their normalised distance under K-S measure to the background distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">RL2-4 tasks: update with events</head><p>For the remaining tasks we can use the general form of the update rule in PDEL <ref type="bibr" coords="4,103.86,246.93,9.52,7.77" target="#b8">[8]</ref>, that is: .</p><p>consisting of a re-normalised product of all the probabilities for events.</p><p>The queries q define in RL2 the event set E. Each component of the event set defines a possible sense of the query. We hypothesise that each query sense can be mapped to a distribution associated with the relevance model after having observed the query, restricted to the original prior. As before the preconditions are the distances of each updated distribution to the distributions at the previous step. To estimate the observation probabilities, we first calculate the marginal distance of a state distribution to each other state distribution. This is a measure of the probability of confusing a query sense with another. We derive a total observation probability by summing onto the marginals.</p><p>In the RL3 task we consider the effect that the output of the retrieval system has on a user's epistemic state. We assume that browsing a ranked list amounts to creating an additional state that extends the original prior. Interactions can therefore, not only reweight some state, but also add new ones to the model. We recalculate the entire query update upon a new model, where the distributions, including those of the prior, now range onto a possibly larger session vocabulary.</p><p>In the RL4 task, clicks induce an update similar to queries. We use the top ec most frequent terms from the snippets of the clicked documents to update the entire model as if they were query terms. This is a coarse approximation of using the term distributions from the whole clicked document, but given the limited session vocabulary, we expect that effect on the final rank will be negligible. We also limit the snippet size to ec = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results and discussion</head><p>The left side of the table here below shows the nDCG at k of our proposed model. To the right side we report the median among the TREC participants.</p><p>Clearly this is only a preliminary assessment of our results based exclusively on the evaluation script output provided by the organisers.  These results only allows to evaluate our choice of the prior and its associated weighting scheme for RL1. Our system has been explicilty designed to capture a user's learning process, that is how, starting from a possibly uninformative prior, the probability mass can be redistributed to states associated with the relevant topics. Aspects of the topic that a user learned to be less relevant should be discounted. Under the assumption of a moderately effective user, who learns as a session goes on, high probable states in the model should correspond to the relevant distribution of the current query.</p><p>We interpret the high score on all tasks, combined with a negligible difference between tasks, to the nature of the topics. They require just few interactions to be resolved and test users show a good understanding of what kind of documents might possibly be used to accomplish these tasks. The query update rule improves on the prior, indicating that queries are the most effective mode of interaction, hence a great deal of the uncertainty is in how to issue the correct query terms in this particular system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">KNOWLEDGE BASE ACCELERATION 3.1 Intial ideas on approaches and architecture</head><p>It was stated that 4% of the Wikipedia citations did not mention the Wikipedia entities they are cited by. We thought there could be more documents in the stream that do not mention the Wikipedia entities by name and yet are relevant. Capturing relevant documents that do not mention entities by name was our first interest. At this moment, we wanted to capture two things:</p><p>• Stream documents that do not mention Wikipedia entities by name and yet are relevant(central), and</p><p>• The evolution of an entity as documents that are relevant(central) are found</p><p>To accomodate these two interests, we thought of an approach as in Figure <ref type="figure" coords="4,342.46,670.88,3.36,7.77">3</ref>.1.</p><p>The model could be anything. We thought of a machine-learned model, string similarity, string-matching, or any other. The output was taken as input because we thought the evolution of an entity has changed the entity and so the new representation should be used in the query. The big questions here were</p><p>• How to represent the WP entities and the stream-documents</p><p>• What to add to the representation of the Wikipedia entities when a relevant stream document is found</p><p>• What models, approaches to use Statistics from the few weeks annotation changed the way we see the task. Out of all non-mentioning documents, only 0.4% are relevant, 0% are central. So we did not see, from a performance perspective, a point in concentrating our efforts on detecting nonmentioning-yet-relevant stream documents. Instead, we decided to focus only on mentioning and relevant or/and central. The implication of this is that the two big questions we raised above are no longer important. The reason why the first question is not important is evident. The second question is also the same because for a document to be relevant or central, it will almost always mention the entity, thus no need to update representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">A new challenge and a new approach</head><p>Out of all mentioning documents, 23.8% are garbage, 35.3% are garbage or neutral. Now, the challenge is not how to filter nonmentioning-yet relevant, but how to exclude mentioning-yet-nonrelevant i.e. garbage and neutral. Another challenge is that the entities are ambiguous in the sense that two entities can have the same or nearly the same name, and thus the same representation. This observation informed our next choices of approaches. We thought of approaches that can, at least, solve one of the two problems. Our supervised approaches attempt to solve both problems while our unsupervised ones mainly attempt to solve the ambiguity problem. When we were pondering about solutions to this, we came across a resource called Google-Cross-Lingual dictionary (GCLD). We thought that using the GCLD captures the second problem since it has probabilities for the strings and concepts. We also thought that, in combination with other disambiguation methods, the GCLD could be used to distinguish garbage and neutral from relevant and central.</p><p>Therefore the approach now is exactly like Figure <ref type="figure" coords="5,235.92,556.94,3.36,7.77">3</ref>.1, but without the output going as input. This means to first represent the queries (the Wikipedia entities) in some way and to query the stream. After finding a match of the strings for an entity in the stream, we use the probabilities to give a confidence score. All our approaches revolve around the choices of entity representation, the scoring function to measure confidence and scaling functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Representation</head><p>Representing the Wikipedia entities (the queries) and the stream documents in some way is mandatory. At first, we thought we can represent the streams in terms of n-gram tokens thereby reducing the size of the corpus, but then that would confine us to only some approaches that can consume the tokens. So we left stream documents representation to simple representation during processing. However, we needed to represent the Wikipedia entities in some way. All our approaches used a different entity representation and that is the main component. The components of any of our approaches are entity representation, string matching, scoring and, to some extent, scaling functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Development Environment</head><p>We used JAVA and python as main programming languages. We used Hadoop architecture provided by SARA (the Natherlands SARA Computing and Networking Services) and Java to process the data in a map reduce architecture. Python was used to process the same data in a federated fashion: six computers each with a 8-core processors and GNU tool called GNU Parallel to parallalize the process. The Java Hadoop architecture was much faster than the federated architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Supervised Approaches</head><p>We describe now the two supervised approaches used in this challenge. The first approach, Prefix-Suffix Learning, focuses on precision, penalizing the recall. The second, Language Model, focuses on balancing the precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Prefix-Suffix Learning Approach</head><p>In this approach, for each entity e, we learned a set of strings Se of the form uv and vw that occur in the documents annotated as central (denoted as ∆ + ) and does not occur in document annotated as relevant, neutral or garbage (denoted as ∆ -); where v ∈ Ve, u is prefix of v of size K and w is a suffix of v of size K. We vary K in the interval [1, .., K], then we learned at most 2K different strings per unique v ∈ Ve.</p><p>Example: Consider the entity Nassim Taleb, its set V N assimT aleb = {Nassim Nicholas Taleb, Nassim Taleb} and the news document below: "Among the people we reached out to while reporting this week's cover story on Rich Marin was Nassim Taleb. Not only is he a well-known talking head (and presumably accessible), but he also got his start at Bankers Trust, just like Mr. Marin. "</p><p>For K = 4, the set S N assimT aleb = { Nassim Taleb, s Nassim Taleb, as Nassim Taleb, was Nassim Taleb, Nassim Taleb., Nassim Taleb. , Nassim Taleb. N, Nassim Taleb. No} Then, for each entity e, an arbitrary document D is annotated as central if any string in Se occur in D.</p><p>The algorithm to learn the string Se is described in the Alg.1. In the run that we submitted, we used K = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Language Model Approach</head><p>This approach focuses on balancing precision and recall. To do so, we build a statistic language model for each entity e ∈ E using the training documents annotated as central. Then we score the documents based on the perplexity measure between the entity language model and the document text. We normalize the values between [0,1000]. This measure may produce some documents with low score; however, we consider all those documents with score &gt; 0 as central. Below we detail this approach. DEFINITION 1 (ENTITY LANGUAGE MODEL). Given an entity e and a corpora ∆ + of documents annotated as central, we build a statistical language model, trigram model, LMe for e over S </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Unsupervised Approaches</head><p>We have experimented with two main unsupervised approaches: one so-called disambiguator and another so-called Google-Cross-Lingual Dictionary (GCLD). In both of them, The Wikipedia Wikipedia entities are represented by strings and those strings are used to query the stream. If a matching string is found, then the document is relevant and/or central to a degree provided by a scoring function.</p><p>The disambiguator targets the central bins while the GCLD targets the relevant and central bins. Under the GCLD based approach, we have experimented with many variations of scoring functions, thresholds and scaling functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Disambiguator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Representation</head><p>Given an entity e among the 29 given Wikipedia entities, we represent e as a set of strings Ve, defined as: In other words, the set Ve is the set of labels and name of the entity e in DBPedia. For example, the set V for the entity Nassim Taleb 4 , is: V N assimT aleb = {Nassim Nicholas Taleb, Nassim Taleb}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disambiguator Approach</head><p>This approach aims at producing high recall and improve the precision over the baseline by solving only the cases where the source entities are ambiguous. DEFINITION 3 (AMBIGUOUS ENTITIES). Given two distinct entities e and f , they are ambiguous if their entity representations Ve ∩ V f = ∅. 9 2 http://en.wikipedia.org/wiki/SPARQL 9 3 http://dbpedia.org 9 4 In DBPedia: http://dbpedia.org/page/Nassim_Nicholas_Taleb Basically, to produce high recall, we select the documents using the string in the entity representation Ve, and then to improve precision over this initial selection, we filter the documents using an extended entity representation Te that we describe next.</p><p>When two or more entities are ambiguous, it is impossible to decide what exactly entity a string mentioned in a document refers to. For example, among the 29 entities provided in TREC-KBA 2012, four entities were ambiguous: For example, the string Boris Berezovsky may refer to a pianist or a businessman. Without context information, we cannot decide which one it refers to.</p><p>In order to decide which one of the ambiguous entity an document mention, we contextualize an entity representation using type information extracted from DBPedia representation of this entity. An typified entity representation is defined such as: DEFINITION 4 (TYPIFIED ENTITY REPRESENTATION). A typified entity representation Te of an entity e is the results of the SPARQL query below:</p><formula xml:id="formula_2" coords="7,53.73,353.64,74.84,6.05">SELECT d i s t i n c t ? c</formula><p>WHERE { e &lt; h t t p : / / www. w3 . o r g / 1 9 9 9 / 0 2 / 2 2r d f -s y n t a x -n s # t y p e &gt; ? o . ? o &lt; h t t p : / / www. w3 . o r g / 2 0 0 0 / 0 1 / r d f -schema # l a b e l &gt; ? c . } UNION SELECT d i s t i n c t ? c WHERE { e &lt; h t t p : / / p u r l . o r g / dc / t e r m s / s u b j e c t &gt; ? o . ? o &lt; h t t p : / / www. w3 . o r g / 2 0 0 0 / 0 1 / r d f -schema # l a b e l &gt; ? c . }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNION SELECT d i s t i n c t ? c</head><p>WHERE { e &lt; h t t p : / / p u r l . o r g / dc / t e r m s / s u b j e c t &gt; ? z . ? z &lt; h t t p : / / www. w3 . o r g / 2 0 0 4 / 0 2 / s k o s / c o r e # b r o a d e r &gt; ? o . ? o &lt; h t t p : / / www. w3 . o r g / 2 0 0 0 / 0 1 / r d f -schema # l a b e l &gt; ? c . } In other words, this query retrieves a set of string representing the type of an entity e in its DBPedia representation. The property type and subject above define the type of a specific DBPedia representation. For example, considering the entity Boris_Berezovsky_ (businessman), the instantiation of the query above would be: SELECT d i s t i n c t ? c WHERE { &lt; h t t p : / / d b p e d i a . o r g / p a g e / B o r i s _ B e r e z o v s k y _ ( p i a n i s t ) &gt; &lt; h t t p : / / www. w3 . o r g / 1 9 9 9 / 0 2 / 2 2r d f -s y n t a x -n s # t y p e &gt; ? o . ? o &lt; h t t p : / / www. w3 . o r g / 2 0 0 0 / 0 1 / r d f -schema # l a b e l &gt; ? c . } UNION SELECT d i s t i n c t ? c WHERE { &lt; h t t p : / / d b p e d i a . o r g / p a g e / B o r i s _ B e r e z o v s k y _ ( p i a n i s t ) &gt; &lt; h t t p : / / p u r l . o r g / dc / t e r m s / s u b j e c t &gt; ? o . ? o &lt; h t t p : / / www. w3 . o r g / 2 0 0 0 / 0 1 / r d f -schema # l a b e l &gt; ? c . } UNION SELECT d i s t i n c t ? c WHERE { &lt; h t t p : / / d b p e d i a . o r g / p a g e / B o r i s _ B e r e z o v s k y _ ( p i a n i s t ) &gt; &lt; h t t p : / / p u r l . o r g / dc / t e r m s / s u b j e c t &gt; ? z . ? z &lt; h t t p : / / www. w3 . o r g / 2 0 0 4 / 0 2 / s k o s / c o r e # b r o a d e r &gt; ? o . ? o &lt; h t t p : / / www. w3 . o r g / 2 0 0 0 / 0 1 / r d f -schema # l a b e l &gt; ? c . } Then T Boris_Berezovsky_(pianist) = {pianist, musician, russian}.</p><p>Given an entity e, the algorithm process the news stream sequentially and for all document D that contains a string v ∈ Ve, we produce a Jaccard score between Te and de, where de and Jaccard is defined such as: DEFINITION 5 (ENTITY CONTEXT). A entity context de of an entity e is a set of tokens of a string uvw in D, where u ∈ Ve, and u and w have at most a length L. In the run that we submitted, we set L = 400. DEFINITION 6 (JACCARD SCORE). Given two sets A and B, the Jaccard score of these sets are:</p><formula xml:id="formula_3" coords="7,396.72,206.52,78.08,19.75">Jaccard = (A ∩ B) (A ∪ B)</formula><p>A document D is considered central for an entity e if:</p><p>Jaccard(Te, de) &gt; 0 ∧ ∀x ∈ E : x = e, Jaccard(Te, de) &gt; Jaccard(Tx, dx)</p><p>Alg. 2 describes the process of annotating central documents using this method.</p><p>Algorithm 4 DisambiguatorCentralAnnotator(E, D, L). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Google Cross Lingual Dictionary (GCLD) Approach</head><p>In our main unsupervised approach, we use a resource called Google-Cross-Lingual dictionary (GCLD) that maps language independent strings of words and Wikipedia articles (also called concepts or URLs). The resource assigns empirical probability distributions to strings given a URL and to URLs given a string <ref type="bibr" coords="7,509.17,585.22,9.52,7.77" target="#b7">[7]</ref>. Our approach here is to represent the queries (the Wikipedia entities) by the strings in the dictionary and to use the new representation as a query to filter documents that are central or/and relevant from the stream. The probabilities are used to give a confidence score for the relevance of a document for a Wikipedia entity. The dictionary has many different statistical information that can be used in different ways to improve performance and we have tried to experiment with some and examined their effects. Below we will detail the dictionary, the approach we used, the experiments we did and discuss the results and draw conclusions.</p><p>The dictionary tf-IDF ditionary t =term l(s,e) -an instance of a link between anchor s and WP e tf = term-frequency #l(s,e) -the total number of hyperlinks to a Wikipedia article having s as anchor d P s∈S l(s, e)-all links to a WP article dft</p><p>The total number of links that contain s, # P s∈S l(s, e) that contain particular l(s,e), lf l(s,e) N # P s∈S l(s, e) -the collection, the number of WP entities in this case idft = log N df t idf l(s,e) = #l(s,e) P s∈S l(s,e)</p><p>Table <ref type="table" coords="8,136.98,202.04,3.49,7.77">3</ref>: Analogy to tf-IDF table</p><p>The dictionary is bi-directional in the sense it provides a mapping from free-form-natural language strings to concepts and vice versa.</p><p>The strings are gathered from anchor texts to all Wikipedia pages and the English Wikipedia titles. This means the strings include anchor texts from inter-wikipedia linking, and anchor texts to non-English Wikipedia articles. The strength of association between strings and concepts is quantified by conditional probabilities.</p><p>Let s ∈ S be a string and let e ∈ E be a Wikipedia entity. l(s, e) is a link between s and e where s is used as an anchor in a link to a Wikipedia entity e. #l(s, e) is the total number of hyperlinks into a Wikipedia article e using anchor text s. P e∈E l(s, e) is the total number of links into Wikipedia pages that use s as an anchor and P s∈S l(s, e) is the total number of links to a Wikipedia article e. Based on this, the dictionary defines two probabilities : P (U RL|s) for strings to concepts and P (s|U RL) for concepts to strings as follows.</p><p>P (U RL|s) = #l(s, e) P e∈E l(s, e)</p><p>P (s|U RL) = #l(s, e) P s∈S l(s, e)</p><p>Formula one tells whether a string is ever used as an anchor text to a certain Wikipedia entity and if it does it gives the probability. By this, it disambiguates the string by distributing the probability mass over the different Wikipedia entities according to how often it is used as an anchor to each of them. The second formula quantifies how important as an anchor a certain string is in comparison to other strings that also point to the same entity e. All strings that can be used as anchors to a certain Wikipedia article are co-referents, and the second formula measures the relative strength with which a co-referent refers to a Wikipedia article.</p><p>An alternative way to look at the two formulas is to interpret them analogous to the tf-IDF concept ( see table <ref type="table" coords="8,213.38,640.76,4.48,7.77">3</ref> ). Analogous to the tf-IDF concept, a document is the number of links pointing to a WP article.</p><p>One can think that the first formula is like the term frequency normalized by the number of terms and the second formula is a modified idf, i.e. it uses the number of links into a document instead of the number of the collection and normalizes it by the number of all links having anchor s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments, Results and discussions</head><p>We conducted many experiments by varying dictionary strings for representation, probabilities for scoring, and thresholds for selecting strings. The algorithm is string matching, i.e. once the Wikipedia entities are represented with our choice of set of strings, we query each document of the stream if it has a match for the elements of our set. If there is a match, we give the document-entity pair a confidence score computed based on the probabilities. When more than one element of the set of strings for an entity are matched, we take either the average or the maximum of the probabilities of the matched strings. Our measures were recall, precision, and Fmeasure against relevance cut-off. But to distinguish between two approaches, we mainly looked at F-measure.</p><p>Our first experiment was with probabilities given by P (U RL|s) . We lowercased all the string representations of the Wikipedia entities and the stream documents. When two strings are lowercased to the same form, we assign the form we keep the higher probability. We also stripped punctuation and white spaces. We did experiments with strings that come only from non-Wikipedia pages, and all strings to English or corresponding non-English Wikipedia pages. We compared the results on F-measure and the later representation performed better. The reason for increment in F-measure was because of an increase in recall. And the increase in recall is due to the additional strings. Using average of the probabilities of the matching strings performed worse than the maximum. Next, we experimented by setting different thresholds on probabilities in order to select strings that have higher probabilities. We tried thresholds 0.01, 0.001, 0.0001. However, the performance did not improve significantly. In fact, in the case of threshold 0.01, performance dropped significantly.</p><p>Our second experiment was in lowercasing and stripping punctuations. The dictionary strings that we used are not lowercased, i.e. "Nasim" and "NASIM" are considered different strings. The dictionary strings also contain punctuations and white spaces. We decided to experiment without lowercasing the entity representations and the stream documents. The performance was a big improvement over the lowercased and punctuation-stripped approach. It is not surprising that it is so since it better captures the capitalizations which are a feature of proper nouns.</p><p>However, we were not happy with results since the performance scores were still poor. Moreover, thus the confidence scores were very small and were very susceptible to scaling functions. P (U RL|s) is like a tf , it never tells us how discriminative a string is to a certain Wikipedia entity with respect to other Wikipedia entities. P (s|U RL) is the right probability to use for this purpose. P (s|U RL) exposes the ambiguity in a string by distributing the probability mass over the entities it can be used as anchor in a link. There are many strings whose P (s|U RL) probabilities were 1, which shows that the document containing the string is highly probably relevant to the WP entity the string represents. And, indeed, experiments using these probabilities for scoring showed better performance. The use of P (s|U RL) disambiguates ambiguous entities naturally.</p><p>Varying thresholds for string selection and using averaging instead of maximum did not improve results significantly.</p><p>Our third experiment was combining the two probabilities for scoring. When more than one string is matched, we multiply both probabilities first, and keep the maximum as a score. Our best scores on the relevant and central bins was obtained by this approach. Our main run submissions were from this approach. We also submitted runs using the same approach but with lowecased and punctuationstripped. For both cases, we used two different simple per-entity scaling functions. First, we selected the maximum score per entity and use that to scale the results as:</p><p>sscaled(doc -entity) = s(doc -entity) smax(entity) * 100%. (3)</p><p>Our second scaling function was using a threshold on maximum score per entity to discourage entities whose maximum score is less than 10. we used a threshold of 10 for smax(entity such that those entities whose highest score is less than 10 will be divided by 10.</p><p>Using the combined scoring with and without lowercasing and stripping and the two scaling functions, we submitted 4 runs. The nonstripping results were good. In total, we submitted 7 runs (1 prefixsuffix, 1 language model, 1 disambiguator, 4 GCLDbased). The GCLD submissions are google_dic_1(no lowercasing and no stripping off, normalizing by the highest score except those whose score is less than 10), google_dic_3(lowercasing and stripping off normalized by the highest score) , google_strip_1 (lowercasing stripping off, normalize by highest score except those whose score is less than 10) and google_strip_2(lowercasing and stripping off, normalizing by highest score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCLD challenges</head><p>The GCLD is a mapping between strings and Wikipedia concepts or vice versa. While the probabilities show how likely a string can be used as an anchor in a link to a Wikipedia page, it never shows how important the anchor text is for a document. The only relationship between the strings in the dictionary and the strings in the document is through string matching. This means a word may have a high probability of being used in a link to a Wikipedia page, but if the word is not important for the document, say Obama in a restaurant's name, the match becomes useless. We believe incorporating some third function that measures the importance of a term for a document can improve the performance. This is something we want to try next. Another challenge is the presence of noise in the GCLD. Strings such as "here" are present in many of the entity representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Run Graphs and comparisons</head><p>Figures <ref type="figure" coords="9,84.12,495.65,4.48,7.77" target="#fig_6">2</ref> and<ref type="figure" coords="9,108.40,495.65,4.48,7.77">3</ref> show the performance on the test set of disambiguator and prefix-suffix learning respectively. Similarly, Tables <ref type="table" coords="9,288.42,505.70,4.48,7.77">4</ref> and<ref type="table" coords="9,69.28,515.74,4.48,7.77">5</ref> show the two best performing variations of GCLD. 6 shows the highest score for each entity and the run that generated it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In the session track, we experimented with our system that is explicitly designed to capture a user's learning process. It achieves high scores on all tasks with a negligible difference between tasks.</p><p>In KBA track, we have experimented with supervised and unsupervised approaches. Under the supervised approach, we have experimented with prefix-suffix learning and language models. On the training set, both learning approaches have comparable performance. Under the unsupervised approach, we have experimented with DBpedia labels and GCLD. Under GCLD, we have tried different entity representations, different scoring functions, and different scaling functions. The two supervised approaches and the DBpedia labels approach target the central bin. In the GCLD, we have targeted the central+relevant bins because the nature of the strings is not in a position to differentiate between central and relevant. On the test set, the DbPedia labels approach showed a better performance. On the central+relevant bins, the GCLD with no  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,53.80,286.82,76.45,8.86;4,163.85,281.01,125.03,7.86;4,134.01,291.92,9.73,5.76;4,143.74,297.68,38.86,5.24;4,184.95,292.90,133.77,7.86"><head></head><label></label><figDesc>P a ((s, e), (s , e )) = Pa(s)(s ) • PRE(s , e ) • Pa(e)(e ) P s ∈S,e ∈E Pa(s)(s ) • PRE(s , e ) • Pa(e)(e )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,390.40,251.98,91.93,7.77;4,316.81,140.64,252.08,106.72"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Initial approach</figDesc><graphic coords="4,316.81,140.64,252.08,106.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,324.49,713.77,19.92,7.44;5,339.29,717.89,3.37,4.37;5,346.94,710.80,40.76,7.86;5,387.70,709.03,5.59,5.24;5,387.70,710.80,59.16,8.86;5,446.86,709.03,5.59,5.24;5,457.24,710.80,82.50,7.86"><head></head><label></label><figDesc>d∈∆ + e d, where∆ + e = {d|∀d ∈ ∆ + ∧ v ∈ Ve ∧ d = uvw}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,327.00,417.55,228.91,7.93;6,316.81,427.45,211.94,7.86;6,528.75,425.88,2.99,5.18;6,535.50,427.59,20.42,7.72;6,316.81,437.63,51.45,7.72;6,368.26,435.92,2.99,5.18;6,375.20,437.63,180.72,7.72;6,316.81,447.67,38.85,7.72;6,316.75,469.35,105.32,6.05;6,317.23,476.78,228.63,6.27;6,316.56,492.31,21.44,6.05;6,316.75,507.61,116.81,6.05;6,317.36,515.26,169.92,6.05"><head>DEFINITION 2 (</head><label>2</label><figDesc>ENTITY REPRESENTATION). An entity representation Ve of an entity e is the result of the SPARQL 2 query over DBPedia 3 representation of an entity e -a RDF version of Wikipedia: SELECT d i s t i n c t ? o WHERE { e &lt; h t t p : / / www. w3 . o r g / 2 0 0 0 / 0 1 / r d f -schema # l a b e l &gt; ? o . } UNION SELECT d i s t i n c t ? o WHERE { e &lt; h t t p : / / xmlns . com / f o a f / 0 . 1 / name&gt; ? o . }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,54.69,153.57,127.45,6.05;7,54.69,161.22,110.71,6.05;7,54.66,168.87,98.20,6.05;7,54.66,176.52,114.87,6.05"><head></head><label></label><figDesc>B o r i s _ B e r e z o v s k y _ ( b u s i n e s s m a n ) ; B o r i s _ B e r e z o v s k y _ ( p i a n i s t ) ; B a s i c _ E l e m e n t _ ( company ) ; B a s i c _ E l e m e n t _ ( m u s i c _ g r o u p ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,325.78,345.72,59.32,6.12;7,325.78,353.37,114.27,6.12;7,325.78,361.02,65.94,6.12;7,334.99,368.67,111.72,6.12;7,334.99,376.32,110.04,6.12;7,334.99,383.97,68.74,6.12;7,344.20,391.62,66.81,6.12;7,353.40,399.27,81.70,6.12;7,362.61,406.93,74.30,6.12;7,362.61,414.58,186.16,6.12;7,353.40,423.14,25.11,4.74;7,344.20,430.79,29.29,4.74;7,334.99,438.44,29.29,4.74;7,325.78,446.09,29.29,4.74;7,325.78,452.83,66.81,6.12;7,334.99,460.49,112.44,6.12;7,344.20,468.14,207.91,6.12;7,334.99,476.70,25.11,4.74;7,325.78,484.35,29.29,4.74;7,325.78,491.09,76.24,6.12"><head></head><label></label><figDesc>CEN T RAL ← ∅ M AT RIX ← newM atrix(D, E) for all e ∈ E do Ve ← QueryDBP ediaLabels(e) Te ← QueryDBP ediaT ypes(e) for all v ∈ Ve do for all d ∈ D do if d.contains(v) then de ← context(d, e, L) M AT RIX[d][e] ← M AT RIX[d][e] + Jaccard(Te, de) all d ∈ D do if M AT RIX[d].max &gt; 0 then CEN T RAL ← CEN T RAL ∪ [d, M AT RIX[d].maxEntity] end if end for return CEN T RAL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,402.10,213.94,68.53,7.77;9,316.81,53.80,207.36,155.52"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: DbPedia.</figDesc><graphic coords="9,316.81,53.80,207.36,155.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,380.80,393.77,111.12,7.77;9,316.81,233.63,207.36,155.52"><head>Figure 3 : 4 : 5 :</head><label>345</label><figDesc>Figure 3: Prefix-sufix learning.</figDesc><graphic coords="9,316.81,233.63,207.36,155.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,316.81,55.78,239.11,72.53"><head>Table 1 :</head><label>1</label><figDesc>nDCG at k of the CWI submission and median of TREC 2012 over all sessions</figDesc><table coords="4,336.92,55.78,198.90,50.11"><row><cell>Task</cell><cell cols="2">CWI submission median TREC</cell></row><row><cell>RL1 (prior)</cell><cell>0.2422</cell><cell>0.1746</cell></row><row><cell>RL2 (query upd.)</cell><cell>0.2529</cell><cell>0.1901</cell></row><row><cell>RL3 (RL upd.)</cell><cell>0.2313</cell><cell>0.2160</cell></row><row><cell>RL4 (click upd.)</cell><cell>0.2319</cell><cell>0.2261</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 LearningPrefixSuffixStrings(E, ∆ + ,∆ -, K). In other words, for each entity e we build a trigram language model LMe over an aggregate document containing all document annotated as central for the entity e.</p><p>We particularly used the Kyoto Language Modeling Toolkit (Kylm) 1  to build this language model. The only parameter set in this api was smoothuni. Alg. In order to annotated the test corpora as central, we compute the perplexity between a new document and each specific language model. The perplexity is based on entropy, where the entropy gives a measure of how likely the ngram model is to have generated the test data. Entropy is defined (for a sliding-window type ngram) as:</p><p>where Q is the number of words of test data and N is the order of the ngram model. Perplexity is a more intuitive measure, defined as:</p><p>The perplexity of an ngram model with vocabulary size W will be between 1 and W. Low perplexity indicates a more predictable language. All documents with perplexity &lt; 100 were consider as central.</p><p>9 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,321.30,656.86,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,331.75,669.31,213.18,7.77;9,331.75,679.20,213.47,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,459.23,669.31,85.71,7.77;9,331.75,679.35,54.33,7.77">Information retrieval by logical imaging</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J V</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,392.27,679.20,92.55,7.72">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="3" to="17" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,331.75,690.24,200.85,7.93;9,331.75,700.43,146.58,7.77" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,379.40,690.24,149.50,7.72">Probability Theory : The Logic of Science</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-04">April 2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,331.75,711.32,210.05,7.93;10,68.74,423.55,210.83,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,380.12,711.32,114.92,7.72">A generative theory of relevance</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<idno>AAI3152722</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,68.74,434.59,213.75,7.77;10,68.74,444.63,215.27,7.77;10,68.74,454.68,183.95,7.77" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,268.54,434.59,13.94,7.77;10,68.74,444.63,215.27,7.77;10,68.74,454.68,73.08,7.77">The smoothed dirichlet distribution: Explaining kl-divergence in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>CIIR</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="10,68.74,465.71,220.57,7.77;10,68.74,475.60,211.97,7.93;10,68.74,485.65,184.46,7.93;10,68.74,495.84,101.12,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,167.22,465.71,122.09,7.77;10,68.74,475.76,169.30,7.77">Using dempster-shafer&apos;s theory of evidence to combine aspects of information use</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ruthven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1020114205638</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,243.82,475.60,36.89,7.72;10,68.74,485.65,110.89,7.72">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="267" to="301" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,68.74,506.73,201.77,7.93;10,68.74,516.77,208.51,7.93;10,68.74,526.96,39.35,7.77" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,123.17,506.73,147.34,7.72;10,68.74,516.77,107.80,7.72">Everyday information practices: a social phenomenological perspective</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Savolainen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Scarecrow Press</publisher>
			<pubPlace>Lanham, Md</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,68.74,538.00,218.14,7.77;10,68.74,547.89,220.49,7.93;10,68.74,557.94,194.16,7.72;10,68.74,567.98,195.36,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,193.24,538.00,93.64,7.77;10,68.74,548.05,111.81,7.77">A cross-lingual dictionary for English Wikipedia concepts</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,196.24,547.89,92.99,7.72;10,68.74,557.94,194.16,7.72;10,68.74,567.98,88.04,7.72">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC 2012)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC 2012)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,68.74,579.17,215.46,7.77;10,68.74,589.06,209.97,7.93;10,68.74,599.25,123.54,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,128.53,579.17,152.30,7.77">Conditional probability meets update logic</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van Benthem</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1025002917675</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,68.74,589.06,158.89,7.72">Journal of Logic, Language and Information</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="409" to="421" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,68.74,610.29,222.53,7.77;10,68.74,620.18,212.81,7.93;10,68.74,630.23,213.35,7.72;10,68.74,640.27,214.32,7.93;10,68.74,650.46,160.15,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,254.62,610.29,36.65,7.77;10,68.74,620.34,144.06,7.77">Revisiting logical imaging for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.91,620.18,53.64,7.72;10,68.74,630.23,213.35,7.72;10,68.74,640.27,188.12,7.93">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;09</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="766" to="767" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
