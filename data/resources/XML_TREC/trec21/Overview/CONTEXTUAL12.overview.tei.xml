<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.28,96.74,401.45,15.12">Overview of the TREC 2012 Contextual Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,114.64,134.59,86.97,10.48"><forename type="first">Adriel</forename><surname>Dean-Hall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.51,134.59,105.90,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.36,134.59,63.95,10.48"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.82,168.34,68.18,10.48;1,227.62,182.29,36.58,10.48"><forename type="first">Paul</forename><forename type="middle">Thomas</forename><surname>Csiro</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Ellen Voorhees NIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.28,96.74,401.45,15.12">Overview of the TREC 2012 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8B0A493A6F0EFCE410BA7C0F6F07B7C6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The contextual suggestion track investigates search techniques for complex information needs that are highly dependent on context and user interests. According to a report from the Second Strategic Workshop on Information Retrieval in Lorne <ref type="bibr" coords="1,278.53,279.72,9.96,8.74" target="#b0">[1]</ref>: "Future information retrieval systems must anticipate user needs and respond with information appropriate to the current context without the user having to enter an explicit query. . . In a mobile context such a system might take the form of an app that recommends interesting places and activities based on the user's location, personal preferences, past history, and environmental factors such as weather and time. . . In contrast to many traditional recommender systems, these systems must be open domain, ideally able to make suggestion and synthesize information from multiple sources. . . "</p><p>For example, imagine a group of information retrieval researchers with a November evening to spend in beautiful Gaithersburg, Maryland. A contextual suggestion system might recommend a beer at the Dogfish Head Alehouse (www.dogfishalehouse.com), dinner at the Flaming Pit (www.flamingpitrestaurant.com), or even a trip into Washington on the metro to see the National Mall (www.nps.gov/nacc). As its primary goal, the contextual suggestion track seeks to develop evaluation methodologies for such systems.</p><p>TREC 2012 is the first year for the track. For this first year, we introduced a single task to evaluate contextual suggestion from the open Web. As input to the task participants were given a set of example suggestions, a set of user preference profiles, and a set of geotemporal contexts. The task was to take the profiles and contexts and to produce up to 50 ranked suggestions for each combination of profile and context. Participants gathered suggestions from the open Web.</p><p>Each profile corresponds to a single user, and indicates that user's preference with respect to each sample suggestion. For example, one suggestion might be to have a beer at the Dogfish Head Alehouse, and the profile might include a negative preference with respect to this suggestion. Each suggestion includes a title, description, and an associated URL. Each context corresponds to a particular geotemporal location, including city, day of the week, time of day, and season. For example, the context might be Gaithersburg, Maryland, on a weekday evening in the fall. The geographical contexts are very coarse-grained (i.e., an entire city) to help simplify the task.</p><p>A total of 14 groups submitting 27 runs participated in this first year of the track, this includes the 2 baseline submissions from CSIRO and 2 baseline submissions from the University of Waterloo. These baselines will be discussed later in this report. Given the newness of the track, participants were given the option of basing their suggestions solely on the user profiles (returning suggstions appropriate to any place and time) or solely on geotemporal context (returning suggestions appropriate to a generic user). Only one group, from the University of Delaware, took advantage of this option, submitting runs based solely on user profile. Detailed formats for the sample suggestions, profiles, and contexts are given below. These descriptions are followed by a brief discussion of the result suggestions returned by the participants. An experimental run consists of a single result suggestion file generated automatically from the sample suggestions, profiles, and contexts. Each participating group was permitted to return up to two experimental runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sample Suggestions</head><p>Suggestions are descriptions of attractions that are intended to be recommended to a user as something they would find interesting. Suggestions consist of a title, short description and a website URL. Fifty sample suggestions were gathered manually to be used to create profiles. However, due to problems described in the next section, one sample was excluded from the distributed sample suggestion file. The sample suggestions were all attractions within the Toronto region. The URL for these sample attractions were the attraction's main page and the descriptions were taken from the website's meta tag description or, if that was unavailable, from a description provide by a third-party website. In the survey, sample suggestions were presented to users in a random order. Users were asked to give two ratings for each attraction, one for the attraction's description and one for the attraction's website. Each rating given was one of three levels of interest in the attraction. The levels were: +1 (looks interesting), 0 (indifferent), and -1 (looks boring). Examples of the survey interface are given in figure <ref type="figure" coords="3,124.37,277.04,3.87,8.74" target="#fig_0">1</ref>.</p><p>In total 34 results from survey participants were released as profiles. A few users responded to the survey but did not complete it. Preferences from these users were not were not included in the released profiles.</p><p>&lt; p r o f i l e number=" 1 "&gt; &lt;example number=" 1 " i n i t i a l ="-1" f i n a l=" 0 "&gt; &lt;example number=" 2 " i n i t i a l =" 1 " f i n a l=" 1 "&gt; . . . &lt;example number=" 49 " i n i t i a l =" 1 " f i n a l=" 1 "&gt; &lt;/ p r o f i l e&gt; . . .</p><p>Listing 2: Example of part of a profile.</p><p>Survey participants were given 50 example suggestions. However during the course of the survey one of the participants brought to our attention that one of the example suggestions had a URL that did not match with the title and description. This error was caused by a incorrect listing for the attraction on a tourist website. This example suggestion was removed from the profiles distributed to the track participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contexts</head><p>Contexts describe where the user physically is located and what time of day, week, and year it is when the user is searching. Each context consists of a city in the United States, a part of the week (weekday or weekend), a time of day (morning, afternoon, or evening), and a season (spring, summer, fall, or winter). For the contexts distributed to the participants, the part of week, the time of day, and the season were chosen at random, with each option in each field having an equal chance of being selected. The cities were chosen from the geonames.org list of US cities, cities with populations less than 100,000 were filtered out. The likelihood of being chosen was proportionate to the population of the city. If this random process generated the same context more than once, the duplicate context was discarded, and a new context was generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Result Suggestions</head><p>Each submitted run consists of 50 ranked suggestions for each profile/context pair, with formatting similar to that of the sample suggestions, including a title, description, and url. In generating the suggestions, task participants could use whatever resources they wished, including review Websites.</p><p>The goal was that each suggestion should be appropriate to the profile (based on the user's preferences) and the context (according to the geotemporal location). Ideally, the description of the suggestion would be tailored to reflect the preferences of that user. For the purposes of generating suggestions, participating groups could assume that the users were persons of legal drinking age in their early-tomid twenties. Participating groups could also assume that a user has up to five hours available to follow a suggestion and has access to appropriate transportation (e.g., a car).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Judging</head><p>The task of judging the result suggestions was split up into two independent tasks: judging the suggestions with regard to the profile and judging the suggestions with regard to the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Profile Relevance</head><p>In order to judge the relevance of suggestions with respect to a profile, we conducted a second survey.</p><p>In this second survey, users from the first survey were invited to return to judge suggestions from task participants. Returning users were given the option to judge up to three sessions, with a session consisting of suggestions from a single geotemporal context. Each session was made up of all returned suggestions up to rank 5 for all runs for a particular profile-context pair (giving at most 135 suggestions per session).</p><p>As we did for the first survey, suggestions were ordered randomly and users were asked to give one rating for the suggestion's description and one for the suggestion's website. Scores given by users were incremented by one before being distributed to task participants (i.e., from -1, 0, or 1 to 0, 1, or 2). This renumbering was done for consistency with the geographical and temporal judgment scores described below.</p><p>All 34 users who had a profile corresponding to them distributed for this task were invited to participate in the second survey, 17 students responded to this invitation and completed the second survey. A total of 44 profile-context pairs were judged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context Relevance</head><p>Assessors at NIST judged how relevant a suggestion was with regard to a context by visiting the attraction's website and looking for where the attraction is located and what time the attraction is open. 15 contexts for all profiles up to suggestions at rank 5 were judged this way. Each judged suggestion was given a temporal and a geographical score.</p><p>• If the suggestions was appropriate it was given a score of 2.</p><p>• If the suggestions was marginally appropriate it was given a score of 1.</p><p>• If the suggestions was not appropriate it was given a score of 0.</p><p>• If the website for the suggestion did not load at the time of assessment it was given a score of -2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline Runs</head><p>There were four baseline runs submitted as part of the track. The first baseline (waterloo12a) consist of the top 50 attractions from each city according to the ranking given by tripadvisor.com. Only the geographical aspect of the context is taken into accout for this run, the profiles and temporal ascpect of the context are ignored.</p><p>The second baseline (waterloo12b) also uses tripadvisor.com, incorporating the profiles by using the site's search tool to search for attractions similar to the ones the user gave a high rating to in their profile. The search terms used were manually generated based on the sample suggestions. Again the attractions were sorted according to the rating given on the website.</p><p>The third baseline (baselineA) is output from the Google Places API, a commercial database of places and reviews. It allows us to compare TREC runs with simply using a commercial API, admittedly simple-mindedly. Each context was split into day, time, and place. The API was queried for the place at a number of days and times to cover the context: for example, a "weekend morning" would see queries issued from 0800 to 1045, local time, on a Saturday and Sunday. The union of all results was taken, deduplicated, and the top-rated results were kept as suggestions. Descriptions were generated by feeding the URLs for each suggestion to the Yahoo BOSS search API and taking Yahoo's snippets. Profile information wasn't used in this run since commercial APIs do not expose this for general use.</p><p>The final baseline is the "pub run" baseline (baselineB), it is the same as the commercial baseline, with candidates restricted to pubs, restaurants, and cafes. The intuition is that a pub will likely be a popular choice, but the run serves as a test of diversity: if baselineB does well despite only ever recommending pubs, we may need to consider an explicit diversity measure of some sort in future.</p><p>4 Participant Approaches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CSIRO</head><p>Runs: csiroht and csiroth CSIRO's two runs used the intersection of candidate venues provided by Google Places and Foursquare. Venues were ranked according to a linear combination of two scores: one that captures the venue's appropriateness at the given time, and another that captures its appropriateness given the user's prior likes and dislikes.</p><p>For time-of-day scores, CSIRO sampled a large number of locations from Foursquare and noted the number of people "checked in" at regular intervals. By aggregating over locations and time it was possible to learn for example that throughout the week movie theatres are popular on evenings; while theme parks are popular throughout the day on weekends, but only on afternoons during the week.</p><p>For matches to profiles, the description of each example was first reduced to a BM25-weighted vector. Vectors were summed, with fixed multipliers of 0.75 for positive examples and -0.25 for negative examples. The resulting term weights were used to score the description of each candidate suggestion.</p><p>Run csiroth mixed the text-based scores with the time-of-day scores with ratio 7:3. Run csiroht mixed the scores in the ratio 3:7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fasilkom UI from Universitas Indonesia</head><p>Runs: FASILKOMUI01 and FASILKOMUI02</p><p>Fasilkom UI (GroupID: FASILKOMUI) from Universitas Indonesia submitted two runs for the TREC 2012 Contextual Suggestion tracks. Their contextual suggestion system combines the user model and the geolocation from the context information to generate the place suggestions. The system gathers place data from various web services such as Yelp, Google Place, and Trip Advisor. The suggestion search is based on the Yelp's place category list. Thus, given a user model and a context, the system will find places that match the categories listed in the user model and also located in the proximity of the current location. It will also expand the search to the similar categories if it cannot find a particular category in the current location. As for the scoring, they use the review rating for the places to produce the ranked results. They also try to apply diversity to the suggestion results to make the suggestion varied and more interesting for the users. Both submitted runs namely FASILKOMUI01 and FASILKOMUI02 use same techniques, the difference is that FASILKOMUI01 uses diversity while FASILKOMUI02 does not apply diversity to its suggestion results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">HP Labs China</head><p>Runs: hplcrating and hplcranking HPLC uses a context-aware recommendation approach to produce the ranked list of suggestions. In their approach, they crawled all places in all related cities. They employed Matrix Factorization for collaborative filtering to learn the latent factor of each user in profiles and Yelp. In detail, hplcrating uses SVD++ to predict the scores of all suggestions for a user, while hplcranking uses pairwise ranking model to rank a list of suggestions for a user. In addition, their approach makes use of category information in the matrix factorization model to learn user's general preference. They assume that if a person likes "Hammam Spa", he or she probably prefers similar places in the category "Beauty and Spas". Due to lack of contextual information in profile and dataset of users' preferences crawled from Yelp, they use the contextual post-filtering approach to adjust the resulting set of suggestions. They filter out suggestion according to the category feature and the information extracted from Yelp. For example, "Bars" usually do not open in the morning. Some places extracted from Yelp contain the opening and closing time which can be used to filter unmatched context. Website of each suggestion is extracted from Yelp in their approach. The description for each suggestion is generated by a human defined template which includes location, category and many other features about the suggestion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">IRIT Lab</head><p>Runs: iritSplit3CPv1 and iritSplit3CPv2</p><p>The IRIT Lab defined a retrieval framework that combines two modules:</p><p>• Context processing. IRIT used the Google Places API to retrieve a list of places. This takes as input geographic coordinates (those defined in each track context) and a set of place types. Different sets of place types were intuitively defined to match the combinations of temporal parts provided in the track contexts.</p><p>• Preference processing i.e., result personalization according to user interests. IRIT used vectorbased profiles relying on the Vector Space Model. For each profile, the positive user preferences were based on the positively rated examples. In the same way, the negative preferences were based the negatively rated examples. Two approaches were defined to build and use positive and negative user preferences.</p><p>The coarse-grained approach consisted in defining a positive preference vector and a negative preference vector for each user profile. The fine-grained approach consisted in defining positive and negative preferences as sets of positive and negative preference examples (one vector per example). A similarity score between each place vector (from Google Places) and each preference vector based on the cosine measure was then computed.</p><p>The two submitted runs built on the same approach of context processing. The iritSplit3CPv1 run featured the coarse-grained approach of preference processing, while the iritSplit3CPv2 run featured the fine-grained approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">TNO RaboudUniv</head><p>Runs: run01TI and run02K</p><p>First, initial recommendations were collected by using the location context as search query in Google Places. The recommendations were ranked by their textual similarity to the user profiles. In run01TI this similarity was based on a TF-IDF measure. For each term the term frequency was based on the number of occurrences of a term in the textual content of examples from the profile that were rated positively. The inverse document frequency was based on all the terms in all the documents. The cosine similarity of an initial recommendation to the positive profile determined the ranking. In run02K, the textual similarity was based on a point-wise Kullback Leibler divergence score. This measure is based on the probability of observing a term in the set of examples that were rated positively, compared to the probability of observing it in the set as a whole. The sum of Kullback Leibler scores for the terms that occur in the initial recommendations determined the rank. In order to improve the ranking of generally popular sights, the resulted ranking was combined with a number of other rankings based on Google Search, popularity and categories. As a final step, items that did not match the temporal context were filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">InfoLab from University of Delaware</head><p>Runs: UDInfoCSTc and UDInfoCSTdc</p><p>The InfoLab from University of Delaware (i.e., udel fang) submitted two runs for the contextual suggestion task. Their goal is to evaluate (1) an information gathering strategy that crawls and integrates the information about candidate places from multiple sources, such as Yelp, Google, Foursquare etc.; and</p><p>(2) a ranking-based method that computes the similarity between a candidate place and a user profile based on both descriptions and categories of the candidate and example places. In UDInfoCSTc, they use categories only to compute the similarities between each potential suggestion/example suggestion pair. For each user, positive and negative similarities are combined using optimal coefficients learned from examples. Final ranking is based on combined similarities scores. In UDInfoCSTdc, they used the similar strategy with UDInfoCSTc but when computing similarities they used categories together with descriptions. For both runs, they post-process the results to meet the geographic and temporal requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Measures</head><p>A total of 12 different measures were used in this task. There were 4 scores as output from the judgments: description rating (D), website rating (W), geographical relevance (G), and temporal relevance (T). For each score the mean reciprocal rank (MRR) up to rank 5 and precision at rank 5 (P@5) were calculated for each profile-context pair. A perfect score (i.e., a 2) was treated as a 1, otherwise the score was treated as a 0 for these measures. There were also two combinations of scores computed: WGT combined and GT combined. In order to score a 1 in these combined measures, a perfect score was required for each of the individual scores in the combination. Again for these combinations MRR and P@5 were calculated. MRR and P@5 for each of the 4 individual scores as well as the 2 combination scores makes for a total of 12 measures. In addition to calculations for each profile-context pairs there are also calculations for a mean over all contexts for each profile, a mean over all profiles for each context and an overall mean across all profile-context pairs. The overall means across all profile-context pairs are the ones reported in the results section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Table <ref type="table" coords="8,113.37,186.35,4.98,8.74" target="#tab_1">1</ref> presents all runs ordered by P@5 on the WGT score; table 2 presents all runs ordered by MRR on the WGT score. All runs submitted were judged according to the judging criteria of both the profile scores and the context scores. As mentioned early, the University of Delaware submitted runs based solely on the user profile. However, their suggestions generally appeared to be geotemporally appropriate, so we have also included their full results in the table.</p><p>Figure <ref type="figure" coords="8,116.72,252.10,4.98,8.74" target="#fig_1">2</ref> shows the correlation between 4 different pairs of measures. All pairs of MRR and P@5 for the same score or score combination had a correlation similar to figure <ref type="figure" coords="8,404.16,264.06,9.96,8.74" target="#fig_1">2a</ref> with the mean Kendall's tau coefficient among these 6 pairs being 0.8267 (the lowest being at 0.7361). The correlation between P@5 WGT and P@5 W shown in figure <ref type="figure" coords="8,257.25,287.97,9.41,8.74" target="#fig_1">2c</ref> is not as strong between the correlation between P@5 WGT and P@5 GT shown in figure <ref type="figure" coords="8,215.41,299.92,8.86,8.74" target="#fig_1">2b</ref>. Figure <ref type="figure" coords="8,264.75,299.92,10.52,8.74" target="#fig_1">2d</ref> shows somewhat of a correlation between the description and website P@5 measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>At the time of writing, we plan to continue the track for TREC 2013. Task details should remain essentially the same, but suggestions will not be taken from the open Web. Instead, we plan to use the new ClueWeb12 collection, developed at CMU. This collection includes approximately a billion Web pages crawled from the general Web in mid-2012. The crawl was seeded (in part) with a large number of travel and review sites, which we hope will make it appropriate for use in the track.</p><p>Another limitation of the track this year was the low number of profiles available, moving from a survey based method to a crowdsourcing based method will allow us to gather more profiles. Instead of tens of profiles our goal for TREC 2013 is to have hundreds of profiles made available to track participants.</p><p>Finally, NIST assessors found that judging the temporal aspects of the context difficult and ended up labelling most suggestions as temporally relevant. For TREC 2013 we plan to re-access our use of the temporal aspects of the context and we may remove them from the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>P@5 WGT P@5 GT P@5 G P@5 T P@5 W P@5 D (a) MRR WGT vs P@5 WGT. r = 0.97677; τ = 0.8730 (b) P@5 WGT vs P@5 GT. r = 0.76522; τ = 0.5535 (c) P@5 WGT vs P@5 W. r = 0.27899; τ = 0.4429 (d) P@5 W vs P@5 D. r = 0.84979; τ = 0.6143 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,85.04,218.79,441.93,8.74;2,85.04,230.74,441.92,8.74;2,85.04,56.69,220.96,150.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: When judgeing contexts users were presented with (a) A list with descriptions and links to website; after clicking on a link they were presented with (b) the website with a rating bar at the top</figDesc><graphic coords="2,85.04,56.69,220.96,150.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,85.04,545.40,441.92,8.74;11,85.04,557.35,183.90,8.74;11,85.04,351.22,220.95,165.72"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scatter plots for different measures plotted against each other. Pearson's r coefficients and Kendall's tau coefficient are also included.</figDesc><graphic coords="11,85.04,351.22,220.95,165.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,113.71,203.04,383.21,341.49"><head>Table 1 :</head><label>1</label><figDesc>All 6 P@5 measures sorted by WGT.</figDesc><table coords="9,113.71,203.04,383.21,319.57"><row><cell>iritSplit3CPv1</cell><cell>0.3235</cell><cell>0.6027</cell><cell>0.8930 0.6156</cell><cell>0.4599</cell><cell>0.3605</cell></row><row><cell>guinit</cell><cell>0.2920</cell><cell>0.6635</cell><cell>0.8802 0.6997</cell><cell>0.4451</cell><cell>0.5019</cell></row><row><cell>gufinal</cell><cell>0.2710</cell><cell>0.6689</cell><cell>0.8852 0.7031</cell><cell>0.4241</cell><cell>0.5191</cell></row><row><cell>UDInfoCSTc</cell><cell>0.2481</cell><cell>0.4950</cell><cell>0.7565 0.5794</cell><cell>0.3500</cell><cell>0.2852</cell></row><row><cell>PRISabc</cell><cell>0.2475</cell><cell>0.5464</cell><cell>0.9036 0.5510</cell><cell>0.4198</cell><cell>0.5160</cell></row><row><cell>hplcranking</cell><cell>0.2333</cell><cell>0.6032</cell><cell>0.8148 0.6147</cell><cell>0.3889</cell><cell>0.3815</cell></row><row><cell>UDInfoCSTdc</cell><cell>0.2210</cell><cell>0.5442</cell><cell>0.7939 0.6210</cell><cell>0.3500</cell><cell>0.3173</cell></row><row><cell>run02K</cell><cell>0.2185</cell><cell>0.5649</cell><cell>0.9034 0.5839</cell><cell>0.4049</cell><cell>0.4710</cell></row><row><cell>hplcrating</cell><cell>0.2117</cell><cell>0.5725</cell><cell>0.8815 0.5833</cell><cell>0.4124</cell><cell>0.3802</cell></row><row><cell>udelp</cell><cell>0.2111</cell><cell>0.5181</cell><cell>0.8530 0.5365</cell><cell>0.3519</cell><cell>0.2593</cell></row><row><cell>run01TI</cell><cell>0.1907</cell><cell>0.5392</cell><cell>0.8934 0.5598</cell><cell>0.3963</cell><cell>0.4185</cell></row><row><cell>ICTCONTEXTRUN2</cell><cell>0.1907</cell><cell>0.4624</cell><cell>0.8274 0.4955</cell><cell>0.3593</cell><cell>0.3969</cell></row><row><cell>udelnp</cell><cell>0.1883</cell><cell>0.4893</cell><cell>0.8414 0.5049</cell><cell>0.3722</cell><cell>0.2432</cell></row><row><cell>iritSplit3CPv2</cell><cell>0.1790</cell><cell>0.5486</cell><cell>0.8466 0.5580</cell><cell>0.3235</cell><cell>0.2593</cell></row><row><cell>baselineA</cell><cell>0.1784</cell><cell>0.5114</cell><cell>0.7908 0.5694</cell><cell>0.4086</cell><cell>0.3031</cell></row><row><cell>baselineB</cell><cell>0.1704</cell><cell>0.5482</cell><cell>0.8060 0.5883</cell><cell>0.2654</cell><cell>0.2444</cell></row><row><cell>waterloo12a</cell><cell>0.1377</cell><cell>0.4229</cell><cell>0.8230 0.4451</cell><cell>0.3463</cell><cell>0.3272</cell></row><row><cell>UAmsCS12wtSUM</cell><cell>0.1352</cell><cell>0.2363</cell><cell>0.4011 0.4335</cell><cell>0.3753</cell><cell>0.4377</cell></row><row><cell>ICTCONTEXTRUN1</cell><cell>0.1111</cell><cell>0.3986</cell><cell>0.8045 0.4055</cell><cell>0.2623</cell><cell>0.3463</cell></row><row><cell>waterloo12b</cell><cell>0.0864</cell><cell>0.4065</cell><cell>0.6827 0.4988</cell><cell>0.1741</cell><cell>0.3117</cell></row><row><cell>csiroth</cell><cell>0.0772</cell><cell>0.4516</cell><cell>0.7579 0.4734</cell><cell>0.1531</cell><cell>0.1438</cell></row><row><cell>UAmsCS12wtSUMb</cell><cell>0.0704</cell><cell>0.2202</cell><cell>0.4145 0.4040</cell><cell>0.3198</cell><cell>0.4463</cell></row><row><cell>csiroht</cell><cell>0.0698</cell><cell>0.4483</cell><cell>0.7573 0.4712</cell><cell>0.1623</cell><cell>0.1864</cell></row><row><cell>FASILKOMUI02</cell><cell>0.0667</cell><cell>0.4935</cell><cell>0.7770 0.5243</cell><cell>0.1136</cell><cell>0.1648</cell></row><row><cell>FASILKOMUI01</cell><cell>0.0660</cell><cell>0.5701</cell><cell>0.7894 0.6154</cell><cell>0.0883</cell><cell>0.1519</cell></row><row><cell>watcs12a</cell><cell>0.0049</cell><cell>0.0120</cell><cell>0.0134 0.6967</cell><cell>0.5790</cell><cell>0.6784</cell></row><row><cell>watcs12b</cell><cell>0.0000</cell><cell>0.0147</cell><cell>0.0187 0.5365</cell><cell>0.6117</cell><cell>0.6833</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,203.18,535.79,205.63,8.74"><head>Table 2 :</head><label>2</label><figDesc>All 6 MRR measures sorted by WGT.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,100.54,574.84,426.42,8.74;8,100.54,586.79,110.03,8.74" xml:id="b0">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="8,100.54,574.84,390.23,8.74">Frontiers, challenges and opportunities for information retrieval: Report from SWIRL 2012</title>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
