<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.40,114.69,277.25,12.31">Overview of the TREC 2012 Web Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.00,148.42,105.76,10.97"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.00,148.42,70.08,10.97;1,293.04,162.46,48.10,10.97"><forename type="first">Nick</forename><forename type="middle">Craswell</forename><surname>Microsoft</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.32,148.42,93.77,10.97"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.40,114.69,277.25,12.31">Overview of the TREC 2012 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A8C0C122A69873F8B866D4F85729788E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>If you are an experienced participant, you may not need to read the full report. Apart from the results themselves (see tables 1, 2, and 3) little has changed from TREC 2011 <ref type="bibr" coords="1,441.96,263.14,10.91,10.91" target="#b5">[6]</ref>. A six-point scale was used for relevance assessment (see section 4.1). Limitations on available assessor time meant that some topics were judged to depth 30 and others to depth 20, as well as causing other minor problems (see section 4.3). However, our plans for next year, as outlined in the concluding section, are quite different from this year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Web Track explores and evaluates Web retrieval technology over large collections of Web data. In its current incarnation, the Web Track has been active since TREC 2009, where it included both a traditional adhoc retrieval task and a new diversity task <ref type="bibr" coords="1,441.48,403.06,10.91,10.91" target="#b3">[4]</ref>. The goal of this diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. For TREC 2010 the track introduced a new Web spam task <ref type="bibr" coords="1,152.28,443.74,10.82,10.91" target="#b4">[5]</ref>. For both TREC 2011 and 2012, we dropped the spam task but continued the other two tasks essentially unchanged. As we did since TREC 2009, we based our TREC 2012 experiments on the billion-page ClueWeb09<ref type="foot" coords="1,303.36,469.78,4.23,5.47" target="#foot_0">1</ref> collection created by the Language Technologies Institute at Carnegie Mellon University.</p><p>The two tasks use a common topic set, differing only in their evaluation methodology. Topics are created from the logs of a commercial search engine, with the aid of tools developed at Microsoft Research <ref type="bibr" coords="1,117.36,525.10,15.95,10.91" target="#b9">[10]</ref>. Given a target query, these tools extract and analyze groups of related queries, using co-clicks and other information, to identify clusters of queries that highlight different aspects and interpretations of the target query. These clusters are employed by NIST for topic development. Each resulting topic is structured as a representative set of subtopics, each related to a different user need. The selection of subtopics attempts to reflect a mix of genuine user requirements for the topic.</p><p>For the adhoc task documents are judged with respect to the topic as a whole. Relevance levels are similar in structure to the levels used in commercial Web search, including a spam/junk level. Moreover, the top two levels of the assessment structure are closely related to the homepage finding and topic distillation tasks appearing in older Web Tracks. For the diversity task, documents are judged with respect to the subtopics, as well as with respect to the topic as a whole.  <ref type="table" coords="2,119.28,168.34,5.45,10.91" target="#tab_0">1</ref> summarizes participation in the TREC 2012 Web Track. A total of 12 groups participated in the track this year, a slight decrease from last year, when 16 groups participated, and a substantial decrease from 2009 and 2010, when more than 20 groups participated. One group from the University of Delaware, submitted a manual run for the diversity task; all other runs were automatic, with no human intervention at any stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Category A and B Collections</head><p>The billion-page ClueWeb09 collection was crawled from the general Web during January and February 2009, and consists of 25TB of uncompressed data (5TB compressed) in multiple languages. Since some participants were not able to work with the full collection, the track accepted runs based on the smaller "Category B" subset of the full "Category A" collection. This Category B data set comprises about 50 million English-language pages, including the entirety of the English-language Wikipedia. Nonetheless, we strongly encouraged participants to use the full Category A data set, if possible. Results reported in this paper are labeled by their collection category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topics</head><p>NIST created and assessed 50 new topics for the track. Figure <ref type="figure" coords="2,367.68,421.30,5.45,10.91" target="#fig_0">1</ref> provides two examples. Each topic contains a query field, a description field, and several subtopic fields. The query is intended to represent the text a user might enter into a Web search engine, if they were seeking the information indicated by the description field or by any of the subtopics. For the adhoc task, relevance is judged on the basis of the description. For the diversity task, relevance is judged separately with respect to each subtopic. Initially, only the query field was released to track participants. The full topics were not released until the participants had submitted their runs.</p><p>Each topic is assigned one of two types. Topics with ambiguous queries, such as topic 162 in figure <ref type="figure" coords="2,116.40,529.66,4.21,10.91" target="#fig_0">1</ref>, have several unrelated interpretations. One of these interpretations is chosen for the description, while a wider range of interpretations appear in the subtopics. Topics with faceted queries, such as topic 155 in the figure, have one primary interpretation, reflected in the description field. For these queries, the subtopics address various aspects of the broader topic. In all topics, the description field and the first subtopic field are identical.</p><p>Each subtopic is assigned one of two types. Navigational subtopics (with type "nav") assume the user is seeking a specific page or site. Navigational subtopics may often have only a single relevant page. Informational subtopics (with type "inf") assume the user is seeking information without regard to its source, provided that the source is reliable. Informational subtopics may often have a large number of relevant pages. Subtopics were chosen to be roughly balanced in terms of popularity. Strange and unusual aspects and interpretations were avoided as much as possible.</p><p>All topics are expressed in English. Non-English documents are never considered relevant, even if the assessor understands the language of the document and the document would be relevant in that language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology and Measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adhoc Task</head><p>An adhoc task in TREC investigates the performance of systems that search a static set of documents using previously-unseen topics. The goal of an adhoc task is to return a ranking of the documents in the collection in order of decreasing probability of relevance. The probability of relevance of a document is considered independently of other documents that appear before it in the result list.</p><p>For the adhoc task, documents are judged on the basis of the description field using a six-point scale, defined as follows:</p><p>1. Nav: This page represents a home page of an entity directly named by the query; the user may be searching for this specific page or site. (relevance grade 4)</p><p>2. Key: This page or site is dedicated to the topic; authoritative and comprehensive, it is worthy of being a top result in a web search engine. (relevance grade 3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HRel:</head><p>The content of this page provides substantial information on the topic. (relevance grade 3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Rel:</head><p>The content of this page provides some information on the topic, which may be minimal; the relevant information must be on that page, not just promising-looking anchor text pointing to a possibly useful page. (relevance grade 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Non:</head><p>The content of this page does not provide useful information on the topic, but may provide useful information on other topics, including other interpretations of the same query.</p><p>(relevance grade 0)</p><p>6. Junk: This page does not appear to be useful for any reasonable purpose; it may be spam or junk. (relevance grade -2)</p><p>After each description, we list the relevance grade assigned to that level as they appear in the judgment (i.e., qrels) file. These relevance grades are also used for calculating graded effectiveness measures, except that a value of -2 is treated as 0 for this purpose. For binary effectiveness measures, we treat grades 1/2/3/4 as relevant and grades 0/-2 as non-relevant. The primary effectiveness measure for the adhoc task is expected reciprocal rank (ERR) as defined by Chapelle et al. <ref type="bibr" coords="3,200.88,596.62,10.91,10.91" target="#b1">[2]</ref>. We also report a variant of nDCG <ref type="bibr" coords="3,393.36,596.62,10.91,10.91" target="#b8">[9]</ref>, as well as standard binary measures, including mean average precision (MAP) and precision at rank k (P@k). We compute ERR at rank k (ERR@k) as follows:</p><formula xml:id="formula_0" coords="3,217.08,647.63,322.83,32.00">ERR@k = k i=1 R(g i ) i i-1 j=1 (1 -R(g i )),<label>(1)</label></formula><p>&lt;topic number="155" type="faceted"&gt; &lt;query&gt;last supper painting&lt;/query&gt; &lt;description&gt; Find a picture of the Last Supper painting by Leonardo da Vinci. &lt;/description&gt; &lt;subtopic number="1" type="nav"&gt; Find a picture of the Last Supper painting by Leonardo da Vinci. &lt;/subtopic&gt; &lt;subtopic number="2" type="nav"&gt; Are tickets available online to view da Vinci's Last Supper in Milan, Italy? &lt;/subtopic&gt; &lt;subtopic number="3" type="inf"&gt; What is the significance of da Vinci's interpretation of the Last Supper in Catholicism? &lt;/subtopic&gt; &lt;/topic&gt; &lt;topic number="162" type="ambiguous"&gt; &lt;query&gt;dnr&lt;/query&gt; &lt;description&gt; What are "do not resuscitate" orders and how do you get one in place? &lt;/description&gt; &lt;subtopic number="1" type="inf"&gt; What are "do not resuscitate" orders and how do you get one in place? &lt;/subtopic&gt; &lt;subtopic number="2" type="nav"&gt; What is required to get a hunting license online from the Michigan Department of Natural Resources? &lt;/subtopic&gt; &lt;subtopic number="3" type="inf"&gt; What are the Maryland Department of Natural Resources' regulations for deer hunting? &lt;/subtopic&gt; &lt;/topic&gt; where R(g) = 2 g -1 16 and g 1 , g 2 , ..., g k are the relevance grades associated with the top k documents. We compute nDCG@k as DCG@k ideal DCG@k , where</p><formula xml:id="formula_1" coords="5,238.80,117.95,301.11,32.12">DCG@k = k i=1 2 g i -1 log 2 (1 + i) .<label>(2)</label></formula><p>We apply trec_eval to compute MAP and other traditional measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Diversity Task</head><p>The diversity task is similar to the adhoc retrieval task, but differs in its judging criteria and evaluation measures. The goal of the diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. For this task, the probability of relevance of a document is conditioned on the documents that appear before it in the result list.</p><p>For the diversity task, documents are judged on the basis of the subtopics. For each subtopic, a binary judgment indicates whether or not a document satisfies the information need associated with that subtopic. For the TREC 2012 track, assessors made graded judgments. To apply evaluation measures, we mapped these graded judgments to binary judgments by treating values &gt; 0 as relevant and values ≤ 0 as not relevant. However, the graded judgments are available in the TREC data repository for the use of interested participants.</p><p>The primary effectiveness measure for the adhoc task is a variant of intent-aware expected reciprocal rank (ERR-IA) as defined by Chapelle et al. <ref type="bibr" coords="5,332.40,373.18,10.91,10.91" target="#b1">[2]</ref>. We also report a number of other intent aware measures appearing in the literature, including α-nDCG@k <ref type="bibr" coords="5,388.56,386.62,10.91,10.91" target="#b7">[8]</ref>, NRBP <ref type="bibr" coords="5,441.48,386.62,10.91,10.91" target="#b6">[7]</ref>, and MAP-IA <ref type="bibr" coords="5,525.48,386.62,10.82,10.91" target="#b0">[1]</ref>. Clarke et al. <ref type="bibr" coords="5,135.48,400.18,11.54,10.91" target="#b2">[3]</ref> provide a detailed description and analysis of the novelty and diversity measures employed in the TREC Web track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pooling and Judging</head><p>For each topic, participants in the adhoc and diversity tasks submitted a ranking of the top 10,000 documents for that topic. All submitted runs were included in the pool for judging. A common pool was used for both tasks, and all runs were judged using both the adhoc and diversity judging criteria. In this paper, we report results only for runs explicitly submitted to one task or the other.</p><p>We initially planned to judge all runs to depth 30. Unfortunately, judging went more slowly this year than last year, for reasons that are not clear to us. As a result, we cut back the size of the pools for 25 topics to depth 20. The topics with depth-20 pools are <ref type="bibr" coords="5,425.52,544.90,114.26,10.91;5,72.00,558.46,456.74,10.91">152, 156, 159, 160, 161, 164, 166, 167, 169, 173, 177, 179, 181, 183, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198.</ref> Even with depth-20 pools, two topics had documents remaining to be judged when available assessor time ran out. Topic 156 ("university of phoenix") had about 20 or so documents remaining, while topic 185 ("credit report") had about one-third of the documents remaining. We asked a researcher at NIST to finish judging these last two topics. We included these extra judgments in the official qrels for topics 156 and 185, but you may wish to exclude these topics when using the collection in future research, particularly if single-assessor judging is important.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Table <ref type="table" coords="6,101.52,403.66,5.45,10.91" target="#tab_1">2</ref> presents the top adhoc task results ordered by ERR@20. Table <ref type="table" coords="6,411.60,403.66,5.45,10.91" target="#tab_2">3</ref> presents the top diversity task results ordered by ERR@20. The figures mix results for both Category A and B runs.</p><p>All runs submitted to the adhoc and diversity tasks were judged according to the judging criteria of both tasks, even runs that were not submitted to both tasks. This additional judging allows us to make direct comparisons between runs optimized for the two tasks, supporting efforts to determine if the different judging criteria and evaluation measures identify genuine differences. For example, figure <ref type="figure" coords="6,149.40,485.02,5.45,10.91" target="#fig_2">2</ref> provides a scatter plot comparing the performance of the runs under ERR@20 and ERR-IA@20, the primary effectiveness measures for the adhoc and diversity tasks respectively. While the values are correlated, there are clear differences in the relative performance of runs under the two measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Plans</head><p>The Web Track will undergo a substantial change for TREC 2013. While the adhoc task will continue, we plan to drop the diversity task in favor of a new risk-sensitive retrieval task. This new task will explore the tradeoffs systems can achieve between effectiveness (overall gains across queries) and robustness (minimizing the possibility of significant failure, relative to a given baseline).</p><p>In addition, Jamie Callan's research group at CMU -who created the ClueWeb09 collectionhave created a new ClueWeb12 collection. The size of this new collection is similar to that of ClueWeb09, but it addresses known problems with the existing collection. We plan to switch to this new collection for TREC 2013.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Cat Type ERR@20 nDCG@20 P@20 MAP </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,176.64,592.78,253.70,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of TREC 2012 Web track topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,156.41,298.87,3.89,6.48;6,150.57,262.02,9.73,6.48;6,150.57,225.17,9.73,6.47;6,150.57,188.27,9.73,6.47;6,150.57,151.42,9.73,6.47;6,150.57,114.57,9.73,6.47;6,150.57,77.72,9.73,6.47;6,163.53,305.87,3.89,6.48;6,197.11,305.87,13.62,6.48;6,237.46,305.87,9.73,6.48;6,273.96,305.87,13.62,6.48;6,314.36,305.87,9.73,6.48;6,350.81,305.87,13.62,6.48;6,391.21,305.87,9.73,6.48;6,427.66,305.87,13.62,6.48;6,468.06,305.87,9.73,6.48;6,133.07,215.11,6.47,38.61;6,133.07,187.50,6.47,25.66;6,133.07,159.88,6.48,25.67;6,133.07,128.38,6.47,29.56;6,263.31,316.37,109.80,6.48"><head></head><label></label><figDesc>@20 (primary diversity measure) ERR@20 (primary adhoc measure)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,82.68,346.30,441.74,10.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of runs under the primary adhoc and diversity effectiveness measures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.92,73.87,341.99,105.38"><head>Table 1 :</head><label>1</label><figDesc>Participation in the TREC 2012 Web track Table</figDesc><table coords="2,210.96,73.87,190.08,37.82"><row><cell cols="4">Task Adhoc Diversity Total</cell></row><row><cell>Groups</cell><cell>11</cell><cell>8</cell><cell>12</cell></row><row><cell>Runs</cell><cell>28</cell><cell>20</cell><cell>48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,72.00,87.22,468.05,286.19"><head>Table 2 :</head><label>2</label><figDesc>Top adhoc task results ordered by ERR@20. Only the best run from each group is included in the ranking.</figDesc><table coords="7,88.68,87.22,431.94,286.19"><row><cell>uogTr</cell><cell>uogTrA44xi</cell><cell>A</cell><cell>auto</cell><cell>0.313</cell><cell>0.238</cell><cell>0.453</cell><cell>0.212</cell></row><row><cell>srchvrs</cell><cell>srchvrs12c09</cell><cell>A</cell><cell>auto</cell><cell>0.305</cell><cell>0.176</cell><cell>0.315</cell><cell>0.126</cell></row><row><cell>uottawa</cell><cell>DFalah121A</cell><cell>B</cell><cell>auto</cell><cell>0.299</cell><cell>0.214</cell><cell>0.405</cell><cell>0.120</cell></row><row><cell cols="2">QUT Para QUTparaBline</cell><cell>B</cell><cell>auto</cell><cell>0.290</cell><cell>0.167</cell><cell>0.305</cell><cell>0.117</cell></row><row><cell>utwente</cell><cell>utw2012fc1</cell><cell>B</cell><cell>auto</cell><cell>0.219</cell><cell>0.113</cell><cell>0.221</cell><cell>0.061</cell></row><row><cell>ICTNET</cell><cell>ICTNET12ADR2</cell><cell>A</cell><cell>auto</cell><cell>0.215</cell><cell>0.110</cell><cell>0.257</cell><cell>0.078</cell></row><row><cell>IRRA</cell><cell>irra12c</cell><cell>B</cell><cell>auto</cell><cell>0.173</cell><cell>0.143</cell><cell>0.367</cell><cell>0.153</cell></row><row><cell>qutir12</cell><cell>qutwb</cell><cell>B</cell><cell>auto</cell><cell>0.166</cell><cell>0.146</cell><cell>0.308</cell><cell>0.131</cell></row><row><cell>Group</cell><cell>Run</cell><cell cols="6">Cat Type ERR-IA@20 α-nDCG@20 NRBP</cell></row><row><cell>uogTr</cell><cell>uogTrA44xu</cell><cell>A</cell><cell>auto</cell><cell>0.505</cell><cell>0.606</cell><cell></cell><cell>0.463</cell></row><row><cell>uottawa</cell><cell>DFalah121D</cell><cell>B</cell><cell>auto</cell><cell>0.431</cell><cell>0.525</cell><cell></cell><cell>0.394</cell></row><row><cell>utwente</cell><cell>utw2012c1</cell><cell>B</cell><cell>auto</cell><cell>0.405</cell><cell>0.508</cell><cell></cell><cell>0.357</cell></row><row><cell>srchvrs</cell><cell>srchvrs12c00</cell><cell>A</cell><cell>auto</cell><cell>0.386</cell><cell>0.485</cell><cell></cell><cell>0.340</cell></row><row><cell cols="2">ICTNET ICTNET12DVR1</cell><cell>A</cell><cell>auto</cell><cell>0.326</cell><cell>0.422</cell><cell></cell><cell>0.280</cell></row><row><cell>udel</cell><cell>autoSTA</cell><cell>A</cell><cell>auto</cell><cell>0.325</cell><cell>0.419</cell><cell></cell><cell>0.282</cell></row><row><cell>LIA</cell><cell>lcm4res</cell><cell>A</cell><cell>auto</cell><cell>0.318</cell><cell>0.424</cell><cell></cell><cell>0.268</cell></row><row><cell cols="2">udel fang UDInfoDivSt</cell><cell>B</cell><cell>auto</cell><cell>0.300</cell><cell>0.420</cell><cell></cell><cell>0.241</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,397.06,467.98,24.47"><head>Table 3 :</head><label>3</label><figDesc>Top diversity task results ordered by ERR-IA@20. Only the best run from each group is included in the ranking.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.56,682.96,171.27,6.22"><p>boston.lti.cs.cmu.edu/Data/clueweb09.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Again this year, we extend our thanks to <rs type="person">Jamie Callan</rs>, <rs type="person">Mark Hoy</rs>, and the <rs type="institution">Language Technologies Institute at Carnegie Mellon University</rs>, who created and continue to distribute the ClueWeb09 collection. The track could not operate without this valuable resource.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,94.44,553.78,445.37,10.91;7,94.44,567.34,445.34,10.91;7,94.44,580.90,111.74,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,450.12,553.78,89.70,10.91;7,94.44,567.34,29.84,10.91">Diversifying search results</title>
		<author>
			<persName coords=""><forename type="first">Sreenivas</forename><surname>Rakesh Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,147.36,567.71,328.96,10.05">2nd ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.44,603.46,445.55,10.91;7,94.44,617.02,445.46,10.91;7,94.44,630.58,99.26,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,419.88,603.46,120.11,10.91;7,94.44,617.02,94.40,10.91">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,212.76,617.39,321.35,10.05">18th ACM Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.44,653.02,445.53,10.91;7,94.44,666.58,445.66,10.91;7,94.44,680.14,208.70,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,412.92,653.02,127.05,10.91;7,94.44,666.58,205.88,10.91">A comparative analysis of cascade measures for novelty and diversity</title>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azin</forename><surname>Ashkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.72,666.95,212.38,10.05;7,94.44,680.51,114.40,10.05">th ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.44,74.74,445.34,10.91;8,94.44,88.30,317.54,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,354.36,74.74,180.90,10.91">Overview of the TREC 2009 web track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,108.12,88.67,148.46,10.05">18th Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.44,110.86,445.60,10.91;8,94.44,124.42,432.14,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,464.04,110.86,76.00,10.91;8,94.44,124.42,105.06,10.91">Overview of the TREC 2010 web track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,222.60,124.79,148.46,10.05">19th Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.44,146.86,445.60,10.91;8,94.44,160.42,432.14,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,462.96,146.86,77.08,10.91;8,94.44,160.42,105.06,10.91">Overview of the TREC 2011 web track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,222.60,160.79,148.46,10.05">20th Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.44,182.98,445.60,10.91;8,94.44,196.54,445.54,10.91;8,94.44,210.10,209.18,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,418.56,182.98,121.48,10.91;8,94.44,196.54,196.26,10.91">An effectiveness measure for ambiguous and underspecified queries</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,314.76,196.91,225.22,10.05;8,94.44,210.47,101.70,10.05">2nd International Conference on the Theory of Information Retrieval</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="188" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.44,232.54,445.70,10.91;8,94.44,246.10,445.47,10.91;8,94.44,259.66,445.57,10.91;8,94.44,273.22,262.82,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,283.56,246.10,256.35,10.91;8,94.44,259.66,17.39,10.91">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Ashkann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,134.88,260.03,405.13,10.05;8,94.44,273.59,101.70,10.05">31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.44,295.78,445.46,10.91;8,94.44,309.34,312.62,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,294.00,295.78,241.11,10.91">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,94.44,309.71,206.36,10.05">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.44,331.78,445.47,10.91;8,94.44,345.34,445.46,10.91;8,94.44,358.90,52.82,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,357.00,331.78,182.91,10.91;8,94.44,345.34,71.57,10.91">Inferring query intent from reformulations and clicks</title>
		<author>
			<persName coords=""><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,187.80,345.71,226.22,10.05">19th International World Wide Web Conference</title>
		<meeting><address><addrLine>Raleigh, North Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
