<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,131.69,102.08,348.63,15.12">Overview of the TREC 2012 Crowdsourcing Track</title>
				<funder>
					<orgName type="full">DARPA Young Faculty Award</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Waterloo</orgName>
				</funder>
				<funder>
					<orgName type="full">John P. Commons Fellowship</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,156.25,134.57,90.23,10.48"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences</orgName>
								<address>
									<country>University of Waterloo</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.36,134.57,80.08,10.48"><forename type="first">Gabriella</forename><surname>Kazai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.08,134.57,77.93,10.48"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,131.69,102.08,348.63,15.12">Overview of the TREC 2012 Crowdsourcing Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9AC0D30F3C1CCCC7E78768A44F6C4A6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In 2012, the Crowdsourcing track had two separate tasks: a text relevance assessing task (TRAT) and an image relevance assessing task (IRAT). This track overview describes the track and provides analysis of the track's results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>2012 was the second year of the TREC Crowdsourcing track. Track goals for the first two years included: building awareness and expertise with crowdsourcing in the IR community, developing and evaluating new methodologies for crowdsourced search evaluation on a shared task and data set, and creating reusable resources to benefit future IR community experimentation.</p><p>The first year was explicitly focused on crowdsourcing. In 2012, we decided to loosen the crowdsourcing requirements and instead focus on a goal of obtaining quality relevance judgments by any means. Any crowdsourcing approach, paid or non-paid, and any platform or home-grown system could be used to obtain the relevance labels, as well as hybrid or fullyautomatic methods. The advantage of this change was to give groups freedom in the creation of their solutions. In addition, in year two, we went to simpler data collections and also increased the scale of judgments required by nearly a factor of 10, which brought the task's scale to be much more representative of the type of challenge crowdsourcing methods face. The open-ended task and increased scale has led to innovative attempts at obtaining high quality relevance judgments at low cost, and many of the participating groups have chosen to break new ground with the combination of machine learning and human relevance judgments.</p><p>The track consisted of two tasks. One track was a text relevance assessing task (TRAT) and the other was an image relevance assessing task (IRAT). Seven groups participated in TRAT and 2 groups participated in IRAT. We next separately describe each of the tasks, their data, evaluation methods, and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TRAT</head><p>The text relevance assessing task (TRAT) was one of the two TREC 2012 Crowdsourcing Track tasks. The goal of the task was to evaluate approaches to text relevance assessing. We assumed that many participating groups would utilize traditional crowdsourcing platforms, such as Amazon Mechanical Turk, to do the relevance assessing, but the task was open to all approaches that followed the task's guidelines.</p><p>The TRAT required participating groups to simulate the relevance assessing role of NIST for 10 of the TREC 8 <ref type="bibr" coords="1,373.60,449.18,15.50,8.74" target="#b10">[11]</ref> ad-hoc topics. A key difference between NIST and the participating groups is that the NIST assessors created the search topics and then determined the relevance of documents, while TRAT groups had to rely on their interpretation of the search topic's description and narrative to determine the relevance of a document. While the topic narrative aims to capture a description of what is relevant, it is always possible that the NIST assessors made relevance judgments based on notions of relevance known only to them.</p><p>Participating groups had to submit a binary relevance judgment for every document in the judging pools of the ten topics. The submission of a probability of relevance (1.0 means relevant and 0.0 means non-relevant) was optional, but if it was submitted, had to be submitted for all documents in a run.</p><p>We placed no limits on the methods that could be employed to obtain relevance judgments. The only restriction was that the NIST judgments (qrels) for TREC 8 and later tracks that used the TREC 8 topics (401-450) were forbidden for use in any way, shape, or manner. For example, participants were forbidden from even using the qrels to help them understand the topic or train workers. Nor could participants make simple counts of the number of relevant documents etc. For training and other purposes, we suggested that participants use the TREC 7 ad-hoc topics and qrels. TREC 7 ad-hoc used the same document collection and had similar topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>TRAT used the TREC 8 ad-hoc track as its source of data. We randomly selected 10 topics from the 50 topics used in TREC 8. The 10 topics selected were: 411, 416, 417, 420, 427, 432, 438, 445, 446, and 447. For these topics, we selected all topic-document pairs that NIST had judged in the qrels. In total, 18260 topic-document pairs needed to be judged. These topic-document pairs represented the "test set" for the TRAT.</p><p>TRAT used the existing topic descriptions. The topic's title, description, and narrative when taken together defined what the original NIST assessor considered to be relevant and non-relevant to the topic. We clarified to the track participants that the narrative was to take precedence over the description and title, but did not provide further guidance on how to determine relevance other than to provide copies of relevant portions of the TREC 7 and 8 judging guidelines provided to the NIST assessors.</p><p>The documents to be judged came from the corpora used by TREC 7 and 8: the Text Research Collection Volumes 4 (May 1996) and 5 (April 1997) minus the Congressional Record (CR). This collection is made available for free to registered TREC participants. The documents come from the following subcollections of volumes 4 and 5:</p><p>1. Financial Times, 1991-1994, (FT) 2. Federal Register, 1994 (FR94)</p><p>3. Foreign Broadcast Information Service (FBIS)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Los Angeles Times (LA).</head><p>We advised participants that they could make use of the submitted runs to TREC 8. These same runs were used by NIST to form the judging pools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adjudication</head><p>Each participating group designated one of their runs as a primary run for use in a majority vote consensus process. All other runs from a group were secondary runs. The majority vote of the submitted runs was compared to the NIST relevance judgment. When the majority vote differed from the NIST judgment, we adjudicated the final relevance judgment for a document. In total, 459 documents needed adjudication. Mark Smucker's research group at the University of Waterloo performed the adjudication of the 459 documents. The University of Waterloo did not participate in the track. Mark and two graduate research assistants, Gaurav Baruah and Le Li, separately judged each of the 459 documents. Prior to judging the documents, each judge practiced making judgments for the topic on up to 10 documents (half relevant, half non-relevant) where the majority consensus had strongly agreed with the NIST assessor. For each document, the judges recorded their decision, and for relevant documents marked the location of relevant material, and for non-relevant document, the judges recorded a written reason why the document was not relevant. Of the 459 documents, the judges did not have full agreement on the relevance or differed from the NIST assessor on 270 documents. For these 270 documents, the three judges sat down together and reviewed the documents to reach a final decision. To aid our review, we created a separate system to view our individual judgments on documents and then select a final verdict. For the first 31 judgments we made, the NIST qrel was displayed to us, and fearing that it might bias us, we removed the display of the NIST qrel from the remaining documents.</p><p>Table <ref type="table" coords="2,348.80,424.80,4.98,8.74">1</ref> shows some basic statistics about the adjudication. One issue with the adjudication process was that some of the runs were very conservative in their judgments and had very low true positive rates. When we took a majority vote of the runs' judgments, the result is that the majority vote is quite conservative and most disagreements with NIST occur on documents NIST said were relevant but which the majority vote said were non-relevant. Overall, we found NIST to be correct in most cases. Table <ref type="table" coords="2,504.86,532.40,4.98,8.74" target="#tab_1">2</ref> shows a breakdown of the number of times we reversed a NIST qrel. When a qrel was reversed, it could have been because of a variety of reasons. In some cases, we could find no reason a document was to be considered relevant, and in other cases it might have been that the document did not appear relevant given the search topic. In many ways, the adjudication acts as a means to create a set of qrels that reflects the standard attainable given a third party reading of the search topic. The reality is that only the NIST assessor would be able to know which "mistakes" were actual mistakes and which were times when the assessor's notion of relevance simply failed to be captured by the search topic's description and narrative.</p><p>As we did the adjudication, certain topics such as  <ref type="table" coords="3,99.31,216.77,3.87,8.74">1</ref>: Each of the 10 TRAT topics were adjudicated. The columns shown from left to right are the topic title, topic number, total number of documents judged by participants, number of documents judged relevant by NIST, the prevalence of NIST relevant documents, the number of documents adjudicated, the percent of documents adjudicated, and the fraction of adjudicated documents that were NIST relevant.</p><p>432 and 446 were quite difficult to judge given the topic's description and narrative. We recommend in the future that some sort of trial adjudication take place prior to the release of topics for crowdsourcing tracks. Such trial adjudication efforts could identify problematic topics that might be considered inappropriate for crowdsourcing given their descriptions and narratives. It might be possible on examination of an assessor's qrels to adjust the topic's narrative. Alternatively, crowdsourcing tracks could consider adopting the notion of a topic authority as found in the TREC Legal track <ref type="bibr" coords="3,156.02,425.93,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation</head><p>All runs are evaluated against the adjudicated qrels.</p><p>For each topic, the performance of a submitted run is judged on both its binary judgments and its probabilities of relevance. The binary judgments were evaluated using the logistic average misclassification rate (LAM) developed for the Spam Track <ref type="bibr" coords="3,240.18,542.21,9.96,8.74" target="#b1">[2]</ref>:</p><formula xml:id="formula_0" coords="3,80.22,566.22,220.80,22.31">LAM = logit -1 logit(f pr) + logit(f nr) 2 ,<label>(1)</label></formula><p>where relevant documents are the positive class, f pr is the false positive rate, f nr is the false negative rate, and</p><formula xml:id="formula_1" coords="3,132.53,647.10,168.49,22.31">logit(p) = log p 1 -p ,<label>(2)</label></formula><p>and We smoothed the calculation of fpr and fnr:</p><formula xml:id="formula_2" coords="3,138.50,702.20,162.52,23.89">logit -1 = e x 1 + e x .<label>(3)</label></formula><formula xml:id="formula_3" coords="3,361.48,586.24,178.53,22.31">f pr = |F P | + 0.5 |F P | + |T N | + 1 ,<label>(4)</label></formula><formula xml:id="formula_4" coords="3,362.17,613.76,177.83,22.31">f nr = |F N | + 0.5 |F N | + |T P | + 1 ,<label>(5)</label></formula><p>where |F P | is the number of false positives, etc, as given in Table <ref type="table" coords="3,376.02,663.65,3.87,8.74" target="#tab_2">3</ref>.</p><p>We measured the performance of the runs given their probabilities of relevance with the area under the ROC curve (AUC). Only runs that provided probabilities were evaluated using AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjudicated Standard Run Submission</head><p>Relevant (Pos.) Non-Relevant (Neg.) Relevant T P = True Pos. F P = False Pos. Non-Relevant F N = False Neg.</p><p>T N = True Neg. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>Seven groups submitted 33 runs, and of these 33 runs, 28 runs also had probabilities of relevance. The seven groups and their run prefixes were:</p><p>• Northeastern University <ref type="bibr" coords="5,200.05,136.52,9.96,8.74" target="#b0">[1]</ref>, runs: NEU*</p><p>• PRIS Lab at Beijing University of Posts and Telecommunications <ref type="bibr" coords="5,182.95,164.98,14.61,8.74" target="#b11">[12]</ref>, runs: BUPTPRISZHS</p><p>• SetuServ <ref type="bibr" coords="5,133.74,181.49,9.96,8.74" target="#b5">[6]</ref>, runs: SS*</p><p>• Stanford University, runs: INFLBSTF</p><p>• University of Iowa <ref type="bibr" coords="5,174.76,214.50,9.96,8.74" target="#b2">[3]</ref>, runs: UIowaS*</p><p>• University of Oxford and University of Southampton <ref type="bibr" coords="5,153.91,242.96,9.96,8.74" target="#b7">[8]</ref>, runs: Orc*</p><p>• York University <ref type="bibr" coords="5,163.99,259.47,9.96,8.74" target="#b4">[5]</ref>, runs: york* Table <ref type="table" coords="5,108.32,278.53,4.98,8.74" target="#tab_3">4</ref> shows the top runs from each group ordered by their LAM score (lower LAM is better). Table <ref type="table" coords="5,296.04,290.48,4.98,8.74" target="#tab_4">5</ref> shows the top runs from each group ordered by the AUC score (higher AUC is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Discussion</head><p>While the best runs on the LAM and AUC measures look very good with high true positive rates and low false positive rates, the LAM and AUC measures do not appear to be ideally suited for evaluating relevance judgments for their ability to evaluate retrieval runs. Of note, several groups' best LAM score occurred on their runs with low false positive rates while their true positive rates suffered. In the case of the run INFLB2012, it received a lower LAM than 3 other groups even though the run only had an average true positive rate of less than 0.04 and a correspondingly high average false negative rate of 0.964. While a low false positive rate is beneficial, such a high false negative rate may make evaluation of retrieval runs difficult.</p><p>On further investigation, we found that this apparent issue with LAM was not with the measure itself but with our smoothing of the FPR and FNR rates. On the advice of Charles Clarke, we modified the smoothing to be proportional to the number of relevant and non-relevant documents:</p><formula xml:id="formula_5" coords="5,105.20,616.94,195.82,22.31">f pr = |F P | + 0.5(1 -R/N ) |F P | + |T N | + (1 -R/N ) ,<label>(6)</label></formula><formula xml:id="formula_6" coords="5,104.23,644.47,192.54,22.31">f nr = |F N | + 0.5(R/N ) |F N | + |T P | + R/N , (<label>7</label></formula><formula xml:id="formula_7" coords="5,296.78,651.21,4.24,8.74">)</formula><p>where R is the number of relevant documents and N is the total number of documents. When we make this change to the smoothing, the non-official results shown in Table <ref type="table" coords="5,141.24,713.20,4.98,8.74" target="#tab_5">6</ref> seem more reasonable.</p><p>While the improved smoothing methodology of Equations 6 and 7 helps LAM better reflect performance, LAM may not tell us which relevance assessing process leads to the best evaluation of a set of retrieval runs. We leave for future work a deeper investigation of LAM's utility for evaluation of crowdsourced relevance judgments.</p><p>We mentioned earlier that during adjudication, topic 432 was one of the more difficult topics to adjudicate. It turns out that when we computed the average LAM per topic across all submitted runs using the improved smoothing of rates, topic 432 had an average LAM of 0.34, and the topic with the next greatest LAM was topic 420 with a LAM of 0.25. For the crowdsourcing runs, the easiest topic was topic 416 with a LAM of 0.16. Based on the average LAM and our experience with adjudicating it, topic 432 might not be adequately described by its description and narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Relevance Assessing</head><p>Task (IRAT)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>The Image Relevance Assessment Task (IRAT) was one of the two tasks of this year's crowdsourcing track. The challenge of this task was for participants to crowdsource high quality relevance judgments for 20k topic-image pairs. In addition to the task given to participants, we were interested in exploring the relationship between judgment quality and the ratio of relevant and non-relevant images for a given topic's assessment pool: Can we observe differences in the quality of the crowdsourced relevance judgments submitted by the participants across topics with different levels of relevance saturation?</p><p>The image labeling task was chosen motivated by its roots in crowdsourcing, e.g., the popular ESP game <ref type="bibr" coords="5,338.31,557.78,14.61,8.74" target="#b9">[10]</ref>, but with the specific aim to investigate crowdsourcing issues in test collection creation that go beyond textual documents. Unlike typical image labeling tasks, where images are labeled with descriptive concepts, e.g., sky, clouds, birds, the task in IRAT was to gather relevance decisions for a set of 90 search topics and 20k images (together with image captions). This task reflects a standard test collection creation scenario enabling the evaluation of image search systems. A property of the task setup was that oftentimes only the combination of an image and its caption together revealed whether it was relevant to a given query or not. This was aimed to encourage the development of hybrid systems, where caption-based retrieval is augmented with human judgments on a sample of the images.</p><p>While we expected this task to draw in new participants due to its appeal in crowdsourcing, only four groups took part in the challenge and, of those, only two groups submitted their results to the track. Despite this, the task resulted in a high quality re-usable test collection, annotated with both NIST and crowdsourced relevance labels. In the following, we detail the creation of the test collection and the evaluation results of the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The IRAT Test Collection</head><p>To build the IRAT test corpus, we partnered with ImageCLEF and made use of two image collections that were previously or presently used by the Image-CLEF evaluation forum. These collections were selected based on similarities with the relevance assessing task setup of IRAT. In particular, we made use of subsets of the BELGA corpus from ImageCLEF 2009 <ref type="bibr" coords="6,96.48,324.89,10.52,8.74" target="#b6">[7]</ref> and the MIRFLICKR corpus from Image-CLEF 2012 <ref type="bibr" coords="6,127.56,336.84,9.96,8.74" target="#b8">[9]</ref>. The complete BELGA test collection consists of around 500,000 images with associated captions, provided by the BELGA news agency, 50 diversity test topics, each with several subtopics, and subtopic level relevance judgments that were obtained by the the ImageCLEF 2009 organizers via crowdsourcing. The MIRFLICKR corpus contains about one million images for which 42 test topics were created by the ImageCLEF 2012 organizers in the context of the Concept Retrieval subtask 1 .</p><p>From the original 50 diversity topics of the BELGA test set (with 206 subtopics), we selected 70 subtopics (from 29 topics) that could be used in a standalone manner. Furthermore, to allow us to study the effects of relevance saturation in the assessment pools on the quality of crowdsourced relevance judgments, the 70 subtopics were selected taking into account the distribution of the relevance labels in the ImageCLEF 2009 qrels. We defined 7 buckets of relevance saturation, varying between 20% to 80% relevant, in steps of 10%. Then, for each topic, we selected 200 images from the ImageCLEF 2009 qrels using a stratified sampling approach, such that we ended up with 10 topics in each bucket, e.g., in the 20% bucket we have 10 topics, each with around 40 relevant images out of the total 200. Due to errors in the crowdsourced qrels, the selection process was guided by manual inspection and adjudication by the organiser, Gabriella Kazai. One topic (topic 59) had to be removed due to errors in the qrels. The selected images, plus an additional up to 30 images per topic were then judged by one or two   NIST assessors (15 of the 69 topics were judged by two assessors). We will refer to the judgment sets as A1 and A2; the topics judged only by a single NIST assessor are added to both A1 and A2. The interassessor agreement measured using Cohen's Kappa for the 15 double judged topics is 0.77 (p &lt; 0.001).</p><p>The raw agreement, calculated as the ratio of the number of matching judgments and total judgments is 88.73%. Using the NIST judgments as the groundtruth, the final set of 200 images per topic were then selected into the IRAT test collection, staying as close as possible to the original stratified sample distributions. However, due to disagreements between the NIST and the original crowdsourced relevance labels by ImageCLEF 2009, a couple of topics fell into different buckets. The final number of topics in the different buckets is shown in Table <ref type="table" coords="6,464.12,552.82,4.98,8.74" target="#tab_6">7</ref> for both the A1 and A2 ground-truth sets. An example topic is shown in Figure <ref type="figure" coords="6,486.65,578.39,3.87,8.74" target="#fig_1">1</ref>. As it can be seen, for each BELGA topic an example relevant image was included in the topic description.</p><p>Another 70 subtopics were selected and distributed with their full set of IamgeCLEF 2009 qrels to the participants as training set.</p><p>In addition to the 69 topics from ImageCLEF 2009, we picked 20 topics out of the 42 topics created in this year's ImageCLEF campaign. Each of these topics contained three example images in the topic description. For each of these topics, 300 images were selected from the pooled submission runs of the Im-ageCLEF 2012 participants by the ImageCLEF organizers and added into the IRAT test collection that was then distributed to the crowdsourcing track participants. However, out of the 300 images per topic, only 230 were judged by NIST assessors (19 topics by two assessor and one topic by one assessor only). We will use the same convention and refer to the NIST ground-truth sets as A1 and A2. The Cohen's Kappa agreement between the two NIST judges is 0.82 (p &lt; 0.001) and the raw agreement ratio is 94.98%. No training set was provided from the Im-ageCLEF 2012 data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Submissions</head><p>Participating groups were required to submit runs, each containing at least one relevance label per sample for all the 19,800 topic-image pairs in the distributed test set (69 topics x 200 images, plus 20 topics x 300 images). All crowdsourced or automatically derived labels had to be submitted. One of the submitted runs had to be designated as the primary run. As in TRAT, participating groups had to submit a binary relevance judgment for every image and an optional probability of relevance value (1.0 means relevant and 0.0 means non-relevant).</p><p>Two groups submitted runs: SetuServ submitted 4 runs and UAustin contributed one run. All the runs contained exactly one label per topic-image pair, all of which was labelled as automatically derived. This either means that the teams did not crowdsource any labels or that they omitted those labels from the submission, keeping only the final label per sample. Either way, as a result, we do not have any worker information upon which additional worker reliability analysis could have been performed. Thus, our evaluation in the next section focuses on the accuracy of the submitted labels only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Results</head><p>Table <ref type="table" coords="7,99.54,565.96,4.98,8.74" target="#tab_7">8</ref> shows the evaluation results for the two primary runs submitted by SetuServ and UAustin. In addition to the metrics discussed in the TRAT section, i.e., LAM and AUC, we also report the number of true positive (TP), true negative (TN), false positive (FP) and false negative (FN) classifications, precision (P), recall (R), accuracy (Acc) and specificity (Spec), where:</p><formula xml:id="formula_8" coords="7,119.11,72.00,347.95,654.08">Acc = T P + T N T P + T N + F P + F N , P = T P T P + F P , R = T P T P + F N , Spec = T N T N + F P .</formula><p>All evaluation results are calculated over the two judgment sets A1 and A2, both containing all topics that were judged by a single NIST assessor and then A1 including one set of judgments for the double-judged topics and A2 including the other judgment set. The min/max/avg results are the worst/best/average performances calculated by picking topics with min/max accuracy (Acc) over the A1 and A2 judgment sets. Note that for the 20 Image-CLEF 2012 topics, we ignored the images in the submissions that were not judged by NIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Relevance Saturation and Judgment Quality</head><p>Figure <ref type="figure" coords="7,341.40,308.26,4.98,8.74" target="#fig_2">2</ref> shows the distribution of Accuracy and LAM scores (using Equations 4 and 5) over the two primary runs submitted by the two participating groups, calculated over the two ground-truth sets A1 and A2. Both Accuracy and LAM show the same trends: both runs perform better when the assessment pools consist largely relevant or non-relevant images. As the number of relevant and non-relevant images gets closer to equal, the performance decreases. One exception is the 40% bucket, which may have contained relatively easy topics. Thus, if the groups obtained the relevance labels from crowd workers, we may then reason that crowds do better on "spot the odd one out" type tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Conclusions and Future Plans</head><p>While the overall performance scores obtained by the participating groups (Accuracy between 0.7-0.9) show already good results, the difference between the runs demonstrates the benefits of different crowdsourcing approaches. In addition, our own investigation of the relationship between judgment quality and relevance saturation levels in the assessment pools suggest that further gains are to be had when relevance saturation can be estimated and the crowdsourcing task tailored accordingly.</p><p>Regarding the future of this task, it is unlikely that IRAT will continue again in 2013. However, we will aim to make the dataset available in the future so that crowdsourcing practitioners may use it for research purposes (subject to license clearance). In 2013, we will aim to partner with the TREC Web Track and run a single web page relevance assessing task combining both text and image media.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,83.09,713.10,165.55,8.67;6,321.44,73.92,61.60,7.89;6,399.47,73.92,146.01,7.89;6,316.96,84.88,90.12,7.89;6,422.74,84.90,54.35,7.86;6,492.73,84.90,4.61,7.86;6,513.00,84.90,31.78,7.86;6,316.96,95.84,90.12,7.89;6,425.04,95.86,4.61,7.86;6,447.61,95.86,4.61,7.86;6,467.86,95.86,76.92,7.86"><head>1</head><label></label><figDesc>http://imageclef.org/2012/photo-flickr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,323.60,326.41,203.79,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Topic 20001 from the track's test set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,72.00,527.80,229.02,8.74;8,72.00,539.75,229.01,8.74;8,72.00,551.71,52.92,8.74;8,72.00,386.36,114.51,114.27"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy and LAM (using Equations 4 and 5) score distributions across relevance saturation buckets</figDesc><graphic coords="8,72.00,386.36,114.51,114.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,310.98,293.23,229.02,234.63"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="3,319.11,293.23,212.76,141.04"><row><cell cols="5">Topic #adj Rel-to-Non Non-to-Rel Total</cell></row><row><cell>411</cell><cell>15</cell><cell>1</cell><cell>1</cell><cell>2</cell></row><row><cell>416</cell><cell>17</cell><cell>0</cell><cell>3</cell><cell>3</cell></row><row><cell>417</cell><cell>60</cell><cell>3</cell><cell>3</cell><cell>6</cell></row><row><cell>420</cell><cell>23</cell><cell>1</cell><cell>5</cell><cell>6</cell></row><row><cell>427</cell><cell>42</cell><cell>14</cell><cell>1</cell><cell>15</cell></row><row><cell>432</cell><cell>34</cell><cell>7</cell><cell>1</cell><cell>8</cell></row><row><cell>438</cell><cell>118</cell><cell>16</cell><cell>5</cell><cell>21</cell></row><row><cell>445</cell><cell>29</cell><cell>3</cell><cell>1</cell><cell>4</cell></row><row><cell>446</cell><cell>119</cell><cell>15</cell><cell>9</cell><cell>24</cell></row><row><cell>447</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Total</cell><cell>459</cell><cell>60</cell><cell>29</cell><cell>89</cell></row></table><note coords="3,350.10,447.40,189.90,8.74;3,310.98,459.35,229.02,8.74;3,310.98,471.31,229.02,8.74;3,310.98,483.26,229.02,8.74;3,310.98,495.22,229.02,8.74;3,310.98,507.17,229.02,8.74;3,310.98,519.13,48.32,8.74"><p>This table shows the number of adjudicated documents per topic and the number of NIST relevance judgments that were reversed as part of the adjudication. For example, topic 416 had 17 documents adjudicated, 0 relevant to non-relevant reversals, 3 non-relevant to relevant reversals, and in total 3 reversals.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,92.32,146.73,422.94,149.94"><head>Table 3 :</head><label>3</label><figDesc>Confusion Matrix. "Pos." and "Neg." stand for "Positive" and "Negative" respectively.</figDesc><table coords="4,183.53,203.84,244.94,92.82"><row><cell>Run</cell><cell>TPR FPR TNR FNR LAM</cell></row><row><cell>UIowaS02r</cell><cell>0.772 0.009 0.991 0.228 0.05</cell></row><row><cell>SSEC3excl</cell><cell>0.705 0.019 0.981 0.295 0.07</cell></row><row><cell>INFLB2012</cell><cell>0.036 0.002 0.998 0.964 0.13</cell></row><row><cell>NEUElo3</cell><cell>0.212 0.014 0.986 0.788 0.18</cell></row><row><cell>yorku12cs03</cell><cell>0.710 0.174 0.826 0.290 0.22</cell></row><row><cell cols="2">BUPTPRISZHS 0.211 0.020 0.980 0.789 0.23</cell></row><row><cell cols="2">OrcVBW16Conf 0.751 0.309 0.691 0.249 0.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,72.00,310.34,468.00,138.54"><head>Table 4 :</head><label>4</label><figDesc>Top runs from each participating group ordered by LAM (lower is better) with official smoothing of rates as per Equations 4 and 5. Also shown are the true positive rate (TPR), false positive rate (FPR), true negative rate (TNR), and false negative rate (FNR).</figDesc><table coords="4,170.92,391.92,270.15,56.96"><row><cell>Run</cell><cell cols="2">TPR FPR TNR FNR LAM AUC</cell></row><row><cell cols="2">SSEC3inclML 0.777 0.024 0.976 0.223 0.07</cell><cell>0.91</cell></row><row><cell>OrcVB1</cell><cell>0.652 0.294 0.706 0.348 0.31</cell><cell>0.81</cell></row><row><cell cols="2">NEUNugget12 0.299 0.026 0.974 0.701 0.21</cell><cell>0.75</cell></row><row><cell>yorku12cs03</cell><cell>0.710 0.174 0.826 0.290 0.22</cell><cell>0.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,72.00,462.56,468.00,186.36"><head>Table 5 :</head><label>5</label><figDesc>Top runs from each participating group ordered by AUC (higher is better). Only runs that submitted probabilities of relevance were evaluated using AUC. Also shown are the true positive rate (TPR), false positive rate (FPR), true negative rate (TNR), false negative rate (FNR), and the logistic average misclassification (LAM) rate.</figDesc><table coords="4,186.30,556.09,239.41,92.82"><row><cell>Run</cell><cell cols="5">TPR FPR FNR TNR LAM</cell></row><row><cell>UIowaS02r</cell><cell>0.78</cell><cell>0.01</cell><cell>0.22</cell><cell>0.99</cell><cell>0.05</cell></row><row><cell>SSEC3excl</cell><cell>0.71</cell><cell>0.02</cell><cell>0.29</cell><cell>0.98</cell><cell>0.07</cell></row><row><cell>yorku12cs03</cell><cell>0.72</cell><cell>0.17</cell><cell>0.28</cell><cell>0.83</cell><cell>0.22</cell></row><row><cell>NEUNugget12</cell><cell>0.29</cell><cell>0.03</cell><cell>0.71</cell><cell>0.97</cell><cell>0.22</cell></row><row><cell cols="2">BUPTPRISZHS 0.21</cell><cell>0.02</cell><cell>0.79</cell><cell>0.98</cell><cell>0.25</cell></row><row><cell cols="2">OrcVBW16Conf 0.76</cell><cell>0.31</cell><cell>0.24</cell><cell>0.69</cell><cell>0.25</cell></row><row><cell>INFLB2012</cell><cell>0.02</cell><cell>0.00</cell><cell>0.98</cell><cell>1.00</cell><cell>0.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,72.00,662.59,468.00,32.65"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note coords="4,111.44,662.59,428.56,8.74;4,72.00,674.55,468.00,8.74;4,72.00,686.50,189.81,8.74"><p>Top runs from each participating group ordered by LAM (lower is better) using rates smoothed as per Equations 6 and 7. Also shown are the true positive rate (TPR), false positive rate (FPR), true negative rate (TNR), and false negative rate (FNR).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,310.98,116.75,229.02,32.65"><head>Table 7 :</head><label>7</label><figDesc>Distribution of topics across the relevance saturation buckets for the 69 IRAT topics selected from the ImageCLEF 2009 BELGA test set</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,72.00,82.63,468.00,152.05"><head>Table 8 :</head><label>8</label><figDesc>Evaluation results for primary runs (LAM is calculated using Equations 4 and 5, while LAM2 uses Equations 6 and 7</figDesc><table coords="8,77.98,106.24,458.73,128.44"><row><cell>UTAustinM</cell><cell>TP</cell><cell>TN</cell><cell>FP</cell><cell>FN</cell><cell>P</cell><cell>R</cell><cell>Acc</cell><cell cols="2">Spec LAM LAM2 AUC</cell></row><row><cell>min</cell><cell>63.213</cell><cell>92.483</cell><cell cols="6">33.135 17.910 0.641 0.765 0.749 0.712</cell><cell>0.230</cell><cell>0.227</cell><cell>0.528</cell></row><row><cell>max</cell><cell>65.472</cell><cell>92.258</cell><cell cols="6">30.876 18.135 0.669 0.776 0.759 0.718</cell><cell>0.223</cell><cell>0.218</cell><cell>0.530</cell></row><row><cell>avg</cell><cell>64.343</cell><cell>92.371</cell><cell cols="6">32.006 18.022 0.655 0.771 0.754 0.715</cell><cell>0.227</cell><cell>0.223</cell><cell>0.529</cell></row><row><cell>A1</cell><cell>63.831</cell><cell>92.674</cell><cell cols="6">32.517 17.719 0.650 0.771 0.753 0.715</cell><cell>0.226</cell><cell>0.223</cell><cell>0.528</cell></row><row><cell>A2</cell><cell>64.854</cell><cell>92.067</cell><cell cols="6">31.494 18.326 0.660 0.771 0.755 0.715</cell><cell>0.227</cell><cell>0.222</cell><cell>0.529</cell></row><row><cell>SSPostECv2</cell><cell>TP</cell><cell>TN</cell><cell>FP</cell><cell>FN</cell><cell>P</cell><cell>R</cell><cell>Acc</cell><cell cols="2">Spec LAM LAM2 AUC</cell></row><row><cell>min</cell><cell cols="3">72.292 112.011 13.202</cell><cell>9.236</cell><cell cols="4">0.817 0.846 0.891 0.877</cell><cell>0.099</cell><cell>0.094</cell><cell>0.865</cell></row><row><cell>max</cell><cell cols="3">74.708 112.685 10.787</cell><cell>8.562</cell><cell cols="4">0.854 0.862 0.906 0.894</cell><cell>0.085</cell><cell>0.080</cell><cell>0.882</cell></row><row><cell>avg</cell><cell cols="3">73.500 112.348 11.994</cell><cell>8.899</cell><cell cols="4">0.836 0.854 0.899 0.885</cell><cell>0.092</cell><cell>0.087</cell><cell>0.873</cell></row><row><cell>A1</cell><cell cols="3">73.045 112.742 12.449</cell><cell>8.506</cell><cell cols="4">0.826 0.855 0.899 0.885</cell><cell>0.092</cell><cell>0.087</cell><cell>0.873</cell></row><row><cell>A2</cell><cell cols="3">73.955 111.955 11.539</cell><cell>9.292</cell><cell cols="4">0.845 0.854 0.899 0.886</cell><cell>0.091</cell><cell>0.086</cell><cell>0.873</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="4">Acknowledgments</head><p>Special thanks to <rs type="person">Gaurav Baruah</rs> for his on-going and extensive assistance with TRAT technical and nontechnical tasks. <rs type="person">Le Li</rs> was one of the three judges that performed the TRAT adjudication. Special thanks to <rs type="institution">Bart Thomee at Yahoo! Research</rs> who is one of the organizers of ImageCLEF 2012 and who has been instrumental in preparing the Flickr topics and image sets for use in the IRAT. We are grateful for the help of <rs type="person">Monica Lestari Paramita</rs>, <rs type="person">Paul D. Clough</rs>, <rs type="person">Mark Sanderson</rs> and <rs type="person">Wuytack Tom</rs> for making the BELGA corpus available to us. Thanks to <rs type="person">Ellen Voorhees</rs> and <rs type="person">Ian Soboroff</rs> for their help and support with running the track. Thanks to <rs type="person">Gordon Cormack</rs> for his feedback on the TRAT guidelines. We thank <rs type="institution">Amazon</rs>, <rs type="person">CrowdFlower, MobileWorks</rs> and <rs type="person">Crowd Computing Systems</rs> for sponsoring the track and providing credits or discounted prices to track participants.</p><p>This work was supported in part by a <rs type="funder">DARPA Young Faculty Award</rs>, in part by a <rs type="funder">John P. Commons Fellowship</rs>, in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>, and in part by the <rs type="funder">University of Waterloo</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,331.46,484.61,208.53,8.74;8,331.46,496.57,208.54,8.74;8,331.46,508.53,208.54,8.74;8,331.46,520.48,208.54,8.74;8,331.46,532.44,143.48,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,374.41,508.53,165.58,8.74;8,331.46,520.48,116.76,8.74">Northeastern university runs at the trec12 crowdsourcing track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Anderton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ekstrand-Abueg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">B</forename><surname>Golbus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,471.08,520.48,68.93,8.74;8,331.46,532.44,82.94,8.74">Online Proceedings of TREC 2012</title>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.46,553.72,208.54,8.74;8,331.46,565.67,208.54,8.74;8,331.46,577.63,52.58,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,468.45,553.72,71.55,8.74;8,331.46,565.67,62.18,8.74">Trec 2005 spam track overview</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,418.00,565.67,117.37,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.46,598.91,208.53,8.74;8,331.46,610.86,208.54,8.74;8,331.46,622.82,208.54,8.74;8,331.46,634.77,22.69,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,455.82,598.91,84.18,8.74;8,331.46,610.86,203.95,8.74">Using hybrid methods for relevance assessment in TREC crowd&apos;12</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,346.33,622.82,156.75,8.74">Online Proceedings of TREC 2012</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.46,656.05,208.54,8.74;8,331.46,668.01,205.14,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,360.81,668.01,171.63,8.74">Overview of the TREC 2009 legal track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.46,689.29,208.54,8.74;8,331.46,701.24,208.54,8.74;8,331.46,713.20,208.41,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,497.43,689.29,42.57,8.74;8,331.46,701.24,191.15,8.74">York university at TREC 2012: Crowdsourcing track</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xiangji</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,331.46,713.20,147.88,8.74">Online Proceedings of TREC 2012</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,75.16,208.54,8.74;9,92.48,87.11,208.54,8.74;9,92.48,99.07,208.54,8.74;9,92.48,111.02,208.54,8.74;9,92.48,122.98,110.89,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,275.81,75.16,25.21,8.74;9,92.48,87.11,208.54,8.74;9,92.48,99.07,208.54,8.74;9,92.48,111.02,75.58,8.74">Skierarchy: Extending the power of crowdsourcing using a hierarchy of domain experts, crowd and machine learning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peerreddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,201.39,111.02,99.63,8.74;9,92.48,122.98,50.36,8.74">Online Proceedings of TREC 2012</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,142.90,208.54,8.74;9,92.48,154.86,208.54,8.74;9,92.48,166.81,208.54,8.74;9,92.48,178.77,208.54,8.74;9,92.48,190.72,208.54,8.74;9,92.48,202.68,208.54,8.74;9,92.48,214.64,148.46,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,92.48,154.86,208.54,8.74;9,92.48,166.81,95.69,8.74">Diversity in photo retrieval: Overview of the imageclefphoto task 2009</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Paramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,161.94,202.68,139.08,8.74;9,92.48,214.64,17.29,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6242</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="59" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,234.56,208.54,8.74;9,92.48,246.52,208.54,8.74;9,92.48,258.47,208.54,8.74;9,92.48,270.43,110.89,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,222.35,234.56,78.67,8.74;9,92.48,246.52,208.54,8.74;9,92.48,258.47,77.97,8.74">Using a Bayesian model to combine LDA features with crowdsourced responses</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reece</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,202.20,258.47,98.82,8.74;9,92.48,270.43,50.36,8.74">Online Proceedings of TREC 2012</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,290.35,137.39,8.74;9,245.27,290.35,55.75,8.74;9,92.48,302.31,208.54,8.74;9,92.48,314.26,63.89,8.74;9,172.75,314.26,128.27,8.74;9,92.48,326.22,208.54,8.74;9,92.48,338.17,170.79,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,245.27,290.35,55.75,8.74;9,92.48,302.31,208.54,8.74;9,92.48,314.26,59.73,8.74">Overview of the imageclef 2012 flickr photo annotation and retrieval task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,237.24,326.22,26.64,8.74">CLEF</title>
		<title level="s" coord="9,272.64,326.22,28.37,8.74;9,92.48,338.17,134.86,8.74">Online Working Notes/Labs/Workshop</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,358.10,208.54,8.74;9,92.48,370.05,208.54,8.74;9,92.48,382.01,208.54,8.74;9,92.48,393.96,208.54,8.74;9,92.48,405.92,126.77,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,229.64,358.10,71.38,8.74;9,92.48,370.05,100.09,8.74">Labeling images with a computer game</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,220.39,370.05,80.63,8.74;9,92.48,382.01,208.54,8.74;9,92.48,393.96,108.26,8.74">Proceedings of the SIGCHI conference on Human factors in computing systems, CHI &apos;04</title>
		<meeting>the SIGCHI conference on Human factors in computing systems, CHI &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,425.84,208.54,8.74;9,92.48,437.80,208.54,8.74;9,92.48,449.75,208.54,8.74;9,92.48,461.71,126.16,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,233.34,425.84,67.68,8.74;9,92.48,437.80,188.62,8.74">Overview of the eigth Text REtrieval Conference (TREC-8)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,92.48,449.75,208.54,8.74;9,92.48,461.71,66.28,8.74">Proceedings of the Eighth Text Retrieval Conference (TREC 8)</title>
		<meeting>the Eighth Text Retrieval Conference (TREC 8)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,481.63,208.54,8.74;9,92.48,493.59,208.54,8.74;9,92.48,505.54,208.54,8.74;9,92.48,517.50,79.04,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,141.21,493.59,159.82,8.74;9,92.48,505.54,67.01,8.74">BUPT PRIS as TREC 2012 crowdsourcing track1</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,177.89,505.54,123.13,8.74;9,92.48,517.50,18.51,8.74">Online Proceedings of TREC 2012</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
