<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.25,114.94,295.50,15.12">Overview of the TREC 2012 Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,77.98,153.54,102.07,10.48;1,180.04,151.92,1.41,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.25,153.54,76.57,10.48;1,278.82,151.92,1.88,6.99"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.02,153.54,52.34,10.48;1,353.37,151.92,1.88,6.99"><forename type="first">Mark</forename><surname>Hall</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.67,153.54,63.57,10.48;1,435.24,151.92,1.88,6.99"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,457.44,153.54,83.30,10.48;1,540.74,151.92,2.59,6.99"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science &amp; Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.25,114.94,295.50,15.12">Overview of the TREC 2012 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">116EBB336F408B9C0B9CE828889AF48B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Session track ran for the third time in 2012. The track has the primary goal of providing test collections and evaluation measures for studying information retrieval over user sessions rather than one-time queries. These test collections are meant to be portable, reusable, statistically powerful, and open to anyone that wishes to work on the problem of retrieval over sessions.</p><p>The experimental design of the track was similar to that of the second year <ref type="bibr" coords="1,436.00,309.58,10.91,9.57" target="#b3">[4]</ref>:</p><p>• sessions were real user sessions with a search engine that include queries, retrieved results, clicks, and dwell times;</p><p>• retrieval tasks were designed to study the effect of using increasing amounts of user data on retrieval effectiveness for the mth query in a session.</p><p>There were a few changes compared to the second year of the track, mostly regarding the topic construction and the relevance assessments:</p><p>1. topics were constructed by the track co-ordinators; past TREC topics (mainly from QA2007 and MQ2009) tracks were used as in the second year; however, for each past topic 2-4 new topics were generated with certain characteristics regarding the type of the task;</p><p>2. relevance was defined at the level of the overall topic instead of subtopics.</p><p>This overview is organized as follows: in Section 2 we describe the tasks participants were to perform. In Section 3 we describe the corpus, topics, and sessions that comprise the test collection. Section 4 gives some information about submitted runs. In Section 5 we describe relevance judging and evaluation measures, and Sections 6 present evaluation results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation Tasks</head><p>We use the word "session" to mean a sequence of reformulations along with any user interaction with the retrieved results in service of satisfying an information need. The primary goal for participants of the 2012 track was to provide the best possible results for the mth query in a session given data from the session leading up to it.</p><p>NIST provided a set of 98 sessions of varying length (described in more detail in Section 3). Each session consists of:</p><p>• the current query q m ;</p><p>• the query session prior to the current query:</p><p>1. the set of past queries in the session, q 1 , q 2 , ..., q m-1 ;</p><p>2. the ranked list of URLs for each past query;</p><p>3. the set of clicked URLs/snippets and the time spent by the user reading the corresponding to each clicked url webpage.</p><p>Participants then ran their retrieval systems over only the current query under each of the following four conditions separately:</p><p>RL1 ignoring the session prior to this query RL2 considering only the item (1) above, i.e. the queries prior to the current RL3 considering only the items (1) and (2) above, i.e. the queries prior to the current along with the ranked lists of URLs and the corresponding web pages RL4 considering all the items (1), ( <ref type="formula" coords="2,260.15,428.91,4.65,9.57">2</ref>) and (3) above, i.e the queries prior to the current, the ranked lists of URLs and the corresponding web pages and the clicked URLs and the time spent on the corresponding web pages</p><p>Comparing the retrieval effectiveness in (RL1) with the retrieval effectiveness in (RL2)-(RL4) one can evaluate whether a retrieval system can use increasing amounts of information prior to a query to improve effectiveness for that query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test Collection</head><p>Our test collection consists of a corpus, a set of topics, and relevance judgments (described in the next section). But unlike most test collections, ours also includes a set of sessions of user interactions (including query reformulations). A single topic can have more than one session associated with it, since two different sessions could go about satisfying the same information need in very different ways and with different degrees of success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpus</head><p>The track used the ClueWeb09 collection. The full collection consists of roughly 1 billion web pages, comprising approximately 25TB of uncompressed data (5TB compressed) in multiple languages. The dataset was crawled from the Web during January and February 2009. Participants were encouraged to use the entire collection, however submissions over the smaller "Category B" collection of 50 million documents were accepted. Note that Category B submissions was evaluated as if they were Category A submissions. Four out of ten participating groups used the "Category B" collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topics</head><p>Topics were defined as a typical Ad Hoc track description. To define a set of topics, we started with TREC 2009 Million Query and Web track queries and TREC 2007 Question Answering track questions. Different from the 2011 track we attempted to control two facets of search tasks as defined in <ref type="bibr" coords="3,121.58,281.94,10.91,9.57" target="#b4">[5]</ref>: "Product" and "Goal quality". The "Product" facet varied between Intellectual and Factual tasks. Intellectual tasks produce new ideas or findings (e.g. learn about a topic or make decision based on information collected), while Factual tasks only involve locating facts, data and other information items. An example of such a varietion can be viewed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MQ topic: 20210</head><p>Query: dehumidifiers Based on the same MQ topic, two diffent topics were developed for the Session track. The former (topic 35) describes a factual fact, with the user being asked to locate a number of facts about dehumidifiers, while the latter (topic 37) describes an intellectual task, with the user asked to put an intellectual effort and produce a set of criteria over which different dehumidifiers could be compared.</p><p>The "Goal quality" facet varied between specific goal(s) and amorphous goal(s). This is very similar to the dimension that <ref type="bibr" coords="4,218.54,120.85,11.52,9.57" target="#b2">[3]</ref> proposed as well-defined and ill-defined information need. Tasks with specific goals have a well-defined information need, while in tasks with amorphous goals, the information need is ill-defined. Tasks with amorphous goals might require users to redefine the topic or identify specific aspects of the subject themselves. An example can be viewed bellow:</p><p>MQ topic: 20251</p><p>Query: Swahili dishes • Task: Exploratory search Description: A friend has been complaining for months that she is unhappy with her life. She has also mentioned that she can't easily sleep at nights. You think that she may be suffering from depression. You want to understand if this is the case and how you could assist her in getting some help from medical professionals.</p><p>Constructing topics of different task types allows to study both how user interactions different across varying task types and whether/how systems can improve search quality under different session characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sessions</head><p>A session is a series of actions, including queries and clicks on ranked results, that a user performs in the process of trying to satisfy the information need represented by the topic. Through the process described above we arrived at a large set of candidate topics for the track. These topics were then presented to actual users (minus the explicit subtopics, but with the narrative including the list of questions), who would see five randomly-selected topics and asked to choose one to try to satisfy. Users then were able to use a fully-functional custom search engine for ClueWeb09 in order to satisfy the information need described by the topic.</p><p>The custom-built search interface first provided instructions to users on the tasks to be conducted (Fig. <ref type="figure" coords="6,99.71,89.06,5.45,9.57">3</ref> in Section A). At the beginning of each session with the system, the user was shown three topics sampled randomly from the collection of topics. The user was then prompted to select one of the topics and use the search interface to satisfy the information need. The user was free to input any queries they liked, see titles and snippets for retrieved results (Fig. <ref type="figure" coords="6,443.33,129.71,4.24,9.57">4</ref>), click on URLs to see pages, and continue in this way until they determined they were finished with the topic.</p><p>The search interface used the Yahoo! BOSS (Build your Own Search Service) API to perform the actual queries, and then filtered the ranked results against the ClueWeb09 collection before they were shown to the user. This guarantees that the URLs returned are in the ClueWeb09 collection.</p><p>It cannot guarantee that the document content is the same; in fact, it is likely that many pages have changed since ClueWeb09 was crawled. This is a compromise that we made in order to have a search system that users were likely to find satisfactory. The system requested results from Yahoo! BOSS until at least 50 documents matching ClueWeb09 URLs had been found, or 50 result pages had been requested from Yahoo! BOSS (whichever came first).</p><p>During the course of interacting with the engine, it recorded the user's interactions with the retrieval system, including the queries issued, query reformulations, items clicked, and mouse movements in the results page. Users were also asked to save web pages they clicked if they find them useful for the completion of their task. (Fig. <ref type="figure" coords="6,243.47,315.24,4.24,9.57" target="#fig_5">5</ref>). All logged information was anonymous and no identifying information about the users was saved. When the users indicated that they had fulfilled the information need, they were presented with a brief exit survey (Fig. <ref type="figure" coords="6,397.25,342.34,4.85,9.57" target="#fig_6">6</ref>) that aimed to quantify how well the search system did.</p><p>Users were mostly faculty, staff, and students at the University of Sheffield. We sent a universitywide mailing asking for participation; anyone was free to use the system. The overall approach is similar to that described by Zuccon et al. <ref type="bibr" coords="6,273.70,401.23,11.50,9.57" target="#b5">[6]</ref> for "crowdsourcing" interactions.</p><p>When data collection was complete, we had acquired a set of candidate sessions to go with the candidate topics we defined above. Each session consists of a topic, a set of queries actual users posed to Yahoo! BOSS about the topic, the returned results and the user interactions with the returned results.</p><p>We then performed some automatic and manual culling of sessions to try to achieve a set that would be interesting for the track. This involved eliminating sessions in which the user clearly didn't understand the task or the information need, eliminating sessions in which the need was satisfied after only one query, and preferring sessions with more interactions. When the culling was complete, we had a set of 98 sessions for 48 topics to release to participants.</p><p>The sessions were provided in an XML file format. An example session containing all RL4 data might look like this: Each experimental condition drops more data from the XML format. An RL3 session would include everything except the &lt;clicked&gt; blocks. An RL2 session eliminated the &lt;results&gt; blocks along with the &lt;clicked&gt; blocks. An RL1 session had virtually no information, eliminating entire &lt;interaction&gt; blocks.</p><p>There is a median of one reformulation prior to the last query (mean = 2.03). 19% of sessions have three or more reformulations prior to the last query. The maximum number of queries in any session in the set is 10. There are a total of 272 recorded clicks across all sessions (2.8 per session on average). However, there are 26 sessions with no recorded clicks, and therefore an average of 3.8 clicks per session that has at least one click.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submissions</head><p>Sites were permitted to submit up to three runs. Each submitted run includes four separate ranked result lists for all 98 sessions. Files were named "runTag.RLn", where "runTag" is a unique identifier for the site and the particular submission, and "RLn" is RL1, RL2, RL3, or RL4 depending on the experimental condition.</p><p>The track received 27 runs from the 10 groups listed in Table <ref type="table" coords="8,369.30,243.17,4.24,9.57" target="#tab_3">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Bauhaus-Universitt Weimar, Germany 2.</p><p>Beijing University of Posts and Telecommunications, China 3.</p><p>Centrum Wiskunde &amp; Informatica (CWI), Netherlands 4.</p><p>Georgetown University, USA 5.</p><p>Institute of Computing Technology, Chinese Academy of Sciences, China 6.</p><p>Rutgers University, USA 7.</p><p>University of Albany, USA 8.</p><p>University of Delaware, USA 9.</p><p>University of Essex, UK 10. University of Pittsburgh, USA Details about the methods used by each of the participating sites can be found in the individual group's reports for the Session Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Session Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relevance Judgments</head><p>Judging was done by assessors at NIST. As described above, each topic was the subject of one or more sessions. For each one of the 49 topics, a pool was formed from the ranked results for the past queries q 1 ...q m-1 and the current query q m produced by Yahoo! BOSS along with the top 10 ranked documents from the submitted runs on the current query q m for all corresponding sessions. <ref type="foot" coords="8,535.27,625.78,4.23,6.99" target="#foot_0">1</ref>The NIST assessors then judged each document in the pool with respect to topic description. Note that this is different from the 2011 track with document then being judged with respect to the different subtopics along with the general topic description.</p><p>The qrels produced have the following format: &lt;topic-id&gt; 0 &lt;doc-id&gt; &lt;judgment&gt;</p><p>The second column used to represent the different subtopics in the 2011 track and is kept for consistency reasons. Again different from the 2011 track judgment are: -2 for spam document (i.e. the page does not appear to be useful for any reasonable purpose; it may be spam or junk.); 0 for not relevant (i.e. the content of this page does not provide useful information on the topic, but may provide useful information on other topics, including other interpretations of the same query); 1 for relevant (i.e. the content of this page provides some information on the topic, which may be minimal; the relevant information must be on that page, not just promising-looking anchor text pointing to a possibly useful page); 4 for highly relevant (i.e. the content of this page provides substantial information on the topic); 2 for key, (i.e. the page or site is dedicated to the topic; authoritative and comprehensive, worthy of being a top result in a web search engine; typically, key pages are more comprehensive, have higher quality, and are from more trustworthy sources than the merely highly relevant page); and 3 for navigational (i.e. this page represents a home page of an entity directly named by the query; the user may be searching for this specific page or site; there is often at most one page that deserves a Navigational judgment for an aspect).</p><p>The assessors were also provided with the following additional instruction: "The judgments are on the same scale of Key down to Not Relevant, and the same basic definitions of those judgment levels holds. Since an individual topic may be looking for a variety of different, though related, things, a document that covers many of those points is likely to be more relevant than a document that covers only one. But, as always, it is your opinion as the assessor that determines whether and to what extent any particular document is relevant." Topics were assigned to assessors such that the topics that were highly related to one another (e.g., four topics on cooking Swahili foods) were all judged by the same assessor. For those topics taken from 2012 web track topics, the same assessor did both the session and web track assessing.</p><p>Relevance judgments were eventually transformed to relevance grades with spam and non-relevant documents assigned a grade of 0, relevant assigned a grade of 1, highly relevant assigned a grade of 2, key assigned a grade of 3, and navigational assigned a grade of 4.</p><p>A total of 17,861 pages were judged. Out of these 17,861 pages, 5 were judged as navigational, 458 as key, 1,360 as highly relevant, 2,679 as relevant, 12,384 as non-relevant and 975 as spam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Measures</head><p>Based on the qrels provided by NIST and the decisions described above, we evaluated the submitted runs by eight measures:</p><p>• Expected Reciprocal Rank (ERR) <ref type="bibr" coords="9,265.67,659.86,11.52,9.57" target="#b1">[2]</ref> • ERR@10 • ERR normalized by the maximum ERR per query (nERR)</p><formula xml:id="formula_0" coords="10,88.36,98.02,61.14,54.60">• nERR@10 • nDCG • nDCG@10</formula><p>• Average Precision (AP)</p><p>• Precision@10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Results</head><p>Table <ref type="table" coords="10,101.67,256.20,5.45,9.57">2</ref> shows all results (by nDCG@10) for all submitted runs in all four experimental conditions. If RL1 (no information about the session) is the baseline, about half of the submitted runs were able to improve on that using only the information about prior queries (RL2) or using information about prior queries and retrieved results (RL3). A majority of submitted runs improved on the baseline using the interaction information (RL4). Though we cannot say for sure that RL1 is a baseline for every submission, it seems prima facie reasonable to conclude that the interaction information provided by RL4 can be used to improve automatic retrieval results.</p><p>Figure <ref type="figure" coords="10,106.72,355.75,5.45,9.57" target="#fig_1">1</ref> shows changes in nDCG@10 from the RL1 baseline (left) or with increasing information (right). The three plots going down the left column show changes in nDCG@10 from using no previous data (RL1) to using greater and greater amounts of previous data. The dashed line is a difference in nDCG of zero; points above that line represent systems that saw an improvement from using the additional data while points below it represent systems that were hurt with the additional data. The 95% confidence intervals give a rough idea of whether the results are significant.</p><p>On the right-hand side, Figure <ref type="figure" coords="10,221.48,441.74,5.45,9.57" target="#fig_1">1</ref> shows changes in nDCG@10 with increasing amounts of previous data: going from RL1 to RL2, RL2 to RL3, and RL3 to RL4. A few systems see improvement at every step. This suggests that the extra data really is beneficial for effectiveness. <ref type="foot" coords="10,458.06,466.88,4.23,6.99" target="#foot_2">2</ref>Tables <ref type="table" coords="10,111.78,487.08,5.45,9.57" target="#tab_5">3</ref> and<ref type="table" coords="10,148.48,487.08,5.45,9.57">4</ref> show changes in nDCG@10 when novelty in the ranking of documents for the current query is considered. In the former table documents that have been shown and clicked by the user in queries prior the current query are considered duplicates if they are also presented in the ranking for the current query and their relevance is downgraded to zero. In the latter all documents shown to the user in the queries prior to the current one are considered duplicates.</p><p>A possibility is that some systems were optimized for different aspects of effectiveness. Figure <ref type="figure" coords="10,534.54,559.53,5.45,9.57" target="#fig_3">2</ref> shows changes in mean average precision over the RL conditions. Comparing to Figure <ref type="figure" coords="10,498.24,573.07,5.45,9.57" target="#fig_1">1</ref> reveals some striking differences: the UvA runs tended to see large and significant improvements in MAP despite not seeing such improvements in nDCG@10, while Rutgers systems that had large and significant improvements in nDCG@10 did not see such improvements in MAP. This lends support to that hypothesis.  <ref type="table" coords="11,101.79,551.88,4.24,9.57">2</ref>: All results by nDCG@10 for the current query in the session for each condition (sorted in decreasing order of RL1 nDCG@10). Boldface indicates the highest nDCG@10 in the condition. ↑, ↓ indicate positive or negative differences from RL1. ⇑, ⇓ indicate statistically significant (p &lt; 0.05 by a paired two-sided t-test) positive or negative differences from RL1. ↔ indicates no difference from RL1. The baseline system is our custom search system described above.    <ref type="table" coords="14,102.55,565.43,4.24,9.57">4</ref>: All results by nDCG@10 for the current query in the session for each condition (sorted in decreasing order of RL1 nDCG@10). Shown documents in previous queries of the session are considered duplicates and their relevance is downgraded to zero.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Screenshots of the Search Interface</head><p>ClueWeb Search Instructions A set of tasks will appear on the next screen. Please read the tasks carefully and select the task that you are most familiar with and understand best.</p><p>Please do not select any tasks that you have already completed. If you have completed all the tasks that are listed, then please refresh the page for a new list of tasks.</p><p>After clicking on the "Select this task" button for the task that you wish to select, a search box will appear. Type in the keywords that you believe are the best to find web pages that will help you fulfil the selected task. Please refer to the task description on the left-hand side any time you need to review the task.</p><p>Use the search system naturally, as you would do in your everyday searching activities. Browse the ranked list of web-pages, click on any web-page that you think may be useful, click on the next page button if you want to see more results, or reformulate your search query and search again if you think that the returned results are not satisfying.</p><p>Any web-page you click on will be opened in a new browser window / tab. Please only view the content on that web-page and do not click on any links on that web-page. After you have viewed the web-page, please close the new window / tab and return to the ClueWeb search results. You must then select whether you would save that web-page in order to solve your task or not.</p><p>You can finish the task by clicking on the "Finish" button in the left-hand side-panel. You may finish at any time if you are satisfied with the web pages you have observed so far.</p><p>After you finish a task, you will be shown a short questionnaire. Please fill this out and click on the "Save" button. You will then be taken back to the task selection page and can select another task to do. Please do as many tasks as you can in the time that you have.</p><p>Note that your browsing activities will be recorded. The recored data is anonymised and then stored securely. It will be used for research purposes only.</p><p>Note that the search engine is still experimental and thus slower than most current search engines and will also return fewer results.</p><p>There is a help icon (?) in the top-right corner of every page that will show these notes at any time.</p><p>If you have any further questions regarding the tasks, please contact ekanoulas at gmail dot com. If you find a bug or the system just doesn't work for you, please contact m dot mhall at sheffield dot ac dot uk.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,72.00,636.46,468.00,9.57;12,72.00,650.01,468.00,9.57;12,72.00,663.56,119.48,9.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Left: Changes in nDCG@10 from RL1 to (from top to bottom) RL2, RL3, and RL4. Right: Changes in nDCG@10 from RL1 to RL2, RL2 to RL3, and RL3 to RL4. Error bars are 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="15,72.00,648.00,468.00,9.57;15,72.00,661.54,468.00,9.57;15,72.00,675.09,43.13,9.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Changes in MAP from RL1 to (from top to bottom) RL2, RL3, and RL4. Right: Changes in MAP from RL1 to RL2, RL2 to RL3, and RL3 to RL4. Error bars are 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="17,152.74,493.72,306.52,9.57"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: The instruction text used for the data-generation task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="18,234.53,649.03,142.94,9.57;18,118.80,497.81,374.40,135.16"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The page rating UI.</figDesc><graphic coords="18,118.80,497.81,374.40,135.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="19,230.44,508.47,151.12,9.57;19,118.80,244.07,374.43,248.34"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The exit interview UI</figDesc><graphic coords="19,118.80,244.07,374.43,248.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,594.33,475.96,91.98"><head></head><label></label><figDesc>India was crowned Miss Universe in 2000, and between 1994 and 2000 women from India won two Miss Universe competitions, four Miss World competitions, and many less well-known competitions. To what extent can decisions and policies of the Indian government be credited with these wins?</figDesc><table coords="6,72.00,594.33,475.96,91.98"><row><cell>&lt;session num="16" starttime="15:14:23.276482"&gt; &lt;topic&gt; &lt;desc&gt;Lara Dutta of India was crowned Miss Universe in 2000, and between 1994 and 2000 women from India won two Miss Universe competitions, four Miss World competitions, and many less well-known competitions. To what extent can decisions and policies of the Indian government be credited with these wins? &lt;/desc&gt; &lt;interaction num="3" starttime="15:16:33.448408"&gt; &lt;query&gt;politics 1994-2000 Indian Miss Universe&lt;/query&gt; &lt;results&gt; &lt;result rank="1"&gt; &lt;url&gt;http://en.wikipedia.org/wiki/Miss_Universe_1994&lt;/url&gt; &lt;clueweb09id&gt;clueweb09-enwp01-55-00034&lt;/clueweb09id&gt; &lt;title&gt;Miss Universe 1994 -Wikipedia, the free encyclopedia&lt;/title&gt; &lt;snippet&gt;Miss Universe 1994, the 43rd Miss Universe pageant ... Later that year, another Indian, Aishwarya Rai ... Malaysian Foreign Minister not to make political remarks. Miss ... &lt;/snippet&gt; &lt;/result&gt; &lt;result rank="2"&gt; &lt;url&gt;http://en.wikipedia.org/wiki/Miss_Universe_2000&lt;/url&gt; &lt;clueweb09id&gt;clueweb09-enwp01-62-00316&lt;/clueweb09id&gt; &lt;title&gt;Miss Universe 2000 -Wikipedia, the free encyclopedia&lt;/title&gt; &lt;snippet&gt;Miss Universe 2000, the 49th Miss Universe pageant was held at Eleftheria Stadium, ... the field they wanted to enter, be it entrepreneurship, the armed forces, politics ...&lt;/snippet&gt; &lt;/result&gt; ... &lt;result rank="10"&gt; &lt;url&gt;http://www.indianautographs.com/productdetail-216125.html&lt;/url&gt; &lt;clueweb09id&gt;clueweb09-en0023-23-23376&lt;/clueweb09id&gt; &lt;title&gt;Welcome to Thematic Gallery of Indian Autographs -Detailed ...&lt;/title&gt; &lt;snippet&gt;Thematic Gallery of Indian Autographs -A ... Stylists etc. TV Stars: TV Personalities: Miss India / World / Universe ... World Celebrities -Political: World Celebrities ...&lt;/snippet&gt; &lt;/result&gt; &lt;/results&gt; &lt;clicked&gt; &lt;click num="1" starttime="15:16:43.141470" endtime="15:16:56.658945"&gt; &lt;rank&gt;2&lt;/rank&gt; &lt;/click&gt; &lt;/clicked&gt; &lt;/interaction&gt; &lt;currentquery starttime="15:16:33.448408"&gt; &lt;query&gt;politics 1994-2000 Indian Miss Universe&lt;/query&gt; &lt;/currentquery&gt; &lt;narr&gt;Lara Dutta of &lt;/topic&gt; &lt;/session&gt;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,168.22,424.02,275.56,9.57"><head>Table 1 :</head><label>1</label><figDesc>Groups participating in the 2012 Sessions Track.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,72.00,162.05,468.00,440.05"><head>Table 3 :</head><label>3</label><figDesc>All results by nDCG@10 for the current query in the session for each condition (sorted in decreasing order of RL1 nDCG@10). Clicked documents in previous queries of the session are considered duplicates and their relevance is downgraded to zero.</figDesc><table coords="13,143.90,162.05,324.20,389.34"><row><cell>run</cell><cell>RL1</cell><cell>RL2</cell><cell>RL3</cell><cell>RL4</cell></row><row><cell cols="2">PITTSHQMsnov 0.2540</cell><cell>0.2956 ⇑</cell><cell>0.3009 ⇑</cell><cell>0.3009 ⇑</cell></row><row><cell>PITTSHQMsdm</cell><cell>0.2522</cell><cell>0.2925 ⇑</cell><cell>0.2961 ⇑</cell><cell>0.2953 ⇑</cell></row><row><cell>PITTSHQMnov</cell><cell cols="4">0.2517 0.3009 ⇑ 0.3152 ⇑ 0.3070 ⇑</cell></row><row><cell>PITTSHQM</cell><cell>0.2485</cell><cell>0.2955 ⇑</cell><cell>0.3072 ⇑</cell><cell>0.2978 ⇑</cell></row><row><cell>ICTNET12SER3</cell><cell>0.2401</cell><cell>0.2387 ↓</cell><cell>0.2548 ↑</cell><cell>0.2686 ↑</cell></row><row><cell>CWIrun1</cell><cell>0.2330</cell><cell>0.2426 ↑</cell><cell>0.2257 ↓</cell><cell>0.2257 ↓</cell></row><row><cell>CWIrun3</cell><cell>0.2330</cell><cell>0.2426 ↑</cell><cell>0.2227 ↓</cell><cell>0.2233 ↓</cell></row><row><cell>gurelaxphr</cell><cell>0.2270</cell><cell>0.2694 ⇑</cell><cell>0.2891 ⇑</cell><cell>0.2768 ⇑</cell></row><row><cell>guphrase1</cell><cell>0.2249</cell><cell>0.2797 ⇑</cell><cell>0.2878 ⇑</cell><cell>0.2878 ⇑</cell></row><row><cell>guphrase2</cell><cell>0.2215</cell><cell>0.2701 ⇑</cell><cell>0.2855 ⇑</cell><cell>0.2855 ⇑</cell></row><row><cell>baseline</cell><cell>0.2210</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>wildcat1</cell><cell>0.2086</cell><cell>0.1952 ↓</cell><cell>0.2551 ⇑</cell><cell>0.2253 ↑</cell></row><row><cell>ICTNET12SER2</cell><cell>0.2064</cell><cell>0.2087 ↑</cell><cell>0.2639 ⇑</cell><cell>0.2646 ⇑</cell></row><row><cell>wildcat3</cell><cell>0.1995</cell><cell>0.1842 ↓</cell><cell>0.2707 ⇑</cell><cell>0.2429 ⇑</cell></row><row><cell>webis12indqe</cell><cell>0.1945</cell><cell>0.1989 ↑</cell><cell>0.1994 ↑</cell><cell>0.1969 ↑</cell></row><row><cell>essexSAnchor</cell><cell>0.1836</cell><cell>0.2046 ↑</cell><cell>0.2122 ⇑</cell><cell>0.2082 ⇑</cell></row><row><cell>essexSWiki</cell><cell>0.1836</cell><cell>0.1769 ↓</cell><cell>0.1769 ↓</cell><cell>0.1769 ↓</cell></row><row><cell>ICTNET12SER1</cell><cell>0.1556</cell><cell>0.1967 ⇑</cell><cell>0.1973 ⇑</cell><cell>0.2084 ⇑</cell></row><row><cell>WQExpFqDSnip</cell><cell>0.1446</cell><cell>0.1633 ↑</cell><cell>0.1703 ↑</cell><cell>0.1683 ↑</cell></row><row><cell>BDocExpDoc</cell><cell>0.1383</cell><cell>0.1548 ⇑</cell><cell>0.1853 ⇑</cell><cell>0.1818 ⇑</cell></row><row><cell>UAlbany</cell><cell>0.1374</cell><cell>0.1242 ↓</cell><cell>0.1648 ↑</cell><cell>0.1246 ↓</cell></row><row><cell>RutgersHu</cell><cell>0.1240</cell><cell>0.0000 ⇓</cell><cell>0.0000 ⇓</cell><cell>0.1507 ↑</cell></row><row><cell>RutgersM</cell><cell>0.1240</cell><cell>0.0000 ⇓</cell><cell>0.0000 ⇓</cell><cell>0.1517 ↑</cell></row><row><cell>ACombSnip</cell><cell>0.1158</cell><cell>0.1161 ↑</cell><cell>0.1701 ⇑</cell><cell>0.1669 ⇑</cell></row><row><cell>webis12cnse</cell><cell>0.1038</cell><cell>0.1176 ↑</cell><cell>0.1336 ⇑</cell><cell>0.1744 ⇑</cell></row><row><cell>webis12cnqe</cell><cell>0.0854</cell><cell>0.1109 ⇑</cell><cell>0.1141 ⇑</cell><cell>0.1106 ⇑</cell></row><row><cell>wildcat2</cell><cell>0.0783</cell><cell>0.1232 ⇑</cell><cell>0.1968 ⇑</cell><cell>0.2479 ⇑</cell></row><row><cell>TUDrun</cell><cell>0.0492</cell><cell>0.0265 ⇓</cell><cell>0.0265 ⇓</cell><cell>0.0265 ⇓</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="8,88.59,648.81,36.57,7.86"><p>For topic</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_1" coords="8,140.68,648.81,399.32,7.86;8,72.00,659.77,369.95,7.86"><p>only, because of assessing resource constraints, document were pooled at depth 5 from submissions but still at depth 10 from the Yahoo! BOSS system for the last query and previous queries.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="10,88.59,648.35,451.41,7.86;10,72.00,659.31,147.60,7.86"><p>We caution against over-interpreting this, though, as we cannot say for certain whether these conditions are comparable for every submitted run.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="16,88.97,104.34,451.03,9.57;16,88.97,117.89,451.03,9.87;16,88.97,132.23,134.76,9.09" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<ptr target="http://ir.cis.udel.edu/ECIR11Sessions" />
		<title level="m" coord="16,423.98,104.34,116.02,9.57;16,88.97,117.89,300.19,9.57">Proceedings of the ECIR 2011 Workshop on Information Retrieval Over Query Sessions</title>
		<meeting>the ECIR 2011 Workshop on Information Retrieval Over Query Sessions</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,88.97,153.96,451.04,9.57;16,88.97,167.51,451.03,9.57;16,88.97,181.06,113.83,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,362.65,153.96,177.35,9.57;16,88.97,167.51,41.75,9.57">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,166.65,167.51,373.35,9.57;16,88.97,181.06,79.33,9.57">Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,88.97,203.57,451.03,9.57;16,88.97,217.12,451.03,9.57;16,88.97,230.67,53.94,9.57" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="16,237.54,203.57,302.46,9.57;16,88.97,217.12,210.22,9.57">The Turn: Integration of Information Seeking and Retrieval in Context (The Information Retrieval Series)</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,88.97,253.19,451.03,9.57;16,88.97,266.73,451.03,9.57;16,88.97,280.28,451.03,9.57;16,88.97,294.62,104.64,9.09" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,406.99,253.19,128.10,9.57">Session track 2011 overview</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec20/papers/SESSION.OVERVIEW.2011.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,103.04,266.73,324.14,9.57">The Twentieth Text REtrieval Conference Proceedings (TREC 2011</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="16,88.97,316.35,451.03,9.57;16,88.97,329.90,244.40,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,210.33,316.35,325.00,9.57">A faceted approach to conceptualizing tasks in information seeking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,88.97,329.90,101.09,9.57">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1822" to="1837" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,88.97,352.41,451.03,9.57;16,88.97,365.96,411.80,9.57" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="16,469.27,352.41,70.73,9.57;16,88.97,365.96,293.07,9.57">Crowdsourcing interactions: Capturing query sessions through crowdsourcing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leelanupab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In Carterette et al. [1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
