<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.01,81.09,295.99,12.91;1,243.63,97.03,124.74,12.91">A Multi-Strategy and Multi-Source Approach to Question Answering</title>
				<funder ref="#_q8bPrpU">
					<orgName type="full">Advanced Research and Development Activity (ARDA)&apos;s Advanced Question Answering for Intelligence (AQUAINT) Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.47,127.93,108.85,10.76"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center</orgName>
								<address>
									<postBox>P.O. Box 704</postBox>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,281.21,127.93,63.24,10.76"><forename type="first">John</forename><surname>Prager</surname></persName>
							<email>jprager@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center</orgName>
								<address>
									<postBox>P.O. Box 704</postBox>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.34,127.93,95.19,10.76"><forename type="first">Christopher</forename><surname>Welty</surname></persName>
							<email>welty@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center</orgName>
								<address>
									<postBox>P.O. Box 704</postBox>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.68,141.88,85.99,10.76"><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
							<email>kczuba@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center</orgName>
								<address>
									<postBox>P.O. Box 704</postBox>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.56,141.88,76.76,10.76"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
							<email>ferrucci@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center</orgName>
								<address>
									<postBox>P.O. Box 704</postBox>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.58,155.82,81.17,10.76"><forename type="first">Ibm</forename><forename type="middle">T J</forename><surname>Watson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center</orgName>
								<address>
									<postBox>P.O. Box 704</postBox>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.01,81.09,295.99,12.91;1,243.63,97.03,124.74,12.91">A Multi-Strategy and Multi-Source Approach to Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D352926A22EF66AB4428848DE58C8EA2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditional question answering systems typically employ a single pipeline architecture, consisting roughly of three components: question analysis, search, and answer selection (see e.g., <ref type="bibr" coords="1,139.96,298.30,84.18,8.97">(Clarke et al., 2001a;</ref><ref type="bibr" coords="1,226.84,298.30,71.96,8.97" target="#b3">Hovy et al., 2000;</ref><ref type="bibr" coords="1,72.00,310.25,90.98,8.97" target="#b8">Moldovan et al., 2000;</ref><ref type="bibr" coords="1,165.88,310.25,74.48,8.97" target="#b10">Prager et al., 2000)</ref>). The knowledge sources utilized by these systems to date primarily focus on the corpus from which answers are to be retrieved, WordNet, and the Web (see e.g., <ref type="bibr" coords="1,242.79,346.12,56.01,8.97;1,72.00,358.08,27.68,8.97">(Clarke et al., 2001b;</ref><ref type="bibr" coords="1,102.36,358.08,111.48,8.97" target="#b9">Pasca and Harabagiu, 2001;</ref><ref type="bibr" coords="1,216.50,358.08,73.99,8.97" target="#b11">Prager et al., 2001)</ref>). More recent research has shown that introducing feedback loops into the traditional pipeline architecture results in a performance gain <ref type="bibr" coords="1,182.35,393.94,93.09,8.97" target="#b2">(Harabagiu et al., 2001)</ref>.</p><p>We are interested in improving the performance of QA systems by breaking away from the strict pipeline architecture. In addition, we require an architecture that allows for hybridization at low development cost and facilitates experimentation with different instantiations of system components. Our resulting architecture is one that is modular and easily extensible, and allows for multiple answering agents to address the same question in parallel and for their results to be combined.</p><p>Our new question answering system, PIQUANT, adopts this flexible architecture. The answering agents currently implemented in PIQUANT vary both in terms of the strategies used and the knowledge sources consulted. For example, an answering agent may employ statistical methods for extracting answers to questions from a large corpus, while another answering agent may transform select natural language questions into logical forms and query structured knowledge sources for answers.</p><p>In this paper, we first describe the architecture on which PIQUANT is based. We then describe the answering agents currently implemented within the PIQUANT system, and how they were configured for our TREC2002 runs. Finally, we show that significant performance improvement was achieved by our multi-agent architecture by comparing our TREC2002 results against individual answering agent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Modular and Extensible QA Architecture</head><p>The architecture adopted by our PIQUANT system, shown in Figure <ref type="figure" coords="1,382.17,281.75,3.74,8.97">1</ref>, defines several basic roles that components of a QA system can play. The definition of each role includes a consistent interface that allows components implementing that role to be easily plugged into the system. This architectural approach is not simply to facilitate good software engineering in a group, but it allows hybridization at a fairly low development cost, and it also facilitates experimentation based on the choices available within the different component roles.</p><p>The main components of our architecture are briefly described as follows:</p><p>1. Question Analysis components analyze questions to produce information consumed by other components in the form of a QFrame. Information contained in the QFrame should minimally include a question type that would help guide the selection of one or more answering agents (see below) appropriate for addressing the question. A QA system typically has one question analysis component, but may possibly have as many as one per answering agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Answering</head><p>Agent components implement answer finding strategies given the results of question analysis and a knowledge source. These may be as simple as composing a bag-of-words query for document/passage retrieval, or as complex as breaking the question into sub-questions and consulting multiple knowledge sources. We expect QA systems to have multiple answering agents that pursue different strategies in parallel, which we believe to be an important feature of our architecture: not only can we experiment with different question answering strategies and knowledge sources, but with combining them as well.</p><p>3. Answer Resolution components combine the results of multiple answering agents into a single rank- ing. These components may simply perform ranking over the combined set of answers of all answering agents, or may do something more complex such as feeding answers from one agent back into others. Ultimately, the final answers of a QA system are provided by this component, so there is only one such component in any QA system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Knowledge Source Adapter components insulate</head><p>the other components of the QA system that consult knowledge sources from the multitude of data formats, access mechanisms, representation languages, reasoning services, and ontologies that consumers of existing structured knowledge sources must be acutely aware of.</p><p>An obvious benefit of our component-based approach is that we can easily experiment with and compare different techniques for filling these roles by keeping the rest of the components of the QA system fixed and changing only the components that implement the techniques we wish to compare. Thus we could, for example, measure the overall impact on QA performance of using statistical vs. rule-based annotators, or using machine learning vs. rule-based answering agents. In addition, as noted above, we can often combine the strengths of different techniques to improve overall performance, which will be the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Multi-Agent Approach to Question Answering</head><p>Answering agents that can be adopted for QA may differ along various dimensions. One such dimension is the type of knowledge source from which answers are extracted, which may include unstructured text resources or structured knowledge sources such as Cyc <ref type="bibr" coords="2,511.50,354.91,28.49,8.97;2,313.20,366.87,23.24,8.97" target="#b6">(Lenat, 1995)</ref> or WordNet <ref type="bibr" coords="2,392.28,366.87,55.83,8.97" target="#b7">(Miller, 1995)</ref>. Even when two answering agents consult the same knowledge source, they may adopt different processing strategies. For example, existing question answering systems vary greatly, from utilizing primarily knowledge-driven components, e.g., <ref type="bibr" coords="2,313.20,426.64,96.38,8.97" target="#b2">(Harabagiu et al., 2001;</ref><ref type="bibr" coords="2,413.02,426.64,78.12,8.97" target="#b10">Prager et al., 2000)</ref> to adopting mainly statistical methods, e.g., <ref type="bibr" coords="2,442.99,438.60,97.01,8.97" target="#b5">(Ittycheriah et al., 2001;</ref><ref type="bibr" coords="2,313.20,450.55,122.27,8.97" target="#b13">Ravichandran and Hovy, 2002)</ref>.</p><p>We have so far integrated into our PIQUANT system answering agents that utilize both structured and unstructured knowledge sources. For the latter class, we have incorporated two answering agents adopting fundamentally different processing strategies. This section describes each of these answering agents, as well as how their answers are combined to formulate the system's final answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Agents Based on Unstructured Information</head><p>Perhaps motivated by the TREC QA track, the vast majority of existing question answering systems adopt a large text corpus as their information source. Additionally, while many such systems adopt a classic pipeline architecture, each typically employs a different approach in instantiating its components. Currently, we have incorporated two text-based answering agents into PIQUANT, one utilizing a primarily knowledge-driven approach and the other adopting statistical methods. These two answering agents have performed quite comparably in past TREC QA tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Knowledge-Based Answering Agent</head><p>Our first answering agent utilizes a primarily knowledge-driven approach to question answering, based on Predictive Annotation <ref type="bibr" coords="3,176.84,114.00,81.64,8.97" target="#b10">(Prager et al., 2000;</ref><ref type="bibr" coords="3,262.25,114.00,36.54,8.97;3,72.00,125.95,38.27,8.97" target="#b12">Prager et al., 2003)</ref>. A key characteristic of this system is that potential answers, such as person names, locations, and dates, in the text corpus are predictively annotated. In other words, the text corpus is indexed not only with keywords, as is typical for most search engines, but also with the semantic classes of these pre-identified potential answers.</p><p>During the question analysis phase, a rule-based mechanism is used to determine one or more of about 80 semantic types of the candidate answer, along with a set of keywords. A weighted search engine query is then constructed from the keywords and the candidate semantic classes. The search engine then returns a small (typically 10-passage) set of 1-to-3-sentence passages based on the query. The candidate answers in these passages are identified and ranked based on three criteria: 1) match in semantic type between candidate answer and expected answer, 2) match in weighted grammatical relationships between question and answer passages, and 3) answer frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Statistical Answering Agent</head><p>The second answering agent used in PIQUANT is the statistical question answering system of Ittycheriah et al. <ref type="bibr" coords="3,72.00,412.25,94.44,8.97" target="#b5">(Ittycheriah et al., 2001)</ref>. This statistical answering agent is also based on the pipeline architecture; however, instead of adopting rule-based mechanisms, it utilizes a maximum entropy approach for training system components.</p><p>In question analysis, one of a set of 32 potential answer types is selected based on features such as words, POS tags, bigrams, and question word markers. The search module adopts a two-pass approach in which high scoring passages from an encyclopedia are used to augment the query terms, which are then used for search against the TREC corpus. The search engine returns a large set of passages (100) for further consideration. Named entities and their semantic types are identified from these passages, again using a maximum entropy based mechanism, and a confidence value computed for each named entity based on its likelihood of being a correct answer to the given question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Agents Based on Structured Knowledge Sources</head><p>It has been previously established that finding the answers to questions in structured knowledge sources such as WordNet and including the answer in a bag of words can improve accuracy <ref type="bibr" coords="3,162.33,689.29,78.56,8.97" target="#b11">(Prager et al., 2001)</ref>. We have expanded on this notion in two ways, first by adapting a wide variety of knowledge sources into our QA system, and second by handling the case of numerical answers in post-hoc answer filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Knowledge Server Portal</head><p>For certain classes of routine fact-seeking questions, such as populations and capitals of geo-political entities, the answering agent recognizes a number of ways of asking these questions and formulates a query to a structured knowledge source. These knowledge sources include public databases such as the US Geological Survey, websites with data in formatted tables from websites such as http://www.UselessKnowledge.com, public domain lexicons such as WordNet, and the Cyc knowledge base.</p><p>Each of these knowledge sources is maintained by external groups and is out of our direct control. Each source has data in a different format, requires a different access mechanism, is expressed in a different representation language, provides different reasoning services, and assumes a different ontology. In addition, this external control means that any of these formats, access mechanisms, etc., may change, and of course adding new knowledge sources introduces a new set of choices to be aware of.</p><p>Rather than require that each answering agent understand all these dependencies in order to use the knowledge sources, we have isolated the role of adapting external structured knowledge sources and presenting a consistent set of choices to all the QA components through a set of knowledge-source adapters. We refer to the system component that provides access to these knowledgesource adapters as the knowledge server portal (KSP). The adapters provided by KSP support the set of queries the question analysis component is capable of recognizing, such as "What is the capital of Syria?" or "What is the state bird of Alaska?", and are responsible for composing the proper query to the knowledge sources that may have the answer. The answering agent then may formulate a query that includes the answer as a search term similar to <ref type="bibr" coords="3,353.61,525.68,77.60,8.97" target="#b11">(Prager et al., 2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cyc Sanity Checker</head><p>For certain questions, in particular questions that have numerical answers, adding the answer as a search term is not effective, because there are innumerable variations on the way the number may be expressed in the corpus. Populations, for example, vary over time by a significant amount, and are usually in the millions. For a question like, "What is the population of Maryland?", knowing that the latest figure for the population of Maryland is 5,296,486 does not quite help us search the corpus, because we are almost guaranteed that precise number will not appear. It could be expressed as "5 million", "5.1 million", "5.3 million", or "5,200,390", etc. This process is complicated further when unit conversions are required, as in the question, "How big is Australia?" In addition to having to find a number in the vicinity of "1 million square miles", we also need to account for the fact that the passage may talk about square kilometers, or acres. Instead of folding the known answer into the query in cases like this, we allow the question answering system's regular procedure to generate a set of candidate answers first, and check them to be within some experimentally determined range of the answer the knowledge source provides.</p><p>We have implemented the validation of answers with numerical values using an interface to Cyc called the Cyc sanity checker. The sanity checker is invoked with the expected semantic type of the answer (such as POPULA-TION in the first example above), the focus of the question ("Maryland"), and the system's proposed answer ("X people"). It returns one of the following verdicts: "in range", if the proposed answer is within a certain "fudge factor" (currently 10%) of the value in Cyc's knowledge base, "out of range", if the value falls outside of the acceptable range of values, or "don't know", indicating that Cyc either has no information about the focus itself, or about the particular attribute in question about the focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Resolution -Putting it All Together</head><p>We have described four independent answering agents currently incorporated into our multi-agent architecture. With the exception of the Cyc sanity checker, which is invoked as a post-hoc filtering process for rejecting unreasonable answers, the other three answering agents actively contribute potential answers to a given question. It is then the task of the answer resolution component to determine how the various answers proposed by each answering agent should be combined and reconciled.</p><p>Because of the TREC requirement that all answers be justified by passages from the given corpus (henceforth referred to as the AQUAINT corpus), we feed potential answers given by KSP back into the search process to identify relevant passages in a process similar to that described in <ref type="bibr" coords="4,113.85,533.88,78.18,8.97" target="#b11">(Prager et al., 2001)</ref>. These passages typically contain answers identified by KSP, as well as relevant question terms; thus, they are good candidate passages for locating justification for the answer provided by KSP in the reference corpus. Because of this answer feedback mechanism, all answering agents produce relevant passages and ranked candidate answers in a uniform fashion, simplifying the answer resolution process.</p><p>Currently, PIQUANT's answer resolution component allows for merging at two different points in the pipeline as follows:</p><p>• Passages proposed by multiple answering agents can be combined to feed through the answer selection component of our knowledge-based answering agent.</p><p>• Candidate answers proposed by different answering agents can contribute to determining PIQUANT's final output.</p><p>In addition to determining the answer to a given question, the answer resolution also computes a confidence value indicating the system's certainty in the given answer being a correct answer to the question. This confidence value can then be used for ranking system responses for TREC submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Recognizing When the System Does Not Know</head><p>To make the task more realistic, the test set for the QA track contains a number of questions for which no answer can be found in the document collection, as verified by NIST (we call such questions "NIL questions" or "noanswer questions"). To simplify the task of detecting noanswer questions, we reduce it to the problem of finding the questions for which we can reasonably assume that the system was not able to find a correct answer. This is a much weaker condition since it is dependent on the answer search strategy the system implements, i.e., there might be other strategies that would be successful at finding an answer. It can, however, be implemented easily by setting a threshold on the confidence value that is assigned to a question by the answer resolution module. We implemented two strategies for determining which questions had no answers: a knowledge-based strategy and strategy based on confidence processing. The knowledge-based strategy makes use of KSP and is evoked for questions that were classified as appropriate for KSP look-up. If KSP was able to provide an answer to such a question and the answer string could not be found in the collection, we assumed with high confidence that the question is a NIL-question. Since KSP has only recently been integrated into the system and the number of questions that are referred to it is still limited, this NILassignment strategy applied to only two questions in the final submission.</p><p>In our confidence-based approach, we adopted a twostage processing strategy for detecting and ranking noanswer questions. The first stage detects which questions are likely to have no answer in the collection by comparing their scores with a trained confidence threshold. The second stage takes care of the proper ranking of questions likely to have no answers by increasing their rank.</p><p>In order to train the NIL assignment algorithm, we ran our system on the TREC-10 question set and plotted the distribution of different question types in the final ranking. We marked the questions that did not have an answer according to NIST, the questions for which the system produced a correct answer, and the questions for which the system's output was wrong. The resulting plot is in  Next to each block we plotted the number of questions within that block that were answered correctly and the number of NIL questions within that block. As can be seen in Figure <ref type="figure" coords="5,133.82,311.78,3.74,8.97" target="#fig_0">2</ref>, the numbers change almost monotonically, which suggests that the confidences produced by the system could be a reasonably reliable indicator of the system's performance on a given question. According to Figure <ref type="figure" coords="5,170.18,361.00,3.74,8.97" target="#fig_0">2</ref>, the final two blocks contain more NIL questions than correctly answered questions. This means that changing the system's answer to NIL for all the questions in these two blocks will produce a net gain of 12 correctly answered questions. It will also change the incorrect answers to NIL for 68 questions, which is valuable from the user's point of view, assuming that "NIL" could be interpreted as "I don't know." 1  Based on this analysis we manually picked (on the training data) a confidence threshold that would allow us to select the 100 lowest ranked questions. We used the same threshold on the test set and changed whatever answers the system found for the questions below the threshold to NIL.</p><p>We also looked at how the average precision changed within a 50-question window as we moved it by one question at a time down the ranking, and we found the trend to be close to monotonic. Since changing the answers in the final two blocks to NIL caused the average precision within these blocks to increase, we decided to move the two blocks higher in the ranking to the rank with the same average precision. We computed the difference in confidence value between the answer at the target rank and the highest ranking answer in the NIL-block. This difference was then added to the confidence values of all 1 If the systems participating in the competition were penalized for providing incorrect answers, the questions in the thirdto-last block could also be changed to NIL with no net gain in the number of correctly answered questions but significantly fewer potentially confusing answers.</p><p>NIL answers in our runs submitted to TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>For the 2002 TREC QA track, we submitted three runs, each evaluating a different aspect of PIQUANT's multistrategy, multi-source architecture. These three runs were set up as follows:</p><p>1. Run "IBMPQ" exploits the multi-source aspect of PIQUANT with the knowledge-based answering agent. However, instead of only searching in the AQUAINT corpus for relevant passages, we adopt two other supporting corpora: the corpus used in the TRECs 8-10 QA tracks (henceforth referred to as the TREC corpus) and a subset of the Encyclopedia Britannica. A corpus plays a supporting role when candidate answers found in that corpus can be used to boost the confidence of the same answer found in the main corpus, but the corpus cannot propose new answers not found in the main corpus.</p><p>2. Run "IBMPQSQA" exploits the multi-strategy aspect of PIQUANT by incorporating results from the SQA statistical answering agent made available to us by Ittycheriah and Roukos <ref type="bibr" coords="5,464.35,381.16,75.65,8.97;5,333.13,393.12,21.45,8.97" target="#b5">(Ittycheriah et al., 2001)</ref>. The knowledge-based answering agent was configured to retrieve relevant passages from the AQUAINT and TREC corpora. Additionally, the top 10 passages with the correct answer type retrieved by the statistical answering agent were also considered. PIQUANT's answer resolution component then selects and ranks answers based on passages from the three answering agents/sources.</p><p>Once the top answer for each question is determined, PIQUANT's confidence score for the answer is adjusted based on the answer given independently by the statistical answering agent. A large boost in confidence is given to identical answers proposed by both systems, whereas a small boost in confidence is given to partially overlapping answers.</p><p>3. Run "IBMPQSQACYC" examines the effect of the Cyc sanity checker as a post-hoc filtering process.</p><p>The system is configured exactly as in run "IBM-PQSQA" with the following exception. Prior to determining the top answer for each question, PI-QUANT repeatedly invokes the Cyc sanity checker with a semantic representation of the question and the topmost uneliminated candidate answer as long as the sanity checker deems the given answer "out of range". PIQUANT then eventually selects its most confident answer acceptable to the sanity checker. Note that if this top ranked answer is considered "in range" (as opposed to "don't know"), its confidence is given a strong boost, as it is independently validated by a structured knowledge source.</p><p>After PIQUANT generates the answer to each question and its associated confidence, the NIL-assignment process discussed in Section 4 is invoked. As a result, answers with low confidences were changed to NIL and their confidences slightly increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Results of Submitted Runs</head><p>Table <ref type="table" coords="6,105.74,221.03,4.98,8.97" target="#tab_2">1</ref> shows the results of our three runs both in terms of percent correct and average precision. For comparison purposes, it shows, in addition, the performance of the statistical answering agent submitted independently to the same track (ibmsqa02a) <ref type="bibr" coords="6,198.53,268.85,100.27,8.97;6,72.00,280.81,21.45,8.97" target="#b4">(Ittycheriah and Roukos, 2002)</ref>, as well as the performance of the knowledgebased answering agent using only the AQUAINT corpus (PQ single).<ref type="foot" coords="6,120.87,303.15,3.49,6.28" target="#foot_0">2</ref> A comparison between the results for PQ single and IBMPQ shows the impact of the multi-source aspect of PIQUANT. Our results show that by attempting to identify supporting evidence from two additional corpora, the system achieved 19.9% relative improvement in the percentage of correct answers, and the average precision score improved by 14.6%. A comparison of the results for runs IBMPQ, ibmsqa02a, and IBMPSQA shows the contribution of adopting multiple strategies for question answering in PIQUANT. Although the percentage of questions answered correctly improved for both systems (from 33.8% for IBMPQ and 28% for ibmsqa02a to 35.6% combined), the gain in average precision is much more substantial (9.7% relative improvement compared to IBMPQ). This confirms our intuition that when answering agents (semi-)independently arrive at the same answer, we can be more confident that the answer is a correct one. A comparison of the results for runs IBM-PQSQA and IBMPQSQACYC illustrates the impact of the Cyc sanity checker. Although the impact as shown is very minimal, we should note that because of the limitations in PIQUANT's current question understanding capabilities, the sanity checker was invoked only for 3 out of the 500 questions (although there were several more questions which fit the profile but were not detected as such). Additionally, out of the 3 questions, Cyc only had knowledge about one of them, "What is the population of Maryland?" It is the effect of sanity checking on this question that led to the improved performance for our last run. PIQUANT's top ranked answer for this question in run IBMPQSQA was "50,000", from the sentence "Maryland's population is 50,000 and growing rapidly." This would otherwise be an excellent answer if it were not for the fact that the article from which this passage is extracted discusses (the Maryland population of) an exotic species called nutria. By employing sanity checking, however, PIQUANT was able to consider that answer "out of range", and return an initially lower-ranked correct answer "5.1 million" instead with high confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Effects of NIL Assignment</head><p>In our best submission run (IBMPQSQACYC), the confidence-based NIL-assignment strategy resulted in 147 NIL answers, which was more than we anticipated. This is due to the generally lower confidences on a new question set. The system correctly assigned NIL to 29 out of 46 questions, which translates to a recall of 0.630 and precision of 0.196. By assigning the NIL answers, the system changed 9 correct answers incorrectly to NIL, which gave us a net gain of 20 questions (given the answer pattern set currently available to us). The questions for which the answer was changed to NIL were then moved to rank 288, which resulted in a very minimal (below 0.5%) improvement in the final average precision score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Analysis of the Average Precision Metric</head><p>If the same scoring method had been used this year as in previous TREC QA tracks, the mean reciprocal rank (MRR), exercised over a single answer per question would amount to a simple count of number correct. However, in order to begin to tackle the issue of answer reliability, answers this year were returned by participants in decreasing order of system confidence (although no numerical values representing confidence were returned). The systems' final scores were evaluated by Average Precision, the average being computed over the first answer, the first two answers, the first three answers, and so on up to the whole set. Clearly, this gives considerably more relative weight to the earlier answers, and considerably less to the last answers. The contribution c k of a correct answer in position k out of N questions in total is given by ln <ref type="figure" coords="6,401.12,569.73,4.98,8.97" target="#fig_1">3</ref> shows this contribution, in units of 1/500, for positions 1 to 500 for a set of 500 questions. Relative to a score of approximately 1 unit for the greater part of the range, the contribution of the first position is nearly 7, indicating how important it is for systems to sort their submissions well.</p><formula xml:id="formula_0" coords="6,323.16,555.90,131.60,22.80">( N +1 k+1 ) ≤ N c k ≤ ln( N k ) + 1 k . The plot in Figure</formula><p>Another view of the evaluation space introduced by the Average Precision metric is presented in Figure <ref type="figure" coords="6,532.53,653.42,3.74,8.97" target="#fig_2">4</ref>. The diagonal line and "cloud" represent what happens with no attempt to sort the results. The solid line in the center of the cloud is the ideally-uniformly-distributed case (i.e. if 1/3 of the answers are right, the submitted list goes ...RWWRWWRWWRWW...), and the  The upper curve is the optimal case (e.g. RRRRRR......WWWWW), while the lower curve is the pessimal case (i.e. all the right answers are sorted to the end.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Ranking Ability</head><p>The circled points in the middle of Figure <ref type="figure" coords="7,254.36,521.91,4.98,8.97" target="#fig_2">4</ref> represent our TREC runs. The maximum possible score, represented by the upper curve, for n correct out of N is approximately n N (1 + ln N n+1 ). By examining how far up a virtual vertical line from the diagonal (expected) to the upper curve (max) a plotted point (actual) lies, one can see how well the system sorted its answers for submission -i.e. how well it knows what it knows. This fraction, which we call the Ranking Ability, can be computed as actual-expected max-expected . In the case of our best run, we scored 179 questions correct (35.8%), for which the expected unsorted average precision is 0.358. The maximum possible average precision is 0.726 for this number correct, based on the above formula. Our score of 0.588 represents a ranking ability of .625, indicating a good correlation of confidence and correctness. The top 15 submissions are shown in Table <ref type="table" coords="7,172.51,713.20,3.74,8.97" target="#tab_3">2</ref>, sorted by ranking ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have presented here the first quantitative results from our new PIQUANT question answering system. PI-QUANT exploits a multi-strategy and multi-source approach to QA, enabling not only the best approach to be taken on a per-question basis, but the use of mutual reinforcement when multiple agents or sources are used simultaneously. Based on our submissions to TREC and their results, we have shown significant improvements achieved by our approaches over baseline systems. First, we have shown an 14.6% relative gain in average precision score with multiple corpora over a single one, and a further 9.7% relative gain by adding a statistical answering agent. Second, we have identified an effective method for assigning NIL answers to questions based on the confidence values generated by our system. This method identified 63% of all no-answer questions in the test set with minimal false negatives. Third, we have shown that a multi-agent approach to question answering allows us to achieve a good correlation of confidence values and correctness. Our average precision of 0.588 on 179 correct questions achieved 62.5% of the gain achievable by sorting, a significant improvement over the baseline of random sorting.</p><p>We have only just begun to incorporate a knowledge base and inference engine (Cyc) to do sanity checking of answer candidates: the number of times this capability was invoked are too few to do other than say that the approach looks promising. In our future work, we plan to expand PIQUANT's ability to recognize cases when sanity checking is appropriate, improve Cyc's coverage of valid answer ranges, as well as adopt a confidencebased approach to selecting answering agents. Improvements since TREC have led to 16 invocations of the sanity checker on the TREC 2002 question set. These invocations led to one additional correct 1st, 2nd, and 3rd place answers each, validated 4 correct 1st place answers while erroneously validating 3 incorrect 1st place answers, and rejected 122 incorrect answers without any erroneous rejections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,79.81,193.24,211.18,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: TREC-10 training data for NIL assignment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,72.00,373.64,226.80,8.97;7,72.00,385.59,62.26,8.97"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Contribution of Correct Answers to Average Precision Score</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,313.20,373.64,226.80,8.97;7,313.20,385.59,66.13,8.97"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Upperbounds and Lowerbounds for Average Precision Scores</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.00,71.75,229.54,102.33"><head></head><label></label><figDesc>.-x.xxxxxxxx..x-xxxxxxxxxx.xxxxx.x.xxx.-xx 4 38 xx.....x.-xx.....xx....x.xx.x..xxx.xx...xx.x..xx.x 1 22 .-...x.xx-..x..x.xx....xx.x...xx.....x..xxx....xx. 2 18 ........x....x..xxxx...x...xx....xxxxx--......xxx. 2 17 ..x.xxx...-x-...xx.....x...xx--.xx-....xx..x..x... 5 16 ..x.x.-......x....x.x-.x.xx...-x-x-x-...-..x-x.x.x 8 15 x..-x.....x.x.....-..........-...-..x.-....-..x...</figDesc><table coords="5,72.00,71.75,229.54,102.33"><row><cell></cell><cell cols="2">NIL CORRECT</cell></row><row><cell>xxxxxxxxxxxxxx.xx.xxxxxxxxxxxxxx.x..xx.xx</cell><cell>0</cell><cell>35</cell></row><row><cell cols="2">xxxxxx-x6</cell><cell>6</cell></row><row><cell>.x--......xx....-.-..x.-....-.-..x...........--...</cell><cell>9</cell><cell>5</cell></row><row><cell cols="2">-.-.-..--...-x.xx....-.-x......-.....-..-...-.x.-. 13</cell><cell>5</cell></row><row><cell>x correctly answered question</cell><cell></cell><cell></cell></row><row><cell>. incorrectly answered question</cell><cell></cell><cell></cell></row><row><cell>-NIL question according to NIST</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,151.39,73.25,311.47,49.75"><head>Table 1 :</head><label>1</label><figDesc>PIQUANT's TREC 2002 Run Results    </figDesc><table coords="7,151.39,73.25,311.47,28.39"><row><cell></cell><cell cols="3">IBMPQ IBMPQSQA IBMPQSQACYC</cell><cell cols="2">ibmsqa02a PQ single</cell></row><row><cell>% Correct</cell><cell>33.8%</cell><cell>35.6%</cell><cell>35.8%</cell><cell>28.0%</cell><cell>28.2%</cell></row><row><cell>Avg Prec</cell><cell>0.534</cell><cell>0.586</cell><cell>0.588</cell><cell>0.454</cell><cell>0.466</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,81.22,73.25,210.60,184.84"><head>Table 2 :</head><label>2</label><figDesc>Ranking Ability of Top 15 Submissions</figDesc><table coords="8,81.22,73.25,210.60,163.49"><row><cell>Submission</cell><cell>AP</cell><cell cols="2">% Correct Ranking Ability</cell></row><row><cell>limsiQalir2</cell><cell>.497</cell><cell>26.6</cell><cell>.657</cell></row><row><cell cols="2">IBMPQSQACYC .588</cell><cell>35.8</cell><cell>.627</cell></row><row><cell>BBN2002C</cell><cell>.499</cell><cell>28.4</cell><cell>.603</cell></row><row><cell>nuslamp2002</cell><cell>.396</cell><cell>21.0</cell><cell>.569</cell></row><row><cell>IRST02D1</cell><cell>.589</cell><cell>38.4</cell><cell>.559</cell></row><row><cell>isi02</cell><cell>.498</cell><cell>29.8</cell><cell>.555</cell></row><row><cell>FDUT11QA1</cell><cell>.434</cell><cell>24.8</cell><cell>.539</cell></row><row><cell>ibmsqa02c</cell><cell>.455</cell><cell>29.0</cell><cell>.461</cell></row><row><cell>exactanswer</cell><cell>.691</cell><cell>54.2</cell><cell>.449</cell></row><row><cell>ilv02wt</cell><cell>.450</cell><cell>30.8</cell><cell>.392</cell></row><row><cell>uwmtB3</cell><cell>.512</cell><cell>36.8</cell><cell>.392</cell></row><row><cell>ali2002b</cell><cell>.496</cell><cell>36.2</cell><cell>.365</cell></row><row><cell>aranea02a</cell><cell>.433</cell><cell>30.4</cell><cell>.357</cell></row><row><cell>LCCmain2002</cell><cell>.856</cell><cell>83.0</cell><cell>.168</cell></row><row><cell>pris2002</cell><cell>.610</cell><cell>58.0</cell><cell>.095</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="6,88.14,683.99,210.66,8.07;6,72.00,693.95,226.79,8.07;6,72.00,703.91,226.79,8.07;6,72.00,713.88,38.29,8.07"><p>The results for PQ single were obtained by manual evaluation by one of the authors with reference to available judgments by NIST accessors and answer patterns made available by Ken Litkowski.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Abe Ittycheriah</rs> and <rs type="person">Salim Roukos</rs> for making available their system and results for experimental purposes, <rs type="person">Stefano Bertolo</rs> for his help with Cyc integration, and <rs type="person">Ruchi Kalra</rs> for ontology population. This work was supported in part by the <rs type="funder">Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program</rs> under contract number <rs type="grantNumber">MDA904-01-C-0988</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_q8bPrpU">
					<idno type="grant-number">MDA904-01-C-0988</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,628.02,226.80,8.97;8,81.96,638.97,216.83,8.97;8,81.96,649.93,216.84,8.97;8,81.96,660.89,37.36,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,113.76,638.97,180.69,8.97">Exploiting redundancy in question answering</title>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,93.92,649.93,173.87,8.97">Proceedings of the 24th SIGIR Conference</title>
		<meeting>the 24th SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,680.32,226.80,8.97;8,81.96,691.28,216.83,8.97;8,81.96,702.24,216.84,8.97;8,81.96,713.20,113.08,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,182.29,691.28,116.51,8.97;8,81.96,702.24,29.78,8.97">Web reinforced question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Mclearn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,136.08,702.24,162.72,8.97;8,81.96,713.20,43.68,8.97">Proceedings of the Tenth Text Retrieval Conference</title>
		<meeting>the Tenth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="673" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,75.16,226.80,8.97;8,323.16,86.12,216.83,8.97;8,323.16,97.08,216.83,8.97;8,323.16,108.03,216.83,8.97;8,323.16,118.99,216.83,8.97;8,323.16,129.95,216.83,8.97;8,323.16,140.91,111.25,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,524.51,97.08,15.49,8.97;8,323.16,108.03,216.83,8.97;8,323.16,118.99,94.94,8.97">The role of lexico-semantic feedback in open-domain textual question-answering</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Morarescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,441.86,118.99,98.14,8.97;8,323.16,129.95,216.83,8.97;8,323.16,140.91,42.37,8.97">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="274" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,157.90,226.80,8.97;8,323.16,168.86,216.83,8.97;8,323.16,179.82,216.84,8.97;8,323.16,190.78,141.43,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,459.83,168.86,80.17,8.97;8,323.16,179.82,60.23,8.97">Question answering in Webclopedia</title>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Junk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.34,179.82,136.66,8.97;8,323.16,190.78,72.03,8.97">Proceedings of the Ninth Text REtrieval Conference</title>
		<meeting>the Ninth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,207.77,226.80,8.97;8,323.16,218.73,216.83,8.97;8,323.16,229.69,216.84,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,514.53,207.77,25.47,8.97;8,323.16,218.73,197.35,8.97">IBM&apos;s statistical question answering system -TREC-11</title>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,323.16,229.69,212.47,8.97">Proceedings of the Eleventh Text Retrieval Conference</title>
		<meeting>the Eleventh Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,246.68,226.80,8.97;8,323.16,257.63,216.83,8.97;8,323.16,268.59,216.84,8.97;8,323.16,279.55,113.08,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,352.26,257.63,187.74,8.97;8,323.16,268.59,32.50,8.97">IBM&apos;s statistical question answering system -TREC10</title>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,379.39,268.59,160.61,8.97;8,323.16,279.55,43.68,8.97">Proceedings of the Tenth Text Retrieval Conference</title>
		<meeting>the Tenth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="258" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,296.54,226.80,8.97;8,323.16,307.50,216.83,8.97;8,323.16,318.46,54.77,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,418.33,296.54,121.67,8.97;8,323.16,307.50,110.96,8.97">Cyc: A large-scale investment in knowledge infrastructure</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,445.22,307.50,94.78,8.97;8,323.16,318.46,17.42,8.97">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,335.45,226.80,8.97;8,323.16,346.41,184.55,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,408.33,335.45,131.67,8.97;8,323.16,346.41,27.36,8.97">Wordnet: A lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,358.02,346.41,112.34,8.97">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,363.40,226.80,8.97;8,323.16,374.36,216.83,8.97;8,323.16,385.32,216.83,8.97;8,323.16,396.28,216.83,8.97;8,323.16,407.23,216.83,8.97;8,323.16,418.19,154.71,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,370.82,385.32,169.18,8.97;8,323.16,396.28,138.16,8.97">The structure and performance of an opendomain question answering system</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,480.33,396.28,59.67,8.97;8,323.16,407.23,216.83,8.97;8,323.16,418.19,85.83,8.97">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="563" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,435.18,226.80,8.97;8,323.16,446.14,216.83,8.97;8,323.16,457.10,216.83,8.97;8,323.16,468.06,164.23,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,500.38,435.18,39.61,8.97;8,323.16,446.14,117.50,8.97">High performance question answering</title>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,463.69,446.14,76.31,8.97;8,323.16,457.10,216.83,8.97;8,323.16,468.06,95.31,8.97">Proceedings of the 24th SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,485.05,226.80,8.97;8,323.16,496.01,216.83,8.97;8,323.16,506.97,216.84,8.97;8,323.16,517.93,62.54,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,380.07,496.01,159.93,8.97;8,323.16,506.97,21.59,8.97">Question-answering by predictive annotation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anni</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,364.62,506.97,171.01,8.97">Proceedings of the 23rd SIGIR Conference</title>
		<meeting>the 23rd SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,534.92,226.80,8.97;8,323.16,545.88,216.83,8.97;8,323.16,556.83,216.84,8.97;8,323.16,567.79,121.66,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,352.69,545.88,187.30,8.97;8,323.16,556.83,21.59,8.97">Answering what-is questions by virtual annotation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,363.70,556.83,176.30,8.97;8,323.16,567.79,62.22,8.97">Proceedings of Human Language Technologies Conference</title>
		<meeting>Human Language Technologies Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="26" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,584.78,226.80,8.97;8,323.16,595.74,216.83,8.97;8,323.16,606.70,216.84,8.97;8,323.16,617.66,78.89,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,433.52,595.74,106.47,8.97;8,323.16,606.70,83.92,8.97">Question answering using predictive annotation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,431.77,606.70,108.23,8.97;8,323.16,617.66,29.79,8.97">Advances in Question Answering</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="8,313.20,634.65,226.80,8.97;8,323.16,645.61,216.83,8.97;8,323.16,656.57,216.84,8.97;8,323.16,667.53,216.71,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,504.03,634.65,35.97,8.97;8,323.16,645.61,212.52,8.97">Learning surface text patterns for a question answering system</title>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,334.45,656.57,205.55,8.97;8,323.16,667.53,157.79,8.97">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,684.52,226.80,8.97;8,323.16,695.48,216.84,8.97;8,323.16,706.43,160.51,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,423.43,684.52,116.57,8.97;8,323.16,695.48,99.64,8.97">Overview of the TREC 2001 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,443.61,695.48,96.39,8.97;8,323.16,706.43,101.08,8.97">Proceedings of the 10th Text Retrieval Conference</title>
		<meeting>the 10th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
