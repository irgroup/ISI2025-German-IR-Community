<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,199.50,59.44,223.95,12.60;1,153.50,77.44,315.95,12.60">Report on the TREC-11 Experiment: Arabic, Named Page and Topic Distillation Searches</title>
				<funder ref="#_tQAxGRQ #_kCQgw9E">
					<orgName type="full">SNSF (Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,238.50,96.80,67.25,10.80"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
							<email>jacques.savoy@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institut interfacultaire d&apos;informatique</orgName>
								<orgName type="institution">Université de Neuchâtel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.12,96.80,70.43,10.80"><forename type="first">Yves</forename><surname>Rasolofo</surname></persName>
							<email>yves.rasolofo@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institut interfacultaire d&apos;informatique</orgName>
								<orgName type="institution">Université de Neuchâtel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,199.50,59.44,223.95,12.60;1,153.50,77.44,315.95,12.60">Report on the TREC-11 Experiment: Arabic, Named Page and Topic Distillation Searches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF1EA20222CE0E846C3E4BE00E92EB46</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year we took part in the Arabic cross-language information retrieval track (for us limited to monolingual Arabic retrieval) and also in both named page and topic distillation searches. In the last two tasks, we made use of link anchor information and document content in order to construct Web page representatives. This document representation uses multi-vectors in order to highlight the importance of both link anchor information and document content.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Today the IR community is faced with a new paradigm and many exciting challenges with regards to Web page searches. Some of these include: managing huge volumes of documents via distributed IR models, crawling across the Web in order to find appropriate Web sites to index, accessing documents written in various languages, measuring the quality or authority of available information, providing answers to user requests that are often very short and expressed in ambiguous terms, satisfying a large range of search types (ad hoc, question-answering, location of online services, topic distillation, known-item and interactive searches for specific document types, or satisfying specific geographical or time constraints).</p><p>In this context, the first part of this paper presents our monolingual Arabic retrieval model. Section 2 describes our procedures for indexing and retrieving Web pages based on two document representations, and our distributed indexing framework based on the Okapi probabilistic model. Section 3 explains the IR approach we use when combining both Web page content and anchor information when searching for specific named pages. Finally, Section 4 describes how our IR scheme can be used within the context of topic distillation task.</p><p>In order to evaluate our hypothesis, we used the SMART system as a testbed, implementing various vector-space IR schemes and the Okapi probabilistic model <ref type="bibr" coords="1,71.50,651.17,101.43,9.00" target="#b14">(Robertson et al., 2000)</ref>. This year our experiments were conducted on an Intel Pentium III/600 (memory: 1 GB, swap: 2 GB, disk: 6 x 35 GB) and all experiments were fully automated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Arabic Information Retrieval</head><p>During the last two CLEF evaluation campaigns we suggested various IR models and tools for handling different European languages <ref type="bibr" coords="1,450.50,205.17,64.22,9.00" target="#b18">(Savoy, 2002a;</ref><ref type="bibr" coords="1,520.50,205.17,26.43,9.00" target="#b19">2002b)</ref>. This year we expanded upon our knowledge by adding the Semitic language family, which includes Arabic.</p><p>The IR model we are proposing for Arabic text searches involves indexing both documents and queries, based on the words described in Section 1.1, or using n-gram segmentation, as presented in Section 1.2. Section 1.3 shows how we can combine result lists provided using various indexing and searching schemes that process the same document collection. The last section provides an account of the retrieval effectiveness achieved by various IR models and also that of various combined approaches. The diverse IR tools may be seen on our Web site (www.unine.ch/info/clef/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Word-Based Indexing</head><p>In order to effectively search Arabic documents, we first convert and normalize the Arabic Unicode characters into Latin letters (a technique used in Malta where the Arabic language is written using the Latin alphabet). Due to variations in morphological rules or geographical traditions however many Arabic characters have more than one Unicode representation. For example there are different forms of alef (alef madda "Â", alef hamza above "Ã", alef hamza below "Å"), transliterated within our approach into the same letter "A" (see Table <ref type="table" coords="1,351.78,528.17,3.72,9.00" target="#tab_0">1</ref>). Our conversion procedure is based on but is not identical to that used in previous work by <ref type="bibr" coords="1,516.53,540.17,34.83,9.00;1,325.50,552.17,50.55,9.00">Darwish et al. (2002)</ref>. There are of course some questionable assignments such as the hamza letter "Á", which is considered equivalent to the alef maksura "v". In this phase some Unicode characters have also been removed (e.g., diacritic marks that are usually optional in newspapers such as tatweel "?", fathatan "ë " or various punctuation marks "¿". These diacritic marks may however be important in other contexts in order to resolve underlying ambiguity (e.g., in legal documentation)).</p><p>In a second step we ignore words appearing in our Arabic stopword and Arabic stoplist (the latter contains 347 words, available at www.unine.ch/info/clef/).</p><p>In a third step we automatically remove both prefixes and suffixes to form our Arabic stems. These relatively light stemming approaches are similar to those suggested by <ref type="bibr" coords="2,150.75,100.17,103.32,9.00">Larkey &amp; Connell (2002)</ref> or <ref type="bibr" coords="2,267.71,100.17,29.75,9.00;2,71.50,112.17,53.99,9.00">Lackey et al. (2002)</ref>. As shown in Table <ref type="table" coords="2,230.93,112.17,3.85,9.00" target="#tab_1">2</ref>, our stemmer "stem2" is more conservative while our "stem3" represents a more aggressive affix-stripping process. The word length must be greater than a given threshold (between 4 and 6 letters) before we will remove a given affix. Some of our rules are questionable and our stemmers must be viewed, as they are only a first draft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arabic</head><p>Latin Arabic Latin </p><formula xml:id="formula_0" coords="2,85.50,217.17,198.83,191.83">v Á Y Ç Â Ã Å A alif u R w waw T y yeh V b beh É p teh X t teh Y v theh Ì j jeem Í H hah Î x khah b d dal c O thal d r reh f s seen g P sheen h S sad i D dad j T tah k Z zah Ù E ain Ú g ghain n f feh o q qaf p k kaf q l lam r m meem s n noon h heh w X yeh e z zain</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">N-gram Indexing</head><p>As an alternative procedure we can index Arabic documents using 3-gram (or tri-gram) indexing <ref type="bibr" coords="2,71.50,703.17,104.68,9.00">(Darwish &amp; Oard, 2002)</ref>. In this case, each word is replaced by a set of three-letter sequences. For example the word "document" will be replaced by {"doc", "ocu", "cum", "ume", "men" and "ent"}. In our current implementation we do not stem words before splitting them into tri-grams and we also remove very frequent tri-grams (obtained from our stopword list).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Data Fusion</head><p>We use a single search model (or engine) when searching document collections. We might however suggest sending the request to different search engines that handle the same document collection but that use different indexing or search schemes. Finally, once we have obtained result lists from these various search engines, we need to merge them in an effective manner (data fusion problem). Thus, even though certain degrees of retrieval effectiveness may be attributed to each search approach, combining the result lists might provide better average precision. If we were to use RSVk to denote the retrieval status value (or document score) for a given document retrieved by the kth search engine, <ref type="bibr" coords="2,325.50,319.17,86.28,9.00" target="#b6">Fox &amp; Shaw (1994)</ref> suggest using various operators (see Table <ref type="table" coords="2,369.59,331.17,4.18,9.00">3</ref>) and show that the best performance can be achieved using "combSUM". combMAX MAX (RSVk) combMIN MIN (RSVk) combSUM SUM (RSVk) combANZ SUM (RSVk) / # of nonzero (RSVk) combNBZ SUM (RSVk) * (# of nonzero (RSVk)) combRSV% SUM (RSVk / maxRSV) combRSVn SUM[(RSVk-minRSV)/(maxRSV-minRSV)]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3. Data fusion strategies</head><p>Of course we might also employ the round-robin merging strategy whereby we take the first retrieved item from the first result list, then the first retrieved document from the second list, etc., and finally the first item from the last result list and then back again to the first results list, thus providing the next item to be put in the final list. Duplicates encountered in this process are simply ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Evaluation</head><p>We evaluated various vector-space schemes (see Appendix 1 for detailed specifications of these models) together with the Okapi probabilistic model. As shown in Table <ref type="table" coords="2,362.74,632.17,8.21,9.00" target="#tab_2">4a</ref>, we also evaluated our two light stemmers and the tri-gram indexing scheme using short queries ("Title" or T), medium-size queries (constructed using the Title and Descriptive logical sections or TD) or long queries (based on Title, Descriptive and Narrative sections or TDN).</p><p>An examination of this data shows that the best average precision is obtained using the Okapi model while second best results are usually obtained using the vector-space model "Lnu-ltc", and the "dtu-dtn" scheme usually ranks third. Moreover, it seems that our "stem2" stemmer performs slightly better than the more aggressive "stem3" procedure (the mean difference over 10 retrieval models was 4.7% for T queries, 6.1% for TD queries, and 7.4% for TDN queries). On average, the tri-gram indexing scheme seems to be a little bit less effective than word-based indexing (using the "stem3" or "stem2" stemming approaches). Note however that with the Okapi model, the tri-gram approach performed better for shorter queries (Title only) or TD requests.</p><p>From previous evaluations on different European languages <ref type="bibr" coords="3,116.53,260.17,62.61,9.00" target="#b18">(Savoy, 2002a)</ref>, it is clearly apparent that requests containing more search terms provide improved average precision (from "Title" to TD, with a mean improvement of around 13.3% and a mean difference between "Title" and TDN of around 17.5%). With the Arabic corpus these differences appear to fall within a similar range, as shown in Table <ref type="table" coords="3,476.30,120.17,8.39,9.00" target="#tab_2">4a</ref>. For example, using the Title only evaluation as a baseline, performance can be improved by about 9.9% for TD queries (mean over 10 retrieval models, using stem2) or 4.9% with TD requests (mean over 10 retrieval models, using tri-grams). When comparing short request query performances (Title only and TDN), the mean difference over 10 IR models is around 18.6% (stem2) or 15.6% (stem3). For tri-gram models however the average precision differences are around -2.1%, due to the poor performance by the "nnn-nnn" and "bnn-bnn" approaches during TDN queries. Pseudo-relevance feedback (or blind-query expansion) has proven to be a useful technique for enhancing retrieval effectiveness. In this study, we adopted Rocchio's approach <ref type="bibr" coords="4,163.50,353.17,106.33,9.00" target="#b1">(Buckley et al., 1996)</ref> with α = 0.75, β = 0.75 whereby the system was allowed to add t terms extracted from the k best ranked documents obtained from the original query. We used the Okapi probabilistic model to evaluate this proposal and then enlarged the query by 10 to 75 terms taken from the 5 or 10 best-ranked articles (see Table <ref type="table" coords="4,230.74,425.17,8.37,9.00" target="#tab_3">4b</ref>). From examining the data in this table we were able to conclude that overall blind-query expansion does indeed improve retrieval performance. For example, with short requests the improvement is +21.5% when applying the stem2 stemmer, +23.1% with stem3 and +16% with the trigram model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Precision</head><p>Finally, we evaluated various data fusion strategies that might be employed to improve retrieval effectiveness. In our case we used the same document collection and simply submitted the same request to our three search engines (stem2, stem3, and tri-grams). Based on the data in Table <ref type="table" coords="4,142.86,573.17,3.80,9.00" target="#tab_4">5</ref>, it appears that data fusion based on combRSVn performs better when blind query expansion is taken into account. The combMAX strategy seems to be the more appropriate solution when we ignore pseudo-relevance feedback. However, both combRSVn and combSUM seem to be more robust data fusion operators. We should however mention that the percentage improvement over the best single approach is not really significant (e.g., in Title only queries without query expansion, the combRSVn increases the average precision by +4.9% compared to +2.2% when using TDN requests). Finally, the last three columns of Table <ref type="table" coords="4,362.22,313.17,5.00,9.00" target="#tab_4">5</ref> show the results of the same three individual runs where we instead multiplied each document score obtained from the tri-gram model by 1.5, without modifying document scores for both the word-based indexing schemes.</p><p>Table 6 lists the exact specifications for our official runs. These runs were carried out using different numbers of documents and terms during blind query expansions but all runs were built using the combRSVn operator and multiplying the tri-gram document scores by 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Okapi Search Model</head><p>As shown in the preceding section, the Okapi search model provides significantly greater retrieval effectiveness. However, in order to manage the Web collection (1,247,753 documents as extracted from the .GOV domain, or about 18.1 GB of data), we needed to modify this search model for two reasons. First, we wanted to incorporate two document representatives for each Web page, and secondly we needed to distribute the inverted file in order to respect the 2 GB limit.</p><p>When using multiple document representations, the retrieval status value (or document score denoted RSV(Di)) was calculated as follows (inner product):</p><formula xml:id="formula_1" coords="4,381.50,626.54,113.22,27.20">RSV(D i ) = w ij ⋅ qw j j=1 m ∑</formula><p>within which wij indicates the weight assigned to the term Tj in the document Di, qwj the indexing weight assigned to the same term in the current query and m the number of search keywords.</p><p>When processing two (or more) document representations, we estimated the degree of similarity between the document Di and the current query as a linear combination of the inner product of the two document representations to be given as:</p><formula xml:id="formula_2" coords="5,77.50,123.54,216.16,27.20">RSV(D i ) =α⋅ w ij (1) ⋅qw j + (1 -α)⋅ j=1 m ∑ w ij (2) ⋅ qw j j=1 m ∑ (1)</formula><p>where w</p><p>(1) ij indicates the weight attached to the term Tj in the document Di in the first document representation (and w</p><p>(2) ij for the second document surrogate), and α a parameter used to assign a comparative importance to the first document representative as relative to the second.</p><p>Thus, by assigning α a value close to 1.0, we give more importance to the first document representation. At the limit, setting α = 1.0 implies that we ignore the second document surrogate.</p><p>Creating a single inverted file from a collection of around 18 GB might be impossible using a 32-bit system (e.g., Linux). To overcome this limit, we will concentrate on the last scheme, and in this case we will follow the approach described in <ref type="bibr" coords="5,213.30,340.17,83.95,9.00;5,71.50,352.17,21.89,9.00" target="#b13">(Rasolofo &amp; Savoy, 2003)</ref>, whereby we merge the result lists obtained from searching different collections (collection fusion problem). This is achieved by using the document scores computed by each collection as a key for the merging and sorting process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Named Page Searching</head><p>When submitting a request to a search engine, sometimes users do not want a ranked list of Web pages regarding a particular topic but rather they would prefer the location of an underlying service or knownitem (usually presented in a short list of the most probable locations). For example, an appropriate answer to the requests "US passport renewal", "Maryland unemployment insurance benefits" or "FBI's most wanted list" does not consist of a ranked list of documents about these subjects but rather the site(s) containing the required form/information/list. To accomplish this goal we need to implement an IR system that can retrieve a short number of pages (at the limit, only one) corresponding to the user's request. In this context of known-item search, the underlying IR system must clearly place the emphasis on precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Models</head><p>As a basis for our search model we used the Okapi model as described in Section 2. Our first document representative was based on information found in the Web page and its corresponding &lt;TITLE&gt; and &lt;META&gt; tags ("keywords" and "description"). Web pages might of course also contain links and their anchor texts (or anchor texts for outgoing links) and this combined set of internal textual information would thus form the first representatives of these pages.</p><p>On the other hand, previous studies <ref type="bibr" coords="5,486.43,124.17,65.00,9.00;5,325.50,136.17,22.66,9.00" target="#b2">(Craswell et al., 2001)</ref>, <ref type="bibr" coords="5,355.73,136.17,103.91,9.00" target="#b20">(Westerveld et al., 2002)</ref>, <ref type="bibr" coords="5,467.22,136.17,84.03,9.00" target="#b9">(Kraaij et al., 2002)</ref> have shown that anchor texts from other Web pages pointing to the current page provide compact and often accurate descriptions of the current page's content. Thus link anchor texts extracted from all Web pages pointing to the current page were concatenated to form our second document representative. To this second surrogate we also added the text contained in the current page's &lt;TITLE&gt; tag (with the text delimited by this tag appearing in both document representatives). Finally, we might also consider the URL content (or more precisely, the similarity between the URL text and the request, or also the URL length). This additional source of information has not taken into account in our current search models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation</head><p>In this IR search model based on two document representatives, we first needed to determine the relative importance assigned to the first document representative (based on internal Web page content) as compared to the weight attached to the second document surrogate (based mainly on link anchor texts from Web pages pointing to the current one). This relative importance for each surrogate is controlled by the α parameter (see Section 2). When α = 1.0, we would only account for internal textual representation while when setting α = 0.5, we would attribute equal importance to both document representatives. When high precision results are required for indexing documents or requests, it is usually not a good idea to include a stemming procedure. However a very light stemming procedure might only be adopted when removing the final "s" in English (such stemming is called "S-stemming" <ref type="bibr" coords="6,174.50,503.17,63.83,9.00" target="#b7">(Harman, 1991)</ref>). Thus the words "house" and "houses" will be reduced to the same root while the term "housing" will be treated as a different indexing unit.</p><p>A comparison of results depicted in Table <ref type="table" coords="6,273.70,555.17,5.00,9.00" target="#tab_8">9</ref> (Sstemming) to those in Table <ref type="table" coords="6,190.00,567.17,5.00,9.00" target="#tab_7">8</ref> (no stemming) indicates that performance differences are rather small. Better performances can however always only achieved from IR approaches that ignore the stemming phase.</p><p>Finally, Table <ref type="table" coords="6,147.70,619.17,10.31,9.00" target="#tab_9">10</ref> provides a summary description of our four official runs. Only the UniNEnp3 run needed an additional comment. This run was based on UniNEnp1 and after obtaining a ranked list, we reranked the first ten retrieved items according to the number of matches between the query terms and the corresponding Web page's title field (however, such a strategy does not improve retrieval effectiveness). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Topic Distillation Searches</head><p>Under the label "topic distillation", we had to implement an IR scheme able to find a list of key resources on a given topic. Explicitly defining what does or does not constitute a good key resource is however difficult, and each definition seems to become more ambiguous. Of course, Web pages with appropriate content might be considered as good key resources and we could retrieve them using a classical IR model. On the other hand, key resources may also be good hubs (or Web pages pointing to different pages containing pertinent content with respect to the submitted request). Moreover, if a Web page is linked to two, three or more sons having a high degree of similarity with the request, it seems more appropriate to return this father page rather than the two, three of more sons. More generally however returning many pages extracted from the same Web site would not be viewed as a wise strategy. Thus to suggest a proper solution for this specific task, we decided to employ different strategies capable of pointing to reliable starting points for browsing rather than simply retrieving Web pages with good content. An overview of such strategies that might be applied in a Web environment can be found in <ref type="bibr" coords="6,515.86,447.17,35.59,9.00;6,325.50,459.17,63.60,9.00" target="#b17">Savoy &amp; Rasolofo (2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Search Models</head><p>As for the task of named page searching, we built two document representatives for each Web page contained in the .GOV collection. The first representative accounted for Web page content along with its &lt;TITLE&gt; and &lt;META&gt; tags ("keywords" and "description") plus all link anchor texts extracted from other pages pointing to this current page. The second document representative was built from the text delimited by the &lt;TITLE&gt; tag together with link anchor texts from all outgoing links. These two document representations may be useful both for accounting for the content of both the Web page (first surrogate) and other pages accessible within a one-click distance from the current page (our second representative).</p><p>Once the pages are retrieved, we followed hyperlinks coming into them in order to define proper starting points for browsing (in this case we followed existing hyperlinks in the reverse orientation). To retrieve these starting points we used our spreading activation (SA) searching scheme <ref type="bibr" coords="7,151.50,72.17,59.53,9.00" target="#b15">(Savoy, 1996)</ref>, <ref type="bibr" coords="7,222.50,72.17,75.49,9.00;7,71.50,84.17,23.14,9.00" target="#b3">(Crestani &amp; Lee, 2000)</ref>, <ref type="bibr" coords="7,102.54,84.17,101.38,9.00" target="#b16">(Savoy &amp; Picard, 2001)</ref>. Using this method, document scores initially computed by the IR system (denoted RSV(D i )), are propagated to the linked documents through a certain number of cycles, using a propagation factor. We used a simplified version with only one cycle and a fixed propagation factor λ for all links. Thus the final retrieval status value for a document D i linked to k documents is computed using the following equation:</p><formula xml:id="formula_3" coords="7,85.50,191.54,213.66,27.20">RSV'(D i ) = RSV(D i ) + λ • RSV(D j ) j= 1 k ∑ (3)</formula><p>When trying in our experiments to extract the proper starting sites for browsing, we only considered all incoming links for each of the k best-ranked documents (in this paper the constant k was fixed to 200 and the parameter λ to 0.35).</p><p>As an alternative, we assumed that the first k topranked items would form a "root set" or a kernel of pertinent pages from which we could consider all incoming and all outgoing links in order to form an extended set (called the base set) of pages that might be of interest for a given topic. Based on Kleinberg's HITS algorithm, we assumed that a Web page pointing to many other information sources must be viewed as a "good" hub while a document with many Web pages pointing to it must be viewed as a "good" authority. Likewise, a document that points to many "good" authorities is an even better hub while a Web page pointed to by many "good" hubs is an even better authority <ref type="bibr" coords="7,252.50,440.17,45.82,9.00;7,71.50,452.17,21.53,9.00" target="#b8">(Kleinberg, 1998)</ref>.</p><p>For document D i after c+1 iterations, the updated formulas for the hub and authority scores H c+1 (D i ) and A c+1 (D i ) are:</p><formula xml:id="formula_4" coords="7,84.50,504.82,117.33,76.13">A c+1 (D i ) = ∑ D j =parent(D i ) H c (D j ) H c+1 (D i ) = ∑ D j =child(D i ) A c (D j )</formula><p>which is computed for the k best-ranked documents (defined as the root set) retrieved by a classical search model, together with their children and parents (which defined the base set). The hub and authority scores were updated for five iterations (while the ranking did not change after this point), and a normalization procedure (dividing each score by the sum of all square values) was applied after each step.</p><p>As other possibilities, we might consider the Page-Rank algorithm <ref type="bibr" coords="7,141.21,700.17,87.31,9.00" target="#b0">(Brin &amp; Page, 1998)</ref> or probabilistic argumentation systems <ref type="bibr" coords="7,165.94,712.17,55.41,9.00" target="#b12">(Picard, 1998)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>In order to evaluate the performance of a topic distillation IR scheme, we could use the precision achieved after retrieving 5 or 10 documents (under the labels "Prec@5" or "Prec@10") together with the number of relevant items retrieved (out of a total of 1,574 for the 49 queries included in the .GOV collection). Table <ref type="table" coords="7,364.09,477.17,10.05,9.00" target="#tab_10">11</ref> shows various statistics based on relevance assessments. The mean number of relevant items (or key resources) per request is 32.122. From considering the number of distinct roots (e.g., the first part of an URL, e.g., "trec.nist.gov"), we find that in mean, there were 9.4 different roots per query (for Query# 581, all relevant items coming from the root page "www.cancer.gov").</p><p>On the other hand, for Query# 558, we found 26 relevant pages extracted from the root page "www.whitehouse.gov" (and of these, 25 were from "www.whitehouse.gov/news/releases/2001/").</p><p>In our first set of experiments, we evaluated our extended Okapi IR model (see Section 2). By varying the value attached to the α parameter, we assigned more or less weight to each document representation. More precisely, when we set α = 0.0, we accounted for text delimited by the &lt;TITLE&gt; tag and all link anchor texts from outgoing links. In other words, we viewed the page as a good starting point for browsing (limited however to one-click distance). On the other hand, when α = 1.0, our search model was based on Web page content and from the various link anchor texts contained in all pages pointing to this particular document.</p><p>Table <ref type="table" coords="8,111.04,100.17,14.86,9.00" target="#tab_11">12a</ref> displays the various results produced by our IR model (without stemming) when varying the relative importance of each document representative. From this data, the best α value seemed to be around 0.9, based upon the precision achieved after 10 retrieved items (or 0.7 for 5 retrieved records). Thus, our first representation (content-oriented) seems to be more valuable for this specific IR task. Data in Table <ref type="table" coords="8,249.45,184.17,10.08,9.00" target="#tab_10">11</ref> seems to confirm these findings, given the various statistics on relevance assessments used in this task. For example, of the 1,574 pertinent items, 1,380 (or 87.7%) correspond to a filename while only 194 (or 12.3%) to subdirectories or path entries (URLs ending with a "/" or with "index.htm" or similar terms When we considered longer queries (built using the Title, Descriptive and Narrative logical sections), retrieval performance seemed to decrease relative to the precision achieved upon retrieving 5 or 10 items. Of course this value clearly increases for longer requests, as shown by the number of relevant and retrieved records (last column of Table <ref type="table" coords="8,413.82,72.17,12.50,9.00" target="#tab_12">12b</ref>).</p><p>Our UniNEdi1 run is based on short requests (Title only) while our UniNEdi3 run is based on the same processing but for TDN queries. For both runs, after retrieving content-based Web pages using our extended Okapi model, we applied spreading activation with λ = 0.35 for the first k = 200 top-ranked items. Following this stage, we pruned the retrieved URL (keeping only three URLs per site).</p><p>Using the SA method and based on the best run data shown in Table <ref type="table" coords="8,416.04,200.17,13.52,9.00" target="#tab_11">12a</ref>, we tried various parameter settings as depicted in Table <ref type="table" coords="8,467.81,212.17,8.57,9.00" target="#tab_0">13</ref>.</p><p>Clearly, the propagation factor λ must be smaller than 0.35, and the SA must be limited to the first 50 best-ranked items (instead of k = 200).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><p>Prec@5 Prec@10 rel. &amp; retr.</p><p>no stem, α = 0.9 Table <ref type="table" coords="8,366.48,425.17,8.56,9.00" target="#tab_0">13</ref>. Evaluation of various parameter settings for the spreading activation approach For the UniNEdi2 run, we applied the Kleinberg's HITS algorithm in order to define hub and authority pages (k = 200), and to form our ranked list we summed the hub and authority scores of each Web page, defining the new document score. Finally we pruned the retrieved URL.</p><p>Using the best run from Table <ref type="table" coords="8,472.15,529.17,15.17,9.00" target="#tab_11">12a</ref> as the starting point, we varied the number k of the top-ranked items included in the root set from the HITS method, as shown in Table <ref type="table" coords="8,396.50,565.17,8.94,9.00" target="#tab_0">14</ref>. The data in this table seems to clearly indicate that in this task the HITS algorithm does not perform well, whatever the value of k, whether we account for the hub score, the authority score or both.</p><p>Finally, Table <ref type="table" coords="8,401.70,629.17,10.31,9.00" target="#tab_4">15</ref> provides a summary description of our five official runs, all of which were created without a stemming procedure. Searching for good browsing starting points when using the SA or Kleinberg approaches clearly fails, or more precisely searching key resource does not means searching for browsing proper starting points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,71.50,417.17,212.15,204.00"><head>Table 1 .</head><label>1</label><figDesc>Our Arabic letter transliterations</figDesc><table coords="2,71.50,439.17,212.15,182.00"><row><cell></cell><cell>Remove from front</cell><cell>Remove from rear</cell></row><row><cell cols="2">stem2 fAl, XAl, bAl, wAl,</cell><cell>An, At,</cell></row><row><cell></cell><cell>Al,</cell><cell>hA,</cell></row><row><cell></cell><cell>w, Y</cell><cell>Yp, Yh, Yn,</cell></row><row><cell></cell><cell></cell><cell>wn,</cell></row><row><cell></cell><cell></cell><cell>Y, p, h</cell></row><row><cell>stem3</cell><cell>wAl, fAl, bAl,</cell><cell>km, tm,</cell></row><row><cell></cell><cell>Al, ll,</cell><cell>At, An,</cell></row><row><cell></cell><cell>bt, yt, lt, mt, tt, wt,</cell><cell>wn, wh,</cell></row><row><cell></cell><cell>st, nt</cell><cell>hn, hm,</cell></row><row><cell></cell><cell>bm, lm, wm, km, fm,</cell><cell>wA, tA, hA, nA,</cell></row><row><cell></cell><cell>wA, fA, lA, bA,</cell><cell>tk, ty, th</cell></row><row><cell></cell><cell>wy, ly, sy, fy,</cell><cell>yn, yh, yp</cell></row><row><cell></cell><cell>t, y, m, b, n, l, k,</cell><cell>p, h, y, t, k, A</cell></row><row><cell></cell><cell>w, A, f</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,72.50,632.17,222.97,9.00"><head>Table 2 .</head><label>2</label><figDesc>Main rules used by our two Arabic stemmers</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,71.50,291.17,474.77,410.00"><head>Table 4a .</head><label>4a</label><figDesc>Average precision of various IR models using the Arabic corpus (monolingual)</figDesc><table coords="3,71.50,291.17,474.77,410.00"><row><cell></cell><cell></cell><cell>Title</cell><cell></cell><cell></cell><cell>TD</cell><cell></cell><cell></cell><cell>TDN</cell><cell></cell></row><row><cell></cell><cell>Word</cell><cell>Word</cell><cell>3-grams</cell><cell>Word</cell><cell>Word</cell><cell>3-grams</cell><cell>Word</cell><cell>Word</cell><cell>3-grams</cell></row><row><cell>Model</cell><cell>stem2</cell><cell>stem3</cell><cell>no stem</cell><cell>stem2</cell><cell>stem3</cell><cell>no stem</cell><cell>stem2</cell><cell>stem3</cell><cell>no stem</cell></row><row><cell>Okapi-npn</cell><cell>27.41</cell><cell>26.09</cell><cell>28.77</cell><cell>30.51</cell><cell>29.22</cell><cell>31.45</cell><cell>32.87</cell><cell>30.40</cell><cell>29.95</cell></row><row><cell>Lnu-ltc</cell><cell>25.80</cell><cell>24.93</cell><cell>25.95</cell><cell>29.88</cell><cell>28.68</cell><cell>29.79</cell><cell>32.33</cell><cell>30.41</cell><cell>29.49</cell></row><row><cell>dtu-dtn</cell><cell>24.92</cell><cell>24.35</cell><cell>23.98</cell><cell>27.25</cell><cell>25.22</cell><cell>27.96</cell><cell>28.45</cell><cell>27.30</cell><cell>26.30</cell></row><row><cell>atn-ntc</cell><cell>22.71</cell><cell>21.30</cell><cell>22.76</cell><cell>24.44</cell><cell>22.42</cell><cell>24.00</cell><cell>25.98</cell><cell>25.47</cell><cell>22.44</cell></row><row><cell>ltn-ntc</cell><cell>24.19</cell><cell>22.94</cell><cell>22.65</cell><cell>26.45</cell><cell>24.77</cell><cell>24.50</cell><cell>27.94</cell><cell>26.60</cell><cell>21.58</cell></row><row><cell>lnc-ltc</cell><cell>20.66</cell><cell>19.55</cell><cell>20.46</cell><cell>24.77</cell><cell>23.38</cell><cell>25.10</cell><cell>29.58</cell><cell>27.05</cell><cell>27.39</cell></row><row><cell>ntc-ntc</cell><cell>20.27</cell><cell>19.09</cell><cell>19.64</cell><cell>23.03</cell><cell>21.46</cell><cell>23.92</cell><cell>25.38</cell><cell>23.39</cell><cell>23.26</cell></row><row><cell>ltc-ltc</cell><cell>18.41</cell><cell>17.90</cell><cell>19.73</cell><cell>21.33</cell><cell>20.30</cell><cell>23.95</cell><cell>26.25</cell><cell>24.43</cell><cell>25.40</cell></row><row><cell>nnn-nnn</cell><cell>12.65</cell><cell>12.11</cell><cell>8.22</cell><cell>13.84</cell><cell>13.21</cell><cell>6.31</cell><cell>14.84</cell><cell>13.60</cell><cell>5.10</cell></row><row><cell>bnn-bnn</cell><cell>12.47</cell><cell>11.64</cell><cell>11.29</cell><cell>10.86</cell><cell>9.92</cell><cell>5.90</cell><cell>8.54</cell><cell>7.00</cell><cell>1.62</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Average Precision</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Title</cell><cell></cell><cell></cell><cell>TD</cell><cell></cell><cell></cell><cell>TDN</cell><cell></cell></row><row><cell></cell><cell>Word</cell><cell>Word</cell><cell>3-grams</cell><cell>Word</cell><cell>Word</cell><cell>3-grams</cell><cell>Word</cell><cell>Word</cell><cell>3-grams</cell></row><row><cell cols="2">#doc/#term stem2</cell><cell>stem3</cell><cell>no stem</cell><cell>stem2</cell><cell>stem3</cell><cell>no stem</cell><cell>stem2</cell><cell>stem3</cell><cell>no stem</cell></row><row><cell>Okapi-npn</cell><cell>27.41</cell><cell>26.09</cell><cell>28.77</cell><cell>30.51</cell><cell>29.22</cell><cell>31.45</cell><cell>32.87</cell><cell>30.40</cell><cell>29.95</cell></row><row><cell>d=5 / t=10</cell><cell>31.51</cell><cell>31.13</cell><cell>32.30</cell><cell>33.62</cell><cell>32.46</cell><cell>33.56</cell><cell>35.21</cell><cell>33.06</cell><cell>32.66</cell></row><row><cell>d=5 / t=20</cell><cell>32.45</cell><cell>31.91</cell><cell>32.40</cell><cell>34.15</cell><cell>33.31</cell><cell>34.60</cell><cell>35.75</cell><cell>33.59</cell><cell>33.67</cell></row><row><cell>d=5 / t=30</cell><cell>32.98</cell><cell>31.85</cell><cell>32.49</cell><cell>34.23</cell><cell>33.24</cell><cell>35.30</cell><cell>36.25</cell><cell>33.86</cell><cell>33.96</cell></row><row><cell>d=5 / t=40</cell><cell>33.34</cell><cell>31.75</cell><cell>33.37</cell><cell>34.47</cell><cell>33.27</cell><cell>35.68</cell><cell>36.36</cell><cell>34.07</cell><cell>34.19</cell></row><row><cell>d=5 / t=50</cell><cell>33.26</cell><cell>31.64</cell><cell>33.35</cell><cell>34.45</cell><cell>33.20</cell><cell>35.78</cell><cell>36.47</cell><cell>34.12</cell><cell>34.07</cell></row><row><cell cols="2">d=5 /t=100 33.32</cell><cell>31.54</cell><cell>32.62</cell><cell>34.64</cell><cell>33.19</cell><cell>36.19</cell><cell>36.40</cell><cell>34.27</cell><cell>34.34</cell></row><row><cell cols="2">d=5 /t=150 33.07</cell><cell>31.32</cell><cell>32.23</cell><cell>34.48</cell><cell>33.22</cell><cell>36.24</cell><cell>36.39</cell><cell>33.87</cell><cell>34.17</cell></row><row><cell cols="2">d=10 /t=10 32.39</cell><cell>31.57</cell><cell>32.75</cell><cell>34.03</cell><cell>32.81</cell><cell>33.52</cell><cell>35.27</cell><cell>32.68</cell><cell>32.37</cell></row><row><cell cols="2">d=10 /t=20 33.35</cell><cell>32.41</cell><cell>34.44</cell><cell>34.78</cell><cell>33.53</cell><cell>34.72</cell><cell>36.01</cell><cell>33.69</cell><cell>32.69</cell></row><row><cell cols="2">d=10 /t=30 33.82</cell><cell>32.78</cell><cell>34.78</cell><cell>35.23</cell><cell>33.88</cell><cell>35.04</cell><cell>36.39</cell><cell>33.60</cell><cell>32.85</cell></row><row><cell cols="2">d=10 /t=40 34.13</cell><cell>32.97</cell><cell>34.71</cell><cell>35.52</cell><cell>34.08</cell><cell>35.39</cell><cell>36.46</cell><cell>33.49</cell><cell>33.03</cell></row><row><cell cols="2">d=10 /t=50 34.20</cell><cell>32.89</cell><cell>34.54</cell><cell>35.66</cell><cell>34.08</cell><cell>35.50</cell><cell>36.42</cell><cell>33.40</cell><cell>33.14</cell></row><row><cell cols="2">d=10/t=100 34.00</cell><cell>32.17</cell><cell>34.56</cell><cell>35.85</cell><cell>33.69</cell><cell>35.70</cell><cell>36.52</cell><cell>33.56</cell><cell>33.42</cell></row><row><cell cols="2">d=10/t=150 33.75</cell><cell>31.76</cell><cell>34.35</cell><cell>35.46</cell><cell>33.40</cell><cell>35.36</cell><cell>36.55</cell><cell>33.41</cell><cell>33.41</cell></row><row><cell cols="2">d=25 /t=10 32.06</cell><cell>31.33</cell><cell>31.91</cell><cell>33.75</cell><cell>32.21</cell><cell>32.72</cell><cell>34.99</cell><cell>32.53</cell><cell>31.61</cell></row><row><cell cols="2">d=25 /t=25 33.09</cell><cell>32.81</cell><cell>32.02</cell><cell>34.59</cell><cell>33.03</cell><cell>33.66</cell><cell>35.74</cell><cell>33.04</cell><cell>32.07</cell></row><row><cell cols="2">d=25 /t=50 33.78</cell><cell>32.96</cell><cell>32.55</cell><cell>35.24</cell><cell>33.21</cell><cell>34.01</cell><cell>36.01</cell><cell>33.54</cell><cell>32.47</cell></row><row><cell cols="2">d=25/t=100 33.88</cell><cell>32.90</cell><cell>32.57</cell><cell>35.42</cell><cell>33.09</cell><cell>34.11</cell><cell>36.07</cell><cell>33.45</cell><cell>32.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,103.50,712.17,415.02,9.00"><head>Table 4b .</head><label>4b</label><figDesc>Average precision using blind query expansion(Okapi model, Arabic corpus, monolingual)    </figDesc><table coords="4,71.50,60.17,477.74,137.00"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Average Precision</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Title</cell><cell>TD</cell><cell>TDN</cell><cell>Title</cell><cell>TD</cell><cell>TDN</cell><cell>Title</cell><cell>TD</cell><cell>TDN</cell></row><row><cell></cell><cell cols="9">no expand no expand no expand + expand + expand + expand + expand + expand + expand</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell>1 / 1 / 1</cell><cell>1 / 1 / 1</cell><cell cols="4">1 / 1 / 1 1 / 1 / 1.5 1 / 1 / 1.5 1 / 1 / 1.5</cell></row><row><cell>Best single</cell><cell>28.77</cell><cell>31.45</cell><cell>32.87</cell><cell>34.78</cell><cell>36.24</cell><cell>36.55</cell><cell>34.78</cell><cell>36.24</cell><cell>36.55</cell></row><row><cell cols="2">Round-robin 28.68</cell><cell>31.40</cell><cell>32.28</cell><cell>35.21</cell><cell>36.81</cell><cell>36.59</cell><cell>35.21</cell><cell>36.81</cell><cell>36.59</cell></row><row><cell>combRSVn</cell><cell>30.19</cell><cell>33.13</cell><cell>33.60</cell><cell>36.54</cell><cell>36.90</cell><cell>36.94</cell><cell>36.75</cell><cell>37.16</cell><cell>36.98</cell></row><row><cell>combMAX</cell><cell>30.79</cell><cell>33.58</cell><cell>31.78</cell><cell>35.25</cell><cell>36.56</cell><cell>37.12</cell><cell>34.44</cell><cell>36.57</cell><cell>34.85</cell></row><row><cell>combSUM</cell><cell>29.69</cell><cell>33.03</cell><cell>33.23</cell><cell>35.98</cell><cell>36.65</cell><cell>36.63</cell><cell>36.31</cell><cell>37.01</cell><cell>36.59</cell></row><row><cell cols="2">combRSV% 29.02</cell><cell>32.63</cell><cell>32.85</cell><cell>36.00</cell><cell>36.33</cell><cell>36.27</cell><cell>36.04</cell><cell>36.61</cell><cell>36.09</cell></row><row><cell>combNBZ</cell><cell>29.07</cell><cell>32.78</cell><cell>33.02</cell><cell>35.94</cell><cell>36.58</cell><cell>36.36</cell><cell>36.08</cell><cell>37.05</cell><cell>36.48</cell></row><row><cell>combANZ</cell><cell>27.26</cell><cell>28.34</cell><cell>27.67</cell><cell>33.83</cell><cell>32.78</cell><cell>33.67</cell><cell>27.17</cell><cell>22.54</cell><cell>23.88</cell></row><row><cell>combMIN</cell><cell>20.52</cell><cell>20.77</cell><cell>20.09</cell><cell>30.29</cell><cell>27.14</cell><cell>27.85</cell><cell>18.41</cell><cell>11.94</cell><cell>14.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,88.50,208.17,454.13,78.00"><head>Table 5 .</head><label>5</label><figDesc>Average precision of various data merging strategies(Arabic corpus, monolingual)    </figDesc><table coords="4,88.50,226.17,454.13,60.00"><row><cell>Run name</cell><cell>Query</cell><cell>Okapi stem3</cell><cell>Okapi stem2</cell><cell>Okapi 3-grams</cell><cell>Average precision</cell></row><row><cell>UniNE1</cell><cell>T-D</cell><cell>doc=10 / term=15</cell><cell>doc=10 / term=75</cell><cell>doc=10 / term=20</cell><cell>37.12</cell></row><row><cell>UniNE2</cell><cell>T</cell><cell>doc=10 / term=40</cell><cell>doc=5 / term=20</cell><cell>doc=25 / term=15</cell><cell>35.72</cell></row><row><cell>UniNE3</cell><cell>T-D-N</cell><cell>doc=5 / term=50</cell><cell>doc=10 / term=40</cell><cell>doc=10 / term=20</cell><cell>38.07</cell></row><row><cell>UniNE4</cell><cell>T-D</cell><cell>doc=10 / term=15</cell><cell>doc=10 / term=75</cell><cell>doc=10 / term=20</cell><cell>36.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,153.50,297.17,316.07,9.00"><head>Table 6 .</head><label>6</label><figDesc>Specifications and evaluation of our official monolingual Arabic runs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,325.50,499.17,226.99,199.76"><head>Table 7 .</head><label>7</label><figDesc>Relevance judgment statistics (named page searches, TREC-11) Our evaluation will be based on the mean reciprocal rank (MRR) of the first correct answer found by the system. Table7depicts statistics on the relevance assessments of this test-collection, clearly showing that we usually obtain one correct answer per topic. For each of the 150 queries we considered only the first 100 retrieved items. As seen in Table8, the best α value seems to be around 0.6, thus assigning a little more weight to internal representation.</figDesc><table coords="5,339.50,499.17,185.85,81.00"><row><cell>Number of queries</cell><cell>150</cell></row><row><cell>Number of relevant doc.</cell><cell>170</cell></row><row><cell>Mean rel. doc. / request</cell><cell>1.133</cell></row><row><cell>Standard deviation</cell><cell>0.378</cell></row><row><cell>Median</cell><cell>1</cell></row><row><cell>Maximum</cell><cell>3 (q#: 9, q#: 145)</cell></row><row><cell>Minimum</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,71.50,275.17,225.85,131.76"><head>Table 8 .</head><label>8</label><figDesc>IR model evaluation for various combinations of two document representatives (no stemming)</figDesc><table coords="6,76.73,307.17,213.26,99.76"><row><cell>Run name</cell><cell>MRR</cell><cell># in top 10</cell><cell># not found</cell></row><row><cell>α = 0.0</cell><cell cols="3">0.4991 99 (66.0%) 25 (16.67%)</cell></row><row><cell>α = 0.3</cell><cell cols="3">0.6163 118 (78.67%) 15 (10.0%)</cell></row><row><cell>α = 0.5</cell><cell cols="2">0.6506 120 (80.0%)</cell><cell>9 (6.0%)</cell></row><row><cell>α = 0.7</cell><cell cols="2">0.6735 123 (82.0%)</cell><cell>6 (4.0%)</cell></row><row><cell>α = 0.8</cell><cell cols="2">0.6710 122 (81.33%)</cell><cell>6 (4.0%)</cell></row><row><cell>α = 1.0</cell><cell cols="3">0.5771 116 (77.33%) 13 (8.67%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,71.50,415.17,225.85,21.00"><head>Table 9 .</head><label>9</label><figDesc>IR model evaluation for various combinations of our two document representatives (S-stemming)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="6,328.00,66.17,215.16,80.00"><head>Table 10 .</head><label>10</label><figDesc>Description of official named-page runs</figDesc><table coords="6,328.00,66.17,215.16,60.76"><row><cell>Run name</cell><cell>MRR</cell><cell>Description</cell></row><row><cell>UniNEnp1 UniNEnp2</cell><cell>0.636 0.616</cell><cell>No stemming, α = 0.3 S-stemming, α = 0.3</cell></row><row><cell>UniNEnp3 UniNEnp4</cell><cell>0.625 0.504</cell><cell>Reranking the first 10 items No stemming, α = 0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="7,325.50,171.17,225.91,299.00"><head>Table 11 .</head><label>11</label><figDesc>Relevance judgment statistics (topic distillation searching task, </figDesc><table coords="7,339.50,171.17,177.93,269.00"><row><cell>Number of queries</cell><cell>49</cell></row><row><cell>Number of relevant doc.</cell><cell>1,574</cell></row><row><cell>Mean rel. doc. / request</cell><cell>32.122</cell></row><row><cell>Standard deviation</cell><cell>37.33</cell></row><row><cell>Median</cell><cell>22</cell></row><row><cell>Maximum</cell><cell>188 (q#: 558)</cell></row><row><cell>Minimum</cell><cell>1 (q#: 588)</cell></row><row><cell cols="2">Number of distinct roots / query</cell></row><row><cell>Mean</cell><cell>9.429</cell></row><row><cell>Standard deviation</cell><cell>15.27</cell></row><row><cell>Median</cell><cell>13</cell></row><row><cell>Maximum</cell><cell>64 (q#: 596)</cell></row><row><cell>Minimum</cell><cell>1 (q#: 581)</cell></row><row><cell>URL length 1</cell><cell>31</cell></row><row><cell>length 2</cell><cell>194</cell></row><row><cell>length 3</cell><cell>536</cell></row><row><cell>length 4</cell><cell>402</cell></row><row><cell>length 5</cell><cell>263</cell></row><row><cell>length 6</cell><cell>110</cell></row><row><cell>length 7 and more</cell><cell>38</cell></row><row><cell># pertinent items file</cell><cell>1,380</cell></row><row><cell># pertinent items path</cell><cell>194</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="8,71.50,256.17,225.82,354.76"><head>Table 12a .</head><label>12a</label><figDesc>Evaluation of various document representatives combinations (no stemming, TD queries)</figDesc><table coords="8,71.50,256.17,225.75,354.76"><row><cell></cell><cell></cell><cell cols="2">). Moreover, URLs</cell></row><row><cell cols="4">of unitary length (or roots) correspond to only 31 (or</cell></row><row><cell cols="2">2%) relevant items.</cell><cell></cell><cell></cell></row><row><cell>Run name</cell><cell>Prec@5</cell><cell>Prec@10</cell><cell>rel. &amp; retr.</cell></row><row><cell>α = 0.0</cell><cell>15.92</cell><cell>13.06</cell><cell>457</cell></row><row><cell>α = 0.1</cell><cell>17.14</cell><cell>14.69</cell><cell>562</cell></row><row><cell>α = 0.2</cell><cell>18.37</cell><cell>15.92</cell><cell>626</cell></row><row><cell>α = 0.3</cell><cell>18.78</cell><cell>17.35</cell><cell>683</cell></row><row><cell>α = 0.4</cell><cell>20.82</cell><cell>17.14</cell><cell>793</cell></row><row><cell>α = 0.5</cell><cell>21.22</cell><cell>17.96</cell><cell>926</cell></row><row><cell>α = 0.6</cell><cell>22.04</cell><cell>19.39</cell><cell>982</cell></row><row><cell>α = 0.7</cell><cell>24.08</cell><cell>19.59</cell><cell>973</cell></row><row><cell>α = 0.8</cell><cell>22.86</cell><cell>21.43</cell><cell>991</cell></row><row><cell>α = 0.9</cell><cell>22.86</cell><cell>21.63</cell><cell>965</cell></row><row><cell>α = 1.0</cell><cell>23.67</cell><cell>18.37</cell><cell>919</cell></row><row><cell>Run name</cell><cell>Prec@5</cell><cell>Prec@10</cell><cell>rel. &amp; retr.</cell></row><row><cell>α = 0.0</cell><cell>11.84</cell><cell>9.39</cell><cell>635</cell></row><row><cell>α = 0.2</cell><cell>18.37</cell><cell>13.47</cell><cell>877</cell></row><row><cell>α = 0.4</cell><cell>21.63</cell><cell>16.12</cell><cell>1,217</cell></row><row><cell>α = 0.6</cell><cell>20.82</cell><cell>18.16</cell><cell>1,231</cell></row><row><cell>α = 0.8</cell><cell>22.04</cell><cell>18.98</cell><cell>1,159</cell></row><row><cell>α = 1.0</cell><cell>22.04</cell><cell>17.35</cell><cell>1,064</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="8,71.50,619.17,225.83,21.00"><head>Table 12b .</head><label>12b</label><figDesc>Evaluation of various document representatives combinations (no stemming, TDN queries)</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">S. Abdou</rs> for his help in understanding some aspects of the Arabic language and <rs type="person">C. Buckley</rs> from <rs type="affiliation">SabIR</rs> for allowing us the opportunity to use the SMART system. This research was supported by the <rs type="funder">SNSF (Swiss National Science Foundation</rs>) under grants <rs type="grantNumber">21-58'813.99</rs> and <rs type="grantNumber">21-66'742.01</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tQAxGRQ">
					<idno type="grant-number">21-58&apos;813.99</idno>
				</org>
				<org type="funding" xml:id="_kCQgw9E">
					<idno type="grant-number">21-66&apos;742.01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><p>Prec@5 Prec@10 rel. &amp; retr.</p><p>no stem, α = 0.9 Only the UniNEdi4 run needs any additional comments. This run is based on UniNEdi5 and after we obtained a ranked list, we computed and sorted the Web sites according to number of pages present in the top 50 best-ranked items. Following this step, we selected pages from those sites having the greatest number of matches between the query terms and the underlying URL texts (however, this selection and reranking procedure did not improve the retrieval effectiveness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 1. Weighting schemes</head><p>To assign an indexing weight w ij reflecting the importance of each single-term Tj in a document Di, we may use the formula shown in Table <ref type="table" coords="10,479.59,115.17,4.42,9.00">A</ref>.1, where document length (the number of indexing terms) for document Di is denoted by nt i , and n indicates the number of documents in the collection. For the Okapi weighting scheme, K represents the ratio between the length of document Di measured by l i (sum of tf ij ) and the collection's mean is noted by advl or more precisely</p><p>For the Arabic corpus, the constant advl is set at 300, the constant b at 0.55, the constant k 1 at 3. For both Web searching tasks, we set advl at 750, the constant b at 0.9, the constant k 1 at 1.2. For the Lnu scheme, the constant pivot was fixed at 125 and the constant slope at 0.1.</p><p>(1-slope) ⋅ pivot + slope ⋅ nt i </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,326.50,87.17,210.96,9.00;9,340.50,99.17,203.07,9.00;9,340.50,111.17,76.80,9.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,439.60,87.17,97.86,9.00;9,340.50,99.17,148.02,9.00">The anatomy of a largescale hypertextual Web search engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,495.08,99.17,48.49,9.00;9,340.50,111.17,29.31,9.00">Proceedings WWW</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,123.17,203.80,9.00;9,340.50,135.17,197.46,9.00;9,340.50,147.17,207.93,9.00;9,340.50,159.17,25.83,9.00" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<title level="m" coord="9,372.16,135.17,165.80,9.00;9,340.50,147.17,203.31,9.00">New retrieval approaches using SMART. Proceedings TREC-4, NIST Publication #500-236</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="25" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,171.17,210.88,9.00;9,340.50,183.17,206.09,9.00;9,340.50,195.17,166.23,9.00" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,340.50,183.17,201.99,9.00">Effective site finding using link anchor information</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,340.50,195.17,97.29,9.00">Proceedings ACM-SIGIR</title>
		<meeting>ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,207.17,213.93,9.00;9,340.50,219.17,197.78,9.00;9,340.50,231.17,178.91,9.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,462.63,207.17,77.80,9.00;9,340.50,219.17,140.61,9.00">Searching the Web by constrained spreading activation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,490.50,219.17,47.78,9.00;9,340.50,231.17,107.47,9.00">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="585" to="605" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,243.17,218.96,9.00;9,340.50,255.17,180.04,9.00;9,340.50,267.17,89.94,9.00" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,470.60,243.17,74.86,9.00;9,340.50,255.17,96.38,9.00">Term selection for searching printed Arabic</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,443.53,255.17,77.01,9.00;9,340.50,267.17,23.20,9.00">Proceedings ACM-SIGIR</title>
		<meeting>ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,279.17,220.86,9.00;9,340.50,291.17,196.95,9.00;9,340.50,303.17,208.98,9.00;9,340.50,315.17,154.93,9.00" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,435.99,291.17,101.46,9.00;9,340.50,303.17,108.32,9.00">TREC-10 experiments at Maryland: CLIR and video</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rautiainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,455.61,303.17,93.87,9.00;9,340.50,315.17,111.20,9.00">Proceedings TREC-10, NIST Publication #500-250</title>
		<meeting>TREC-10, NIST Publication #500-250</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="549" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,327.17,219.88,9.00;9,340.50,339.17,201.95,9.00;9,340.50,351.17,106.10,9.00" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,459.63,327.17,86.75,9.00;9,340.50,339.17,53.57,9.00">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,400.82,339.17,141.63,9.00;9,340.50,351.17,63.23,9.00">Proceedings TREC-2, NIST Publication #500-215</title>
		<meeting>TREC-2, NIST Publication #500-215</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,363.17,221.11,9.00;9,340.50,375.17,195.25,9.00;9,340.50,387.17,47.49,9.00" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,406.49,363.17,103.63,9.00">How effective is suffixing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,516.50,363.17,31.11,9.00;9,340.50,375.17,191.12,9.00">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,399.17,217.18,9.00;9,340.50,411.17,185.54,9.00;9,340.50,423.17,187.95,9.00" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,412.04,399.17,131.64,9.00;9,340.50,411.17,76.13,9.00">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,423.55,411.17,102.49,9.00;9,340.50,423.17,144.54,9.00">Proceedings ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="668" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,435.17,207.90,9.00;9,340.50,447.17,205.06,9.00;9,340.50,459.17,186.77,9.00" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,340.50,447.17,205.06,9.00;9,340.50,459.17,24.03,9.00">The importance of prior probabilities for entry page search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,371.04,459.17,97.29,9.00">Proceedings ACM-SIGIR</title>
		<meeting>ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,471.17,220.91,9.00;9,340.50,483.17,189.07,9.00;9,340.50,495.17,207.74,9.00;9,340.50,507.17,166.23,9.00" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,340.50,483.17,189.07,9.00;9,340.50,495.17,203.88,9.00">Improving stemming for Arabic information retrieval: Light stemming and co-occurrence analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,340.50,507.17,97.29,9.00">Proceedings ACM-SIGIR</title>
		<meeting>ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,519.17,220.95,9.00;9,340.50,531.17,199.96,9.00;9,340.50,543.17,200.90,9.00" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,482.02,519.17,65.43,9.00;9,340.50,531.17,142.72,9.00">Arabic information retrieval at UMass in TREC-10</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,491.04,531.17,49.42,9.00;9,340.50,543.17,156.87,9.00">Proceedings TREC-10, NIST Publication #500-250</title>
		<meeting>TREC-10, NIST Publication #500-250</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,555.17,211.97,9.00;9,340.50,567.17,208.61,9.00;9,340.50,579.17,193.95,9.00" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,398.27,555.17,140.20,9.00;9,340.50,567.17,208.61,9.00;9,340.50,579.17,17.29,9.00">Modeling and combining evidence provided by document relationships using PAS systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,364.73,579.17,125.63,9.00">Proceedings ACM-SIGIR&apos;1998</title>
		<meeting>ACM-SIGIR&apos;1998</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="182" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,591.17,202.92,9.00;9,340.50,603.17,197.05,9.00;9,340.50,615.17,118.03,9.00" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="9,464.36,591.17,65.07,9.00;9,340.50,603.17,173.34,9.00">Term proximity scoring for keyword-based retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rasolofo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<idno>ECIR-03</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Proceedings</note>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="9,326.50,627.17,213.85,9.00;9,340.50,639.17,200.25,9.00;9,340.50,651.17,206.91,9.00;9,340.50,663.17,17.50,9.00" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,340.50,639.17,194.64,9.00">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,340.50,651.17,158.49,9.00">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,326.50,675.17,206.97,9.00;9,340.50,687.17,207.49,9.00;9,340.50,699.17,206.89,9.00;9,340.50,711.17,17.50,9.00" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,398.36,675.17,135.11,9.00;9,340.50,687.17,73.36,9.00">Citation schemes in hypertext information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,430.50,687.17,117.49,9.00;9,340.50,699.17,25.81,9.00">Information retrieval and hypertext</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Agosti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="99" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.50,60.17,211.63,9.00;10,86.50,72.17,197.96,9.00;10,86.50,84.17,87.91,9.00" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,192.77,60.17,91.36,9.00;10,86.50,72.17,44.73,9.00">Retrieval effectiveness on the Web</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,139.50,72.17,144.96,9.00;10,86.50,84.17,17.20,9.00">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="543" to="569" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.50,96.17,192.92,9.00;10,86.50,108.17,201.91,9.00;10,86.50,120.17,197.94,9.00;10,86.50,132.17,128.96,9.00" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rasolofo</surname></persName>
		</author>
		<title level="m" coord="10,209.83,96.17,55.59,9.00;10,86.50,108.17,201.91,9.00;10,86.50,120.17,197.94,9.00;10,86.50,132.17,85.50,9.00">Report on the TREC-9 experiment: Link-based retrieval and distributed collections. Proceedings TREC-9, NIST Publication #500-249</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.50,144.17,221.95,9.00;10,86.50,156.17,197.14,9.00;10,86.50,168.17,184.92,9.00;10,86.50,180.17,196.97,9.00;10,86.50,192.17,198.86,9.00" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,148.55,144.17,145.90,9.00;10,86.50,156.17,182.34,9.00">Report on CLEF-2001 experiments: Effective combined query-translation approach</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,113.50,180.17,169.97,9.00;10,86.50,192.17,40.45,9.00">Cross-language information retrieval and evaluation</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Brachler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2002a. 2001</date>
			<biblScope unit="page" from="27" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.50,204.17,222.96,9.00;10,86.50,216.17,197.75,9.00;10,86.50,228.17,104.91,9.00" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<title level="m" coord="10,149.29,204.17,146.17,9.00;10,86.50,216.17,197.75,9.00;10,86.50,228.17,70.79,9.00">Report on CLEF-2002 experiments: Combining multiple sources of evidence. Proceedings CLEF-2002</title>
		<imprint>
			<date type="published" when="2002">2002b</date>
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.50,240.17,207.90,9.00;10,86.50,252.17,201.95,9.00;10,86.50,264.17,200.96,9.00;10,86.50,276.17,106.10,9.00" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,86.50,252.17,201.95,9.00;10,86.50,264.17,47.14,9.00">Retrieving Web pages using content, links, URLs and anchors</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,140.57,264.17,146.89,9.00;10,86.50,276.17,63.23,9.00">Proceedings TREC-10, NIST Publication #500-250</title>
		<meeting>TREC-10, NIST Publication #500-250</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="663" to="672" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
