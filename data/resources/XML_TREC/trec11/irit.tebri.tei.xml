<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.56,124.85,311.61,15.51">IRIT at TREC&apos;2002: Filtering Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.00,156.31,71.73,9.96"><forename type="first">M</forename><surname>Boughanem</surname></persName>
							<email>boughane@irit.f</email>
							<affiliation key="aff0">
								<orgName type="department">IRIT/SIG Campus Univ. Toulouse III 118</orgName>
								<address>
									<addrLine>Route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse Cedex 4</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.55,156.31,43.06,9.96"><forename type="first">H</forename><surname>Tebri</surname></persName>
							<email>tebri@irit.f</email>
							<affiliation key="aff0">
								<orgName type="department">IRIT/SIG Campus Univ. Toulouse III 118</orgName>
								<address>
									<addrLine>Route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse Cedex 4</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.64,156.31,44.13,9.96"><forename type="first">M</forename><surname>Tmar</surname></persName>
							<email>tmar@irit.f</email>
							<affiliation key="aff0">
								<orgName type="department">IRIT/SIG Campus Univ. Toulouse III 118</orgName>
								<address>
									<addrLine>Route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse Cedex 4</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.56,124.85,311.61,15.51">IRIT at TREC&apos;2002: Filtering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A7DA3E87E0597DA02905098405F49AB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The experiments we undertaken this year for TREC2002 Filtering track, are focussed on threshold calibration. We proposed a new approach to calibrate the dissemination threshold in an adaptive information filtering. It consists of optimizing a utility function represented by a linearized form of the probability distributions of the scores of the relevant and the non-relevant documents already filtered. The profiles are learned using the same method used last year. It is based on a reinforcement algorithm. We submitted results on three tasks: adaptive, batch and routing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Information representation and filtering</head><p>Our adaptive filtering model is inspired from the connectionist model Mercure <ref type="bibr" coords="1,399.22,369.19,9.98,9.96" target="#b0">[1]</ref>. The profile and the document are represented by a set of weighted terms. The filtering process consists of computing a relevance value RSV Retrieval Status Value. The document is delivered only if the RSV is greater than the dissemination threshold. A learning process is then carried out by modifying the profile and the dissemination threshold to be more efficient in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">System initialization</head><p>The user profile is represented by a set of terms :</p><formula xml:id="formula_0" coords="1,91.20,480.99,76.07,14.77">p (0) = tp 1 , w (0) 1</formula><p>, tp 2 , w (0) 2 . . . tp n , w (0) n <ref type="bibr" coords="1,529.80,483.91,12.70,9.96" target="#b0">(1)</ref> where tp i is a term and w (0) i is its weight in the initial profile (at t = 0), in the rest of this paper, the termprofile weight is noted w (t) i where t represents the instant when the system receives a document. Initially, the term-profile weight is computed as follows:</p><formula xml:id="formula_1" coords="1,91.20,551.21,451.30,23.56">w (0) i = tf p i max j (tf p j )<label>(2)</label></formula><p>Where tf p i is the term frequency of tp i in the profile. The formula seems to be abusively simplistic, but at the beginning of the filtering process, no information is known but a set of terms and their occurrence frequencies in the initial user profile. However, this weight will be adjusted by learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Filtering incoming documents</head><p>Each incoming document at time t is indexed, to build a list of stemmed terms (Porter <ref type="bibr" coords="1,437.25,651.55,10.29,9.96" target="#b2">[3]</ref>), the terms belonging to a stop-list are removed. Each term is then weighted according to the following formula:</p><formula xml:id="formula_2" coords="1,91.20,692.43,451.30,35.91">d (t) i = tf (t) i h 3 + h 4 * dl (t) ∆l (t) + tf (t) i * log N (t) n (t) i + 1<label>(3)</label></formula><p>Where tf (t) i : term frequency t i in the document d (t) , h 3 , h 4 : constant parameters, for the experiments h 3 = 0.2 and h 4 = 0.7, dl (t) : document length d (t) (number of index terms), ∆l (t) : average document length, N (t) :number of incoming documents until time t, n (t) i : number of incoming documents containing the term t i . This weighting formula is a form of Okapi <ref type="bibr" coords="2,261.69,139.75,10.55,9.96" target="#b4">[5]</ref> term-query weight used in the information retrieval system Mercure.</p><formula xml:id="formula_3" coords="2,66.36,160.71,40.31,13.00">N (t) , n<label>(t)</label></formula><p>i and ∆l (t) are collection based parameters that are computed on the cumulative documents filtered at time t. They can not be known while the document stream does not stop sending documents, the used values are recomputed for each incoming document. A relevance value noted rsv d (t) , p (t) is then computed corresponding to the document and the profile, as follows:</p><formula xml:id="formula_4" coords="2,91.20,225.51,451.30,25.09">rsv d (t) , p (t) = ti∈d (t) ,tpj ∈p (t) and ti=tpj d (t) i * w (t) j (4)</formula><p>The binary decision rule used for document filtering is the following: if rsv d (t) , p (t) ≥ threshold (t) accept the document otherwise reject the document The threshold value is initially fixed to a low value, 0 in our experiment. It is modified while filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Profile adaptive learning</head><p>The learning process adopted in our case is incremental. It is processed at each filtered document judged as relevant by the user. The basic idea of our learning process is based on the relevance reinforcement process. When a document is judged as relevant, it is necessary to be able to find a new representation of the profile which makes it possible to find the document with a strong score. In other words, one will be brought to improve the profile such as rsv d (t) , p (t) = β where β is the desired score rsv. This technique was used in TREC-10, we do not give more details, the reader can be refer to TREC-10 <ref type="bibr" coords="2,399.65,407.83,9.98,9.96" target="#b0">[1]</ref>. The problem is assimilated to a linear equation resolution. Each solution of this system is a set of weights affected to the document terms, knowing that there is an infinite number of solutions, we add a constraint to obtain only one solution. The system to resolve is given by :</p><formula xml:id="formula_5" coords="2,91.20,459.15,451.30,54.99">         ti∈d (t) tpj ∈p (t) ti=tpj d (t) i w (t) j = β w (t) i f d (t) i ,r (t) i ,s (t) i = w (t) j f d (t) j ,r (t) j ,s (t) j ∀ (t i , t j ) ∈ d (t) 2 (5)</formula><p>The solution of the system 5 is a set of "provisional" weights, let n be the number of different terms in the processed document at time t, f</p><formula xml:id="formula_6" coords="2,211.32,532.11,297.83,19.95">(t) i = f d (t) i , r (t) i , s (t) i , R (t) , S (t) ∀i ∈ {1 . . . n} where r (t) i (resp. s (t)</formula><p>i ) is the number of relevant (resp. non-relevant) documents containing the term t i until the time t. For each term appearing in the document, the provisional weight solution of the system 5 is the following: ∀i, pw</p><formula xml:id="formula_7" coords="2,120.12,578.43,422.38,40.71">(t) i = βf d (t) i , r (t) i , s (t) i j f d (t) j , r (t) j , s (t) j * d (t) j (6)</formula><p>The function f is proportional to the term importance. The function f used for the experiments is the following:</p><formula xml:id="formula_8" coords="2,66.36,648.80,476.14,61.19">∀i, f d (t) i , r (t) i , s (t) i = d i * log r (t) i +0.5 R (t) -r (t) i +0.5 s (t) i +0.5 S (t) -s (t) i +0.5 (7) R (t) (resp. S (t)</formula><p>) is the total number of relevant (resp. non-relevant) documents until the time t. The "provisional" weight pw (t) i contributes in learning the term-profile weight corresponding to the term t i , we use the following gradient propagation formula:</p><formula xml:id="formula_9" coords="2,91.20,742.35,451.30,14.57">w (t+1) i = w (t) i + log 1 + pw (t) i (8)</formula><p>We add 1 to pw (t) i to avoid adding negative value to the term-profile weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Threshold calibration</head><p>To find the best threshold, the system should follow the document score evolution and regulate the threshold in order to select as maximum as possible the relevant documents and reject as maximum as possible the nonrelevant documents. The threshold regulation can be made by maximizing a utility function 1 . According to the sampling theory, the behavior of a random sample is the same of all the population, so the threshold allowing to maximize the utility function in a random sample of documents allows to maximize the same utility function in all the documents of the stream. The approach we propose for threshold calibration consists of estimating the discrete probabilities of the scores of the relevant and the non-relevant documents already filtered and then using a kind of probability plotting method to build a linearized probability density distribution. The threshold that maximizes a utility function, represented by both distributions, is then selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.1">Score distribution modelling</head><p>The probability that a random document has a particular score is equal to the number of documents having the same score divided by the total number of documents:</p><formula xml:id="formula_10" coords="3,91.20,339.81,451.30,30.82">p(X = score) = |{d|rsv(d, p) = score}| |{d}|<label>(9)</label></formula><p>As score values are very distinct, they tend to be equiprobable (|{d|rsv(d, p) = score}| = 1 or 0). Indeed, it is very difficult to find two or many documents having exactly the same score. Consequently the score probability distribution tends to be uniform. Instead of computing the probability that a document has a score, we compute the probability that the document score belongs to an interval 2 . We define a set of intervals enough reduced so that the document scores belonging to the same interval are really close. We define n adjacent intervals I 1 , I 2 . . . I n having the same ray:</p><formula xml:id="formula_11" coords="3,91.20,458.23,451.41,10.33">I i = [score i-1 , score i ]<label>(10)</label></formula><p>Where : score 0 = min d rsv(d, p) and score n = max d rsv(d, p).</p><p>The number of intervals is proportional to the sample size, indeed the great is the number of documents, the great is the definition field of the document scores. We define n as the half of the total number of documents: n = |{d}| 2 . The probability that a document score belongs to an interval is given by:</p><formula xml:id="formula_12" coords="3,91.20,539.13,451.41,30.82">p(score i &lt; X &lt; score i+1 ) = |{d|score(d, p) ∈]score i , score i+1 [}| |{d}| (11)</formula><p>The representation of the probability distribution based on formula 11 corresponding to documents scores is poissonnian. There is many methods to estimate the probability law parameters followed by the document scores: the parametric regression and the maximum likelihood estimation method <ref type="bibr" coords="3,429.07,592.15,9.89,9.96" target="#b3">[4]</ref>. In both cases, the law must be assumed to be known in advance, these methods allow to estimate the parameters of this function. But, in an experimental context many limits of these methods can be noted. In deed, even though the assumption that the scores follow a known distribution density could be acceptable, but it strongly depends on the experimental conditions such as the filtering and the learning approaches and the size (number of documents) of the sample used for deriving the parameters of the distributions, the sample size must be enough important to obtain non-skewed estimations. To resolve these problems we propose that instead of assuming that the distributions are known, they are built by estimating the discrete probabilities of the scores of the relevant and the non-relevant documents delivered at a certain time and then by linearizing these probabilities to obtain the corresponding probability density distributions. A utility function, to be optimized, is then represented using these distributions. The best threshold is the threshold that maximizes this utility.</p><p>1 a function used to evaluate the filtering systems performance 2 There is a method to estimate the parameters of a probability law allowing to define a more or less precise interval containing this parameter called confidence interval rather than a value which will be less probably equal to this parameter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.2">Probability distribution linearization</head><p>Based on the regression theory, the representative curve of any function can be linearized in order to facilitate many tasks (computing surfaces, searching an extremum . . .). The linearization consists of considering the domain of the function and divide it into a set of intervals such as the representative curve of the restriction of that function in each interval can be assimilated to a linear curve.</p><p>We use this technique to linearize the representative curve of the probability density distribution of the scores. We assume that this function exists and we try to linearize it. As we do not know this function, we propose to linearize it using the corresponding discrete probabilities. The first stage of this process is to identify a set of linear intervals. We define a linear interval by two scores [score x ..score y ] where score x &lt; score y and all the points formed by (s i , p i ) fit a straight line, s i is a score in that interval and p i is the discrete probability of s i computed according to Formula 11. The linearity of a set of points is measured using the least squares method <ref type="bibr" coords="4,102.94,265.39,9.98,9.96" target="#b3">[4]</ref>. The least squares method requires that a straight line be fitted to a set of points such that the sum of the squares of the distance of the points to the fitted line is minimized. In our work the detection of linear interval is measured incrementally by considering all the points (s i , p i ), ordered in increasing order of the scores. It consists of adding a point to a given set of points representing a straight line and computing an error which measures the standard deviation between that "new" set and a linear curve. If this error is below than a "linearity threshold", the considered point is definitely added to that set, the next point is then considered. Otherwise this point is removed from the set, and we continue the search of a new linear interval, and so on until the last point of the distribution. The following algorithm is applied:</p><p>1. c = 1 (c is the index of the classes of the points with scores within a linear interval) 2. P = ∅, 3. threshold error = 0.0001, 4. &lt; M + 1 : the total number of the points of the form (s i ,p i ). /*We consider that the points are ranked in increasing order of their scores. i = 0 in the first score.</p><p>5. for i ∈ {0 . . . M }, (a) P ← P ∪ {i}, (b) determine the equation of the line D c : y(x) = a + bx based on the linear regression for all points (s j , p j )∀j ∈ P , (c) compute the standard deviation error between the points (s j , p j )∀j ∈ P and the line D c : </p><formula xml:id="formula_13" coords="4,94.92,542.79,447.69,97.73">E = j∈P d 2 ((s j , p j ), D c ) (12) d 2 ((s j , p j ), D c ) = ( a + b.s j -p j √ a 2 + 1 ) 2 (13) (d) if E &gt; threshold error, i. /*a class of points is formed. C c = (d c , f c ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(e) end if (f ) end for</head><p>A transformation is necessary at this stage such that all lines form a continuous representation: </p><formula xml:id="formula_14" coords="5,116.16,186.11,426.45,14.73">α c = ac+bc.fc-ac+1+bc+1.dc+1 fc-dc+1<label>(14)</label></formula><formula xml:id="formula_15" coords="5,116.88,205.07,425.73,19.39">β c = a c + b c .f c -ac+bc.fc-ac+1+bc+1.dc+1 fc-dc+1 f c<label>(15)</label></formula><p>2. Normalize the coefficients a c , b c , α c and β c such that:</p><formula xml:id="formula_16" coords="5,121.68,257.03,420.89,26.49">scoreM score0 f (x)dx = 1 (16)</formula><p>The equation 16 is the fundamental property of the probability density functions. As scoreM score0 f (x)dx represents the surface formed by the graphical representation of f and X-coordinate axis, the coefficients a c , b c , α c and β c are divided by this surface. This surface becomes unit. This surface is computed as the sum of the surfaces formed by all the linear intervals and the X-coordinate axis, let cl be the number of linear classes, so:</p><formula xml:id="formula_17" coords="5,96.72,363.47,256.31,49.51">scoreM score0 f (x)dx = 1 2 ( cl c=1 (f c -d c )(2a c + b c .(f c + d c )) + cl-1 c=1 (f c -d c+1 )(2α c + β c (f c + d c+1 )))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.3">Threshold optimization</head><p>The proposed method is based on utility maximization. Generally the utility function is done by :</p><formula xml:id="formula_18" coords="5,91.20,459.93,451.41,17.26">F = aR + -bS +<label>(17)</label></formula><p>Where R + (resp. S + ) is the number of relevant (resp. non-relevant) selected documents.</p><p>Our goal is to determine a threshold allowing to maximize the theoretical value of F :</p><formula xml:id="formula_19" coords="5,91.20,510.03,451.41,17.33">threshold * = arg max threshold F<label>(18)</label></formula><p>Where a, b : positive constants, R + : number of relevant selected documents, S + : number of non-relevant selected documents. R + and S + are both inversely proportional according to the threshold, p(score &gt; threshold|r) (resp. p(score &gt; threshold|s)) represents the probability that a document is selected when it is relevant (resp. non-relevant). It represents the surface formed by the curve of function f corresponding to the relevant (resp. non-relevant) documents and the X-coordinate axis.</p><formula xml:id="formula_20" coords="5,91.20,581.01,451.27,17.26">R + = p(r|score &gt; threshold) * R<label>(19)</label></formula><p>However, p(r) = R N (resp. p(s) = S N ) is the probability that a document is relevant (resp. non-relevant). Utility done by equation 18 is equivalent to:</p><formula xml:id="formula_21" coords="6,91.20,159.09,451.27,32.14">F = p(score &gt; threshold) * N * (a * p(score &gt; threshold|r) +b * p(score &gt; threshold|s))<label>(23)</label></formula><p>The retained threshold value allows to maximize F .</p><p>2 Experiments and results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Results of adaptive filtering task</head><p>The pre-test documents were used to learn the profile and to set the initial threshold. Then the filtering process is processed on the test data. At each selected the relevant document, the profile is learned and the new threshold is also computed. The algorithm that has been used for this experiment is the following:</p><p>• if a document d t is selected and is judged as relevant,</p><p>• learn the profile,</p><p>• re-estimate the scores of all delivered relevant documents and of a sample of 1000 non relevant documents extracted from the training data that have been used in batch,</p><p>• build the linearized probability distributions of these samples,</p><p>• Compute the new threshold that maximizes the utility function T 11U = 2R + -S + . This threshold is measured by varying the threshold value from the smallest score of the relevant documents to the greatest score of the non-relevant documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Batch and Routing Experiments</head><p>In batch and routing tasks the profile and the threshold were learned from the training collection. The learned profile and threshold were applied to the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Batch filtering</head><p>We built a sample of 1082 documents, which corresponds to the documents of all the profiles which have been labelled as relevant in the training dataset. This sample is then used to learn the profile and the dissemination threshold. We only consider the 40 top terms. The learned profiles and thresholds were applied to the test database. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Routing track</head><p>We experiment routing using a similar method then the batch filtering track. The new representing profile obtained from the sample documents is selected as routing profile, and applied on the test documents. The final output is the top 1000 ranked documents for each topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>We described in this paper a learning and threshold updating method for information filtering. Adaptive learning is based on equation system resolution under constraints, a gradient propagation formula uses the system solution to improve the user profile representation. The threshold updating is done independently from learning, it controls perfectly the random variation of rsv values affected to incoming documents. We have presented our experiments for TREC2002 who are focused on the Filtering (adaptive, batch and routing) tracks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,187.20,630.53,21.60,10.00;4,209.28,630.19,47.04,10.33;4,259.56,630.19,35.62,9.96;4,295.20,634.19,3.24,6.33;4,299.40,630.19,18.84,10.33;4,321.48,630.19,37.52,9.96;4,359.04,634.19,3.24,6.33;4,363.24,630.19,3.84,9.96;4,370.43,629.97,19.67,17.26;4,392.86,630.19,23.17,10.33;4,419.88,630.19,27.00,10.33;4,450.72,630.19,91.58,9.96;4,131.76,642.07,130.32,10.33;4,264.24,642.07,256.30,10.33;4,520.80,642.41,16.92,10.00;4,538.68,642.07,3.84,9.96;4,131.76,654.07,32.24,9.96;4,167.36,653.85,6.60,17.26;4,176.72,654.41,6.36,9.18;4,184.51,653.85,19.30,17.26;4,117.60,667.99,20.51,9.96;4,142.31,667.77,29.25,17.26;4,175.04,667.99,72.88,9.96;4,114.60,681.91,21.47,9.96;4,138.83,681.69,9.95,17.26;4,151.54,681.91,24.57,9.96"><head></head><label></label><figDesc>a c , b c ) where, d c = min(s j ), f c = max(s j ) ∀j ∈ P , a c and b c are the coefficients of the equation of the line y = a c + b c x derived using the linear regression of all the points (s j , p j ) where j ∈ P |{i}, ii. P ← {i}, /* re-initialize P iii. c ← c + 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,78.12,127.39,463.81,9.96;5,91.20,139.39,299.28,10.33;5,395.76,139.39,41.27,10.33;5,437.64,139.39,36.96,10.33;5,479.88,139.39,39.35,10.33;5,524.52,139.39,17.98,9.96;5,91.20,151.27,148.20,10.33;5,242.64,151.27,227.07,10.33;5,469.92,151.61,17.40,10.00;5,490.56,151.27,27.48,10.33;5,518.40,151.27,24.10,9.96;5,91.20,163.27,8.99,9.96;5,100.20,163.61,37.67,10.00;5,140.52,163.27,49.91,10.33;5,190.92,163.27,22.66,9.96"><head>1 .</head><label>1</label><figDesc>Transform the representation into a continuous one by relying the extremities of two adjacent classes. This liaison is done as follows: for two adjacent linear classes C c and C c+1 , rely f c and d c+1 with a line having as equation y = α c + β c x. This line should pass through the points (f c , a c + b c f c ) and (d c+1 , a c+1 + b c+1 .d c+1 ), so:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,92.64,596.11,139.31,10.65;5,234.23,595.89,5.03,17.26;5,241.43,596.45,6.12,9.18;5,524.74,596.11,17.73,9.96;5,81.24,618.07,379.62,9.96;5,66.36,629.95,210.29,9.96;5,91.20,667.15,31.91,10.65;5,134.28,660.77,98.88,9.18;5,235.56,660.21,5.03,17.26;5,242.76,660.77,87.61,9.18;5,223.68,674.33,17.38,9.18;5,333.96,666.93,5.03,17.26;5,341.15,667.49,7.55,9.18;5,524.86,667.15,17.73,9.96;5,92.64,694.63,30.47,10.65;5,134.28,688.25,98.88,9.18;5,235.44,687.69,5.03,17.26;5,242.63,688.25,87.62,9.18;5,223.68,701.81,17.37,9.18;5,333.84,694.41,5.03,17.26;5,341.03,694.97,6.12,9.18;5,524.87,694.63,17.73,9.96"><head>S</head><label></label><figDesc>+ = p(s|score &gt; threshold) * S (20) R and S represent the total number of relevant and non-relevant documents examined. Based on Bayes transformation rule, we obtain : R + = p(score &gt; threshold|r) * p(score &gt; threshold p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,66.36,448.63,366.38,165.72"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="6,101.97,448.63,330.78,165.72"><row><cell cols="3">lists the comparative adaptive filtering results for all topics.</cell><cell></cell><cell></cell></row><row><cell cols="4">TREC adaptive filtering for topics 101-150</cell><cell></cell></row><row><cell>Evaluation T 11U</cell><cell cols="4">= max ≥ median &lt; median Avg 2 31 19 0.386</cell></row><row><cell>T 11F</cell><cell>1</cell><cell>28</cell><cell>22</cell><cell>0.387</cell></row><row><cell>Set precision</cell><cell>0</cell><cell>24</cell><cell>26</cell><cell>0.261</cell></row><row><cell>Set recall</cell><cell>0</cell><cell>30</cell><cell>20</cell><cell>0.409</cell></row><row><cell cols="4">TREC adaptive filtering for topics 151-200</cell><cell></cell></row><row><cell>T 11U</cell><cell>1</cell><cell>43</cell><cell>7</cell><cell>0.282</cell></row><row><cell>T 11F</cell><cell>3</cell><cell>44</cell><cell>6</cell><cell>0.054</cell></row><row><cell>Set precision</cell><cell>3</cell><cell>44</cell><cell>6</cell><cell>0.092</cell></row><row><cell>Set recall</cell><cell>0</cell><cell>41</cell><cell>9</cell><cell>0.031</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,200.40,630.31,204.65,9.96"><head>Table 1 :</head><label>1</label><figDesc>Comparative adaptive filtering results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,66.36,193.63,366.38,174.36"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="7,101.97,193.63,330.77,174.36"><row><cell cols="3">lists the comparative batch results for all topics.</cell><cell></cell><cell></cell></row><row><cell cols="4">TREC batch filtering of topics 101-150</cell><cell></cell></row><row><cell>Evaluation T 11U</cell><cell cols="4">= max ≥ median &lt; median Avg 15 47 3 0.485</cell></row><row><cell>T 11F</cell><cell>10</cell><cell>46</cell><cell>4</cell><cell>0.454</cell></row><row><cell>Set precision</cell><cell>9</cell><cell>47</cell><cell>3</cell><cell>0.661</cell></row><row><cell>Set recall</cell><cell>0</cell><cell>33</cell><cell>17</cell><cell>0.321</cell></row><row><cell cols="4">TREC batch filtering of topics 151-200</cell><cell></cell></row><row><cell>Evaluation T 11U</cell><cell cols="4">= max ≥ median &lt; median Avg 8 31 19 0.236</cell></row><row><cell>T 11F</cell><cell>12</cell><cell>46</cell><cell>4</cell><cell>0.090</cell></row><row><cell>Set precision</cell><cell>21</cell><cell>47</cell><cell>3</cell><cell>0.288</cell></row><row><cell>Set recall</cell><cell>0</cell><cell>36</cell><cell>14</cell><cell>0.044</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,206.88,383.95,191.71,9.96"><head>Table 2 :</head><label>2</label><figDesc>Comparative batch filtering results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,66.36,484.03,361.59,110.28"><head>Table 3</head><label>3</label><figDesc>shows the routing results at average uninterpolated precision for all topics.</figDesc><table coords="7,211.44,515.35,187.48,78.96"><row><cell>TREC Routing for topics 101-150</cell></row><row><cell>= max ≥ median &lt; median AvgP 13 45 5 0.369</cell></row><row><cell>TREC Routing for topics 151-200</cell></row><row><cell>= max ≥ median &lt; median AvgP 11 42 8 0.004</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,221.76,610.27,161.97,9.96"><head>Table 3 :</head><label>3</label><figDesc>Comparative routing results</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,86.75,149.23,455.24,9.96;8,86.76,161.23,245.57,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,292.30,149.23,249.69,9.96;8,86.76,161.23,76.12,9.96">Mercure and MercureFiltre applied for Web and Filtering tasks at TREC-10</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Boughanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chrisment</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,172.35,161.23,124.71,9.96">Proceedings of TREC-10</title>
		<meeting>TREC-10</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,86.76,181.15,455.20,9.96;8,86.76,193.03,455.48,9.96;8,86.76,205.03,48.83,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,240.51,181.15,301.45,9.96;8,86.76,193.03,44.16,9.96">Incremental profile adaptive filtering: profile learning and threshold calibration</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Boughanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,139.26,193.03,341.95,9.96">Proceedings of 17th ACM Symposium on Applied Computing (SAC)</title>
		<meeting>17th ACM Symposium on Applied Computing (SAC)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="640" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,86.75,224.95,399.09,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,153.19,224.95,140.11,9.96">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mostafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,301.37,224.95,42.60,9.96">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,86.76,244.87,358.52,9.96" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Saporta</surname></persName>
		</author>
		<title level="m" coord="8,149.97,244.87,196.61,9.96">Probabilits, analyse des donnes et statistiques</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="report_type">Technip</note>
</biblStruct>

<biblStruct coords="8,86.76,264.79,455.23,9.96;8,86.76,276.79,455.50,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,474.04,264.79,67.95,9.96;8,86.76,276.79,293.67,9.96">Okapi/Keenbow at TREC-6 automatic and ad hoc, VLC, routing, filtering and QSDR</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Boughanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,389.60,276.79,105.51,9.96">Proceedings of TREC</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
