<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.25,98.27,410.32,13.18;1,103.50,114.02,387.05,13.18">Coupling Named Entity Recognition, Vector-Space Model and Knowledge Bases for TREC-11 Question Answering Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,131.25,149.80,41.35,10.80"><forename type="first">P</forename><surname>Bellot</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique d&apos;Avignon (LIA)</orgName>
								<address>
									<addrLine>339 ch. des Meinajaries</addrLine>
									<postBox>BP 1228</postBox>
									<postCode>F-84911</postCode>
									<settlement>Avignon Cedex 9 (</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,189.03,149.80,50.20,10.80"><forename type="first">E</forename><surname>Crestan</surname></persName>
							<email>eric.crestan@lia.univ-avignon.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique d&apos;Avignon (LIA)</orgName>
								<address>
									<addrLine>339 ch. des Meinajaries</addrLine>
									<postBox>BP 1228</postBox>
									<postCode>F-84911</postCode>
									<settlement>Avignon Cedex 9 (</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.72,149.80,53.06,10.80"><forename type="first">M</forename><surname>El-Bèze</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique d&apos;Avignon (LIA)</orgName>
								<address>
									<addrLine>339 ch. des Meinajaries</addrLine>
									<postBox>BP 1228</postBox>
									<postCode>F-84911</postCode>
									<settlement>Avignon Cedex 9 (</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.67,149.80,46.67,10.80"><forename type="first">L</forename><surname>Gillard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique d&apos;Avignon (LIA)</orgName>
								<address>
									<addrLine>339 ch. des Meinajaries</addrLine>
									<postBox>BP 1228</postBox>
									<postCode>F-84911</postCode>
									<settlement>Avignon Cedex 9 (</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,393.78,149.80,59.93,10.80"><forename type="first">C</forename><surname>De Loupy</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique d&apos;Avignon (LIA)</orgName>
								<address>
									<addrLine>339 ch. des Meinajaries</addrLine>
									<postBox>BP 1228</postBox>
									<postCode>F-84911</postCode>
									<settlement>Avignon Cedex 9 (</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.25,98.27,410.32,13.18;1,103.50,114.02,387.05,13.18">Coupling Named Entity Recognition, Vector-Space Model and Knowledge Bases for TREC-11 Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D448900597229156F2181E5946B249CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Named Entity Recognition</term>
					<term>Vector-Space Model</term>
					<term>Knowledge Bases</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a question-answering system combining Named Entity Recognition, Vector-Space Model and Knowledge Bases to validate answers candidates. Applying this hybrid approach, for our first participation in the TREC Q&amp;A.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Our approach combines a Named Entity Recognition System developed at Sinequa 1 and an answer retrieval system based on Vector Space model that uses some Knowledge Bases developed at the Laboratoire d'Informatique d'Avignon 2 .</p><p>First, the Named Entity Recognition system is briefly described, including specific features (section 2). Then, a summarized description of the SIAC (Segmentation et Indexation Automatique de Corpus) information retrieval system is given (section 3). For the purpose of Question Analysis, several Question Taggings have been employed, they are exposed in section 4. The approach using Knowledge Bases is then depicted (section 5), with a summary of its coverage (section 5.3). Section 6 is devoted to the Question Ordering problem. Finally, we present several experiments in the frame of TREC-11 (section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Named Entities</head><p>Detection of Named Entities (NE) is one of the key elements in the Question Answering task. In the past few years, there was a growing interest in NE analysis. Most current techniques for NE recognition are based on handcrafted finite state patterns <ref type="bibr" coords="1,192.79,661.34,93.06,8.77" target="#b0">[Appelt et al., 1995;</ref><ref type="bibr" coords="1,48.75,673.34,73.23,8.77">Weischedel, 1995]</ref>, on Hidden Markov Model <ref type="bibr" coords="1,235.06,673.34,50.88,8.77;1,48.75,684.59,23.45,8.77">[Bikel et al., 1999]</ref> or on Maximum entropy approach <ref type="bibr" coords="1,239.25,684.59,47.21,8.77;1,48.75,695.84,22.55,8.77" target="#b0">[Borthwick, 1999]</ref> 1 Sinequa S.A.S.: http://www.sinequa.com 2 LIA: http://www.lia.univ-avignon.fr/</p><p>The NE analysis approach used in this task is based on a cascade of transducers. Some special features have been added to enhance the NE recognition. Among those features, a normalization function for normalizing proper noun occurrences in a text frame has been engineered, as well as a trivial pronominal anaphora resolution module. All these aspects are described further on.</p><p>For each type of NE, a transducer has been manually developed using a test corpus for validation. The transducer vocabulary is not only based on lexical information, but on semantic information too. For the purpose of NE analysis, we built several resources: list of words for entities like FIRST NAME, PROFESSION, CURRENCY and thesaurus for GEOGRAPHY for instance. Most of the expected answer types (presented in appendix 10.1) are NE recognized by our system, except for NPP entity (person names) hyponyms.</p><p>With regard to the output, XML has been used to represent the tagged documents, as shown below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"&lt;NPP&gt;Brown&lt;/NPP&gt;, &lt;PROF&gt;director&lt;/PROF&gt; of the &lt;ORGAN&gt;&lt;CITY&gt;Los Angeles&lt;/CITY&gt; Centers for Alcohol and Drug Abuse&lt;/ORGAN&gt;."</head><p>One can observe from the previous example that embedded entities are allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Normalization Function</head><p>The identification of all the occurrences of person names is a difficult task when performed by transducers only. Many reasons could be mentioned to explain this phenomenon. The most common case is when a LAST NAME is given without any FIRST NAME. We are also aware that our resource of FIRST NAME is not (and will never be) exhaustive. This prevents us from using this semantic information in order to detect person names. However, as observed by several authors, in most cases, person names are given at least once in full form (FIRST NAME followed by eventually a MIDDLE NAME and the LAST NAME). This appears to be exact when dealing with newspaper articles (their style obeying certain editorial rules).</p><p>In order to reduce the number of unrecognized person name occurrences, a straightforward algorithm was developed based on the previous observations. First, the LAST NAME parts of the detected person name are extracted. Then, the document is parsed again in order to detect all the LAST NAME occurrences that were forgotten by the transducer. This could be done thanks to the person name previously extracted. The additional person name could then be used by the other transducers in the sequence.</p><p>In the following example, a correct normalization of the person's name "John Paloma" is presented:</p><p>"The biggest problem is identifying where these people are," said John Paloma, 36, one of the outreach workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…</head><p>For that reason, Paloma used to stash equipment around town --for example, high atop public toilets.</p><p>The biggest inconvenient of this technique is that an incorrect detection of a word as a last name will affect the rest of the document processing. This mainly occurs when a first name is ambiguous (e.g. Rose, France, …).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pronominal Anaphora Resolution</head><p>Pronominal Anaphora is the most widespread type of anaphora. Resolving them could lead to an improvement in the Q&amp;A task. For example, the following question expects an answer of type DATE:</p><p>"When did president Herbert Hoover die ?" One of the top documents found on the Internet contains the answer to that question. However, the sentence containing the answer does not contain the key element "Hoover", but only the anaphora "he": "After his 1932 defeat, Hoover returned to private business. … He died in New York City on <ref type="bibr" coords="2,434.25,369.59,76.73,8.77">October 20, 1964."</ref> Resolving this particular case would greatly help finding the correct answer. For this reason, we have chosen to develop a Pronominal Anaphora Resolution, even though it is a " naïve" one. We decided to not resolve all the pronominal anaphora, but only for personal pronouns he and she, when they do not occur in quotations. The approach is based on syntactic roles of person names. A person name used as a subject is a candidate for a future anaphora resolution (according to its sex).</p><p>Although this method is quite naïve, we achieved reasonable results on our test corpus. However, we have not yet evaluated the benefit of such a resolution in the whole QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SIAC</head><p>The SIAC information retrieval system (Figure <ref type="figure" coords="3,251.41,213.59,4.88,8.78">1</ref> shows the Java-based GUI of SIAC) has been designed to evaluate the classification and segmentation methods we work on <ref type="bibr" coords="3,84.77,248.09,99.62,8.78" target="#b0">[Bellot &amp; El-Bèze, 2000]</ref>. During TREC-11 Q&amp;A track, SIAC has been used to index and to rank sentences extracted from the top-docs documents by employing some classical methods: vector space model, cosine similarity and TFIDF weighting scheme.</p><p>Let Q be a question and S be a sentence. Let u be a lemma<ref type="foot" coords="3,75.75,321.90,3.00,5.40" target="#foot_0">3</ref> , N(u) be the number of sentences containing u in the set of top-docs related to question Q, TF(u) be the frequency of u and N be the total number of sentences extracted from top-docs. The similarity between Q and S is estimated by the cosine measure (formula 1):</p><formula xml:id="formula_0" coords="3,48.75,382.10,238.29,84.01">∑ ∑ ∑ ∈ ∈ ∩ ∈ = S u Q u Q u S u Q S u Q u S u w w w w Q S . . . ) , cosine( 2 , 2 , , ,<label>(1) with:</label></formula><p>for document words: w u,S = TF (u, S). 1 -log 2</p><formula xml:id="formula_1" coords="3,203.25,471.40,84.54,27.93">N (u) N       (2)</formula><p>for query words:</p><formula xml:id="formula_2" coords="3,143.25,504.40,144.54,27.70">w u,Q = TF ( u,Q ). 1 -log 2 N ( u ) N       (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Question Tagging</head><p>We defined a hierarchical set of tags corresponding to the types of expected answers (see appendix 10-1). This set was built according to a manual analysis of the TREC-9 and TREC-10 Q&amp;A questions.</p><p>For tagging TREC-11 Q&amp;A questions, we have developed a rule-based tagger and we have employed a probabilistic tagger based on supervised decision trees <ref type="bibr" coords="3,225.18,644.09,61.50,8.77;3,48.75,656.09,23.35,8.77" target="#b0">[Béchet et al., 2000]</ref> for the question patterns that did not correspond to any rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Rule-based tagger</head><p>Our rule-based tagger is a set of Perl scripts. The main input consists on an XML file that contains 156 manually built regular expressions. These regular expressions are not exhaustive since they are based on TREC-9 and TREC-10 questions only. The following is an extract of this file: the &lt;CITY&gt; tag defines 3 question patterns for which the expected answer is a city. &lt;s&gt; ZTRM &lt;\/s&gt; Name VB name the DT the (\w+ JJS \w+ )?city NN city &lt;/CITY&gt; Among the 500 TREC-11 questions, 277 questions were tagged with the rule-based tool and 223 using decision trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Probabilistic Tagger</head><p>The probabilistic tagger is based on the named-entity recognizer presented during <ref type="bibr" coords="3,432.27,395.09,43.12,8.77">ACL-2000</ref><ref type="bibr" coords="3,480.91,395.09,66.03,8.77;3,309.00,407.09,21.57,8.77" target="#b0">[Béchet et al., 2000]</ref>. This recognizer uses a supervised learning method to select their most distinctive features automatically select from a set of noun phrases, embedding named entities of different semantic classes,. The result of the learning process is a semantic classification tree (a particular decision tree introduced by <ref type="bibr" coords="3,427.41,464.09,107.27,8.77" target="#b1">[Kuhn &amp; De Mori, 1996]</ref> to classify new strings from a corpus of tagged strings) that tags an unknown entity relying on its context. The adaptation of this recognizer to this task was realized by Fréderic Béchet: the tags are not linked to a particular entity but to the question as a whole.</p><p>To "grow" decision trees, one needs a sample corpus (manually tagged TREC-10 questions in our case) and a set of key features to split tree nodes. The list of features is generated from the training corpus. Each feature corresponds to a sequence of words and/or POS tags. Splitting is made by asking whether a selected feature matches a certain regular expression involving words, POS and gaps occurring in the TREC-11 question.</p><p>In order to evaluate our probabilistic tagger, we have subdivided the 500 TREC-10 questions into two sets: a learning set (259 questions) and a test set (150 questions). Over this 150 questions test set, we obtained a 68.5% precision level for 127 questions (23 questions were not tagged because the probability of the chosen tag was less than a minimal threshold).</p><p>For example, CITY is the tag chosen for question 1204 whereas all other candidate tags have a zero probability.</p><p>Question 1204:</p><formula xml:id="formula_3" coords="4,48.75,114.56,240.80,166.23">sample_1204 &lt;s&gt; ZTRM &lt;/s&gt; What WP What is VBZ be the DT the cap=tal NN capital of IN of &lt;UNK&gt; NP &lt;UNK&gt; ? ZTRM ? &lt;/s&gt; ZTRM=&lt;/s&gt; = CITY sample_1204 ACTOR_ACTRESS 0 BIOGRAPHY 0 BIRD 0 BODY_PART 0 CITY=1 COMMON_WORD 0 COMPANY 0 CONTINENT 0 COUNTRY 0 COUNTY 0&lt;=R&gt; CURRENCY 0 DATE 0 DEFINITION 0 DEPTH 0 DIAMETER 0 DISTANCE 0 DURATION 0 EVENT 0 EXPANDED_ACRONYM 0 EXPLANATION 0 EXPLORATOR_RESEARCHER 0 FAMOUS_NPP 0 FAMOUS_PLACE 0 FAMOUS_PLACES 0 F=OWER 0 FOOD 0 HEIGHT 0 HEMISPHERE 0 INVENTOR 0 LENGTH 0 &lt;=R&gt; MEDIA 0 MINERAL 0 MONEY 0 MOUNTAIN 0 MUSICIAN 0 NUMBER 0 =THER_NP 0 PERCENTAGE 0 PHRASE 0 PLANET 0 POLITICIAN 0 POPULATION 0 RIVER 0 SEA 0 SEASON 0 SPEED 0 SPORTSMAN 0 STAR 0=00 STATE 0 TEAM 0 TEMPERATURE 0 UNIV 0 VEGETAL 0 WEIGHT 0.0&lt;=R&gt; 0 WRITER 0 YEAR 0</formula><p>In order to tag TREC-11 questions that were not tagged by our rule-based tagger, the learning was realized over the whole set of TREC-10 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Filtering and Answer Extraction</head><p>The sentences allowing to answer questions do not necessarily contain a word of the questions. At the opposite, a sentence may contain some keywords of the question without being related to it. Thus, a classical retrieval scheme such as similarity computation in the vector space model is not sufficient.</p><p>In our case, the sentences from top-docs (the list of topdocs is the one given by NIST) are ranked by SIAC according to the similarity between them and the question. We had no time to implement a specific module to detect the focus of questions or to analyze their domaindependent semantic properties. In order to filter sentences that probably did not contain the answer, we only kept those with a proper name appearing in the question 4 and those containing an entity of the same type than the expected answer type. This strategy prevents us from answering some questions (a NIL answer is given by the Q&amp;A system because of the lack of proper names in the ranked sentences and/or in the question) but it enables us to select some answers more easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The Use of Knowledge Bases</head><p>We have chosen to take benefit from a set of knowledge DataBases (KDB) for several reasons, mainly: i.) Assess the reliability of our search engine, ii.) For a given relation between two NE, provide a bootstrap that may be used in the later steps of an iterative process (we plan to develop it soon). This process will be useful to extract other instances of such relations from full text collections. Therefore, it may be misleading to consider that the underlying idea of this component was to constitute a large Data Base of FAQ (Frequently Asked Questions), even though it has also been used as such.</p><p>Figure <ref type="figure" coords="4,265.50,721.10,4.50,8.10">2</ref> -The SIAC user interface</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Coupling SIAC and the use of KDB</head><p>The link between a question and the production of the KDB component may be seen as a relation more than a function since the output may be multiple. To handle this (1-n) generation, we found it convenient to code the set of candidate answers using a regular expression. This regular expression is then applied on the sentences extracted by the search engine for 2 purposes: i.) Select the most likely answer ii.) Provide a support to the answer as required by the QA TREC protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Some characteristics of the KDB used</head><p>5.2.1. USA topics As it appears obviously from a quick analysis of the Q set (TREC-8 through TREC-11), several questions are focused on various attributes related to the United States of America. Thus, we have searched the net (mainly from the following url: http://www.50states.com/ ) in order to collect as many data related to these topics as possible. The coverage of such a "USA-centered" KDB is shown in Table <ref type="table" coords="5,73.99,349.34,3.77,8.78">5</ref>.1. <ref type="table" coords="5,87.43,511.34,3.68,8.78">5</ref>.1. : Coverage of some KDB on the Q sets #1 centered on the US + #2 not centered on the US It was also an opportunity to cope with similar questions when they can be asked on other countries. In each cell of Table <ref type="table" coords="5,79.87,563.84,3.77,8.78">5</ref>.1, the first number concerns US centered questions, the second one, other countries.</p><formula xml:id="formula_4" coords="5,48.75,368.09,230.80,152.03">TREC 8 9 10 11 Total Motto 0+0 0+0 1+0 1+0 2+ 0 Flower 0+0 0+0 2+1 0+0 2+1 Song 0+0 0+0 0+0 1+0 1+ 0 Tree 0+0 1+0 0+0 0+0 1+ 0 Bird 0+0 1+0 3+0 1+0 5+ 0 Governor 0+0 0+0 1+0 3+0 4+ 0 Creation 0+0 1+0 3+0 2+0 6+ 0 Capital 1+5 2+1 0+5 1+6 4+17 Population 0+4 4+5 1+4 1+3 6+16 President 1+1 2+1 4+0 5+0 12+ 2 Total 2+10 11+7 15+10 15+9 43+36 Table</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.2.</head><p>Book topics In another direction, we have included in this process the relation book/author (who wrote the book "title"?). We have extracted from the web a list of bibliographical references. There are currently 15 800 entries in this specific KDB. Most of them come from the Pennsylvania University library and may be found at the following url: http://onlinebooks.library.upenn.edu/titles.html. We have 4 The proper name detection was realized according to the POS-tags. The formulation of a question is not always as precise as who wrote the book "y"?. Elliptic sentences as who wrote "y"? or who is the author of "y"? are more ambiguous. For instance, in Q8/196, "Hamlet" may be a movie or the famous play. The case is also encountered in Q11/1759: "Fiddler on the Roof" may be a novel or a musical. The novel was not in our KDB and it is a chance since only the musical has been considered as the correct answer by the judges. Whether we decide to enrich our resource or not, we have to take this kind of difficulty into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.3.</head><p>Archives It was also natural to check whether questions found in TREC-11 were not already present in previous TRECs. In such case, the answer provided could be reused. Let us call an Archive A i , a pair of two sets: (Questions, Answers) of TREC i . Until we got A 11 (the patterns of TREC 11 ), we have considered the following: for Q 8 use A 9+10 , for Q 9 use A 8+10 , for Q 10 use A 8+9 , for Q 11 use A 8+9+10 (1 st line of Table <ref type="table" coords="5,309.00,403.34,3.74,8.78">5</ref>.3). As shown in 2 nd line of Table <ref type="table" coords="5,461.08,403.34,3.77,8.78">5</ref>.3, the coverage on Q 8-10 does not increase a lot when A 11 is also taken into account, except for Q 10 .  <ref type="table" coords="5,365.35,479.84,3.90,8.78">5</ref>.3: Considering other Q sets as FAQ Note that we did not search for a similar question but for exactly the same one. Therefore, some improvements can be made here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.4.</head><p>Typos and Variants Typos may be seen as a noise disturbing the canal between the input (Q) and the output (A). For a question such as Q10/1249/ Who wrote "The Devine Comedy"? the relation (Dante -Divine Comedy) included in the KDB described in 5.2.2 could not be exploited. We have used the classical edit distance <ref type="bibr" coords="5,362.25,616.34,114.69,8.77" target="#b1">[Lowrance &amp; Wagner, 1975]</ref> and the dynamic time wrapping method to find the optimal way to associate words as Divine and Devine. Penalty weights have been assigned to operations (substitution, omission, insertion), and a threshold has been empirically chosen in order to avoid confusion such as Mexico/Monaco. This procedure is not only useful to handle typos but also to cope with the numerous variants, which can be observed for the Proper Nouns transcription of Foreign Entities (there are, for example, more than 50 ways to write Kahdafi. This can be coded by a regular expression <ref type="bibr" coords="5,431.25,731.09,100.64,8.77">[GK]h?ah?dd?h?ah?ff?i)</ref>.</p><p>As far as we want to take into account human factors, we have chosen to generate an answer where the graphemes involved are the most similar ones and not necessarily the ones used in the question. Our assumption is that the user will find more acceptable a system answering sometimes to another question than a system giving a wrong answer to his question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">KDB Summary</head><p>In the subsections 5.2.1 to 5.2.3, we have given some examples of the domains covered by the KDB we used. They correspond to about half of the answers currently supported by our KDB component. The second half concerns various topics such as rivers, mountains, Nobel's, hurricanes and so on. It is impossible to describe each of them in detail here, but it is interesting to see that the coverage is more or less the same on each TREC.  <ref type="table" coords="6,114.10,369.59,3.90,8.78">5</ref>.4: Global Coverage of 36 KDB While for 12.2 % of the Q 11 set, the KDB are able to produce an answer, it is not possible to insert all of them in our run. As mentioned in section 5.1, we have also to match each answer with the output of SIAC. Sometimes (8 / 61 cases) the search engine is too silent, therefore the set of candidates may be empty. In more than half of the cases (35 / 61), it was possible to find a pattern matching the regular expression. For the remainder (18 / 61 cases), no match has been found in the sentences retrieved by SIAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ordering answers</head><p>This year's QA track introduced newness in the evaluation measure in such a way that systems have to cope with the following principle: rank the answers from the most reliable to the less one. In order to take into account this requirement, our answers have been ordered according to results provided by the use (or non-use) of knowledge databases (KDB) -as a way to validate an answer -and by the question classifier output. So, for each question, the question classifier assigns one (or several) expected NE(s) and its (their) corresponding confidence(s). If it cannot be decided which of the 44 available entities should be responsive, the question is tagged as "unknown". From these points, our ordering strategy can be summarized schematically as follows: divide the Q-set in three main groups,</p><p>• Q1: questions for which answers have been found by SIAC and validated with KDB. Since there is an agreement between two independent components, it is justified to assign a highest reliability score to the group produced by such a combination and to place it at the top ranks. Thirty-five questions were in this group and were ranked from 1 to 35.</p><p>• Q2: questions for which answers have been found only by SIAC and not covered by any database. This group, the major one with 438 questions, could be divided in two parts: non NIL answers (389) and NIL answers (49). As described in section 4.3, filters are applied on SIAC output in order to keep only expected entities mapping question class(es) -it may happen that all the candidates are eliminated by this filtering -that is how NIL is produced by the system. It was decided to put these NIL at the end of this group, as they are the results of many treatments and therefore the decision process becomes too uncertain. Inside non NIL answers, order was defined first by decreasing confidences (in question classes) and second by question classes. Order among question classes (see table <ref type="table" coords="6,345.91,334.34,3.97,8.78">6</ref>.1) has been derived from previous experiments performed for tuning purpose. For example, our classification component performs well for questions asking for YEAR and DATE, and as named entities mapping these classes are also well detected, we are more confident in answers coming from these series.</p><p>On the other hand, by the time of our participation, for questions asking for frequencies, named entities finder was not able to detect these expressions -accordingly it should be risky to bet on the class mapping this entity.</p><p>• Q3: questions for which the classifier did not assign a class (and tagged "unknown"). This is clearly a flaw in our system's answering process as answer selection depends on these classes. Therefore, such questions will be answered with a NIL and put at the bottom ranks. It happened thirty times over the entire TREC-11's set but three of them were finally overhandled by KDB -and backed up in the first group Q1. The remainders ( <ref type="formula" coords="6,391.83,560.09,8.29,8.78">27</ref>) have been left as NIL.</p><p>This ordered list (Q1, Q2, Q3) corresponds to the way we ranked the three groups.</p><p>YEAR, DATE, COUNTRY, COUNTY, NPP, ACRONYM, CITY, MAIL, MONTH, URL, STATE, ADDRESS, TITLE, LOCATION, ORGAN Table <ref type="table" coords="6,370.23,653.84,3.90,8.78">6</ref>.1: Top 15 questions classes (ordered by preference)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Official results</head><p>Table <ref type="table" coords="7,74.34,141.59,4.14,8.78">7</ref>.1 shows the results obtained by our run LIA2002a (only one run was submitted).</p><p>Number </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Experiments</head><p>After the dead line, we performed some additional experiments. It was possible to evaluate them thanks to the TREC-11 answers patterns made available by Ken Litkowski. For this purpose, a home-made tool was developed to compute the confidence weighted score ("CWS"). In the following, these new experiments will be referred as LIA2002o (o standing for October) and LIA2002n (n for November).</p><p>• Evaluation of the KDB contribution:</p><p>The results reported in table 7.2 are useful to focus only on the behavior of questions for which a KDB was involved.</p><p>For this, we assume that the other answers (from rank+1 to 500) were wrong: -"R", "U", "CWS" stand respectively for "right", "unsupported", and "confidence weighted score".</p><p>-"Lia2002a" is the run submitted in August for TREC-11, -"Lia2002o" is a run with few additions in KDB. Also, minor bug corrections inside our whole system and specially in ordering strategy were done (ordering for SIAC answers was broken in our TREC submission) -"Lia2002n" is our last run. It includes two more entries (a tiny extension of the KDB). The main difference with the previous ones is that answers powered by KDB are ranked by applying the same ordering strategy as answers from SIAC.</p><p>• Answers allocation (table <ref type="table" coords="7,436.78,139.34,3.64,8.78">7</ref>.3): System succeeded in finding a non nil, right and supported answer in about 10% of the cases (column reported as "R-nil"). It provided document containing a correct answer in 15% of the cases (column reported as "D") but failed to extract it in about 5% of the cases (column reported as "D-(R-nil)"). SIAC was able to find 5% of the non nil correct answers but ten of them were overlapped by the KDB. </p><formula xml:id="formula_5" coords="7,309.00,255.59,213.60,20.02"># KDB Size R U R-nil D D-(R-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>For our first participation in TREC -question answering, we focused on a small number of questions, that is questions for which an answer can be produced with a sufficient level of confidence. The goal was to reach 30% of accuracy which is honorable as a first trial.</p><p>A lot of work remains. Firstly, we could have gone into entity recognition in greater depth, using more statistics. Secondly, because two different tools have been used in order to tag (NE) the documents and the questions, we experienced some problems making a mapping from one to the other. The lack of compatibility should be solved by using the same set of tag. Also, anaphora resolution is too simple and could be applied on many other anaphora phenomena. Another important point: question tagging is quite weak. For example, for many questions, it assigns the same confidence to different tags. The selection of the tag to be considered could be easily improved. Moreover, answer extraction is too much simple. Because no syntactic tagging is done, it is impossible to choose precisely a phrase in which the answer is supposed to be. So, the only thing we did was to extract the searched entity wherever it was in the candidate sentence. Consequently, many wrong answers were retrieved.</p><p>Relying on some knowledge bases clearly improves the results of our system. Typo correction is quite efficient and allows us to answer correctly several questions. We can improve the cases where an answer is provided by the KDB and SIAC fails to retrieve any expected NE, by enriching the question with this answer in order to retrieve supporting documents. Moreover, we could increase the coverage of this KDB in two directions: i.) Find other knowledge sources on more and more subjects, ii.) Use each KDB as a bootstrap in order to enlarge it thru text extraction. We consider that the second item is a key point to make the first one feasible.</p><p>Finally, let us consider the graph plotted in figure <ref type="figure" coords="8,267.54,368.84,3.70,8.78" target="#fig_3">3</ref>. It represents the growth of correct answers. We can see that the curve grows in stages. Important improvements are followed by long flat lines.  If we do not consider the first stage (due to the KDB), we have 4 stages (which length is between 11 and 33) where accuracy is quite good (between 0.25 and 0.36). This concentration is sufficiently significant to conclude that some questions have the same behavior and the system performs quite well on these types of questions. Since they are grouped, it should be possible to detect and locate them higher in the list. It could be possible to improve the results by detecting these types of questions. This concerns 48 questions, that is more than 90% of our correct answers. If they were located at the beginning of the list, the CWS would be 0.32 instead of 0.246. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,228.75,661.10,123.73,8.10"><head></head><label></label><figDesc>Figure 1 -From corpus to answers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,309.00,197.81,28.70,6.48;3,344.25,206.81,188.32,6.48;3,309.00,215.81,230.40,6.48;3,309.00,225.56,225.89,6.48;3,309.00,234.56,129.95,6.48;3,344.25,243.56,198.12,6.48;3,309.00,252.56,230.39,6.48;3,309.00,261.56,230.40,6.48"><head></head><label></label><figDesc>&lt;CITY&gt; &lt;s&gt; ZTRM &lt;\/s&gt; (In IN in )?(([Ww]hat WP [Ww]hat)|([Ww]hich WDT [Ww]hich)) (\w+ JJ\w? \w+ )?((city NN city)|(seaport NN seaport)|(capital NN capital)|(town NN town)) &lt;s&gt; ZTRM &lt;\/s&gt; What WP What is VBZ be the DT the (\w+ JJ\w? \w+)? ((city NN city)|(seaport NN seaport)|(capital NN capital)|(town NN town))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,59.25,574.85,214.41,8.10"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of correct answers by number of answers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,49.50,447.50,505.50,259.50"><head></head><label></label><figDesc></figDesc><graphic coords="4,49.50,447.50,505.50,259.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,309.00,98.09,236.87,74.02"><head></head><label></label><figDesc>also exploited shorter lists as the ones available at the url: http://www.state.nh.us/nhsl/bookbag/a.html .</figDesc><table coords="5,309.00,128.09,233.10,44.03"><row><cell>TREC</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>Total</cell></row><row><cell>Book author</cell><cell>2 / 3</cell><cell>5 / 7</cell><cell>1 / 1</cell><cell>0 / 2</cell><cell>8 / 13</cell></row><row><cell cols="6">Table 5.2 : Coverage of the author KDB on the Q sets</cell></row><row><cell cols="6">#1 answers produced by KDB / #2 questions on this topic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,57.75,623.09,220.29,80.77"><head></head><label></label><figDesc>Location of correct answers in the list</figDesc><table coords="8,57.75,623.09,220.29,80.77"><row><cell cols="4">Stage Correct answers Accuracy Length of stage</cell></row><row><cell>1-30</cell><cell>24</cell><cell>0.8</cell><cell>30</cell></row><row><cell>78-110</cell><cell>11</cell><cell>0.33</cell><cell>33</cell></row><row><cell>145-166</cell><cell>6</cell><cell>0.27</cell><cell>22</cell></row><row><cell>433-443</cell><cell>4</cell><cell>0.36</cell><cell>11</cell></row><row><cell>484-495</cell><cell>3</cell><cell>0.25</cell><cell>12</cell></row><row><cell>Table 8.1:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,55.35,726.35,230.32,8.10;3,48.75,736.10,107.94,8.10"><p>We used the TreeTagger<ref type="bibr" coords="3,156.75,726.35,52.86,8.10" target="#b1">[Schmid, 1994</ref><ref type="bibr" coords="3,217.43,726.35,21.05,8.10" target="#b2">[Schmid, , 1995] ]</ref> in order to obtain POS-tags and lemmas.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sixth Message Understanding Conference, Columbia, Maryland: Morgan Kaufmann Publishers, pp. 55-69, 1995.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.">Hierarchical List of Expected Answer Types</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,313.50,219.55,70.40,10.80;8,309.00,235.34,238.39,8.78;8,309.00,248.09,237.58,8.78;8,309.00,259.34,237.64,8.77;8,309.00,270.59,237.59,8.77;8,309.00,282.59,237.62,8.77;8,309.00,293.84,167.90,8.77;8,309.00,310.34,238.41,8.77;8,309.00,323.09,237.01,8.77;8,309.00,334.34,236.90,8.77;8,309.00,351.59,238.40,8.77;8,309.00,363.59,238.33,8.77;8,309.00,374.84,237.63,8.77;8,309.00,386.09,195.68,8.77;8,309.00,403.34,239.13,8.77;8,309.00,415.34,237.37,8.77;8,309.00,426.59,223.40,8.77;8,309.00,444.59,237.63,8.77;8,309.00,455.84,237.65,8.77;8,309.00,467.09,237.67,8.77;8,309.00,479.09,237.64,8.77;8,309.00,490.34,104.10,8.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,357.65,259.34,188.99,8.77;8,309.00,270.59,83.89,8.77;8,313.72,323.09,223.98,8.77;8,313.29,363.59,234.03,8.77;8,309.00,374.84,167.22,8.77;8,369.75,415.34,176.62,8.77;8,309.00,426.59,19.89,8.78">Clustering by means of decision trees without learning or hierarchical and K-Means like algorithms</title>
		<author>
			<persName coords=""><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Appelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,404.98,270.59,141.62,8.77;8,309.00,282.59,105.75,8.77;8,319.26,334.34,74.55,8.77;8,497.82,374.84,48.82,8.78;8,309.00,386.09,55.57,8.77;8,459.24,444.59,87.39,8.77;8,309.00,455.84,168.86,8.77">Proceedings of the Sixth Message Understanding Conference</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</editor>
		<meeting>the Sixth Message Understanding Conference<address><addrLine>Columbia, Maryland; Hong-Kong, China; Paris, France; Borthwick; New York University</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1995">1995. 1995. 2000. 2000. 2000. 1999. 1999. 1999</date>
			<biblScope unit="page" from="344" to="363" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Courant Institute</orgName>
		</respStmt>
	</monogr>
	<note>A Maximum Entropy Approach to Named Entity Recognition. Ph.D.. Specialized in artificial intelligence and computational linguistics</note>
</biblStruct>

<biblStruct coords="8,314.52,507.59,232.10,8.78;8,309.00,519.59,237.59,8.77;8,309.00,530.84,236.14,8.77;8,309.00,542.09,208.41,8.77;8,309.00,560.09,238.38,8.77;8,309.00,571.34,236.89,8.77;8,309.00,582.59,210.62,8.77;8,309.00,600.59,237.57,8.77;8,309.00,611.84,238.33,8.77;8,309.00,623.09,237.60,8.77;8,309.00,635.09,238.36,8.77;8,309.00,646.34,65.84,8.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,533.66,507.59,12.96,8.78;8,309.00,519.59,237.59,8.77;8,309.00,530.84,56.66,8.78;8,355.50,571.34,190.39,8.77;8,309.00,582.59,23.86,8.78;8,429.28,600.59,117.29,8.77;8,309.00,611.84,130.57,8.77">The application of semantic decision trees to natural language understanding</title>
		<author>
			<persName coords=""><forename type="first">De</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">R</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">R</forename><surname>De Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Lowrance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,468.32,611.84,79.01,8.77;8,309.00,623.09,237.60,8.77;8,309.00,635.09,86.09,8.77">Proc. of the First International Conference on New Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">&amp;</forename><surname>Lowrance</surname></persName>
		</editor>
		<editor>
			<persName><surname>Wagner</surname></persName>
		</editor>
		<meeting>of the First International Conference on New Methods in Natural Language essing<address><addrLine>Manchester, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1975">1996. 1996. 1975. 1975. 1994</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
	<note>Probabilistic Part-of-Speech Tagging Using Decision Trees</note>
</biblStruct>

<biblStruct coords="8,313.80,663.59,232.83,8.77;8,309.00,675.59,237.75,8.77;8,309.00,686.84,237.62,8.77;8,309.00,698.09,201.66,8.77;8,309.00,716.09,238.34,8.77;8,309.00,727.34,238.30,8.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,437.51,663.59,109.12,8.77;8,309.00,675.59,197.86,8.77">Improvements In Part-of-Speech Tagging With an Application to German</title>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,521.28,675.59,25.47,8.78;8,309.00,686.84,81.61,8.77;8,369.08,698.09,69.49,8.77">EACL SIGDAT Workshop</title>
		<editor>
			<persName><forename type="first">Hinrichs</forename><surname>Feldweg</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page" from="47" to="50" />
		</imprint>
	</monogr>
	<note>Lexikon und Text. Weischedel, 1995] R. Weischedel, &quot;BBN: Description of the PLUM System as Used for MUC-6&quot;, Proceedings of the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
