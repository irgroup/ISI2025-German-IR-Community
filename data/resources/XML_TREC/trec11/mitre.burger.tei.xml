<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,202.08,60.28,204.18,15.84;1,406.32,58.79,3.65,9.36">MITRE&apos;s Qanda at TREC-11 *</title>
				<funder ref="#_mQxEfTF">
					<orgName type="full">Advanced Research and Development Activity</orgName>
					<orgName type="abbreviated">ARDA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,212.88,79.66,64.36,11.96"><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
						</author>
						<author>
							<persName coords="1,284.61,79.66,43.08,11.96"><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
							<email>lferro@mitre.org</email>
						</author>
						<author>
							<persName coords="1,334.75,79.66,60.07,11.96"><forename type="first">Warren</forename><surname>Greiff</surname></persName>
							<email>greiff@mitre.org</email>
						</author>
						<author>
							<persName coords="1,180.48,92.38,67.68,11.96"><forename type="first">John</forename><surname>Henderson</surname></persName>
						</author>
						<author>
							<persName coords="1,255.81,92.38,48.91,11.96;1,304.56,90.76,1.74,7.70"><forename type="first">Marc</forename><surname>Light</surname></persName>
							<email>marc-light@uiowa.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Iowa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.48,92.38,53.48,11.96"><forename type="first">Scott</forename><surname>Mardis</surname></persName>
							<email>mardis@mitre.org</email>
						</author>
						<author>
							<persName coords="1,373.42,92.38,57.92,11.96"><forename type="first">Alex</forename><surname>Morgan</surname></persName>
							<email>amorgan@mitre.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">The MITRE Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,202.08,60.28,204.18,15.84;1,406.32,58.79,3.65,9.36">MITRE&apos;s Qanda at TREC-11 *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5288AA26E4F172EEDFD43F6776783079</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Qanda is MITRE's TREC-style question answering system. Since last year's evaluation, principal improvements to the system have been aimed at making it faster and more robust. We discuss the current architecture of the system in Section 1. Some work has gone into better answer formation and ranking, which we discuss in Section 2. After this year's evaluation, we have done a number of ROVERstyle system combination experiments using the judged answer strings made available by NIST. We report on some success with this in Section 3. We have also performed a detailed categorization of previous TREC results according to answer type and grammatical category, as well as an analysis of Qanda's own question analysis component-see Section 4 for these analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">TREC-11 System Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Catalyst</head><p>Last year, Qanda was re-engineered to use a new architecture for human language technology called Catalyst, <ref type="bibr" coords="1,103.39,470.86,115.97,11.96" target="#b2">(Burger &amp; Mardis 2002)</ref>. Developed at MITRE for the DARPA TIDES program, the Catalyst architecture is specifically designed for fast processing and for combining the strengths of Information Retrieval (IR) and Natural Language Processing (NLP) into a single framework. Catalyst uses a dataflow architecture in which standoff annotations are passed from one component to another, the components being connected in arbitrary topologies (currently restricted to acyclic ones). The use of standoff annotations permits components to be optimized for just those pieces of information they require for their processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Major system components</head><p>Qanda has a by now familiar architecture-questions are analyzed for expected answer types, documents are retrieved using an IR system and are then processed by various taggers to find entities of the expected type in contexts that match the question. Below we describe each of the major components in turn.</p><p>• Question analysis: This component is run after the question has been subjected to POS and named entity tagging. It uses a simple grammar, currently hand-written, to identify important components of the question-see Section 4 below for more detail.</p><p>• IR wrappers: Catalyst components have been written for several IR engines, taking the results of the question analysis and formulating an IR query.</p><p>For TREC-11, we used the Java-based Lucene engine (Apache 2002). Lucene's query language has a phrase operator, and also allows query components to be given explicit weights. Qanda uses both of these capabilities in constructing queries from the information extracted from the question. For TREC-11, the top 25 documents were retrieved.</p><p>• Passage processing: Retrieved documents are tokenized, and sentence boundaries are detected.</p><p>Because some downstream components run more slowly than the rest of the system, Qanda scores each sentence by summing the log-IDF (inverse document frequency) of each word that overlaps with the question. Only those sentences with a sufficient score are passed on to the rest of the system.</p><p>• Named entity tagging: Qanda uses Phrag <ref type="bibr" coords="1,523.91,610.06,34.31,11.96;1,333.12,622.78,53.16,11.96">(Burger et al. 2002)</ref>, an HMM-based tagger, to identify named persons, locations and organizations, as well as temporal expressions. Phrag is also used as a POS tagger for question analysis.</p><p>• Numeric tagging: A simple pattern-based tagger uses an extensive list of unit phrases to identify measures, as well as currency, percentages and other numeric phrases.</p><p>• Other taggers: We have a simple facility for constructing taggers from fixed word-and phraselists. These were used to re-tag many named locations more specifically as cities, states/ provinces, and countries. Qanda also identifies various other (nearly) closed classes such as precious metals, birthstones, various animal categories, etc.</p><p>• Answer formation and ranking: Candidates are identified and merged, a number of features are collected, and a score is computed-see Section 2.</p><p>Qanda's answer formation component attempts to find the best answer phrase as well as the best supporting context for that answer-the former may not be a substring of the latter due to candidate merging. For our TREC-11 submission, the answer phrase was used as the actual (scored) answer string, while the context was included as the secondary (unscored) justification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Answer Ranking</head><p>Qanda only examines sentences that match the question sufficiently using the IDF-weighted overlap described above. It collects candidate answers by gathering phrasal annotations from all of the semantic taggers, and identifies the following features:</p><p>• Context IDF Overlap: Described above.</p><p>• Context Bigram Overlap: Raw count of word bigrams in common with the question.</p><p>• IR Ranking of the source document by the IR system.</p><p>• Type Same: Boolean, true if the candidate and expected answer types are identical.</p><p>• Type Similar: Partial credit if the candidate's type is "related" to the expected answer type, e.g., COUNTRY and generic LOCATION.<ref type="foot" coords="2,231.12,527.32,3.48,7.70" target="#foot_0">1</ref> </p><p>• Candidate Overlap: Raw count of non-stop words in common between the candidate itself and the question, to bias against entities from the question being chosen as answers.</p><p>• Minimal Overlap Distance: Number of characters between the candidate and the closest non-stop question word in the context. 2</p><p>• Numeric Date: 1 if the expected answer type is temporal and the candidate contains a numeric token, 0 otherwise, to bias against unresolved relative dates such as yesterday.</p><p>Candidates with similar textual realizations are merged, with the combined candidate retaining the highest value for each feature. Accordingly, an additional feature is maintained:</p><formula xml:id="formula_0" coords="2,318.72,167.74,71.71,12.85">• Merge Count</formula><p>After all of the (merged) candidates have been acquired, the raw feature values described above are normalized with respect to the maximum across all candidates, resulting in values between 0 and 1. <ref type="foot" coords="2,554.64,222.52,3.48,7.70" target="#foot_1">3</ref>Features normalized in this way are more commensurate across questions, especially word overlap and related features <ref type="bibr" coords="2,437.53,262.30,76.66,11.96" target="#b13">(Light et al. 2001)</ref>.</p><p>Each feature has a fixed weight, and a simple additive model is used to give each candidate an overall score.</p><p>Our official TREC submission used (minimally) handtuned weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Log-linear models for answer scoring and confidence estimation</head><p>We are currently experimenting with acquiring the weights for the answer scoring component using logistic regression on past TREC datasets, resulting in a log-linear model, as has been used by <ref type="bibr" coords="2,315.12,422.86,107.12,11.96" target="#b11">Ittycheriah et al. (2001)</ref> and others. One issue is that because the model estimates a conditional probability (namely correctness given features of the question and candidate), the resulting scores are not necessarily commensurate across questions, and so the answers cannot be easily ranked for confidence, as required in TREC this year. Our current approach is to re-score the top candidate for each question using a second loglinear model. This uses features of the question, such as expected answer type, that do not affect the first model's candidate ranking, as well as features derived from applying the first model, such as the top candidate's original score, the total score mass given to the top N candidates, etc. These last features are similar to those used by <ref type="bibr" coords="2,420.46,599.98,81.00,11.96" target="#b3">Czuba et al. (2002)</ref>.</p><p>2 Words would arguably be a more intuitive unit for this feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System-Combination Experiments-Exploiting Diversity</head><p>Progress in question answering technology can be measured as individual systems improve in accuracy, but it is not the only way to witness technological progress. A question one can ask is how well we can perform automatic question answering as a community. If we were asked to enter an Earth English system in an intergalactic TREC, how well would we do? One easy answer is that we would perform as well as the best QA system. A second answer is that perhaps we could do even better by combining systems-this might be expected to work if different systems were independent in their errors.</p><p>The follow-up question is how would we build such a system?</p><p>Lower bounds on the highest possible performance current technology can achieve on a given dataset have practical value, as well. They allow us to better estimate how well systems are doing with respect to the underlying difficulty of the dataset, and continually provide performance targets that are known to be achievable. Without such lower bounds on optimal performance, one cannot determine if technological progress in a domain has simply stalled.</p><p>NIST's ROVER system for combining speech recognizer output gives ASR researchers an updated goal to shoot for after every evaluation, as well as an implicit measure of the extent to which systems are making the same errors <ref type="bibr" coords="3,173.04,442.30,62.53,11.96" target="#b6">(Fiscus 1997)</ref>. The work herein initiates a similar set of experiments for question answering technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>The task we are faced with is straightforward. Given a collection of answers to a question, choose the one most likely to be correct. For our purposes, each answer consists of the answer string and an identifier for an associated document. Our data was limited in that it did not indicate which answers were provided by which system-see the discussion below. Note that we use no knowledge of the question or of the document collection. Our assumption is that the authors of the individual systems have milked the information in their inputs to the best of their capabilities. Our goal is to combine their outputs, not to re-investigate the original problem.</p><p>In this year's main QA evaluation there were 67 different systems or variants thereof involved. Thus, our corpus consists of 67x500 answers. To guard against any implicit bias due to repeated experimentation on the small dataset available, we randomly selected a 100-question subset for development of our techniques-the remaining 400 questions were kept as a test set, evaluated only once, when development was complete. While we may have wished to pursue parametric techniques, we felt that this training set was too small to explore any but the simplest (non-parametric) techniques. An exception is the experiments described below involving priors over the document sources, which still only involved four parameters.</p><p>Voting is an easily understood technique for selecting an answer from among the 67 suggestions.</p><p>Unfortunately, voting techniques do not provide a mechanism for utilizing full knowledge of partial matches between proposed answers. While his original goal was the selection of representative DNA sequences, <ref type="bibr" coords="3,371.80,275.50,77.28,11.96" target="#b7">Gusfield (1993)</ref> introduced a general method for selecting a candidate sequence that is close to an ideal centroid of a set of sequences. His technique works for all distance measures that support a triangle inequality, and offers a bound that the sum of pairwise distances (SOP) from proposed answers to the chosen answer will be no more than twice the SOP to the actual centroid (even though the centroid may not be in the set). This basic technique has been used successfully for combining parsers <ref type="bibr" coords="3,474.94,389.26,78.64,11.96" target="#b9">(Henderson 1999)</ref>. Appealingly, the centroid method reduces to simple voting when an "exact match" distance is used (the complement of the Kronecker delta).</p><p>One advantage of both simple voting and the centroid method is that they give values (distances) that are comparable between questions. An answer that receives 20 votes is more reliable than an answer that receives 10 votes, and likewise for generalized SOP values. This gives a principled method for ranking results by confidence and measuring average precision, as required for this year's TREC evaluations.</p><p>In selecting appropriate distance measures between answers, both words and characters were explored as atomic units of similarity. Two well-known nonparametric distances are available in the literature: Levenshtein edit distance on strings and Tanimoto distance on sets <ref type="bibr" coords="3,388.75,629.02,75.09,11.96" target="#b5">(Duda et al. 2001</ref>). We experimented with each of these, and also generalized the Tanimoto distance to handle multisets by defining a function to map multisets to simple sets: Given a multiset containing instances of a repeated element x we can create a simple set by subscripting, e.g., &lt;x,x,y,z&gt; fi {x 1 ,x 2 ,y,z}. We can then use the standard Tanimoto distance on the resulting simple sets. 4   Overall, systems seemed to be conservative and answered with the NIL document (no answer) at a rather high rate (17% of all answer strings this year).</p><p>To compensate for this, a "source prior" was collected from the 100-question training set. These four numbers recorded the accuracy expected when systems generated answers from the four document sources (AP, NYT, XIE, and NIL). Those numbers were then used to scale the distance measures for the corresponding answer strings. Other than these priors, no other features of the document ID string were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Several measurements were made to ascertain the quality of the various selection techniques, as seen in Figure <ref type="figure" coords="4,87.13,461.98,4.12,11.96">1</ref>. Precision, P, indicates the accuracy of the technique, the percentage of the answers that were judged to be correct. avgP is the main measure used by NIST this year-the average precision of all prefixes of the sequence of answers placed in order of high to low confidence. Strict corresponds to the correctness criterion used by NIST-the answer must be exact and justified by the referenced document (assessor judgment 1). The Loose figures discard these two criteria (assessor judgment ≥ 1). The Loose P measure was the one that was optimized during development.</p><p>In Figure <ref type="figure" coords="4,103.92,619.66,5.40,11.96">1</ref>   <ref type="bibr" coords="4,315.12,530.14,43.08,11.96">20, 1969;</ref><ref type="bibr" coords="4,361.57,530.14,68.34,11.96">July 18, 1969;</ref><ref type="bibr" coords="4,433.44,530.14,60.57,11.96">July 14, 1999</ref>; even simply 20. All of these, including the incorrect 1969s , contributed to the correct answer being selected.</p><p>The disparity between the dynamic range of these systems on the development dataset and the test dataset suggests that the dev set sample size of 100 (6700 proposed answers and NILs) is too small to draw conclusions on the relative quality of selection techniques. Still, consistencies in rank orderings of selection techniques between the two datasets strongly suggest that these methods of system combination are effective.</p><p>It is important to note that in these experiments we did not have access to several useful evidence sources. First, this year's submissions included system estimates on answer confidence, if only implicitly.</p><p>The selection mechanism could take advantage of this by weighting each submitted answer string appropriately. Second, past TRECs show that some systems are reliably more accurate than others, and if each answer string were labeled with a system ID, even if anonymized, we could use system-level features in the selector, such as a simple prior. Given sufficient training, we might even take question features into account, learning that certain systems are better at certain types of questions. We would like to pursue the use of these and other evidence sources in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis of Questions and Answers</head><p>In this section, we report on a number of analyses we have performed, both on Qanda and on all-system results from previous TRECs. We describe the features Qanda extracts from questions, and evaluate its performance on one of these. We also describe a detailed categorization of the TREC-9 answer corpus with respect to semantic and syntactic types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question analysis in Qanda</head><p>Phrag, our HMM-based tagger, first annotates questions using separate models for part-of-speech and named entities. Qanda also runs a simple lookupbased tagger that maps head words to answer types in Qanda's ontology using a set of approximately 6000 words and phrases, some extracted semi-automatically from WordNet, some identified by hand. Based on these annotations, Qanda's main question analysis component uses a parser with a simple hand-optimized grammar to identify the following aspects of each question:</p><p>• Answer type: a type in Qanda's (rather simple) ontology, e.g., PERSON or COUNTRY.</p><p>• Answer restriction: an open-domain phrase from the question that describes the entity being sought, e.g., first woman in space.</p><p>• Salient entity: What the question is "about". Typically a named entity, this corresponds roughly to the classical notion of topic discussed below, e.g., Matterhorn in What is the height of the Matterhorn?</p><p>• Geographical restriction: Any phrase that seems to restrict the question's geophysical domain, e.g., in America.</p><p>• Temporal restriction: Any phrase that similarly restricts the relevant time period, e.g., in the nineteenth century.</p><p>As part of our post-TREC analysis, we have begun to examine how well Qanda performs on these various aspects. One way of evaluating this is to create an annotated test set of questions, tagged with the "correct" result, and score Qanda against this. For example, we might annotate When did the art of quilting begin?, with the answer type LOCATION-if Qanda's prediction matches this, its question analysis was correct in this instance. However, there is another approach to this evaluation. As described in the next section, we have annotated the TREC-9 answer key with semantic types, and so one might ask how often the system predicts one of the actual answer types, according to the answer key. For our example question, medieval Europe-a LOCATION answerwas judged to be correct by the TREC assessors. Had this been the only correct answer found, Qanda's prediction would be counted wrong, under the analysis we describe here.</p><p>In Figure <ref type="figure" coords="5,362.61,300.70,5.40,11.96">2</ref> we present this analysis in terms of the question phrase used, and as a percentage of all questions in the set. This helps us to see which question types might have the biggest impact on our performance. For example, Qanda does rather well at predicting an answer type for how many questions, but these only constitute 5.44% of the questions in the set.</p><p>On the other hand, of the 29.71% of the set that were unadorned w h a t questions, Qanda's question component performed very poorly. We hope to perform similar evaluations for the other question aspects listed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual answer analysis of the TREC-9 question corpus</head><p>Here we report on an analysis of the answers returned by all systems participating in TREC-9. Our study was done as part of a larger investigation, consisting of two levels: First, to identify Topic and Focus constituents for each question, and second, to characterize the Topic and Focus constituents by referent, and in the case of certain expressions, by grammatical type.</p><p>Before we explain what each of these levels of analysis entailed, we will first establish what we mean by Topic and Focus, as the terms and concepts are often used interchangeably in the Q&amp;A literature. We use the terms Topic and Focus as they are defined in classic formal linguistics, dating back to the mid 19th century (see Hajicova 1984, for an early historical overview) and continuing on to recent times in linguistic schools such as Functional Grammar <ref type="bibr" coords="6,54.00,351.34,69.32,11.96" target="#b4">(Dik et al. 1981</ref>) and generative grammar <ref type="bibr" coords="6,243.03,351.34,54.00,11.96;6,54.00,363.82,23.57,11.96" target="#b14">(Rochemont 1986)</ref>. Variably termed theme/rheme, topic/comment, presupposition/focus, they are defined in discourse theory roughly as follows:</p><p>Topic: The constituent(s) of a sentence identifying given, presupposed, or "old" information at the time of the utterance.</p><p>Focus: The constituent(s) of a sentence identifying what is new to the discourse at the time of the utterance. 5</p><p>In questions, the wh-word is the Focus, and the rest of the question is typically the Topic. The answer to a question is also Focused. Question/Answer pairs have long been used in traditional Topic/Focus research papers to unambiguously illustrate and identify Topic and Focus constituents. E.g., from <ref type="bibr" coords="6,207.06,558.94,69.88,11.96" target="#b4">Dik et al. (1981)</ref>:</p><p>( anticipate the context in which humans would be entering factual questions into computers "out of the blue." However, since TREC has yet to intentionally introduce questions with false presuppositions, in our analysis we assumed the presuppositions were true and considered them Topic constituents.</p><p>Returning to the discussion of the analysis of the TREC question set, we identified the Topic and Focus constituents of each question, for example:</p><p>(2) &lt;FOCUS&gt;Who&lt;/FOCUS&gt; &lt;TOPIC&gt;invented the paper clip&lt;/TOPIC&gt;?</p><p>In addition, we used a REF attribute to classify the entity or activity REFerenced by the constituent, where the value for REF comes from an entity/activity ontology, shown in Figure <ref type="figure" coords="6,450.24,249.58,5.40,11.96" target="#fig_0">3</ref> below. For certain expressions, we also used an EXP attribute to identify whether the EXPression is a name, descriptor, or directional phrase. Except for cases requiring a "direction" value (see example 5), EXP is typically only used for classic "Named Entities" such as persons, locations and organizations. Artifacts will also sometimes have an EXP attribute. Here is the previous example with these attributes marked:</p><p>(3) &lt;FOCUS REF="person" EXP="name"&gt; Who&lt;/FOCUS&gt; &lt;TOPIC REF="levin_26_4"&gt; invented the paper clip&lt;/TOPIC&gt;?</p><p>The markup in example 3 identifies the answer to this question as a named person and identifies the Topic of the question as a creation activity (levin_26_4 is the class of create verbs.) The annotation of the Topic constituents in the TREC-9 questions has not been finalized at this time, so in the remainder of this section we will discuss only the results of the Focus tagging.</p><p>In determining the value for REF and EXP in Focus constituents, we looked at the actual answers as recorded in an answer key we developed previously. This answer key<ref type="foot" coords="6,402.72,556.84,3.48,7.70" target="#foot_2">6</ref> was compiled by manually examining all the answers returned by all of the TREC-9 systems. From those judged correct by the TREC assessors, we extracted short answer phrases.</p><p>To perform the Focus analysis, we annotated the answer key itself, rather than the wh-word as shown in example 3, because there are often multiple correct answers to a given question. <ref type="foot" coords="6,439.44,645.40,3.48,7.70" target="#foot_3">7</ref> We tagged each possible answer as a Focus constituent, and applied the correct REF and EXP attributes. For example:</p><p>(4) What is Francis Scott Key best known for? &lt;FOCUS REF="levin_26_7"&gt;penned the national anthem&lt;/FOCUS&gt;; &lt;FOCUS REF="music" EXP="descriptor"&gt;the national anthem&lt;/FOCUS&gt;; &lt;FOCUS REF="music" EXP="name"&gt;Star-Spangled Banner&lt;/FOCUS&gt;</p><p>(5) Where did Woodstock take place? &lt;FOCUS REF="city" EXP="name"&gt;Bethel &lt;/FOCUS&gt;; &lt;FOCUS REF="city" EXP="direction"&gt; 50 miles from Woodstock&lt;/FOCUS&gt; Metonyms, dangling modifiers, and similar expressions can occur as answer phrases, creating the difficulty that the literal interpretation out of context, versus the intended referent within the given context, may be distinct. Thus, a third attribute, LITREF, identifies the entity or activity referred to by the phrase in isolation. REF is used for the intended referent in the context of the question. For example:</p><p>(6) What is the most common cancer? &lt;FOCUS REF="disease"&gt;skin cancer&lt;/FOCUS&gt;; &lt;FOCUS REF="disease" LITREF="body_part"&gt; skin&lt;/FOCUS&gt; (7) Name an American made motorcycle. &lt;FOCUS REF="vehicle" LITREF="organization"&gt; Harley-Davidson&lt;/FOCUS&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question corpus analyses</head><p>We took the annotation of the answer key and collapsed all identically tagged answers in order to identify the set of unique answer types associated with each question. We consider an answer type "unique" if it differs by all three attributes (REF, LITREF, and  . Thus an answer of type PERSON NAME is considered distinct from answer of type PERSON DESCRIPTOR. We also categorized each question by its w h -phrase (question phrase) to provide a rudimentary form of question typing. Some of the patterns that emerged are presented and discussed below.</p><p>Figure <ref type="figure" coords="8,86.82,275.26,5.40,11.96">4</ref> shows the range of answer entities/activities associated with the major question types in TREC-9. The what questions exhibit the highest number of different answer types ( <ref type="formula" coords="8,165.79,313.18,8.55,11.96">63</ref>), but only 14.72% of the individual what questions have more than one answer type. This is because, although what questions have as their foci a broad range of entities/activities, each individual question is typically concerned with only a particular entity or activity. For example What is platinum? has four different answer phrasings, but they all refer to an entity of type SOLID.</p><p>In contrast, the where questions utilize only 13 answer types, but 68.33% of the where questions have more than one answer type. This is largely explained by the range of granularity that is acceptable as an answer, where a geological area, country, state, or city can suffice, as well as what we called d i r e c t i o n expressions like 110 miles northwest of New York City.</p><p>Thus the granularity of the entity ontology has an effect here; had we grouped all of these under a single LOCATION category, the number of answer types for where questions would be greatly reduced. Finally, Figure <ref type="figure" coords="9,122.48,322.78,5.40,11.96" target="#fig_1">7</ref> shows the common EXPression types for those questions that can be answered with names. Many answers lack an EXP value because they refer to entities that do not typically bear names. However, the high number of answers with no EXP values also reflects the preliminary nature of this annotation scheme, particularly for the what and name questions. While unambiguous names were marked consistently as such, we were conservative in the use of the DESCRIPTOR value until we could see what entities emerged from the data. In the future, we will be refining the guidelines to make better use of the DESCRIPTOR value, and perhaps expanding EXP to include other values like ADJECTIVE and ADVERB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other analyses of question corpora</head><p>There have been many previous efforts at classifying questions. We mention a few here for comparison purposes. <ref type="bibr" coords="9,106.72,551.74,116.22,11.96" target="#b15">Weischedel et al. (2002)</ref> reported on an analysis of the combined questions of TREC-8, 9 and 10. They found a prevalence of people, locations, countries/cities/states, and definitions. Their cumulative results for all three TRECs are not directly comparable to what we've reported here, due to differences in the ontologies used, and also because our analysis is based on an examination of the answers rather than the questions. <ref type="bibr" coords="9,178.28,652.78,85.42,11.96" target="#b10">Hovy et al. (2000)</ref> use an ontology similar to the one in Figure <ref type="figure" coords="9,219.36,665.50,4.07,11.96" target="#fig_0">3</ref>. But where our ontology is used to characterize the Topic and Focus constituents, theirs represents the user's intention in asking the question, so that the ontology includes categories like Why-Famous. Thus, similar-looking tactics can have very different underlying approaches; One future goal is to apply multiple approaches to the same corpus, for a richer understanding of questioning and answering phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>As well as the requisite description of this year's system architecture, we have discussed some preliminary work on log-linear models for answer selection and confidence estimation. We would like to pursue this further, using more features and more sophisticated models. We also presented promising initial results on question answering system combination-we will be exploring this further, hopefully making use of system-specific priors as well as confidence information in the answer selection.</p><p>We analyzed the TREC-9 answer corpus and examined the output of Qanda's question processing component with respect to those questions. This indicated some mismatches between the system's expectations about answer types and the actual answers found in TREC-9. We hope to remedy these problems, as well as subject other system components to such scrutiny. We would also like to analyze the TREC-11 answers in a like manner.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPression</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,170.64,469.66,271.00,11.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Entity and activity ontology for question analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,151.20,145.66,311.22,11.96"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Expression types for selected question types (percentages)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,71.28,55.90,425.88,185.96"><head>Answer selection results (percentages, best results in bold)</head><label></label><figDesc></figDesc><table coords="4,71.28,55.90,425.88,185.96"><row><cell></cell><cell cols="3">Dev Set (100 Qs)</cell><cell></cell><cell cols="2">Test Set (400 Qs)</cell></row><row><cell></cell><cell>Strict</cell><cell></cell><cell>Loose</cell><cell></cell><cell>Strict</cell><cell></cell></row><row><cell></cell><cell>P</cell><cell>avgP</cell><cell>P</cell><cell>avgP</cell><cell>P</cell><cell>avgP</cell></row><row><cell>exact string match</cell><cell>50</cell><cell>70</cell><cell>54</cell><cell>74</cell><cell>42</cell><cell>65</cell></row><row><cell>word set</cell><cell>54</cell><cell>75</cell><cell>58</cell><cell>78</cell><cell>46</cell><cell>68</cell></row><row><cell>word bag</cell><cell>54</cell><cell>75</cell><cell>58</cell><cell>78</cell><cell>46</cell><cell>68</cell></row><row><cell>character set</cell><cell>51</cell><cell>65</cell><cell>57</cell><cell>67</cell><cell>46</cell><cell>62</cell></row><row><cell>character bag</cell><cell>60</cell><cell>81</cell><cell>64</cell><cell>85</cell><cell>50</cell><cell>74</cell></row><row><cell>word bag w/ doc priors</cell><cell>66</cell><cell>83</cell><cell>74</cell><cell>88</cell><cell>51</cell><cell>72</cell></row><row><cell>character bag w/ doc priors</cell><cell>64</cell><cell>81</cell><cell>69</cell><cell>86</cell><cell>50</cell><cell>72</cell></row><row><cell>5-character bag w/ doc priors, weighted numeric strings</cell><cell>66</cell><cell>85</cell><cell>76</cell><cell>90</cell><cell>53</cell><cell>73</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,80.16,55.18,478.31,672.20"><head>no answer) label in Figure 5 reflects Question Phrase who what when where how name</head><label></label><figDesc>As stated above, we consider answer types unique if the form of the answer (EXP= name, descriptor, or direction) differs. However, for individual questions, it is not very common to have answer types that differ only by the expression form. Where questions, which can have three values for EXP, exhibit the most cases of this: of the 60 where questions, nine (15%) have duplicate REF values but unique EXP values. For example, Where are diamonds mined? is answered variously by country name, country descriptor, geological name, and geological direction. W h o questions come in second, but fairly low; of the 102 who questions, eight (8%) have answer types that differ only by EXP (person name and person descriptor). Of the 231 what questions, only two have both organization name and organization descriptor, and only one has both person name and person descriptor.Figure5shows the top ten answer types for what questions, and Figure6does the same for where questions. The (</figDesc><table coords="8,80.16,88.30,456.54,639.08"><row><cell></cell><cell>Number of questions 102</cell><cell>231</cell><cell>40</cell><cell>60</cell><cell>48</cell><cell>15</cell></row><row><cell></cell><cell>Number of answer types 8</cell><cell>63</cell><cell>2</cell><cell>13</cell><cell>12</cell><cell>13</cell></row><row><cell cols="2">Average number of answer types per question 1.19</cell><cell>1.19</cell><cell>1.03</cell><cell>2.57</cell><cell>1.02</cell><cell>1.60</cell></row><row><cell cols="3">Percentage of questions with more than one answer type 16.67 14.72</cell><cell cols="2">2.50 68.33</cell><cell cols="2">2.08 33.33</cell></row><row><cell></cell><cell cols="3">Figure 4: Range of answer types by question type</cell><cell></cell><cell></cell></row><row><cell cols="2">questions for which there were no answers in the key,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">because no systems answered them correctly.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">For who questions, 80.17% of the answers were of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">type PERSON, 9.09% were ORGANIZATION, and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.96% had no answer. All but one of the w h e n</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">questions had a DATE answer type-When did the art</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">of quilting begin? had medieval Europe (a GSP) as one</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">possible answer. Name imperatives (see example 7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">above) display a range of foci, but 42% fall into one of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">three categories: VEHICLE (16.67%), ORGAN-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">IZATION (12.5%), and OTHER_LOCATION (12.5%).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Percentage of</cell><cell></cell><cell></cell><cell cols="3">Percentage of</cell></row><row><cell>Answer Type</cell><cell>what questions</cell><cell cols="2">Answer Type</cell><cell cols="3">where questions</cell></row><row><cell>organization</cell><cell>11.64</cell><cell cols="2">city</cell><cell cols="2">19.48</cell></row><row><cell>person</cell><cell>8.73</cell><cell cols="2">country</cell><cell cols="2">18.83</cell></row><row><cell>animal</cell><cell>6.18</cell><cell cols="2">geological</cell><cell cols="2">18.18</cell></row><row><cell>artifact</cell><cell>5.45</cell><cell cols="2">province</cell><cell cols="2">15.58</cell></row><row><cell>date</cell><cell>4.36</cell><cell cols="2">gsp</cell><cell></cell><cell>6.49</cell></row><row><cell>disease</cell><cell>4.36</cell><cell cols="2">other_location</cell><cell></cell><cell>6.49</cell></row><row><cell>(no answer)</cell><cell>3.64</cell><cell cols="2">facility</cell><cell></cell><cell>5.19</cell></row><row><cell>geological</cell><cell>3.64</cell><cell cols="2">recreational</cell><cell></cell><cell>4.55</cell></row><row><cell>quantity</cell><cell>3.64</cell><cell cols="2">organization</cell><cell></cell><cell>2.60</cell></row><row><cell>city</cell><cell>3.27</cell><cell cols="2">body_part</cell><cell></cell><cell>0.65</cell></row><row><cell cols="2">Figure 5: Top ten what-question</cell><cell cols="5">Figure 6: Top ten where-question</cell></row><row><cell cols="2">answer types</cell><cell></cell><cell cols="2">answer types</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,57.12,688.43,240.00,10.89;2,54.00,699.95,243.13,10.89;2,54.00,711.47,243.19,10.89;2,54.00,722.75,55.52,10.89"><p>Currently this is done using a simple hand-built table, but with sufficient training data, we expect to use the log-linear model described below to acquire weights for most sensible pairs of types.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,318.24,688.43,240.17,10.89;2,315.12,699.95,243.09,10.89;2,315.12,711.47,243.15,10.89;2,315.12,722.75,162.65,10.89"><p>The normalized values are computed so that the intuitively "best" feature value is 1, the worst 0-this is primarily for the developers' convenience, but also so weights are all positive, and more easily reasoned about.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="6,318.24,678.35,208.87,10.89"><p>See http://trec.nist.gov/data/qa/add_qaresources.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="6,318.24,691.79,240.16,10.89;6,315.12,703.31,243.05,10.89;6,315.12,714.59,243.26,10.89;6,315.12,726.35,119.23,10.89"><p>Multiple answers are due to two factors: different phrasings of the same correct answer and completely different correct answers. We did not distinguish between these two factors in our analysis of the answers.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* This work was supported in part by the <rs type="funder">Advanced Research and Development Activity (ARDA)</rs> under contract number contract <rs type="grantNumber">F19628-99-C-0001</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mQxEfTF">
					<idno type="grant-number">F19628-99-C-0001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,54.00,76.06,194.44,11.96;10,54.00,88.78,237.29,11.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,217.47,76.06,30.98,11.96;10,54.00,88.78,81.79,11.96">Jakarta Lucene-Overview</title>
		<ptr target="http://jakarta.apache.org/lucene/" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Apache Software Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,107.26,207.07,11.96;10,54.00,119.98,223.83,11.96;10,54.00,132.70,204.89,11.96;10,54.00,145.42,160.20,11.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,127.27,119.98,150.56,11.96;10,54.00,132.70,43.74,11.96">Statistical named entity recognizer adaptation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">C</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">T</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,111.88,132.70,147.01,11.96;10,54.00,145.42,119.85,11.96">Proceedings of the Conference on Natural Language Learning</title>
		<meeting>the Conference on Natural Language Learning<address><addrLine>Taipei</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,163.90,226.86,11.96;10,54.00,176.62,219.60,11.96;10,54.00,189.34,219.89,11.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,219.18,163.90,61.69,11.96;10,54.00,176.62,88.25,11.96">Qanda and the Catalyst architecture</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Mardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,153.25,176.62,120.34,11.96;10,54.00,189.34,215.26,11.96">AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,207.82,227.50,11.96;10,54.00,220.54,233.12,11.96;10,54.00,233.26,208.47,11.96;10,54.00,245.74,229.79,11.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,90.00,220.54,197.12,11.96;10,54.00,233.26,133.17,11.96">A machine-learning approach to introspection in a question answering system</title>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,199.17,233.26,63.30,11.96;10,54.00,245.74,225.19,11.96">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,264.46,225.09,11.96;10,54.00,277.18,229.60,11.96;10,54.00,289.90,208.81,11.96;10,54.00,302.38,214.28,11.96;10,54.00,315.10,195.86,11.96;10,54.00,327.82,74.46,11.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,60.00,289.90,156.39,11.96">On the typology of focus phenomena</title>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Dik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><forename type="middle">E</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">R</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sei</forename><surname>Ing Djiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harry</forename><surname>Stroomer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,83.69,315.10,160.51,11.96">Perspectives on Functional Grammar</title>
		<editor>
			<persName><forename type="first">Teun</forename><surname>Hoekstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Harry</forename><surname>Van Der Hulst</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Moortgat</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Foris</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,346.30,235.88,11.96;10,54.00,359.02,187.54,11.96" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m" coord="10,54.00,359.02,93.62,11.96">Pattern Classification</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,377.74,232.92,11.96;10,54.00,390.46,226.54,11.96;10,54.00,402.94,228.27,11.96;10,54.00,415.66,213.54,11.96;10,54.00,428.38,43.20,11.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,177.35,377.74,109.57,11.96;10,54.00,390.46,226.54,11.96;10,54.00,402.94,137.45,11.96">A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER)</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,217.15,402.94,65.11,11.96;10,54.00,415.66,208.71,11.96">Proceedings of the European Conference on Speech Technology</title>
		<meeting>the European Conference on Speech Technology</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,446.86,228.97,11.96;10,54.00,459.58,222.62,11.96;10,54.00,472.30,175.78,11.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,151.29,446.86,131.68,11.96;10,54.00,459.58,212.94,11.96">Efficient methods for multiple sequence alignment with guaranteed error bounds</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Gusfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,54.00,472.30,142.44,11.96">Bulletin of Mathematical Biology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,490.78,207.00,11.96;10,54.00,503.50,220.80,11.96;10,54.00,516.22,185.09,11.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,152.59,490.78,68.07,11.96">Topic and focus</title>
		<author>
			<persName coords=""><forename type="first">Eva</forename><surname>Hajicová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,119.37,503.50,155.43,11.96;10,54.00,516.22,180.01,11.96">Contributions to Functional Syntax, Semantics, and Language Comprehension</title>
		<editor>
			<persName><forename type="first">Jan</forename><surname>Horecky</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,534.94,220.24,11.96;10,54.00,547.42,203.78,11.96;10,54.00,560.14,87.65,11.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,171.59,534.94,102.64,11.96;10,54.00,547.42,114.50,11.96">Exploiting Diversity for Natural Language Parsing</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">C</forename><surname>Henderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="10,54.00,578.86,236.15,11.96;10,54.00,591.34,225.68,11.96;10,54.00,604.06,210.39,11.96;10,54.00,616.78,140.06,11.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,182.51,591.34,97.17,11.96;10,54.00,604.06,54.56,11.96">Question answering in Webclopedia</title>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Junk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,135.36,604.06,129.03,11.96;10,54.00,616.78,92.10,11.96">Proceedings of the Ninth Text Retrieval Conference</title>
		<meeting>the Ninth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>TREC-9</note>
</biblStruct>

<biblStruct coords="10,54.00,635.26,220.88,11.96;10,54.00,647.98,231.26,11.96;10,54.00,660.70,224.43,11.96;10,54.00,673.42,50.69,11.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,88.93,647.98,186.95,11.96">IBM&apos;s statistical question answering system</title>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<idno>TREC-10</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,54.00,660.70,224.43,11.96">Proceedings of the Tenth Text Retrieval Conference</title>
		<meeting>the Tenth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,54.00,691.90,192.05,11.96;10,54.00,704.62,234.00,11.96;10,54.00,717.34,75.95,11.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,134.69,691.90,111.36,11.96;10,54.00,704.62,181.46,11.96">English Verb Classes and Alternations: A Preliminary Investigation</title>
		<author>
			<persName coords=""><forename type="first">Beth</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,315.12,76.06,236.77,11.96;10,315.12,88.78,210.05,11.96;10,315.12,101.26,184.80,11.96;10,315.12,113.98,76.80,11.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,347.06,88.78,178.11,11.96;10,315.12,101.26,93.36,11.96">Analyses for elucidating current question answering technology</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,420.39,101.26,79.53,11.96;10,315.12,113.98,53.41,11.96">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,315.12,132.70,243.35,11.96;10,315.12,145.42,176.16,11.96" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,464.40,132.70,94.07,11.96;10,315.12,145.42,39.64,11.96">Focus in Generative Grammar</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rochemont</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>John Benjamins</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,315.12,163.90,207.06,11.96;10,315.12,176.62,219.03,11.96;10,315.12,189.34,225.68,11.96;10,315.12,201.82,231.00,11.96;10,315.12,214.54,194.99,11.96;10,315.12,227.26,87.63,11.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,350.04,189.34,190.75,11.96;10,315.12,201.82,90.27,11.96">Answering questions through understanding and analysis (AQUA)</title>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ada</forename><surname>Brunstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Granville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.98,201.82,112.13,11.96;10,315.12,214.54,189.83,11.96">notebook from AQUAINT R&amp;D Program Phase 1 Mid-Year Workshop</title>
		<meeting><address><addrLine>Monterey CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">2002. June</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
