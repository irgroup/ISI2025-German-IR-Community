<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.47,164.54,317.93,15.68">TREC Feature Extraction by Active Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.55,197.18,52.77,10.87"><forename type="first">J</forename><surname>Vendrig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mediamill/ISIS</orgName>
								<orgName type="institution" key="instit1">Informatics Institute</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.51,197.18,70.64,10.87"><forename type="first">J</forename><surname>Den Hartog</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Mediamill/TNO-TPD</orgName>
								<orgName type="institution" key="instit2">Netherlands Organisation for Applied Scientific Research</orgName>
								<address>
									<addrLine>Stieltjesweg 1</addrLine>
									<postCode>2628 CK</postCode>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.35,197.18,81.57,10.87"><forename type="first">D</forename><surname>Van Leeuwen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">TNO-Human Factors</orgName>
								<orgName type="institution" key="instit2">Netherlands Organisation for Applied Scientific Research</orgName>
								<address>
									<addrLine>Kampweg 5</addrLine>
									<postCode>3769 ZG</postCode>
									<settlement>Soesterberg</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,146.38,211.13,44.40,10.87"><forename type="first">I</forename><surname>Patras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mediamill/ISIS</orgName>
								<orgName type="institution" key="instit1">Informatics Institute</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.35,211.13,76.55,10.87"><forename type="first">S</forename><surname>Raaijmakers</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Mediamill/TNO-TPD</orgName>
								<orgName type="institution" key="instit2">Netherlands Organisation for Applied Scientific Research</orgName>
								<address>
									<addrLine>Stieltjesweg 1</addrLine>
									<postCode>2628 CK</postCode>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.50,211.13,57.89,10.87"><forename type="first">J</forename><surname>Van Rest</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Mediamill/TNO-TPD</orgName>
								<orgName type="institution" key="instit2">Netherlands Organisation for Applied Scientific Research</orgName>
								<address>
									<addrLine>Stieltjesweg 1</addrLine>
									<postCode>2628 CK</postCode>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,413.96,211.13,46.13,10.87"><forename type="first">C</forename><surname>Snoek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mediamill/ISIS</orgName>
								<orgName type="institution" key="instit1">Informatics Institute</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.51,225.08,59.43,10.87"><forename type="first">M</forename><surname>Worring</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mediamill/ISIS</orgName>
								<orgName type="institution" key="instit1">Informatics Institute</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.47,164.54,317.93,15.68">TREC Feature Extraction by Active Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DFE85D350868020FF2A94A98FE3F7E80</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current multimedia retrieval research can be divided roughly into two camps. One camp is looking for the panacea which solves all problems in one system. The other camp focuses on very specific problems in restricted domains. In our opinion, the answer lies in the middle. A system should not desire to solve all problems, but should take advantage of a user's knowledge about his or her specific problem, so that the system can focus on it. On the other hand, available video analysis techniques should be extended to other domains which possibly were not envisioned upon their design. The challenge is transparent application of video analysis techniques to the appropriate user domains.</p><p>A user, especially an expert, has best knowledge about the characteristics of a particular domain. In this paper the user's input is given at index-time, rather than at query-time as done in our TREC 2001 contribution <ref type="bibr" coords="1,420.31,504.84,9.94,9.96" target="#b1">[2]</ref>. In <ref type="bibr" coords="1,449.70,504.84,9.94,9.96" target="#b1">[2]</ref>, we associated user queries with video content descriptors via general Wordnet concepts. For example, query term woman maps to Wordnet hypernyms "person, individual, human" which we associated with the "face presence" descriptor. In this TREC 2002 contribution, we focus on building models for the association of content descriptors with generic concepts, such as the Wordnet hypernyms. Specifically, we focus on the ten generic concepts given by the TREC feature extraction task <ref type="foot" coords="1,189.24,587.75,3.97,6.37" target="#foot_0">1</ref> . User and machine interact in order to map the semantic feature concept to content descriptors for a training set, so that shots can be classified for use in retrieval applications.</p><p>In this paper, we assume that every feature model is specific to not only the domain, but even to a collection, in order to exploit domain characteristics and user domain knowledge. The use of a small number of broadly applicable features for video content classification is described by IBM <ref type="bibr" coords="2,401.49,139.35,15.48,9.96" target="#b10">[11]</ref> in last year's TREC, showing relatively good results. Their probabilistic system mixes the use of models for general features (e.g. outdoors and face) with models for domain-specific features (e.g. rocket and fire). In our approach, every model is collection-specific. That is, even a general feature is considered to be specific. For example, although the outdoors semantic concept can be found in many video collections, its visual representation in a video may be quite different. Footage of the Discovery Channel shows a different kind of outdoors sceneries than television sitcoms. The old instructional videos for school kids in the TREC 2002 collection are quite different from the videos shown at school to the MTV-and Nintendo-generations. In addition, a feature can be defined differently amongst domains, resulting in the need for different models as well. For example, the definition of the face feature for a Cartoon Network collection is different from the one for C-SPAN.</p><p>Focusing on one particular collection enables for use, or some might say abuse, of simple content descriptors that are correlated to a semantic concept. This may be caused by the style of one or more people in the film crew, such as director, editor and camera man. An example of specific collection characteristics is found in the TREC 2001 video collection. The camera movement descriptor could be used to classify shots as mountains, as they roughly correspond with camera pans. Although such a classification method cannot be generalized to other collections, it allows for uncomplicated retrieval of videos in a specific collection. We present a system which interactively learns user-defined semantic concepts for a specific collection from a domain expert. For each concept, the domain expert builds a model by feeding visual evidence to the system in the form of examples, without knowledge about the underlying classifier and descriptors. We employ a large set of multimedia descriptors for use in a Maximum Entropy classifier. The space for example selection is determined by the output of the incrementally improved model. The system is evaluated against the TREC 2002 feature extraction collection. The user information consists of the ten semantic concepts defined for the feature extraction task.</p><p>As our system is based on visual evidence, we focus on visual content of videos. That is, we focus on the features outdoors, indoors, face, people, cityscape, landscape, text overlay and monologue. The classification of the audio features (speech and instrumental sound) is provided independently and is described briefly in section 2.2.</p><p>The paper is organized as follows. In section 2 we describe how video content is represented for example selection and classification. In section 3 the use of the Maximum Entropy classifier for multimedia content is described. In section 4 the interactive selection of examples using active learning is explained. In section 5 we describe the experimental setup for TREC evaluation. Results are discussed in section 6. Finally, we present conclusions and future research in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Content representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data representation</head><p>The elementary unit in the context of TREC is a shot. However, the shot itself, i.e. the sequence of frames, is not always a good representation for the visual content. There are two reasons for employing an alternative representation. Firstly, a shot is not necessarily visually and semantically coherent. In <ref type="bibr" coords="3,439.68,205.46,15.48,9.96" target="#b13">[14]</ref> such fractions of a shot are called a shot-let, while in <ref type="bibr" coords="3,354.83,217.41,15.48,9.96" target="#b11">[12]</ref> this division is referred to as named events, which are short segments with a meaning that does not change in time. Division of the shot into smaller coherent fractions allows for better representation of the shot's content. In the context of TREC the further division is especially important, as the reference shot segmentation suffers from undersegmentation, combining consecutive shots into one shot. In addition, division into lower level units prevents loss of information due to aggregation. This is especially important in the context of TREC, as the feature definitions state that a shot is assigned to a class when at least an observable part of the shot belongs to that class. That is, a shot could be assigned to disjuncts concepts, e.g. both outdoors and indoor.</p><p>The second reason is computational feasibility. Using expensive descriptors derived from image processing on each frame in a shot requires a large amount of computing power. Meanwhile, the descriptor values can be expected to be highly similar in consecutive frames, as the content of frames change just gradually within a shot. Therefore it is expected that choosing a representation which requires less computing power does not result in loss of information as a consequence.</p><p>We choose to use content-dependent key frames as representation of a shot for image processing based descriptors. Motion descriptors are calculated on shot level for practical reasons, i.e. compatibility with existing systems. Contentdependent selection of key frames is based on the change in visual content during a shot compared to the change of content in the entire video <ref type="bibr" coords="3,397.58,480.43,9.94,9.96" target="#b6">[7]</ref>. Shots containing a relatively high amount of changes are assigned more key frames. Within a shot, key frames are chosen such that the total amount of change in the surrounding segment is approximately equal for all key frames in the shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Descriptors</head><p>Descriptors describe the content of a shot, or the content of a representation of a shot, in order to enable comparison of the content of two shots. For automatic use by the classifier, we employed a descriptor pool containing over 60 descriptors. The descriptor pool is not geared towards a specific data collection, as the system is designed to be independent of the data collection. The descriptors include atomic descriptors such as color values and color distributions, edge characteristics, and motion descriptors; and complex descriptors such as face presence <ref type="bibr" coords="3,173.11,646.26,15.47,9.96" target="#b9">[10]</ref> and camera movement <ref type="bibr" coords="3,290.93,646.26,9.94,9.96" target="#b0">[1]</ref>. Due to the large amount of descriptors, it cannot be assumed that they are independent. For example, one descriptor (e.g. dominant color) may coexist with a specialization of itself (e.g. dominant color in the top half).</p><p>In the following sections, we describe in more detail the descriptors that relate specifically to the temporal component of video shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Motion</head><p>Motion descriptors are extracted by a two-level analysis. At the block of frames level we analyze the motion by estimating a parametric motion model with a robust regression scheme <ref type="bibr" coords="4,257.17,231.45,9.94,9.96" target="#b0">[1]</ref>. In this way we obtain two types of low-level features. On the one hand we obtain the camera operation (pan, tilt, zoom-in, zoom-out or unknown) and the factors that are related to it (focus of expansion, pan-factor, etc). On the other hand, we obtain descriptors such as the average motion, the percentage of outliers from the dominant motion model, and the average position and motion of the outliers.</p><p>At the shot level, the descriptors of the block of frames level are combined for the estimation of descriptors such as the average pan factor in the shot, the motion activity due to camera operations and the percentage of frames in which the camera zooms in. The shot level descriptors are used for classification of the shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Audio</head><p>The classifications for the two audio features (speech and music) are provided independently by TNO-Human Factors. Classification is done without prior knowledge about the data set.</p><p>The speech/music discrimination is based on amplitude variation in the audio's signal envelope shape. Generally, speech has higher amplitude variations than music in the spectral regions around 475 and 2700 Hz. The input signal is partitioned into 1 second segments. For each segment, the amplitude fluctuations in these bands are determined and they are low pass filtered at 8 Hz. When the amplitude variation in either of the two spectral bands is above a certain threshold, the segment is identified as speech, otherwise as music. A third category silence is used if the total acoustic energy is low. The thresholds are based on values found for Dutch radio and television broadcast material, as well as eight music CD tracks with various music styles (classical instrumental, vocal, pop and jazz).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Classifier</head><p>The concept classifier used to model features has to deal with three issues concerning the data set and the descriptors. Firstly, the classifier has to cope with descriptors that are undefined for a shot. For example, when there is no pan camera movement in a shot, the pan factor descriptor is undefined. Secondly, in contrast with <ref type="bibr" coords="4,194.05,653.29,14.59,9.96" target="#b10">[11]</ref>, it is our opinion that the classifier cannot assume descriptors are independent. Independence is not expected in multimedia objects, such as video shots, because they comprise several correlated information sources. In addition, the use of a large descriptor pool leads to interdependencies of descriptors. Thirdly, the classifier has to cope with an imbalanced data set. Just a relatively small number of positive examples is available in the training set. Training should focus on the positive examples, since the given features are one class problems <ref type="bibr" coords="5,199.76,187.17,14.58,9.96" target="#b14">[15]</ref>.</p><p>For classification we choose the Maximum Entropy classifier, as it deals with the above issues. Firstly, it makes use of sparse vector format, thereby dealing with missing descriptor values. It is not necessary to provide dummy values for undefined descriptors. Secondly, the classifier assumes no independence between descriptors, in contrast to classifiers as Naive Bayes <ref type="bibr" coords="5,366.66,246.94,9.94,9.96" target="#b7">[8]</ref>. Thirdly, we found in experiments that a Maximum Entropy classifier is less sensitive to the majority effect than a Support Vector classifier <ref type="bibr" coords="5,307.51,270.85,9.94,9.96" target="#b7">[8]</ref>. Hence it is well suited for a data collection containing few positive examples.</p><p>Maximum Entropy classification has been applied successfully in a variety of domains, including the area of statistical natural language processing where it achieved state-of-the-art performance <ref type="bibr" coords="5,308.50,318.67,9.94,9.96" target="#b3">[4]</ref>. The Maximum Entropy framework was originally proposed by Jaynes <ref type="bibr" coords="5,288.77,330.63,10.50,9.96" target="#b8">[9]</ref> as a means to make inference based on partial information. Jaynes claimed that "the only unbiased assignment one could make, should use the probability distribution which has maximum entropy subject to whatever is known", i.e. the probability distribution keeping the uncertainty maximal.</p><p>The Maximum Entropy approach allows for the use of a large amount of descriptors without the need to specify their relevance for training a specific semantic concept. The relative importance of each descriptor is computed automatically by the Generalized Iterative Scaling (GIS) algorithm <ref type="bibr" coords="5,411.27,426.28,9.94,9.96" target="#b5">[6]</ref>. This makes Maximum Entropy classification generally applicable.</p><p>A general problem for classifiers is diversity in descriptor types. The Maximum Entropy classifier suffers from this problem as well as it makes use of binary trigger descriptors, i.e. either the descriptor is true or it is false/undefined. Our original multimedia descriptors are both categorical and numerical. An example of the former descriptor type is "type of camera movement", which takes values such as "zoom" and "pan". Categorical descriptors can be used directly as a binary trigger. The mapping of numerical descriptor values to binary triggers, however, is non-trivial.</p><p>For further discretization of numerical descriptor values we employ a binning function which maps each value to a categorical representation. The binning process itself does not need to lead to loss of information. For many descriptors, numerical values have a higher precision than is needed or used. For example, there is no significant difference between value 0.10 and 0.101, i.e. more precision does not necessarily lead to better description of the content. It is often sufficient to express a descriptor's value categorically. An example is the use of "mostly orange" or "very orange" to describe a color as done in <ref type="bibr" coords="5,375.99,629.51,14.58,9.96" target="#b12">[13]</ref>. The choice for the number of categorical values (bins) has to be established experimentally.</p><p>The binary trigger descriptor resulting from binning is used by the Maximum Entropy classifier. A disadvantage of binning which is specific to the Maximum Entropy approach is the loss of order. That is, for the classifier bins are not related in any way. Bin 1 is not closer to bin 3 than to bin 9. Even when binning does not lead to loss of information for an individual descriptor value, it does lead to information loss for the similarity between shots.</p><p>The binning function has to take into account that not all descriptor values are normalized. Therefore we choose the use of equal frequency binning, which is performed after descriptor computation for the entire collection. It divides the descriptor values into a fixed number of bins such that each bin contains approximately the same number of values. Equal frequency binning is not sensitive to outlying values and skewed distribution of values over the range. The remaining problem is to choose the number of bins for discretization.</p><p>The implementation used for the Maximum Entropy classification is the publicly available OpenNLP Maximum Entropy Package <ref type="bibr" coords="6,383.74,270.85,9.94,9.96" target="#b2">[3]</ref>. The traditional approach of random sampling is often chosen to acquire examples from a collection. However, for the problem at hand random sampling does not seem optimal. Cohn <ref type="bibr" coords="6,267.16,397.35,10.50,9.96" target="#b4">[5]</ref> addresses this issue in general in the context of neural networks, stating that in many formal problems it is more efficient to focus on a region of uncertainty rather than the entire collection. For the specific problem of TREC 2002 feature extraction we found this to be the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interactive teaching</head><p>The reason to use more intelligent example selection than random sampling lies in the nature of the TREC features. Although the features can be perceived as binary (e.g. a shot contains overlay text or not), really the feature can be defined by positive examples only as the scope of negative examples is too broad. Therefore, focus on finding positive examples is needed.</p><p>In addition to the relative importance of positive examples, the positive examples are relatively rare in the collection. For example, we estimate 5% of the shots in the training collection contain overlay text. Even when a classifier does not need many positive examples, a sufficiently large set in absolute numbers must be given. Hence, it is more important to find positive examples describing the feature than to find examples representative for the collection.</p><p>To give precedence to labeling of positive examples we apply active learning, which is defined by <ref type="bibr" coords="6,220.92,588.64,10.50,9.96" target="#b4">[5]</ref> as "any form of learning in which the learning program has some control over the inputs it trains on". That is, the classifier controls which examples are presented to the teacher for judgement.</p><p>Because of our focus on positive examples, for employment of active learning in our system we use a variant on the region of uncertainty described in <ref type="bibr" coords="6,463.90,636.45,9.94,9.96" target="#b4">[5]</ref>. Instead of presenting the teacher shots of which classification is most uncertain, we present shots for which labeling is most important, i.e. the shots most likely to be positive examples. This way, the teacher does not just provide examples, but indirectly he or she gives feedback on the classifier.</p><p>Theoretically, the active learning approach described could lead to undersampling of negative examples. However, in practice this would occur only when the model is very good from the start, or when positive examples are abundant in the collection. Both cases are unlikely in the context of the TREC feature extraction task.</p><p>Ideally, all presented examples would be classified by the teacher as either positive or negative. In practice, we have to introduce an "unclassified" category for two reasons. The first reason is that some shots do not contain sufficient information to be classified unambiguously. That is, the feature value cannot be determined without speculation or knowledge about the context. The second reason relates to definitions. The definitions given for the TREC experiment do not always match with the intuitive classification for a feature. This problem is not specific to TREC, but would occur in any large data collection for a broad domain, especially when there is more than one teacher. In cases where intuition and strict definition clash, the teacher uses the "unclassified" label. In terms of the classifier it means the model is not trained on "unclassified" examples, and we have no opinion on the outcome of the classifier for such examples when applied to the test collection.</p><p>We use the i-Notation system described in <ref type="bibr" coords="7,346.46,366.50,14.58,9.96" target="#b15">[16]</ref>, extended with access to the OpenNLP Maximum Entropy Package <ref type="bibr" coords="7,329.78,378.45,10.50,9.96" target="#b2">[3]</ref> for selecting possibly positive examples. It is depicted in figure <ref type="figure" coords="7,280.44,390.41,3.87,9.96" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup</head><p>In this section we describe briefly the most important parameters influencing the experiment. Key frames are selected as described in section 2.1, with a minimum of 2 key frames per shot and an average (per video) of 1 key frame every 2 seconds.</p><p>The amount of bins is fixed to 4 for all numeric descriptors in the pool. Experiments with other bin amounts in the range of 3 . . . 10 showed no significant impact on results for the training set.</p><p>As the Maximum Entropy classifier's decision is binary for each feature, the results consists of shots for which a positive decision with a likelihood ≥ 0.5 is found.</p><p>The confidence of the existence of an audio feature (speech or music) within a video shot is computed as the number of segments classified as the feature normalized over the total shot length. The confidences of all shots are ranked and cut off at the given maximum of 1000 shots.</p><p>The two runs are different for the eight visual features only. The two aural features are constant. In the second run, we add the confidence measures for the aural features as descriptors used to classify the eight visual features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and discussion</head><p>The likelihood threshold on the classifier decision confidence has a large impact on the evaluation, as the results lists are smaller than the maximum of 1000 shots for evaluation. The results lists vary from 744 shots for outdoors to 4 for landscape. The overall TREC results show that many more shots with the same feature are available in the collection, indicating that our threshold is too high.</p><p>Employment of a large descriptor pool does not have a negative impact on results according to a comparative experiment. We estimated results for the face feature using the "face presence" descriptor only, which is obviously a very powerful descriptor for this feature. Although adding the other descriptors do not lead to better classification results, they do not corrupt the classification either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and future research</head><p>The use of active learning in combination with the Maximum Entropy classifier leads to a generic approach for feature classification applying to a specific collection. The results of classification for the eight visual TREC features are not satisfactory. This may be due to the heterogeneity of the TREC collection, which is composed of several semantically unrelated collections. Although our approach should be able to cope with such a collection as well, the active learning component should be designed to take the heterogeneity into account. That is, the active learning component should take examples from all sub-collections for labeling by the user, thereby avoiding a local optimum.</p><p>The threshold used to determine positive decisions is too high. In the context of TREC evaluation, it would be better to rank all decision confidence values. Further research is needed to find out whether a ranking approach conflicts with the theory on which the Maximum Entropy classifier and its implementation are based.</p><p>The use of a large descriptor pool does not confuse the classifier when one very powerful, specific descriptor for a feature is present, as in the case of the face feature. Therefore, in future experiments we intend to use more descriptors rather than less. However, an automatic mechanism for selecting relevant descriptors from the pool for a particular feature is desired to be more robust against noise. Although initial experiments on the video data set do not yet show significant effects, we found selection and combination of descriptors to be useful in other Maximum Entropy applications.</p><p>The most important future research theme for use of Maximum Entropy classification in multimedia is the binning function. The effect of variations on the current binning function need to be measured. Examples of variations are small versus large amount of bins, determination of amount of bins for each descriptor individually, and using overlapping bins to deal with border values.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,133.77,325.62,343.39,9.96;6,133.77,337.58,343.40,9.96;6,133.77,349.53,343.37,9.96;6,133.77,361.48,199.02,9.96"><head></head><label></label><figDesc>As all learning classifiers, Maximum Entropy suffers from the need to select training examples from the collection. It requires a great deal of human effort to label the examples. Minimizing the human effort without compromising the quality of the examples is an important issue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,133.77,377.41,343.38,9.96;8,133.77,389.36,56.91,9.96;8,133.77,125.18,254.98,237.78"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Screendump of the i-Notation system extended with active learning functionality.</figDesc><graphic coords="8,133.77,125.18,254.98,237.78" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,149.01,653.84,328.18,7.24;1,133.77,663.31,76.06,7.24"><p>In the remainder of the paper we follow the TREC terminology, referring to the semantic concepts as features.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors wish to thank <rs type="person">Jan Baan</rs> for providing camera work techniques and <rs type="person">Thang Viet Pham</rs> for the face presence detection.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,154.24,499.51,322.94,9.96;9,154.25,511.46,22.65,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,200.18,499.51,111.28,9.96">Camera techniek detectie</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,403.74,499.51,18.86,9.96">TNO</title>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="9,154.24,530.89,322.97,9.96;9,154.25,542.84,322.94,9.96;9,154.25,554.80,322.93,9.96;9,154.25,566.76,322.91,9.96;9,154.25,578.71,322.89,9.96;9,154.25,590.66,22.65,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,362.30,554.80,114.88,9.96;9,154.25,566.76,161.22,9.96">Lazy users and automatic video retrieval tools in (the) lowlands</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Ballegooij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hartog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>List</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Raaijmakers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Todoran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vendrig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,336.63,566.76,140.52,9.96;9,154.25,578.71,114.95,9.96">Proceedings of the 10th Text Retrieval Conference (TREC)</title>
		<meeting>the 10th Text Retrieval Conference (TREC)<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11">November 2001</date>
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,154.24,610.08,322.95,9.96;9,154.25,622.04,199.56,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,339.04,610.08,138.15,9.96;9,154.25,622.04,31.91,9.96">The opennlp maximum entropy package</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bierner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>SourceForge</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="9,154.24,641.47,322.97,9.96;9,154.25,653.42,322.91,9.96;9,154.25,665.38,79.03,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,383.86,641.47,93.35,9.96;9,154.25,653.42,185.93,9.96">A maximum entropy approach to natural language processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Della Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,357.45,653.42,115.58,9.96">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,127.39,322.95,9.96;10,154.25,139.35,244.23,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,343.29,127.39,133.90,9.96;10,154.25,139.35,62.18,9.96">Improving generalization with active learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,225.04,139.35,76.59,9.96">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="221" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,159.27,322.94,9.96;10,154.25,171.22,309.30,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,292.09,159.27,185.09,9.96;10,154.25,171.22,28.23,9.96">Generalized iterative scaling for log-linear models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">N</forename><surname>Darroch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,191.62,171.22,165.83,9.96">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,191.15,322.95,9.96;10,154.25,203.11,322.94,9.96;10,154.25,215.06,322.90,9.96;10,154.25,227.02,59.72,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,206.39,203.11,270.80,9.96;10,154.25,215.06,37.27,9.96">Image and Video Databases: Restoration, Watermarking and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">C</forename><surname>Langelaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M B</forename><surname>Van Roosmalen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Biemond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Lagendijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,252.74,215.06,151.68,9.96">Advances in Image Communication</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2000">2000</date>
			<publisher>Amsterdam</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,246.94,322.96,9.96;10,154.25,258.90,322.87,9.96;10,154.25,270.85,74.06,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,329.06,246.94,148.14,9.96;10,154.25,258.90,26.32,9.96">Statistical pattern recognition: A review</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,189.08,258.90,284.04,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="37" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,290.78,322.92,9.96;10,154.25,302.73,130.19,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,215.23,290.78,194.97,9.96">Information theory and statistical mechanics</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,420.84,290.78,56.32,9.96;10,154.25,302.73,28.19,9.96">The Phyiscal Review</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="620" to="630" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.23,322.66,322.94,9.96;10,154.25,334.62,322.82,9.96;10,154.25,346.57,86.16,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,374.15,322.66,103.03,9.96;10,154.25,334.62,144.49,9.96">Face detection by aggregated bayesian network classifiers</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,306.58,334.62,117.91,9.96">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="461" />
			<date type="published" when="2002-02">February 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.23,366.50,322.96,9.96;10,154.25,378.45,322.92,9.96;10,154.25,390.41,322.90,9.96;10,154.25,402.36,316.50,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,334.53,378.45,142.64,9.96;10,154.25,390.41,139.93,9.96">Integrating features, models, and semantics for trec video retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Ponceleon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,314.04,390.41,163.11,9.96;10,154.25,402.36,85.06,9.96">Proceedings of the 10th Text Retrieval Conference (TREC)</title>
		<meeting>the 10th Text Retrieval Conference (TREC)<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11">November 2001</date>
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.23,422.29,322.95,9.96;10,154.25,434.24,294.18,9.96" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,299.82,422.29,177.36,9.96;10,154.25,434.24,81.45,9.96">Multimodal video indexing: A review of the state-of-the-art</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<imprint>
			<publisher>Multimedia Tools and Applications</publisher>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="10,154.23,454.17,322.90,9.96;10,154.25,466.12,192.17,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,214.10,454.17,263.03,9.96;10,154.25,466.12,27.51,9.96">Automatic indexing and content-based retrieval of captioned images</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,190.78,466.12,68.21,9.96">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.23,486.04,322.93,9.96;10,154.25,498.01,322.90,9.96;10,154.25,509.96,267.94,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,300.50,486.04,176.67,9.96;10,154.25,498.01,239.25,9.96">Determining computable scenes in films and their structures using audio visual memory models</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,415.71,498.01,61.44,9.96;10,154.25,509.96,158.58,9.96">Proceedings of the 8th ACM Multimedia Conference</title>
		<meeting>the 8th ACM Multimedia Conference<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.23,529.88,287.01,9.96" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,211.46,529.88,99.34,9.96">One-class classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>TU Delft</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,154.23,549.81,322.95,9.96;10,154.25,561.76,101.71,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,279.86,549.81,164.18,9.96">Interactive adaptive movie annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vendrig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,453.08,549.81,24.10,9.96;10,154.25,561.76,46.69,9.96">IEEE Multimedia</title>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
