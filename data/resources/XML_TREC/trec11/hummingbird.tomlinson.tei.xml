<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.74,109.17,314.50,18.08;1,195.23,131.09,221.52,18.08;1,112.38,153.00,234.96,18.08;1,347.35,151.23,22.11,12.55;1,376.41,153.00,123.20,18.08">Experiments in Named Page Finding and Arabic Retrieval with Hummingbird SearchServer TM at TREC 2002</title>
				<funder ref="#_pxH546S">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2003-02-07">February 7, 2003</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,264.32,188.07,83.36,10.46"><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
							<email>stephen.tomlinson@hummingbird.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hummingbird Ottawa</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.74,109.17,314.50,18.08;1,195.23,131.09,221.52,18.08;1,112.38,153.00,234.96,18.08;1,347.35,151.23,22.11,12.55;1,376.41,153.00,123.20,18.08">Experiments in Named Page Finding and Arabic Retrieval with Hummingbird SearchServer TM at TREC 2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2003-02-07">February 7, 2003</date>
						</imprint>
					</monogr>
					<idno type="MD5">3A06D0A6877BFDFBA512794C42E9A32B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hummingbird participated in the named page finding task of the TREC 2002 Web Track (find the named page in 18GB from the .GOV domain) and the monolingual Arabic topic relevance task of the TREC 2002 Cross-Language Track (find all relevant documents in 869MB of Arabic news data). In the named page finding task, SearchServer returned the named page in the first 10 rows for more than 80% of the 150 queries. Searching the full document content produced mean reciprocal rank (MRR) scores more than 20 points higher than just searching particular HTML properties (such as the Title), but enhancing a content search with a little extra weight for HTML properties further increased MRR by 6 points (with standard error of just 2 points). Treating queries as phrases was not found to help significantly (on average), but document length normalization increased MRR by more than 20 points. For Arabic topic relevance, light algorithmic stemming increased mean average precision (MAP) by 5 points, use of Arabic stop words increased MAP by 1 point, and query expansion from blind feedback increased MAP by 3 points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hummingbird SearchServer<ref type="foot" coords="1,192.99,510.96,3.97,7.32" target="#foot_0">1</ref> is an indexing, search and retrieval engine for embedding in Windows and UNIX information applications. SearchServer, originally a product of Fulcrum Technologies, was acquired by Hummingbird in 1999. Founded in 1983 in Ottawa, Canada, Fulcrum produced the first commercial application program interface (API) for writing information retrieval applications, Fulcrum r Ful/Text TM . The SearchServer kernel is embedded in many Hummingbird products, including SearchServer, an application toolkit used for knowledge-intensive applications that require fast access to unstructured information.</p><p>SearchServer supports a variation of the Structured Query Language (SQL), SearchSQL TM , which has extensions for text retrieval. SearchServer conforms to subsets of the Open Database Connectivity (ODBC) interface for C programming language applications and the Java Database Connectivity (JDBC) interface for Java applications. Almost 200 document formats are supported, such as Word, WordPerfect, Excel, PowerPoint, PDF and HTML.</p><p>SearchServer works in Unicode internally <ref type="bibr" coords="1,269.94,643.53,10.52,10.46" target="#b4">[5]</ref> and supports most of the world's major character sets and languages. The major conferences in text retrieval evaluation (TREC <ref type="bibr" coords="1,380.47,655.50,14.61,10.46" target="#b9">[10]</ref>, CLEF <ref type="bibr" coords="1,432.52,655.50,10.52,10.46" target="#b1">[2]</ref> and NTCIR <ref type="bibr" coords="1,502.45,655.50,10.79,10.46" target="#b6">[7]</ref>) have provided opportunities to objectively evaluate SearchServer's support for a dozen languages.</p><p>This paper looks at experimental work with SearchServer for named page finding (just one right answer, i.e. a known-item search task) and monolingual Arabic retrieval (find all the relevant documents, i.e. a topic relevance task). All experiments were conducted on a single-cpu desktop system, OTWEBTREC, with a 600MHz Pentium III cpu, 512MB RAM, 186GB of external disk space on one e: partition, running Windows NT 4.0 Service Pack 6. For the submitted runs in July 2002, an internal development build of SearchServer 5.3 was used <ref type="bibr" coords="2,187.32,133.27,50.29,10.46">(5.3.500.264</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Named Page Finding</head><p>The .GOV collection of the TREC 2002 Web Track consists of pages downloaded from the .gov domain of the World Wide Web in early 2002. It was distributed on 7 CDs. We copied the contents of each CD onto a "compressed NTFS" area of OTWEBTREC's e: drive (e:\data\compressed\gov\cd1e:\data\compressed\gov\cd7). The TRANS.TBL files were not considered part of the collection and were removed. The 4613 .gz files comprising the collection were uncompressed (and Windows NT internally recompressed them on the compressed NTFS drive). Uncompressed, the 4613 files consist of 19,455,030,550 bytes (18.1 GB). Based on the change in bytes free on the drive, the files occupied 9,329,721,344 bytes (8.7 GB) on the compressed NTFS drive. Hence, NTFS compression saved about 9.4 GB of space, noticeably less than gzip compression (which saved 13.9 GB). Each file contains on average about 270 "documents", for a total of 1,247,753 documents. The average document size is 15,592 bytes. For more information on the .GOV collection, see <ref type="bibr" coords="2,135.15,301.61,9.96,10.46" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>The custom text reader called cTREC, described in last year's paper <ref type="bibr" coords="2,372.87,350.40,14.61,10.46" target="#b12">[13]</ref>, was enhanced for the named page finding experiments.</p><p>In expansion mode, cTREC, which previously just extracted the DOCNO identifier, was enhanced to support a /H option for extracting the following property information from each .GOV document's header and content for storage in columns 129-140 of the SearchServer table:</p><p>• 129: "non-empty" title, which was filled with the first non-empty value of columns 130-135 (i.e. the title was used if there was one, otherwise the meta title was used if there was one, etc., with the URL used as a last resort).</p><p>• 130: title (text following &lt;TITLE&gt; up to &lt;/TITLE&gt;, if any).</p><p>• 131: meta title (content of the META TITLE tag, if any).</p><p>• 132: meta subject (content of the META SUBJECT tag, if any).</p><p>• 133: meta description (content of the META DESCRIPTION tag, if any).</p><p>• 134: first heading (text following the first occurrence of the &lt;H1&gt;, &lt;H2&gt; or &lt;H3&gt; tag up to its closing tag, if any).</p><p>• 135: URL (which was always included in the DOCHDR section, before the document content).</p><p>• 136: URL type (calculated from the URL as described below).</p><p>• 137: URL depth (calculated from the URL as described below).</p><p>• 138: meta keywords (content of the META KEYWORDS tag, if any).</p><p>• 139: all properties except keywords (i.e. a concatenation of columns 130-135).</p><p>• 140: all properties (a concatenation of columns 139 and 138).</p><p>The URL was truncated at 256 bytes (only 5 were longer), and the other properties were truncated at 1024 bytes.</p><p>The URL type was set to ROOT, SUBROOT, PATH or FILE, based on the convention which worked well last year for the Twente/TNO group <ref type="bibr" coords="3,255.75,109.36,15.50,10.46" target="#b13">[14]</ref> on the entry page finding task (also known as the home page finding task). Our exact rules were as follows. The slash count of the URL was calculated (a count of the '/' characters not including the leading "http://"). The URL was considered of homepage-type if it ended with "/", "/index.html", "/index.htm", "/default.html", "/default.htm", "/default.asp", "/home.html", "/home.htm", "/welcome.html" or "/welcome.htm" (case-insensitive comparisions were used). ROOT was assigned if the URL was of slash count 0 or was a homepage-type URL of slash count 1. SUBROOT was assigned if the URL was a homepage-type URL of slash count 2. PATH was assigned if the URL was a homepage-type URL of slash count 3 or more. FILE was assigned for all other URLs.</p><p>The URL depth was based on the sum of the slash count and the node count, minus one if the URL was of homepage-type. The node count was the count of the dots before the first slash after "http://" (not counting the first dot if the URL began with "http://www.") plus the number of "?", ";" or "#" characters. (As every URL contained ".gov", the URL depth was guaranteed to be at least 1.) For convenience of wildcard searching and readability, the depth was converted to a term as follows: 1 was assigned URLDEPTHA, 2 was assigned URLDEPTHAB, 3 was assigned URLDEPTHABC, etc. with depths greater than 25 treated the same as 25.</p><p>In format translation mode, cTREC was enhanced to support a /q option which resumed indexing at the first quotation mark inside a tag, rather than always waiting for the end of the tag to resume indexing. This option hence would index potentially helpful text such as VALUE fields of INPUT tags and NAME fields of IMG tags, although more noise would also be indexed.</p><p>For the .GOV collection, the documents were assumed to be in Latin-1, and as for the web collections of past years, the /w option of cTREC was used to convert non-ASCII Latin-1 bytes to the ASCII range (if any occurred).</p><p>A SearchServer The DOCNO column was assigned number 128 and the remaining columns were assigned numbers 129-140 to correspond to the properties written by the /H option the cTREC text reader. (The reserved external text column, FT TEXT, which corresponds to the document content, does not need to be specified in the schema.) The mygov.stp stopfile of 99 stop words is a little different from previous years in that it no longer contains single letters or any numbers.</p><p>Into the GOV table, just one row was inserted, specifying the top directory of the data set relative to the basepath: insert into GOV ( ft_sfname, ft_flist ) values ( 'gov', 'cTREC/E/d=128/H:s!cTREC/w/q/@:s');</p><p>To index the GOV table, a Validate Index statement was executed:</p><formula xml:id="formula_0" coords="4,82.46,163.16,177.83,10.46">validate index GOV validate table;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Searching</head><p>For the named page finding task of the Web Track, the 150 "topics" were in a file called "webnamed page topics.1-150.txt". The topics were numbered NP1-NP150, and each contained a description of a page (e.g. "visiting pandas national zoo"). The task was to rank the named page as highly as possible. The topics were assumed to be in the Latin-1 character set, the default on North American Windows systems (though accent-sensitive searching was not enabled for the GOV table ).</p><p>For the submitted hum02pd run of the named page finding task, below is an example SearchSQL query. This query would create a working table with the 2 columns named in the SELECT clause, a REL column containing the relevance value of the row for the query, and a DOCNO column containing the document's identifier. The ORDER BY clause specifies that the most relevant rows should be listed first. The statement "SET MAX SEARCH ROWS 50" was previously executed so that the working table would contain at most 50 rows:</p><formula xml:id="formula_1" coords="4,82.46,353.41,198.75,34.38">SELECT RELEVANCE('V2:3') AS REL, DOCNO FROM GOV WHERE</formula><p>(ALL_PROPS CONTAINS 'visiting pandas national zoo' WEIGHT 1) OR (ALL_PROPS IS_ABOUT 'visiting pandas national zoo' WEIGHT 1) OR (FT_TEXT IS_ABOUT 'visiting pandas national zoo' WEIGHT 10) ORDER BY REL DESC;</p><p>The ALL PROPS column contained all the properties of column 140 (described earlier), e.g. the title and meta description but not most of the document content.</p><p>The CONTAINS predicate does phrase searching, so the listed terms would have to occur adjacently in the specified order (except stop words). "SET PHRASE DISTANCE 4" was previously specified so that there could be up to 4 characters between adjacent terms (plus additional whitespace). By default, the CONTAINS predicate does exact searching (i.e. no stemming), though some normalizations (e.g. case normalization and canonical Unicode) are still done. The motivation for including the query as a phrase was that it seemed the query might often be in the title or other property information of the document (e.g. a query in mind was "Washington State Legislature" (which was not one of the 150 official queries)). The phrase searching was just given one-tenth the weight of content searching for relevance ranking purposes. Experiments on last year's entry page finding task suggested a small weight was helpful (on average) but a strong weight hurt results.</p><p>The IS ABOUT predicate uses SearchServer's Intuitive Searching, described in last year's paper <ref type="bibr" coords="4,509.81,590.53,14.61,10.46" target="#b12">[13]</ref>. It by default uses English stemming and just requires one of the terms to match. It was used with WEIGHT 1 on the ALL PROPS column to increase the ranking of documents with the query in the title or other property information. It was used with WEIGHT 10 on the FT TEXT column (which represents the external document). Again, these weights were chosen based on what worked well on the previous year's entry page finding task.</p><p>For the submitted hum02upd and hum02up runs, a higher weight was given to URLs of particular type and depth, using a SearchSQL WHERE clause of the following form which was was found to work well on last year's entry page finding task:</p><p>Although it might seem this query is giving the same weight to the ROOT, SUBROOT and PATH types of URLs (all WEIGHT 10), because a ROOT term is much less frequent in the URL TYPE column, it in effect gets a higher weight in relevance ranking because of its higher inverse document frequency (and SUBROOT has more impact than PATH for the same reason). The URL type of FILE was given WEIGHT 0, which means it did not affect the relevance calculation, but it was included so that the AND clause would match all rows.</p><p>Similarly, giving URL depths 1-4 some extra weight was found to be modestly helpful on last year's entry page finding task. Again, URLs of depth 1 (for which URLDEPTHA was included in the URL DEPTH column) internally had a higher weight from the inverse document frequency.</p><p>For the submitted hum02uhp run, an even higher weight was given to URL TYPE (the 3 terms of WEIGHT 10 were given WEIGHT 25). On last year's entry page finding task, the stronger URL TYPE weights gave similar MRR scores to the lower ones.</p><p>For the submitted hum02ud run, the SearchSQL query was the same as for hum02upd except that the ALL PROPS searches were omitted (i.e. properties and phrases in properties were not given extra weight). Note that the FT TEXT column indexed all of the properties except for the URL of the document header.</p><p>The difference between the hum02upd and hum02up runs was in the importance of document length normalization (in general, runs ending with 'd' used "SET RELEVANCE DLEN IMP 500" and the others used "SET RELEVANCE DLEN IMP 250").</p><p>For the named page queries, no query terms were discarded (e.g. there was no expectation that discarding the words "find", "relevant" and "document" would be beneficial, unlike for some previous year's tasks). Of course, the index omitted a few stop words (e.g. "the", "by") as previously mentioned.</p><p>For the named page queries, besides linguistic expansion from stemming in the IS ABOUT predicate, we did not do any query expansion. For example, we did not use approximate text searching for spell-correction (the organizers tried to ensure the topics were spelled correctly), and we did not use row expansion or any other kind of blind feedback technique.</p><p>SearchServer's relevance value calculation is the same as described last year <ref type="bibr" coords="5,425.51,537.75,14.61,10.46" target="#b12">[13]</ref>. Briefly, SearchServer dampens the term frequency and adjusts for document length in a manner similar to Okapi <ref type="bibr" coords="5,469.07,549.71,10.52,10.46" target="#b7">[8]</ref> and dampens the inverse document frequency using an approximation of the logarithm. SearchServer's relevance values are always an integer in the range 0 to 1000.</p><p>When multiple predicates are combined, as was done for the named page queries this year, SearchServer currently does not normalize by query length. For example, the URL TYPE clauses of the earlier examples would have a lot less relative impact if the named page query contained 5 words instead of 1.</p><p>SearchServer's RELEVANCE METHOD setting can be used to optionally square the importance of the inverse document frequency (by choosing a RELEVANCE METHOD of 'V2:4' instead of 'V2:3'). The importance of document length to the ranking is controlled by SearchServer's RELEVANCE DLEN IMP setting (scale of 0 to 1000). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Official Results</head><p>The evaluation measures are likely explained in an appendix of this volume. Briefly, "Reciprocal Rank" for a topic is one divided by the rank in which the named page was found (using the smallest rank if there were duplicates of the named page), or zero if the named page was not found. "Mean Reciprocal Rank" (MRR) is the average of the reciprocal ranks over all the topics. "%Top10" is the percentage of topics for which the named page was found in the first 10 rows. "%Fail" is the percentage of topics for which the named page was not found in the first 50 rows. Table <ref type="table" coords="6,114.33,410.31,4.98,10.46" target="#tab_1">1</ref> shows the scores of the submitted named page finding runs. Most of the remaining tables will focus on one particular precision measure (usually reciprocal rank or average precision), comparing the scores when a particular feature (such as stemming) is enabled to when it is disabled. The columns of these tables are as follows:</p><p>• "Experiment" is the feature tested.</p><p>• "AvgDiff" is the average difference in the score.</p><p>• "95% Confidence" is an approximate 95% confidence interval for the average difference calculated using Efron's bootstrap percentile method<ref type="foot" coords="6,255.89,516.84,3.97,7.32" target="#foot_1">2</ref> [3] (using 100,000 iterations). If zero is not in the interval, the result is "statistically significant" (at the 5% level), i.e. the feature is unlikely to be of neutral impact, though if the average difference is small (e.g. &lt;0.020) it may still be too minor to be considered "significant" in the magnitude sense.</p><p>• "vs." is the number of topics on which the score was higher, lower and tied (respectively) with the feature enabled. These numbers should always add to the number of topics for the task.</p><p>• "2 Largest Diffs (Topic)" lists the two largest differences in the precision score (based on the absolute value), with each followed by the corresponding topic number in brackets (the named page topic numbers range from 1 to 150).</p><p>Table <ref type="table" coords="6,113.76,649.42,4.98,10.46">2</ref> shows the impact when isolating each technique distinguishing the submitted named page finding runs:</p><p>• The 'p' factor (extra weight for HTML properties and phrases in properties) increased MRR 8 points. Some diagnostics of this result, including whether it holds up when URL techniques are not also used, are in the next section.</p><p>• The 'd' factor (document length importance of 500 instead of 250) made little difference.</p><p>• The 'u' factor (extra weight for URL type and depth) lowered MRR by 9 points, contrary to its substantial beneficial impact on last year's entry page task.</p><p>• The 'h' factor (even more extra weight for URL type) lowered MRR by another 19 points, even though it had a neutral impact on last year's entry page task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Diagnostic Results</head><p>For the diagnostics, we defined a "base run" which set the document length importance to 500 and executed an IS ABOUT search of just the content (i.e. the FT TEXT column). For example:</p><p>SELECT RELEVANCE('V2:3') AS REL, DOCNO FROM GOV WHERE (FT_TEXT IS_ABOUT 'visiting pandas national zoo') ORDER BY REL DESC;</p><p>The base run scored a 0.564 mean reciprocal rank, finding the named page in the top 10 for 75.3% of the queries while failing to find it in the top 50 for 11.3% of the queries.</p><p>Table <ref type="table" coords="7,115.04,343.73,4.98,10.46" target="#tab_2">3</ref> shows a comparison of various runs to the base run (always subtracting the base run's scores in reciprocal rank from the listed run). The first row compares submitted run hum02pd to the base run, like the above 'p' factor experiment but without the ill-fated URL techniques. While the average gain from properties and phrases is a little smaller (6 points), the (approximate) 95% confidence interval (1 to 11 points) indicates it is still statistically significant. Topic 64 (work/life center map) benefitted most.</p><p>The runs in the next group of comparisons in Table <ref type="table" coords="7,305.90,403.51,4.98,10.46" target="#tab_2">3</ref> (listed with a leading + sign) added the listed column to the WHERE clause with one-tenth the weight of the content search. For example, for the "+ TITLE" row, the query's WHERE clause was of this form:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WHERE</head><p>(TITLE IS_ABOUT 'visiting pandas national zoo' WEIGHT 1 ) OR (FT_TEXT IS_ABOUT 'visiting pandas national zoo' <ref type="bibr" coords="7,343.98,472.74,52.30,10.46">WEIGHT 10)</ref> Apparently it is the extra weight on particular columns and not the use of phrases which explains most of the gain of the 'p' factor. For example, the "+ ALL PROPS" run differs from hum02pd in that phrases are not used, but it produces similar gains. Of the HTML properties, just giving extra weight to the TITLE produces most of the gains. The URL and FIRST HEADING also appear to be helpful for named page finding on average, while the META properties were harmful on average, but most of these latter results were not statistically significant. Note that the FT TEXT column included the content of the other columns except for the URL column.</p><p>The next group of rows in Table <ref type="table" coords="7,237.24,577.83,4.98,10.46" target="#tab_2">3</ref> shows the importance of the content to named page finding. The compared runs just searched the listed column. For example, for the "TITLE" row, the query's WHERE clause was of this form:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WHERE (TITLE IS_ABOUT 'visiting pandas national zoo')</head><p>The content column (FT TEXT) scored significantly higher, on average, than any other column by itself. The end of the confidence interval closest to zero represents a difference of at least 13 points in every case. Still, the "vs." column shows that for approximately one-sixth of the queries, just searching the columns containing the TITLE outscored content searching.</p><p>The last group of rows contains some miscellaneous experiments. The results with positive impacts were not statistically significant, but the negative impacts were. Just searching for the named page query as a phrase in the document (WHERE FT TEXT CONTAINS 'query'), as in the "phrase only" row, significantly hurt on average, but enhancing the base run by giving a little extra weight (one-tenth) to the query as a phrase (OR FT TEXT contains 'query'), as in the "+ phrase" row, was modestly helpful. Disabling stemming (via SET VECTOR GENERATOR '') was only modestly helpful as per the "stemming off" row. Increasing the importance of document length normalization (via SET RELEVANCE DLEN IMP 750, as opposed to the base run setting of 500) didn't make much difference (as per the "DLEN 750" row), but decreasing it to 0 (as per the "DLEN 0") row significantly hurt. Increasing the importance of inverse document frequency (by using relevance method 'V2:4' instead of 'V2:3') was modestly detrimental as per the "idf squared" row. Even for the impacts which were modest on average, individual queries could have large changes in their scores as indicated by the "2 Largest Diffs" column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Arabic Retrieval</head><p>The Arabic document set was the same as last year: Arabic Newswire A Corpus <ref type="bibr" coords="8,435.82,672.31,10.52,10.46" target="#b0">[1]</ref>    <ref type="bibr" coords="9,488.60,262.78,14.76,10.46" target="#b14">[15]</ref>). All of our submitted runs both last year and this year used this default behaviour. For the submitted runs, two different SearchServer tables called ARAB01 and ARAB01AS were created. ARAB01 was the same as last year, i.e. no stop words and no Arabic morphological normalizations were used. ARAB01AS used the stop words <ref type="bibr" coords="9,245.48,310.60,10.52,10.46" target="#b8">[9]</ref> and the experimental morphological normalizations described in section 5.2 of last year's paper <ref type="bibr" coords="9,209.13,322.55,15.50,10.46" target="#b12">[13]</ref> (which were just used for diagnostic runs last year, not submitted runs like this year).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Searching</head><p>Compared to last year, there were twice as many Arabic topics (50). They were numbered AR26-AR75 and were distributed in a file called "final arabic02.txt". The topics contained a "Title" (subject of the topic), "Description" (typically a one-sentence specification of the information need) and "Narrative" (more detailed guidelines for what a relevant document should or should not contain). The topics were encoded in the ISO 8859-6 character set, so "SET CHARACTER SET 'ISO 8859 6' " was executed before the searches.</p><p>Like last year, Intuitive Searching of the content was used (i.e. FT TEXT IS ABOUT). The statement "SET MAX SEARCH ROWS 1000" was previously executed so that the working table would contain at most 1000 rows. There were no experiments with phrases nor columns other than FT TEXT.</p><p>Submitted runs humAR02tdm and humAR02td both used the Title and Description fields in the query. Run humAR02tdm searched the ARAB01 table, and run humAR02td searched the ARAB01AS table. The other settings (e.g. RELEVANCE METHOD 'V2:3' and RELEVANCE DLEN IMP 500) were the same as described in section 5.2 of the final version of last year's paper <ref type="bibr" coords="9,348.03,514.81,14.61,10.46" target="#b12">[13]</ref>.</p><p>Submitted run humAR02tde used query expansion from blind feedback in the same way as described in last year's paper <ref type="bibr" coords="9,149.25,538.71,14.61,10.46" target="#b12">[13]</ref>. The base run was humAR02td and the first 5 rows were used to generate broader queries.</p><p>Submitted runs humAR02te and humAR02tdne differed from humAR02tde in that the former just used the Title field in its base run, and the latter additionally used the Narrative field in its base run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Official Results</head><p>To review the evaluation measures for topic relevance tasks: "Precision" is the percentage of retrieved documents which are relevant. "Precision@n" is the precision after n documents have been retrieved. "Average precision" for a topic is the average of the precision after each relevant document is retrieved (using zero as the precision for relevant documents which are not retrieved). "Recall" is the percentage of relevant documents which have been retrieved. "Interpolated precision" at a particular recall level for a topic is the maximum precision achieved for the topic at that or any higher recall level. For a set of topics, the measure is the average of the measure for each topic (i.e. all topics are weighted equally).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,372.38,468.02,271.48"><head></head><label></label><figDesc>table called GOV was created for the .GOV collection with the following SearchSQL statement:</figDesc><table coords="3,82.46,406.25,183.06,237.61"><row><cell>create schema GOV</cell></row><row><cell>create table GOV</cell></row><row><cell>(</cell></row><row><cell>DOCNO varchar(256) 128,</cell></row><row><cell>NONEMPTY_TITLE varchar(2048) 129,</cell></row><row><cell>TITLE varchar(2048) 130,</cell></row><row><cell>META_TITLE varchar(2048) 131,</cell></row><row><cell>META_SUBJECT varchar(2048) 132,</cell></row><row><cell>META_DESCRIPTION varchar(2048) 133,</cell></row><row><cell>FIRST_HEADING varchar(2048) 134,</cell></row><row><cell>URL varchar(2048) 135,</cell></row><row><cell>URL_TYPE varchar(2048) 136,</cell></row><row><cell>URL_DEPTH varchar(2048) 137,</cell></row><row><cell>META_KEYWORDS varchar(2048) 138,</cell></row><row><cell>ALL_BUT_KEYWORDS varchar(2048) 139,</cell></row><row><cell>ALL_PROPS varchar(2048) 140</cell></row><row><cell>)</cell></row><row><cell>periodic</cell></row><row><cell>stopfile 'mygov.stp'</cell></row><row><cell>basepath 'e:\data\compressed';</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,116.32,80.42,379.35,202.96"><head>Table 1 :</head><label>1</label><figDesc>Scores of Submitted Named Page Finding Runs</figDesc><table coords="6,116.32,95.10,379.35,188.28"><row><cell></cell><cell>Run</cell><cell></cell><cell>MRR</cell><cell>%Top10</cell><cell>%Fail</cell></row><row><cell></cell><cell cols="2">hum02pd</cell><cell>0.626</cell><cell>82.0%</cell><cell>9.3%</cell></row><row><cell></cell><cell cols="2">hum02upd</cell><cell>0.538</cell><cell>75.3%</cell><cell>13.3%</cell></row><row><cell></cell><cell cols="2">hum02up</cell><cell>0.527</cell><cell>74.0%</cell><cell>11.3%</cell></row><row><cell></cell><cell cols="2">hum02ud</cell><cell>0.456</cell><cell>68.0%</cell><cell>16.7%</cell></row><row><cell></cell><cell cols="2">hum02uhp</cell><cell>0.337</cell><cell>51.3%</cell><cell>33.3%</cell></row><row><cell cols="6">Table 2: Impact of Submitted Named Page Finding Techniques on Reciprocal Rank</cell></row><row><cell>Experiment</cell><cell>AvgDiff</cell><cell cols="2">95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>p (upd -ud)</cell><cell>0.082</cell><cell cols="2">( 0.041, 0.124)</cell><cell>54-17-79</cell><cell>0.900 (2), 0.889 (51)</cell></row><row><cell>d (upd -up)</cell><cell>0.011</cell><cell cols="2">(-0.020, 0.043)</cell><cell>33-28-89</cell><cell>0.833 (36), 0.800 (41)</cell></row><row><cell>u (upd -pd)</cell><cell>-0.088</cell><cell cols="2">(-0.130,-0.047)</cell><cell>15-50-85</cell><cell>-1.000 (64), -0.917 (138)</cell></row><row><cell>h (uhp -up)</cell><cell>-0.190</cell><cell cols="2">(-0.236,-0.146)</cell><cell>0-87-63</cell><cell>-1.000 (2), -1.000 (117)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,80.35,80.42,451.29,402.13"><head>Table 3 :</head><label>3</label><figDesc>Comparison with Plain Content Diagnostic Run in Reciprocal Rank</figDesc><table coords="8,80.35,95.10,451.29,387.45"><row><cell>Experiment</cell><cell>AvgDiff</cell><cell>95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>hum02pd</cell><cell>0.061</cell><cell>( 0.017, 0.107)</cell><cell>49-16-85</cell><cell>0.968 (64), 0.909 (51)</cell></row><row><cell>+ ALL BUT KEYWORDS</cell><cell>0.058</cell><cell>( 0.014, 0.103)</cell><cell>46-15-89</cell><cell>0.968 (64), 0.909 (51)</cell></row><row><cell>+ ALL PROPS</cell><cell>0.055</cell><cell>( 0.013, 0.097)</cell><cell>48-15-87</cell><cell>0.968 (64), 0.909 (51)</cell></row><row><cell>+ NONEMPTY TITLE</cell><cell>0.047</cell><cell>( 0.008, 0.086)</cell><cell>43-15-92</cell><cell>0.909 (51), 0.900 (38)</cell></row><row><cell>+ TITLE</cell><cell>0.042</cell><cell>( 0.004, 0.080)</cell><cell>41-16-93</cell><cell>0.909 (51), 0.900 (38)</cell></row><row><cell>+ URL</cell><cell>0.034</cell><cell>( 0.005, 0.066)</cell><cell>26-20-104</cell><cell>0.909 (51), 0.900 (106)</cell></row><row><cell>+ FIRST HEADING</cell><cell>0.034</cell><cell>(-0.001, 0.071)</cell><cell>25-26-99</cell><cell>0.909 (51), 0.889 (138)</cell></row><row><cell>+ META TITLE</cell><cell>-0.006</cell><cell>(-0.022, 0.012)</cell><cell>10-14-126</cell><cell>0.857 (143), -0.500 (101)</cell></row><row><cell>+ META SUBJECT</cell><cell>-0.012</cell><cell>(-0.025,-0.001)</cell><cell>7-15-128</cell><cell>-0.500 (101), -0.500 (57)</cell></row><row><cell>+ META DESCRIPTION</cell><cell>-0.022</cell><cell>(-0.052, 0.009)</cell><cell>19-31-100</cell><cell>0.889 (2), -0.857 (61)</cell></row><row><cell>+ META KEYWORDS</cell><cell>-0.024</cell><cell>(-0.052, 0.002)</cell><cell>20-30-100</cell><cell>-0.750 (61), -0.667 (93)</cell></row><row><cell>FT TEXT</cell><cell>0.000</cell><cell>(-0.001, 0.001)</cell><cell>0-0-150</cell><cell>0.000 (76), 0.000 (2)</cell></row><row><cell>ALL PROPS</cell><cell>-0.212</cell><cell>(-0.292,-0.131)</cell><cell>25-81-44</cell><cell>-1.000 (124), -1.000 (69)</cell></row><row><cell>ALL BUT KEYWORDS</cell><cell>-0.218</cell><cell>(-0.299,-0.138)</cell><cell>29-79-42</cell><cell>-1.000 (108), -1.000 (28)</cell></row><row><cell>NONEMPTY TITLE</cell><cell>-0.277</cell><cell>(-0.357,-0.196)</cell><cell>23-87-40</cell><cell>-1.000 (132), -1.000 (117)</cell></row><row><cell>TITLE</cell><cell>-0.268</cell><cell>(-0.347,-0.188)</cell><cell>23-86-41</cell><cell>-1.000 (92), -1.000 (107)</cell></row><row><cell>FIRST HEADING</cell><cell>-0.445</cell><cell>(-0.524,-0.363)</cell><cell>14-114-22</cell><cell>-1.000 (61), -1.000 (35)</cell></row><row><cell>META DESCRIPTION</cell><cell>-0.487</cell><cell>(-0.563,-0.409)</cell><cell>8-119-23</cell><cell>-1.000 (90), -1.000 (27)</cell></row><row><cell>URL</cell><cell>-0.499</cell><cell>(-0.577,-0.420)</cell><cell>10-122-18</cell><cell>-1.000 (92), -1.000 (43)</cell></row><row><cell>META KEYWORDS</cell><cell>-0.502</cell><cell>(-0.574,-0.430)</cell><cell>4-123-23</cell><cell>-1.000 (84), -1.000 (85)</cell></row><row><cell>META TITLE</cell><cell>-0.541</cell><cell>(-0.612,-0.470)</cell><cell>2-128-20</cell><cell>-1.000 (77), -1.000 (80)</cell></row><row><cell>META SUBJECT</cell><cell>-0.562</cell><cell>(-0.631,-0.491)</cell><cell>1-132-17</cell><cell>-1.000 (1), -1.000 (80)</cell></row><row><cell>+ phrase</cell><cell>0.015</cell><cell>(-0.012, 0.045)</cell><cell>18-11-121</cell><cell>0.950 (68), 0.909 (51)</cell></row><row><cell>stemming off</cell><cell>0.013</cell><cell>(-0.014, 0.041)</cell><cell>32-16-102</cell><cell>-0.957 (150), 0.900 (38)</cell></row><row><cell>DLEN 750</cell><cell>0.011</cell><cell>(-0.016, 0.039)</cell><cell>30-20-100</cell><cell>-0.800 (85), 0.750 (139)</cell></row><row><cell>idf squared</cell><cell>-0.025</cell><cell>(-0.049,-0.001)</cell><cell>16-32-102</cell><cell>-0.833 (96), -0.667 (84)</cell></row><row><cell>DLEN 0</cell><cell>-0.210</cell><cell>(-0.262,-0.158)</cell><cell>7-85-58</cell><cell>-1.000 (36), -0.974 (96)</cell></row><row><cell>phrase only</cell><cell>-0.424</cell><cell>(-0.503,-0.345)</cell><cell>11-108-31</cell><cell>-1.000 (97), -1.000 (92)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,72.00,80.42,468.01,192.82"><head>Table 4 :</head><label>4</label><figDesc>Impact of Submitted Arabic Techniques on Average Precision</figDesc><table coords="9,72.00,95.10,468.01,178.14"><row><cell>Experiment</cell><cell>AvgDiff</cell><cell>95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>Exp (tde -td)</cell><cell>0.033</cell><cell>( 0.021, 0.046)</cell><cell>37-13-0</cell><cell>0.204 (27), 0.131 (58)</cell></row><row><cell>Morph+Stop (td -tdm)</cell><cell>0.032</cell><cell>( 0.005, 0.061)</cell><cell>34-16-0</cell><cell>0.360 (29), 0.335 (60)</cell></row><row><cell>Narr (tdne -tde)</cell><cell>0.027</cell><cell>(-0.014, 0.073)</cell><cell>27-22-1</cell><cell>0.794 (34), -0.551 (27)</cell></row><row><cell>Desc (tde -te)</cell><cell>0.014</cell><cell>(-0.011, 0.039)</cell><cell>29-20-1</cell><cell>0.348 (58), -0.311 (49)</cell></row><row><cell>3.1 Indexing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">SearchServer (as of version 5.0) internally uses the Unicode canonical decomposition of text and, by default,</cell></row><row><cell cols="5">does not index combining characters (accents, diacritics, etc.). For Arabic, this means by default that</cell></row><row><cell cols="5">composite characters 0622 (alef with madda above), 0623 (alef with hamza above) and 0625 (alef with</cell></row><row><cell cols="5">hamza below) are treated as 0627 (alef), 0624 (waw with hamza above) becomes 0648 (waw), and 0626 (yeh</cell></row><row><cell cols="5">with hamza above) becomes 064A (yeh) (the codes and names are from the Unicode Standard</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,685.82,452.76,9.49;1,72.00,696.41,468.13,8.37"><p>Fulcrum r is a registered trademark, and SearchServer TM , SearchSQL TM , Intuitive Searching TM and Ful/Text TM are trademarks of Hummingbird Ltd. All other copyrights, trademarks and tradenames are the property of their respective owners.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,87.24,678.70,452.87,8.37;6,72.00,688.16,221.01,8.37"><p>See<ref type="bibr" coords="6,102.85,678.70,13.18,8.37" target="#b11">[12]</ref> for some comparisons of confidence intervals from the bootstrap percentile, Wilcoxon signed rank and standard error methods for both average precision and Precision@10.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>WHERE ((<rs type="programName">ALL_PROPS CONTAINS 'visiting pandas national zoo' WEIGHT 1) OR (ALL_PROPS IS_ABOUT 'visiting pandas national zoo' WEIGHT 1) OR (FT_TEXT IS_ABOUT 'visiting pandas national zoo' WEIGHT 10) ) AND ( (URL_TYPE CONTAINS 'ROOT' WEIGHT 10) OR (URL_TYPE CONTAINS 'SUBROOT' WEIGHT 10) OR (URL_TYPE CONTAINS 'PATH' WEIGHT 10) OR (URL_TYPE CONTAINS 'FILE' WEIGHT 0) OR (URL_DEPTH CONTAINS 'URLDEPTHA' WEIGHT 5) OR (URL_DEPTH CONTAINS 'URLDEPTHAB' WEIGHT 5) OR</rs> (<rs type="grantNumber">URL_DEPTH CONTAINS 'URLDEPTHABC' WEIGHT 5) OR (URL_DEPTH CONTAINS 'URLDEPTHABCD' WEIGHT 5)</rs> )</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pxH546S">
					<idno type="grant-number">URL_DEPTH CONTAINS &apos;URLDEPTHABC&apos; WEIGHT 5) OR (URL_DEPTH CONTAINS &apos;URLDEPTHABCD&apos; WEIGHT 5)</idno>
					<orgName type="program" subtype="full">ALL_PROPS CONTAINS &apos;visiting pandas national zoo&apos; WEIGHT 1) OR (ALL_PROPS IS_ABOUT &apos;visiting pandas national zoo&apos; WEIGHT 1) OR (FT_TEXT IS_ABOUT &apos;visiting pandas national zoo&apos; WEIGHT 10) ) AND ( (URL_TYPE CONTAINS &apos;ROOT&apos; WEIGHT 10) OR (URL_TYPE CONTAINS &apos;SUBROOT&apos; WEIGHT 10) OR (URL_TYPE CONTAINS &apos;PATH&apos; WEIGHT 10) OR (URL_TYPE CONTAINS &apos;FILE&apos; WEIGHT 0) OR (URL_DEPTH CONTAINS &apos;URLDEPTHA&apos; WEIGHT 5) OR (URL_DEPTH CONTAINS &apos;URLDEPTHAB&apos; WEIGHT 5) OR</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The scores of the submitted runs are expected to be listed in the appendix of the conference proceedings. Table <ref type="table" coords="10,98.61,307.11,4.98,10.46">4</ref> shows the impact when isolating each technique distinguishing the submitted runs. The query expansion technique ("Exp" experiment) increased mean average precision by 3 points and was fairly consistent (small standard error), as evidenced by the narrow confidence interval. The experimental morphological normalizations plus stop words ("Morph+Stop") also increased mean average precision by 3 points, but less consistently, as evidenced by the wider confidence interval. Including the Narrative field ("Narr") increased mean average precision by 3 points, but was very inconsistent; using the Narrative often hurt the scores. Including the Description field ("Desc") increased mean average precision by just 1 point, though again was not very consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Diagnostic Results</head><p>After the official runs were submitted, we used SearchServer's plug-in parser architecture to experiment with plugging in an implementation of the light algorithmic "Light8" stemmer of Larkey et al. <ref type="bibr" coords="10,468.63,451.54,9.96,10.46" target="#b5">[6]</ref>. It contained several stemming rules we were not previously using. On topic AR30, we found the light stemmer, in combination with the default dropping of combining characters, did not stem Arabic words for "satellite" to the same form, leading us to also experiment with indexing combining characters 0653 (maddah above), 0654 (hamza above) and 0655 (hamza below) via an extra line in the stopfile ("IAC="\u0653-\u0655" "). But the light stemmer explicitly dropped these combining characters if they followed 0627 (alef).</p><p>Table <ref type="table" coords="10,114.52,523.27,4.98,10.46">5</ref> lists precision scores for the diagnostic runs. Listed for each run are its mean average precision (AvgP), the mean precision after 5, 10 and 20 documents retrieved (P@5, P@10 and P@20 respectively), the mean interpolated precision at 0% and 30% recall (Rec0 and Rec30 respectively), and the mean precision after R documents retrieved (P@R) where R is the number of relevant documents for the topic. The following run codes were used: "Y01" (Year 2001) specifies the run used the 25 TREC 2001 topics. "Y02" (Year 2002) specifies the run used the 50 TREC 2002 topics. "L" (Light stemming) specifies the run used the light algorithmic stemmer. "C" (Combining characters) specifies that combining characters 0653-0655 were not dropped by SearchServer. "S" (Stop words) specifies the run used a table which did not index stop words. "E" (Expansion) specifies the run used query expansion from blind feedback. "td" specifies the Title and Description fields were used. "def" specifies the default settings. In particular, the "def-td-Y02" run of Table <ref type="table" coords="10,72.00,642.82,4.98,10.46">5</ref> is the same as submitted run "humAR02tdm" (a baseline Title+Description run not using light stemming, combining character indexing, stop words nor expansion).</p><p>Table <ref type="table" coords="10,114.21,666.73,4.98,10.46">6</ref> isolates the impact of various techniques on the average precision measure. All of these comparisons use "td" topics, and most of them are statistically significant at 5% level:</p><p>• The "+L" rows isolate the impact of light stemming. The impact when indexing combining characters ("CL-C", i.e. subtracting the "C" run from the "CL" run) was particularly substantial on the TREC 2001 topics, where we found an increase of 0.122 (from .180 to .302). Larkey's comparable figure was 0.182 (from 0.194 to 0.376) which is inside our approximate 95% confidence interval of (0.061, 0.192).</p><p>For the TREC 2002 topics, which did not exist when the stemmer was developed, we find a smaller, though still significant, increase.</p><p>• The "+C" rows isolate the impact of indexing the combining characters. When this was the only change from the default ("C-def"), the impact tended to be detrimental, perhaps because the alef forms were no longer conflated. When applied to light stemming runs ("CL-L"), the light stemmer re-conflated the alefs, and the net impact (in effect preserving composite characters 0624 and 0626) tended to be beneficial.</p><p>• The "+LC" rows show the combined impact of light stemming and indexing combining characters. Of course, the average impacts add within rounding differences (the calculations were done to 4 decimal places though just 3 are shown). However, confidence interval endpoints do not add (e.g. 0.027 plus 0.000 does not add to 0.045).</p><p>• The "+S" rows isolate the impact of using the Arabic stop word list (subtracting the "CL" run from the "CLS" run). The increases are small but fairly consistent, much like in European stop word experiments when using the full topics <ref type="bibr" coords="11,264.38,589.64,14.61,10.46" target="#b11">[12]</ref>, but for the Arabic task this result also holds when omitting the Narrative.</p><p>• The "+E" rows isolate the impact of the query expansion technique (subtracting the "CLS" run from the "CLSE" run). The results are similar to the official "Exp" experiment. As the expansion terms are chosen from the first 5 rows, the first 5 rows are usually the same after expansion, which moderates how much the result can change. We haven't done a lot of work on our expansion technique and it is likely underachieving. Expanding queries generally leads to much longer processing times which can be a high price to pay for improvements in the part of the result list that users might not even look at. In practical systems, users can control the query terms themselves rather than depend on blind feedback.</p><p>• The "+CLSE" rows show the combined impact of all 4 techniques. Even the low end of the confidence intervals represent a substantial impact.</p><p>In all 9 cases in Table <ref type="table" coords="12,183.45,104.92,3.87,10.46">6</ref>, the confidence interval for the 50 topic experiment (Y02) was narrower than the confidence interval for the corresponding 25 topic experiment (Y01). Also reassuringly, the corresponding confidence intervals always overlapped. The interval widths ranged from 1 to 8 points for the 50 topic experiments and from 4 to 13 points for the 25 topic experiments.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,87.50,189.49,452.51,10.46;12,82.51,201.44,298.61,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,203.08,189.49,290.19,10.46">Linguistic Data Consortium (LDC) catalog number LDC</title>
		<ptr target="http://www.ldc.upenn.edu/Catalog/LDC2001T55.html" />
	</analytic>
	<monogr>
		<title level="j" coord="12,87.50,189.49,98.62,10.46">Arabic Newswire Part</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,221.14,332.21,10.46" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><surname>Cross-Language</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/" />
		<title level="m" coord="12,159.72,221.14,116.36,10.46">Evaluation Forum web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,240.84,452.52,10.46" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,263.17,240.84,142.17,10.46">An Introduction to the Bootstrap</title>
		<author>
			<persName coords=""><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,260.54,360.87,10.46" xml:id="b3">
	<monogr>
		<ptr target="http://www.ted.cmis.csiro.au/TRECWeb/govinfo.html" />
		<title level="m" coord="12,87.50,260.54,113.50,10.46">The .GOV Test Collection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,280.24,452.51,10.46;12,82.51,292.20,243.48,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,167.44,280.24,214.97,10.46">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,401.72,280.24,138.29,10.46;12,82.51,292.20,46.07,10.46">Sixteenth International Unicode Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,311.90,452.51,10.46;12,82.51,323.85,457.49,10.46;12,82.51,335.81,457.49,10.46;12,82.51,347.76,387.25,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,345.71,311.90,194.30,10.46;12,82.51,323.85,235.60,10.46">Improving Stemming for Arabic Information Retrieval: Light Stemming and Co-occurrence Analysis</title>
		<author>
			<persName coords=""><forename type="first">Leah</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,281.32,335.81,258.69,10.46;12,82.51,347.76,290.57,10.46">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Micheline</forename><surname>Beaulieu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sung</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hyon</forename><surname>Myaeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</editor>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,367.47,32.79,10.46;12,143.76,367.47,58.68,10.46;12,225.91,367.47,18.60,10.46;12,267.99,367.47,43.73,10.46;12,335.22,367.47,11.93,10.46;12,370.64,367.47,10.93,10.46;12,405.04,367.47,39.13,10.46;12,467.66,367.47,25.18,10.46;12,516.34,367.47,23.67,10.46;12,82.51,379.42,213.77,10.46" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,149.10,367.47,53.35,10.46;12,225.91,367.47,18.60,10.46;12,267.99,367.47,43.73,10.46;12,335.22,367.47,11.93,10.46;12,370.64,367.47,10.93,10.46;12,405.04,367.47,39.13,10.46;12,467.66,367.47,25.18,10.46;12,516.34,367.47,18.94,10.46">NII-NACSIS Test Collection for IR Systems) Home Page</title>
		<author>
			<persName coords=""><surname>Ntcir</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/∼ntcadm/index-en.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,399.12,452.51,10.46;12,82.51,411.08,457.49,10.46;12,82.51,423.03,354.34,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,501.88,399.12,38.13,10.46;12,82.51,411.08,33.92,10.46">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec3/t3proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,240.18,411.08,266.79,10.46">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="500" to="226" />
		</imprint>
		<respStmt>
			<orgName>City University.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,442.73,452.51,10.46;12,82.51,454.68,457.50,10.46;12,82.51,466.64,69.88,10.46" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stalls</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/∼yaser/arabic/arabic-stop-words.html" />
		<title level="m" coord="12,245.59,454.68,102.40,10.46">Arabic Stop Words List</title>
		<imprint/>
		<respStmt>
			<orgName>University of Southern California, Information Sciences Institute, Natural Language Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,92.47,486.34,303.88,10.46" xml:id="b9">
	<monogr>
		<ptr target="http://trec.nist.gov/" />
		<title level="m" coord="12,92.47,486.34,206.36,10.46">Text REtrieval Conference (TREC) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,92.47,506.05,447.53,10.46;12,82.51,518.00,457.49,10.46;12,82.51,529.95,381.46,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,273.93,506.05,219.44,10.46">Hummingbird&apos;s Fulcrum SearchServer at TREC-9</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Blackwell</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec9/t9proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,253.28,518.00,281.47,10.46">Proceedings of the Ninth Text REtrieval Conference (TREC-9)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,92.47,549.66,434.20,10.46;12,526.65,548.58,12.85,7.32;12,82.51,561.61,457.50,10.46;12,82.51,573.56,239.22,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,188.64,549.66,338.03,10.46;12,526.65,548.58,12.85,7.32;12,82.51,561.61,72.91,10.46">Experiments in 8 European Languages with Hummingbird SearchServer TM at CLEF 2002</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://clef.iei.pi.cnr.it:2002/workshop2002/WN/26.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,302.17,561.61,232.70,10.46">Working Notes for the CLEF 2002 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,92.47,593.26,205.93,10.46;12,298.39,592.19,12.85,7.32;12,314.01,593.26,225.99,10.46;12,82.51,605.22,457.49,10.46;12,82.51,617.18,258.72,10.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,179.79,593.26,118.61,10.46;12,298.39,592.19,12.85,7.32;12,314.01,593.26,39.61,10.46">Hummingbird SearchServer TM at TREC</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec10/t10proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,117.51,605.22,224.74,10.46;12,409.23,605.22,130.77,10.46;12,82.51,617.18,13.28,10.46">Proceedings of the Tenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
	<note>NIST Special Publication 500-250</note>
</biblStruct>

<biblStruct coords="12,92.47,636.87,447.53,10.46;12,82.51,648.83,457.49,10.46;12,82.51,660.79,457.49,10.46;12,82.51,672.74,237.69,10.46" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,372.05,636.87,167.95,10.46;12,82.51,648.83,159.93,10.46">Retrieving Web Pages using Content, Links, URLs and Anchors</title>
		<author>
			<persName coords=""><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec10/t10proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,501.92,648.83,38.08,10.46;12,82.51,660.79,216.00,10.46;12,378.51,660.79,157.00,10.46">Proceedings of the Tenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>NIST Special Publication 500-250</note>
</biblStruct>

<biblStruct coords="12,92.47,692.44,370.33,10.46" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,92.47,692.44,264.86,10.46">The Unicode Standard Version 3.0. The Unicode Consortium</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
