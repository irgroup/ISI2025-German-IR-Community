<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,181.11,154.89,249.03,15.11">CWI at the TREC-2002 video track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,168.72,187.37,84.16,10.48"><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alex van Ballegooij CWI</orgName>
								<orgName type="laboratory">Arjen de Vries</orgName>
								<address>
									<postBox>PO Box 94079</postBox>
									<postCode>1090 GB</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,181.11,154.89,249.03,15.11">CWI at the TREC-2002 video track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">15B0887CB4BAFF4D24505DFAEC4FBD18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We present a probabilistic model for the retrieval of multimodal documents. The model is based on Bayesian decision theory and combines models for text based search with models for visual search. The textual model, applied to the LIMSI transcripts, is based on the language modelling approach to text retrieval. The visual model, a mixture of Gaussian densities, describes keyframes selected from shots. Both models have proved successful on media specific retrieval tasks. Our contribution is the combination of both techniques in a unified model, ranking shots on ASR-data and visual features simultaneously.</p><p>Using this model, we tried to answer the following questions.</p><p>• Is it useful to identify important parts in query images?</p><p>• Can using (additional) query images from outside the search collection<ref type="foot" coords="1,198.92,525.37,3.97,6.12" target="#foot_0">1</ref> help improve retrieval results?</p><p>• Does it help to have multiple image examples for a query, or are we better of using only one good example?</p><p>• Can a combination combined textual and visual query perform better than queries in a single modality?</p><p>Because of problems with the similarity measure we used in the submitted runs, we mainly report on post-hoc experiments on the TREC-2002 data in which we used a different measure. Both measures are discussed in Section 2, where we also present our retrieval model. Section 3 reports on the post-hoc experiments and Section 4 summarises our main findings. The official results can be found in appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probabilistic Multimedia Retrieval</head><p>In a probabilistic retrieval setting, the goal is to find the document D * with highest probability given a query Q: <ref type="bibr" coords="1,347.88,527.87,12.73,8.74" target="#b0">(1)</ref> is used as a scoring function and a ranked list is returned rather than the one most probable document.</p><formula xml:id="formula_0" coords="1,310.61,491.38,228.65,45.23">D * = arg max i P (D i |Q) = arg max i P (Q|D i )P (D i ) P (Q) (1) Usually,</formula><p>If we assume that all documents have equal prior probability, (1) reduces to the maximum likelihood (ML) criterion, which is approximated by the minimum KL-divergence between query model and document model:</p><formula xml:id="formula_1" coords="1,310.61,609.98,251.13,62.01">D * = arg min i KL[P q (x)||P i (x)]. KL[P q (x)||P i (x)] = P (x|D q ) log P (x|D q ) P (x|D i ) dx = P (x|D q ) log P (x|D q )dx -P (x|D q ) log P (x|D i )dx,</formula><p>where x are feature vectors describing the documents. The first integral is independent of D i and can be ignored, thus</p><formula xml:id="formula_2" coords="2,99.05,171.75,201.60,16.51">D * = arg max i P (x|D q ) log P (x|D i )dx (2)</formula><p>Now suppose query and document models generate a mixture of textual features x t and visual features</p><formula xml:id="formula_3" coords="2,72.00,218.12,208.86,28.64">x v : 2 P (x|D i )) = P (x t |D i )P (t) + P (x v |D i )P (v).</formula><p>We can then integrate over these different feature sets separately and arrive at the following ranking formula for multimodal retrieval <ref type="bibr" coords="2,179.46,278.44,9.96,8.74" target="#b4">[5]</ref>.</p><formula xml:id="formula_4" coords="2,73.48,298.88,227.16,57.87">D * = arg max i [P (t) xt P (x t |D q ) log P (x t |D i )dx t +P (v) xv P (x v |D q ) log P (x v |D i )dx v ] (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Model</head><p>To describe the probability distributions of the textual terms, we take a language modelling approach to information retrieval <ref type="bibr" coords="2,179.70,416.36,9.96,8.74" target="#b1">[2]</ref>. Such a model operates on discrete signals (i.e. words), thus we can replace the integral from (3) by a sum. Moreover, the query model D q is usually nothing more than the empirical distribution of the query, therefore we only need to sum over the words in the query. The document model is usually taken to be a mixture of foreground (P (x t,j |D i )) and background (P (x t,j )) probabilities for the query terms x t,j , interpolated using mixing parameter λ (cf. Section 2.1.1). If our textual query consists of N t terms x t = (x t,1 , x t,2 , . . . , x t,Nt ) then the textual part of our ranking formula is the following.</p><formula xml:id="formula_5" coords="2,78.51,578.15,222.14,60.18">D * t = arg max i 1 N t Nt j=1 log [λP (x t,j |D i ) + (1 -λ)P (x t,j )]<label>(4)</label></formula><p>Using the statistical language modelling approach for video retrieval, we would like to exploit the hierarchical data model of video, in which a video is subdivided in scenes, which are subdivided in shots, which are in turn subdivided in frames. Statistical language models are particularly well-suited for modelling such complex representations of the data. We can simply extend the mixture to include the different levels of the hierarchy, with models for shots and scenes:<ref type="foot" coords="2,340.05,233.98,3.97,6.12" target="#foot_2">3</ref> </p><formula xml:id="formula_6" coords="2,320.57,269.35,218.68,61.58">Shot * = arg max i 1 N t Nt j=1 log[λ Shot P (x t,j |Shot i )+ λ Scene P (x t,j |Scene i ) + λ Coll P (x t,j )] with λ Coll = 1 -λ Shot -λ Scene (5)</formula><p>The main idea behind this approach is that a good shot contains the query terms and is part of a scene having more occurrences of the query terms. Also, by including scenes in the ranking function, we hope to retrieve the shot of interest, even if the video's speech describes it just before it begins or just after it is finished. Depending on the information need of the user, we might use a similar strategy to rank scenes or complete videos instead of shots, that is, the best scene might be a scene that contains a shot in which the query terms (co-)occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Estimating Parameters</head><p>The features in the textual part of our model are simply the words themselves. For the textual part of our retrieval function <ref type="bibr" coords="2,388.74,531.20,11.63,8.74" target="#b4">(5)</ref>, we only need to estimate foreground (P (x t,j |D i )) and background (P (x t,j )) probabilities. Both measures are estimated in the standard way, by taking the term frequency and document frequency respectively <ref type="bibr" coords="2,434.02,579.02,9.97,8.74" target="#b1">[2]</ref>. We used the TREC-2002 video search collection to find the optimal values for the mixing parameters: λ Shot = 0.090, λ Scene = 0.210, and λ Coll = 0.700. Since we trained these parameters on the test collection, we cannot say anything about how well these numbers generalise across collections. <ref type="foot" coords="3,119.66,126.39,3.97,6.12" target="#foot_3">4</ref> Yet, for each of the mixing parameters, there is quite a large range of values for which the scores are close to optimal. In this work we do not look into the stability of these parameters across collections, we are only interested in finding the optimal settings for this collection and evaluating the retrieval model with these optimal settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Model</head><p>We use a Gaussian Mixture Model for describing document densities <ref type="bibr" coords="3,143.24,264.21,9.96,8.74" target="#b3">[4]</ref>.</p><formula xml:id="formula_7" coords="3,97.88,287.22,176.89,30.20">P (x v |D i ) = C c=1 P (θ i,c ) G(x v , µ i,c , Σ i,c ),</formula><p>where C is the number of components in the mixture model, θ i,c is component c of document model D i and G(x, µ, Σ) is the Gaussian density with mean vector µ and co-variance matrix Σ:</p><formula xml:id="formula_8" coords="3,107.00,386.83,193.64,23.70">G(x, µ, Σ) = 1 (2π) n |Σ| e -1 2 x-µ Σ ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9" coords="3,132.30,415.36,141.83,11.98">x -µ Σ = (x -µ) T Σ -1 (x -µ)</formula><p>and n is the length of the feature vector x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Bags of Blocks</head><p>Just like in our textual approach, for the query model, we can simply take the empirical distribution of the query samples. If a query-image x v consists of N v samples:</p><formula xml:id="formula_10" coords="3,72.00,527.77,228.64,21.64">x v = (x v,1 , x v,2 , . . . , x v,Nv ) then P (x v,i |D q ) = 1</formula><p>Nv . For the document model, we take a mixture of foreground and background probabilities, i.e. the (foreground) probability of drawing a query sample from the document's Gaussian mixture model, and the (background) probability of drawing it from any Gaussian mixture in the collection. In other words, the query image is viewed as a bag of blocks (BoB), and its probability is estimated as the joint probability of all its blocks. The BoB measure for query images then becomes:</p><formula xml:id="formula_11" coords="3,319.06,148.61,220.19,60.18">D * v = arg max i 1 N v Nv j=1 log [κP (x v,j | i ) + (1 -κ)P (x v,j )],<label>(7)</label></formula><p>where κ is a mixing parameter and the background probability P (x v,j ) can be found by marginalising over all M documents in the collection:</p><formula xml:id="formula_12" coords="3,356.14,259.83,137.57,30.32">P (x v,j ) = M i=1 P (x v,j |D i )P (D i ).</formula><p>Again we assume uniform document priors (P (D i ) =</p><p>1 M for all i). In text retrieval, one of the reasons for mixing the document model with a collection model is to assign non-zero probabilities to words that are not observed in a document. Smoothing is not necessary in the visual case, since the documents are modelled as mixtures of Gaussians, having infinite support. Another motivation for mixing is to weight term importance: a common sample x (i.e., a sample that occurs frequently in the collection) has a relatively high probability P (x) (equal for all documents), and therefore P (x|D) has only little influence on the probability estimate. In other words, relatively common terms and common blocks influence the final ranking only marginally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Asymptotic Likelihood Approximation</head><p>A disadvantage of using the BoB measure is its computational complexity. In order to rank the collection given a query, we need to compute the posterior probability P (x v |ω i ) of each image block x v in the query for each document ω i in the collection. For evaluating a retrieval method this is fine, but for an interactive retrieval system, optimisation is necessary. An alternative is to represent the query image, like the document image, as a Gaussian model (instead of by its empirical distribution as a bag of blocks), and then compare these two models using the KL-divergence. Yet, if we use Gaussians to model the class conditional densities of the mixture components, there is no closed-from solution for the visual part of the resulting ranking formula (3). As a solution, Vasconcelos assumes that the Gaussians are well separated and derives an approximation, ignoring the overlap between the mixture components: the asymptotic likelihood approximation (ALA) <ref type="bibr" coords="4,265.89,175.78,9.97,8.74" target="#b3">[4]</ref>. The ALA is the measure we used in our official TREC-2002 runs, (see Appendix A). However, in post hoc analysis, we found that one of the assumptions underlying the ALA is not plausible for the collection at hand and, moreover, using it decreases performance compared to the BoB measure (for details see <ref type="bibr" coords="4,270.12,247.51,10.29,8.74" target="#b4">[5]</ref>). In the remainder of this work we will concentrate on the BoB measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Estimating Parameters</head><p>For estimating the parameters of the Gaussian mixture model, we used the EM algorithm <ref type="bibr" coords="4,249.71,334.46,9.96,8.74" target="#b0">[1]</ref>. We described a document as a set of samples, where each sample is described by a number of DCT coefficients in the YCbCr colour space <ref type="foot" coords="4,191.83,368.75,3.97,6.12" target="#foot_4">5</ref> . Then we used EM to fit a mixture of 8 Gaussian (for details see <ref type="bibr" coords="4,264.77,382.28,10.29,8.74" target="#b4">[5]</ref>). Finally, we described the position in the image plane of each component as a 2D-Gaussian with mean and covariance computed from the positions of the samples assigned to this component. We evaluated different values for mixing parameter κ on the TREC-2002 video search collection and found the optimal value: κ = 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Fully automatic creation of queries from topic descriptions was not required in this year's video track. However, there was a distinction between manual and interactive runs. In an interactive run a user can interact with a system to locate relevant shot. In a manual run a user has one go at creating a query from a topic descriptions and then submits this query to the system to retrieve relevant shots. All our runs are manual runs in which we experimented with different ways of creating queries from topic statements. In the following subsections, we investigate the following questions:</p><p>• Is it useful to identify important parts in query images?</p><p>• Can using (additional) query images from outside the search collection help improve retrieval results?</p><p>• Does it help to have multiple image examples for a query, or are we better of using only one good example?</p><p>• Can a combination combined textual and visual query perform better than queries in a single modality?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Selecting Query Images</head><p>In general, it is hard to guess what would be a good example image for a specific query. If we look for shots of the Golden Gate bridge, we might not care from what angle the bridge was filmed, or if the clip was filmed on a sunny or a cloudy day; visually however, such examples may be very different (Figure <ref type="figure" coords="4,310.61,426.38,3.88,8.74" target="#fig_0">1</ref>). If a user has presented three examples and no additional information, the best we can do is try to find documents that describe all example images well.</p><p>Unfortunately, a document may be ranked low even though it models the samples from one example image well, as it may not explain the samples from the other images.</p><p>For each topic, we computed which of the example images would have given the best results if it had been used as the only example for that topic. We compared these best example results to the full topic results in which we used all available visual examples. In the full topic case, the set of available topics was regarded as one large bag of blocks. We ranked documents by their probability of generating all blocks in all query images. For the single image queries in the best example, we used all samples from the single visual example to rank documents.</p><p>Since it is problematic to use multiple examples in a query, we wanted to see if it is possible to guess in advance what would be a good example for a specific topic. Therefore, we hand-picked for each topic a single representative from the available examples and compared these manual example results to the other two result sets.</p><p>The results for the different settings are listed in Table <ref type="table" coords="5,99.59,303.59,3.88,8.74">1</ref>. A first thing to notice is that all scores are rather low. When we take a closer look at the topics with higher average precision scores, we see that these mainly contain examples from the search collection. In other words, we can find similar shots from within the same video, but generalisation is a problem.</p><p>The fact that using the best image example outperforms the use of all examples shows that indeed combining results from different visual examples can degrade results. Looking at the results, manually selecting good examples seems a non-trivial task, but the drop in performance is partly due to the generalisation problem. If one of the image examples happens to come from the collection it scores high. If we fail to select that particular example, the score for the manual example run drops. Simply counting how often the manually selected example was the same as the best performing example, we see that this was the case for 8 out of 13 topics. 6   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Selecting Important Regions</head><p>In last year's video track, we saw that query articulation, i.e. the manual identification of important parts in a query image, can help improve retrieval results <ref type="bibr" coords="5,95.45,603.65,9.97,8.74" target="#b2">[3]</ref>. We also noticed that this requires an enormous effort from a user. In our probabilistic setting, selecting important (and coherent) regions is much 6 We ignored the topics for which there is only one example and the ones for which none of the examples retrieved relevant documents. In retrieval, we can then use only the Bag of Blocks corresponding to the selected component(s). For example in Figure <ref type="figure" coords="6,143.67,187.74,8.49,8.74" target="#fig_2">2a</ref>, we selected the components that together form the US flag. Similarly, we can indicate we want multiple parts to be present in the target shots, e.g. boat and water and sky (Figure <ref type="figure" coords="6,258.37,223.60,8.58,8.74" target="#fig_2">2b</ref>). Note that even though the union of the sets of samples is in this case the full image, this differs from simply taking the using all samples as a query. If the full image were used, we would have looked for shots with relatively few water samples; the selection of components compensates for that and looks for documents that explain all 3 concepts equally well.</p><p>From each of the query images, we selected meaningful components and we used the corresponding samples as queries. If we take a look at the individual components and their results, we see that the components are often homogeneous in colour and/or texture and that results are often meaningful (Figure <ref type="figure" coords="6,72.00,390.97,4.43,8.74" target="#fig_3">3</ref>) or, if there is little semantics in the component, at least visually similar (Figure <ref type="figure" coords="6,203.22,402.93,3.87,8.74">4</ref>). It is not clear yet how this can be used for highly specific queries like the video track queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using Query Examples from Outside the Collection</head><p>In Section 3.1, we argued that selecting the right query image is important. On the one hand therefore, one would like to expand a query to have as many different query images as possible. On the other hand, we saw that it is difficult to combine multiple examples in one query (Section 3.1). We investigate whether using (additional) examples from outside the collection can improve retrieval effectiveness. We expect that this is not the case; in previous experiments <ref type="bibr" coords="6,72.00,594.21,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="6,86.26,594.21,7.74,8.74" target="#b2">3]</ref> we saw that we can only find relevant shots if the query images are highly similar to the relevant shots, i.e. if they are from the same collection and preferably from the same video. First of all, we had a look at the original examples provided by NIST. Most, if not all, of the video examples in this set come from either the search col-lection itself, or the highly comparable<ref type="foot" coords="6,477.50,126.39,3.97,6.12" target="#foot_5">7</ref> feature train or feature test set. We found that the topics that contributed most to our MAP score were the ones with examples from the search collection. If we remove videos from which shots are used as examples from the relevance judgements, our MAP score for a purely visual run (using full examples for all queries) drops from .0287 to .0029; purely visual runs from other groups show a similar drop in performance. This indicates that visual retrieval systems are able to locate the query examples in the collection, but generalisation seems problematic. Furthermore, the best examples as reported in table 1 are mainly video examples from either the search collection or the comparable training data. Only for three topics, the best scoring example was an image example from outside these collections. Yet, for these three topics no video examples were available.</p><p>We experimented with query expansion by adding additional example images found using Google image search<ref type="foot" coords="6,337.53,365.49,3.97,6.12" target="#foot_6">8</ref> . We manually created short queries from the topic descriptions and submitted these to Google image search. From the result list we selected images based that we thought were good examples for the topic. This way we expanded topics with up to 7 additional image examples. We ran these new examples as queries against the collection and recomputed the best scoring examples for each topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Combining Textual and Visual runs</head><p>We combined textual and visual runs using our combined ranking formula (3). Since we had no data to estimate the parameters for mixing textual and visual information we used P (t) = P (v) = 0.  <ref type="table" coords="7,139.26,400.88,4.98,8.74" target="#tab_1">2</ref> shows the results for combinations with the BoB measure. We also experimented with combinations with the ALA measure, but we found that in the ALA case it is difficult to combine textual and visual scores, because they are on different scales (see also Appendix A). The BoB measure is closer to the KL-divergence and, on top of that, more similar to our textual approach, and thus easier to combine with the textual scores.</p><p>For most of the topics, textual runs give the best results, however for some topics using the visual examples is useful. This is mainly the case when either the topics come from the search collection or when the relevant documents are outliers in the collection. This illustrates how difficult it is to search a generic video collection using visual information only. We only succeed if the relevant documents are either highly similar to the examples provided or very dissimilar from the other documents in the collection (and therefore relatively similar to the query examples). When both textual and visual runs have reasonable scores, combining the runs can improve on the individual runs, however, when one of them has inferior performance, a combination only adds noise and lowers the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We presented a probabilistic framework for multimodal retrieval in which textual and visual retrieval models are integrated seamlessly and evaluated the framework using the search task from the TREC-2002 video track. We found that even though the topics were specifically designed for content-based retrieval, and relevance was defined visually, a textual search outperforms visual search for most topics. The main conclusion in this work is that visual retrieval using the presented model for specific queries does not generalise very well. The model could retrieve shots that are highly similar to the query examples (i.e. shots from the same video), but other similar shots were found mostly by coincidence, because they happened to have for example the same colour sky or grass. For more general queries, the model seems useful. When we select a single component from an example, results are intuitive, i.e. visually similar. It is unclear how this helps in retrieving relevant documents for highly  specific topics like the video track topics, but it helps in gaining insight in the models performance. In future work, we will further investigate the influence of individual components on retrieval results. In addition, we intend to look at how incorporating different sources of additional information (e.g. contextual frames, the movement in video or user interaction) can help improve results across collections. Combining multiple examples in one query is still problematic, but combining textual and visual runs seems possible using the presented framework. When one of the runs is poor, a combined run, including the noise, is less effective than the single best run. However, when the individual runs have reasonable scores, combining them improves retrieval effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,188.34,201.70,234.57,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visual examples of the Golden Gate bridge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,497.53,450.75,41.72,8.74;6,310.61,462.71,228.64,8.74;6,310.61,474.66,228.64,8.74;6,310.61,486.62,228.64,8.74;6,310.61,498.57,228.64,8.74;6,310.61,510.53,228.65,8.74;6,310.61,522.48,228.64,8.74;6,310.61,534.44,228.64,8.74;6,310.61,546.39,228.64,8.74;6,310.61,558.35,160.05,8.74"><head></head><label></label><figDesc>For 5 out of 25 topics none of the examples retrieved any relevant documents. The best scoring examples for the remaining 20 topics were video examples in 12 cases and image examples from Google in 8 cases. Clearly, if we try more examples we have a better chance of having a good example among them, yet the problem remains how to combine multiple examples or how to identify a good example without knowledge of the relevant documents in the collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,208.89,345.15,193.46,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Selecting components from images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,137.09,297.56,337.07,8.74;9,72.00,209.96,88.78,60.53"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top 5 results for a homogeneous query with clear semantics ('Sky')</figDesc><graphic coords="9,72.00,209.96,88.78,60.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,77.98,206.32,483.95,376.05"><head>Table 2 :</head><label>2</label><figDesc>Average precision per topic, for Textual runs, BoB runs and combined runs</figDesc><table coords="8,77.98,206.32,483.95,342.23"><row><cell>Topic Tshort</cell><cell>Tlong</cell><cell>BoBfull</cell><cell>BoBbest</cell><cell>BoBfull +Tshort</cell><cell>BoBfull +Tlong</cell><cell>BoBbest +Tshort</cell><cell>BoBbest +Tlong</cell></row><row><cell>vt075 .0000</cell><cell>.0082</cell><cell>.0038</cell><cell>.2438</cell><cell>.0189</cell><cell>.0569</cell><cell>.2405</cell><cell>.3537</cell></row><row><cell>vt076 .4075</cell><cell>.6242</cell><cell>.4854</cell><cell>.4323</cell><cell>.5931</cell><cell>.7039</cell><cell>.5757</cell><cell>.6820</cell></row><row><cell>vt077 .1225</cell><cell>.5556</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell></row><row><cell>vt078 .1083</cell><cell>.2778</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell></row><row><cell>vt079 .0003</cell><cell>.0006</cell><cell>.0000</cell><cell>.0040</cell><cell>.0003</cell><cell>.0000</cell><cell>.0063</cell><cell>.0050</cell></row><row><cell>vt080 .0000</cell><cell>.0000</cell><cell>.0048</cell><cell>.0977</cell><cell>.0066</cell><cell>.0059</cell><cell>.0845</cell><cell>.0931</cell></row><row><cell>vt081 .0154</cell><cell>.0333</cell><cell>.0000</cell><cell>.0000</cell><cell>.0037</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell></row><row><cell>vt082 .0080</cell><cell>.0262</cell><cell>.0330</cell><cell>.0234</cell><cell>.0181</cell><cell>.0335</cell><cell>.0145</cell><cell>.0210</cell></row><row><cell>vt083 .1669</cell><cell>.1669</cell><cell>.0000</cell><cell>.0000</cell><cell>.0962</cell><cell>.0962</cell><cell>.0078</cell><cell>.0078</cell></row><row><cell>vt084 .7500</cell><cell>.7500</cell><cell>.0046</cell><cell>.0046</cell><cell>.6875</cell><cell>.6875</cell><cell>.6875</cell><cell>.6875</cell></row><row><cell>vt085 .0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell></row><row><cell>vt086 .0554</cell><cell>.0676</cell><cell>.0053</cell><cell>.0704</cell><cell>.0536</cell><cell>.0215</cell><cell>.0791</cell><cell>.0600</cell></row><row><cell>vt087 .0591</cell><cell>.0295</cell><cell>.0000</cell><cell>.0000</cell><cell>.0052</cell><cell>.0003</cell><cell>.0052</cell><cell>.0003</cell></row><row><cell>vt088 .0148</cell><cell>.0005</cell><cell>.0046</cell><cell>.0069</cell><cell>.0052</cell><cell>.0046</cell><cell>.0069</cell><cell>.0069</cell></row><row><cell>vt089 .0764</cell><cell>.0764</cell><cell>.0000</cell><cell>.0000</cell><cell>.0503</cell><cell>.0503</cell><cell>.0045</cell><cell>.0045</cell></row><row><cell>vt090 .0229</cell><cell>.0473</cell><cell>.0000</cell><cell>.0305</cell><cell>.0006</cell><cell>.0075</cell><cell>.0356</cell><cell>.0477</cell></row><row><cell>vt091 .0000</cell><cell>.0000</cell><cell>.0095</cell><cell>.0095</cell><cell>.0000</cell><cell>.0086</cell><cell>.0000</cell><cell>.0086</cell></row><row><cell>vt092 .0627</cell><cell>.0687</cell><cell>.0003</cell><cell>.0106</cell><cell>.0191</cell><cell>.0010</cell><cell>.0078</cell><cell>.0106</cell></row><row><cell>vt093 .1977</cell><cell>.1147</cell><cell>.0006</cell><cell>.0006</cell><cell>.0099</cell><cell>.0021</cell><cell>.0071</cell><cell>.0012</cell></row><row><cell>vt094 .0232</cell><cell>.0252</cell><cell>.0021</cell><cell>.0021</cell><cell>.0122</cell><cell>.0036</cell><cell>.0122</cell><cell>.0036</cell></row><row><cell>vt095 .0034</cell><cell>.0021</cell><cell>.0000</cell><cell>.0000</cell><cell>.0008</cell><cell>.0012</cell><cell>.0011</cell><cell>.0010</cell></row><row><cell>vt096 .0000</cell><cell>.0000</cell><cell>.0323</cell><cell>.0323</cell><cell>.0161</cell><cell>.0161</cell><cell>.0323</cell><cell>.0323</cell></row><row><cell>vt097 .1002</cell><cell>.0853</cell><cell>.1312</cell><cell>.1408</cell><cell>.1228</cell><cell>.1752</cell><cell>.1521</cell><cell>.1474</cell></row><row><cell>vt098 .0225</cell><cell>.0086</cell><cell>.0000</cell><cell>.0003</cell><cell>.0068</cell><cell>.0000</cell><cell>.0004</cell><cell>.0003</cell></row><row><cell>vt099 .0726</cell><cell>.0606</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell><cell>.0000</cell></row><row><cell>MAP .0916</cell><cell>.1212</cell><cell>.0287</cell><cell>.0444</cell><cell>.0691</cell><cell>.0750</cell><cell>.0784</cell><cell>.0870</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,648.37,213.40,6.99;1,72.00,657.16,228.64,7.86;1,72.00,667.30,17.00,6.99"><p>Throughout this document, we refer to the search collection used in the TREC-2002 video track as the search collection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,87.24,648.37,213.40,6.99;2,72.00,657.84,228.64,6.99;2,72.00,667.30,98.72,6.99"><p>P (t) and P (v) are the prior probabilities of drawing respectively textual or visual features from a document; assumed uniform across documents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,325.85,657.84,212.90,7.82;2,310.61,667.30,41.74,7.82"><p>We assume each shot is a separate class and replace ω i with Shot i .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,87.24,657.84,213.40,6.99;3,72.00,667.30,120.29,6.99"><p>Obviously, the official runs have used different mixing parameter values, see Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,87.24,657.84,213.40,6.99;4,72.00,667.30,189.10,6.99"><p>We use the first 10 coefficients from the Y channel and the two DC coefficients of the Cb and the Cr channels.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="6,325.85,657.80,211.72,6.99"><p>In fact, these are distinct subsets of one larger collection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="6,325.85,667.88,101.62,6.64"><p>http://images.google.com</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Official Results</head><p>In the official runs, we used the Asymptotic Likelihood Approximation (see section 2.2.2). We distinguished between the NIST images (the visual examples from the official topics) and Google images (additional examples we found with manual query expansion using Google and submitted four runs:  The text model's mixing parameters have been optimized using the TREC-2001 corpus, giving λ Shot = 0.015, λ Scene = 0.135, and λ Coll = 0.850. For run3 and run4, we manually selected important components from the query model (cf. Section 3.2). In all runs that involved visual examples, we computed a single new (8 component) Gaussian mixture model from all available visual blocks and we used that model in our ALA ranking formula. The results for the official runs and for the same runs after fixing some bugs 9 are shown in Table <ref type="table" coords="10,213.78,558.88,3.88,8.74">3</ref>. We see that also with the ALA measure text only results are by far the best (run 1). Combinations that also used visual information scored lower, not only on MAP, but also on average precision for each individual topic. In contrast to our findings with the BoB measure we were not able to combine textual and visual information </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.50,566.33,213.14,8.74;9,87.50,578.29,213.14,8.74;9,87.50,590.24,213.15,8.74;9,87.50,602.20,115.41,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,256.64,566.33,44.00,8.74;9,87.50,578.29,213.14,8.74;9,87.50,590.24,22.62,8.74">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,122.63,590.24,178.02,8.74;9,87.50,602.20,33.03,8.74">Journal of the Royal Statistical Society, series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.50,630.08,213.14,8.74;9,87.50,642.03,213.14,8.74;9,87.50,653.99,213.15,8.74;9,87.50,665.94,60.88,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,153.37,630.08,147.27,8.74;9,87.50,642.03,69.04,8.74">Using language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Centre for Telematics and Information Technology, University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="9,326.10,329.94,213.14,8.74;9,326.10,341.90,213.15,8.74;9,326.10,353.85,205.42,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,424.13,329.94,115.11,8.74;9,326.10,341.90,156.55,8.74">Lazy users and automatic video retrieval tools in (the) lowlands</title>
	</analytic>
	<monogr>
		<title level="m" coord="9,326.10,329.94,86.00,8.74;9,501.25,341.90,38.00,8.74;9,326.10,353.85,112.71,8.74">The 10th Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>The Lowlands team. TREC-2001</note>
</biblStruct>

<biblStruct coords="9,326.10,372.27,213.15,8.74;9,326.10,384.23,213.14,8.74;9,326.10,396.18,124.59,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,400.52,372.27,138.73,8.74;9,326.10,384.23,84.34,8.74">Bayesian Models for Visual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institut of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="9,326.10,414.60,213.15,8.74;9,326.10,426.56,213.14,8.74;9,326.10,438.51,213.14,8.74;9,326.10,450.47,213.15,8.74;9,326.10,462.42,213.15,8.74;9,326.10,474.38,213.15,8.74;9,326.10,486.33,22.70,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,499.09,426.56,40.15,8.74;9,326.10,438.51,213.14,8.74;9,326.10,450.47,20.76,8.74">A probabilistic multimedia retrieval modela and its evaluation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Ballegooij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M G</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,364.61,462.42,174.64,8.74;9,326.10,474.38,208.74,8.74">special issue on Unstructured Information Management from Multimedia Data Sources</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
