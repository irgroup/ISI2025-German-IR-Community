<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,131.43,74.81,350.94,23.69;1,207.02,105.05,199.99,23.69">The YorkQA Prototype Question Answering System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,169.59,144.59,67.97,10.17"><forename type="first">Marco</forename><surname>De Boni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>Y010 5DD</postCode>
									<settlement>York</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.30,144.59,106.55,10.17"><forename type="first">Jose-Luis</forename><surname>Jara-Valencia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>Y010 5DD</postCode>
									<settlement>York</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.41,144.59,85.00,10.17"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>Y010 5DD</postCode>
									<settlement>York</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,131.43,74.81,350.94,23.69;1,207.02,105.05,199.99,23.69">The YorkQA Prototype Question Answering System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B215F2E6CA610C452702933B4FAA3DC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A preliminary analysis of our QA system implemented for TREC-11 is presented, with an initial evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The aim of our system in this year's TREC QA track was 1) to see how much our previous system could be improved simply by removing bugs and inconsistencies and 2) to test new techniques on "real" data. The system produced was therefore again a prototype experimental system rather than a "complete" machine ready for deployment. In particular it still lacks an information retrieval engine (we relied again on the documents retrieved by NIST's PRISE IR engine), and has only an outline Named Entity Recogniser and an incomplete answer extraction module. Nevertheless it did give an indication as to which ideas are most promising and should be investigated further, in particular as regards determining an answer in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvements on the previous version</head><p>A close examination of the system used for TREC 2001 <ref type="bibr" coords="1,152.91,642.67,100.23,10.17" target="#b0">[Alfonseca et al. 2002</ref>] revealed a number of small bugs across all modules which were significant enough to affect performance. These were corrected for this year's entry. Furthermore, a close analysis of our results taking into account the contribution of each module revealed that a number of components we used did not improve performance and in fact were detrimental. In particular, the Noun Phrase Chunker which we used last year was removed as its output was not precise enough to be used productively. The question recogniser which we previously used proved very hard to maintain and improve, based as it was on a large number of patterns and exceptions with very limited use of linguistic resources. It was therefore re-written from scratch in a much more elegant way in order to make much more use of linguistic resources such as WordNet. The Named Entity recogniser was also rewritten from scratch, as was the answer extractor, which had to cope with this year's track aim, which was to extract an exact answer as opposed to a string of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Recogniser</head><p>A simple set of rules allowed the question recogniser to determine the focus of the question. The initial recognition, however, was too refined to be of much use: given a question such as"What president's wife commented on the affair", it would return a question type of "president's wife". The WordNet is-a hierarchy was therefore used to determine a less fine-grained answer type. WordNet, however, is deceptive without accurate word-sense disambiguation: satellite is a person, if satellite is intended in the sense of synset 107546753, "a person who follows or serves another", but not if it is intended in the more common meaning of synset 103275905, "a man-made object that orbits around the earth". Given that we did not have an accurate word-sense disambiguation module, we resorted to assuming that the meaning of any word was the most common word and hand-crafted a series of rules which reflected this (e.g. our rules state that "satellite" is not a person, but is a "thing"). On the other hand WordNet could not be used to determine whether a question was looking for MUC entities such as locations due to inconsistencies such as the fact that according to the hypernym hierarchy, a city is a location, but a lake is not; again handcrafter rules were applied to state facts such as "sea is a location" and hence determine that a question asking "What seaâ€¦" could be answered by looking for a location Named Entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recogniser</head><p>Texts were initially tokenized by applying several hand-coded heuristics. Then they were tagged using the TnT part-of-speech tagger <ref type="bibr" coords="2,128.31,435.93,63.13,10.17" target="#b2">(Brants 2000)</ref>. Sentences were then split by an algorithm that utilises a number of heuristics and a list of abbreviations extracted by another algorithm that uses active learning when the case is highly ambiguous. There is no formal evaluation of this sentence splitter, but it appears to work well in most of cases. For the Named entity recognition YorkQA system uses our own implementation of Nymble <ref type="bibr" coords="2,137.31,565.88,84.72,10.17">(Bikel et.al. 1997</ref>) which utilises hidden Markov models to identify named entities in the text. An improved version of this algorithm showed high accuracy in MUC-7. We estimates that the this version of Nymble is reaching about 70% recall and 80% precision. These results are a big improvement form YorkQA-2001. However, the program identifies only MUC entities (person names, organisation names, location names, dates, time expressions, currency expression and percentage expression). This means that for question about other type of entities (speeds, distances, durations, etc.) our system has much less information to work with. We also detected that in question asking for fine-grain pieces of information (e.g. the first name of a person), this module turned out to be inappropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Distance Metrics</head><p>The semantic distance metric used for the TREC-10 was improved (see De Boni and Manandhar 2002 for an in-depth explanation of the implementation) and then subjected to a thorough analysis of its components. In particular we evaluated different approaches to the measure of semantic relevance for question answering in order to decide how the use of WordNet information, part-ofspeech information and head-phrase chunking could provide a reliable measurement for semantic relevance. It was seen that a semantic distance metric based on all WordNet relations (is-a, satellite, similar, pertains, meronym, entails, etc) and using compound word information, proper noun identification, part-of-speech tagging and word frequency information gave best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Word Finder</head><p>Our Named Entity recogniser only recognised a small number of entity types, which did not correspond to all types identified by the question type recogniser. In order to determine possible answers, the system made use of Wordnet's is-a relationships, looking in the answer sentence which had the highest semantic similarity measure for hypernyms of the question type. This approach appeared to be fruitful in determining "rare" entities which it would be impractical to build a Named Entitiy recogniser to tag: a good example was the question asking the name of King Arthur's sword: the question type was "sword", the answer found was Excalibur as Excalibur was listed in WordNet as a hypernym of sword.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Further Work</head><p>This year's task confirmed that Question Answering is a complex task which requires accurate component parts: underperformance in any of the components results in a deterimental effect to the system as a whole. In such a complex system, however, it is not a simple task to determine why things go wrong and indeed why things go right. Nevertheless is is important to determine how the individual parts interact in order to improve performance, and, while such an analysis can be extremely laborious, it is necessary. The disadvantages of working in a very small team were also highlighted, as the complexity of the problem requires extensive work in disparate area. Future work will include an extension an improvement of our Named Entity component, the use of an information retrieval engine (as opposed to relying on the documents provided by NIST), improvements in the answer locator and possibly some inference mechanism.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,94.35,476.00,205.73,10.17;3,94.35,488.96,205.67,10.17;3,94.35,502.04,205.67,10.17;3,94.35,515.00,205.73,10.17;3,94.35,527.96,205.80,10.17;3,94.35,541.04,205.73,10.17;3,94.35,554.00,25.50,10.17" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,188.30,488.96,111.72,10.17;3,94.35,502.04,205.67,10.17;3,94.35,515.00,197.51,10.17">A prototype Question Answering system using syntactic and semantic information for answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Boni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Jara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<idno>TREC-10</idno>
	</analytic>
	<monogr>
		<title level="m" coord="3,109.35,527.96,190.81,10.17;3,94.35,541.04,52.05,10.17">Proceedings of the 10th Text Retrieval Conference</title>
		<meeting>the 10th Text Retrieval Conference<address><addrLine>Gaithersburg, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,94.35,580.03,205.79,10.17;3,94.35,592.99,55.50,10.17;3,166.59,592.99,10.38,10.17;3,193.71,592.99,44.06,10.17;3,254.54,592.99,5.02,10.17;3,276.14,592.99,23.92,10.17;3,94.35,605.95,205.67,10.17;3,94.35,618.91,205.67,10.17;3,94.35,631.99,72.97,10.17;3,94.35,644.95,174.53,10.17" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,199.21,592.99,38.55,10.17;3,254.54,592.99,5.02,10.17;3,276.14,592.99,23.92,10.17;3,94.35,605.95,173.44,10.17">Nymble: a highperformance learning name-finder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,94.35,618.91,205.67,10.17;3,94.35,631.99,72.97,10.17;3,94.35,644.95,94.46,10.17">Proceedings of the Fifth Conference on Applied Natural Language Processing</title>
		<meeting>the Fifth Conference on Applied Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,94.35,670.99,205.83,10.17;3,94.35,683.95,163.73,10.17;3,313.70,70.92,205.73,10.17;3,313.70,84.00,205.71,10.17;3,313.70,96.95,205.80,10.17;3,313.70,109.91,205.61,10.17;3,313.70,122.87,25.38,10.17" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,160.12,670.99,140.06,10.17;3,94.35,683.95,62.08,10.17;3,319.17,84.00,200.23,10.17;3,313.70,96.95,38.41,10.17">Automated discovery of telic relations for WordNet</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Boni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,392.29,96.95,127.21,10.17;3,313.70,109.91,165.69,10.17">Proceedings of the first International WordNet conference</title>
		<meeting>the first International WordNet conference<address><addrLine>India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000. 2002</date>
		</imprint>
	</monogr>
	<note>TnT -A statistical Part-Of-Speech tagger</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
