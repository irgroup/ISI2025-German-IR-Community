<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.80,137.20,342.45,14.02;1,178.20,156.70,258.51,14.02">TREC-11 Experiments at NII: The Effect of Virtual Relevant Documents in Batch Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,196.32,212.55,74.18,9.61"><forename type="first">Kyung-Soon</forename><surname>Lee</surname></persName>
							<email>kslee@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">NII (National Institute of Informatics</orgName>
								<address>
									<addrLine>2-1-2 Hitotsubashi, Chiyoda-ku</addrLine>
									<postCode>101-8430</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.02,212.55,58.91,9.61"><forename type="first">Kyo</forename><surname>Kageura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NII (National Institute of Informatics</orgName>
								<address>
									<addrLine>2-1-2 Hitotsubashi, Chiyoda-ku</addrLine>
									<postCode>101-8430</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.91,212.55,64.52,9.61"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NII (National Institute of Informatics</orgName>
								<address>
									<addrLine>2-1-2 Hitotsubashi, Chiyoda-ku</addrLine>
									<postCode>101-8430</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.80,137.20,342.45,14.02;1,178.20,156.70,258.51,14.02">TREC-11 Experiments at NII: The Effect of Virtual Relevant Documents in Batch Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">35BCFBAE0541B44C4E4C6791AE6D2DF1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In machine learning techniques, many researches have shown the effectiveness according to training examples by sampling from training set and incorporating prior knowledge into training set.</p><p>Researches on document retrieval, text categorization and routing have shown the effects of learning by sampling relevant documents or non-relevant document from training set. <ref type="bibr" coords="1,375.12,455.83,72.43,8.77">Allan et al. (1995)</ref> considered only the top K non-relevant documents, which is the same number of all known relevant documents in the training set to learn a routing query. This is motivated by the need to have a balance between the number of the relevant and the negative documents in Rocchio's learning. <ref type="bibr" coords="1,325.46,508.33,83.19,8.77" target="#b6">Singhal et al. (1997)</ref> selectively used the nonrelevant documents that belong to a query's domain to learn the feedback query. <ref type="bibr" coords="1,404.54,525.85,108.02,8.77" target="#b3">Kwok and Grunfeld (1997)</ref> selected  <ref type="bibr" coords="1,82.74,653.29,65.81,8.77" target="#b1">Schölkopf, 2002)</ref>. For example, image recognition system uses new examples by small distortions of the input image such as translations, rotations, scaling; speech recognition system produces those by time distortions or pitch shifts. In 3D object recognition problem, <ref type="bibr" coords="1,265.59,688.27,98.14,8.77" target="#b4">Poggio and Vetter (1992)</ref>   <ref type="bibr" coords="2,163.54,190.33,65.72,8.77" target="#b1">Schölkopf, 2002)</ref>.</p><p>In TREC-11 batch filtering, we have incorporated prior knowledge called virtual relevant documents to training documents by combining each two relevant documents pair and giving distinct weight for cooccurring terms on assumption that they might be related to the topic. Support vector machine (SVM) was used to learn decision boundary for the artificially enlarged training documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Virtual Relevant Documents at Batch Filtering</head><p>Intuitively, a document produced by concatenating two relevant documents will be relevant to the topic since one large size of document can be divided into two documents while preserving the topic. And relevant documents will share terms which describe the topic. This characteristic has been used in feature selection.</p><p>Therefore, prior knowledge generated by multiplying weights of a term which is co-occurring in each two relevant documents pair will provide new information about the decision boundary for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Virtual Relevant Documents</head><p>A document is represented as a weight vector, di = &lt;w 1 , w 2 ,…, w k , …, w n &gt;. The weight is calculated by LogTF, IDF and cosine normalization.</p><p>A virtual relevant document (VRD) is generated by combining two relevant documents in training documents. For n relevant documents, n*(n-1)/2 documents are produced:</p><formula xml:id="formula_0" coords="2,167.22,526.59,337.38,27.87">1 2 ) 1 ( 2 ⋅ - ⋅ = n n C n (1)</formula><p>The weight of term which occurs in two relevant documents is calculated by multiplying two weights of a term of each vector. The weight of a VRD is calculated as follows:</p><formula xml:id="formula_1" coords="2,168.42,601.43,332.77,26.71">k k k dj di vij w w w ⋅ = (2)</formula><p>where vij k is the term k of a VRD, di k and dj k is the term k of relevant document di and dj, respectively. If the term k does not occur in one document of two relevant documents, the weight is assigned as minimum value instead of zero value and is multiplied to keep the term's existence. Finally, the weight vector of terms is normalized by cosine normalization.</p><p>The effect of VRD is that if two relevant documents do not have any sharing term, the resulting VRD become generalized vector of two documents. If two relevant documents share common terms, the resulting VRD would represent strong indicator of relevance to the topic for co-occurring terms. In case of general terms which are not related to the topic, they will have low value by idf in basic vector representation.</p><p>Therefore, their effects would not be strong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Support Vectors</head><p>Given training documents which include not only training documents but also VRDs, we have used support vector machine <ref type="bibr" coords="3,149.35,281.76,58.72,8.77" target="#b5">(Vapnik, 1995)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Procedure</head><p>In the runs (kNII11bf1 and kNII11bf2) submitted to TREC-11 batch filtering, VRDs are generated from whole relevant documents in training set (VRDs_TRs). In additional experiments, VRDs are generated from support vector set obtained after training SVM for training set (VRDs_SVs). In this paper, we compare two incorporating methods for batch filtering.</p><p>We used SVM light system <ref type="bibr" coords="3,194.11,536.52,66.08,8.77" target="#b2">(Joachims, 1999)</ref>, and trained classifiers via radial-basis function (RBF) kernels and left all SVM light options that affect learning as their default value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Submitted Runs</head><p>We have submitted two runs tagged kNII11bf1 and kNII11bf2. In the submitted runs, VRDs were generated from whole training documents. In kNII11bf1, VRDs were generated by multiplying weights from two relevant documents, and subtracting terms in non-relevant documents from in relevant documents. It also included virtual non-relevant documents produced by averaging weights from two non-relevant documents, and subtracting terms in relevant documents from in non-relevant documents. In kNII11bf2, VRDs were generated by multiplying weights.</p><p>For evaluation measure T11U and T11F, refer TREC-11 filtering track guideline. For the assessor topic, the performances of kNII11bf1 and kNII11bf2 on MeanT11U are 0.305 and 0.302, respectively. The performances of kNII11bf1 and kNII11bf2 on MeanT11F are 0.190 and 0.188, respectively. The results of two runs are almost similar. It means that virtual non-relevant documents do not affect the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Additional Experimental Results</head><p>We have compared the effectiveness for VRDs generated from different sources:</p><p>Org training set: performance of SVM for training set.</p><p>VRDs_SVs: the performance of SVM after incorporating prior knowledge generated from relevant support vector set into support vector set.</p><p>Table <ref type="table" coords="4,106.63,315.79,4.87,8.77" target="#tab_1">1</ref> shows the performance for assessor topics (from R101 to R150).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In TREC-11 batch filtering, we have incorporated virtual relevant documents to training documents by combining each two relevant documents pair on assumption that they might be related to the topic. Support vector machine was used to learn from the artificially enlarged training documents. By adding virtual relevant documents generated by transformation of original documents to training set, we could improve performance significantly. However, the base performance of SVM on the training set is low for TREC-11 test collection.</p><p>For many topics, SVM system classified test documents as non-relevant to the many topics. For future work,</p><p>VRDs can be applied in other classification model and adapted new virtual transformation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,117.00,543.37,395.65,8.77;1,82.74,560.83,392.41,8.77;1,92.46,583.21,420.21,8.77;1,82.74,600.79,429.94,8.77;1,82.74,618.25,429.94,8.77;1,82.74,635.77,429.97,8.77"><head></head><label></label><figDesc>the best training subset of the relevant documents for creation of a feedback query based on genetic algorithm. Most sampling techniques in machine learning aim at the reducing the size of training set. On the other hand, many machine learning applications on image recognition, image classification and character recognition have incorporated prior knowledge about the desired behavior of the system into training data. Prior knowledge is information for the learning which is available in addition to the training examples and makes it possible to generalize from the training examples to novel test examples (DeCoste and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,208.07,281.76,304.57,8.77;3,82.74,299.29,429.89,8.77;3,82.74,316.75,204.23,8.77;3,92.46,339.13,420.25,8.77;3,82.74,356.65,429.93,8.77;3,82.74,374.11,302.85,8.77"><head></head><label></label><figDesc>. Support vectors (SVs) are essential subset of relevant and non-relevant examples in training set. They represent the whole training examples. In test phase, SVs are used for determining on which side of the decision boundary.Scholkopf et al. (1995) and<ref type="bibr" coords="3,205.18,339.13,56.74,8.77" target="#b5">Vapnik (1995)</ref> observed that the SV set contains all information necessary to solve a given classification task. In handwritten digit recognition task, DeCoste and<ref type="bibr" coords="3,411.83,356.65,68.48,8.77" target="#b1">Schölkopf (2002)</ref> showed that it is sufficient to generate virtual examples only from the support vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,93.78,333.31,340.63,90.19"><head>Table 1 .</head><label>1</label><figDesc>The performances on original training set and incorporating VRDs into SV set.</figDesc><table coords="4,105.42,353.17,327.87,70.33"><row><cell>Evaluation Measure</cell><cell>Org training set</cell><cell>VRDs_SVs</cell><cell>Performance change</cell></row><row><cell>Mean T11U</cell><cell>0.359</cell><cell>0.376</cell><cell>4.7%</cell></row><row><cell>Mean T11F</cell><cell>0.090</cell><cell>0.190</cell><cell>111.1%</cell></row><row><cell>Avg. Precision</cell><cell>0.269</cell><cell>0.400</cell><cell>48.7%</cell></row><row><cell>Avg. Recall</cell><cell>0.046</cell><cell>0.101</cell><cell>119.6%</cell></row><row><cell>Micro-avg. F1</cell><cell>0.181</cell><cell>0.310</cell><cell>71.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,82.74,434.52,429.88,135.55"><head>Table 2 .</head><label>2</label><figDesc>The statistics of information used in support vector learning (RDs: relevant documents, NRDs: non-relevant documents).</figDesc><table coords="4,302.46,478.03,136.95,8.77"><row><cell>Org training set</cell><cell>VRDs_SVs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,82.74,598.62,429.93,109.93"><head>Table 2</head><label>2</label><figDesc>shows the statistics of information in learning process for assessor topics. A lot of relevant SVs included in the new support vectors are taken from VRDs generated artificially, rather than original relevant documents. And the size of SV set learned from VRDS_SVs are similar with that learned from original training set.In the experimental results, the proposed method achieved a significant performance improvement on the overall evaluation measures. These results indicate that our VRDs give new information to learn decision boundary in SVM. It is only 47 topics among the total 50 topics that VRDs_SVs improved performance compared to Org training set. Therefore, VRDs generated by multiplying two relevant documents can be applied to transformation in batch filtering task.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,82.74,414.66,432.52,8.77;5,96.60,428.28,202.42,8.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,336.44,414.66,139.74,8.77">Recent experiments with INQUERY</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,494.99,414.66,20.27,8.77;5,96.60,428.28,197.76,8.77">Proc. of the Fourth Text REtrieval Conference (TREC-4)</title>
		<meeting>of the Fourth Text REtrieval Conference (TREC-4)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,82.74,441.90,429.83,8.77;5,96.60,455.52,46.98,8.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,237.54,441.90,168.42,8.77">Training invariant support vector machines</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,413.28,441.90,72.69,8.77">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="190" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,82.74,469.14,429.92,8.77;5,96.60,482.76,281.12,8.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,164.15,469.14,248.57,8.77">Making large-scale support vector machine learning practical</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,432.84,469.14,79.82,8.77;5,96.60,482.76,138.52,8.77">Advances in Kernel Methods: Support Vector Machines</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,82.74,496.38,429.83,8.77;5,96.60,509.94,218.07,8.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,223.97,496.38,257.10,8.77">TREC-5 English and Chinese retrieval experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,500.75,496.38,11.82,8.77;5,96.60,509.94,173.70,8.77">the Proc. of the Fifth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>TREC-5</note>
</biblStruct>

<biblStruct coords="5,82.74,523.62,429.92,8.77;5,96.60,537.24,416.00,8.77;5,96.60,550.80,152.39,8.77" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,224.56,523.62,288.10,8.77;5,96.60,537.24,268.95,8.77">Recognition and structure from one 2D model view: observations on prototypes, object classes and symmetries. A.I. Memo No. 1347</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>Artificial Intelligence Laboratory, Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,82.74,564.48,430.09,8.77;5,96.60,578.04,365.70,8.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,278.02,564.48,152.58,8.77">Extracting support data for a given task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,447.30,564.48,65.53,8.77;5,96.60,578.04,258.65,8.77">Proc. of the First International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>of the First International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Menlo Park</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,82.74,591.66,429.87,8.77;5,96.60,605.28,398.37,8.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,279.01,591.66,165.12,8.77">Learning routing queries in a query zone</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,464.10,591.66,48.51,8.77;5,96.60,605.28,352.40,8.77">Proc. of the Twentieth ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the Twentieth ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,82.74,618.90,360.03,8.77" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnick</surname></persName>
		</author>
		<title level="m" coord="5,163.20,618.90,161.60,8.77">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
