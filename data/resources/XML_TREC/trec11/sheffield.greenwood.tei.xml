<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,209.71,148.91,183.58,15.11;1,211.94,170.83,179.12,15.11">The University of Sheffield TREC 2002 Q&amp;A System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.03,203.31,101.65,10.48"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
							<email>m.greenwood@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>Portobello Road Sheffield</addrLine>
									<postCode>S1 4DP</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.63,203.31,60.71,10.48"><forename type="first">Ian</forename><surname>Roberts</surname></persName>
							<email>i.roberts@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>Portobello Road Sheffield</addrLine>
									<postCode>S1 4DP</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.00,203.31,95.97,10.48"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
							<email>r.gaizauskas@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>Portobello Road Sheffield</addrLine>
									<postCode>S1 4DP</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,209.71,148.91,183.58,15.11;1,211.94,170.83,179.12,15.11">The University of Sheffield TREC 2002 Q&amp;A System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2A5622713D4B5ECD4B02344237D12C81</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The system entered by the University of Sheffield in the question answering track of TREC 2002 represents a significant development over the Sheffield system entered into TREC-8 <ref type="bibr" coords="1,449.41,358.60,10.52,8.74" target="#b8">[9]</ref> and TREC-9 <ref type="bibr" coords="1,98.38,370.56,14.62,8.74" target="#b14">[15]</ref>, although the underlying architecture remains the same. The essence of the approach is to pass the question to an information retrieval (IR) system which uses it as a query to do passage retrieval against the text collection. The top ranked passages output from the IR system are then passed to a modified information extraction (IE) system. Syntactic and semantic analysis of these passages, along with the question, is carried out to identify the "sought entity" from the question and to score potential matches for this sought entity in each of the retrieved passages. The potential matches are then combined or discarded based on a number of criteria. The highest scoring match is then proposed as the answer to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description 2.1 Overview</head><p>The key features of the question answering system, for processing a single question, are shown in Figure <ref type="figure" coords="1,136.18,541.35,3.88,8.74" target="#fig_0">1</ref>. Firstly the TREC document collection is indexed using the probabilistic Okapi information retrieval system (this is done once only in advance of any questions) <ref type="bibr" coords="1,444.11,553.30,14.62,8.74" target="#b13">[14]</ref>. This index is then used to return the top n passages relevant to the question, the query to Okapi being the question words. The top n passages are then submitted along with the question to QA-LaSIE, our modified IE system, which should produce one or more answers.</p><p>The reasoning behind this architecture is straightforward. The text collection is too large to be processed in its entirety by the IE system. It is, however, the IE system which is capable of carrying out the detailed linguistic analysis needed to answer the questions. IR systems, however, are specifically designed to process huge amounts of text, and to return the result of a query in a short space of time. Using an IR system as a filter between the text collection and the IE system should allow us to benefit from the systems' respective strengths.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Okapi</head><p>An important question involving the information retrieval component of the QA system is how much text to return. We must decide a) how many documents to retrieve and b) how much retrieved text per document to return (passage size).</p><p>For TREC 2002 we decided to process the top 20 documents returned by Okapi. Experimentation not complete at the time of the test run subsequently showed we should have considered about five times this number of documents as on average the top twenty documents only contained answers for about 60% of the questions while the top 100 documents contained on average answers for about 85% of the questions <ref type="bibr" coords="2,231.66,516.32,15.50,8.74" target="#b12">[13]</ref> (of course the more documents examined per question, the greater the number of entities which can potentially be confused with the answer).</p><p>Okapi supports passage retrieval which can be parameterised by setting a minimum passage length, a maximum passage length and a step value controlling how the passage window is moved over the text. We experimented with different sizes of passage using a random sample of 100 TREC-9 and TREC 2001 questions as queries against the TREC-2001 document collection. These experiments are documented in <ref type="bibr" coords="2,229.20,588.05,15.50,8.74" target="#b12">[13]</ref> and their main results are detailed in Table <ref type="table" coords="2,439.61,588.05,3.88,8.74" target="#tab_0">1</ref>. The definition of the data in each column of the table is as follows:</p><p>Coverage the percentage of questions for which at least one relevant answer bearing passage was found in the retrieved data.</p><p>Correct Answers the number of questions for which the exact answer returned by the system matched one of the Perl patterns supplied for that question.</p><p>TREC Score the TREC 2002 confidence score for the run.</p><p>From these results it seems that using passages of one paragraph in length gives the best performance (13 of the 100 questions were answered correctly), even though the best coverage is provided for passages of length two paragraphs. As the main experiments were not completed in time, two of the submitted runs used the retrieval techniques previously used with our question answering system (i.e. passages of up to three paragrpahs in length, see <ref type="bibr" coords="3,413.06,135.93,15.50,8.74" target="#b14">[15]</ref>) and one run used passages of just one paragraph to attempt to confirm the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LaSIE</head><p>The basis of the question answering system is the LaSIE information extraction system, originally developed to participate in the Message Understanding Conference evaluations <ref type="bibr" coords="3,429.62,206.12,9.96,8.74" target="#b7">[8]</ref>. LaSIE operates inside the GATE platform <ref type="bibr" coords="3,208.85,218.07,9.97,8.74" target="#b3">[4]</ref>, and as a new version of GATE has become available <ref type="bibr" coords="3,459.86,218.07,10.52,8.74" target="#b2">[3]</ref> since our participation in TREC-9, LaSIE has been ported to use the new version, leading to a few minor changes.</p><p>The system is essentially a pipeline of modules each of which process the entire text before the next module is invoked. The following is a brief description of each of the modules in the LaSIE system:</p><p>Tokeniser Identifies token boundaries and text section boundaries.</p><p>Gazetteer Identifies single and multi-word matches against multiple domain specific full name and keyword lists, and tags matching phrases with appropriate name categories.</p><p>Sentence Splitter Identifies sentence boundaries in the text body.</p><p>POS Tagger A rule-based part-of-speech tagger <ref type="bibr" coords="3,309.12,371.50,9.96,8.74" target="#b5">[6]</ref>.</p><p>Tagged Morph Simple morphological analysis to identify the root form and inflectional suffix for tokens that have been tagged as noun or verb.</p><p>NE Transducer Identifies names of people, organisations etc.</p><p>Parser Performs two-pass bottom-up chart parsing, pass one with a special named entity grammar, and pass two with a general phrasal grammar. A best parse is then selected, which may be only a partial parse, and a quasi-logical form (QLF) of each sentence is constructed.</p><p>Discourse Interpreter Adds the QLF representation to a semantic net, which encodes the system's world and domain knowledge as a hierarchy of concepts. Additional information inferred from the input is also added to the model, and coreference resolution is attempted between instances mentioned in the text, producing an updated discourse model. A representation of the question is then matched against the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">QA-LaSIE</head><p>The QA-LaSIE system takes as input a question and a set of passages retrieved by the IR system and outputs the highest ranked answer. When multiple questions are processed by the system it outputs one answer per question ranking the answers based on how confident it is in the answers.</p><p>Figure <ref type="figure" coords="3,137.85,617.03,4.98,8.74" target="#fig_1">2</ref> shows the end-to-end layout of the system as entered in TREC 2002. Four key alterations were made to the original LaSIE IE system for entry into the question answering track at TREC-8 and TREC-9 and these have been developed further for this year's entry. These alterations are as follows:</p><p>1. the grammar used by the parser was extended to cover question types; 2. the discourse interpreter was modified to allow the QLF representations of each question to be matched against the discourse model of a candidate answer text; 3. an answer identification procedure which scored all discourse entities in each candidate text as potential answers was added to the discourse interpreter;</p><p>4. a Question Answer module was added to examine the discourse entity scores across all passages, determine the ranking of the answers and then output the appropriate answer text.</p><p>Exact details of these changes would not sufficiently explain the essence of the approach taken to question answering by the QA-LaSIE system. Therefore the following sections describe the key processes involved in our approach to question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Parsing: Syntactic and Semantic Analysis</head><p>Questions were one of the sentence constructions not handled by the original LaSIE parser. Extra grammar rules were developed to cover the example questions that were available. The syntactic grammar rules have a semantic component that is used to build a QLF representation of the question. One major difference between LaSIE and QA-LaSIE is the introduction of a special semantic predicate, qvar (question variable), which is used to indicate the entity requested by the question. For example, the question "Who wrote Hamlet?" produces the following QLF representation:</p><p>qvar(e1), qattr(e1,name), person(e1), lsubj(e2,e1), write(e2), time(e2,past), aspect(e2,simple), voice(e2,active), lobj(e2,e3), name(e3,'Hamlet')</p><p>In this representation each entity in the question gives rise to a unique identifier of the form eN. The use of the word Who in the question suggests the answer will be a person and so person(e1) is added to the QLF. Also the qvar is set to e1 showing that the question is seeking a person (as person and qvar share the same entity). The relational predicates lsubj (logical subject) and lobj (logical object) link any verb arguments found in the text with the verb in the correct relationship.</p><p>The QLF representation of the question is stored for use in subsequent processing against the candidate answer texts and the entity identifiers are replaced by question entity identifiers of the form qN (i.e. e1 becomes q1, e2 becomes q2 etc.) to facilitate later processing.</p><p>Candidate answer texts are processed in exactly the same fashion although the grammar rules do not instantiate a qvar and the entity identifiers are not altered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Resolution of Question and Candidate Answer Texts</head><p>After a candidate answer text has been parsed the QLFs are passed to the discourse interpreter. This behaves as in the LaSIE system apart from the addition of a final processing stage.</p><p>The discourse interpreter has (by this stage) produced a semantic net or discourse model of all the entities and relationships present in the multiple QLFs for a document. This is built by running a coreference algorithm against the semantic representation of successive sentences as they become available, in order to unify them with the discourse model built so far. This results in multiple references to the same entity across the text being merged into a single unified instance.</p><p>Given this discourse model of a text, the QLF of the question is added to the model as the first sentence and coreference is then carried out between question entities (qN ) and entities within the text (eN ).</p><p>The method for determining and scoring each candidate answer is then as follows:</p><p>1. Each sentence in a candidate answer document is given a constraint score, C, equal to 1 point for each question constraint that matches a member of the sentence, where a question constraint is a unary predicate specifying the type of an entity (eg. person is the type of person(eY) in the question.</p><p>2. Within each sentence every remaining entity (eY) is tested for:</p><p>(a) Semantic Similarity to the qvar, S : the reciprocal of the length of the path between the type of the qvar entity and the type of eY in the semantic lattice (ontology) or if this fails (usually because the two entities are not both present in the system's small ontology) the reciprocal of the Leacock-Chodorow distance <ref type="bibr" coords="5,397.79,313.98,15.49,8.74" target="#b9">[10]</ref> between the qvar and eY in WordNet <ref type="bibr" coords="5,206.42,325.93,14.61,8.74" target="#b11">[12]</ref>. For instance if qvar and eY are of the same type then they will receive a score of 1.</p><p>(b) Object Relation, O: 0.25 if eY is related to a question constraint within the sentence by apposition, a qualifying relationship, or with the prepositions of or in.</p><p>(c) Event Relation, E : 0.5 if there is an event entity in the QLF of the question which is related to the qvar by a lsubj or lobj relation and is not the be event and eY stands in the same relation to an event entity of the same type as qvar does.</p><p>These three values are then combined with the scores for the sentence and the number of question constraints, Q, to give Equation 1 (where 2.8 is a normalising factor determined via experimentation).</p><p>Score for eY = ( S+O+E 2.8</p><formula xml:id="formula_0" coords="5,321.78,460.36,191.23,22.89">) + C 1 + Q (1)</formula><p>The discourse interpreter then returns all the candidate answers and their associated scores for processing by the answer module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Answer Output and Ranking</head><p>Due partly to the differences between TREC 2002 and the previous question answering tracks and partly to the move to using the new version of GATE, the final Question Answering module has been completely redeveloped and a number of new ideas have been included, which are outlined in this section.</p><p>The limitations of window-based methods for pinpointing answers have been discussed in numerous papers including <ref type="bibr" coords="5,199.48,604.96,9.96,8.74" target="#b6">[7]</ref>. The main concerns with these methods are:</p><p>• It is impossible to accurately pinpoint the boundaries of an answer (e.g. an exact name or phrase).</p><p>• These rely solely on word level information and do not use semantic information (hence no knowledge of the type, such as person or location, of the answer being sought).</p><p>• It is impossible to see how such methods could be extended to composing an answer from many different documents or even from different sentences or phrases within a single document.</p><p>One way to filter out some inappropriate candidate answers is to assume that overlap between the question and a candidate answer is inherently bad. Clearly for a question such as "Where is Perth?" an answer of "Perth is in" is not correct and can be eliminated using the following method.</p><p>In most cases it is unlikely that a correct exact answer to a question will contain many, if any, of the non-stopwords in the question. We can use this assumption to throw away some of the candidate answer strings before we even look at the score assigned to them. Word overlap between a question and candidate answer can be expressed as a percentage. At 0% there is no overlap between the question and candidate answer and so the string may be a correct answer to the question and therefore requires further processing. At 100% overlap all the non-stopwords in the candidate answer appear in the question, at which point it is highly unlikely that this string will be a correct answer to the question and can therefore be discarded (an exception is TREC 2001 question 1026 "What does target heart rate mean?" which has as one of its possible answers "target heart rate", although the more important question here is whether "target heart rate" is in fact a valid answer to the question). At points between 0% and 100% overlap it is unclear whether the candidate answer may or may not be correct. Our system simply discards any candidate answers which overlap 100% with the question they seek to answer.</p><p>Having carried out some limited analysis of the performance of our system over the TREC 2001 questions, one thing was clear; we would often return two or more semantically equivalent answers. Clearly if the answer is correct then this is alright, but if these answers are wrong then this may well prevent correct answers from appearing in the top n answers which we are allowed to return. On some occasions we were actually returning identical answers (i.e. for Q1000 "The sun's core, what is the temperature?" we returned five answers all of which were "the sun"), these are easy to remove by simply keeping only the highest scoring of two identical answers.</p><p>Furthermore it may be possible to prune candidate answers that are substrings of longer candidate answers, when the question is suitably vague; as is the case in the question "Where is Perth?" to which our system returns a list of ranked answers containing: Australia and Western Australia. Clearly Australia and Western Australia are both acceptable answers to the question, so only one of them need be returned.</p><p>The approach taken to deal with these answer strings, similar to that used in <ref type="bibr" coords="6,448.23,458.72,9.97,8.74" target="#b0">[1]</ref>, is to test if two proposed answers A and B are the similar by checking that the stem of every non-stopword in A matches a stem of a non-stopword in B, or vice versa. Using this test, if two answers match, then both are removed and a new answer is created from the highest of the two scores and the longest answer string. The effect of this method on our example question was that now only Western Australia is listed as a possible answer.</p><p>Applying the same approach to the question "In which country is Perth?" would not be as effective, since Western Australia is not an exact country name so while this method is better than simple string matching approaches, there is still scope for improvement.</p><p>Using this approach improved the system performance slightly. More importantly was the unexpected side effect which caused the system to clarify some answer strings, with the most obvious being peoples names: 'Armstrong' becomes 'Neil A. Armstrong' and 'Davis' becomes 'Eric Davis', etc.</p><p>These techniques (overlap, similar answers, etc.) are then used to discard, merge and rank the candidate answers found within the document collection for a single question (full details of the ranking algorithm can be found in <ref type="bibr" coords="6,243.09,638.05,10.30,8.74" target="#b4">[5]</ref>).</p><p>The method of ranking single answers to multiple questions (i.e. to produce the confidence sorted list required for this years submission) is based on the following attributes of each answer:</p><p>• the score (the higher the better)</p><p>• the number of other answers which were semantically the same as this one (the higher the better)</p><p>• the IR system rank of the document from which the answer originates (the lower the better as the top document returned by the IR step is ranked 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Answering Questions Requiring Multiple Answers</head><p>List questions are inherently harder to answer than standard, single answer questions, mainly because systems have to combine information from multiple sources to locate the required number of answers. Also a system has to be able to extract from the question the number of different answers required.</p><p>Our simple solution to these problems is as follows:</p><p>1. The system processes the question in the usual way, producing a long list of ranked answers.</p><p>2. The question is then scanned, token by token, until the first token whose part-of-speech signifies that it is a number. This is then assumed to be the number of answers sought.</p><p>3. The requested number of answers is then returned from the top of the ranked list.</p><p>Clearly this approach suffers from the problem that some questions may contain more than one number, i.e. "In the 2001 US Presidential election who were the 2 main candidates?". This problem did not surface during the evaluation, however, as our system correctly identified the number of answers to return for all the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.5">Boosting Performance using Answer Redundancy</head><p>As has been reported in Light et al. <ref type="bibr" coords="7,246.16,387.89,14.61,8.74" target="#b10">[11]</ref>, the number of answer instances to a given question in the document collection (within a single document or multiple documents each containing the answer once) directly effects the end-to-end performance of a QA system. This is partly due to the fact that the IR engine is more likely to find a relevant document, and also because the answers are likely to occur in different contexts, giving the parser a better chance of analysing at least one of them in a way that is beneficial to the rest of the system.</p><p>To this end it was decided to attempt to boost the knowledge available to our system, not as may be expected, by returning more documents at the initial IR step, but by using two different text collections. The second text collection that was chosen was the World Wide Web. A document collection for a single question is constructed from the snippets displayed on the Google results page for the top ten documents returned by Google. These snippets are certainly not full documents, and are rarely full sentences but this is not a problem as the bottom-up chart parser we employ is not constrained to only selecting full sentence or complex phrase categories. This method of using just the snippets has been shown to be successful in <ref type="bibr" coords="7,317.70,543.31,9.96,8.74" target="#b1">[2]</ref>, although they used the snippets from the first one thousand documents rather than the first ten.</p><p>The QA system is run against both text collections and then the results are merged together. The end result must be an answer which references a document in the TREC collection so the process of merging is as follows: for each answer returned from the Google corpus, if an answer exists from a document in the TREC corpus which is semantically equivalent, then merge by keeping the highest score etc., but the reference to the TREC document (other answers found using Google are simply discarded).</p><p>Over a sample of one hundred questions (TREC questions 1000 to 1099) the results of combining the collections in this way (based on returning the top five answers for each question) can be seen in 3 Results and Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results Observed During Development</head><p>Unfortunately we did not enter the system into TREC 2001 and so there were no official scores for the system over that question set. However, the first development task was to produce unofficial scores for our unaltered TREC-9 system over the TREC 2001 questions, using the regular expression patterns kindly made available by NIST. The result was that the unaltered TREC-9 system achieved a mean reciprocal rank (MRR) score of 0.169, over the TREC 2001 questions compared to its official TREC 9 MRR score of 0.206 (both using answers of 50 bytes or less). This drop in performance may be due to the fact that our system is not designed to handle definition style questions, which made up a significant percentage of the TREC 2001 question set. This system now scores an MRR of 0.343 over the TREC 2001 question set, clearly a significant improvement over the previous system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Final Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Main Track</head><p>We submitted three runs to the main question answering track. The differences between the runs all concernes the size and composition of the document collection generated for a single question, these were:</p><p>sheft11mo3: This run used the top twenty passages retrieved from the AQUAINT collection by Okapi. The maximum length of a passage was three paragraphs, the minimum was one paragraph.</p><p>sheft11mog3: This run used the same collection as did sheft11mo3, augmented with the top ten snippets returned by Google when given the question as a search query.</p><p>sheft11mog1: This run is the same as sheft11mog3 except the passages retrieved by Okapi from the AQUAINT collection are limited to at most one paragraph in length.</p><p>Table <ref type="table" coords="8,131.78,658.47,4.98,8.74" target="#tab_3">3</ref> shows the full evaluation results for the three different runs over the 500 test questions. From this it can be seen that the sheft11mog1 run was the best of the three configurations, suggesting that using documents from more than one source is beneficial, and also that documents of one pargraph in length are more suited to this work than longer documents, confirming what was demonstrated during development (see Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">List Track</head><p>We submitted two runs to the list track. The differences between the runs concern the size and composition of the document collection generated for a single question, these were: sheft11lo: This run used the top twenty passages retrieved from the AQUAINT collection by Okapi. The maximum length of a passage was a single paragraph.</p><p>sheft11log: This run used the same collection as sheft11lo, augmented with the top ten snippets returned by Google when given with the question as a search query.</p><p>Unfortunately we did not have time to test the list answering system with the result that the system scored an average accuracy of only 0.06 (for both runs). A serious flaw in the processing of list questions was subsequently discovered, although fixing this resulted in a system whose average accuracy was only 0.09.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>At its core, Sheffield's entry in this year's QA track remains the same as our TREC-9 system in 2000 <ref type="bibr" coords="9,113.24,328.48,14.62,8.74" target="#b14">[15]</ref>. There were, however, a number of enhancements, the most significant were:</p><p>• using a semantic similarity metric over WordNet as one factor in determining the score of candidate answer entities;</p><p>• filtering the final ranked answer list to remove duplicate and near-duplicate answers and simultaneously boost the remaining candidate's rank;</p><p>to eliminate answers which completely overlap with the question;</p><p>• employing Google to search the Web for documents relevant to a given question and boosting the rank of answers found by the QA system both in the Google-returned document snippets and the TREC collection.</p><p>Each of these enhancements produced small but noticeable improvements. Ideas for future work include:</p><p>• expanding the size of the document set passed on from the IR system to the QA systemexperiments not completed till after the TREC run showed that for 40% of the questions in a test sample the QA system was simply not receiving any document containing an answer;</p><p>• experimenting with adaptive algorithms to optimise the weightings of the various features used to rank the answer candidates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,179.49,232.87,244.03,8.74;2,192.75,254.51,39.92,8.77;2,326.28,254.51,83.98,8.77;2,192.75,266.47,33.17,8.77;2,268.69,266.47,45.63,8.77;2,337.70,266.47,61.13,8.77;2,192.75,278.85,52.64,8.74;2,282.37,278.85,18.26,8.74;2,363.28,278.85,9.96,8.74;2,192.75,290.81,56.56,8.74;2,282.37,290.81,18.26,8.74;2,363.28,290.81,9.96,8.74;2,192.75,302.76,56.56,8.74;2,282.37,302.76,18.26,8.74;2,365.77,302.76,4.98,8.74;2,192.75,314.72,56.56,8.74;2,282.37,314.72,18.26,8.74;2,365.77,314.72,4.98,8.74;2,192.75,326.67,56.56,8.74;2,282.37,326.67,18.26,8.74;2,365.77,326.67,4.98,8.74;2,192.75,338.63,56.56,8.74;2,282.37,338.63,18.26,8.74;2,365.77,338.63,4.98,8.74;2,192.75,350.58,56.56,8.74;2,282.37,350.58,18.26,8.74;2,365.77,350.58,4.98,8.74;2,192.75,362.54,63.98,8.74;2,282.37,362.54,18.26,8.74;2,365.77,362.54,4.98,8.74;2,91.02,108.86,417.64,108.90"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System setup for the question answering task.Passage Correct Answers Length Coverage (out of 100) 1 paragraph 67% 13 2 paragraphs 74% 11 3 paragraphs 72% 7 4 paragraphs 72% 8 5 paragraphs 70% 7 6 paragraphs 70% 7 7 paragraphs 71% 7 full documents 72% 7</figDesc><graphic coords="2,91.02,108.86,417.64,108.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,130.83,200.16,341.35,8.74;4,98.00,108.86,403.68,76.19"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: QA-LaSIE system modules (* denotes a standard GATE 2 module).</figDesc><graphic coords="4,98.00,108.86,403.68,76.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,145.78,278.85,311.44,114.29"><head>Table 1 :</head><label>1</label><figDesc>Results of IR experiments and their effects on the QA system.</figDesc><table coords="2,192.75,278.85,180.50,92.42"><row><cell>1 paragraph</cell><cell>67%</cell><cell>13</cell></row><row><cell>2 paragraphs</cell><cell>74%</cell><cell>11</cell></row><row><cell>3 paragraphs</cell><cell>72%</cell><cell>7</cell></row><row><cell>4 paragraphs</cell><cell>72%</cell><cell>8</cell></row><row><cell>5 paragraphs</cell><cell>70%</cell><cell>7</cell></row><row><cell>6 paragraphs</cell><cell>70%</cell><cell>7</cell></row><row><cell>7 paragraphs</cell><cell>71%</cell><cell>7</cell></row><row><cell>full documents</cell><cell>72%</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,136.83,662.86,35.15,8.74"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table coords="8,212.17,110.79,178.65,45.03"><row><cell cols="3">Collection MRR Not Found (%)</cell></row><row><cell>TREC</cell><cell>0.256</cell><cell>68 (68%)</cell></row><row><cell>Google</cell><cell>0.227</cell><cell>68 (68%)</cell></row><row><cell>Combined</cell><cell>0.285</cell><cell>65 (65%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,95.98,168.95,419.33,80.89"><head>Table 2 :</head><label>2</label><figDesc>Results of using Google to boost system score.</figDesc><table coords="8,95.98,192.58,419.33,57.27"><row><cell></cell><cell></cell><cell>Not</cell><cell></cell><cell></cell><cell>Confidence</cell><cell cols="2">No Answer</cell></row><row><cell>Run Tag</cell><cell cols="4">Wrong Supported Inexact Right</cell><cell>Score</cell><cell cols="2">Precision Recall</cell></row><row><cell>sheft11mo3</cell><cell>422</cell><cell>9</cell><cell>18</cell><cell>51</cell><cell>0.128</cell><cell>0.162</cell><cell>0.130</cell></row><row><cell>sheft11mog3</cell><cell>394</cell><cell>12</cell><cell>22</cell><cell>72</cell><cell>0.203</cell><cell>0.150</cell><cell>0.130</cell></row><row><cell>sheft11mog1</cell><cell>389</cell><cell>11</cell><cell>20</cell><cell>80</cell><cell>0.222</cell><cell>0.150</cell><cell>0.065</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,190.18,262.69,222.64,8.74"><head>Table 3 :</head><label>3</label><figDesc>Results from the three main track entries.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,633.98,402.51,8.74;9,110.48,645.94,339.76,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,421.48,633.98,91.51,8.74;9,110.48,645.94,63.89,8.74">Data-Intensive Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,196.06,645.94,223.55,8.74">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,665.84,402.52,8.74;9,110.48,677.79,271.81,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,252.90,665.84,260.10,8.74;9,110.48,677.79,78.01,8.74">Complex Answers: A Case Study using a WWW Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bucholz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,197.78,677.79,130.16,8.74">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,697.69,402.52,8.74;9,110.48,709.64,132.14,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,189.30,697.69,229.26,8.74">GATE, a General Architecture for Text Engineering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,429.13,697.69,83.88,8.74;9,110.48,709.64,47.67,8.74">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="223" to="254" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,112.02,402.52,8.74;10,110.48,123.98,402.53,8.74;10,110.48,135.93,402.52,8.74;10,110.48,147.89,255.57,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,459.87,112.02,53.13,8.74;10,110.48,123.98,382.08,8.74">GATE -an Environment to Support Research and Development in Natural Language Engineering</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Humphreys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,135.93,402.52,8.74;10,110.48,147.89,48.41,8.74">Proceedings of the 8th IEEE International Conference on Tools with Artificial Intelligence (ICTAI-96)</title>
		<meeting>the 8th IEEE International Conference on Tools with Artificial Intelligence (ICTAI-96)<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10">October 1996</date>
			<biblScope unit="page" from="58" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,167.81,402.52,8.74;10,110.48,179.77,402.51,8.74;10,110.48,191.72,275.25,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,217.92,167.81,87.80,8.74">Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Greenwood</surname></persName>
		</author>
		<ptr target="http://www.dcs.shef.ac.uk/˜mark/phd/work/index.html" />
		<imprint>
			<date type="published" when="2002-10">October 2002. 2002</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, The University of Sheffield, UK. Available</orgName>
		</respStmt>
	</monogr>
	<note>First year PhD Progress Report</note>
</biblStruct>

<biblStruct coords="10,110.48,211.65,402.51,8.74;10,110.48,223.60,402.52,8.74;10,110.48,235.56,377.58,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,174.18,211.65,338.81,8.74;10,110.48,223.60,134.85,8.74">Independence and Commitment: Assumptions for Rapid Training and Execution of Rule-based POS Taggers</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,264.59,223.60,248.41,8.74;10,110.48,235.56,184.27,8.74">Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000)</title>
		<meeting>the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10">October 2000</date>
			<biblScope unit="page" from="278" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,255.48,402.51,8.74;10,110.48,267.44,402.52,8.74;10,110.48,279.39,230.41,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,498.33,255.48,14.67,8.74;10,110.48,267.44,188.19,8.74">Towards Semantics-Based Answer Pinpointing</title>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurie</forename><surname>Geber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,318.86,267.44,194.14,8.74;10,110.48,279.39,128.26,8.74">Proceedings of the DARPA Human Language Technology Conference (HLT)</title>
		<meeting>the DARPA Human Language Technology Conference (HLT)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,299.32,402.52,8.74;10,110.48,311.27,402.52,8.74;10,110.48,323.23,263.87,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,160.89,311.27,244.62,8.74">Description of the LaSIE-II system as used for MUC-7</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Azzam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huyck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,432.90,311.27,80.09,8.74;10,110.48,323.23,191.25,8.74">Proceedings of the Seventh Message Understanding Conference</title>
		<meeting>the Seventh Message Understanding Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,343.15,402.52,8.74;10,110.48,355.11,402.53,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,455.97,343.15,57.04,8.74;10,110.48,355.11,140.68,8.74">University of Sheffield TREC-8 Q &amp; A System</title>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,272.13,355.11,210.48,8.74">Proceedings of the 8th Text REtrieval Conference</title>
		<meeting>the 8th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,375.03,402.52,8.74;10,110.48,386.99,402.52,8.74;10,110.48,398.94,174.71,8.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,248.88,375.03,264.13,8.74;10,110.48,386.99,83.05,8.74;10,303.09,386.99,209.91,8.74;10,110.48,398.94,24.01,8.74">Combining Local Context and WordNet Similarity for Word Sense Identification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
		<editor>C. Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="265" to="285" />
		</imprint>
	</monogr>
	<note>WordNet: An Electronic Lexical Database, chapter 11</note>
</biblStruct>

<biblStruct coords="10,110.48,418.87,402.52,8.74;10,110.48,430.82,331.59,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,370.47,418.87,142.53,8.74;10,110.48,430.82,137.96,8.74">Analysis for Elucidating Current Question Answering Technology</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,257.56,430.82,130.16,8.74">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,450.75,402.52,8.74;10,110.48,462.70,69.77,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,187.95,450.75,128.08,8.74">WordNet: A Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,323.94,450.75,121.93,8.74">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995-11">November 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,482.63,402.52,8.74;10,110.48,494.59,402.51,8.74;10,110.48,506.54,318.53,8.74" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,175.86,482.63,293.88,8.74">Information Retrieval for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<ptr target="http://www.dcs.shef.ac.uk/teaching/eproj/msc2002/abs/m1ir.htm" />
		<imprint>
			<date type="published" when="2002">2003. 2002</date>
			<pubPlace>UK. Available, Februray</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, The University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note>MSc Dissertation</note>
</biblStruct>

<biblStruct coords="10,110.48,526.47,402.52,8.74;10,110.48,538.42,121.73,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,244.93,526.47,121.45,8.74">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,392.37,526.47,120.62,8.74;10,110.48,538.42,91.11,8.74">Proceedings of the 8th Text REtrieval Conference</title>
		<meeting>the 8th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,558.35,402.53,8.74;10,110.48,570.30,242.72,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,277.70,558.35,212.27,8.74">University of Sheffield TREC-9 Q &amp; A System</title>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,570.30,212.09,8.74">Proceedings of the 9th Text REtrieval Conference</title>
		<meeting>the 9th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
