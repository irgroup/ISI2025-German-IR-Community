<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,124.80,115.53,343.34,15.68;1,184.56,137.49,223.93,15.68">A machine learning approach for QA and Novelty Tracks: NTT system description</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.80,170.04,74.68,10.87"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
							<email>kazawa@cslab.kecl.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Communication Science Laboratories Nippon Telegraph and Telephone Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.55,170.04,76.04,10.87"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
							<email>hirao@cslab.kecl.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Communication Science Laboratories Nippon Telegraph and Telephone Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.69,170.04,72.04,10.87"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
							<email>isozaki@cslab.kecl.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Communication Science Laboratories Nippon Telegraph and Telephone Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.60,170.04,72.30,10.87"><forename type="first">Eisaku</forename><surname>Maeda</surname></persName>
							<email>maeda@cslab.kecl.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Communication Science Laboratories Nippon Telegraph and Telephone Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,124.80,115.53,343.34,15.68;1,184.56,137.49,223.93,15.68">A machine learning approach for QA and Novelty Tracks: NTT system description</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">66369C7D66E9237563AF058B1E056C1A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 A unified approach to QA and Novelty Tasks</p><p>In one sense, the goals of QA and Novelty tasks are the same: extracting small document parts which are relevant to users' queries. Additionally, the unit of extraction is almost always fixed in both tasks. For QA, an answer is a noun phrase in most cases, and for Novelty, a sentence is recognized as the basic information unit.</p><p>This observation leads us to the following unified approach to both QA and Novelty tasks: first identify information units in documents, then judge whether each unit is relevant to the query. This two step approach is amenable to machine learning methods because each step can be cast as a classification problem. For example, noun phrase identification can be achieved by classifying each word into the start/middle/end/exterior of a noun phrase; sentence identification by classifying whether each period marks the of a sentence. Additionally, relevance judgment can be regarded as the classification of a pair of query and an information unit into a relevant-pair or non-relevant-pair.</p><p>In QA and Novelty Tracks at TREC 2002, we studied the feasibility of this two step approach, using Support Vector Machines as the learning algorithm of the classifiers. Since many studies on identifying information units have already been reported, we concentrate on the relevance judgment step in QA and Novelty tasks in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Question Answering Track</head><p>Because of limited time, we applied our machine learning approach only to questions concerning dates and quantities; the other questions were processed in the same way as reported in <ref type="bibr" coords="1,258.01,568.67,9.89,9.96" target="#b0">[1]</ref>. Hereafter, we limit ourselves to the questions about dates and quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Answer Candidate Identification</head><p>For date/quantity questions, answers are likely base noun phrases (base NPs), including date or number. Thus, we extracted base NPs including date/number expressions as information units (answer candidates). However, since identifying such base NPs is arguably rather easy work, we constructed an answer candidate identification module based on a Naive-Bayes classifier instead of SVMs<ref type="foot" coords="2,441.36,89.59,3.95,6.37" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevant Candidate Selection</head><p>To train SVMs on the relevance jugdement of candidates, we created a training dataset that consists of pairs of date/quantity questions and their answers.</p><p>As positive example pairs, we used the pairs of the past QA Track questions that ask dates/quantities, and the answers collected from the past judgment files using answer patterns. In addition, we randomly selected incorrect answer candidates from the judgement files, then combined them with the questions to obtain negative example pairs. Each question-candidate pair is converted into a feature vector. The features consists of the following two types of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keyword densities</head><p>Keyword density in 5/10/20-word Hanning windows centered on the candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combined features</head><p>All combinations of question and candidate features. The question features consist of:</p><p>• Wh-word in the question,</p><p>• Headword of wh-phrase in the question and its WordNet category,</p><p>• Keywords (nouns, verbs) in the question and its WordNet category.</p><p>Here, wh-words are who, when, where, and what, whereas wh-phrases are phrases including wh-words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The candidate features consist of:</head><p>• Headword of the candidate and its WordNet category,</p><p>• Binary indicator of whether the candidate includes number/month/day expressions,</p><p>• The number of digits in the candidates.</p><p>3 Novelty Track</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relevant Sentence Extraction</head><p>We made a training data set that consists of query-sentence pairs whose relevance was judged by us. Our data includes 21 queries chosen from TREC topics, which are not used in the novelty track, and 4044 sentences chosen from past TREC results.</p><p>To apply SVM, we transformed each query-sentence pair into a feature vector. The features consist of:</p><p>• Sentence position normalized by the document length.</p><p>• Sentence length normalized by the longest sentence length in the same document.</p><p>• The sum of the weights of the sentence vector.</p><p>• Keyword density in the sentence. The keywords are terms in the description section of the query and the title section of the document.</p><p>• Cosine between the headline term vector and the sentence term vector.</p><p>• Cosine between the query term vector and the sentence term vector.</p><p>• Cosine between the query term vector and the document term vector.</p><p>Here, term vectors are commonly-used tfidf vectors. The inverse document frequency (idf) is calculated using all the TIPSTER document sets.</p><p>For the TREC 2002 Novelty Track, we trained SVMs with the quadratic kernel, (x•x + 1)<ref type="foot" coords="3,199.92,276.91,3.95,6.37" target="#foot_1">2</ref> , and the Gaussian kernel, exp(-a|xx | 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">"New" Sentence Selection</head><p>For the TREC 2002 Novelty Track, it is not sufficient to merely judge the relevance of each information unit (sentence); it is required that only "new" sentences be reported at the end.</p><p>To select "new" sentences from relevant sentences, we used Marginal Relevance (MR) as the selection criteria. Originally, MR was proposed as a measure of "information" increased by the addition of a new document to the documents already selected <ref type="bibr" coords="3,191.33,395.63,11.78,9.96" target="#b1">[2]</ref>.</p><formula xml:id="formula_0" coords="3,202.92,417.59,187.44,14.48">MR D (d) = λrel(d) -(1 -λ) max d ∈D sim(d, d ).</formula><p>(1)</p><p>Here, d is the document whose contribution to information increase is to be measured, and D is the set of documents already selected. The function rel(d) is the relevance of d to the query, and sim(d, d ) is the "similarity" between d and d .</p><p>To apply MR to a new sentence selection in the Novelty task, we modified MR as follows.</p><p>• Sentences are regarded as (very) short "documents".</p><p>• For the relevance measure of sentences, we use the value of a discriminant function which is used in the relevant sentence extraction step.</p><p>• For the similarity measure, we use the number of the common words between the sentences divided by the average number of words in both sentences.</p><p>In the new sentence selection step, we repeat the selection of the sentence with the largest MR and add it to the selected sentence set until the largest MR is less than 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We submitted two runs for Novelty Tracks: quadratic kernel SVMs were used in one run and Gaussian kernel SVMs in the other. Table <ref type="table" coords="4,168.34,209.39,5.03,9.96" target="#tab_0">1</ref> shows "precision multiplied by recall" values of our submissions. Quadratic kernel SVMs perform slightly better than Gaussian kernel ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,184.91,641.95,11.52,10.72"><head>Table 1 :</head><label>1</label><figDesc>2 2 Results of Novelty Track</figDesc><table coords="4,221.04,79.19,151.24,34.32"><row><cell></cell><cell cols="2">Quadratic Gaussian</cell></row><row><cell>P*R(rel)</cell><cell>0.0694</cell><cell>0.0664</cell></row><row><cell cols="2">P*R(new) 0.0545</cell><cell>0.0477</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,140.04,643.73,328.13,7.24;2,124.80,653.21,307.61,7.24"><p>In general, SVMs show higher performance than Naive-Bayes classifiers. However, we prefer Naive-Bayes in this case because training Naive-Bayes classifiers is very fast.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,140.04,660.17,57.20,7.24"><p>We set λ = 0.7.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,140.27,276.11,327.91,9.96;4,140.28,288.11,240.80,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,312.31,276.11,155.87,9.96;4,140.28,288.11,50.38,9.96">NTT Question Answering System in TREC 2001</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,203.09,288.11,66.30,9.96">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,140.27,308.03,328.15,9.96;4,140.28,320.03,327.94,9.96;4,140.28,331.91,98.63,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,280.35,308.03,188.07,9.96;4,140.28,320.03,245.07,9.96">The Use of MMR, Diversity-Based Reranking for Reordering Document and Producing Summaries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,398.92,320.03,69.30,9.96;4,140.28,331.91,8.55,9.96">Proc. of SIGIR-98</title>
		<meeting>of SIGIR-98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
