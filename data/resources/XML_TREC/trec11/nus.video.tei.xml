<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.34,86.54,410.03,11.15;1,256.98,100.34,116.70,11.15">Temporal Multi-Resolution Framework for Shot Boundary Detection and Keyframe Extraction</title>
				<funder ref="#_fy7KpHH">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,289.58,117.21,55.87,8.74"><forename type="first">Huamin</forename><surname>Feng</surname></persName>
							<email>fenghm@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.91,117.21,60.33,8.74"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<email>chuats@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.34,86.54,410.03,11.15;1,256.98,100.34,116.70,11.15">Temporal Multi-Resolution Framework for Shot Boundary Detection and Keyframe Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">833BEADD52DFA3B9E8BF1C1B69329C8A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video shot boundary detection and keyframe extraction is an important step in many video-processing applications. We observe that video shot boundary is a multi-resolution edge phenomenon in the feature space. In this experiment, we expanded our previous temporal multi-resolution analysis (TMRA) work by introducing the new feature vector based on motion, incorporating functions to detect flash and camera/object motion, and selecting automatic thresholds for noise elimination based on the type of video. The framework is used to extract meaningful keyframes. Experiments show that our new system can detect and characterize both the abrupt (CUT) and gradual (GT) transitions effectively. It has good accuracy for both the detection of transitions as well as their boundaries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the presence of many types of transitions and the wide varying lengths of GTs, the task of detecting the type and location of transitions in video is a complex task. In fact, the transition of video is not a single resolution phenomenon. For example, although longer GT can't be observed at a high temporal resolution, it is apparent at a low temporal resolution of the same video stream. Thus the detection of transitions of video shots is a temporal multi-resolution problem. Information across resolutions can also be used to detect as well as locate both the CUT and GT transition points. Since wavelet is well known for its ability to model sharp discontinuities and to process signals according to scales <ref type="bibr" coords="1,227.78,374.91,10.63,8.74" target="#b0">[1]</ref>, we employ Canny-like B-Spline wavelets in this multi-resolution analysis. This work provides: a) an unified approach for CUT and GT detection; b) accurate location of gradual transition boundary; c) adaptive threshold value selection based on video variance within a sliding window; d) flash elimination by characterizing the phenomenon in multi resolution; e) motion elimination by computing quadratic similarity measures within the transition and its neighborhood; and f) keyframe extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Basic Theory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video Representation</head><p>We model the video according to the content of the video frames in the stream. The feature for representing the content of video frames could be of any type: color, shape, texture or motion. Thus video is modeled in a Ndimensional feature space of : (a) gray-level representation, (b) RGB value, and (c) Optic flow/motion vector value. The dimension of the space depends on the dimensionality of the chosen features. Since the colorhistogram representation has been found to be useful for the video segmentation problem, we use the N-color histogram for each frame of video. Our experiment shows that the local histogram-based method has difficulties in improving both the recall and precision of the shot boundary detection at the same time. In addition it has difficulty locating precise boundary of GT due to the flash and camera/object motions. To overcome this problem we constructed a motion-based feature using the motion-vectors of MPEG compressed stream. Besides the use of these features, we also use derivatives to detect the transitions. The maximas of the first order derivative or zero crossing in the second order derivative will correspond to transition points. In this paper, the first order derivative was taken for easier implementation.</p><p>By empirically observing GTs that exists in most video streams, we find that different types of GTs exist like fade in/fade out, dissolve, wipe, morph etc. Moreover, the length of the transition may vary greatly too. Different shot transitions have different characteristics, so it is hard to use just one single feature and single algorithm to capture the characteristics of all kinds of shot transitions efficiently. Just as the assumptions most existing algorithm follows, one can clearly observe that the content between shots change much more than intra-shot change. However different types of shot transitions are observable at different scales in the feature space. Whatever the type or length of the transition, there will always be a change big enough that we can detect. The difference is only the resolution of our observation. For CUT, we could see the change both in a detailed observation (between two successive frames), or a coarse observation (across several frames), while GT only shows the change in a coarse observation. So the transition must be defined with respect to different resolutions. By viewing the video at multiple resolutions, the CUT and those GTs could be unified. The only difference is that GTs means boundaries of signal in low resolutions while CUT means in all the resolutions. By making this fundamental observation that a video shot boundary is a multi-resolution phenomenon, we can characterize the transitions with the following features: the scale of the transition, the strength of the transition, and the singularity of the transition point. We have developed a unique multi-resolution analysis technique to detect and characterize both the CUT &amp; GT shot boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Applying Wavelet</head><p>The multi-resolution phenomenon has been widely studied in other areas, and wavelets provide a good mathematical basis for such an analysis. In the analysis, we need to construct a scale space. The Gaussian scalespace approach is widely adopted as the Gaussian function is the unique kernel, which satisfies the causality property as guaranteed by the scaling theorem. Because the first order derivative of the Gaussian function could be a mother wavelet, one can easily show that the sharper variation points of the signal corresponds to the local maxima of the wavelet transform. Thus a maxima detection of the wavelet transform is equivalent to boundary detection. If the mother wavelets is the Canny wavelet, which is the first order derivative of the Gaussian , then</p><formula xml:id="formula_0" coords="2,122.52,283.77,407.55,24.95">dx d x a θ ψ = ) (<label>( 1 )</label></formula><p>and the dilation at scale is</p><formula xml:id="formula_1" coords="2,122.52,323.04,407.56,21.08">) ( 1 ) ( s x s x a a s ψ ψ = ( 2 )</formula><p>When choosing a dyadic scale sequence j 2 , we get:</p><formula xml:id="formula_2" coords="2,122.52,362.18,407.56,21.62">) 2 ( 2 1 ) ( 2 j a j a x x j ψ ψ = ( 3 )</formula><p>The wavelet transform here is defined as:</p><formula xml:id="formula_3" coords="2,122.94,397.49,407.14,34.65">( ) [ ] x f dx d s s x s x d d f s s x s f x f W s a a s θ θ ϕ × =       × =       × = 1 1 ) ( ( 4 )</formula><p>From the right side of equation (4), we can see that the resulting output is a smoothed signal generated by a Gaussian filter that calculates the first order derivatives. It could be shown here that this wavelet transform is equivalent to smoothening the signal by applying the different scale Gaussian filters and then calculate the first order derivatives. The detailed derivation of Equations (1-5) can be found in <ref type="bibr" coords="2,413.83,469.53,10.61,8.74" target="#b2">[3]</ref>. The local maximas of the resulting signal will indicate where the transitions happen, and the magnitude of the maximas will show the strength of the transitions. Tracing the maximas in different resolutions is equivalent to finding transition points in different resolutions. In many cases, the presence of noise may result in maximas too. We distinguish the real transitions from the noise by examining the cross-resolution information. A real transition will still be a maxima in all resolutions. However, the noise may be lost or eroded in a lower resolution of the smooth function <ref type="bibr" coords="2,508.09,527.07,10.64,8.74" target="#b2">[3]</ref>. The TMRA system has 3 phases. In the feature extraction phase, feature vectors suitable for the TMRA are computed. In the shot boundary detection phase, transitions are characterized and the TMRA algorithm is applied to obtain the transition boundaries. This result will include many wrong transitions (insertions). In the elimination phase, motion analysis and flash detection are applied to remove insertions due to motion and abrupt flash noises. Figure <ref type="figure" coords="3,115.37,120.63,5.01,8.74" target="#fig_0">1</ref> shows the system architecture for shot boundary detection. The following sections discuss in detail each of these three phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature extraction</head><p>We extract motion vector feature and the color histogram feature. The DC64 color histogram is computed by extracting the DCT DC value for each block in the frame. The value is quantized to 64 values. The MA64 direction histogram is computed using the motion vectors for each macro block and quantizing the angle values to 60 bins. Since the motion vectors tend to be sparse, a 3X3 median filtering is applied to the motion vectors. Also boundary blocks are not considered in the formation of the feature vectors, as they tend to be erroneous. The last 4 bins contain the macroblock type count of forward predicted, backward predicted, intra and skip macroblocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TMRA Algorithm</head><p>In this phase, TMRA algorithm is applied to determine the potential transition point and their type. It performs the wavelet transformation on the color and motion based feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Locating potential transitions</head><p>The goal of video segmentation is not only to detect the occurrence of a transition, but also to locate the exact positions of the CUT/GT to segment the video. In a GT, both the start position and end position need to be detected. Low resolution of the wavelet coefficients helps to detect the occurrence; where as high resolution helps to characterize the start and end of the transition. In the higher resolution, the boundaries would show up as the maxima points. To identify this boundary we use both the DC64 and MA64 wavelet coefficients. For low resolution (Resolution 3) DC64 wavelet coefficients are used and for high resolution (Resolution 0) MA64 wavelet coefficients are used. The DC64 feature space fails to characterize the beginning and ending frames of the transitions accurately at the high resolution. This is due to the fact that the rate of change in DC64 feature space is not high and doesn't result in a distinguishable peak. Hence we designed a new feature space based on direction of motion along with counts representing static (skip) and intra (blocks having significant change) blocks. As a result of this we observe that even a gradual change resulted in an abrupt spike (peak) at the start and end of the transition. This is captured as the boundary of the transition. After we identify the local maxima points at some lower resolution (also called potential transitions), we use these local maxima points as the anchor points, and trace up to the higher resolution of the motion-based wavelet coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Adaptive thresholding</head><p>The problem of choosing appropriate threshold is a key issue in applying TMRA for shot boundary detection. Heuristically chosen global threshold is not suitable as the shot content changes from scene to scene. So adaptive thresholds are better than a simple global threshold. Here we use a sliding window method to calculate the thresholds. Our system has one weighting factor which can adaptively adjust based on the sliding window size and the standard deviation of dc64 feature of the neighborhood frames.. For different video clips, their standard deviations (see equation 5) are different. The standard deviation of home videos is larger than the general videos. The choice of sliding window size is also very important. In our system, we scan the whole wavelet coefficients of DC64 and choose the sliding window size as the sum of max interval of peak points and max interval of valley points (see equation 6). This removes most of the noise peak points due to brightness/contrast variations, blurring and small motions. The weighting factor can be used to adjust the threshold, which is chosen in the range of 0.8~1.5. In general, for home video, the weighting factor can be larger, in the range of 1.2~1.5, whereas for other video, it can be in the range of 0.8~1.1.</p><formula xml:id="formula_4" coords="3,123.96,645.16,406.12,46.69">              -       - - - - = ∑ ∑ = = - - N i N i i i i i N f f f f N S 1 2 1 1 2 1 1 | | ) ( 2 1 ( 5 )</formula><p>where N denotes the total frame number of the video sequence; f denotes the feature (DC64).</p><formula xml:id="formula_5" coords="4,124.20,73.43,405.88,41.84">v p v i N i v p i N i p dis dis size D dis D dis v p + = = = ∈ ∈ } { max arg }, { max arg ) ( ) ( ( 6 )</formula><p>where Np denote the number of the peak points and Nv denote the number of valley points.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Elimination and Keyframe Extraction</head><p>The last phase of the TMRA algorithm is the elimination of wrong transitions due to motion and abrupt flashes in the shot. Also representative keyframes are extracted for each shot. The following sections give a brief description on the method to characterize such activities in the multi resolution framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Flash detection</head><p>With our TMRA, flash is easily detected by testing the changes of frame's wavelet coefficients at all resolutions. For abrupt noise, the magnitude of coefficient value also decreases as the resolution decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Camera/Object motion detection</head><p>With our ATMRA, we detect camera and object motion points. In principle, for the correct transitions (CUT/GT), the mean absolute differences (MAD) of DC64 and MA64 should be consistent. At CUT and GT points, the MADs are consistent across both the DC64 and MA64 feature space. If MAD changes are not consistent across DC64 and MA64 around the potential transition points, then it is likely to be a wrong transition caused by the camera/object motion. Summarizing our motion detection method: first we compute three kinds of quadratic differences (distance between mean absolute difference of feature vector) for each potential transition we have found. They are represented as QMADbefore(similarity before the transition), QMADintra (similarity within the transition), QMADafter (similarity after the transition) as shown in Equations (7-9). Here</p><formula xml:id="formula_6" coords="4,440.70,402.90,71.27,23.83">)! ( ! ! r k r k C r k - =</formula><p>is the normalizing factor. For each transition, we compute these three parameters for DC64 and MA64 feature, respectively.</p><formula xml:id="formula_7" coords="4,123.66,463.74,406.45,88.37">2 1 1 / | | k start k start i start i j j i before C f f QMAD         - = ∑ ∑ - - = + = ( 7 ) 2 1 1 1 int / | | + - - = + =         - = ∑ ∑ start end end start i end i j j i er C f f QMAD (8) 2 1 1 / | | k k end end i k end i j j i after C f f QMAD         - = ∑ ∑ - + = + + = ( 9 )</formula><p>where: start and end represents the begin and end frame number of the potential transition. k represents the computing range ( 9 2 ≤ ≤ k</p><p>). fi denotes the feature.</p><p>We remove those potential transitions that meet the following condition: </p><formula xml:id="formula_8" coords="4,124.44,620.08,308.94,37.48">( ) ( ) ( ) ( )         + &lt; ∧ + ≥ 2 / 2 / ) ( ) ( ) ( int ) ( ) ( ) ( int</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Keyframe extraction</head><p>Keyframes are very useful to summarize videos, and to provide access points into them. In this paper we also derive distinct keyframes to represent each shot. The keyframes are extracted as a by-product of multi-resolution analysis for shot boundary detection. We extend the concept of finding the scene transitions as local maxima's in the feature space; the local minima's can be chosen to represent the keyframe. Only the Resolution 3 (low resolution) is used to find the local minima points. For every shot two minima's are identifies, one in the DCT DC feature space and the other in the MA64 feature space. The color histogram distance between these two representative frames are computed and thresholded to chose either one or both the frames. Also a time constraint of 1 sec distance between the two keyframes is imposed. This ensures that the keyframes are distant and well represents the action in the video. The DCT DC selection results in color constant keyframes, where as the MA64 minima represents minimal motion angle change in the consecutive frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>The effectiveness of the algorithm was evaluated on TREC-2002 test data set. We submitted 2 runs represented as Nus1 and Nus2. The video contained a total of 2090 transitions. About 70% of them were cuts and 30 % gradual transitions and others. The results suffer from a poor detection of gradual transitions. We observed that our system throws many short gradual transitions (SGT) for single long gradual transitions. Also Fade-In-Out was considered as two separate transitions. We never eliminated the start and end transitions. By changing the SGT value to 4 we see that there is a 7% improvement in GT recall and 5% improvement in GT precision. Also we made small experiments to merge neighboring SGT's. The results show a 7% improvement in GT recall and 15 % improvement in Frame recall. These results are tabulated in the following table. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In conclusion, it has been demonstrated that TMRA framework offers a general and novel approach to flexibly and accurately probe the structure and content of digital video and meantime, it provides the ability to incorporate the new function to expand and improve the performance. Our future work is (1) to improve gradual transition detection and frame recall, (2) to investigate the active learning via artificial neural network to classify the CUT/GT, (3) to investigate the usage of other features for analyzing the video data, especially at semantic level, (4) to improve and to incorporate it into our video retrieval system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,210.54,684.63,210.54,8.74"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. TMRA system for shot boundary detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,481.74,121.03,2.34,6.13;4,473.88,121.03,7.29,6.13;4,472.74,129.43,1.95,6.13;4,464.64,123.13,8.70,10.51;4,489.30,124.41,55.04,8.74;4,86.40,143.37,155.95,8.74;4,266.52,139.93,2.34,6.13;4,260.04,139.93,5.94,6.13;4,258.96,148.33,1.95,6.13;4,250.86,142.03,206.87,10.51"><head>D</head><label></label><figDesc>is the interval of neighborhood valley points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,480.84,148.34,3.50,6.11;4,465.90,142.06,35.69,10.47;4,523.68,148.34,3.11,6.11;4,509.70,142.07,34.62,10.46;4,86.40,159.63,263.35,8.74"><head></head><label></label><figDesc>peak points and valley points, respectively.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to acknowledge the support of the National Science and technology <rs type="person">Board</rs>, and the <rs type="institution">Ministry of Education of Singapore</rs> for supporting this research under research grant <rs type="grantNumber">RP3989903</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fy7KpHH">
					<idno type="grant-number">RP3989903</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,100.53,572.49,396.27,8.74" xml:id="b0">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Yu-Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,250.54,572.49,144.99,8.74">Scale-Space Derived From B-Spline</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,100.62,583.95,427.99,8.74" xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R D</forename><surname>Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,232.56,583.95,167.62,8.74">Wavelet and Multiscale Signal Processing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Chapman and Hall Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,101.20,595.47,443.04,8.74;5,86.40,606.99,457.85,8.74;5,86.40,618.45,122.00,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,296.72,595.47,238.80,8.74">Temporal Multi-resolution Analysis for video Segmentation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,86.40,606.99,302.90,8.74">Proc. SPIE Conf. Storage and Retrieval for Media Database VIII, SPIE</title>
		<meeting>SPIE Conf. Storage and Retrieval for Media Database VIII, SPIE<address><addrLine>Bellingham, Wash</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE Press</publisher>
			<date type="published" when="2000-01">2000. Jan. 2000</date>
			<biblScope unit="volume">3970</biblScope>
			<biblScope unit="page" from="494" to="505" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
