<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,237.56,76.16,136.87,15.33;1,135.73,94.58,340.47,15.33">Question Answering: CNLP at the TREC-2002 Question Answering Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,95.17,111.72,76.42,10.51"><forename type="first">Anne</forename><forename type="middle">R</forename><surname>Diekema</surname></persName>
							<email>diekemar@syr.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies4-206 Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.66,111.72,66.37,10.51"><forename type="first">Jiangping</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies4-206 Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.00,111.72,116.87,10.51"><roleName>Necati</roleName><forename type="first">Nancy</forename><surname>Mccracken</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies4-206 Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.64,111.72,66.07,10.51"><forename type="first">Ercan</forename><surname>Ozgencil</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies4-206 Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,447.20,111.72,65.42,10.51"><forename type="first">Mary</forename><forename type="middle">D</forename><surname>Taffet</surname></persName>
							<email>mdtaffet@syr.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies4-206 Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.68,124.44,68.36,10.51"><forename type="first">Ozgur</forename><surname>Yilmazel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies4-206 Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.24,124.44,84.03,10.51"><forename type="first">Elizabeth</forename><forename type="middle">D</forename><surname>Liddy</surname></persName>
							<email>liddy@syr.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Natural Language Processing</orgName>
								<orgName type="department" key="dep2">School of Information Studies4-206 Center for Science and Technology Syracuse</orgName>
								<orgName type="institution">Syracuse University</orgName>
								<address>
									<postCode>1324-4100</postCode>
									<settlement>www.cnlp.org</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,237.56,76.16,136.87,15.33;1,135.73,94.58,340.47,15.33">Question Answering: CNLP at the TREC-2002 Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D5A288D30B763419205546C4F53960A7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the retrieval experiments for the main task and list task of the TREC-2002 question-answering track. The question answering system described automatically finds answers to questions in a large document collection. The system uses a two-stage retrieval approach to answer finding based on matching of named entities, linguistic patterns, keywords, and the use of a new inference module. In answering a question, the system carries out a detailed query analysis that produces a logical query representation, an indication of the question focus, and answer clue words.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Center for Natural Language Processing (CNLP) participated in the main task and the list task of the Question Answering track. The main task required answering 500 short fact-based questions, which have been extracted by NIST from MSNSearch and AskJeeves logs. Unlike previous years, the answer had to be exact, the answer string containing nothing but the answer itself. Also, unlike previous years, the answers to all 500 questions had to be ordered by answer confidence rather than by question number. This means that the answers that the system is most confident about should be ranked first, and the least confident should be ranked last. The scoring (see section 3) reflects a system's ability to determine how accurate a certain answer is. Not all questions had a known answer in the collection. Unanswerable questions have to be identified as such by the system to be counted correct.</p><p>The list task required answering 25 short fact-based list questions. List questions include an indication as to how many unique answer instances are needed to answer the question. A response to a list task question consisted of an unordered list of exact answers. The different answer instances could be found within single documents or across multiple documents, or a combination of both. Not all questions have all required answer instances in the collection.</p><p>This year there was a new document collection for the Question Answering track. Answers to both main task and list task questions had to be retrieved automatically from 1,033,461 documents from the following three sources: AP newswire, 1998-2000, New York Times newswire, 1998-2000, and Xinhua News Agency, 1996-2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Overview</head><p>The CNLP question-answering system consists of four different processes: question processing, document processing, paragraph finding, and answer finding. The first three processes are similar to last year's system. <ref type="bibr" coords="1,185.06,671.94,12.78,10.51" target="#b0">[1]</ref> Changes were made to the answer finding module to adapt the system to the track' s new requirements and to incorporate our new inference module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question processing</head><p>Question processing has two major parts -conversion of questions into a logical query representation and question focus recognition. Our L2L (Language-to-Logic) module was used this year to convert the query into a logical representation suitable for keyword matching and weighting in our answer finder module. Question focus recognition is performed in order to identify the type of answer expected, extraction of the number of answers required (used for the list task only), and assignment of a confidence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document processing and paragraph finding</head><p>For document retrieval, we used the ranked document list as provided by NIST. The top 200 documents from the list for each question were extracted from the TREC collection as the source documents for paragraph finding. In the paragraph finding stage, we aim to select the most relevant paragraphs from the top 200 retrieved documents from the first stage retrieval step. Paragraph selection was based on keyword occurrences in the paragraphs. Paragraph detection is based on orthographic clues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer finding</head><p>Four different strategies were applied to find the correct answers to questions. The four strategies were based on entity extraction, inference, answer patterns, and answer context, respectively. The latter three are still under development and did not contribute much to answer finding for TREC-2002. A triage program was developed to classify questions into different answer strategies based on their question type, question focus and the number of keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Entity based approach</head><p>The entity-based strategy used the tagged paragraphs from the paragraph finding stage and identified different paragraph windows (different keyword combinations) within each paragraph. A weighting scheme was used to identify the most promising paragraph window for each paragraph. These paragraph windows were then used to find answer candidates based on the question focus. All answer candidates were weighted and the top one was selected. The strategy is similar to previous years with the addition of a new function for assigning an answer confidence score.</p><p>For each answer candidate, the system assigned a confidence score to indicate the systems' confidence regarding answer correctness. The confidence score was determined by the following factors: 1) number of keywords in the same sentence, 2) question focus, 3) categorization confidence, and 4) the presence of other answer candidates in the same sentence. A threshold was determined for the confidence judgment score. If a question had a top answer whose confidence score was below the threshold, the question would be marked as having no answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Inference based approach</head><p>The inference-based approach used the results of our existing event extraction system on text to assist in finding exact answers for queries that involve events, indicated by a verb. These extractions were saved as information frames, and we implemented an inference engine to search for extracted information and to use some simple forms of linguistic inference. While this question approach was designed to answer queries where identifying events was important to the answer, we also could utilize the inference engine to answer questions that needed two or more pieces of information to find the answer. This approach starts with our existing generic entity and event extraction system. This system extracts event/agent/object information from sentences and also relation extraction about entities, primarily named entities, such as location, point-in-time and characteristic. The generic extraction is implemented using shallow parsing rules. To use generic extraction for Q&amp;A, we processed the queries with the generic extraction system as well by adding shallow parsing rules for query forms and generating an answer template that represents the form of the answer as a generic extraction with the "exact answer" slot filled in with an unknown variable. Informally, in order to answer "When did Hawaii become a state?", we formulate a template "Hawaii became a state in time ?X", where ?X is a variable. For the "What &lt;type of thing&gt;" questions, we would generate a two-part answer template. For example, in "What king signed the Magna Carta?", we would generate both "?X signed the Magna Carta" and "?X is a king".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.1">Rule patterns for queries</head><p>In order to analyze queries, we wrote shallow parsing rules that could recognize the query patterns. We describe a selection of those patterns here. Note that the query rules did not have to indicate additional qualifying phrases as those would be added by the generic extraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patterns with possibly</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.2">Query answer templates</head><p>When a query is processed by one of the query rules, one or more templates is generated to use in finding answers in the extraction database. An answer template is a frame in the same format as the frames in the extraction database. For each query, the frames are generated in the format of possible answers to the query, except that the unknown part is given as a variable, represented as the string ?X. (For TREC queries, we only needed to generate answer templates with one unknown variable.) It is the job of the inference engine to fill in a value for the variables, which will be an exact answer to the query.</p><p>In general, " when" queries ask for a property that is called " point-in-time" by the extraction system. For " do verb" forms, the first nounphrase in a sentence with an active verb is assumed to be the agent of that event.</p><p>when do &lt;nounphrase&gt; &lt;verbphrase&gt; &lt;nounphrase&gt; (When did George Orwell write Animal The " what &lt;typeofthing&gt;" patterns generate two frames for the two pieces of information. The second frame qualifies the answer as to what type of thing it is. The property is called " description" here, and there are several actual extraction properties that can be used to establish that the answer matches this description. Note that this frame is an entity frame.</p><p>what &lt;typeofthing&gt; do &lt;nounphrase&gt; &lt;verbphrase&gt; (What flower did Vincent van Gogh paint?) event = paint agent = Vincent van Gogh object = ?X entity = ?X description = flower</p><p>The other " What &lt;typeofthing&gt;" query types similarly generate the second frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.3">Extraction matching with the inference engine</head><p>For each query, the answer candidate documents were processed using the generic extraction system. The extractions were put into a database that we call the knowledge base. For answering queries, we then tried to match the template from the query, which is the " goal" , with extractions in the knowledge base. We call the matcher the inference engine, but the types of inferencing that we are doing are linguistic in nature. We are not using inference rules that rely on world knowledge.</p><p>The inference engine tries to match all the frames representing the goal template. For each frame, it establishes that each attribute of the goal frame is present in the answer frame and that the values of each attribute " match" . In order to match values of attributes, the inference engine has several rules to establish a match even if it is not an exact match.</p><p>Although we have not shown this in the examples so far, in addition to the string that is kept as the value of an attribute in the extraction frame, some string values also have links to an entity extraction frame. If such a link is present, the inference engine will also check that any additional attributes of that entity are also matched by the answer value. This is used in more complex queries that have additional modifiers.</p><p>Although the inference engine tries to match all of the attributes of the goal frame, it uses an abductive inference rule that allows a frame to match even when not all of the attributes are present, but with a lower probability of matching.</p><p>Finally, the inference engine has a set of axioms that embody linguistic knowledge about different forms of frames to try to match. If the goal frame has no match in the knowledge base, then these axioms are used to generate new goal frames that are sufficient to establish the answer.</p><p>An example of the types of linguistic alternatives is the changing of a goal event frame into an equivalent entity frame where that entity is described by the nominalization of the verb. This rule employs a list of such subject nominalizations. event = invent entity = ?X agent = ?X description = inventor object = road traffic cone modifier = of the road traffic cone</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Pattern based approach</head><p>The pattern based approach is used for certain types of questions only: acronym, counterpart, definition, famous for, stand for, synonym, why. <ref type="bibr" coords="5,306.62,380.34,12.78,10.51" target="#b0">[1]</ref> We developed lexical pattern rules for answer extraction for these special question types. These patterns were used to identify text segments that could possibly provide an answer. Each of the answer identification patterns had its own confidence score indicating the likeliness of that pattern identifying for example, the meaning of a synonym. Unfortunately, the pattern-based approach did not prove effective for the TREC-2002 questions, partly because there were no definition questions this year. However, we find that the pattern based approach is useful in answering student's questions in the aerospace domain in a funded project for NASA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Context-based approach</head><p>The context-based approach deals with those questions for which the system could not determine a question focus, which happens frequently (194 (39%) out of 500). When the system fails to identify a focus, the system attempts to find answers by using the context (the sentence in which the question keywords appear). This approach to answer finding is rather inexact and should be viewed as a last ditch effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>We submitted three runs for the TREC-2002 QA track: one run for the main task and two runs for the list task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main task results</head><p>Average over 500 questions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUT11IR1MT</head><p>Confidence The evaluation measure for the main task (see Table <ref type="table" coords="6,322.82,261.90,4.59,10.51" target="#tab_1">2</ref>) is the confidence-weighted score (similar to the uninterpolated average precision measure from information retrieval). The score for an individual question is the number of correct answers up to and including that question divided by the number of questions answered so far. The score for the entire run is the mean of the individual questions' scores. The confidence-weighted score can range from 0 to 1 inclusive, with 1 a perfect score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">List task results</head><p>Average over 25 questions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUT11IR1LT SUT11IR1LT2</head><p>Average Accuracy 0.11 0.15 Questions with no answer found 17 15 Questions with rank above the median 5 8 Questions with rank on the median 17 15 Questions with rank below the median 3 2</p><p>Table <ref type="table" coords="6,119.66,451.51,4.13,11.34">3</ref>. Question answering results for the list task.</p><p>The evaluation measure for the list task (see Table <ref type="table" coords="6,314.36,476.04,4.59,10.51">3</ref>) is accuracy. The score for an individual question is the fraction of unique, correct instances over the target number of instances. The score for the entire run is the mean of the individual questions' scores. Accuracy can range from 0 to 1 inclusive, with 1 a perfect score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis</head><p>The analysis centers on the performance of our focus identification module and the contribution of each of the four different answer-finding approaches to question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main and list task performance</head><p>The large majority of the questions in the main task (84%) and list tasks (68%) were answered incorrectly. The number of questions for which our performance is the same as the median performance (of all participating systems) is close to (427 =&gt; 422), or identical (17 =&gt; 17, 15 =&gt; 15) to the number of questions that we answered incorrectly. These numbers seem to suggest that a lot of systems could not answer most of the questions. Further analysis is needed to determine why this is the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Focus identification</head><p>Focus identification is the most important procedure of query processing.</p><p>It determines what answer strategy will be applied by the system to search for correct answers and also guides answer candidate selection. The question focus analysis is based on main task run (SUT11IR1MT).</p><p>The system correctly identified the focus for 301 questions out of 500 (60%), and incorrectly identified the focus for 5 questions (1%). There are 194 questions (39%) for which the system could not determine the focus (see Table <ref type="table" coords="7,271.70,176.82,3.98,10.51" target="#tab_2">4</ref>). In cases where the focus is identified incorrectly no correct answers were found. When we look at the questions for which no focus could be determined at all we see that all the questions were answered incorrectly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Answer finding performance</head><p>The system applied several answer finding approaches this year, including a new inference module. However, the effort on these new approaches was limited due to the time constraints, leaving them in an earlier stage of development than we would have liked. When we look at the individual contributions of each of the modules (see Table <ref type="table" coords="7,349.46,389.70,4.59,10.51" target="#tab_3">5</ref>) it becomes clear that the system still largely relies on the entity based approach for the identification of correct answers (49 out of 64 = 77%). Only 37 questions were sent to the inference engine, which managed to answer only 4 of them correctly. As pointed out previously, the pattern-based approach did not prove useful for TREC-2002 questions. Only one question was considered answerable by the pattern based approach and this question was answered incorrectly. The context approach, which is the module that handles questions for which there is no focus available, only answered 10 questions correctly out of the 163 that were assigned to this module. However, as a module that handles questions that no other module can handle, it still answered some questions that would otherwise have been lost. There were 8 questions for which no relevant paragraphs were found. These questions were deemed " unanswerable" . This proved correct for only one of them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and further research</head><p>It appears that most of the questions were not answered correctly by our system and that this is a common problem among participating systems. Analysis of the focus assignment module showed that having a correct focus helps in finding a correct answer but does not guarantee a correct answer. This suggests that improving the focus program to capture more question foci should not be the main center of our future research but rather we have to find other strategies to increase the number of questions we can answer correctly. Analysis of the four different answer finding approaches showed that the three modules other than the entity based module did not contribute much to finding correct answers. However, the reason for this could be that they are at an early stage in development. We will concentrate on further development of these modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.02,640.62,31.68,10.51;3,126.02,653.34,71.90,10.51;3,137.06,666.06,103.85,10.51;3,137.06,678.78,100.16,10.51;3,137.06,691.50,85.33,10.51;4,90.02,74.88,319.44,10.51;4,126.02,87.60,71.93,10.51;4,137.06,100.32,101.83,10.51;4,137.06,113.04,85.33,10.51;4,90.02,138.48,256.72,10.51;4,90.02,151.20,321.48,10.51;4,126.02,163.92,88.46,10.51;4,137.06,176.64,70.70,10.51;4,137.06,189.36,62.77,10.51;4,90.02,214.80,308.62,10.51;4,90.02,227.52,261.30,10.51;4,126.02,240.24,63.05,10.51;4,137.06,252.96,51.13,10.51;4,137.06,265.68,77.33,10.51;4,90.02,291.12,255.78,10.51;4,126.02,303.84,48.29,10.51;4,134.30,316.56,53.23,10.51;4,134.30,329.28,51.43,10.51"><head></head><label></label><figDesc>Farm?) event = do write agent = George Orwell object = Animal Farm point-in-time = ?X when do &lt;nounphrase&gt; &lt;verbphrase&gt; (When did Mt. St. Helens erupt?) event = do erupt agent = Mt. St. Helens point-in-time = ?X In a where query, there is a property " location" for events. where do &lt;nounphrase&gt; &lt;verbphrase&gt;(Where did the ukulele originate?) event = do originate agent = ukulele location = ?X Some query patterns are asking for the agent of the object of an event. who &lt;verbphrase&gt; &lt;nounphrase&gt;(Who invented baseball?) event = invent agent = ?X object = baseball what do &lt;nounphrase&gt; &lt;verbphrase&gt; (What do bats eat?) event = eat agent = bats object = ?X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.02,266.17,420.82,181.98"><head>Table 1 . Rule patterns for queries.</head><label>1</label><figDesc></figDesc><table coords="3,95.42,266.17,415.42,167.79"><row><cell>significant verb phrases:</cell><cell></cell></row><row><cell>when do &lt;nounphrase&gt; &lt;verbphrase&gt; &lt;nounphrase&gt;</cell><cell>When did George Orwell write Animal Farm?</cell></row><row><cell>when do &lt;nounphrase&gt; &lt;verbphrase&gt;</cell><cell>When did Mt. St. Helens erupt?</cell></row><row><cell>Where do &lt;nounphrase&gt; &lt;verbphrase&gt;</cell><cell>Where did the ukulele originate?</cell></row><row><cell>who &lt;verbphrase&gt; &lt;nounphrase&gt;</cell><cell>Who invented baseball?</cell></row><row><cell>what do &lt;nounphrase&gt; &lt;verbphrase&gt;</cell><cell>What do bats eat?</cell></row><row><cell>Patterns with what (or which) &lt;typeofthing&gt;:</cell><cell></cell></row><row><cell>what &lt;typeofthing&gt; do &lt;nounphrase&gt; &lt;verbphrase&gt;</cell><cell>What flower did Vincent van Gogh paint?</cell></row><row><cell>what &lt;typeofthing&gt; be &lt;nounphrase&gt; in</cell><cell>What hemisphere is the Philippines in?</cell></row><row><cell>what &lt;typeofthing&gt; be &lt;nounphrase&gt;</cell><cell>What color is a poison arrow frog?</cell></row><row><cell>what &lt;typeofthing&gt; be &lt;nounphrase&gt; &lt;prepphrase&gt;</cell><cell>What gasses are in the troposphere?</cell></row><row><cell>what &lt;typeofthing&gt; &lt;verbphrase&gt; &lt;nounphrase&gt;</cell><cell>What American composer wrote the music for</cell></row><row><cell></cell><cell>" West Side Story" ?</cell></row><row><cell>what &lt;typeofthing&gt; &lt;verbphrase&gt; &lt;prepphrase&gt;</cell><cell>What currency is used in Australia?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.02,114.29,286.30,133.40"><head>Table 2 . Question answering result for the main task.</head><label>2</label><figDesc></figDesc><table coords="6,95.42,114.29,280.90,119.21"><row><cell>-weighted score</cell><cell>0.225</cell></row><row><cell>Number wrong</cell><cell>422</cell></row><row><cell>Number inexact</cell><cell>5</cell></row><row><cell>Number unsupported</cell><cell>9</cell></row><row><cell>Number right</cell><cell>64</cell></row><row><cell>Precision of recognizing no answer</cell><cell>0.167 ( 12 / 72 )</cell></row><row><cell>Recall of recognizing no answer</cell><cell>0.261 ( 12 / 46 )</cell></row><row><cell>Questions with rank above the median</cell><cell>47</cell></row><row><cell>Questions with rank on the median</cell><cell>427</cell></row><row><cell>Questions with rank below the median</cell><cell>26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.02,202.26,428.63,122.17"><head>Table 4 . Question focus assignment.</head><label>4</label><figDesc></figDesc><table coords="7,413.90,202.26,104.75,10.51"><row><cell>These figures show that</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,90.02,542.63,386.54,167.06"><head>Table 5 . Answer finding performance.</head><label>5</label><figDesc></figDesc><table coords="7,95.42,542.63,381.14,152.87"><row><cell></cell><cell>Correct</cell><cell>Inexact</cell><cell>Unsupported</cell><cell>Wrong</cell><cell>Total</cell></row><row><cell></cell><cell>answers</cell><cell>answers</cell><cell>answers</cell><cell>answers</cell><cell></cell></row><row><cell>Entity based</cell><cell>49</cell><cell>5</cell><cell>9</cell><cell>218</cell><cell>281</cell></row><row><cell>approach</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inference</cell><cell>4</cell><cell></cell><cell></cell><cell>33</cell><cell>37</cell></row><row><cell>approach</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pattern based</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell></row><row><cell>approach</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Context based</cell><cell>10</cell><cell></cell><cell></cell><cell>163</cell><cell>173</cell></row><row><cell>approach</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No paragraphs</cell><cell>1</cell><cell></cell><cell></cell><cell>7</cell><cell>8</cell></row><row><cell>found</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell>64</cell><cell>5</cell><cell>9</cell><cell>422</cell><cell>500</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.44,227.88,395.51,10.51;8,90.02,240.60,423.47,10.51;8,90.02,253.32,389.55,10.51;8,90.02,266.04,203.21,10.51" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,177.02,240.69,316.74,10.30">Question Answering : CNLP at the TREC-10 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Diekema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Taffet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mccracken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ozgencil</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ercan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Yilmazel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Liddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,90.02,253.32,166.75,10.51">The Tenth Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="485" to="494" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
	<note>TREC-2001</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
