<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.66,84.44,256.63,17.88;1,122.34,102.86,367.19,17.88">Video Classification and Retrieval with the Informedia Digital Video Library System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.12,126.95,67.80,10.46"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.83,126.95,31.24,10.46"><forename type="first">R</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.82,126.95,24.66,10.46"><forename type="first">Y</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.48,126.95,26.75,10.46"><forename type="first">R</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.48,126.95,53.13,10.46"><forename type="first">M</forename><surname>Christel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.17,126.95,56.67,10.46"><forename type="first">M</forename><surname>Derthick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.20,140.75,55.00,10.46"><forename type="first">M.-Y</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.87,140.75,40.95,10.46"><forename type="first">R</forename><surname>Baron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.21,140.75,47.75,10.46"><forename type="first">W.-H</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.21,140.75,39.78,10.46"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<settlement>Pittsburgh PA</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.66,84.44,256.63,17.88;1,122.34,102.86,367.19,17.88">Video Classification and Retrieval with the Informedia Digital Video Library System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">835267B964C2E64E5C6777C935ED11A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is organized in three parts. The first part details some of the lower level shot classification work, the second part describes the 'manual' retrieval systems while the last section details the interactive retrieval system for the Carnegie Mellon University TREC Video Retrieval Track runs. The description of the data can be found elsewhere in the proceedings of the 2002 TREC conference video track overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification</head><p>In the TREC02 video track, one of the main tasks is to detect various semantic features concepts such as "Indoor/Outdoor", "People" etc. This part contains the description of the classification tasks. We submitted runs for the following classification concepts in the TREC 2002 Video Track. To obtain training data, we manually annotated each I-frame of the 23.26 hours feature development collection for each category. a. Outdoors: Segment contains a recognizably outdoor location, i.e., one outside of buildings. Should exclude all scenes that are indoors or are close-ups of objects (even if the objects are outdoor). b. Indoors: Segment contains a recognizably indoor location, i.e., inside a building. Should exclude all scenes that are outdoors or are close-ups of objects (even if the objects are indoor c. Cityscape: Segment contains a recognizably city/urban/suburban setting. d. Monologue: an event in which a single person is at least partially visible and speaks for a long time without interruption by another speaker e. Face: at least one human face with nose, mouth, and both eyes f. People: a group of two more humans g. Text Detection: superimposed text large enough to be read h. Speech: human voice uttering recognizable words i. Instrumental Sound: sound produced by one or more musical instruments, including percussion instruments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><p>It is a critical challenge to find a good feature set for image classification. A number of image features based on color and texture attributes have been reported in the literature for image retrieval. We tried several of them and explored some new features at the same time. Color Histograms. We used the histogram of 3*3 image regions in HSV color space for each MPEG I-frame. The color features were derived from a histogram in the quantized HSV color space. Textures. We use the mean and variance of a texture orientation histogram for each of the 3*3 regions as texture feature. Edge features. We used a feature called the Edge Direction Histogram. A Canny edge detector was used to extract the edges from an image. A total of 73 bins were used to represent the edge direction histogram of an image; the first 72 bins are used to represent the edge directions quantized at 5 o intervals and the last bin represents a count of the number of pixels that didn't contribute to any edge. Edge direction coherence vector. This feature stores the number of coherent versus non-coherent edge pixels with the same edge directions (considering only horizontal and vertical axis within a range of +/-5 o ,). We thresholded on the size of every 8 connected components of edges in a given direction to decide whether the region could be considered coherent or not. This feature was used to distinguished structured edges (like edges of buildings) from arbitrary edge distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera motion.</head><p>We used statistical distribution patterns to detect the pan/tilt/zoom camera operations based on the motion vectors of MPEG encoding. The resulting features encoded the presence/absence of these six kinds of camera operations (pan left, pan right, tilt up, tilt down, zoom in, zoom out) as a new type of feature for image classification. MPEG motion vectors. We transformed the motion vectors directly encoded in the MPEG-compressed video into a different kind of feature, namely a histogram of the motion vector angle and velocity, as well as the wavelet coefficients of motion vectors.. Although we experimented extensively with the features derived from camera motion analysis and the raw MPEG motion vectors, these additional features did not contribute to overall classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Algorithms</head><p>We experimented with several classification tools for these tasks, including SVM, KNN, Adaboosting and Decision Trees. Comparing their performance using cross validation on a comparative large data set, we reached the conclusion that support vector machine learning was best, with the power=2 polynomial as the kernel function. Nonlinear functions usually performed better than linear SVM kernel functions. The trade-off is that for nonlinear functions, the parameter space can be huge and therefore it may cause overfitting for small training datasets. Among the tasks, the cityscape classification suffered from the problem of insufficient positive training examples, which is also the reason why we did not submit a landscape classification for evaluation. For the cityscape classification training data, the positive examples (that is, the cityscape images vs. the non-cityscape images) comprised only 12% of the whole data set. Such small ratios of positive examples in the training set cannot be well represented by the classification methods we attempted. In addition, we investigated using the chi-square function as distance function based on published literature. Contrary to published claims, the chi-square function was not superior to any other functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross_validation</head><p>Due to the temporal correlation between adjacent images in a video, an initial cross validation based on random sampling of shots gave much better performance than appropriate for the true prediction capability of the models. This was due to the fact that similar shots appeared throughout a single video or 'movie'. So we performed a video based cross validation based, using 30 complete videos as training and then testing on the remaining 11 videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Selection</head><p>It is a challenge to select a good feature set for image classification. Qualifying their discrimination ability of each feature in the given classification problem is difficult. We performed video-based cross validation on training sets and compared the different features' performance based on the resulting classification error and precision / recall of each task. For the camera motion related features and the MPEG motion vector related features, we explored numerous experiments to test their usefulness to the image classification task. However, they did not give conclusive results clearly. Finally, we ended up not using the camera or motion features in the final submission.</p><p>To get the best feature combination for each task, we performed a 6 folder movie based cross validation on the three training sets on different feature combinations. The best feature combinations were always included texture, edge and color features . Since the results were submitted as shot based features and not classification from individual images, we integrated all I frame classification results in a shot into this shot's feature detection result. The confidence of a particular feature detection is the ratio between number of feature presenting I frames vs. number of feature absent I frames in this shot. Our results showed huge difference of the performance of different classifiers. The reason of this discrepancy is possibly caused by the variability of the training sets, the inconsistency between training set and the test sets, or the varying difficulty of the different classifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-standard classification for text, faces, people, monologues, speech and music</head><p>Variations of the classification approach were used for the face, people, monologue and audio categories. For the face category, we used the Schneiderman face detector <ref type="bibr" coords="2,325.48,668.07,15.35,8.74" target="#b17">[19]</ref>, exclusively. For people. we extracted the following features At the level of shots:</p><p>• Number of frames in a shot For each I-frame within a shot we also extracted these frame-based features:</p><formula xml:id="formula_0" coords="3,90.00,71.78,4.61,13.06">•</formula><p>• Average number of faces per frame,</p><p>• Average number of faces per frame with high confidence • Average number of faces per frame with low confidence</p><p>Since the total number of features was fairly low, we trained a decision-tree based classifier (C4.5), which outperformed SVM on this task in cross-validation experiments.</p><p>Our contrasting people classification submission merely counted the number of faces visible in each I-frame, and averaged this over the whole shot. This baseline approach performed significantly worse with a classification error of 0.403 vs. 0.498.</p><p>The task of text-overlay classifier is to find scenes with superimposed texts. Simply predicting a scene to be a text overlay based on whether or not the OCR engine is able to find text is not good enough because that OCR engine is quite error-prone. The features extracted were:</p><p>1. time: related to the whole movie, when is the OCR detected texts are found 2. #terms_within_a_shot 3. #dictionary_words_within_a_shot 4. average_popularity_valid_trigram_in_a_shot 5. average_popularity_valid_4gram_in_a_shot 6. average_no_alphabets_found_in_a_term 7. ratio_dictionary_words_to_detected_terms 8. ratio_length_of_all_dictionary_words_to_length_of_detected_terms For classification, similar to the people classifier, a decision tree (C4.5) was used instead of a SVM.</p><p>For monologues, we used as features: 1. The portion of time where a least one (face) was detected. 2. The confidence of the face in every I-frame. 3. The number of speaker voice changes in one shot 4. The confidence in any significant audio change during this shot. 5. The number of faces present in one image. These features were also fed into an decision tree classifier.</p><p>Speech and music were classified by the same speaker identification code as in the 2001 TREC video track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>'Manual' video retrieval with classification pseudo-relevance feedback</head><p>Example-based image retrieval task has been studied for many years. The task requires the image search engine to find the set of images from a given image collection that is similar to the given query image. Traditional methods for content-based image retrieval are based on a vector model. These methods represent an image as a set of features and the difference between two images is measured through a (usually Euclidean) distance between their feature vectors. While there have been no large-scale, standardized evaluations of image retrieval systems, most image retrieval systems are based on features such as color, texture, and shape that are extracted from the image pixels.</p><p>In our system two kinds of low-level features are used for finding similar images: color features and texture features. The color features are the cumulative color histograms for each separate color channel, where the three channels are derived from the HSV color space. We use 16 bins for hue and 6 bins for both saturation and value. We generate a texture feature for each subblock of a 3*3 image tessellation. The texture features are obtained through the convolution of the subblock with various Gabor Filters. In our implementation, 6 angles are used and each filter output is quantized into 16 bins. We compute a histogram for each filter and generate their central and second-order moments as the texture features. We concatenate all the features into a longer feature vector for every image; i.e. one vector for all color features and one vector for all texture features. We use a simple nearest neighbor (NN) image matching algorithm on both color and texture to produce the initial similarity results. In a preprocessing step, each element of the feature vectors is scaled by the covariance of its dimension. We adopted the Euclidean distance as the similarity measure between two images.</p><p>Although nearest neighbor search is the most straightforward approach to finding the matching images, it suffers from two major drawbacks. First, irrelevant features in the vector are given equal weight to important features, and thus retrieval accuracy will hurt decrease dramatically. Feature selection is therefore a necessary step prior to computing the nearest neighbor images. In theory, relevance feedback, through re-weighting and query refinement, is a powerful tool to refine the feature weighting so as to provide more accurate results. However, it is impossible to obtain the user judgment information in most automatic retrieval tasks. A second negative aspect is the unjustified distance function. Since an appropriate distance measure is a function of both the characteristics of the dataset and of the queries, a simple Euclidean distance function is unlikely to work for all the queries and images. Another concern is the normalization of the different dimension of a feature vector. To mitigate all these issues, we propose a classification-based pseudo-relevance feedback approach to refine the initial retrieval result. Support Vector Machines (SVMs) are used as our basic classifier mechanism, since SVMs are known to yield good generalization performance compared to other classification algorithms.</p><p>The basic idea for this approach is to augment the retrieval results by incorporating the classification output value through Pseudo-Relevance Feedback (PRF). The input data for the classifier is based on the information provided by our initial retrieval results. Standard PRF methods, which originated in the text information retrieval community, utilize the top-ranked documents as positive examples to improve the accuracy. The idea is to re-weight the words in the document feature vector based on the words in the top ranked documents, which are assumed to be positive examples. However, due to the poor initial performance of current video retrieval system, even the very top-ranked results are not always the correct ones that meet the users' information need. Unlike in text retrieval methods, it is more appropriate to make use of the lowest ranked documents in the collection after the initial search, which are more likely to be the negative examples. Therefore, we construct a classifier where the positive data are the query image examples and the negative data are sampled from the least confident image examples in the initial retrieval results. Since the number of positive examples in our retrieval task is always much smaller than the number of the negative examples, we cast the problem into the imbalanced dataset classification framework. To sample more negative examples but achieve an overall balanced distribution of negative and positive examples in the classifier training set, we apply an ensemble of SVMs to tackle the rare class problem. The overall procedure can be summarized as follows, 1. Generate the initial classification results by nearest neighbor retrieval for all the images in the collection. 2. Choose all the query images as positive data. Denote the number of query images as m. 3. Construct a negative sub-collection based on the initial retrieval results, which are defined by the lowest 10% of the retrieved data from the collection. We sample k groups of negative data from the negative sub-collection, where each group contains m query images. Combine each group of negative data and all the positive data as a training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Build a classifier from each training set to produce new relevant score for any images x</head><p>, where</p><formula xml:id="formula_1" coords="4,72.00,554.82,432.82,40.99">i is the index of training set ) 1 )( ( k i x f i ≤ ≤ 5.</formula><p>Combine the results in form of logistic regression, which is</p><formula xml:id="formula_2" coords="4,74.16,598.95,162.73,62.80">∑ ∑ = = + + + = + k i i i k i i i x f x f x P 1 0 1 0 )) ( exp( 1 )) ( exp( ) | ( β β β β</formula><p>In our system, we simply set 0</p><formula xml:id="formula_3" coords="4,191.10,672.63,160.28,17.32">β as 0, ) 1 ( k i i ≤ ≤ β as equal values.</formula><p>Our approach presented here utilizes the collection distribution knowledge to refine the final result. Due to the good generalization ability of the SVM algorithm, the most relevant features are selected automatically. Also the approach yields a better distance function based on the probability estimation compared with the simple Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination of multiple agents</head><p>As the first step to integrate different types of agents, all the relevance scores of the agents are converted into posterior probability. For each agent other than the classification-based PRF agent, the posterior probability is generated by a linear transformation of their rank and scaled to the range of [0, 1]. All these posterior probabilities are simply linear combinations as follows: is the weight for image agent, text agent, movie information agent respectively, which are set to be 1, 1, 0.2.  </p><formula xml:id="formula_4" coords="5,117.42,196.07,191.14,28.70">) | ( ) | ( )) | ( ) | ( ) |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech Recognition</head><p>The audio processing component of our video retrieval system splits the audio track from the MPEG-1 encoded video file, and decodes the audio and downsamples it to 16kHz, 16bit samples. These samples are then passed to a speech recognizer. The speech recognition system we used for these experiments is a state-of-the-art large vocabulary, speaker independent speech recognizer. For the purposes of this evaluation, a 64000-word language model derived from a large corpus of broadcast news transcripts was used. Previous experiments had shown the word error rate on this type of mixed documentary-style data with frequent overlap of music and speech to be 35 -40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Retrieval</head><p>All retrieval of textual material was done using the OKAPI formula. The exact formula for the Okapi method is shown in Equation <ref type="bibr" coords="5,149.52,419.61,11.68,8.74" target="#b0">(1)</ref> where tf(qw,D) is the term frequency of word qw in document D, df(qw) is the document frequency for the word qw and avg_dl is the average document length for all the documents in the collection.</p><formula xml:id="formula_5" coords="5,322.86,432.02,210.87,44.87">∑ ∈               + + + + - = Q qw D qw tf dl avg D qw df qw df N D qw tf D Q Sim ) , ( _ | | 5 . 1 5 . 0 ) 5 . 0 ) ( 5 . 0 ) ( log( ) , ( ) , (<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We report our results in terms of mean average precision in this section, as shown in Table <ref type="table" coords="5,470.42,503.07,3.76,8.74" target="#tab_1">1</ref>. Four different combination of the retrieval agents are compared in this table, including the combination of text agents (Text), movie agents (Movie), nearest neighbor on color (Color), nearest neighbor on texture (Texture) and classificationbased PRF (Classification). The results show a significant increase in retrieval quality using classification-base PRF technique. While the text information from the speech transcript accounts for the largest proportion of the mean average precision (0.0658), only a minimal gain was observed in the mean average precision when the 'movie title' and abstract were also searched (0.0724) in addition to the speech transcripts. The image retrieval component provided further improvements in the scores to a mean average precision of 0.1046. Finally, the PRF technique managed to boost the mean average precision to the final mean average precision score of 0.1124. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interactive Video Retrieval</head><p>For the 2002 TREC video track interactive condition, we used the basic Informedia Digital Video Library system, as in the 2001 TREC Video TREC. A few refinements to the interface are discussed and illustrated below. Since IDVLS was designed to return 'stories', which can encompass multiple shots as retrieval results, we modified the interface to allow a shot-based presentation of the results which we called "Multiple document storyboards". The text was retrieved in roughly 3-minute story chunks, and all shots for that story were presented to the user. A storyboard display, which concatenated the top N relevant stories and their shots, was used <ref type="bibr" coords="6,438.00,576.69,31.40,8.74">[Figure]</ref>. Thus a user could visually scan for relevant images from a fairly large storyboard display of the top relevant stories and their shots. Selecting a shot as relevant placed this shot onto an answer set display, which could again be edited before final submission <ref type="bibr" coords="6,140.33,611.19,31.41,8.74">[Figure]</ref>. Because of the large number of shots on the result storyboard, we placed the resolution of the keyframe size and the layout under user control. Thus a user can shrink or enlarge the size of the keyframes displayed on the storyboard, depending on the desire to visually inspect the keyframes more closely, or to view the complete set. The size of the window, and the total number of results displayed could also be modified. We found that the query context plays a key role in filtering image sets to manageable sizes. The TREC 2002 image feature set offered filtering capabilities for the classified categories of indoor, outdoor, faces, people, etc. The user interface provided for a display of the classified feature values for every shot <ref type="bibr" coords="6,228.66,691.71,31.37,8.74">[Figure]</ref>. The user was also able to control the threshold values for each of the feature categories. This enabled the display to be more manageable by filtering out shots that were more likely to be feature X, and unlikely to be feature Y, depending on the query context. Since the display showed the number of active results, and provided direct feedback on the distribution of the data, the large number of irrelevant shot could easily be filtered down to a manageable number, that was then visually scanned by the user. The multi-document storyboard facilitated quick inspection of many images. A first-order filtering by query text provided an initial set of images that constituted potential results. The multi-document storyboard based on 3-minute segments and shots enabled the user to find relevant shots, which were temporally near shots where query-words had been matched. The keyframe ordering by video segment and time useful. The classified shot features were useful for filtering, but needed to be manually adjusted depending on the particular queries. Users were able to drilldown to details, going from keyframe images to observing video, which was often necessary to eliminate uncertainty that could not be resolved by looking at a still image frame.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,108.00,75.33,183.58,8.74;3,90.00,84.02,171.67,13.06;3,90.00,96.20,168.84,13.06;3,90.00,108.44,209.27,13.06;3,90.00,120.62,184.35,13.06;3,90.00,132.86,153.24,13.06;3,90.00,145.10,135.80,13.06;3,90.00,157.28,186.07,13.06"><head></head><label></label><figDesc>Number of faces detected by the face detector • Number of faces with high confidence • Number of faces with low confidence • Average confidence score of the faces in a shot, • The standard deviation of the face scores, • A smoothed minimum face score, • A smoothed maximum score, • Average pixel area for each detected face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,158.88,246.21,12.83,6.10;5,145.32,246.21,1.95,6.10;5,131.28,246.21,3.11,6.10;5,125.64,239.93,6.00,10.46;5,176.76,241.23,363.26,8.74;5,72.00,252.27,410.19,8.74"><head></head><label></label><figDesc>for the three search agents for image retrieval: NN on color, NN on texture and classification PRF, which are either set to be 0 or 1 in our contrastive experiments reported below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,106.62,504.39,390.59,8.74;6,106.62,515.85,50.63,8.74;6,99.00,140.52,423.00,351.12"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Multi-document storyboards combine all shots from highly relevant segments into one display.</figDesc><graphic coords="6,99.00,140.52,423.00,351.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,106.62,344.43,333.10,8.74;7,99.00,76.62,424.80,254.82"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Resolution and layout of the storyboard can be modified by the user.</figDesc><graphic coords="7,99.00,76.62,424.80,254.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,79.68,674.97,378.09,8.74;7,72.00,499.98,427.68,164.52"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Users can filter shots based on thresholds in any feature classification category.</figDesc><graphic coords="7,72.00,499.98,427.68,164.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,108.24,426.39,287.58,8.74;8,100.68,77.88,421.32,335.52"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Feature classification statistics are accessible for any shot.</figDesc><graphic coords="8,100.68,77.88,421.32,335.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,91.50,362.76,439.50,344.22"><head></head><label></label><figDesc></figDesc><graphic coords="9,91.50,362.76,439.50,344.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.00,618.57,420.67,91.78"><head>Table 1 Video</head><label>1</label><figDesc>Retrieval Results on the 25 queries of the 2003 TREC video track evaluation.</figDesc><table coords="5,354.86,618.57,137.81,31.72"><row><cell>Precision</cell><cell>Recall</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell>Precision</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,75.77,502.89,445.31,8.74;8,90.00,514.41,436.57,8.74;8,90.00,525.93,22.58,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,363.38,502.89,157.70,8.74;8,90.00,514.41,99.54,8.74">Efficient Color Histogram Indexing for Quadratic Form Distance</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,200.44,514.41,220.61,8.74">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="729" to="736" />
			<date type="published" when="1995-07">July, 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.76,537.39,414.28,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,179.75,537.39,67.79,8.74">Okapi at TREC-4</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,266.06,537.39,194.20,8.74">The Fourth Text Retrieval Conference (TREC-4)</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.77,548.91,456.36,8.74;8,90.00,560.43,373.84,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,284.56,548.91,150.03,8.74">Video OCR for Digital News Archive</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,454.74,548.91,77.38,8.74;8,90.00,560.43,213.07,8.74">Proc. Workshop on Content-Based Access of Image and Video Databases</title>
		<meeting>Workshop on Content-Based Access of Image and Video Databases<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-01">Jan 1998</date>
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.75,571.89,420.22,8.74;8,90.00,583.41,439.34,8.74;8,90.00,594.87,204.53,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,294.21,571.89,201.77,8.74;8,90.00,583.41,253.42,8.74">Speech in Noisy Environments: Robust Automatic Segmentation, Feature Extraction, and Hypothesis Combination</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,354.90,583.41,174.44,8.74;8,90.00,594.87,70.77,8.74">IEEE Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-05">May, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.76,606.39,456.16,8.74;8,90.00,617.91,447.86,8.74;8,90.00,629.37,22.58,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,362.90,606.39,169.02,8.74;8,90.00,617.91,71.40,8.74">Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,172.49,617.91,220.56,8.74">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000-12">December, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.77,640.89,430.98,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,215.17,640.89,59.18,8.74">Color Indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,285.33,640.89,96.14,8.74">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.77,652.41,432.21,8.74;8,90.00,663.87,195.94,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,185.67,652.41,264.14,8.74">The Pragmatics of Information Retrieval Experimentation, revised</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Tague-Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,460.24,652.41,47.74,8.74;8,90.00,663.87,112.53,8.74">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="467" to="490" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.77,675.39,410.37,8.74;8,90.00,686.91,125.31,8.74" xml:id="b7">
	<monogr>
		<ptr target="http://www.trec.nist.gov/" />
		<title level="m" coord="8,335.28,675.39,146.58,8.74">Text REtrieval Conference web page</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>TREC 2002 National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.76,698.37,201.22,8.74;8,90.00,709.89,170.00,8.74" xml:id="b8">
	<monogr>
		<ptr target="http://www-nlpir.nist.gov/projects/trecvid/" />
		<title level="m" coord="8,90.01,698.37,182.57,8.74">The TREC Video Retrieval Track Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.33,74.61,430.97,8.74;9,90.00,86.13,323.80,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,353.41,74.61,157.90,8.74;9,90.00,86.13,193.74,8.74">Lessons Learned from the Creation and Deployment of a Terabyte Digital Video Library</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Wactlar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Christel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,294.36,86.13,64.22,8.74">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.34,120.63,403.63,8.74" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,155.45,120.63,114.26,8.74">Visual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann Ed</publisher>
			<pubPlace>San Francisco, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.34,132.09,438.13,8.74;9,90.00,143.61,397.40,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,370.11,132.09,148.36,8.74;9,90.00,143.61,174.31,8.74">Matching and Retrieval Based on the Vocabulary and Grammar of Color Patterns</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mojsilovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kovacevic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Safranek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ganapathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,274.68,143.61,121.16,8.74">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="54" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.35,155.13,432.91,8.74;9,90.00,166.59,60.89,8.74" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,129.48,155.13,257.38,8.74">Intelligent Image Databases: Toward Advanced Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Hingham, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.35,178.11,459.10,8.74;9,90.00,189.63,101.74,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,243.07,178.11,202.27,8.74">On image classification: city images vs. landscapes</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vailaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,456.12,178.11,79.09,8.74">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1921" to="1935" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.35,201.09,411.44,8.74;9,90.00,212.61,197.15,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,194.18,201.09,234.05,8.74">Consistent Line Clusters for Building Recognition in CBIR</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,440.61,201.09,51.18,8.74;9,90.00,212.61,139.01,8.74">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.34,224.07,456.83,8.74;9,90.00,235.59,299.83,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,203.94,224.07,157.40,8.74">Indoor-Outdoor Image Classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,115.25,235.59,259.30,8.74">Access of Image and Video Databases, in conjunction with ICCV</title>
		<imprint>
			<biblScope unit="page">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.34,247.11,423.42,8.74;9,90.00,258.57,410.35,8.74;9,90.00,270.09,160.02,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,212.40,247.11,291.36,8.74;9,90.00,258.57,26.15,8.74">Applying perceptual grouping to content-based image retrieval: Building images</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,137.64,258.57,305.40,8.74">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Fort Collins, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.35,281.05,455.05,9.36;9,90.00,292.51,442.60,9.31;9,90.00,304.03,145.59,9.31" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,228.55,281.05,306.85,9.31;9,90.00,292.51,96.20,9.31">Probabilistic Modeling of Local Appearance and Spatial Relationships for Object Recognition</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,198.06,292.51,312.56,9.31">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.34,316.11,453.28,8.74;9,90.00,327.57,449.12,8.74;9,90.00,339.09,274.93,8.74;9,97.62,720.39,320.59,8.74" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papernick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thornton</surname></persName>
		</author>
		<title level="m" coord="9,435.04,316.11,98.59,8.74;9,90.00,327.57,348.61,8.74;9,97.62,720.39,316.07,8.74">Video Retrieval with the Informedia Digital Video Library System. The 10th Text REtrieval Conference (TREC</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>Figure 5. The final result set can be reviewed and edited before submission</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
