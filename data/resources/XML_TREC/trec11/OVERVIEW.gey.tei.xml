<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.36,84.36,321.21,17.76">The TREC-2002 Arabic/English CLIR Track</title>
				<funder ref="#_zEeJ3bT #_tE6B2Js">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,267.40,119.19,77.26,12.18"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@glue.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Studies and Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.28,181.11,65.74,12.18"><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
							<email>gey@ucdata.berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="department">UC Data Archive &amp; Technical Assistance</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.36,84.36,321.21,17.76">The TREC-2002 Arabic/English CLIR Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BEC50754D49E1D495BFC4595FA21B1C3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nine teams participated in the TREC-2002 cross-language information retrieval track, which focused on retrieving Arabic language documents based on 50 topics that were originally prepared in English. Arabic translations of the topic descriptions were also made available to facilitate monolingual Arabic runs. This was the second year in which a large Arabic document collection was available. Three new teams joined the evaluation, and the cross-language aspect of the evaluation received more attention this year than in TREC-2001. A set of standard linguistic resources was made available to facilitate cross-system comparisons, and their use as a contrastive condition was encouraged. Unique contributions to the relevance pools were more typical of previous TREC evaluations then the results of TREC-2001 had been for the same document collection, with no run uniquely contributing more than 6% of the known relevant documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the 2002 Text Retrieval Conference (TREC-2002) Cross-Language Information Retrieval (CLIR) task was to develop evaluation methodologies and evaluation resources to assess the effectiveness of ranked retrieval techniques that accept English queries and search Arabic documents. Monolingual Arabic experiments, in which both the queries and the documents were in Arabic, were also supported. Standard French translations were also provided in TREC-2001, but that was not done this year because no team expressed interest in working with French in preference to English. TREC-2002 was the ninth year in which non-English document retrieval has been evaluated at TREC, and the sixth year in which cross-language information retrieval has been the principal focus of that work. For a summary of prior evaluations, readers are referred to <ref type="bibr" coords="1,253.12,530.67,11.79,12.18" target="#b0">[1]</ref>. In this paper we describe the task and the evaluation collection, we summarize the techniques used by each participating team and briefly describe their results, and we offer some guidelines for future use of the TREC Arabic collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>As in past CLIR evaluations, the principal task for each group was to automatically build queries from topic descriptions written in one language (English, in this case) and then use those queries as a basis for ranking documents written in another language (Arabic, in this case) in order of decreasing degree (or probability) of topical relevance. Each participating team was allowed to submit as many as five runs for official scoring. In order to foster comparability, teams submitting cross-language runs were required to submit at least one run in which only the title and description fields of the topic description were used. Evaluation then proceeded by pooling the top 100 documents for each topic from each of the 41 submitted runs, manual examination each document in the pool by a human judge (usually the creator of the topic), and recording binary (yes/no) topical relevance judgments for each document in each topic's judgment pool. Participating teams were also invited to perform additional "post-hoc" runs, scoring them locally using relevance judgments provided by NIST, if they wished to investigate more conditions than would be possible using only the five official runs. The top 1000 documents in the ranked list for each topic was evaluated using a suite of measures. In this paper, we report the mean (over 50 topics) of the precision at 11 levels of recall and the mean (again, over 50 topics) of the uninterpolated average precision. Additional statistics for each run can be found elsewhere in this proceedings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topics</head><p>For TREC-2002, fifty topic descriptions (numbered AR26-AR75) were created in English in a collaborative process between the LDC and NIST (for TREC-2001, only 25 topics had been created). An example from this year's topic set is: The Linguistic Data Consortium also prepared an Arabic translation of the topics, so participating teams also had the option of doing monolingual (Arabic-Arabic) retrieval. The same topic in Arabic was distributed as: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Documents</head><p>As in the TREC-2001 CLIR track, the document collection contained 383,872 newswire stories that appeared on the Agence Française de Presse (AFP) Arabic Newswire between 1994 and 2000.</p><p>The documents were represented in Unicode and encoded in UTF-8, resulting in an 896 MB collection. A typical document is shown in Figure <ref type="figure" coords="2,312.63,659.67,4.13,12.18" target="#fig_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relevance Judgments</head><p>The nine participating teams shown in Table <ref type="table" coords="3,298.60,366.75,5.50,12.18">1</ref> together produced 23 automatic cross-language runs, 17 automatic monolingual runs with Arabic queries, and one manual monolingual run. The total number of relevant documents found over 50 topics was 5,909 with a mean of 118 relevant documents per topic, a maximum of 523, and minimum of 10. In TREC-2001, such a large number of relevant documents were uniquely found by individual runs that there was some concern about the comparability of post-hoc and official runs. In the pooled relevance judgment methodology, most documents remain unjudged, and the usual procedure is to treat unjudged documents as if they are not relevant. Voorhees has shown that the preference order between automatic runs in the TREC ad hoc retrieval task would rarely be reversed by the addition of missing judgments, and that the relative reduction in mean uninterpolated average precision that would result from removing "uniques" (relevant documents found by only a single system) from the judgment pools is typically less than 5% <ref type="bibr" coords="3,289.24,505.83,11.79,12.18" target="#b1">[2]</ref>. In TREC-2001 CLIR collection, 9 of 28 judged automatic runs experienced a relative reduction in mean uninterpolated average precision exceeding 10% relative when "uniques" contributed by that run were removed from the judgment pool. As Figure <ref type="figure" coords="3,164.92,543.75,5.50,12.18">2</ref> shows, for the TREC-2002 CLIR collection, only one of 41 judged runs would experience more than 5% reduction. Figure <ref type="figure" coords="3,291.40,556.47,5.50,12.18">3</ref> shows the unique relevant documents contributed to the final relevance pool by each group; no team uniquely contributed more than 6% of the known relevant documents (with the largest contribution coming from the one team that submitted a manual run). These results suggest that post-hoc use of the TREC-2002 collection will result in meaningful comparisons with the set of judged runs reported in this proceedings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Standard Resources</head><p>Cross-language retrieval effectiveness depends on both the design of the retrieval system and the quality of the linguistic resources that are used. In order to begin to tease apart these factors, each participating team that performed the CLIR task was invited to submit one title+description run in which standard linguistic resources were used to the extent practical given their design. For example, a team using dictionary-based query translation could submit one run in which a standard bilingual dictionary was the only dictionary used. The following standard resources were made available:</p><p>• An Arabic "light" stemmer that used truncation rules to removed a small set of prefixes and suffixes. The light stemmer was developed through collaboration between Kareem Darwish at the University of Maryland and Leah Larkey at the University of Massachusetts. • A bidirectional Arabic-English bilingual dictionary, rekeyed from Salmone's Advanced Learner's Arabic-English Dictionary. The dictionary was provided by David Smith, of Tufts University. • Two tables of translation probabilities, one for English-to-Arabic and the other for Arabic-to-English. These tables were developed using the Giza++ implementation of IBM Model 1 to align Arabic stems produced the track's standard light stemmer with English stems produced using the Porter stemmer. The documents on which this alignment was performed were obtained from the United Nations by Jinxi Xu at BBN and the alignments were produced by Alex Fraser of USC-ISI while working at BBN. • A Web-based bidirectional Arabic-English machine translation system. We designated the system available at http://tarjim.ajeeb.com as the standard resource for the track. Comparing the standard resource runs with the best runs by site (for the same query length) suggests that these resources were generally near, but not at, the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Retrieval Approaches</head><p>The nine groups have written papers about their methods and experiments. Three themes emerge across the reported work: (1) A greater focus on exploring innovative CLIR techniques than was evident in TREC-2001, (2) continued investigation of Arabic-specific issues, such as stemming and stopword removal, and (3) increasing reliance on multiple sources of evidence to overcome the limitation of any single source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BBN</head><p>BBN made use of its probabilistic translation and retrieval model used in TREC-9 (for Chinese) and TREC-2002 (for Arabic) as well as the United Nations corpus. They employed the method of Hiemstra and de Jong to compute English probabilities by projecting Arabic terms to English (a weighted sum of corpus probabilities of Arabic terms where the weights are translation probabilities). To process the UN corpus and generate a new bilingual translation lexicon, BBN utilized the IBM model 4 statistical translation approach, rather than IBM model 1. For stemming they made use of the University of Massachusetts (Light8) stemmer in addition to the Buckwalter stemmer used in TREC-2001. In the area of query expansion they performed English and Arabic query expansions independently rather than sequentially (English query expansion followed by Arabic query expansion) as in TREC-2001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">University of California at Berkeley</head><p>The main focus of Berkeley's work was to develop several approaches to Arabic stemming and stopword list generation. Berkeley used the Ajeeb machine translation system to translate every word in the AFP collection after minimal word normalization. A 3,447-entry Arabic stopword list was created as the set of all Arabic terms that translated to an English stopword. A similar approach was used to generate a stemmer -Arabic words were partitioned into clusters based on their English translations, with Arabic words whose English translations were conflated to the same English stem forming a cluster. A second stemmer in which common one, two, and three character prefixes and suffixes (identified in the AFP corpus) were removed was also tried. These resources were then used in various combinations to perform dictionary-based retrieval using an extension of the logistic regression technique from previous TREC evaluations that incorporated blind relevance feedback. Berkeley also submitted a merged run in which two machine translation systems (Ajeeb and Al-Misbar) were used to perform query translation directly and the Salmone dictionary was used to perform dictionary-based query translation. Each was Score-based merging was then used to produce the final ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hummingbird Technologies</head><p>Hummingbird Technologies chose to focus on monolingual Arabic retrieval again this year. Hummingbird used a minimalist approach, with the same set of stemming rules as last year.</p><p>Hummingbird makes a unique contribution to the CLIR track by evaluating commercially available technology that has been integrated into a comprehensive document management system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Illinois Institute of Technology</head><p>In their TREC-2002 experiments with Arabic CLIR, Illinois Institute of Technology (IIT) continued their investigations of improvement of monolingual Arabic retrieval using different stemming approaches. For TREC-2001 IIT developed a 'light stemming' approach that performed well. For TREC-2002, they developed two new stemmers, one rule-based and one based on pattern matching. Both stemmers used an external training corpus from 1999-2001 editions of two Saudi Arabian newspapers to identify frequent prefixes and suffixes. Both stemmers performed comparably in monolingual Arabic retrieval, and the paper contains detailed examples of how each approach to stemming can affect word meaning. For CLIR, IIT used the Ajeeb machine translation package to translate the English queries into Arabic, and they also tried a second approach based on the translation probability tables provided as part of the standard resource set. In this case, MT produced better results, perhaps because the IIT stemmers differed from those used to produce the translation probability tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">IBM Research</head><p>IBM's cross-language experiments were based entirely upon statistical machine translation using the IBM model 1 approach. IBM used the UN parallel corpus provided by BBN with a newly developed Arabic morphological analyzer. Two statistical machine translation systems were built. The first, an Arabic-to-English sentence translation model, was used to translate the documents into English, followed by monolingual English retrieval. The second, a probabilistic convolution model in which the probability of generating an English query stem was modeled based on the probabilities of generating that stem from an Arabic word or morpheme observed in the document. The convolution model substantially outperformed the sentence translation model, perhaps because it made use of document-wide translation probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Johns Hopkins University -Applied Physics Laboratory (JHU-APL)</head><p>JHU-APL continued its investigation of the use of overlapping character n-grams for monolingual Arabic retrieval. In TREC-2001 they used 4-grams; for TREC-2002 they investigated the use of 3-grams, 4-grams and 5-grams, and their official monolingual runs used combinations of those ngram lengths. They used the same two machine translation systems as the Berkeley team, performing query translation from English to Arabic. The use of stemming in the standard translation probability tables made that resource unsuitable for straightforward use with n-grambased techniques. They did, however, try mapping all English words that share a common stem to all Arabic words that share a common stem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">University of Maryland</head><p>The main focus of this year's experiments at Maryland was on combination-of-evidence techniques for cross-language retrieval. Translation knowledge was obtained from the same two machine translation systems that Berkeley used, the Salmone bilingual dictionary, and both directions of the BBN translation probability tables. Translation probabilities were estimated based on all of this evidence and then used with five CLIR techniques, one of which was a previously developed baseline (Pirkola's method). Some side experiments were also done to investigate the potential of document expansion and variants of light stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">University of Massachusetts</head><p>University of Massachusetts tried the most extensive set of techniques. Generally, acronyms found in the query were expanded using the U Mass Acrophile system. A bilingual lexicon built from a proper name dictionary from New Mexico State University and the same two MT systems that Berkeley used was then used to translate expanded English query terms and add them to the lexicon. Query expansion was performed both before and after translation. Arabic stopword removal was performed after morphological normalization using a 168-term stopword list from University of Lancaster, and several alternative forms of morphological normalization were tried.</p><p>A number of variants on the processing path and the retrieval model were tried, and score-based merging among these variants was then used to create the final submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">University of Neuchatel</head><p>University of Neuchatel only submitted monolingual Arabic runs to the TREC-2002 CLIR Track evaluation. Neuchatel took the interesting approach of independently indexing Arabic words and of indexing tri-grams as alternative indexing and retrieval scheme, following the approach of Darwish and Oard in their SIGIR-2002 paper. Prior to all indexing and retrieval, Neuchatel converted and normalized the Arabic document text into Latin letters ("In Malta, the Arabic language is written using the Latin alphabet"). The Neuchatel word approach developed two stemmers which fall into the same area of 'light stemming' of the standard resource. They then dropped all stopwords which appeared in their 347 Arabic stopword list. From the paper it appears that their tri-gram approach utilized words as a basis (rather than including white space as other n-gram approaches usually do) and removed frequent tri-grams from a tri-gram stoplist generated from the stopword list above. Following independent retrieval of word-based indexed documents and tri-gram-based indexed documents, a "data fusion" summing approach to independently generated RSV rankings was applied to generate the final ranked list. They utilized the Rocchio approach to blind feedback and performed multiple relevance-expansion experiments on both stems and tri-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Monolingual runs establish a useful baseline to which cross-language results can be compared, and they help to enrich the relevance judgment pools. No standard query length was required for monolingual runs, but seven of the eight teams submitting monolingual runs elected to use title+description queries for at least one monolingual run. Figure <ref type="figure" coords="7,433.48,490.23,5.50,12.18" target="#fig_3">4</ref> shows the best title+description monolingual run for those seven teams. The best of these results is somewhat below the best that was achieved last year. Since the document collection is the same, this suggests that the topics this year may be somewhat harder on average than last year's topics were. Figure <ref type="figure" coords="8,121.84,316.11,5.50,12.18" target="#fig_4">5</ref> shows the best automatic cross-language run for the required title+description condition for the seven teams that submitted at least one cross-language run. Five teams ran title+description queries for both the monolingual and the cross-language conditions, with the cross-language retrieval exhibiting a relative effectiveness ranging from 29% relative below monolingual to 6% relative above monolingual. It is difficult to draw any conclusions in the face of such large variability; interpretation of these results will require close attention to the implementation details of individual systems. Five teams submitted standard resource runs. As Figure <ref type="figure" coords="8,359.68,651.63,5.50,12.18" target="#fig_5">6</ref> shows, three teams demonstrated improved performance (ranging from 4% to 11% relative) through the use of additional linguistic resources. Only five runs could be scored officially for each participating team, so post-hoc analysis may reveal greater potential for improvement from the use of additional linguistic resources than can be seen in the official results. As is common in information retrieval evaluations, substantial variation was observed in retrieval effectiveness on a topic-by-topic basis. Figure <ref type="figure" coords="9,300.28,275.43,4.13,12.18" target="#fig_6">7</ref>, which plots the median and maximum average precision over the 23 cross-language runs illustrates this (note, however, that this plot includes queries of different lengths). For example, half of the runs did poorly on topics 34 and 55, but at least one run exceeded the median average precision for those topics by 800% relative. Applying this type of analysis on a run-by-run basis can help to identify effects that would be masked by averages across topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Looking to the Future</head><p>The TREC evaluations produce three things of enduring value: (1) research results, (2) standard test collections on which new techniques can be evaluated, and (3) a research community with shared interests. Over the past nine years, TREC has produced eight non-English test collections in six languages (Arabic, Chinese, French, German, Italian, and Spanish), and the demonstrated utility of these collections has inspired the creation of similar collections for many other languages (including Dutch, Japanese, Korean and Finnish). The results obtained this year indicate that topics AR26-AR75 are suitable for post-hoc use of the collection by automatic systems that did not contribute to the relevance pools. Topics AR1-AR25 have proven to be of some use for system tuning, but the relatively long title fields and the markedly elevated "uniques" effect make use of that collection for comparative studies less advisable. We therefore recommend that researchers working with this collection in the future report results for the 50 topics developed this year rather than treating all 75 topics as a single collection.</p><p>Our community now includes hundreds of researchers working on CLIR in dozens of countries, and research results regularly appear in a broad array of venues. Although this is the last year of the CLIR track at TREC, similar evaluations will continue in Europe at CLEF <ref type="bibr" coords="10,449.08,166.59,12.89,12.18" target="#b2">[3]</ref> and Japan at NTCIR <ref type="bibr" coords="10,127.36,179.19,11.79,12.18" target="#b3">[4]</ref>, and work on searching Arabic will continue in the Topic Detection and Tracking evaluations (TDT) <ref type="bibr" coords="10,175.84,191.91,11.79,12.18" target="#b4">[5]</ref>. The idea of providing standard resources was first tried at TDT, and we have found it to be useful in a more traditional CLIR evaluation design as well. Some of the questions that we have explored in the CLIR track may ultimately migrate into other tracks at TREC. CLIR is, after all, just one capability among the many that are needed to build effective systems to access globally distributed information. Perhaps the future will see the creation of a multilingual Web track, a track for searching multilingual speech, or an interactive multilingual track. When that happens, the baseline technology from which those specialized applications will be built will have been first developed here, in the TREC CLIR track.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,108.04,257.18,29.96,11.24;2,108.04,267.26,137.95,11.24;2,108.04,277.22,221.96,11.24;2,108.04,287.18,113.96,11.24;2,108.04,297.26,413.96,11.24;2,108.04,307.22,203.96,11.24;2,108.04,323.18,101.96,11.24;2,108.04,339.26,414.08,11.24;2,108.04,349.22,414.08,11.24;2,108.04,359.18,414.08,11.24;2,108.04,369.26,191.96,11.24;2,108.04,385.22,35.96,11.24"><head></head><label></label><figDesc>National Council of Resistance relate to the potential independence of Kurdistan? &lt;/desc&gt; &lt;narr&gt; Narrative: Articles reporting activities of the National Council of Resistance are considered on topic. Articles discussing Ocalan's leadership within the context of the Kurdish efforts toward independence are also considered on topic.&lt;/narr&gt; &lt;/top&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,90.04,324.51,264.08,10.99;3,90.24,72.00,468.00,246.24"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A sample Arabic document from the AFP collection.</figDesc><graphic coords="3,90.24,72.00,468.00,246.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,90.04,297.39,388.16,10.99"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. Effect of removing relevant documents found uniquely by each of 41 official runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,90.04,298.95,293.38,10.99"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Best automatic monolingual runs, title+description queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,90.04,634.47,305.87,10.99"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Best automatic cross-language runs, title+description queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,90.04,245.67,296.99,10.99"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Improvement obtained using additional linguistic resources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,90.04,584.19,372.68,10.99"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Average precision by topic (bottom=median, top=maximum), cross-language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.04,445.59,424.52,135.43"><head>‫‪‬آ‪‬اد؟‬ ‫ا‬ ‫ل‬‫ا‬ ‫ا‬ ‫ا‬ ‫و‬‫ا‬   ‫آ‬ &lt;/desc&gt; &lt;narr&gt; Narrative: ‫ا‪‬آ‪‬ا‬ ‫د‬  ‫او‪‬ن‬ ‫دة‬  ‫ث‬ ‫ت‬ ، ‫اﻝ‬ ‫و‬‫اﻝ‬  ‫ت‬‫آ‬  ‫ص‬  ‫ع‬‫اﻝ‬ ‫ل‬‫ﻝ‬ ‫د‬ .</head><label></label><figDesc></figDesc><table coords="2,90.04,445.59,177.32,135.43"><row><cell>&lt;top&gt;</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">&lt;num&gt; Number: AR26 &lt;/num&gt;</cell><cell></cell></row><row><cell>&lt;title&gt;</cell><cell>‫د‬‫اﻝ‬ ‫اﻝ‬ ‫و‬‫اﻝ‬</cell><cell></cell><cell>&lt;/title&gt;</cell></row><row><cell cols="2">&lt;desc&gt; Description:</cell><cell></cell><cell></cell></row><row><cell>&lt;/narr&gt;</cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;/top&gt;</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors are grateful to <rs type="person">Ellen Voorhees</rs> for handling all of the logistics for this track at <rs type="institution">NIST</rs> and for providing the data that was the basis for our analysis, to the LDC for their work on topic development and relevance assessment, to the contributors of the standard resources for making that comparison possible, and to the participating research teams for their advice and insights along the way. This work was supported in part by <rs type="funder">DARPA</rs> cooperative agreements <rs type="grantNumber">N660010028910</rs> and <rs type="grantNumber">N660010018911</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zEeJ3bT">
					<idno type="grant-number">N660010028910</idno>
				</org>
				<org type="funding" xml:id="_tE6B2Js">
					<idno type="grant-number">N660010018911</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,106.36,437.19,415.62,12.18;10,90.04,449.79,330.93,12.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,283.48,437.19,238.50,12.18;10,90.04,449.79,20.49,12.18">The TREC-2001 cross-language information retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,135.16,449.79,223.50,12.18">Proceedings of the 2001 Text Retrieval Conference</title>
		<meeting>the 2001 Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.15,468.51,413.82,12.18;10,90.04,481.11,432.19,12.18;10,90.04,493.83,11.01,12.18;10,101.08,492.42,4.69,7.68;10,109.24,493.83,412.88,12.18;10,90.04,506.43,233.51,12.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,204.28,468.51,317.69,12.18;10,90.04,481.11,55.82,12.18">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,438.52,481.11,83.71,12.18;10,90.04,493.83,11.01,12.18;10,101.08,492.42,4.69,7.68;10,109.24,493.83,412.88,12.18;10,90.04,506.43,38.87,12.18">Proceedings of the 21 st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</editor>
		<meeting>the 21 st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.36,525.03,415.78,12.18;10,90.04,537.75,432.06,12.18;10,90.04,550.35,431.99,12.18;10,90.04,562.95,56.99,12.18" xml:id="b2">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,442.00,525.03,80.14,12.18;10,90.04,537.75,432.06,12.18;10,90.04,550.35,144.45,12.18">Evaluation of 002 Cross-Language System Information systems: Third Workshop of the Cross-Language Evaluation Forum, CLEF-2002</title>
		<title level="s" coord="10,314.20,550.35,203.09,12.18">Springer Computer Science Lecture Notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.60,581.67,415.37,12.18;10,90.04,594.27,431.84,12.18;10,90.04,606.99,431.87,12.18;10,90.04,619.59,98.62,12.18" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Keizo</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emi</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
		<title level="m" coord="10,357.16,581.67,164.81,12.18;10,90.04,594.27,431.84,12.18;10,90.04,606.99,116.74,12.18">NTCIR Workshop 3: Proceedings of the Third NTCIR Workshop on Research Information Retrieval, Automatic Text Summarization and Question Answering</title>
		<imprint>
			<date type="published" when="2001">September2001-October2002. January, 2003</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Informatics, Tokyo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.40,638.19,413.51,12.18;10,90.04,650.91,195.93,12.18" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,194.79,638.19,322.49,12.18">Topic Detection and Tracking: Event-based Information Organization</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
