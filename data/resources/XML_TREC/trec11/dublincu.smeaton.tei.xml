<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.84,62.17,416.60,15.67">Dublin City University Video Track Experiments for TREC 2002</title>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,98.08,92.88,58.07,12.22"><forename type="first">Paul</forename><surname>Browne</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,164.57,92.88,65.43,12.22"><forename type="first">Csaba</forename><surname>Czirjek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.52,92.88,65.73,12.22"><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.20,92.88,65.55,12.22"><forename type="first">Roman</forename><surname>Jarina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.15,92.88,56.80,12.22"><forename type="first">Hyowon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,448.66,92.88,59.65,12.22;1,113.44,105.60,33.05,12.22"><roleName>Kieran</roleName><forename type="first">Seán</forename><surname>Marlow</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,149.13,105.60,49.79,12.22"><forename type="first">Mc</forename><surname>Donald</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.97,105.60,60.10,12.22"><forename type="first">Noel</forename><surname>Murphy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.82,105.60,82.51,12.22"><forename type="first">Noel</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,366.66,105.60,75.90,12.22"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
							<email>alan.smeaton@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,450.78,105.60,48.07,12.22"><forename type="first">Jiamin</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.84,62.17,416.60,15.67">Dublin City University Video Track Experiments for TREC 2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DBD4B34573783D8EB452F77E8DC662B7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dublin City University participated in the Feature Extraction task and the Search task of the TREC-2002 Video  Track.  In the Feature Extraction task, we submitted 3 features: Face, Speech, and Music. In the Search task, we developed an interactive video retrieval system, which incorporated the 40 hours of the video search test collection and supported user searching using our own feature extraction data along with the donated feature data and ASR transcript from other Video Track groups. This video retrieval system allows a user to specify a query based on the 10 features and ASR transcript, and the query result is a ranked list of videos that can be further browsed at the shot level. To evaluate the usefulness of the feature-based query, we have developed a second system interface that provides only ASR transcript-based querying, and we conducted an experiment with 12 test users to compare these 2 systems. Results were submitted to NIST and we are currently conducting further analysis of user performance with these 2 systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This year Dublin City University took part in two of the three tasks in the Video Track: Feature extraction task and Search task. In Section 2 we present the Feature extraction task that we conducted (Speech, Instrumental Sound and Face), the methods used for each feature, and our results. In Section 3 we present the Search task -specifically the interactive video retrieval system that we developed for the task and the experiment procedures and our results. The system is a variation of the Físchlár Digital Video System with an XML-based architecture that uses an MPEG-7 compliant video description. The system provides a web-based user interface from which a user can compose a query based on all the 10 features and the ASR transcript text. We used the system in the interactive search task with 12 test users who each conducted searches for the 25 topics provided by the Video Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Feature Extraction Task</head><p>Ten features were listed for the feature extraction task and we extracted three of the features ourselves: Speech, Instrumental Sound and Face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Speech Extraction</head><p>The task here is to recognise a shot as having a human voice uttering words. In our approach, speech characteristics were derived from the volume (energy) contour of the frequency-limited audio signal. There are some properties that distinguish speech from other signals. Roughly, speech exhibits an alternating sequence of 3 kinds of sounds that have different acoustic properties: i) vowels and vowel-like sounds -longer tonal quasi-periodic segments with high energy, which is concentrated in lower frequencies; ii) fricative consonants -noise-like short segments with lower volume and spectral energy distributed more toward the high frequencies; iii) stop consonants -short silent segments followed by a very short transition noise pulse. These three kinds of sounds alternate and form the regular syllabic structure of speech and therefore strong temporal variations in the amplitude of speech signals can be observed.</p><p>Our speech detector does not use an audio signal waveform as the input data, rather it utilises information taken directly from the MPEG-1 audio encoded bitstream. Thus a time-consuming decoding process is not required, and in addition information from audio signal analysis (e.g. subband filtering, volume estimation) already stored in the MPEG encoded bitstream, is utilized. The MPEG audio layer-II frame consists of 1,152 samples: 3 groups of 12 samples from each of 32 subbands. A group of 12 samples in each subband gets a bit allocation and, if this is not zero, a scalefactor. Scalefactors are weights that rescale samples so that they fully use the range of the quantizer. The encoder uses a different scalefactor for each of the three groups of 12 samples only if necessary. By definition the scalefactors carry information about the maximum level of the signal in each subband. Thus, the volume contour of the overall audio signal can be estimated by the summation of the scalefactors over all subbands. In the case of a frequency-limited signal, this summation is done only over given subbands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Procedure for Speech Extraction</head><p>Our approach was based on the measurement of the duration and the rate of the energy peaks of the audio signal. The method was first introduced in <ref type="bibr" coords="2,195.41,78.25,11.69,11.03" target="#b0">[1]</ref> where theoretical background and results of preliminary studies can be found. For the TREC task, the method had to be slightly modified. By trial examination of various parts of video recordings from the TREC Feature Development Collection, it was decided that at least 7 of the low frequency subbands must be included in the processing. In some cases, the first subband (frequencies up to 0.7 kHz) was excluded from analysis. The procedure of signal analysis and processing for speech detection is described below.</p><p>Each video file was demultiplexed, and the MPEG-1 audio layer II bitstreams were stored in separate files (MP2 files). Then, only the scalefactors of the first 7 subbands were extracted from the MP2 files. First, silence detection was carried out. An energy level of the signal was determined by the superposition of all relevant scalefactors. The frames in which the level was below the threshold, were assigned as silent frames.</p><p>In the case of speech detection, the envelope of the band-limited signal was estimated by summing relevant scalefactors from the 2 nd to the 7 th subbands only. This procedure was followed by a 5 th order median filtering to avoid rapid random changes in the amplitude. For analysis, a sliding window was used with a window length of 3.9 seconds and a 1.3 second shift (i.e. 2/3 overlap). Energy peaks were extracted by a simple thresholding procedure. Two low-level features were chosen for speech detection. These are: i) Lm -the duration of the widest peak within the analysis window (segment); ii) R -the rate of peaks (number of peaks in the analysis window). Each segment was assigned to speech or nonspeech by using a simple rule-based decision procedure. This process has been discussed in greater detail in <ref type="bibr" coords="2,522.69,273.84,10.64,11.03" target="#b0">[1]</ref>.</p><p>All the MP2 audio frames corresponding to an analysed segment were given a relevance value of '1' in the case of a speech segment, and the value '0' otherwise. The silent parts of the signal longer than 1.5 seconds were assigned as non-speech signal.</p><p>Final speech feature measures for the standard video shots were determined by averaging the relevance values over all the audio frames within each video shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Evaluation and Test Results</head><p>For evaluation, we submitted the top 1,000 standard video shots ranked according to the highest possibility of detecting the speech feature. The results of our runs are summarised in Table <ref type="table" coords="2,381.62,383.05,3.75,11.03" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instrumental Sound Extraction</head><p>This feature characterises a sound produced by one or more musical instruments. Henceforth this feature is referred to as the music feature. The music feature detection task is a much more challenging task than the speech detection task. Unlike speech, musical sounds are very difficult to define due to their great variety and uncertain nature. However, musical signals have some unique characteristics, which may help to discriminate them from other sounds. Music tends to be composed of a multiplicity of tones, each with its own distribution of higher harmonics. The energy contour has usually a much smaller number of "peaks" and "valleys" and it shows either very little change over a period of several seconds (e.g. classical music) or strong long term periodicity due to exact rhythm (e.g. dance music).</p><p>For this TREC task we developed a method that is an extended version of the method we already used for speech detection (see section 2.1). Two other low-level features were incorporated into the system to improve discrimination between musical sounds and other environmental sounds. They are: rhythm and harmonicity. We believe that most of the sounds produced by instrumental music have harmonic structure of spectra unlike noise-like environmental sounds. The importance of rhythm detection has been recently discussed in <ref type="bibr" coords="2,373.68,671.16,10.64,11.03" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Procedure for Instrumental Sound Extraction</head><p>The first two features Lm and R (duration and rate of the energy peaks) were computed in the same way as in section 2.1.1. In addition, the rhythm (or pulse) metric Pm and harmonic ratio H were computed.</p><p>Similar to <ref type="bibr" coords="3,131.24,61.45,10.64,11.03" target="#b1">[2]</ref>, the rhythm metric is expressed by the following procedure.</p><p>For each of the first 7 subbands, the normalised autocorrelation function R were computed.</p><formula xml:id="formula_0" coords="3,205.96,90.85,192.72,22.02">) 0 ( ) ( ) ( k k k R t R t R = , ∑ + ⋅ = n k k k t n e n e t R ), ( ) ( ) (</formula><p>where e k (n) is the subband energy contour (or envelope) in the k-th subband, without its DC component. The subband energies were estimated directly from the scalefactors of the MPEG-1 layer II bitsream. For rhythm analysis, a sliding window with a 4/5 overlap was used. We searched R within the analysed window over the interval corresponding to time t = 0.2 -1.75 seconds to find peaks. We set p(j) to the value of the highest peak in the j-th subband. Then we defined the feature rhythm metric P m as</p><formula xml:id="formula_1" coords="3,233.32,172.21,165.18,13.71">{ } 7 ,... 2 , 1 , ) ( max = = k k p P m , 0&lt;P m &lt;1</formula><p>The higher the value of P m , the greater amount of rhythmicality in the signal.</p><p>The harmonicity ratio defines the degree of harmonicity of an audio signal. We computed it in accordance with the MPEG-7 description schema <ref type="bibr" coords="3,221.65,221.77,10.64,11.03" target="#b2">[3]</ref>. By definition, the harmonicity ratio is the ratio of harmonic power to total power. It was computed by the following procedure:</p><p>At first, comb filtering is applied</p><formula xml:id="formula_2" coords="3,205.96,269.33,217.22,35.15">5 . 0 2 2 ) ( * ) ( ) ( ) ( ) (         - - = ∑ ∑ ∑ j j j k j s j s k j s j s k r</formula><p>where s is the sequence of PCM samples of the band-limited signal. Only the 2 nd subband was used for the computation. Thus the sampling frequency was f 2 =44.1kHz / 32. Index k was changed up to the value corresponding to the maximum expected fundamental period (around 20 ms). The Harmonicity ratio H was determined as the maximum value of r(k) for each frame. H = 1 for a purely periodic signal, and it will be close to 0 for white noise. These four low-level features Lm, R, Pm and H were used as inputs for a heuristic rule-based classifier. The relevance for each analysed segment was computed as a weighted sum of these features. The weights and thresholds for the classifier were determined by trial and error examination of various parts of video recordings from the TREC Feature Development Collection. Similarly as in the case of speech detection, silence detection was performed. For the silent parts, which were longer than 1.5 seconds, the music relevance was set to zero. Final music/instrumental sound feature measures for the standard video shots were determined by averaging the relevance scores over all the audio frames corresponding to the given video shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Evaluation and Test Results</head><p>For evaluation, we submitted the top 300 standard video shots ranked according to the highest possibility of detecting the speech feature. The results are summarised in Table <ref type="table" coords="3,335.22,482.77,3.75,11.03" target="#tab_1">2</ref>. We reach the highest precision at 100 results among TREC participants, but the precision at 1,000 results and the average precision are very low because we submitted only 300 results for evaluation. Since we identified much more relevant shots than we submitted for judgement, we have re-calculated precision and average precision for our 1,000 top ranked shots and these unofficial results are also shown marked with * in Table <ref type="table" coords="3,257.47,528.72,3.71,11.03" target="#tab_1">2</ref>. The unofficial average precision is 0.494 which shows that our method performs very well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Face Extraction</head><p>As presented in <ref type="bibr" coords="3,135.55,686.53,10.64,11.03">[4]</ref>, the colour of human skin falls into a relatively narrow band of the colour space. Many colour models have been used in pre-processing the input image, in order to locate potential human presence. We know <ref type="bibr" coords="3,523.47,698.05,11.67,11.03" target="#b5">[5]</ref> that normalised RGB, YUV, HSV, CIEL etc. can be used for this purpose. In this task, we decided to detect skin-like pixels using a similar approach to <ref type="bibr" coords="4,208.25,50.05,10.56,11.03" target="#b6">[6]</ref>, updating the filtering technique based on the available Feature Development Collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Procedure for Face Extraction</head><p>Due to the binary nature of classification, the output skin-mask will be populated with isolated skin-like pixels, i.e. noise. In order to address this undesirable effect, we applied a morphological open-close filtering. After this operation, we expect to obtain homogeneous areas of connected pixels. Having the skin-map, we want to group together connected pixel areas into regions. Therefore a connected component labelling was performed, which gave the number of regions used in further processing. Even applying morphological filtering to the skin-map, regions with a small number of pixels may occur. To reduce the number of false candidate regions, areas with the number of pixels less than N = 625 were ignored. We have chosen this threshold based on the assumption that no face could be detected by this method having a size smaller than 25x25 pixels. Horizontal and vertical strips, which are less likely to contain a human face, were also ignored. These regions were detected by having a huge difference between width and height, with the requirement that the smaller dimension does not exceed 25 pixels.</p><p>Assuming that the human face has an approximately elliptical shape, for each connected component (region) the best-fit ellipse was calculated based on moments <ref type="bibr" coords="4,268.61,239.76,10.63,11.03" target="#b7">[7]</ref>. Unfortunately, many other objects in a visual scene have the same colour characteristics as the human skin, or other object(s) are merged with the face (i.e. hands, background wall, etc.</p><p>). An intermediate step in the processing chain consists of an iterative partitioning of regions having "irregular" shape. This means breaking a region S into component convex sub-regions Sn, n being the number of sub-regions, by applying K-means clustering.</p><p>The detection task is based on principal component analysis of the remaining skin patches. Given a collection of test images, we constructed a face space for discriminating the remaining candidate regions. The measure of "faceness" of the input sample relies on the reconstruction error, expressed as the difference between the input image and its reconstruction using only the M eigenvectors corresponding to the highest eigenvalues:</p><formula xml:id="formula_3" coords="4,256.84,368.66,99.70,14.64">2 2 || || x x - = ε (DFFS)</formula><p>The distance from face space (DFFS) indicates how well the test image can be approximated by the most significant eigenvectors spanning the eigenspace. The distance between the projected input image and the mean face image in the feature space is given by the norm of the principal component vector. Since the variance of a principal component vector y I is given by its associated eigenvalue λ i , the squared Mahalanobis distance measure d 2 gives a measure of the difference between the projection of the test image and the mean face image of the training set {x}:</p><formula xml:id="formula_4" coords="4,274.84,456.22,59.31,32.62">∑ = = M i i i y d 1 2 λ</formula><p>where y I are the projection coefficients and λ i are the associated eigenvalues. Therefore d 2 can be expressed as the distance in face space (DIFS). Given these two distances a combined error criterion was used:</p><formula xml:id="formula_5" coords="4,274.96,515.06,63.81,14.75">2 2 ε c d e + = .</formula><p>where e∈[0,1], and c is a suitable constant value. As we work with digital video, a confidence measure is attached to each continuous video shot, meaning the level of certainty that a face occurs. Because of time constraints, the above algorithm processes each 10-th frame in sequence. The confidence measure for a shot is expressed as the average confidence value of each processed frame within the shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Evaluation and Test Results</head><p>We submitted for evaluation the highest ranked 300 shots and our results are summarised in Table <ref type="table" coords="4,466.51,605.29,3.76,11.03" target="#tab_2">3</ref>. Within the first 100 shots, precision was 0.53 which was similar to the median. Our precision at 1000 is low due to the fact that we only submitted the top 300 results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Interactive Search Task</head><p>For the Search task, we conducted an interactive search experiment with test users. For this we developed an interactive video searching/browsing system which is a variation of our Físchlár system, and conducted a lab experiment using the system with 12 test users. The hypothesis we were testing was that ASR + features searching outperforms ASR-only searching in our controlled environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Description</head><p>The system we used for the search task is a variation of the Físchlár Digital Video System <ref type="bibr" coords="5,433.43,143.89,10.63,11.03" target="#b8">[8]</ref>, an online video system which has been operational for 3 years within the University campus and which we used for the interactive search task in the previous year's Video Track where we compared 3 different keyframe browsers. Currently the Físchlár system has a XML-based architecture and uses MPEG-7 compliant video description internally. While having the same underlying architecture as the Físchlár system, the system we tailored for this year's search task is more sophisticated in its search mechanism and user interface, as it provides various query methods for users based on the feature extraction data, some of which is our own (Face, Speech, Music) as well as donated features namely Indoor, Outdoor, People, Landscape and Text Overlay from IBM, and Monologue and Cityscape from Microsoft Research Asia. The system also allows the users to execute text queries over the test collection, based on the donated Automatic Speech Recognition (ASR) transcript provided by LIMSI. This transcript used an American English broadcast news transcription system and is described in <ref type="bibr" coords="5,330.90,258.84,15.42,11.03" target="#b11">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">System Architecture</head><p>Figure <ref type="figure" coords="5,99.54,299.17,4.98,11.03" target="#fig_0">1</ref> shows the components of the Físchlár system. The system uses an internal XML description as its core element (in the centre of Figure <ref type="figure" coords="5,199.86,310.69,3.61,11.03" target="#fig_0">1</ref>). When a user submits a query via the web-based interface, the web application processes it and sends the query detail to the logic element governing the search engine (see Section 3.1.2). The search engine sends back the retrieved results with relevance scores to the XML generator, which generates the necessary XML descriptions dynamically, to be transformed by appropriate XSL stylesheets to render HTML and SVG for display back on the user's web browser.</p><p>The queries that a user generates can be composed of any, some, or all of the following elements: Feature listing of the required features, there were ten features in all (excluding ASR transcript) and the user could select any of these features for inclusion in the query. However, there were some interface restrictions placed on users, for example, a user could not specify in a query that shots be both Indoor and Outdoor. Query text, which would be matched against the ASR transcript. While the system supported querying based on features alone, our findings indicated that all users relied on ASR text when constructing queries.</p><p>An identifier of the video within which to search, if the query was at the shot level. Our system supported both searching for videos and searching for shots within a particular video, hence the support for specifying a shot identifier within certain queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Retrieval and Weighting Scheme</head><p>In order to support search and retrieval over the video data we developed a search server, which was designed to support both ASR-only querying and ASR + feature querying for both the shots and the videos as a whole (the lower half of Figure <ref type="figure" coords="5,128.41,534.97,3.61,11.03" target="#fig_0">1</ref>). Each user's search session is essentially a two-phase process. The first phase was to generate a ranked list of videos in response to a user query, where each of the 176 videos were scored and ranked before being returned in decreasing rank order to the user. The user could then select one of the videos (usually one of the higher ranked) for shot-level examination, which was the second phase. Shot-level examination results in the search server producing a ranked listing of shots from within the selected video that match the user's query, the same query that originally generated the ranked list of videos. Our ranking technique was developed without using the TREC topics (no training data) and thus it was not developed specifically to provide high retrieval performance on this particular corpus and associated queries.</p><p>The ASR transcripts for each shot (donated by LIMSI) were pre-processed to remove stopwords and then stemmed using Porter's algorithm. When a user submits search term(s) as part of a query, these search terms undergo the same process. Each shot was represented by the ASR transcript text associated with the particular shot while each video was represented by the combination of all ASR text associated with all the shots that comprise the particular video. This required the utilisation of two conventional (text-only) search engines based on BM25 with the following parameter values; advl = 900, b = 0.75, k1 = 1.2 and k3 = 1000 which were set according to the best performance achieved on the WT2g collection from TREC-8 <ref type="bibr" coords="5,317.48,707.40,10.65,11.03" target="#b9">[9]</ref>. The scores for each query were normalised to be in the range [0..1] to allow for easier combination with the feature scores. We note that for any additional experimentation it would be advantageous to tune BM25 parameters to best-fit ASR content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.1">Search and Retrieval of Video Units</head><p>Recall that the first phase of a user's search was to generate a ranked list of videos in response to the query. Each video is represented by an overall feature weight for each of the 10 features, which was generated by calculating the aggregate scores for each feature from each shot within that video and then dividing these aggregate scores by the total number of shots in the video.</p><p>Without having carried out a sampling of the accuracy of the feature detection we were using and given that our features originated from 3 separate participating groups (with large variations in average feature confidence) we normalised the weights of each feature so that no one feature would outweigh any other feature due to differences in confidence levels. In addition, we weighted each feature's influence based on its usefulness as an aid to distinguishing between different videos. For this we utilised a variation of the conventional text-ranking methodology idf. This allowed us to increase the weighting of features that are better able to support distinguishing between relevant and non-relevant videos. In this way we weighted features that were better able to distinguish between videos higher than features that occurred in all or virtually all videos.</p><p>In response to a user's query, a ranked list of videos is returned to the user for further consideration. The overall rank for each video was based on linear combination of required (as specified in the query) feature influence along with the ASR search score. The influence of the ASR text in the video retrieval phase was weighted 4 times higher than any of the other ten features -this being our best-guess parameter reflecting our belief that the ASR transcript feature would be the primary method of ranking videos.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.2">Search and Retrieval of Shot Units</head><p>Upon the user selecting a ranked video from the first phase for shot level examination, the query used to generate the ranked list of videos was augmented with an identifier of the chosen video and then sent to the search server in order to rank shots from within that particular video. The algorithm used to rank shots within a selected video is similar to that used to rank the videos with the following exceptions: the normalisation of feature weights for shots was calculated at a shot level as opposed to the video level; the weighting of each feature's influence was also calculated at the shot level and the ASR text scores were weighted at twice that of features in order to allow features to play a greater role in shot ranking than in video ranking.</p><p>When a user is examining a video at the shot level the ranking outlined above is only one of six sorting options available to the user. These six options discussed in 3.1.3 are chronological (using the weighting above for the SVG timeline as shown in Figure <ref type="figure" coords="7,184.50,181.80,3.62,11.03" target="#fig_1">2</ref>), combined (also using the above weighting but for shot ordering) and four feature groupings as discussed in 3.1.3 which do not use the shot level ranking described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Web Interface with XSL</head><p>Having an internal XML-based architecture allowed us to clearly separate the presentation of data on the user interface from how the system works internally, significantly helping the system development process where software engineering and interface design can happen separately once the full XML format has been agreed.</p><p>For displaying on a web browser, XSL (eXtensible Stylesheet Language) has been extensively used on top of XML descriptions. XSLs were created when designing the interface (at design time), and used in conjunction with internal XML descriptions at user search time. XSLs transformed the internally generated XML video descriptions into 2 different formats based on a user's request -HTML and SVG (Scalable Vector Graphics). HTML is used to render most of the information display on the browser, including video listing with icons, score bars, ASR transcript, and other elements. This includes interactive elements such as ToolTips and JavaScript to enhance interaction. SVG is used to render a timeline on a chronologically displayed shot listing, plotting an indication of the matching status of the four feature groups against the user's query. Transforming to HTML means that any conventional web browser can be used to display the system's interface, though a SVG plug-in is required for viewing the SVG timeline and an Oracle plug-in is also required for streamed playback of video. Figure <ref type="figure" coords="7,355.67,383.04,4.98,11.03" target="#fig_1">2</ref> shows a screen shot of the interface.</p><p>A user specifies her query on the query panel at the top left of the screen. All 10 features and ASR transcript query are grouped into 4 broad groups with distinctive colours associated with each. These are:</p><p>People: Face(s), Group of People Location: Indoor, Outdoor, Cityscape, Landscape Audio: Music, Speech, Monologue, and ASR transcript search box Text: Text Overlay Note that we included ASR transcript search as part of the third group. The query panel is organised by tabs, showing only one of the 4 feature groups at a time. In this way we expected to provide a simple and intuitive query screen to the users (4 features groups rather than 11 features) and the consequent retrieval result visualisation also makes use of the 4 grouping's colour schemes. The user specifies her query by clicking on the radio buttons for each feature, indicating if the feature is required or not. Some features have been intentionally made to be mutually exclusive (e.g. Indoor and Outdoor cannot be specified at the same time). Clicking on the SEARCH button triggers retrieval (see section 3.1.2.1) and the result is displayed below the query panel, as a list of video programmes in a ranked order. For each video programme, score bars are presented indicating the relative scores of the 4 feature groups used in the user's query. Clicking on a title of the video in the list displays the content of the video on the right side and executes shot-level retrieval within that video. Initially an overview of the video programme is presented with the title, textual description and about 30 keyframes selected by equal time distance within the video. The user can further search for the wanted shots if they wish by clicking on the CHRONOLOGICAL button, which presents all of the chronologically ordered individual shots with the detected features, a keyframe, an ASR text portion, as well as score bars for the 4 feature groups. This is shown in Figure <ref type="figure" coords="7,387.15,647.52,3.75,11.03" target="#fig_1">2</ref>. Each of the shot entries also displays small round icons for the features detected for that shot, when their confidence value is above a threshold. At the top of the shot list in the chronological view, an SVG timeline is presented displaying the query matching status for each of the 4 feature groups as well as the combined score. The highlighted segment in the timeline indicates the part of the video that has matched against the query. The user can then click on the timeline to jump to the corresponding shot in the shot list below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Procedure</head><p>For our experiment we created a second version of our system, which supported only ASR transcript searching and not feature-based searching, in order to compare this to the full feature system. Our aim was to compare the two systems to see if the 10 semantic features aided retrieval more than simply relying only on the text-based transcript searching. We also observed the interactive behaviour of users of the system in order to get additional feedback from test users.</p><p>Twelve people participated as test users, 10 postgraduate students and 2 summer intern staff in the School of Computer Applications within the University. All had advanced levels of computer knowledge and familiarity with web-based searching, each conducting some form of online searching on a daily basis. Each of the 12 test users conducted all of the 25 query topics provided by NIST, one by one. The test users were divided into 2 groups, one group conducting the 25 topics in one order, and the other using the same topics but in reverse order. Six users used the full-feature system with all 10 features and the ASR transcript text searchable (3 forward and 3 in reverse order), and another 6 users used the system that had ASR transcript-only searching (also 3 forward and 3 in reverse order).</p><p>Each test user was seated in front of a desktop PC with headphones in a computer lab, and completed the first part of the questionnaire. We used the questionnaire developed over several years by the TREC Interactive track <ref type="bibr" coords="8,496.76,643.92,15.35,11.03" target="#b10">[10]</ref>. The questionnaire included pre-test questions, short post-topic questions, and post-test questions, which each of the users filled in at each stage of the testing. After a brief introduction, test users used a series of web pages which presented each topic, including the audio/image/video examples which form part of the topic descriptions. Users read, viewed, and played the examples that accompanied the topic and then conducted their search. Users were given 4 minutes for searching each topic and whenever a shot was located that the searcher thought answered the topic, they indicated this by checking the relevant box beside the shot entry (see Figure <ref type="figure" coords="8,338.12,712.92,3.62,11.03" target="#fig_1">2</ref>). At the end of the 4 minutes, users filled in a short post-topic questionnaire, and waited to be asked to start the next topic. The time taken to read the topic and examine the associated media elements was included in the four minute allocation per query. At the end of 12 th topic, the users took 10-15 minute break for light refreshments. After the break the next 13 topic searches continued, finishing with the post-test questionnaire. All individual users' interactions were logged by the system, and the results of users' searching were collected and from these results four runs were submitted to NIST for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Submitted Runs</head><p>As mentioned above, we submitted four runs to NIST. These were the following:</p><p>1. Full-feature system with all users (I_B_DCUTrec11B_1), where the selected shots of all users that used the full-feature system were aggregated (combined together) and this aggregated listing was sent as our first run to NIST. 2. ASR transcript-only system with all users (I_B_DCUTrec11C_2), where the selected shots of all users who used the ASR transcript-only system were aggregated and the aggregated shot listing was submitted. 3. User with highest number of shots selected in the Full-feature system (I_B_DCUTrec11B_3), where the results of the individual user who selected the highest number of shots using the full-feature system was submitted as our third run. 4. User with highest number of shots selected in the ASR transcript-only system (I_B_DCUTrec11C_4), where the results of the individual user who selected the highest number of shots using the ASR transcript-only system were submitted as our fourth and final run.</p><p>Note that the run 1 (I_B_DCUTrec11B_1) and 2 (I_B_DCUTrec11C_2) cannot be directly compared with the runs from other groups as these aggregate 6 individual users' results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results of the Experiments</head><p>Figure <ref type="figure" coords="9,99.54,337.57,4.98,11.03" target="#fig_2">3</ref> illustrates the average precision of each of our four runs. As can be seen the figures illustrate that no significant benefit in retrieval performance was found when the features were used in the search and retrieval process, which is the hypothesis we were testing. If we examine the user performance for the user with highest recall then it seems that the ASR+features interface aids the user more than the ASR-only interface, but with such a small number of users we can not say this with confidence. Post-experiment examination of the results of each user show us that while the feature interface worked well for some users, others had difficulties using it and the variance in recall attained by users of the feature interface was almost double that of the ASR-only interface.</p><p>Our observations after running the experiment suggest that a user's primary method of searching was using the ASR transcript, with features being used in addition when their inclusion seemed reasonable. This is clearly illustrated by examination of the two topics for which all 12 of our participants failed to find any relevant documents. Both of these topics (75 and 91) required search terms that were not in our chosen ASR transcript ('Rickenbacker' and 'parrot'), so when the ASR transcript could not aid retrieval, features were found to be of no benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>From the feature extraction task we observe that due to the nature of our approach to face detection, our system ran into difficulties operating on grayscale videos and slightly coloured material. Obvious improvements could be made by using a different approach which does not rely on skin colour segmentation. Our results for Speech extraction showed our method worked very well. If we consider our full 1,000 identified shots of our unofficial run for Instrumental Sound extraction instead of our 300 submitted ones of the official runs, our performance compared favourably with other participants' results.</p><p>From the search task we find ourselves unable to come to any significant conclusions yet about the benefit of incorporating features into the retrieval process. More work needs to be done on methods of combining the features with the ASR transcript. In addition, the experiment has illustrated to us the need to provide users with queryfocussed overview as opposed to our overviews (see section 3.1.3) that used 30 temporally selected keyframes. Observations of the user experiments suggest that some users will not examine a video at the shot level if the overview does not show relevant keyframes regardless of the video's ranked position. Finally, our system seemed to operate very well as a browsing tool supporting search, however we do wonder whether a user needs to go through video level ranking before examining shots. Further experimentation into direct shot-based ranking across videos would answer whether this supports faster resource discovery or reduces the high variability of user performances we observed in our experiment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,201.52,457.81,212.48,11.03"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System architecture of Físchlár-TREC2002</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,188.56,339.13,234.95,11.03;8,70.96,362.17,467.34,11.03;8,70.96,373.69,445.50,11.03;8,70.96,385.21,459.86,11.03;8,70.96,396.61,465.01,11.03;8,70.96,408.13,449.76,11.03;8,70.96,419.65,463.21,11.03;8,70.96,431.17,182.87,11.03;8,105.40,57.12,421.08,270.48"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Web-based user interface of Físchlár-TREC2002The user can re-order the shots by combined score (see section 3.1.2.2), or by any of the 4 feature groups by clicking on the buttons beside CHRONOLOGICAL button, allowing quick access to shots in relation to a subpart of the query she specified. At any point while browsing, the user could click on a keyframe to start streamed playback of the video from that shot onwards, and this allowed the user to clarify if indeed a shot is relevant to a search topic. If a user finds a shot that she believes to be relevant to the search topic, she ticked a checkbox in each shot entry to indicate this, and the initial search result list (on the left of the screen) updated showing the number of shots she has indicated as relevant in the video programme.</figDesc><graphic coords="8,105.40,57.12,421.08,270.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,196.12,533.05,220.01,11.03"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average Precision of our four submitted runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,103.60,406.70,404.93,74.14"><head>Table 1 .</head><label>1</label><figDesc>Speech test results (compared with maximum and median values among TREC participants)</figDesc><table coords="2,421.68,406.70,29.47,10.04"><row><cell>Median</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,79.60,563.89,452.87,93.34"><head>Table 2 .</head><label>2</label><figDesc>Instrumental sound test results (compared with maximum and median values among TREC participants)</figDesc><table coords="3,440.20,563.89,29.47,10.04"><row><cell>Median</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,97.24,651.98,417.62,72.34"><head>Table 3 .</head><label>3</label><figDesc>Face detection results (compared with maximum and median values among TREC participants)</figDesc><table coords="4,421.68,651.98,29.47,10.04"><row><cell>Median</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements:</head><p>We would like to thank the groups from <rs type="funder">IBM</rs>, <rs type="institution">Microsoft Research Asia and LIMSI</rs> who each provided some of the feature detection output that we used in our search submissions. The support of the <rs type="institution">Enterprise Ireland Informatics Research Initiative</rs> is also gratefully acknowledged.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,85.06,299.53,447.49,11.03;10,70.96,311.05,455.30,11.03;10,70.96,322.57,102.04,11.03" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,309.21,299.53,215.30,11.03">Speech-music discrimination from MPEG-1 bitstream</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jarina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,249.88,311.05,242.64,11.03">Advances in signal processing, robotics and communications</title>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Kluev</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Mastorakis</surname></persName>
		</editor>
		<imprint>
			<publisher>WSES Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="174" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.70,340.08,453.69,11.03;10,70.96,351.60,470.25,11.03;10,70.96,363.12,167.61,11.03" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,274.92,340.08,266.48,11.03;10,70.96,351.60,82.36,11.03">Rhythm Detection for Speech-Music Discrimination in MPEG Compressed Domain</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jarina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,166.84,351.60,348.16,11.03">Proc. of the IEEE 14th International Conference on Digital Signal Processing DSP</title>
		<meeting>of the IEEE 14th International Conference on Digital Signal essing DSP<address><addrLine>Santorini, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">2002. July 2002</date>
			<biblScope unit="page" from="129" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,85.16,386.04,442.68,11.03" xml:id="b2">
	<monogr>
		<idno>ISO/IEC JTC 1/SC 29/WG 11</idno>
		<title level="m" coord="10,215.64,386.04,312.21,11.03">Information Technology -Multimedia Content Description Interface -Part</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.95,397.56,91.63,11.03" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,85.50,397.56,22.75,11.03">Audio</title>
		<imprint>
			<date type="published" when="2002-03">March 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,85.41,420.60,455.80,11.03;10,70.96,432.12,110.63,11.03" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,202.12,420.60,163.59,11.03">Detecting Human Faces in Color Images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,373.36,420.60,163.59,11.03">Proc. IEEE Int&apos;l Conf. Image Processing</title>
		<meeting>IEEE Int&apos;l Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="1998-10">October 1998</date>
			<biblScope unit="page" from="127" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.72,455.04,454.50,11.03;10,70.96,466.56,302.22,11.03" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,280.14,455.04,155.76,11.03">Detecting Faces in Images: A Survey</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,448.84,455.04,92.38,11.03;10,70.96,466.56,169.90,11.03">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="58" />
			<date type="published" when="2002-01">January 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.09,489.60,454.13,11.03;10,70.96,501.12,305.35,11.03" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,210.22,489.60,331.00,11.03;10,70.96,501.12,32.71,11.03">A novel method for automatic face segmentation, facial feature extraction and tracking</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sobottka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Pittas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,115.60,501.12,169.83,11.03">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="263" to="281" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,85.06,524.04,325.35,11.03" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,135.45,524.04,167.23,11.03">Fundamentals of digital image processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.25,547.08,455.01,11.03;10,70.96,558.60,350.84,11.03" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,208.06,547.08,274.94,11.03">Designing the user interface for the Físchlár Digital Video Library</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,498.52,547.08,42.74,11.03;10,70.96,558.60,279.34,11.03">Journal of Digital Information, Special Issue on Interactivity in Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.77,581.52,452.50,11.03;10,70.96,593.04,352.51,11.03" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,386.54,581.52,154.73,11.03;10,70.96,593.04,44.12,11.03">Link-based Retrieval an Distributed Collections</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rasolofo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,222.94,581.52,152.75,11.03;10,125.68,593.04,195.24,11.03">Proceedings of the 9th Annual TREC Conference</title>
		<meeting>the 9th Annual TREC Conference</meeting>
		<imprint>
			<date type="published" when="2000">November 16-19, 2000</date>
		</imprint>
	</monogr>
	<note>Report on the TREC-9 Experiment</note>
</biblStruct>

<biblStruct coords="10,90.12,616.08,399.70,11.03;10,70.96,627.60,109.53,11.03" xml:id="b10">
	<monogr>
		<ptr target="http://www-nlpir.nist.gov/projects/t11i/t11i.html" />
		<title level="m" coord="10,90.12,616.08,94.75,11.03">TREC Interactive Track</title>
		<imprint>
			<date type="published" when="2002-10">October 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.19,650.52,399.50,11.03;10,70.96,662.04,180.08,11.03" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,251.18,650.52,198.50,11.03">The LIMSI Broadcast News Transcription System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,461.32,650.52,28.37,11.03;10,70.96,662.04,61.70,11.03">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">890198</biblScope>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
