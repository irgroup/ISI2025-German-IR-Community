<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">‡ VTT Technical Research Centre</orgName>
								<address>
									<addrLine>Kaitoväylä 1</addrLine>
									<postBox>P.O. Box 1100</postBox>
									<postCode>FIN-90571</postCode>
									<settlement>Oulu</settlement>
									<country>Finland, Finland</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">251A9EC0D97C3591BB70AD0CD0558A76</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2002 Video Track MediaTeam Oulu and VTT Technical Research Centre of Finland participated jointly in semantic feature extraction, manual search and interactive search tasks. In the semantic feature extraction task, we sent results for semantic categories of cityscape, landscape, people, speech and instrumental sound. Spatio-temporal correlation of oriented gradient occurrences was used with example shots to detect shots containing people, cityscape or landscape. The audio signal features consisted of various statistical measurements and were used to detect shots containing speech or instrumental sound. Our video browsing and retrieval system, VIRE was used for manual and interactive search tasks. Our system offers two techniques for video retrieval: 1. Multi-modal indexing based on self-organizing feature maps with semantic filtering. 2. An interactive navigating tool that combines two inter-shot properties, temporal coherency and metric similarities, into a view where database shots are presented in a lattice structure. We tested our interactive navigating tool with eight persons to obtain results for 25 pre-defined search topics. In this paper we give an overview of the approaches and a summary of the results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our Temporal Gradient Correlogram (TGC) feature computes local correlations of specific edge orientations producing an autocorrelogram, whose elements correspond to probabilities of edge directions occurring at particular spatial distances. The feature is computed over 20 video frames sampled evenly over the duration of video shot. Due to temporal sampling, autocorrelogram is able to capture also temporal changes in spatial edge orientations. From each sample frame, the edge orientations are quantized into four segments depending on their orientation being horizontal, vertical or either of the diagonal directions.</p><p>To make the feature vector more discriminative, we used detection of skin-colored local regions to generate four values describing the relative size and structure of consistent skin areas in a video shot frame. First value was UHODWLYH DPRXQW, which was simple discrete value between 1 and 5 describing the relative amount of skincolored local regions. Next values indicated number of ORQJ, PHGLXP and VKRUW ]HUR UXQV (adjacent non-skin blocks) between skin-colored local regions measuring the uniformity of skin structured areas. To detect skincolored regions, we marked manually skin areas into 40 key frames selected from the shots in feature development collection and trained a self-organizing map into a skin detector using localized HSV color histogram feature. The histogram was localized into a sector area that covers the typical colors of skin in HSV color space. The sampled local regions of 10x10 pixels were used in localized histogram computation. Degraded quality of test videos was prominent: some of them appeared closely monochromatic and there were large color variances between different videos. Due to this, the prognosis for the success of skin detector was initially set low. However, the feature values were normalized and joined with TGC to examine the discriminative power of less-than-adequately performing skin detector.</p><p>To find the shots consisting of a certain semantic concept, we selected sets of example shots from the collection of video data for training semantic feature development, 13 example shots were selected for people, and 10 for both cityscape and landscape. These example TGC and TGC+skin feature vectors were compared against the vectors computed from the shots in the test video collection. The dissimilarity between the features in the test collection and in the training set was computed with L1 norm. The resulting set of most similar shots was pruned from duplicates and rank-ordered to create the final list of shots most probable to contain the semantic feature in question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6SHHFK DQG ,QVWUXPHQWDO 0XVLF</head><p>Features for detecting speech and music were developed by researchers in VTT Technical Research Centre of Finland. The classification of audio signal between speech and music is widely studied <ref type="bibr" coords="2,472.99,493.53,12.04,9.67" target="#b0">[1]</ref>[2] <ref type="bibr" coords="2,497.07,493.53,12.04,9.67" target="#b3">[4]</ref>. Our approach was based on kNN classification of discrete audio samples with the k value set to 3.</p><p>The extraction of confidence for a shot to contain speech or music was derived from the weighted average of speech/music classification results for the discrete 3 second portions of the signal.</p><p>The classification between speech and music was computationally very inexpensive, and was determined using only four power-related features. A three-second window of the signal was divided into frames of 50 ms overlapping by 10 ms, and the power inside every frame was calculated. The four used features were the variance of the frame-by-frame power, and the variance of the first and the second order differentials of the power, and finally, low energy ratio <ref type="bibr" coords="2,227.11,592.05,11.37,9.67" target="#b4">[5]</ref>, which is computed as the percentage of 50ms frames with RMS power less than the threshold-percentage of the mean RMS power. A threshold level of 20% for low energy ratio was found to give best results, and the spread of the four features was increased by log transformations. In the training stage the features were normalized to zero-average and unit standard deviation. The translation and scaling parameters for each feature were stored and used in the classification stage to normalize the test signal.</p><p>The audio database used to train the system was assembled by sampling music from a vast assortment of CD's and by using a digital recorder to sample speech from Finnish radio broadcasts. All the samples were then converted to 22050 Hz mono. Both conversational speech and single speaker sections were used. The database also contained both male and female speech, sampled from several speakers. Music from various styles and genres was also included in the training set. All samples were 15 seconds long and the length of the whole database was about 20 minutes for speech and 40 minutes for music.</p><p>The classification results of 3 second segments were presented as low-pass filtered time series. Low-pass filtering reduced the effect of separate classification errors and smoothed the transition points between longer segments of speech and music. In addition, mixed signals (containing both speech and music) that would produce a fluctuating series of classification results with a traditional binary decision classifier, are now presented as 'gray' areas that belong to both classes. The new trail of annotation labels shows the degree of certainty of belonging to either class for each three-second audio segment at a time. The numerical results were scaled between 0 and 1, and the weighted mean of the classifications inside each shot was used as the relevance measure for instrumental sound detection. The relevance for speech detection was determined as the inverse of the measure for instrumental sound, so that the sum of these values was always 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5HVXOWV</head><p>The results show that TGC seemed to be most efficient in detecting cityscapes, which may be a result of more structured type of imagery in typical cityscape scenes. Another observation is the degrading effect of poor skin feature detection in the results. Even the detection of people was better using plain TGC, which points out the challenges of using color information in low-saturated and monochromatic videos. The detection of speech and instrumental sounds from audio signal had an averaged precision of 0.641 which was over two times higher than the average precision of 0.246 in people, city and landscape categories. The average precisions for detecting semantic features are shown in Table <ref type="table" coords="3,326.83,370.41,3.98,9.67">1</ref> &amp;OLHQW</p><formula xml:id="formula_0" coords="4,97.04,105.26,405.27,254.49">¢¡ ¤£ ¦¥ ¨ § ¤£ ©¥ § ¦ ¢ § ¥ 4XHU\ 6HUYHU 620 ,QGH[ • ¦ "! $# &amp;% ' ( "# 0) "1 32 54 • 6 ©) 7! "8 71 9 @1 A 3 5% ¨1 B! C "D FE " &amp;) 5 ©G H PI I Q) ©) R! $2 TS 01 UG ¢ 2 V8 QW "9 X &amp;4 Y! ) 64/'DWDEDVH • V ©a ©1 $4 "W cb d# " 0% eC gf d h ¢# " Q) 7! i) • p q% &amp;) 0 ¦4 Y! F "! 1 U2 74 R2 ¢r ¤f d ) 5# 0A s! 58 ¤! i) • t d2 59 VE "1 4 1 34 5W uv# 0A w! 1 x ¢A s yf ) &amp;# &amp;A B! "8 "! ) U % 2 q q) ¤1 4 W )LOH 6\VWHP § ©¡ V § ¤ § F @ &amp; ¥ e § 7 &amp; § ¦ ¥ • b d# " 0% sC Re 64 "W 51 B4 " • 8 1 9 X1 A % f1 w! C Xt Y2 59 gx Y# ¢! U 6! 1 w2 54 • h d ¦4 Y ¦% e 7! $ ) g ¤1 4 5 "A ©f Q) 5# 0A w! U) § ¦ ¦ 7 ¥ ¢ § ¦¥ ¨ i j § ©¡ § 5 § &amp; e • 8 ! $2 &amp;% k ¦W 6 g2 ¢r 6l Qm (n d2 "# 7% s) (2 ¢r YS 01 BG " ©2 • o QC 7r U% e 9 v ) • p 6q Qr &amp;s gh 1 3W 5 "E &amp;C ¢! $ t) (2 r ¤G &amp; 7! i f Q) 52 ¢# 5% I Q) u • b # &amp;1 I a 7v Q1 9 v Vl • w V2 &amp;% xG 5y ¤! ¦z {1 I ! 1 s2 04 " 5% sC • | 5 "} H G q} 4 I G g~9 c ©W ¤1 B4 5W • | " 8 qe q ¦ r • ~ 8 Q9 @ 54 q! 1 I " Q 7! $# 0% yz 2 54 5 6! i1 i2 54 6) &amp;OLHQW ¢¡ ¤£ ¦¥ ¨ § ¤£ ©¥ § ¦ ¢ § ¥ 4XHU\ 6HUYHU 620 ,QGH[ • ¦ "! $# &amp;% ' ( "# 0) "1 32 54 • 6 ©) 7! "8 71 9 @1 A 3 5% ¨1 B! C "D FE " &amp;) 5 ©G H PI I Q) ©) R! $2 TS 01 UG ¢ 2 V8 QW "9 X &amp;4 Y! ) 620 ,QGH[ • ¦ "! $# &amp;% ' ( "# 0) "1 32 54 • 6 ©) 7! "8 71 9 @1 A 3 5% ¨1 B! C "D FE " &amp;) 5 ©G H PI I Q) ©) R! $2 TS 01 UG ¢ 2 V8 QW "9 X &amp;4 Y! ) 64/'DWDEDVH 64/'DWDEDVH • V ©a ©1 $4 "W cb d# " 0% eC gf d h ¢# " Q) 7! i) • p q% &amp;) 0 ¦4 Y! F "! 1 U2 74 R2 ¢r ¤f d ) 5# 0A s! 58 ¤! i) • t d2 59 VE "1 4 1 34 5W uv# 0A w! 1 x ¢A s yf ) &amp;# &amp;A B! "8 "! ) U % 2 q q) ¤1 4 W )LOH 6\VWHP )LOH 6\VWHP § ©¡ V § ¤ § F @ &amp; ¥ e § 7 &amp; § ¦ ¥ • b d# " 0% sC Re 64 "W 51 B4 " • 8 1 9 X1 A % f1 w! C Xt Y2 59 gx Y# ¢! U 6! 1 w2 54 • h d ¦4 Y ¦% e 7! $ ) g ¤1 4 5 "A ©f Q) 5# 0A w! U) § ¦ ¦ 7 ¥ ¢ § ¦¥ ¨ i j § ©¡ § 5 § &amp; e • 8 ! $2 &amp;% k ¦W 6 g2 ¢r 6l Qm (n d2 "# 7% s) (2 ¢r YS 01 BG " ©2 • o QC 7r U% e 9 v ) • p 6q Qr &amp;s gh 1 3W 5 "E &amp;C ¢! $ t) (2 r ¤G &amp; 7! i f Q) 52 ¢# 5% I Q) u • b # &amp;1 I a 7v Q1 9 v Vl • w V2 &amp;% xG 5y ¤! ¦z {1 I ! 1 s2 04 " 5% sC • | 5 "} H G q} 4 I G g~9 c ©W ¤1 B4 5W • | " 8 qe q ¦ r • ~ 8 Q9 @ 54 q! 1 I " Q 7! $# 0% yz 2 54 5 6! i1 i2 54 6)</formula><p>)LJXUH The overall architecture of VIRE, video browsing and retrieval system 6HOHFWHG )HDWXUHV</p><p>Our system uses variety of features, which are used as the components for query. Features are based on different video modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>*HQHULF )HDWXUHV</head><p>Generic features are measured from the physical properties of video data. Generic feature vectors of query and database shots are compared using distance metrics to measure dissimilarities. Different generic features can be used jointly to measure various physical properties simultaneously.</p><p>&amp;RORU properties of a video shot were described with the Temporal Color Correlogram (TCC). Its efficiency against other color descriptors has been proven in <ref type="bibr" coords="4,286.63,550.89,23.59,9.67">[6][7]</ref>. TCC captures the correlation of HSV color pixel values in spatio-temporal neighborhoods. The feature captures temporal dispersion or congregation of color clusters unlike static color features. The parameters we used for the feature are described in <ref type="bibr" coords="4,482.95,575.49,11.37,9.67" target="#b6">[7]</ref>. TCC is computed over 20 video frames that are sampled evenly over a video shot.</p><p>0RWLRQ is a prominent property in video frame sequence. We computed features describing motion activity, based on definitions in MPEG-7 Visual standard <ref type="bibr" coords="4,275.95,624.81,11.28,9.67" target="#b7">[8]</ref>. Motion Activity descriptor defines following properties of motion: intensity, direction, spatial and temporal distribution of motion activity. Our system uses the following subset of those features:</p><p>• Intensity of motion is a discrete value where high intensity indicates high activity and vice versa. Intensity is defined as the variance of motion vector magnitudes normalized by the frame resolution and quantized in the range between 1 and 5. • Average intensity measures the average length of all macroblock flow vectors.</p><p>• Spatial distribution of activity indicates whether the activity is scattered across many regions or in one large region. This is achieved measuring the short, medium and long runs of zeros that provide information about the size and number of moving objects in the scene. Each attribute is extracted from the thresholded flow vectors obtained from the video data. All feature values are normalized so that they can be used jointly with other features in self-organizing indexes.</p><p>$XGLR contains a lot of information about the video shot content. And not just the spoken lexicon, but sounds and noises pinpoint details about video's semantic content. The real challenge here is however to choose meaningful parameters which somehow reflect the response to the various properties of sounds in the human aural perception. There has been extensive research on this topic (see <ref type="bibr" coords="5,365.83,194.73,12.58,9.67" target="#b8">[9]</ref> and references therein) primarily in connection with speech recognition systems <ref type="bibr" coords="5,256.03,206.97,16.45,9.67" target="#b9">[10]</ref>. We have selected following features to construct a generic audio feature descriptor <ref type="bibr" coords="5,166.15,219.33,17.17,9.67" target="#b11">[12]</ref>[13]:</p><p>• Zero-crossing rate (ZCR), one of the most commonly used audio features, gives information about the spectral content of the signal. Taking into account relative spectral deficiency of the sound tracks of the movies in the VideoTREC database (which is obviously the result of their relatively old age) our measurements showed us that the bandwidth of a typical sound track was about 8 kHz. We feel that it is safe to assume that ZCR tracks the fundamental frequency f0 with quite high precision <ref type="bibr" coords="5,491.83,281.61,12.58,9.67" target="#b8">[9]</ref> due to the absence of high-frequency components. Some researchers believe <ref type="bibr" coords="5,409.99,293.97,17.86,9.67" target="#b12">[13]</ref> that ZCR is one of the most indicative and robust measures to discern unvoiced speech. • RMS energy measures mean signal energy, which is what the human ear interprets as a notion of the acoustic volume, but it does not carry any information about the presence of transient sounds.</p><p>• Maximal and minimal energy are two parameters that correspond to the idea of "still" and "loud" sound, respectively. If the minimal energy is large and close to the value of the maximal energy, one can attribute the property of acoustic loudness to the entire sound clip. On the other hand, low maximal energy is a reliable indicator of a silent sound clip and hence it is useful for quick and easy silence detection. • Mean and standard deviation sample energy gives a measure for scattering of the sample energies about their mean value. <ref type="bibr" coords="5,200.11,419.49,17.74,9.67" target="#b10">[11]</ref> • Percentage of samples whose energy is below 50% of the mean energy of the sound clip has been used for the purpose of speech/music discrimination <ref type="bibr" coords="5,328.03,444.81,16.36,9.67" target="#b13">[14]</ref>. The fact is that the speech signal contains more "quiet" frames so this value will be higher for speech than for music. • Percentage of samples whose energy is below 10% of the mean energy of the sound clip might carry some information about transient sounds and the purpose of using this feature was to improve scene detection. If the mean energy is significantly greater than that of the majority of sound samples, it might indicate the presence of short-term shooting-like acoustic events. • Harmonicity ratio describes the proportion of harmonic components in the spectrum. The algorithm output is 1 for purely periodic signal and 0 for the white noise. A useful feature derived from harmonicity ratio was the length of the comb filter, which is an estimate of the delay that maximizes the autocorrelation function. • Upper limit of harmonicity loosely defines the frequency beyond which the spectrum has no harmonic components. These features were calculated in 2048 sample windows that overlapped by 1024 samples. All audio files had sampling frequency of 22050. • Spectral centroid is the center of gravity of the power spectrum. For audio signals that have clearly much energy in lower or higher parts of the spectrum this feature is useful. • Spectral spread is the RMS deviation of the spectrum centroid, and thereby it describes if the spectrum is widely spread out or concentrated around its centroid. The used values were median values in one second window. These features were calculated in non-overlapping windows of 1024 samples.</p><p>To compute the dissimilarities of generic feature vectors, we have used L1 norm. Each generic feature vector is normalized prior inserting into self-organized index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6HPDQWLF )HDWXUHV</head><p>Semantic features are single lexical concepts exist in a shot with certain confidence. Our system utilizes these concepts as binary filters to create subsets from results obtained with fuzzy generic feature dissimilarities. Different combinations of concepts can be used to create filter sets. In this case, the resulting subset will be the intersection of existing concepts in the initial result set. We used following IBM donated semantic features: face, people, indoors, outdoors, instrumental sound, speech, landscape and text. These features had a confidence value that was thresholded into binary filter rules. The threshold value was decided upon criteria where approximately one third of the shots were assigned with the concept. Following threshold values were used: 0.62 for face, 0.75 for indoors, 0.35 for instrumental sound, 0.7 for landscape, outdoors, and people, 0.4 for speech, and 0.3 for text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7H[W )HDWXUHV</head><p>Text features are derived from the automatic speech recognition (ASR) data that was made available for all participants by CLIPS-IMAG. The textual information was not used as a feature vector. Instead, it was used like semantic features as a filter for the results acquired from SOM-index structure. It was used to eliminate shots that did not contain indicated textual terms. To accomplish this, the ASR transcripts were indexed into a database treating the words as single-word terms. A stop word list was used to exclude grammatical and otherwise undiscriminating words that would have led to poor resolution. Qualified words were then lemmatized using the morphological processing features of the WordNet <ref type="bibr" coords="6,376.15,330.09,16.36,9.67" target="#b14">[15]</ref>.</p><p>Because of the effects of lexical semantic phenomena such as homonymy or polysemy on the relevancy of the documents retrieved, techniques of word sense disambiguation (WSD) have been found useful in information retrieval to mitigate the effects of expressive power of natural language. Potentially relevant documents containing close synonyms of the query word such as 'dog' and 'canine' and hyponyms such as 'Malamute' will be missed unless queries are expanded by synonyms and hyponyms of the terms used in original user requests. <ref type="bibr" coords="6,158.71,404.01,17.98,9.67" target="#b15">[16]</ref> To evaluate queries, we used relevance metrics to measure which shots were suitable given a set of topic related query words that were synonym expanded. The ranking of the shots was computed using Term Frequency Inverse Document Frequency (TDIDF) <ref type="bibr" coords="6,288.19,440.97,17.74,9.67" target="#b16">[17]</ref> based classification method that pinpoints relevant single-word terms occurring in the ASR transcripts. First, the given query words were automatically reduced to their base form. Second, the lemmatized query words were expanded with their synonyms using the WordNet <ref type="bibr" coords="6,105.79,477.81,16.36,9.67" target="#b14">[15]</ref>. Third, the relevance metric was computed for every shot that contained at least one of the words in the expanded query set. Finally, the neighbors of suitable shots were also included into the results within a time frame of 4 seconds. This was done for the sake of the temporal locality of the topic that spans over short shots that can hardly contain any ASR information.</p><p>Our approach encounter challenges as the video test material consisted of degraded audio quality, which seemed often lead to false detections of words. To use such data in a rather restricting filter yields inflexibility in the cases of erroneous interpretation of the spoken words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0XOWLPRGDO ,QGH[LQJ %DVHG RQ 6HOIRUJDQL]LQJ 0DSV</head><p>To avoid exhaustive searching on the server side, we used self-organizing index maps that are capable of finding metrically closest matches with any feature combination within tight timing requirements set by interactive video browsing that requires much parallel query processing. Our index structure consists of seven self-organizing maps (SOMs). Three of the SOMs are based on the primary generic features: color, audio and motion. Next three SOMs compose of joint features from the primary generic features: color&amp;audio, color&amp;motion and audio&amp;motion. Last SOM uses all primary features: color&amp;audio&amp;motion. Each of the SOMs is generated with the SOM parameters set in the SOM Toolbox <ref type="bibr" coords="7,365.11,221.61,16.36,9.67" target="#b17">[18]</ref>.</p><p>All the examples in a single query are processed individually and access to specific index maps is directed according the selected set of features. Each individual example launches best matching nodes -search returning a set of closest shots from the appropriate SOM.</p><p>Next these intermediate results are filtered using semantic feature or text description filters. Multiple filters can be selected, for example 'Outdoors' , 'People' and text 'red Chevrolet' . The result sets are evaluated based on the existence of these filter terms and only the shots fulfilling the criteria are selected. Then, these sets of results are combined with fuzzy Boolean OR operator to form the final ranked result set. Different weights can be set to examples to change the order in the final, combined results. The selection of the amount of best matching nodes is guided by the source of the query. In browsing, the speed is emphasized over retrieval precision, so the number of best matching nodes is small. We have used 3, 10 and 70 best matching nodes in fast browsing, more precise browsing and manual querying, respectively. Figure <ref type="figure" coords="7,424.39,357.09,5.37,9.67">2</ref> illustrates the indexing structure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single feature maps</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combined feature maps</head><p>)LJXUH Self-organizing index structure organizes database shots into various maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0DQXDO 4XHU\ ,QWHUIDFH</head><p>Figure <ref type="figure" coords="7,93.55,699.45,5.37,9.67">3</ref> shows the query interface devised for manual search. The view offers selections of various search features and filters in the left side and resulting shots ranked by their similarity with the selected query attributes are displayed in the right side view. Query topics can be changed from the Topic menu. This will change the example shots or images provided by the topic description to the lower left panel. User can select any combination of three generic features (color, motion or audio) for any example shot, but for example image only color feature is supported. User can enable any set of semantic filters in addition to the selected generic features. Additionally, a lexical word filter can be constructed by selecting a set of words in the upper left panel of the interface. At least one example shot with one generic feature enabled is the minimum requirement for submitting a query. From the result set, user can select any interesting shot as a start point for navigation in the browsing interface.</p><p>)LJXUH Manual query interface</p><formula xml:id="formula_1" coords="8,96.80,599.71,140.33,19.79">,QWHUDFWLYH %URZVLQJ ,QWHUIDFH</formula><p>The novelty of our approach in the interactive search task relies on two aspects: interface design and contentbased feature processing. The motivation is to reduce the effect caused by ambiguous results that are usually obtained from a traditional content-based example search. Currently, two dominant approaches are used to realize video searching and browsing. Systems either select content-based presentation of video items or rely on more traditional time-line based organization into temporally adjacent items. The disadvantages of the approaches are in their incapability to associate computed features with the user' s information need (ambiguity of content-based approaches) and to provide a holistic view over the linear temporal presentation (inefficiency of the time-line based browsing).</p><p>Our approach combines both inter-video similarities and local temporal relations of video shots in a single interface. In interactive search, users want the computer to act as a 'humble servant' , providing enough cues and dimensions for users to navigate through the vast search space towards the relevant objects. In VIRE, users can perform FRQWHQWRULHQWHG EURZVLQJ that combines timeline presentation of videos with contentbased retrieval. Content-oriented browsing implies that the video content is not utilized alone, but in conjunction with temporal video structure.</p><p>Figure <ref type="figure" coords="9,104.11,182.37,5.37,9.67">3</ref> illustrates the browsing interface. The panel showing the first row of key frame images displays sequential shots from a single video in a chronological time-line. At any time, user can scroll through the entire video shot sequence to get an overview of the video content. The leftmost key frame in the top row shows always the first shot and name of the video. The first shot may contain initial setup for the entire video, so by viewing it, user can get instant idea about the semantic setting for the rest of the video shots.</p><p>The lower right panel gives user another content-oriented view, but this time from the entire database. The columns below the topmost shots show the most similar matches organized in top-down rank-order. The columns generate a similarity lattice that provides linkage to other database videos. The similarity is measured based on the features selected in the lower left panel. User can select a single feature or any combination depending on what properties they want to browse with.</p><p>Additionally, user can decide whether he want to include shots from the query video to be shown on a similarity lattice. When user locates interesting shots from the lattice, he can open the video in the topmost row so that the interesting shot is located in the middle column. After updating the shots in the topmost row, system re-computes the similarity lattice. At any time, user can update the current lattice using other feature combinations. The features that can be used in browsing are color, motion, audio and texture.</p><p>)LJXUH Content-oriented browsing interface. Lower right panel shows the similarity lattice.</p><p>The requirements to update the similarity lattice are heavy, since the browsing speed should be close to realtime. To update a single lattice, system must perform parallel query processing in several individual examplebased queries. Multi-threaded index queries to self-organizing maps provide efficient access mechanism to proximity search for every feature combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>([SHULPHQWDO 6HWXS</head><p>We tested our system in both manual and interactive search. NIST provided 25 search topics that contained varied from very specific ('Find shots with Eddie Rickenbacker' ) to more generic concepts ('Find overhead views of cities' ). Topic included one or more example clips of video or images to aid the search process. From the image examples, only a color correlogram could be used as a generic search feature whereas the video examples offer color, motion and audio or any of their combinations. All topics were processed both manually and interactively in search test video collection that consisted 40 hours of video material. NIST also provided segmentation for all videos with more than 24000 video shots, from which over 14000 belonged to the search test collection.</p><p>In manual query the user initiated the query by selecting appropriate generic feature cues from the topic examples, sets weight to indicate relevant and non-relevant examples and decides appropriate filters. Three different search configurations were tried for each search topic: search using generic features only, search using generic features with semantic feature filtering, and search using generic features with text feature filtering. Self-organized index was used with 70 best matching nodes.</p><p>A group of eight new users carried out the interactive search. Test users, most of them males, were information engineering undergraduate students, having good skills in using computers, but less experience in searching video databases (obviously at least somewhat experienced in www-search). Every user reported to be somewhat familiar with the search topics. 25 topics were divided into four sets that were randomly given to the test users so that two users carried out the same set of topics. All users were given half an hour introduction to the system, with emphasis on the search and browsing interface functions demonstrated with couple of example search. Users were told to use approximately ten minutes for each search, during which they navigated in the shot database and selected shots that seemed to fit to the topic description. Users were also told to fill a questionnaire about their experiences. The machines that the system was running on were 400-800 MHz PCs with Windows 2000 operating system. After the tests were finished, the two result sets for one topic were joined by removing duplicate matches and averaging their rank values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5HVXOWV</head><p>Average precisions for the four different search configurations are shown in Table <ref type="table" coords="10,425.59,558.45,3.98,9.67">2</ref>. Manual configurations used either only Generic Features (color, motion or audio), Generic Features together with semantic feature filtering (outdoors, landscape) or Generic Features together with text filter (" red Chevrolet" ). Number of hits at depth 10 describes the total amount of found matches when considering the 10 best ranked matches for a topic. Overall average shows the mean value of the average precisions in 25 topics. As can be seen, the performance of interactive search overcomes greatly the manual search results. This indicates clearly the importance of the human factor in search. The average time in making an interactive browsing was 10.08 minutes. During this time, the average of 12.7 matches were found from the database. This means that average of 1.26 matches were found during each minute of searching. Another interesting observation is the counter-effectiveness of semantic and text filters to the results. According to the answers in questionnaire, the VIRE system was easy to learn, but somewhat harder to use. This was due to the ambiguous results that fuzzy-based similarity measurements return in many instances. The browsing interface was appreciated, although the near-real time responses in updating the browsing view would have been preferred to be completely real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&amp;RQFOXVLRQV</head><p>We approached content-based video retrieval from many different perspectives in TREC 2002 Video Track.</p><p>Our experiments showed that the extraction of semantic concepts is a challenging task. However, the results in detecting instrumental sound and speech were promising. Visual semantic features have still a lot to improve: the low visual quality of the videos definitely affected the results, though. The most successful component in our work was the content-oriented browsing that gave the computer merely the role of an assistant in the cognitive process of semantic searching. Our system was subordinated to provide the user multiple parallel paths from which he could choose the direction for his navigation independently. We combined the temporal connectivity of temporally adjacent shots and fuzzy shot similarities into one view to provide the user comprehensive information about the inter-relations between the video shots.</p><p>The manual search methods must still overcome big challenges to reach a satisfactory level of performance in the high-level semantic search problems. However, there are search problems that are more suitable to automatic search than others, for example locating cityscape views or retrieving shots containing speech. It seems that while efficient features can be computed from different modalities, they are not alone appropriate for automated semantic retrieval. Does the missing link lie within a single feature quality, or rather in a way to combine multiple modalities? This still remains an intriguing research problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,357.08,430.55,68.51,19.17;7,357.08,392.51,68.51,19.17;7,127.40,603.68,18.87,14.10;7,264.67,584.72,18.87,14.10;7,96.92,609.15,18.10,7.05;7,230.36,609.15,22.45,7.05;7,154.04,609.15,19.21,7.05;7,93.08,505.98,217.04,10.57;7,176.96,514.43,307.19,19.17;7,264.68,563.43,46.33,7.05;7,329.48,563.43,49.69,7.05;7,394.27,563.43,51.13,7.05;7,409.99,538.71,73.57,7.05;7,197.48,424.55,38.88,24.79;7,206.00,453.33,2.36,8.37;7,211.76,446.31,128.03,17.45;7,210.79,456.87,117.65,17.45;7,157.88,558.42,51.68,10.57;7,157.88,572.22,66.80,10.57;7,312.80,593.58,101.60,10.57;7,357.08,430.55,68.51,19.17;7,357.08,392.51,68.51,19.17;7,127.40,603.68,18.87,14.10;7,264.67,584.72,18.87,14.10;7,96.92,609.15,18.10,7.05;7,230.36,609.15,22.45,7.05;7,154.04,609.15,19.21,7.05;7,93.08,505.98,217.04,10.57;7,176.96,514.43,307.19,19.17;7,264.68,563.43,46.33,7.05;7,329.48,563.43,49.69,7.05;7,394.27,563.43,51.13,7.05;7,409.99,538.71,73.57,7.05;7,197.48,424.55,38.88,24.79;7,206.00,453.33,2.36,8.37;7,211.76,446.31,128.03,17.45;7,210.79,456.87,117.65,17.45"><head></head><label></label><figDesc>Feature and Text Based Filtering 2XWGRRU ,QGRRU /DQGVFDSH 3HRSOH )DFH 0XVLF ³UHG &amp;KHYUROHW´« Feature and Text Based Filtering 2XWGRRU ,QGRRU /DQGVFDSH 3HRSOH )DFH 0XVLF ³UHG &amp;KHYUROHW´« Color &amp; Audio Color &amp; Motion Motion &amp; A udio Color, Mo tion &amp; A udio 6HUYHU • &amp;RPELQLQJ WKH 5HVXOW 6HW ZLWK )X]]\ %RROHDQ 25 2SHUDWRU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,90.20,217.07,416.28,339.24"><head></head><label></label><figDesc></figDesc><graphic coords="8,90.20,217.07,416.28,339.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,101.12,377.15,394.32,301.08"><head></head><label></label><figDesc></figDesc><graphic coords="9,101.12,377.15,394.32,301.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,61.75,370.41,473.14,347.83"><head></head><label></label><figDesc>.</figDesc><table coords="3,61.75,387.31,473.14,330.92"><row><cell cols="4">7DEOH Average precisions for different semantic features</cell></row><row><cell></cell><cell>7*&amp;</cell><cell>7*&amp;6.,1</cell><cell>$8',2</cell></row><row><cell>People</cell><cell>0.248</cell><cell>0.168</cell><cell>-</cell></row><row><cell>Cityscape</cell><cell>0.299</cell><cell>0.197</cell><cell>-</cell></row><row><cell>Landscape</cell><cell>0.193</cell><cell>0.128</cell><cell>-</cell></row><row><cell>Speech</cell><cell>-</cell><cell>-</cell><cell>0.645</cell></row><row><cell>Inst. Sound</cell><cell>-</cell><cell>-</cell><cell>0.637</cell></row><row><cell cols="4">0DQXDO DQG ,QWHUDFWLYH 6HDUFK 7DVNV</cell></row><row><cell cols="4">9LGHR %URZVLQJ DQG 5HWULHYDO 6\VWHP 9,5(</cell></row><row><cell cols="4">Our video browsing and retrieval system VIRE was used in manual and interactive search experiments. It is</cell></row><row><cell cols="4">based on Java code and can be run on both SUN Solaris and Windows 2000 operating systems. System uses</cell></row><row><cell cols="4">J2SE, QuickTime 6 for Java, WordNet dictionary and MySQL JDBC. The system consists of server and</cell></row><row><cell cols="4">client applications, where server controls the querying through self-organizing feature index maps and</cell></row><row><cell cols="4">provides the client query results. Client software offers two views, one for constructing video queries and</cell></row><row><cell cols="4">another for browsing the database shots. References to physical media data and additional feature information</cell></row><row><cell cols="4">are stored in MySQL-database. Figure 1 depicts the architecture of the VIRE system. The browsing view of</cell></row><row><cell cols="4">the client offers content-oriented parallel navigation through database videos. The browsing procedure would</cell></row><row><cell cols="4">be exhausting without efficient indexing structure. By utilizing self-organizing feature maps we were able to</cell></row><row><cell cols="4">unravel the problems with computational requirements.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,82.75,698.47,283.90,19.79"><head></head><label></label><figDesc>The six most successful topics are listed in Table3, best resulting topics being topmost. Finding George Washington, Price Tower and James H. Chandler overall seemed to be the most successful topics in manual and interactive searching across the participating system runs.7DEOH Six most successful topics (topic number in parenthesis)</figDesc><table coords="10,82.75,698.47,283.90,19.79"><row><cell>7\SH RI VHDUFK FRQILJXUDWLRQ ,QWHUDFWLYH 0DQXDO *HQHULF )HDWXUHV 0DQXDO *HQ )HDWXUHV 6HP )LOWHUV 0DQXDO *HQ )HDWXUHV 7H[W )LOWHU</cell><cell>1U RI KLWV DW GHSWK 151 38 22 12</cell><cell>2YHUDOO $YHUDJH 0.26 0.03 0.02 0.01</cell></row><row><cell>,QWHUDFWLYH 6HDUFK</cell><cell cols="2">0DQXDO 6HDUFK *HQHULF )HDWXUHV</cell></row><row><cell>James H. Chandler (76)</cell><cell cols="2">James H. Chandler (76)</cell></row><row><cell>George Washington (77)</cell><cell cols="2">Microscopic living cells (97)</cell></row><row><cell>Parrots (91)</cell><cell cols="2">Eddie Rickenbacker (75)</cell></row><row><cell>Price Tower in Oklahoma (84)</cell><cell cols="2">Musicians (80)</cell></row><row><cell>Microscopic living cells (97)</cell><cell cols="2">Snow covered mountains (90)</cell></row><row><cell>Nuclear explosion (95)</cell><cell cols="2">Overhead view of cities (86)</cell></row><row><cell cols="2">7DEOH Results for different configurations over 25 search topics</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,96.80,141.93,385.53,9.67;12,96.79,154.29,131.21,9.67" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,295.76,141.93,186.57,9.67;12,96.79,154.29,60.30,9.67">A comparison of features for speech, music discrimination</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Parris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lloyd-Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,164.12,154.29,58.32,9.67">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,167.49,394.61,9.67" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,223.39,167.49,197.20,9.67">Detection of human speech in structured noise</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,427.51,167.49,58.33,9.67">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,180.69,435.53,9.67" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,171.55,180.69,231.06,9.67">Spectral analysis and discrimination by zero-crossings</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kedem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,409.75,180.69,45.54,9.67">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">74</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,193.89,371.93,9.67" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,176.83,193.89,220.53,9.67">Real-time discrimination of broadcast speech/music</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,404.83,193.89,58.33,9.67">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,207.09,418.53,9.67;12,96.79,219.45,126.41,9.67" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,230.71,207.09,284.61,9.67;12,96.79,219.45,55.53,9.67">Construction and evaluation of a robust multifeature speech/music discriminator</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,159.43,219.45,58.22,9.67">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,232.65,406.16,9.67;12,96.79,244.89,434.57,9.67" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,350.23,232.65,152.72,9.67;12,96.79,244.89,54.65,9.67">Semantic image retrieval with HSV correlograms</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rautiainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Matinmikko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aittola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,158.71,244.89,237.52,9.67">Proc. 12th Scandinavian Conference on Image Analysis</title>
		<meeting>12th Scandinavian Conference on Image Analysis<address><addrLine>Bergen, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="621" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,258.21,370.01,9.67;12,96.79,270.57,333.17,9.67" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,258.55,258.21,204.41,9.67">Temporal color correlograms for video retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rautiainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,96.79,270.57,252.98,9.67">Proc. 16th International Conference on Pattern Recognition</title>
		<meeting>16th International Conference on Pattern Recognition<address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,283.77,388.19,9.67;12,96.79,296.01,161.21,9.67" xml:id="b7">
	<monogr>
		<title level="m" coord="12,96.79,283.77,388.19,9.67;12,96.79,296.01,116.87,9.67">MPEG-7 standard: ISO/IEC FDIS 15938-3 Information Technology -Multimedia Content Description Interface -Part</title>
		<imprint>
			<publisher>Visual</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,309.33,435.05,9.67;12,96.79,321.57,76.97,9.67" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,176.23,309.33,112.87,9.67">Audio signal classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gerhard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Burnaby, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, Simon Fraser University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,334.77,355.97,9.67" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,233.71,334.77,153.20,9.67">Fundamentals of speech recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B-H</forename><surname>Juang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,348.09,423.58,9.67;12,96.79,360.33,257.81,9.67" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,224.71,348.09,249.95,9.67">Random processes: a mathematical approach for engineers</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Davisson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,373.53,388.79,9.67;12,96.79,385.77,157.89,9.67" xml:id="b11">
	<monogr>
		<title level="m" coord="12,96.79,373.53,388.79,9.67;12,96.79,385.77,87.81,9.67">MPEG-7 standard: ISO/IEC FDIS 15938-4: Information technology --Multimedia content description interface</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,399.09,432.17,9.67;12,96.79,411.33,122.93,9.67" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,252.43,399.09,119.77,9.67">Multimedia content analysis</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,379.15,399.09,144.83,9.67">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="page" from="12" to="36" />
			<date type="published" when="2000-11">2000. November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,424.53,415.05,9.67;12,96.79,437.01,280.01,9.67" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,226.99,424.53,284.85,9.67;12,96.79,437.01,55.53,9.67">Construction and evaluation of a robust multifeature speech/music discriminator</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Scheier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,159.43,437.01,72.74,9.67">Proc. ICASSP-97</title>
		<meeting>ICASSP-97<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-04-21">1997. April 21-24</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,450.21,330.65,9.67" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,181.03,450.21,172.55,9.67">WordNet: An electronic lexical database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,463.41,391.19,9.67;12,96.79,475.65,380.93,9.67" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,234.91,463.41,253.07,9.67;12,96.79,475.65,85.42,9.67">Speech and language processing: an introduction to natural language processing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,189.31,475.65,221.42,9.67">Computational Linguistics, and Speech Recognition</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,488.97,413.14,9.67;12,96.79,501.21,149.09,9.67" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,215.11,488.97,244.36,9.67">On the specification of term values in automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,466.75,488.97,43.18,9.67;12,96.79,501.21,63.78,9.67">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="351" to="372" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,514.41,423.95,9.67" xml:id="b17">
	<monogr>
		<ptr target="http://www.cis.hut.fi/projects/somtoolbox/documentation/somalg.shtml" />
		<title level="m" coord="12,96.79,514.41,62.25,9.67">SOM Toolbox</title>
		<imprint>
			<date type="published" when="2002-11-02">2.11.2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,96.79,527.73,438.09,9.67;12,96.79,539.97,264.29,9.67" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,262.99,527.73,133.67,9.67">On image classification: city vs</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vailaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.15,539.97,158.27,9.67">Access of Image and Video Libraries</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="3" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
