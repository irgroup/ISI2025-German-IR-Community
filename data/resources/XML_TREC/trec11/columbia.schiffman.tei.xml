<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,122.64,133.48,352.19,12.91">Experiments in Novelty Detection at Columbia University</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,255.84,162.71,86.09,10.76"><forename type="first">Barry</forename><surname>Schiffman</surname></persName>
							<email>bschiff@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,122.64,133.48,352.19,12.91">Experiments in Novelty Detection at Columbia University</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8205ABC875D03A684AED78E29084CDEA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the method we used for the Novelty Track for the 2002 Text Retrieval Conference (TREC). We tried to adapt tools we are developing for a task closely related to the novelty part of the this track. The system we are building will scan a stream of documents and present to the user only the new information it finds. For the "relevance" part of the TREC, we decided to test the applicability of some of these tools. Since information retrieval is not a focus of our research, we thought it would be more interesting to use something new rather than try to hurriedly catch up. The results were far from satisfactory, but it is clear from the overall results that novelty detection remains a difficult and unsolved problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task in the Novelty Track at the 2002 Text Retrieval Conference (TREC) was structured in two parts. First, the system had to find sentences in a cluster of documents that are relevant to a query, and second, as the sentences were presented in a predetermined order, it had to remove any that duplicated information in previous sentences. The clusters themselves were culled from the fourth and fifth TREC collections by an Information Retrieval system, selecting the documents relevant to the query. The queries were 50 previous TREC topics, in some cases altered somewhat. Up to 25 documents were collected for each topic.</p><p>Our interest in participating in the Novelty Track was to work with the data in the second part. We are building a system, called the New Information Agent <ref type="bibr" coords="1,265.70,665.92,24.46,8.97">(NIA)</ref> to detect new information from a stream of document. Like the TREC version, the input to our system is a clustered stream of documents, or in an offline version, a collection of documents, that focuses on a particular event or issue. Again, like the TREC task, the output of our system is a short list of the sentences that do not contain any material that duplicates a passage selected earlier. But the presence of the query is the key difference between the TREC version of the task.</p><p>We consider all the documents to be of potential interest to the user. Because the query dominates each problem, the TREC task calls for deciding relevance first and novelty second -the reverse of what we will do in our system. We first identify segments that contain new information and then decide if they are interesting. In our terms, interesting is not the same as relevant, since we have no query to base relevance on.</p><p>With a query, the task is more focused, providing the system with some kind of guide for what to select, but the characteristics of the tasks vary with the kind of topic used. The sample topics suggested that deep understanding of language would help, and might even be necessary for strong performance. For example,the first sample, about the Hubble Space Telescope, asked for material about the achievements of the telescope and not material about repairs or modifications to the telescope. We know of no automated system that can classify events as achievements or not achievements in relation to an arbitrary object, here a telescope. It seemed clear that the relevance portion would dominate the task. The coordinators of the novelty track said so when the guidelines were promulgated.</p><p>Because we have no experience with relevance judgments, we chose to experiment with an unusual approach that borrowed the language analysis tools we are developing for our new information system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">New Information</head><p>NIA analyzes a document in terms of the content words and the contexts in which each one appears, and then compares documents by comparing these contexts in structure called Concept Vectors. In order to build these Concept Vectors, the system groups the words into sets of "referential equivalents" or Concept Sets, so that in a document about the Hubble space telescope, the words telescope and instrument would be equated and put into the same Concept Set. The Concept Vectors are creating by making lists of which Concept Sets co-occur with each other. These vectors are compared across documents -not sentences or clauses.</p><p>The system uses a syntactic analyzer that breaks up documents into clause-sized chunks. These are used in two different ways: 1.) potential "equivalents" are grouped together only if they appear within clause chunks of one another, and 2.) segments of new information are identified by examining the concepts in each clause with respect to how well their corresponding vectors are covered by previously seen material. In our version of the new information task, we hypothesize that sentences are not a good unit for analysis. Rather than consider the similarity or dissimilarity of whole sentences, we are trying to efficiently decompose the documents into small chunks and discover when new relationships between entities appear.</p><p>In the TREC task, we lost that framework since the novelty part examines a collection of sentences that relate to a query, but are each individual passages taken out of context. The result is that the system we are developing is not appropriate and was ignored. In addition, we were running out of time, so that the novelty part of our task was done with a rather simple system of computing the overlap of the words sentence by sentence.</p><p>In the rest of this paper, Section 2 will discuss work related to our experiments; Section 3 will talk briefly about the system we are building; Section 4 will provide a desciption of the program used in the Novelty Track; Section 5 will review its performance; Section 7 will reflect on the lessons learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Novelty detection is a new area of research, with roots in information retrieval, in particular first story detection under the Topic Detection and Tracking (TDT) initiative and in multi-document summarization. The task defined in the TREC Novelty Track is closer to the TDT task. Some recent work by James Allan exemplifies the extension of TDT to the passage level of documents <ref type="bibr" coords="2,352.98,232.72,25.05,8.97">(2001)</ref>. He posit that a sentence is "useful" if it is on topic, and that a sentence is "novel" if it is not redundant with previously seen sentences. Their perspective is topic-based and the experimental corpus comes from the TDT-2 corpus, in which 60,000 news stories were assigned to some 200 news topics. After selecting 22 of these topics, annotators created lists of the events that comprised each topic and assigned each sentence to one or another event. A total of 343 events were derived from 944 articles. Two different language models for deriving "useful" information were developed, based on the probabilities that individual words of a sentence appear in on-topic sentences or articles. The models of novelty are derived in a similar way from the specific words in on-event sentences.</p><p>A numer of efforts in multi-document summarization have sought either to highlight differences or avoid redundancy. A group at CMU <ref type="bibr" coords="2,431.85,436.96,93.72,8.97" target="#b1">(Goldstein et al., 2000)</ref> uses cosine similarity of vectors in the MMR algorithm, which is cited by Allan. They seek to eliminate redundancy from their summaries with a measure similar to Allan's novelty detector. Radev attempted to create a framework for analyzing differences between sentences between sentences from different documents, with relationships such as "equivalence", "subsumption" or "contradiction" <ref type="bibr" coords="2,404.28,532.48,25.05,8.97">(2000)</ref>.</p><p>A graph representation of several relationships between words is used to find similarities and differences between pairs of articles <ref type="bibr" coords="2,410.78,569.44,110.50,8.97" target="#b3">(Mani and Bloedorn, 1997)</ref>. They recognize that sentences cannot be examined independently, without reference to other sentences in the same article. A group from Cornell and Cogentex is looking at the related problem of "discrepancy detection," in particular those of numerical differences <ref type="bibr" coords="2,307.20,640.96,75.98,8.97" target="#b7">(White et al., 2001)</ref>.</p><p>The structure of the task in the Novelty Track is close to the work of Allen and that of Goldstein, although they had used a linear combination of both relevance and novelty qualities, but it requires separate computations. Our developing work views a document in a way close to Mani and Bloedorn, but unfortunately it could not be directly applied to this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>The query-based structure of the Novelty Track prohibited the direct use of our system, NIA. Queries contain only general statements about the topics, and a perfectly functioning NIA would return all the details in the set of documents as new. But we wondered if we could apply the Concept Sets and the syntactic analyzer to both the relevance and novelty parts of the Novelty Track. This strategy was problematic since NIA has no machinery to determine relevance to a given query. NIA is intended to track a topic or event over time and provide updates. It assumes 1.) that the input documents are clustered appropriately, and 2.) that the user cannot predetermine what aspects of the topic or event will be interesting. But, on the other hand, the exercise might offer much insight into the performance of the tools we are developing and might ultimately be more beneficial to us than trying to quickly patch together an information retrieval system.</p><p>The borrowed tools include the lexicon used to build the Concept Sets. It provides what we call "potential referential equivalents", that is words that can be used to refer to one another. In addition to it, we compiled a lexicon of associated words drawn from a background corpus of news and combined these elements in a rulebased system that made a relevant/not relevant decision on each sentence in the document cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sample Sets</head><p>Like the other participants, we had only four sample sets for development, and used those to design and tune the system. The prospects were challenging. It was obvious that the four samples were quite diverse. Further, it was difficult to guess about the test data since the track organizers intended to alter the wording of some of the topics in the actual test and since we had no idea which documents might be listed as the most relevant.</p><p>We also noticed in the sample sets that the annotators' tended to favor a few of the documents. Based on that observation, we built the system to automatically decide if a few documents strongly addressed the issue in the topic. Where that was the case, we drew all our relevant sentences from those central documents.</p><p>Finally, we developed the parameters our system uses by experimenting on the sample sets. We sought to balance the recall and precision on the sample sets, and we aimed to present summaries of reasonable size, given the examples, and avoided submitting either very small or very large summaries.</p><p>The sample sets themselves were interesting. Here are some observations we made from an initial look at the problem:</p><p>Hubble Strong performance here seemed to depend on a clear idea of what is and is not an accomplishment. There were some useful key words, like data and theories, but the set contained a number of off-topic articles that were not likely to discuss Hubble's accomplish, including those on a species of squirrel and on a big earth-bound telescope being built by the Europeans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mutual funds</head><p>The system needs to know what a predictor is. There is a conflict in the language. In the description it says "predictors of mutual fund performance (excluding issues of costs and yields)" and in the narrative it says "a documnet must contain at least one factor such as: rankings, risks, yields or costs". Our initial tests were not able to suggest a strategy for this set, but it was described as atypical.</p><p>mainstreaming The interesting aspect here was that the word mainstreaming rarely occurred in the document set ( 1% of the sentences), but only 3 times in the relevant sentences, forcing the system to rely on the terms "children", "impairments" as well as to have an understanding of "pro" and "cons".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mirjana Milosevic</head><p>Strong performance here was attainable simply by scanning for sentences that mention the woman's first name, or nickname Mira, . Other strategies diminished these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relevance</head><p>Most of our system-building effort went into the relevance part of the task. We settled on a rule-based approach, rather than a vector-space approach. We expected that most participants would be far more experienced in information retrieval methods and would be in a much better position to refine them to this task. Thus we viewed our submission as an opportunity to test unusual ideas that were more closely related to the thrust of our research. Admittedly, this gives our system a patchwork quality, but one that would hopeful provide valuable insight into alternative approaches.</p><p>A number of features are computed for each sentence, and sentences are selected if the rule is satisfied. We submitted five runs, using different combinations of rules and parameters. Development of this system was based almost entirely on four samples.</p><p>1. distance from a title word in a prominent role in a clause (target distance)</p><p>2. word match with a potential referential equivalent (equivalent count)</p><p>3. word match an associated word (associated count)</p><p>The first feature is binary, reflecting whether the current passage is near enough to the previous prominent mention in the document of a term that appears in the title. Passages were either clauses or sentences, and promiment means that the target word appears as a standalone NP before the verb.</p><p>The second and third features refer to the two lexicons mentioned above. The values are just raw counts. We observed that the clause chunks are uniformly short and that the appearence of both "equivalent" words and "associated" words is relatively rare.</p><p>We tried a number of other features, and ended up ignoring several. The ones retained were based on the various fields in the topics, such as "titles", "narratives". The three used are:</p><p>The lexicon that provides potential referential equivalents is a new version of the of a resource we have been using in NIA. There, it is used to build Concept Sets, which are are words linked semantically. In order to avoid the need to disambiguate among word senses, highly polysemous words are filtered out, and a distance constraint is imposed before words are grouped together in a Concept Set. Thus, as the text is scanned, the system checks to see if it belong to an existing set or if it will instantiate a new set. The function to accept a word for inclusion in the Relevance part is:</p><formula xml:id="formula_0" coords="4,313.60,211.15,192.50,44.80">¢¡ £¡ £¤ ¦¥ ¨ § © ! § #" %$ &amp;¤ ' )( 10 ¤ 0 ¤ 0 © ) 2 43 65 7 &amp;' 0 § © ) 5 8 % ( &amp;9 ¨@ 0 ¤ BA C § #D &amp;¤ %" % E' 0 ¤</formula><p>where senses is the count of WordNet senses, and dist is the number of clauses between and 8</p><p>the previous occurrence of a word in the same equivalence class.</p><p>Because the Novelty Track task required us to relate the words in a query to those in a document, we were unsure of how to modify technique, since the queries, that is the topics, are too short to allow the building of Concept Sets. In the end, we risked injecting noise into the decision-making and ignored the second condition for acceptance. We went forward with this strategy because it seemed to work reasonably well in tests conducted on the sample sets.</p><p>The raw equivalence lexicon is built mostly from WordNet <ref type="bibr" coords="4,347.72,426.16,81.75,8.97" target="#b4">(Miller et al., 1990)</ref>, using synsets, hypernyms and hyponyms. NIA uses nouns and verbs, but we included adjectives for this effort. In the future, the lexicon will be altered with the results of corpus statistics that we are in the process of gathering. It is not clear yet whether we will keep the adjectives.</p><p>The lexicon of associated words is based on cooccurrence patterns in a background corpus. The corpus we used was from Reuters in 1996 and might have added some noise to our submission. Using the underlying TREC collections used in the track might have been more effective here, but we wanted to test using an orthogonal corpus, since in NIA we will have no knowledge of future changes in the discussion of a particular topic or event. We also used a clause-level co-occurrence standard rather than a document-level standard, since the task examines and makes decisions on short passages -sentences, which are usually composed of one, two or three clauses. We used mutual information to measure the degree of relatedness between two words.</p><formula xml:id="formula_1" coords="5,129.50,132.10,96.90,36.40">¢¡ © ¤£ 5 ¦¥ @ A ¨ § © ¥ © ©£ 5 ¥ ¥ © ¤£ ¥ © ¥</formula><p>We also added an adaptive capability to our system, given the different types of topics in the Novelty Track. These automatically assess two characteristics of the document set and adjust the system's behavior to these. In previous research in multi-document summarization, we used a similiar technique in the DEMS summarizer, Dissimilarity Engine for Multi-Document summarization <ref type="bibr" coords="5,89.74,268.72,92.52,8.97" target="#b6">(Schiffman et al., 2002)</ref>, to good effect in the Document Understanding Conference 2002.</p><p>One adaptative method controls the value of the feature that check the target distance feature, the distance between the current passage and the last mention of a word in the topic title. We observed that topics did not always contain usable title words -like mainstreaming -so that the target distance would be self-defeating. For this, we measured the likelihood of finding any target word in the document set. If the total was below a threshold, we set the distance at such a large number that it no longer carried any weight.</p><p>The other adaptive method controls the number of documents that were examined. We noticed in the sample sets that in some cases a few documents dominated the selection of relevant sentences, suggesting that cluster contained some documents that were only tangentially related to the topic. In order to discern when this occurred, we used the lexicon of associated words. We computed the likelihood of finding words from any field in the topic and then computed the variance of these likelihoods across the documents in the set. If the "associated-words variance" was below a threshold, we concluded that most of them would contribute to the output of relevant sentences. Otherwise, we concluded that the document set contained some outliers that would be best to ignore.</p><p>One final strategy we adopted was to remove words that frequently appeared in a large number of topicswords like "relevant". To avoid having sentences accepted on the basis of these, we computed an inverse topic frequency value for all words in the 150 topics from which the test set would be drawn. These words were eliminated from the topics before the topics were expanded to include the referential equivalents and the associated words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Novelty</head><p>Unfortunately the relevance part of the task took most of the time we had alloted, and with a limited time left, we adopted a very simple duplication test. From the sample topics, there was little for a novelty detector to do. We first expand each sentence by adding the referential equivalents. We did this despite the risk of eliminating novel sentences because of the appearance of unrelated senses of polysynonymous words. As we considered new sentences, we computed how well the new sentence was covered by each previous sentence and rejected those that exceeded a threshold. The mechanism we developed for NIA would have required us to reference the original documents in order to examine the context of each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Overfitting</head><p>We submitted all five runs that we were allowed. All used the same structure outlined in Section 4, but with different parameters. Three of them were based on clauses, that is the features were computed on the basis of the clauses recognized by our clause-tagging tool. We used these to test whether the on-line adjustments had value. The runs marked cl35 and cl85 in Table <ref type="table" coords="5,520.30,449.68,5.03,8.97" target="#tab_0">1</ref> did not try to adapt to the document set. The numbers 35 and 85 refer to the percentage of documents from which the relevant selections were drawn from. The documents are ordered according to their distributions of associated words. The run marked clfx automatically selected either the .35 or .85 figure according the variance of the likelihood of finding an associated word in the documents.</p><p>The sent run computed the features over sentences, and the merg concatenated the sentence following any sentences that scored high, to test the possibility that segmenting documents might be a valuable idea. Both of these used the automative adaptation mechanism.</p><p>It appears that we were lulled by a painful instance of overfitting. The devlopment of our system was closely guided by its performance on three of the sample sets. The first, topic 303, was described as typical of the entire test. We didn't include topic 359 for several reasons. In our early experiments, it seemed to be impossible to match any of the human selection and futher more it contained a contradiction: Description 2 wants to exclude costs and yields, but the Narrative wants to include them.</p><p>Our results were disappointing even though we did not expect much at the outset. The organizers of the Novelty Track reported that human annotators tested against each other had achieved a score of ¢¡ ¤£ ¦¥ -this is the product of the standard measures of precision and recall - § ¡ £A " C¤ ©¨" C¤ C¡ E¤ C¡ £9 ¨@ @ . They said that the best submission was less than half that of the humans, but Table <ref type="table" coords="6,110.84,485.20,5.03,8.97" target="#tab_0">1</ref> clearly shows all of our runs were far below that.</p><p>If an oracle program were able to choose the best system for each topic, the combined score averaged together would be ¡ ¤£ on the relevant part and ¡ ¤£ on the novelty part. Since this score was a good deal better than the best system, no one system was consistently at the very top.</p><p>The topic sets also varied widely, and some were difficult for all systems, other much easier. Averaging the scores by all systems for each topics showed a wide range, indicating some sets were managable for a number of systems, while others were nearly impossible for all of them. Assuming that the average of all systems indicates the degree of difficult we have:</p><p>On average, the novelty task proved to be much harder. There was a striking drop off in the average scores. This is surprising since the annotators eliminated very few relevant sentences in creating their list of new sentences. In fact, a baseline that does nothing -that does not eliminate any relevant sentencewould have a precision of ¡ !¥ "£ and a recall of ¡ !¥ ¥ . (The recall appears to be short of £ #¡ ! because the relevant and new lists were swapped in two cases.) Just before the paper submission deadline, the Novelty Track organizers restated the results, using the standard F-measure instead of the product of precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>$ % '&amp; '</head><p>The recalculation raised the single-value scores of all groups, and squashed the results into a much narrower range for the automatic systems, as Table <ref type="table" coords="6,472.37,493.60,5.03,8.97" target="#tab_4">5</ref> shows. The recalcuation also tended to eliminate the size of the advantage to systems that generated larger summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Recent Experiments</head><p>To explore performance in the novelty part further, we counted duplicates found, rather than novel sentences found. In the formal task, the scores in the first part address the question of "How many of the relevant sentences can the system find?" The scores for the second part address the similar question of "How many relevant (and nonduplicative) sentences can the system find?" The question makes more senses in the relevance part where only a small portion of the sentences are judged relevant. In the four sample top-  ics given to participants, about 6% of the sentences were accepted as relevant. The situation was the reverse for the novelty side, where nearly all the relevant sentences were considered novel. In the test, the annotators removed only 106 of 1,347 sentences were removed as duplicative. With a lopsided test set, it is hard to beat the baseline of "do nothing." So we recast the question into "How many duplicative sentences can the system find?" We then computed precision and recall for a number of baselines, including a bag of words approach, TF*IDF, longest common subsequence, and Simfinder, another tool for measuring similarity between sentences developed at Columbia <ref type="bibr" coords="8,106.95,283.36,120.85,8.97">(Hatzivassiloglouet al., 2001)</ref>. Table <ref type="table" coords="8,258.48,283.36,5.03,8.97" target="#tab_5">6</ref> shows that our semantic module outperformed the other methods. We show the results when the parameters for the various methods made a reasonable number of selections. By making the duplication thresholds low enough, most of these methods will choose a large proportion of sentences as duplicates and achieve a high recall.</p><p>Here are descriptions of the methods presented in Table 6.. novcol Our semantic module applied to whole sentences.</p><p>sequent The longest common subsequence of words, as a percentage of sentence length.</p><p>wordbag A unweighted bag of words approach, using overlap.</p><p>similar The Simfinder utility.. tfidf TF*IDF metric 1 and computing cosine similarity. Document frequency values were taken from the set of articles for the topics.</p><p>random An extrapolation of random results by computed the expected value of 106 selections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>One positive lesson learned in this exercise is that the adaptive strategy appears to have considerable value. Without sufficient training data, it was impossible to</p><formula xml:id="formula_2" coords="8,84.72,658.85,122.58,13.40">1 ¢¡ ¤£ ¦¥ ¨ § © "! #¥ $ %© '&amp; ( ( ' "! #¥ $ %£ ") &amp; (</formula><p>explore and sharpen the technique, yet it clearly improved our results in the runs where it was applied, despite having only a rough idea of the parameters to use.</p><p>In addition the range of averages across the topics suggests that a one-size-fits-all approach is not the best.</p><p>Our experiments after the evaluation show there is a value using semantic information in detecting similarity and dissimilarity. This was not so clear about our application in the relevance part of semantic data -in the form of the lexicon of referential equivalents. We were hampered because our system was unable to apply the lexicon in the way it is used in our NIA system, where the expansion of the sets is limited by the context in the documents to be summarized. Since the topics were too small to provide any context, the lexicon was used without distance constraints. But in the more straightforward task of detecting duplication, the semantic information without those constraints An assessment of using associated words -those obtained by co-occurrence studies -was clouded by the fact that the data was drawn from a much different collection of background documents. This was due to a lack of time. We had a collection of Reuters news wire already parsed, and would have had to delay experimentation if we had waited to parse the TREC collections used in the Novelty Track. We are planning to create a new lexicon based on the TREC documents to compare against the results here.</p><p>The Novelty Track also confirmed how difficult the task is. The subjectivity of the annotation greatly complicates the conclusions that can be drawn. Judging from the cross annotator scores, inter-annotator agreement was quite low, and the choice of annotator may have had a large effect on the results in various sets.</p><p>We were also struck by the fact that many, but certainly not all, topics included some instruction about was not relevant. We blocked those that were found in such negative sentences from being expanded if they were not already found in the positive sentences -however these were few in number in the sample sets. We did test a feature of noting the presence of negative terms in the passages, but where it did affect the outcome, it was detrimental as often as helpful. Yet, we think the idea of trying to categorize the queries, that is the topics, is worth further experimentation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,344.60,665.92,180.75,8.97"><head>Table 1 :</head><label>1</label><figDesc>Table2shows how the system performed on Precision and Recall of our five runs, humans and random the relevance part. The results seemed to be sufficient on this difficult problem. We didn't think we had a top system, but were satisfied with what we saw.</figDesc><table coords="6,186.24,128.80,225.00,120.09"><row><cell></cell><cell></cell><cell cols="2">Relevant</cell><cell></cell><cell>New</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>P*R</cell><cell>P</cell><cell>R</cell><cell>P*R</cell></row><row><cell>cl35</cell><cell cols="2">.07 .04</cell><cell>.006</cell><cell cols="2">.07 .04</cell><cell>.005</cell></row><row><cell>cl85</cell><cell cols="2">.09 .07</cell><cell>.009</cell><cell cols="2">.08 .05</cell><cell>.007</cell></row><row><cell>clfx</cell><cell cols="2">.07 .12</cell><cell>.012</cell><cell cols="2">.07 .09</cell><cell>.009</cell></row><row><cell>sent</cell><cell cols="2">.11 .09</cell><cell>.012</cell><cell cols="2">.12 .09</cell><cell>.012</cell></row><row><cell>merg</cell><cell cols="2">.11 .15</cell><cell>.020</cell><cell cols="2">.09 .10</cell><cell>.013</cell></row><row><cell>humans</cell><cell></cell><cell></cell><cell>.191</cell><cell></cell><cell></cell><cell>.170</cell></row><row><cell>random</cell><cell></cell><cell></cell><cell>.006</cell><cell></cell><cell></cell><cell>.004</cell></row><row><cell>best system</cell><cell></cell><cell></cell><cell>.095</cell><cell></cell><cell></cell><cell>0.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,76.80,231.76,440.12,132.57"><head>Table 2 :</head><label>2</label><figDesc>Precision and Recall achieved by our system across the three sample topics for detection of relevance.</figDesc><table coords="7,233.28,268.96,130.92,95.37"><row><cell></cell><cell cols="2">Relevance</cell><cell></cell></row><row><cell cols="4">Easiest Topics Hardest Topics</cell></row><row><cell cols="4">Topic Score Topic Score</cell></row><row><cell>368</cell><cell>0.262</cell><cell>312</cell><cell>0.019</cell></row><row><cell>397</cell><cell>0.247</cell><cell>381</cell><cell>0.018</cell></row><row><cell>394</cell><cell>0.193</cell><cell>305</cell><cell>0.018</cell></row><row><cell>365</cell><cell>0.189</cell><cell>432</cell><cell>0.017</cell></row><row><cell>369</cell><cell>0.167</cell><cell>420</cell><cell>0.016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,129.12,368.90,339.23,249.10"><head>Table 3 :</head><label>3</label><figDesc>Average' scores on the Relevance part show a wide range of difficulty.</figDesc><table coords="7,149.28,414.16,295.38,203.85"><row><cell></cell><cell cols="2">Novelty</cell></row><row><cell cols="4">Easiest Topics Hardest Topics</cell></row><row><cell cols="2">368 0.127</cell><cell>445</cell><cell>0.005</cell></row><row><cell cols="2">397 0.108</cell><cell>312</cell><cell>0.005</cell></row><row><cell cols="2">394 0.103</cell><cell>432</cell><cell>0.003</cell></row><row><cell cols="2">365 0.089</cell><cell>377</cell><cell>0.002</cell></row><row><cell cols="2">364 0.080</cell><cell cols="2">420 0.0002</cell></row><row><cell>Table 4: Five highest average</cell><cell>¨</cell><cell cols="2">scores in the Novelty Part of the task.</cell></row><row><cell></cell><cell cols="2">F-Measures</cell></row><row><cell cols="4">Summary generator F-measure relevant</cell></row><row><cell>Humans</cell><cell></cell><cell></cell><cell>0.371</cell></row><row><cell>Top Sys</cell><cell></cell><cell></cell><cell>0.235</cell></row><row><cell>Best Novcol</cell><cell></cell><cell></cell><cell>0.126</cell></row><row><cell>Random</cell><cell></cell><cell></cell><cell>0.040</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.00,630.64,453.10,32.97"><head>Table 5 :</head><label>5</label><figDesc>Restatement of some results on the Relevant part of the task in terms of the standard F-measure. The 10 top scores plus human and random results were distributed by NIST before the paper-submissions were due. The Novcol were recomputed.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,72.00,126.88,453.30,142.89"><head>Table 6 :</head><label>6</label><figDesc>A comparison of results on duplication detection between the semantic module and several word-based methods and a system that would choose at random. Note "do-nothing" gets zero because it selects no duplicates to reject.</figDesc><table coords="9,142.08,126.88,313.44,97.29"><row><cell></cell><cell cols="3">Matched Sys Tries Hum Picks</cell><cell>Prec</cell><cell>Recall</cell><cell>P*R</cell></row><row><cell>novcol</cell><cell>34</cell><cell>139</cell><cell>106</cell><cell cols="3">0.2446 0.3208 0.0785</cell></row><row><cell>sequent</cell><cell>30</cell><cell>156</cell><cell>106</cell><cell cols="3">0.1923 0.2830 0.0544</cell></row><row><cell>wordbag</cell><cell>24</cell><cell>107</cell><cell>106</cell><cell cols="3">0.2243 0.2264 0.0508</cell></row><row><cell>similar</cell><cell>25</cell><cell>158</cell><cell>106</cell><cell cols="3">0.1582 0.2348 0.0373</cell></row><row><cell>tfidf</cell><cell>14</cell><cell>126</cell><cell>106</cell><cell cols="3">0.1111 0.1321 0.0147</cell></row><row><cell>random</cell><cell>8.3</cell><cell>106</cell><cell>106</cell><cell cols="3">0.0783 0.0783 0.0061</cell></row><row><cell>do-nothing</cell><cell>0</cell><cell>0</cell><cell>106</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,72.00,310.72,218.15,8.97;9,82.08,321.52,207.83,8.97;9,82.08,332.56,161.10,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,109.16,321.52,143.42,8.97">Temporal summaries of news topics</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vikas</forename><surname>Khandelwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,272.17,321.52,17.74,8.97;9,82.08,332.56,156.76,8.97">Proceedings of the ACM-SIGIR Conference</title>
		<meeting>the ACM-SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,351.52,218.15,8.97;9,82.08,362.56,207.84,8.97;9,82.08,373.60,208.07,8.97;9,82.08,384.40,208.11,8.97;9,82.08,395.44,46.02,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,189.72,362.56,100.21,8.97;9,82.08,373.60,123.93,8.97">Multi-document summarization by sentence extraction</title>
		<author>
			<persName coords=""><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vibhu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Kantrowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,229.27,373.60,60.88,8.97;9,82.08,384.40,208.11,8.97;9,82.08,395.44,41.83,8.97">Proceedings of ANLP/NAACL-2000 Workshop on Automatic Summarization</title>
		<meeting>ANLP/NAACL-2000 Workshop on Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,414.40,217.93,8.97;9,82.08,425.44,208.10,8.97;9,82.08,436.48,208.07,8.97;9,82.08,447.28,208.11,8.97;9,82.08,458.32,208.11,8.97;9,82.08,469.36,106.43,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,247.95,436.48,42.20,8.97;9,82.08,447.28,184.28,8.97">Simfinder: A flexible clustering tool for summarization</title>
		<author>
			<persName coords=""><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Judith</forename><forename type="middle">L</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melissa</forename><forename type="middle">L</forename><surname>Holcombe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,82.08,458.32,208.11,8.97;9,82.08,469.36,101.91,8.97">Proceedings of the NAACL 2001 Workshop on Automatic Summarization</title>
		<meeting>the NAACL 2001 Workshop on Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,488.32,218.42,8.97;9,82.08,499.36,208.08,8.97;9,82.08,510.16,208.06,8.97;9,82.08,521.20,109.54,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,264.52,488.32,25.90,8.97;9,82.08,499.36,208.08,8.97;9,82.08,510.16,35.15,8.97">Multidocument summarization by graph search and matching</title>
		<author>
			<persName coords=""><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Bloedorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,135.53,510.16,154.61,8.97;9,82.08,521.20,84.87,8.97">Proceedings, American Association for Artificial Intelligence</title>
		<meeting>American Association for Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,540.16,218.17,8.97;9,82.08,551.20,208.34,8.97;9,82.08,562.24,208.09,8.97;9,82.08,573.04,207.81,8.97;9,82.08,584.08,119.35,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,113.00,562.24,177.17,8.97;9,82.08,573.04,46.52,8.97">Introduction to WordNet: An on-line lexical database</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Derek</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,136.97,573.04,152.92,8.97;9,82.08,584.08,55.78,8.97">International Journal of Lexicography (special issue)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="312" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,603.04,218.37,8.97;9,82.08,614.08,208.08,8.97;9,82.08,624.88,208.12,8.97;9,82.08,635.92,154.11,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,177.70,603.04,112.67,8.97;9,82.08,614.08,208.08,8.97;9,82.08,624.88,103.64,8.97">A common theory of information fusion from multiple text sources, step one: Cross-document structure</title>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,213.21,624.88,76.99,8.97;9,82.08,635.92,149.69,8.97">1st ACL SIGDIAL Workshop on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,654.88,217.90,8.97;9,82.08,665.92,207.83,8.97;9,317.28,292.96,208.54,8.97;9,317.28,304.00,95.39,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,140.08,665.92,149.83,8.97;9,317.28,292.96,40.96,8.97">Experiments in multidocument sum-marization</title>
		<author>
			<persName coords=""><forename type="first">Barry</forename><surname>Schiffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,376.97,292.96,148.85,8.97;9,317.28,304.00,91.05,8.97">Proceedings of the Human Language Technology Conference</title>
		<meeting>the Human Language Technology Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,307.20,322.72,218.44,8.97;9,317.28,333.76,208.07,8.97;9,317.28,344.80,208.08,8.97;9,317.28,355.60,208.08,8.97;9,317.28,366.64,208.29,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,486.51,333.76,38.83,8.97;9,317.28,344.80,208.08,8.97;9,317.28,355.60,126.07,8.97">Detecting discrepancies and improving intelligibility: Two preliminary evaluations of riptides</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krii</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daryl</forename><surname>Mccullough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,464.96,355.60,60.40,8.97;9,317.28,366.64,165.38,8.97">Proceedings of the Document Understanding Conference</title>
		<meeting>the Document Understanding Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>DUC01</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
