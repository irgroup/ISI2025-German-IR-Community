<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,121.62,98.88,352.07,13.08">TREC-11 Experiments at CAS-ICT: Filtering and Web</title>
				<funder ref="#_UyaqBeU">
					<orgName type="full">Institute Youth Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,110.70,139.42,40.16,7.85"><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
							<email>hbxu@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,157.57,139.42,47.16,7.85"><forename type="first">Zhifeng</forename><surname>Yang</surname></persName>
							<email>zfyang@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.19,139.42,33.55,7.85"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<email>wangbin@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.62,139.42,26.39,7.85"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<email>yliu@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.99,139.42,36.25,7.85"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.46,139.42,27.34,7.85"><forename type="first">Yue</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.79,139.42,32.65,7.85"><forename type="first">Zhe</forename><surname>Yang</surname></persName>
							<email>yangzhe@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.90,139.42,45.85,7.85"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,451.34,139.42,33.36,7.85"><forename type="first">Shuo</forename><surname>Bai</surname></persName>
							<email>bai@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,121.62,98.88,352.07,13.08">TREC-11 Experiments at CAS-ICT: Filtering and Web</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AB4EB39457B94EF3F0B54B0C0622FCD8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TREC-11</term>
					<term>Filtering</term>
					<term>Web track</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CAS-ICT took part in the TREC conference for the second time this year and we undertook two tracks of TREC-11. For filtering track, we have submitted results of all three subtasks. In adaptive filtering, we paid more attention to undetermined documents processing, profile building and adaptation. In batch filtering and routing, a centroid-based classifier is used with preprocessed samples. For Web track, we have submitted results of both two subtasks. Different factors are considered to improve the overall performance of our Web systems. This paper describes our methods in detail.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>CAS-ICT took part in the TREC conference for the second time this year, and we have submitted results of filtering track and Web track.</p><p>For filtering track, we undertook all three subtasks. Our adaptive filtering system is still based on VSM. Our Rocchio-like profile adaptation algorithm puts stress on the undetermined documents and some strategies are proposed for T11U or T11F optimization. Four runs have been submitted for evaluation: all of them are optimized for T11U measure, but in three of them T11F measure is also considered at the same time. In batch filtering and routing, we use a centroid-based classifier with preprocessed samples. Two batch filtering runs and two routing runs have been submitted for evaluation. In all of our filtering experiments, we do not use any other resources except the New Reuters Corpus.</p><p>For Web track, we undertook both the Named Page Finding task and the Topic Distillation task. Our system is based on SMART(ftp://ftp.cs.cornell.edu/pub/smart). In the former task, we try to integrate different factors to improve the overall system performance. In the latter task, a variant HITS algorithm is used to the top n results returned by SMART. Five Named Page Finding results and three Topic Distillation results have been submitted for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Filtering</head><p>For filtering track, we undertook all three subtasks, but we paid more attention to the adaptive filtering task. Batch filtering and routing tasks are used to test our new classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adaptive Filtering 2.1.1 Introduction</head><p>The total 100 topics used in the filtering task this year can be divided into two sets: the first 50(R101-R150) topics are called assessor topics, which are hand-built by NIST assessors, and the last 50(R151-R200) topics are called intersection topics, which are derived from Reuters category intersections. The two sets have been evaluated separately.</p><p>New Reuters Corpus (http://about.reuters.com/researchandstandards/corpus/) is still used this year, but the training set and testing set are different with TREC-10. The first 83,650 documents are used for training (training set) and the remaining about 720,000 documents for testing (testing set). The official adaptive filtering measures are utility (T11U, scaled using Ault's formula), and F-beta (T11F, beta = 0.5). The former is a linear utility measure and the latter is a kind of F-measure. Additionally, set precision and set recall measures are also reported in the final results. In the adaptive subtask, only three positive samples in training set are given for each topic, and the goal is to retrieve relevant documents one by one from the coming testing documents stream and get maximum T11U or T11F value at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">System Description</head><p>Last year, we have built an adaptive filtering system, which consists of two components: the profile initialization component and the profile adaptation. This year we made some improvement based on this system, in particular, in the profile initialization and optimization modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Initialization</head><p>Our initialization process includes common operations such as term tokenization, stop words elimination, stemming, TF and IDF computation. Each topic is treated as a document and processed in the same way. The initial profile vector can be obtained by summing up the topic vector and the three positive documents vectors with different weight. Meanwhile, we set the initial threshold by computing the similarities between the initial profile and all the documents in the training set.</p><p>Since we can't use the IDF statistics of testing set till now, we take the IDF statistics of the training set as an alternative for term weighting. Ideally, we should update the IDF statistics when retrieving new documents from the testing documents stream. But our previous experiments have indicated that it does not seem to improve the overall filtering performance. Therefore, we use the IDF statistics of the training set without any modification all over our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Term selection</head><p>Last year, we applied a new method for feature selection, which can be regarded as a variation of Mutual Information. The final results indicated that our method is successful when the topic is a single Reuters category.</p><p>However, each topic of this year has been changed into a natural language statement or an intersection of some Reuters categories. Our experiment shows that the method does not work well this year. Several experiments show that the simple term selection according to the TF and DF values is a good choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Profile initialization</head><p>For each topic, the profile vector (denoted as P v</p><p>) is the weighted sum of the topic vector (denoted as T v ) and the feature vector (denoted as F v</p><p>), which is the sum of the initial three positive documents vectors. The formula is:</p><formula xml:id="formula_0" coords="2,134.16,665.24,354.03,21.95">T F P v v v * * β α + = (2.1)</formula><p>In our experiment, we set α=1,β=3 to give prominence to the topic vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity computation</head><p>We still use the vector cosine distance to compute the similarity between a profile vector ( i P v ) and a document vector( ). TFIDF value is used in our system, which is computed by</p><formula xml:id="formula_1" coords="3,91.62,72.94,138.15,570.55">j D v ) 1 log( * ) 1 ) (log( i TF + +      - - + = + D P D P D P P n n n n 1 v v v v i DF N ′ * * *</formula><p>, where N is the number of the total documents in the training set.</p><formula xml:id="formula_2" coords="3,90.00,139.15,135.89,608.40">v v v β β α ∑ + + = 0 1 n P P * v v α 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Adaptation</head><p>For each topic, after initializing the profile and the threshold, we can scan documents one by one from the testing set. If the similarity between the profile and the document is higher than the threshold, the document is retrieved, else not. Then we check the answer list of the testing set to find whether the document is really relevant or not. With this information, we can take some kind of adaptation to improve the system performance. The adaptation may include threshold updating and profile updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Threshold adaptation</head><p>As we know, the goal of the TREC-11 adaptive filtering system is to get maximum T11U or T11F. Therefore, we adjust the threshold for T11U optimization or T11F optimization.</p><p>For T11U, our direct goal is to avoid negative utility value for each topic. When the utility value becomes negative during filtering, which means the system retrieves too many non-relevant documents, we augment the threshold to reduce the number of retrieved documents. Another optimization strategy we take is to improve the precision while the recall can't be greatly reduced.</p><p>For T11F, our goal is to avoid retrieving zero "relevant" documents. We reduce the threshold when the system retrieves zero documents at an interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Profile adaptation</head><p>As the filtering task indicates, each profile vector represents a user's interest. After retrieving more and more relevant or non-relevant documents, we can get more and more useful information about the user's interest, which can help us adapt the profile. Our profile adaptation includes positive adaptation, negative adaptation and adaptation based on undetermined documents. For positive adaptation, we add the positive documents vectors to the old profile vector with weight α. For negative adaptation, we subtract the negative documents vectors from the old profile vector with weight β. For adaptation based on undetermined documents, we set a relative high threshold (we use t=0.6) to filter the retrieved undetermined documents. Those retrieved documents that have similarity below t are regarded as pseudo-negative documents and treated as real negative documents. A pseudo-negative document is used in negative with smallerβvalue. When retrieving the n+1 th document D n+1 , we can adapt the nth profile to the n+1 th profile according to the following formula:</p><formula xml:id="formula_3" coords="3,226.98,597.71,261.20,42.96">&lt; + + + + + + t D P D D n n n n n n n ) , sim( and otherwise irrelevant is if relevant is if 1 1 1 1 1 1 r v (2.2)</formula><p>Thus after we have retrieved n+1 documents, all the retrieved documents are divided into four sets: the relevant set denoted as {D + }, the irrelevant set {D -}, the undetermined but pseudo-negative set {D u -} and the remaining documents set {D u + }. We do not use {D u + } in the adaptation. Then the new profile vector is computed by：</p><formula xml:id="formula_4" coords="3,204.60,721.19,283.58,35.01">∑ ∑ - - + ∈ ∈ ∈ ′ - - } { } { } { * * u k i i D D k D D j D D i D D D v v v β β (2.3) Formula (2.</formula><p>3) is some kind of the Rocchio [15] algorithm except one point: we do not compute the centroid of a document set and regard all documents in each set as one vector. In other words, we emphasize the retrieved documents and endow them the ability to adjust the profile vector quickly. As in last year, we investigate the values of α, β and β ' . In our experiments, we set α=1, β=1.8 and β ' =1. <ref type="bibr" coords="4,155.49,139.03,4.35,9.16" target="#b2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Undetermined documents processing</head><p>In TREC-11, the relevance of most documents in the testing set is unknown to the system. In order to get more feedback information, we make some experiments on the undetermined documents.</p><p>Experiment 1: Ignoring the undetermined documents when filtering, we adjust the threshold only according to the relative proportion between the known relevant documents and irrelevant ones. But there is an important presupposition that such a distribution is the same in the undetermined documents. Unfortunately, we can' t prove this presupposition.</p><p>Experiment 2: A simple idea is that if we could know the real relevance of all documents in the testing set, the adaptation strategy proved effective in TREC-10 can still be applied. Therefore, we make a positive centroid and a negative centroid with the retrieved relevant and irrelevant documents during retrieving the testing set. When retrieving an undetermined document, we judge its relevance by computing its distance from the positive centroid and the negative centroid. Those undetermined documents that are nearer to the positive centroid will be treated as real relevant documents, while others will be treated as irrelevant documents. Thus we can simulate a situation as in TREC-10. This method allows the system retrieving plenty of "relevant" documents, which is helpful to the recall but against the precision. It seems that the initial values of the positive centroid and negative centroid greatly affect the judgment of undetermined documents. The positive centroid can be made by the known three positive samples, but we can't make a good negative centroid because we haven't any negative samples.</p><p>Experiment 3: Suppose the answer list has provided most real relevant documents in the testing set, we treat all or most of the undetermined documents as irrelevant documents. As we've introduced above, a threshold t can be used to filter the undetermined documents, those have similarity below t will be treated as irrelevant documents. The discussion of TREC-11 filtering mailing list shows that such a supposition is reasonable. With this method, we can control the retrieved "relevant" documents effectively, which is helpful to the precision. But when the number of real relevant documents in the testing set is big, such a system will suffer a heavy loss.</p><p>Of the three methods above, we apply the third one finally, partly suggested by the discussion of TREC-11 filtering mailing list. The results are encouraging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Evaluation Results and Analysis</head><p>We have submitted four adaptive filtering runs: all for T11U optimization, in three of them we make balance between T11U and T11F. ICTAdaFT11Ud is optimized for recall, avoiding the heavy loss of relevant documents. As to the optimization method, we use local maximum optimization strategy at every adaptation interval to obtain the holistic maximum. We also adopt a method to avoid zero return at next interval by learning from the current adaptation interval.</p><p>Table <ref type="table" coords="4,137.89,700.63,4.38,9.16" target="#tab_1">2</ref>.1 shows the results of the 50 assessor topics. Table <ref type="table" coords="4,371.57,700.63,4.36,9.16" target="#tab_1">2</ref>.2 shows the results of the 50 intersection topics. Table <ref type="table" coords="4,199.21,716.23,4.37,9.16" target="#tab_1">2</ref>.3 is the evaluation results of all 100 topics. Of the assessor topics, the system exhibits a good performance. But of the intersection topics, the system behaves badly.   We had partly noticed the problem of intersection topic in our experiment. It seems that the intersection topic itself makes the VSM unsuccessful. After comparing the assessor topics with the intersection topics, we guess the reason maybe that the natural language style of the assessor topics makes them appropriate to be represented and computed with vectors, while the intersection topics are not, because the different dimensions of an intersection topic vector have no internal relations as organic as those of a natural document. Another reason we guess is that there are few relevant documents on each topic in the testing set that can be used to adjust the profile vector. In TREC-10 our system has proved suitable for "big" topics but not so for "small" topics. The results of last year have also proved that as long as enough relevant documents can be provided, on the intersection-like topics we can still obtain good performance. Although in such circumstances we may not make a good initial profile vector, enough feedback can greatly adapt it to the best position. But this year the case is different. We don't have so many relevant documents, so the weakness of VSM on the intersection topics becomes distinct. An evidence is that our system still gets better scores on most intersection topics with relative more relevant documents, such as topic R164, R175, R185, R186 and R199.</p><p>In next step, our goal is to find a new way to effectively process the semi-automatically made intersection topics. We believe such topics represent the trend in future and are worthy of much more efforts. Accomplishment of the efforts will to some extent lighten assessors' burden in the filtering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Batch Filtering and Routing Subtasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Text Representation</head><p>In our batch and routing filtering system, when preprocessing the documents, we give additional prominence to the words that occur in the &lt;title&gt; field and we only use TF weight in the vector representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Samples Preprocessing</head><p>We believe some samples in the training set are not good enough to train the classifier, so we want to eliminate them beforehand. Indeed, samples have different weights since features of documents have different weights. Importance of samples and importance of features are closely related:</p><p>An important sample contains many important features;</p><p>An important feature appears in many important samples; We calculate the weights of samples as following: Let A mn is the matrix of the feature frequency in each sample, m is the number of the documents and n the number of the features. a ij is the frequency of the jth feature in the ith sample.</p><p>The weight vectors of samples and features are respectively W</p><p>. Their initial values are W f (0) and W t (0)</p><p>, with each component set to 1.</p><formula xml:id="formula_5" coords="6,90.66,296.84,412.99,48.82">' ) , , , ( 2 1 m f f f f W W W L = )' , , , ( 2 1 n t t t t W W W W L =</formula><p>The formulas below are to compute the weights. It can be proved that the computing process is convergent.</p><formula xml:id="formula_6" coords="6,132.42,389.16,358.60,71.90">∑ = + = m i k f ij k t i j W A W 1 ) ( ) 1 ( * (2.4) ∑ = + + = n j k t ij k i f j W A W 1 ) 1 ( ) 1 ( * (2.5) (j = 1, 2, …, n，i = 1, 2, …, m)</formula><p>After computing the weights of all samples, for each topic, we remove the lowest 10% samples and use the remaining samples to train the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Training</head><p>The system uses Rocchio method in the training process. For topic i, its representative feature vector r is calculated as following: In test process, those documents with high cosine distance to i P r are retrieved to form the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Evaluation Results and Analysis</head><p>We have submitted two batch-filtering runs and two routing runs. All of them are optimized for T11U. The only difference between the two runs are thresholds and the parameter β in the formula <ref type="bibr" coords="6,125.81,743.83,18.99,9.16">(2.6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Filtering</head><p>The evaluations of batch results are shown in Table <ref type="table" coords="7,331.59,92.23,3.94,9.16" target="#tab_1">2</ref>.4. Table <ref type="table" coords="7,376.20,92.23,4.39,9.16" target="#tab_1">2</ref>.4 shows that in each run, the scores of T11U and T11F are close to medians. For the first 50 topics, we get a set precision higher than the median, but the set recall is lower than it. For the last 50 topics, we set a very strict threshold to avoid T11U becoming negative, because the baseline of T11U is 0.333. As a result, the scores of T11F, Set Precision, and Set Recall are all very low. Since we have set the same threshold for all 100 topics, we think the results show that the threshold for every topic should be different.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing</head><p>We set a lower threshold to get 1000 documents for each topic to form the routing results.</p><p>The only difference between the two runs is the parameter β. We can see that all of our results are similar to the medians. We think this is because we only set one same threshold for all topics and lack an effective parameter optimization method. We will try to research on automatic parameter optimization methods.</p><p>In the future, we have a lot of work to do to improve our work. For feature selection, we want to use N-Gram to add more terms to represent the documents. For the last 50 topics, we have tried to use KNN to improve the classification results. To our surprise, its result is much worse than the Rocchio method. We will research on the phenomenon and try more complex methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Web Track 3.1 Introduction</head><p>Last year we took part in TREC for the first time and we only submitted four runs for the ad hoc task. This year we submitted runs for both two tasks. This year, Web track consists of two new subtasks: the Named Page Finding task, which is introduced to investigate methods for finding a particular page that has been named by the user, and the Topic Distillation task, which is introduced to investigate methods for finding key resources in a particular topic area. In the former task, the system should return a single named page as the result. For instance, for the query "passport application form", the correct answer should be the page travel.state.gov/dsp11.pdf, which contains the electronic copy of requested form. In the Topic Distillation task, a single relevant document is not important any more. The concept resource is introduced as the basic element of results and judgments. The test collection of this year's Web track is changed to .Gov data set which substitutes Wt10g used in previous years.</p><p>Though the Web track tasks have been significantly modified, the basis of experiments is still the traditional IR systems. In TREC 2001 we investigated the effectiveness of the combination of classical Boolean model and probabilistic model in the ad hoc task. We also investigated methods that make use of link information between pages in the same task. Neither of the results was as good as we had expected. So this year we decide to adopt vector space model and to make use of only text contents and internal structure of pages. Our retrieval system is based on SMART. In order to deal with large data set such as Wt10g and .Gov test collection, we modified the basic SMART system, and the Lnu-Ltu weighting method was added to the system. This method has been proven to be very effective and efficient in our experiments. The classical weighting methods such as lnc-ltc do not behave well in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Named Page Finding Task</head><p>As introduced above, the goal of Named Page Finding task is to find appropriate page(s) named by users. It is rather close to a special kind of user requirement, i.e., finding a few documents that precisely meet the information need of users. The query "passport application form" is an example. Another one is the query "table of contents gnu make manual", by which a user would like to find the exact page that is the table of contents of GNU make manual. By analyzing these examples we have found some features that can be utilized.</p><p>Firstly, the content-based ranking score of traditional IR system is still the most important factor in Named Page Finding. If we assign the content-based score a less important coefficient in result merging process that will be described below, the final results will be worse. This can be explained if we notice that single term is more important in Named Page Finding task than in ad hoc task. This task pays more attention to precision than to recall. Only those pages that contain all or most of the query terms would have high possibility of meeting information need implied by the query in, thus they would have higher content-based scores than most of the irrelevant documents. Certainly some of irrelevant documents will also have high content-based scores, but we will enhance the scores of relevant documents by result merging process.</p><p>Secondly, the internal structure of documents will give us plenty of information. As the name of task suggests, query terms of Named Page Finding task are the names of relevant documents. Usually they are precise representations of topics. They should more possibly appear in important positions such as document title, beginning sentences of paragraphs and section headers, or display in a striking manner, for example a bold, italic, and large size font face. In such situation authors of documents have explicitly defined them as important terms. We can get a lot of relevance information by comparing the query terms with them. Besides this there is another reason why the method is especially useful for the task. Queries in ad hoc task are often about general topics. They must be described by natural language so that people can understand the information need under which the queries are developed. So they are prone to ambiguity. Correspondingly the relevant documents cannot be named clearly and easily. On the contrary, the information need of Named Page Finding task can be very easily understood, even without extra descriptions, so authors and searchers of the same documents will in the gross adopt the same terms as topic descriptions. The Homepage Finding task in last year's web track can be regarded as a kind of Named Page Finding task. In fact, when we added the phrase "home page" to the original queries we got obvious improved results. In our contrast experiment, ad hoc runs using document structure information gave poor results whose average precisions are too low to be mentioned while Homepage Finding runs gave fairly satisfactory results.</p><p>The last factor we have proven to be effective for the Named Page Finding task is anchor texts of documents. They act as almost the same role as the second factor. They can be regarded as names given by referrers to target documents. When the target documents can be easily named and referrers adopt the same names widely, retrieval results using the names are fairly satisfactory.</p><p>As we have stated above we believe that Homepage Finding task is a special kind of Named Page Finding task. So except some special methods for Homepage Finding such as analysis of URL depth, the methods that are effective for Homepage Finding should also be effective for Named Page Finding. We ran our experiments on Wt10g data set using topics and qrels developed for the Homepage Finding task to find the most optimized parameters. The results are shown in Table <ref type="table" coords="9,116.66,279.43,4.38,9.16" target="#tab_5">3</ref>.1 and Table <ref type="table" coords="9,178.55,279.43,3.94,9.16" target="#tab_5">3</ref>.2. We then applied the same system to the .Gov data set and Named Page Finding task. The experimental results that we observed have proven to be satisfactory.</p><p>We use the linear result merging method to get the last result of Named Page Finding task. The merging formula is</p><formula xml:id="formula_7" coords="9,132.84,337.85,355.34,12.84">) ( * ) ( * ) ( * ) ( p w p w p w p W a s c γ β α + + = (3.1)</formula><p>Where w c (p) is the content weight of page p, w s (p) is the weight from structure information, w a (p) is the weight from the anchor text of page p and α,β,γ are their coefficients. In our experiments only the titles of documents are used as structure information. The evaluation results are shown in Table <ref type="table" coords="9,115.92,404.23,3.95,9.16" target="#tab_5">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Topic Distillation Task</head><p>As described in the TREC-2002 Web Track Guideline, a key resource might be:</p><p>The home page of a site dedicated to the topic. The main page of a sub-site (part of a site) dedicated to the topic. (If there are several relevant pages but no main page linking them, then the individual pages must be judged on their own merit.)</p><p>A highly useful html, pdf, doc, ps page dedicated to the topic (should be an outstandingly useful page). Return the page's URL.</p><p>A highly useful page of links (hub page) on the topic. Return its URL. A relevant service e.g. perhaps http://www.nasa.gov/search/ for the NASA topic. Except the last two cases key resources are some important pages inside individual sites. Our first experiment was based on HITS algorithm. We submitted queries to SMART and retrieved ranked page lists, and then applied HITS to every group of pages coming from the same site. We extracted the page that had the maximum Hub+Authority value from each group of pages and added them to the final result. We found that the average result of this method was disappointing, partly because many Hub and Authority pages computed by HITS cannot meet the definition of key resource. Our last experiment on this task was based on a simple idea. After the first retrieval, we scanned the page list. If we found a page's url containing the other's, we then re-weighted the latter page by adding the former's weight to the latter's. After re-weighting the weight of a certain result page x is Where p is a page whose url string contains x's, w p is the content weight of page p and r p is the rank of page p. The run icttd2 is based on this approach, and ictted3 is based on icttd2 plus some additional re-weighting methods. The evaluation result is shown in Table <ref type="table" coords="10,424.83,466.63,3.95,9.16" target="#tab_5">3</ref>.4.</p><p>The run icttd1 is a baseline run produced by our retrieval system. It is the best one among the three runs. It seems that our re-weighting methods are not so effective as we have expected. We believe that more attentions should be paid to the instances of key resources given by the TREC qrels so that characters of them can be found. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We've participated in the TREC conference for two times. By communicating with the researcher all over the world, we've learned more. We've got many experiences in English information processing, which will benefit us greatly in our Chinese information processing.</p><p>TREC not only advances our research on IR, but also enlighten our insights. From here, we can find our advantages and disadvantages comparison to the foreign friends going the same way. We are glad to take part in TREC continuously.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,129.60,538.66,2.20,6.89;6,123.84,531.54,8.27,11.80;6,190.14,555.17,3.83,8.56;6,160.62,555.17,3.83,8.56;6,168.54,547.15,6.57,14.67;6,144.72,547.15,6.57,14.67;6,184.26,551.34,7.32,10.45;6,154.68,551.34,7.32,10.45;6,132.24,551.34,7.32,10.45;6,137.94,557.61,1.94,6.10;6,186.24,539.31,4.90,15.56;6,156.72,539.31,4.90,15.56;6,134.28,539.31,4.90,15.56;6,177.78,546.47,6.57,15.51;6,149.40,553.71,5.61,17.81;6,466.08,552.31,20.18,9.16;6,389.04,553.71,5.61,17.81;6,111.00,570.31,28.02,9.16;6,165.30,570.31,213.45,9.16;6,153.30,571.86,4.39,9.79;6,147.30,567.48,8.38,11.96;6,392.94,571.86,4.39,9.79;6,386.94,567.48,118.47,11.99;6,90.00,585.67,307.75,11.80;6,111.00,603.43,394.35,9.16;6,90.00,619.03,415.23,9.16;6,90.00,634.63,278.29,9.16"><head>P</head><label></label><figDesc>is the centroid of the irrelevant documents in the training set, β is an experiential parameter. Since the file filter2002_qrels.test cannot be used for training, we use the training set to choose proper values ofβand the threshold by LOOCV (Leave-one-out cross-validation), which is the most extreme and most accurate version of cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.48,173.71,414.22,248.71"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="5,90.48,173.71,414.22,248.71"><row><cell></cell><cell cols="7">.1 ICT adaptive filtering runs(Assessor topics) in TREC-11</cell><cell></cell></row><row><cell>Run ID</cell><cell>MeanT11U</cell><cell cols="3">T11U vs. median(topic nums)</cell><cell>MeanT11F</cell><cell cols="3">T11F vs. median(topic nums)</cell></row><row><cell></cell><cell></cell><cell cols="2">&gt;(Best) =</cell><cell>&lt;(Worst/Zero)</cell><cell></cell><cell cols="3">&gt;(Best) = &lt;(Worst/Zero)</cell></row><row><cell>ICTAdaFT11Ua</cell><cell>0.335</cell><cell>50(18)</cell><cell>0</cell><cell>0(0/0)</cell><cell>0.061</cell><cell>12(5)</cell><cell>32</cell><cell>6(6/6)</cell></row><row><cell>ICTAdaFT11Ub</cell><cell>0.330</cell><cell>49(17)</cell><cell>0</cell><cell>1(1/1)</cell><cell>0.062</cell><cell>13(3)</cell><cell>31</cell><cell>6(6/6)</cell></row><row><cell>ICTAdaFT11Uc</cell><cell>0.335</cell><cell>50(18)</cell><cell>0</cell><cell>0(0/0)</cell><cell>0.061</cell><cell>12(5)</cell><cell>32</cell><cell>6(6/6)</cell></row><row><cell>ICTAdaFT11Fd</cell><cell>0.240</cell><cell>19(0)</cell><cell>7</cell><cell>24(3/3)</cell><cell>0.052</cell><cell>21(1)</cell><cell>24</cell><cell>5(5/5)</cell></row><row><cell></cell><cell cols="7">Table 2.2 ICT adaptive filtering runs(Intersection topics) in TREC-11</cell><cell></cell></row><row><cell>Run ID</cell><cell>MeanT11U</cell><cell cols="3">T11U vs. median(topic nums)</cell><cell>MeanT11F</cell><cell cols="3">T11F vs. median(topic nums)</cell></row><row><cell></cell><cell></cell><cell cols="2">&gt;(Best) =</cell><cell>&lt;(Worst/Zero)</cell><cell></cell><cell cols="3">&gt;(Best) = &lt;(Worst/Zero)</cell></row><row><cell>ICTAdaFT11Ua</cell><cell>0.405</cell><cell>96(24)</cell><cell>3</cell><cell>1(0/0)</cell><cell>0.244</cell><cell cols="2">55(10) 32</cell><cell>13(8/8)</cell></row><row><cell>ICTAdaFT11Ub</cell><cell>0.4025</cell><cell>95(23)</cell><cell>3</cell><cell>2(1/1)</cell><cell>0.245</cell><cell>56(8)</cell><cell>31</cell><cell>13(8/8)</cell></row><row><cell>ICTAdaFT11Uc</cell><cell>0.403</cell><cell>95(24)</cell><cell>3</cell><cell>2(0/0)</cell><cell>0.2415</cell><cell>53(9)</cell><cell>32</cell><cell>15(8/8)</cell></row><row><cell>ICTAdaFT11Fd</cell><cell>0.2805</cell><cell>37(0)</cell><cell>9</cell><cell>54(6/6)</cell><cell>0.179</cell><cell>50(1)</cell><cell>26</cell><cell>24(7/7)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,161.64,430.33,271.98,9.16"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note coords="5,196.31,430.33,237.31,9.16"><p><p><ref type="bibr" coords="5,196.31,430.33,4.39,9.16" target="#b2">3</ref> </p>ICT adaptive filtering runs(all 100 topics) in TREC-11</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,90.42,250.08,409.95,73.19"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table coords="7,90.42,250.08,409.95,56.19"><row><cell>-50)</cell><cell>0.35</cell><cell>0.377</cell><cell>20</cell><cell>10</cell><cell>20</cell><cell>0.18</cell><cell>0.233</cell><cell>19</cell><cell>5</cell><cell>26</cell></row><row><cell>ICTBatFT11Ub(1-50)</cell><cell>0.323</cell><cell></cell><cell>15</cell><cell>7</cell><cell>28</cell><cell>0.248</cell><cell></cell><cell>26</cell><cell>7</cell><cell>17</cell></row><row><cell>ICTBatFT11Ua(51-100)</cell><cell>0.333</cell><cell>0.254</cell><cell>47</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0.024</cell><cell>0</cell><cell>17</cell><cell>33</cell></row><row><cell>ICTBatFT11Ub(51-100)</cell><cell>0.304</cell><cell></cell><cell>40</cell><cell>4</cell><cell>6</cell><cell>0.011</cell><cell></cell><cell>4</cell><cell>17</cell><cell>29</cell></row></table><note coords="7,211.97,314.11,227.07,9.16"><p><p><ref type="bibr" coords="7,211.97,314.11,4.39,9.16" target="#b3">4</ref> </p>ICT batch filtering runs (all 100 topics) in TREC-11</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,90.24,392.94,416.41,105.47"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table coords="7,90.24,392.94,416.41,88.41"><row><cell>Run ID</cell><cell>Average precision</cell><cell cols="3">Average precision vs. medians(Topic nums) &gt; = &lt;</cell><cell>min</cell><cell cols="2">All Results med max</cell></row><row><cell>ICTRouFT11Ua(1-50)</cell><cell>0.243</cell><cell>26</cell><cell>7</cell><cell>17</cell><cell>0</cell><cell cols="2">0.223 0.507</cell></row><row><cell>ICTRouFT11Ub(1-50)</cell><cell>0.25</cell><cell>31</cell><cell>8</cell><cell>11</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICTRouFT11Ua(51-100)</cell><cell>0.024</cell><cell>18</cell><cell>16</cell><cell>16</cell><cell>0</cell><cell>0.02</cell><cell>0.085</cell></row><row><cell>ICTRouFT11Ub(51-100)</cell><cell>0.025</cell><cell>18</cell><cell>18</cell><cell>14</cell><cell></cell><cell></cell><cell></cell></row></table><note coords="7,226.43,489.25,198.07,9.16"><p><p><ref type="bibr" coords="7,226.43,489.25,4.39,9.16" target="#b4">5</ref> </p>ICT routing runs (all 100 topics) in TREC-11</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,119.88,404.23,339.69,216.02"><head>Table 3 .</head><label>3</label><figDesc>.3. 1 Our content-based experiment for the ad hoc task of TREC-10.</figDesc><table coords="9,135.84,436.26,323.73,183.99"><row><cell>Average Precision</cell><cell></cell><cell>R-precision</cell><cell></cell><cell>Recall</cell></row><row><cell>0.1938</cell><cell></cell><cell>0.2185</cell><cell></cell><cell>2243</cell></row><row><cell>Content(α)</cell><cell>Structure(β)</cell><cell>Anchor text(γ)</cell><cell>MRR</cell><cell>Correct Answers</cell></row><row><cell>1</cell><cell>0</cell><cell>0</cell><cell>0.4185</cell><cell>122/145</cell></row><row><cell>0</cell><cell>1</cell><cell>0</cell><cell>0.4467</cell><cell>105/145</cell></row><row><cell>0</cell><cell>0</cell><cell>1</cell><cell>0.3769</cell><cell>94/145</cell></row><row><cell>1</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5880</cell><cell>133/145</cell></row><row><cell>1</cell><cell>0.5</cell><cell>0.8</cell><cell>0.6032</cell><cell>130/145</cell></row><row><cell>1</cell><cell>0.5</cell><cell>1</cell><cell>0.5806</cell><cell>130/145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,140.64,628.15,328.06,120.38"><head>Table 3 .</head><label>3</label><figDesc><ref type="bibr" coords="9,207.12,628.15,4.39,9.16" target="#b1">2</ref> Our Homepage Finding experiments of TREC-10</figDesc><table coords="9,140.64,660.18,328.06,88.35"><row><cell>Run ID</cell><cell>MRR</cell><cell>Answers Found@10</cell><cell>Not Found@all</cell></row><row><cell>ictnp2</cell><cell>0.559</cell><cell>114/150</cell><cell>18/150</cell></row><row><cell>ictnp3</cell><cell>0.557</cell><cell>116/150</cell><cell>18/150</cell></row><row><cell>ictnp4</cell><cell>0.555</cell><cell>116/150</cell><cell>18/150</cell></row><row><cell>ictnp6</cell><cell>0.613</cell><cell>127/150</cell><cell>14/150</cell></row><row><cell>ictnp7</cell><cell>0.613</cell><cell>127/150</cell><cell>14/150</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,185.34,756.43,224.60,9.16"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note coords="9,220.02,756.43,189.92,9.16"><p>3 ICT Named Page Finding runs in TREC-11</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,140.64,561.06,310.61,73.25"><head>Table 3 .</head><label>3</label><figDesc>4 Result of Topic Distillation task in TREC-11</figDesc><table coords="10,140.64,561.06,310.61,56.19"><row><cell>Run ID</cell><cell>Average Precision</cell><cell>R-Precision</cell><cell>Rel_ret</cell></row><row><cell>icttd1</cell><cell>0.1620</cell><cell>0.1919</cell><cell>1038/1574</cell></row><row><cell>icttd2</cell><cell>0.1364</cell><cell>0.1599</cell><cell>1038/1574</cell></row><row><cell>icttd3</cell><cell>0.0597</cell><cell>0.1034</cell><cell>288/1574</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is supported by the national <rs type="programName">973 fundamental research program</rs> under contact of <rs type="grantNumber">G1998030413</rs>, the <rs type="funder">Institute Youth Fund</rs> under contact 20016280-9 and the <rs type="funder">Institute Youth Fund</rs> under contact 20026180-24. We give our thanks to all the people who have contributed to this research and development, in particular <rs type="person">Yanbo Han</rs>, <rs type="person">Li Guo</rs>, <rs type="person">Qun Liu</rs>, <rs type="person">Xin Zhang</rs>, <rs type="person">Hao Zhang</rs>, <rs type="person">Dongbo Bu</rs> and <rs type="person">Huaping Zhang</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UyaqBeU">
					<idno type="grant-number">G1998030413</idno>
					<orgName type="program" subtype="full">973 fundamental research program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,107.40,201.43,397.91,9.16;11,90.00,217.03,325.03,9.16" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,317.98,201.43,187.33,9.16;11,90.00,217.03,81.05,9.16">Structuring and Expanding Queries in the Probabilistic Model</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Honma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,190.14,217.03,155.42,9.16">The Ninth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.08,232.63,398.09,9.16;11,90.00,248.23,327.85,9.16" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,324.20,232.63,180.97,9.16;11,90.00,248.23,79.40,9.16">Structuring and expanding queries in the probabilistic model</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Yasushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hiroko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Masumi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sakiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,188.28,248.23,203.29,9.16">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.55,263.83,396.77,9.16;11,90.00,279.43,117.72,9.16" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,237.23,263.83,121.69,9.16">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,385.50,263.83,119.82,9.16;11,90.00,279.43,91.42,9.16">The Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,105.75,295.03,399.56,9.16;11,90.00,310.63,103.35,9.16" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,190.61,295.03,262.12,9.16">The anatomy of a large scale hypertextual web search engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,472.68,295.03,32.64,9.16;11,90.00,310.63,75.38,9.16">The 7th WWW Conference</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,105.05,326.23,400.39,9.16;11,90.00,341.83,327.61,9.16" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,375.29,326.23,130.15,9.16;11,90.00,341.83,92.96,9.16">The Pagerank citation ranking: Bring order to the web</title>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Windograd</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Stanford Digital Libraries working paper, 1997-0072</note>
</biblStruct>

<biblStruct coords="11,108.60,357.43,396.71,9.16;11,90.00,373.03,53.19,9.16" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,165.97,357.43,232.81,9.16">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,409.74,357.43,95.57,9.16;11,90.00,373.03,21.28,9.16">Proc 9th ACM-SIAM SODA</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.50,388.63,397.82,9.16;11,90.00,404.23,197.74,9.16" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<title level="m" coord="11,332.04,388.63,173.28,9.16;11,90.00,404.23,133.00,9.16">Managing gigabytes: Compressing and indexing documents and images</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.53,419.83,397.79,9.16;11,90.00,435.43,277.48,9.16" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,460.42,419.83,44.90,9.16;11,90.00,435.43,33.05,9.16">OKAPI at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,142.92,435.43,198.10,9.16">The Third Text REtrieval Conference (TREC 3)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,105.10,451.03,400.29,9.16;11,90.00,466.63,163.99,9.16" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,216.62,451.03,165.32,9.16">The TREC 2001 Filtering Track Report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,400.74,451.03,104.65,9.16;11,90.00,466.63,48.21,9.16">The Tenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.44,482.23,392.97,9.16;11,90.00,497.83,415.30,9.16;11,90.00,513.43,23.73,9.16" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,394.05,482.23,111.36,9.16;11,90.00,497.83,140.82,9.16">TREC-10 Experiments at CAS-ICT: Filtering, Web and QA</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,251.82,497.83,156.82,9.16">The Tenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">109</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.11,529.03,395.27,9.16;11,90.00,544.63,113.12,9.16" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,227.22,529.03,243.25,9.16">Maximum Likelihood Estimation for Filtering Thresholds</title>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="294" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.18,560.23,394.16,9.16;11,90.00,575.83,252.32,9.16" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,200.74,560.23,269.13,9.16">The Bias Problem and Language Models in Adaptive Filtering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,489.60,560.23,15.74,9.16;11,90.00,575.83,183.42,9.16">The Tenth Text REtrieval Conference (TREC 10)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.23,591.43,394.07,9.16;11,90.00,607.03,252.31,9.16" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,215.50,591.43,252.47,9.16">Rocchio and Metrics for Information Filtering at TREC-10</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,489.56,591.43,15.74,9.16;11,90.00,607.03,183.41,9.16">The Tenth Text REtrieval Conference (TREC 10)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">84</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.34,622.63,395.03,9.16;11,90.00,638.23,287.24,9.16" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,271.16,622.63,229.46,9.16">Oracle at TREC 10: Filtering and Question-Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alpha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,101.40,638.23,154.82,9.16">The Tenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">423</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.40,653.83,394.95,9.16;11,90.00,669.43,194.09,9.16" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,170.07,653.83,190.77,9.16">Relevance Feedback in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,379.38,653.83,121.60,9.16">The SMART Retrieval system</title>
		<meeting><address><addrLine>Englewood NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
