<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,73.20,83.59,444.34,12.19;1,517.62,80.38,4.50,7.85">Incremental Learning for Profile Training in Adaptive Document Filtering *</title>
				<funder ref="#_YjBvFue">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_HT3bXwS">
					<orgName type="full">Chinese National Key Foundation Research &amp; Development Plan</orgName>
				</funder>
				<funder ref="#_zeQs8kW">
					<orgName type="full">Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,158.04,113.83,38.26,9.16"><forename type="first">Liang</forename><surname>Ma</surname></persName>
							<email>maliang00@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.49,113.83,53.92,9.16"><forename type="first">Qunxiu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.03,113.83,53.40,9.16"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.56,113.83,44.68,9.16"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.74,113.83,57.48,9.16"><forename type="first">Lianhong</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,73.20,83.59,444.34,12.19;1,517.62,80.38,4.50,7.85">Incremental Learning for Profile Training in Adaptive Document Filtering *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9327E224845802DBF136D36B15AFEDA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our ideas and related experiments in TREC-11 Adaptive Filtering Track. In the track we focused much on a robust way for effective profile training. We developed an incremental learning method which selects pseudo positive documents in less bias from a few initial positive training documents. We also did some experiments with newly emerged information retrieval model, language model-based retrieval mechanism, to evaluate its performance when used in adaptive filtering task. Related experiment results show the incremental learning method can be helpful for profile training, while the new language model perform not well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In adaptive filtering, firstly we do profile training to get an initial profile, then based on this profile we do adaptive profile updating. Most of the research work now focus on the algorithms for adaptive profile updating because of its immediate effect to the performance. While even a perfect adaptive profile updating mechanism will suffer a poor result if starting updating from a biased initial profile. In fact, it is high potential to get a bias initial profile because of insufficient topic features provided by such few initial positive training documents.</p><p>A common method for profile training is like this. First use the query constructed from initial positive documents to score the training set, then get all the pseudo positive documents(used to expand initial profile vector) or setup initial profile threshold. In <ref type="bibr" coords="1,290.54,457.03,11.18,9.16" target="#b0">[1]</ref>, initial profile threshold is set in a rank position of these scored documents. System in <ref type="bibr" coords="1,228.80,472.63,12.28,9.16" target="#b2">[3]</ref> select n documents (n=k*m, m is the number of initial positive documents) with the highest score value in the training set to be pseudo positive documents. Then these documents, together with initial positive documents, are used as positive documents to set initial profile vector and threshold.</p><p>It is a simple method together with some problems. By this way of one-step learning, the pseudo positive document and profile threshold are totally depend on these highly limited initial positive documents. Thus any bias in training from these initial training documents will lead to amplified bias in pseudo documents and initial profile. Also the fixed number of pseudo documents, usually set by experience in <ref type="bibr" coords="1,130.93,597.43,11.18,9.16" target="#b2">[3]</ref>, is hard to be determined for various topics.</p><p>In TREC-11, we do further research work in profile training to find a better way for un-bias profile training. In the next section, the detail of profile training will be introduced. After that, experiment data and evaluation result, including our experiments on language model, will be listed. At the end of this paper there is a summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Incremental Learning in Profile Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature selection for initial profile</head><p>We introduce a two-phase selection mechanism for feature selection from positive documents(in this section, term 'document' are called as 'doc'). First we select key features by a step-extend morphological analysis, then get more extended features by incremental learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(1) Get key features for initial profile</head><p>We extract initial key terms from topic statement and 3 initial positive training documents.</p><p>For topic statement, we use a parser <ref type="bibr" coords="2,246.31,139.03,12.66,9.16" target="#b5">[6]</ref> and get terms(called TK term) step by step, something unlike the common way which simply extract all the terms once. The idea is explained as follow:</p><p>1. From title field, get all the words as key terms and add them to KeyTerm set. 2. From desc field, we only find the words which limit key terms in KeyTerm set and add them to KeyTerm set. 3. From narr field, we do the same process as step 2. For 3 positive training docs, we statistic the terms in title and text field(after stopword removing) . The terms(called TK term) whose weight are higher than the double of the average term weight and occur in more than 1 doc are selected as key terms.</p><p>We combine the TK terms and DK terms to construct a basic profile. Here the DK terms use their statistic weight. We set the average weight of DK terms as the weight of TK terms. For terms from desc and title field, increase their weight with different weight plus(&gt;1).</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Incremental learning for initial profile</head><p>We use an incremental learning mechanism for more extend features from pseudo positive docs. Different from the common ways only score training set once and select all the pseudo docs(easily cause the bias problem), in our mechanism training set be repeated scored more than once. After each scoring only a small number of pseudo documents who is highly relative to the exist positive docs are selected, and these docs are used to do limited feedback for new profile vector terms. By this step learning process we can decrease the potential bias pseudo positive docs. The detail of learning process is:</p><p>1. Define a set U for positive docs(including pseudo positive docs from learning). Here Pu is number of elements in U. Also we get initial profile by process described above. 2. Use current profile to score all the docs in training set, then sort them by their score in descend order. Set AVGu as average score of all the docs in U and Smin as minimal score of docs in U. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Setup Initial Threshold</head><p>With the key features and extend features we create the term vector of initial profile. The initial profile threshold should be set to value that can result in the highest value of T11F. In calculating the T11F, we count all the docs in U as positive docs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Language Model in Adaptive Filtering</head><p>Besides the research work in profile training, we have the interest in how well the language model IR model perform in adaptive filtering. The Lemur <ref type="bibr" coords="2,275.73,759.43,12.28,9.16" target="#b6">[7]</ref> tool kits is chosen here to support our experiment. As a newly released IR tool kits, it provide a language model-based IR mechanism for relevance score and related feedback methods. We submitted one run simply with the default parameters based on this system. Thinking that other TREC team which also use it in adaptive filtering will deliver a detail report about it, we just run our experiment with the default parameters and make no deep analysis to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance of Incremental Learning</head><p>We use Reuter training corpus as training set to do incremental learning for 100 TREC topics, and let the positive training documents for batch filtering as relative documents for our test. The average T11F and T11U score of 100 topics are calculated for performance estimation. We do two training, one by fixed learning used in <ref type="bibr" coords="3,228.23,260.23,12.28,9.16" target="#b2">[3]</ref> with different fixed number ( see figure <ref type="figure" coords="3,221.89,275.83,4.38,9.16" target="#fig_0">1</ref>) and one by incremental learning(see table <ref type="bibr" coords="3,214.30,291.43,8.77,9.16" target="#b0">1)</ref> In figure <ref type="figure" coords="3,138.86,307.03,3.93,9.16" target="#fig_0">1</ref>, the performance of fixed learning decrease as the number of pseudo positive document increase. Though there is a higher score for small fixed pseudo documents, it is no practical use for training because of low recall. Two evaluation results of incremental learning run with two set of typical parameters are listed in table <ref type="table" coords="3,72.00,560.11,3.95,9.16" target="#tab_1">1</ref>. With both sets system reach to the similar performance(for example, pseudo documents number). In comparison with the old way, we also list another 3 results by fixed learning which has the approximate pseudo documents number with that in incremental learning. It is obviously that the new incremental learning get the high score both in T11U and T11F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Runs Submitted and Evaluation Result</head><p>This year we submit 3 runs for adaptive filtering(algorithm for adaptive profile updating is ignored here). To compare the performance of new IR model , traditional Vector Space Model are also used for guideline, and all the runs are optimized by same criteria (T11F). Table <ref type="table" coords="3,373.98,681.31,5.25,9.16" target="#tab_2">2</ref> show the technology used in each runs.  Table <ref type="table" coords="4,119.38,109.33,5.25,9.16" target="#tab_3">3</ref> list the evaluation result for each runs. For the 4 evaluation criteria, we calculate the average value for the first 50 and second 50 topics. Also the average of median value in all the TREC-11 runs is listed for comparison. From the date above we find the Language Model perform not as well as we expect. Compared to the traditional model, it does not show the predominance in adaptive filtering. We also notice that second 50 topics get better position in all Trec-11 runs than the first 50 topics do, indicating our incremental learning in profile training is more effective for second 50 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary and Future Work</head><p>In TREC-2001 adaptive filtering track, we developed a general method for effective learning in profile training, and did some performance evaluation for language model. Though the language model applied to adaptive filtering wok not well as wish, the new incremental learning method demonstrate its advantage than the old ways. Knowing little on the effect that feedback method we used in incremental learning, we are going to do detailed analysis for different feedback methods in incremental learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,315.72,377.68,199.59,7.85"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Fixed learning for pseudo positive documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,93.00,497.83,430.30,133.96"><head></head><label></label><figDesc>3. Select new pseudo positive documents from scored docs and add them to U. Two rules for selecting them: Rule 1: If the first Pu docs are all in U, select the No.(Pu +1) doc. Rule 2: Else, select lower value among AVGu and Smin as threshold. The docs whose score are higher than threshold are new pseudo docs. 4. Do feedback(for example, Rocchio method) to profile with new selected pseudo positive docs. 5. goto step 2 for next learning. Exit loop if : 1. if no new pseudo positive docs can be got. 2. if Pu &gt;n or already kept learning for r times.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,135.42,408.82,327.65,120.97"><head>Table 1 :</head><label>1</label><figDesc>Average Score of All Topics For two methods</figDesc><table coords="3,135.42,424.51,327.65,105.28"><row><cell>Method</cell><cell>Parameters</cell><cell>Pseudo Documents</cell><cell>T11F</cell><cell>T11U</cell></row><row><cell></cell><cell></cell><cell>Number</cell><cell></cell><cell></cell></row><row><cell>Incremental</cell><cell>n=15; r=3</cell><cell>11</cell><cell>21.55%</cell><cell>27.16%</cell></row><row><cell>Learning</cell><cell>n=20; r=4</cell><cell>13</cell><cell>20.63%</cell><cell>25.78%</cell></row><row><cell>Fixed</cell><cell>n=15</cell><cell>15</cell><cell>12.85%</cell><cell>14.79%</cell></row><row><cell>Learning</cell><cell>n=12</cell><cell>12</cell><cell>14.15%</cell><cell>16.31%</cell></row><row><cell></cell><cell>n=9</cell><cell>9</cell><cell>15.74%</cell><cell>19.04%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,87.06,712.90,406.53,40.99"><head>Table 2 :</head><label>2</label><figDesc>Technology Used in Each Run</figDesc><table coords="3,87.06,728.59,406.53,25.30"><row><cell>Runs</cell><cell>IR model</cell><cell>Score Method</cell><cell>Feedback</cell><cell>Other Process</cell></row><row><cell>ThuT11af1</cell><cell cols="4">Vector Space Model TF/IDF(bm25) Improved Rocchio Query Expansion</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,105.06,156.52,381.53,137.17"><head>Table 3 :</head><label>3</label><figDesc>Average Result of Evaluation for Each Runs</figDesc><table coords="4,105.06,172.21,381.53,121.48"><row><cell></cell><cell cols="2">R101-R150</cell><cell></cell><cell></cell><cell cols="2">R151-R200</cell></row><row><cell>Runs</cell><cell>T11U T11F</cell><cell>Set</cell><cell>Set</cell><cell cols="2">T11U T11F</cell><cell>Set</cell><cell>Set</cell></row><row><cell></cell><cell></cell><cell>Precision</cell><cell>Recall</cell><cell></cell><cell></cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>ThuT11af1</cell><cell>0.395 0.417</cell><cell>0.512</cell><cell cols="3">0.367 0.059 0.040</cell><cell>0.038</cell><cell>0.101</cell></row><row><cell>ThuT11af2</cell><cell>0.389 0.422</cell><cell>0.474</cell><cell cols="3">0.417 0.061 0.052</cell><cell>0.057</cell><cell>0.065</cell></row><row><cell>ThuT11af3</cell><cell>0.277 0.337</cell><cell>0.357</cell><cell cols="3">0.504 0.052 0.030</cell><cell>0.037</cell><cell>0.060</cell></row><row><cell cols="2">Avg median 0.381 0.306</cell><cell>0.395</cell><cell cols="2">0.286 0.257</cell><cell>0.02</cell><cell>0.031</cell><cell>0.021</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>* Supported by the <rs type="funder">Chinese National Key Foundation Research &amp; Development Plan</rs> (Grant <rs type="grantNumber">G1998030509</rs>), <rs type="funder">Natural Science Foundation</rs> No.<rs type="grantNumber">60223004</rs>, and National 863 High Technology Project No. <rs type="grantNumber">2001AA114082</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HT3bXwS">
					<idno type="grant-number">G1998030509</idno>
				</org>
				<org type="funding" xml:id="_zeQs8kW">
					<idno type="grant-number">60223004</idno>
				</org>
				<org type="funding" xml:id="_YjBvFue">
					<idno type="grant-number">2001AA114082</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,87.05,511.41,436.30,8.74;4,89.10,527.01,300.03,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,305.11,511.41,218.24,8.74;4,89.10,527.01,24.28,8.74">Optimization in CLARIT TREC-8 Adaptive Filtering Trec 8</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,130.47,527.01,191.77,8.74">Proceeding of eighth Text Retrieval Conference</title>
		<meeting>eeding of eighth Text Retrieval Conference<address><addrLine>TREC-8), NIST</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,87.08,542.61,436.31,8.74;4,89.10,558.21,113.85,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,192.21,542.61,169.96,8.74">The TREC-9 Filtering Track Final Report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,381.49,542.61,141.89,8.74;4,89.10,558.21,84.45,8.74">Proceeding of ninth Text Retrieval Conference(TREC-9)</title>
		<meeting>eeding of ninth Text Retrieval Conference(TREC-9)</meeting>
		<imprint>
			<publisher>NIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,87.78,573.81,435.54,8.74;4,89.10,589.41,258.68,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,274.14,573.81,232.64,8.74">FDU at TREC-10: Filtering, QA, Web and Video Tasks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno>TREC-10</idno>
	</analytic>
	<monogr>
		<title level="m" coord="4,89.10,589.41,186.94,8.74">Proceeding of tenth Text Retrieval Conference</title>
		<meeting>eeding of tenth Text Retrieval Conference</meeting>
		<imprint/>
		<respStmt>
			<orgName>NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,86.25,605.01,437.08,8.74;4,89.10,620.61,406.07,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,98.59,605.01,424.74,8.74;4,89.10,620.61,130.25,8.74">Arampatzis Unbiased S-D Threshold Optimization, Initial Query Degradation, Decay, and Incrementality, for Adaptive Document Filtering</title>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<idno>TREC-10</idno>
	</analytic>
	<monogr>
		<title level="m" coord="4,236.33,620.61,187.12,8.74">Proceeding of tenth Text Retrieval Conference</title>
		<meeting>eeding of tenth Text Retrieval Conference</meeting>
		<imprint/>
		<respStmt>
			<orgName>NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,87.46,636.21,435.78,8.74;4,89.10,651.81,118.89,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,197.72,636.21,162.84,8.74">The TREC 2001 Filtering Track Report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,380.60,636.21,142.65,8.74;4,89.10,651.81,89.47,8.74">Proceeding of tenth Text Retrieval Conference(TREC-10)</title>
		<meeting>eeding of tenth Text Retrieval Conference(TREC-10)</meeting>
		<imprint>
			<publisher>NIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,86.14,667.41,287.75,8.74" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Miniparser</surname></persName>
		</author>
		<ptr target="http://www.cs.ualberta.ca/~lindek/minipar.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,86.02,683.01,420.12,8.74" xml:id="b6">
	<monogr>
		<ptr target="http://www-2.cs.cmu.edu/~lemur/" />
		<title level="m" coord="4,86.02,683.01,278.16,8.74">The Lemur Toolkit for Language Modeling and Information Retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
