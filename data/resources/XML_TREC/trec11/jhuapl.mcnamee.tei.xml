<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.20,87.38,433.62,12.64">JHU/APL at TREC 2002: Experiments in Filtering and Arabic Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,197.32,116.44,58.89,8.96"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>mcnamee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.28,116.44,63.16,8.96"><forename type="first">Christine</forename><surname>Piatko</surname></persName>
							<email>piatko@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.04,116.44,63.54,8.96"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<email>mayfield@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.20,87.38,433.62,12.64">JHU/APL at TREC 2002: Experiments in Filtering and Arabic Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">33C68D8B5AAE990E0C99E9BA688A4790</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>The Johns Hopkins University Applied Physics Laboratory (JHU/APL) participated in two tracks at this year's conference. We participated in the filtering track, again addressing the batch and routing subtasks, as well as the adaptive task for the first time. We also continued experiments in Arabic retrieval, emphasizing language-neutral approaches.</p><p>For ranked retrieval, we relied on a statistical language model to compute query/document similarity values. Hiemstra and de Vries describe such a linguistically motivated probabilistic model and explain how it relates to both the Boolean and vector space models <ref type="bibr" coords="1,161.92,358.72,10.69,8.96" target="#b3">[4]</ref>. The model has also been cast as a rudimentary Hidden Markov Model <ref type="bibr" coords="1,268.72,369.88,15.43,8.96" target="#b12">[13]</ref>. Although the model does not explicitly incorporate inverse document frequency, it does favor documents that contain more of the rare query terms. The similarity measure can be computed as Sim(q,d) = α ⋅ f (t,d) + (1-α) ⋅ df (t) ( )</p><formula xml:id="formula_0" coords="1,121.48,434.24,142.72,28.15">t ∈q ∏ f (t,q )</formula><p>Equation 1. Similarity calculation.</p><p>where α is the probability that a query word is generated by a document-specific model, and (1-α) is the probability that it is generated by a generic language model. df(t) denotes the mean relative document frequency of term t. In our experiments an α of between 0.15 and 0.3 has worked well, but performance is fairly insensitive to the precise value used.</p><p>For text classification problems we used Support Vector Machines (SVMs), which efficiently perform binary classification tasks. We applied SVMs to this year's Filtering tasks; however, some of our routing runs were based on statistical language models instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filtering Track</head><p>We participated in the routing, batch and adaptive tasks of the filtering track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filtering Approach Background</head><p>We continued to investigate the application of Support Vector Machines (SVMs) to filtering tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filtering Training</head><p>Over the course of the year we had performed several experiments using the Reuters Corpus [18] and topics from TREC 2001. Based on track guidelines, we wanted to establish various parameters necessary for our system based on alternative data. We chose to reset these based on performance on Financial Times data that had been used in the TREC-8 Filtering Track. We did this in a straightforward way for the routing and batch training and test sets. However, no training documents had been used for the adaptive task in TREC-8, so for this training we randomly selected three relevant training documents from the batch FT training qrels for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing Task</head><p>We submitted two official runs for routing. We submitted an SVM based run apl11Fsvm and a rankbased merge run apl11Frm, the merge of the SVM run with an unofficial score-based run apl11Frs.</p><p>Our statistical language model-based run apl11Frs used simulated routing (using a modified version of our HAIRCUT system to score indexed test documents using training index statistics). We formed queries using 60 terms per topic that were selected from the positive qrels training documents. In subsequent analysis of our results we realized that we submitted the wrong SVM-based routing run. We had intended to submit the SVM run apl11Frsvm2 based on 40000 df terms (the best of numbers to use on FT data). This run does well, and makes an excellent run (apl11Frm2) when rank-merged with the score-based run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Filtering Task</head><p>We used the score of the test document from the topic-specific SVM to decide whether to return a document as possibly relevant. We used crossvalidation for threshold selection. We applied n-fold cross-validation on training data to find the best threshold per topic for the given score function being optimized.</p><p>Lewis had applied exhaustive leave-one out to find optimal SVM j weights per topic last year <ref type="bibr" coords="2,508.60,157.49,10.69,8.96" target="#b8">[9]</ref>, but this was computationally unrealistic for our implementation. Particular choices of n we used for cross-validation are noted below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Using Linear SVMs with TF/IDF Vectors</head><p>For the submitted apl11FbF run, we used the top 20000 df terms. We used all the presumed negative training examples from the training index. We used three-fold cross-validation on the training data to select the best topic-specific score thresholds for the T11F measure.</p><p>For the submitted run apl11FbSU run, we used the top 12000 df terms. Our two approaches were similar to early filtering score-based buffer window approaches. An SVM was created based on training data for each topic. Three "buffers" of documents from the test document stream were maintained: a "good" buffer of documents correctly judged relevant; a "bad" buffer of those that had been incorrectly judged relevant; and a "presumed bad" (unjudged) buffer of those documents not retrieved (and presumed irrelevant).</p><p>All buffers were capped to a fixed size. Window sizes for the buffers were set somewhat arbitrarily based on limited experimentation as follows: 750 documents for the known positive documents, 750 for the known negative documents and 2000 or 50 (noted below) for the presumed negative documents (those not retrieved). We also used a heuristic parameter to guess occasionally if no documents had been retrieved for a long time.</p><p>Our first approach used queues for all three of these buffers of documents, expiring the old documents as the buffers overfilled. The notion here is that older documents are less valuable than newer ones. (In this case we used the larger size 2000 buffer for negative documents.) Our second approach used heaps, based on the absolute value of the SVM score (smallest value on top), throwing away documents with larger scores as the buffers overfilled. The notion here is that documents closer to the margin of the current SVM are more useful as discriminative examples for training. (Here we used the smaller buffer size of 50 for the presumed negative documents.)</p><p>Our strategy was to update the topic-specific SVMs at data-driven intervals, using the documents in the current buffers. The intervals were based on sizes of the current buffers, as well as a "rate of change" heuristic.</p><p>Using the queue approach we had observed good (but statistically variable performance) based on the TREC 2001 data and topics (0.35 average T10SU, and a boxplot as good or better than the official boxplots from TREC-10 filtering). However, we did not do as well for this year's official adaptive task. Clearly, the heap approach returned too few documents, whereas the queue approach returned too many. This is probably mainly due to the much lower amount of feedback. It was probably also adversely affected by our choice of "guess occasionally" parameter that guessed too often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filtering Results Discussion</head><p>In a low training/feedback situation, filtering seems to require more of a Statistical Language Model score-based approach. Based on the good performance possible in situations with lots of training and feedback (as in TREC-2001), there seems to be a continuum between score-based and classification approaches, depending on the amount of training and feedback available. We conjecture a hybrid approach will be useful to support this continuum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arabic Language Retrieval</head><p>The Cross-Language Retrieval task at TREC 2002 consisted of bilingual retrieval of Arabic newspaper articles given English topic statements. The document collection was the same as that used in the TREC 2001 CLIR Track. Monolingual submissions were also accepted using Arabic versions of the topics created by human translators. JHU/APL submitted five official runs; one monolingual and four bilingual runs that used only the &lt;title&gt; and &lt;desc&gt; topic fields. We continued to use the HAIRCUT retrieval engine for our experiments, again emphasizing language-neutral approaches to multilingual retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tokenization</head><p>Over the past year several studies explored alternate representations for indexing Arabic text. Mayfield et al. <ref type="bibr" coords="3,337.60,257.92,16.76,8.96" target="#b9">[10]</ref> investigated the use of character n-grams for Arabic retrieval in TREC-2001 and found that ngrams of length 4 were most effective. Similarly, Darwish and Oard examined multiple tokenization strategies for retrieval of scanned Arabic documents and concluded that character n-grams of lengths 3 or 4 were the basis for the most successful approach <ref type="bibr" coords="3,525.76,324.88,10.69,8.96" target="#b0">[1]</ref>. Linguistic methods of combating Arabic morphology have also been fruitful. Xu et al. <ref type="bibr" coords="3,470.32,347.20,16.76,8.96" target="#b13">[14]</ref> investigated several problems unique to Arabic language text retrieval, specifically misspelled words, broken plurals, and infix morphology, and empirically evaluated techniques to overcome them. Larkey et al. <ref type="bibr" coords="3,324.04,403.00,11.72,8.96" target="#b7">[8]</ref> investigated methods for effectively stemming Arabic.</p><p>Given the successful reports of n-gram based retrieval for Arabic, we opted to continue using them this year. However, we decided to use a combination of tokenization methods in the same term space. We used n-grams of more than one length, and we included space-delimited words. We do perform one minor language-specific function, elimination or replacement of certain Arabic characters. Specifically, we map Alef Maksura to Yeh and Teh Marbuta to Teh, and we eliminate Hamza, Madda and any remaining Arabic letters or symbols that did not appear in a list of 28 letters that we had available.</p><p>Recent work in Asian language retrieval has shown that multiple length n-grams can be quite effective, and may result in a 10% relative improvement in mean average precision over the use of single length n-grams <ref type="bibr" coords="3,361.48,626.20,15.43,8.96" target="#b11">[12]</ref>. Accordingly, we examined multiple length n-grams. In particular, we construct the set of all 3-grams, 4-grams, and 5-grams that can be generated from a given input sequence.</p><p>We initially built several indexes to compare different methods for tokenization. Summary information about each is shown in Table <ref type="table" coords="3,493.12,704.32,3.77,8.96" target="#tab_5">6</ref> Using the TREC-2001 CLIR test collection (i.e., Arabic topics 1-25) we compared several knowledgelight methods for indexing Arabic text (see the chart in Figure <ref type="figure" coords="4,111.76,222.04,3.63,8.96">1</ref>). These experiments used only the &lt;title&gt; and &lt;desc&gt; portions of the topic statements and made use of pseudo-relevance feedback. Plain 4-grams did quite well, but slightly superior performance was found when a hybrid indexing scheme was used.</p><p>Based on these training experiments, we selected this strategy for TREC-2002.</p><p>Thus, our official runs used both words and 3-, 4-, 5grams to represent text in a single term-space. It should be noted that this tripled the disk space consumed by the index data structures compared to the use of solitary 4-grams; the use of 4-grams alone is probably justified when storage limitations are a concern.</p><p>Figure <ref type="figure" coords="4,100.72,616.48,3.77,8.96">1</ref>.</p><p>Comparison of tokenization methods using the TREC-2001 CLIR test suite. Mean average precision is plotted. The combination of words plus 3-, 4-, and 5-grams was the best performing approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translation</head><p>Although we recently explored efficient methods for translating document representations at the CLEF-2002 evaluation <ref type="bibr" coords="4,405.64,93.64,15.43,8.96" target="#b10">[11]</ref>, we focused on query translation for our work in the CLIR Track at TREC. We are convinced that the caliber of translation resources has a great effect on bilingual retrieval performance, so we were glad to see the track guidelines stipulate a standard set of resources. However, in several ways the formatting of these resources prevented us from using them in an optimal fashion. In particular, we had hoped to use the English / Arabic parallel texts from the United Nations. We were grateful for the statistical lexicon that was made available by BNN; however, it was of limited use to our system since we do not routinely stem English or Arabic.</p><p>Most of our bilingual runs simply relied on machine translation software. However, in an attempt to make use of the BBN statistical lexicon, we derived a surrogate dictionary. We first ran a Porter stemmer to create a set E of English words that could produce a given English stem; we also created a set of Arabic words A, that created the stems in BBN's lexicon using Kareem Darwish's Al-Stem stemmer. Then, we created an unweighted translation dictionary with entries between each English word in E and every word in A to which that word might be mapped. Queries were translated by substituting all possible translations for a given source language query term, preserving the original query term frequency. Lastly, we performed n-gram processing over the translated queries using only within-word n-grams.</p><p>Each of our official submissions used only the &lt;title&gt; and &lt;desc&gt; fields, augmented by pseudo-relevance feedback. For our monolingual Arabic run, apl11ca1, we used</p><p>• word plus 3-, 4-, and 5-gram indexing • relevance feedback using queries expanded to 300 terms Apl11ce1 was our first bilingual run using the English topics. We used the same approach as apl11ca1, but used the Almisbar web-based service to create translated queries. We also created a run using the (standard) Ajeeb translator, apl11ce3. Mappings derived from the statistical lexicon provided by BBN were used for apl11ce4. Finally, hoping that a combination of resources would maximize lexical coverage, and thus retrieval performance, we submitted a run based on merging scores from our two MT-based runs, apl11ce2. This run was not our best official run; use of only the standard MT-resource, the Ajeeb translator, was best.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comb chart for apl11ca1</head><p>The MT-based runs obtained performance between 71% and 78% of a monolingual baseline in terms of mean average precision; a relative recall at 1000 documents of 88% was found. A precision-recall graph comparing these results is plotted in Figure <ref type="figure" coords="5,273.04,512.20,3.77,8.96" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>This year we participated in two tracks: filtering, and Arabic.</p><p>We continued our investigation of using Support Vector Machines (SVMs) to tackle text filtering challenges. We found promise for the use of SVMs for relevance feedback for routing. We plan to further investigate related SVM pseudo-relevance feedback effects on ad hoc retrieval. Our batch results appeared to be about median, and we had many "zero returns," so more remains to be done to tune this approach for low training situations. Perhaps Financial Times data was not similar enough to the evaluation data for use in parameter selection. Again, this is related to the much smaller amount of feedback in the track this year. It is possible to make better use of unlabeled (unjudged) data for SVM training, and we hope to revisit this in future experiments.</p><p>One thing we have observed in our CLIR work is that it is difficult to define standard translation resources. For example, it has proved difficult this year to separate specific stemming algorithms (and implementations) from some of the standard resources. We also wonder whether cross-system comparisons would be facilitated if participants submitted runs that used only a single translation resource. For the TREC-2001 CLIR guidelines, systems could use any of the three options (dictionary, statistical lexicon, or MT system), thus giving 7 ways to use 'standard' resources. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.04,240.40,215.94,8.96;5,72.04,251.56,174.21,8.96"><head>Figure 2 (</head><label>2</label><figDesc>Figure 2 (below) compares our monolingual run against the median of 18 monolingual runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.04,445.24,36.21,8.96;5,126.04,445.24,101.22,9.08"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.Comb chart for apl11ca1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,324.04,200.32,36.21,8.96;5,378.04,200.32,160.51,8.96;5,324.04,211.48,174.63,8.96"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.Recall-precision graph for APL's official Arabic track automatic submissions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,324.04,231.35,216.03,399.56"><head></head><label></label><figDesc>SVMs are used to create classifiers from a set of labeled training data, finding a hyperplane (possibly in a transformed space) to separate positive examples from negative examples. This hyperplane is chosen to maximize the margin (or distance) to the training points. The promise of large margin classification is that it does not overfit the training data and generalizes well to test data of similar distribution.</figDesc><table coords="1,324.04,521.51,215.98,109.40"><row><cell>other</cell></row><row><cell>training qrels documents as (presumed) negative</cell></row><row><cell>examples, or randomly sampled (number of known</cell></row><row><cell>positive examples) * NegativeToPositiveRatio</cell></row><row><cell>presumed negative examples from the training index,</cell></row><row><cell>throwing away any that were actually positive. (The</cell></row><row><cell>use of all negatives or a particular ratio is noted</cell></row><row><cell>below.) We trained linear SVMs, weighting positive</cell></row><row><cell>and negative training examples equally (-j 1 flag in</cell></row><row><cell>SVM-light).</cell></row></table><note coords="1,324.04,320.63,215.98,8.96;1,324.04,331.79,215.94,8.96;1,324.04,342.95,215.94,8.96;1,324.04,354.11,215.94,8.96;1,324.04,365.27,216.01,8.96;1,324.04,376.43,215.94,8.96;1,324.04,387.59,215.98,8.96;1,324.04,398.75,73.77,8.96;1,324.04,421.07,215.94,8.96;1,324.04,432.23,215.94,8.96;1,324.04,443.39,215.94,8.96;1,324.04,454.55,216.01,8.96;1,324.04,465.71,215.96,8.96;1,324.04,476.87,215.96,8.96;1,324.04,488.03,216.03,8.96;1,324.04,499.19,215.98,8.96;1,324.04,510.35,215.94,9.20;1,324.04,521.51,189.73,8.96"><p><p><p><p><p><p>See Hearst</p><ref type="bibr" coords="1,370.48,320.63,11.72,8.96" target="#b2">[3]</ref> </p>for a general discussion of SVMs. We used the SVM-light package (version 3.50, by Thorsten Joachims</p>[15]</p>) to create classifiers based on the training data for classification of the test data, and wrote a JNI interface to SVM-light to support filtering with our HAIRCUT system. All runs used stem indices using a derivative version of the SMART stemmer.</p>We slightly reduced the term space to create test and training document vectors. Terms were selected using the top stems by document frequency in the training set. (Exact numbers of stems differed for different runs, noted per task in descriptions below.) Stopwords were not removed. We used tf/idf weighted vectors for each document. IDF values were based on training index statistics. Vectors were normalized to unit length. Given n positive training documents for a topic, we chose either all</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,55.24,270.04,232.81,305.15"><head>Table 1 .</head><label>1</label><figDesc>APL Routing Results, Assessor topics. Highlighted rows indicate unofficial runs.</figDesc><table coords="2,55.24,270.04,232.81,305.15"><row><cell cols="6">Term selection was accomplished using mutual</cell></row><row><cell cols="6">information based difference statistics with respect to</cell></row><row><cell cols="3">the training documents.</cell><cell></cell><cell></cell></row><row><cell cols="6">For the SVM routing run apl11Fsvm, we used the top</cell></row><row><cell cols="6">12000 features ranked by document frequency and 10</cell></row><row><cell cols="6">times as many negative documents as known positive</cell></row><row><cell cols="6">examples to train an SVM. We kept track of the top</cell></row><row><cell cols="6">1000 documents for a topic in a heap based on the</cell></row><row><cell>SVM score.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Avg. prec. # terms</cell><cell>#</cell><cell># ≥ median</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>bests</cell><cell>(50 topics)</cell></row><row><cell>apl11Frm</cell><cell>0.330</cell><cell cols="2">12000</cell><cell>4</cell><cell>43</cell></row><row><cell>apl11Fsvm</cell><cell>0.218</cell><cell cols="2">12000</cell><cell>1</cell><cell>24</cell></row><row><cell>apl11Frs</cell><cell>0.364</cell><cell>60</cell><cell></cell><cell cols="2">Score run</cell></row><row><cell>apl11Frsvm2</cell><cell>0.364</cell><cell cols="2">40000</cell><cell cols="2">SVM run</cell></row><row><cell>apl11Frm2</cell><cell>0.412</cell><cell cols="2">40000</cell><cell cols="2">Merge run</cell></row><row><cell></cell><cell></cell><cell>Avg.</cell><cell cols="2"># bests</cell><cell># ≥ median</cell></row><row><cell></cell><cell></cell><cell>prec.</cell><cell></cell><cell></cell><cell>(50 topics)</cell></row><row><cell>apl11Frm</cell><cell></cell><cell>0.042</cell><cell>5</cell><cell></cell><cell>37</cell></row><row><cell cols="2">apl11Frsvm</cell><cell>0.043</cell><cell>4</cell><cell></cell><cell>35</cell></row><row><cell>apl11Frs</cell><cell></cell><cell>0.035</cell><cell cols="2">Score run</cell></row><row><cell cols="2">apl11Frsvm2</cell><cell>0.041</cell><cell cols="2">SVM run</cell></row><row><cell>apl11Frm2</cell><cell></cell><cell>0.045</cell><cell cols="2">Merge run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,72.04,577.85,215.97,20.12"><head>Table 2 .</head><label>2</label><figDesc>APL Routing Results, Intersection topics. Highlighted rows indicate unofficial runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="2,324.04,317.93,222.22,255.92"><head>Table 4 .</head><label>4</label><figDesc>APL Batch Results, Intersection topics</figDesc><table coords="2,324.04,374.01,222.22,199.85"><row><cell></cell><cell>T11SU</cell><cell>T11F</cell><cell>SetPrec</cell><cell>SetRecall</cell></row><row><cell>apl11FbF</cell><cell>0.391</cell><cell>0.216</cell><cell>0.409</cell><cell>0.117</cell></row><row><cell>apl11FbSU</cell><cell>0.293</cell><cell>0.181</cell><cell>0.244</cell><cell>0.255</cell></row><row><cell cols="5">Table 3. APL Batch Results, Assessor topics</cell></row><row><cell></cell><cell>T11SU</cell><cell>T11F</cell><cell>SetPrec</cell><cell>SetRecall</cell></row><row><cell>apl11FbF</cell><cell>0.338</cell><cell>0.026</cell><cell>0.068</cell><cell>0.013</cell></row><row><cell>apl11FbSU</cell><cell>0.035</cell><cell>0.028</cell><cell>0.027</cell><cell>0.275</cell></row><row><cell cols="2">Adaptive Filtering Task</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">We developed two heuristic approaches to using</cell></row><row><cell cols="5">SVMs for adaptive filtering. While there is theory to</cell></row><row><cell cols="5">explain how a static SVM generalizes to test data of</cell></row><row><cell cols="5">similar distribution to training data, this theory has</cell></row><row><cell cols="5">not yet been well developed for SVMs that are</cell></row><row><cell cols="4">adapting over time based on feedback.</cell><cell></cell></row></table><note coords="2,408.64,317.93,131.34,8.96;2,324.04,329.09,215.97,8.96;2,324.04,340.25,215.94,8.96;2,324.04,351.41,215.98,8.96;2,324.04,362.57,67.29,8.96"><p>We again used all the presumed negative training examples from the training index. We used five-fold cross-validation on the training data to select the best topic-specific thresholds for the T11SU measure.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,72.04,435.67,222.22,61.98"><head>Table 5 .</head><label>5</label><figDesc>APL Adaptive Results, Assessor topics</figDesc><table coords="3,82.00,435.67,212.26,50.34"><row><cell></cell><cell>T11SU</cell><cell>T11F</cell><cell>SetPrec</cell><cell>SetRecall</cell></row><row><cell>apl11Fah1</cell><cell>0.342</cell><cell>0.104</cell><cell>0.377</cell><cell>0.039</cell></row><row><cell>apl11Fah2</cell><cell>0.342</cell><cell>0.104</cell><cell>0.377</cell><cell>0.039</cell></row><row><cell>apl11Faq1</cell><cell>0.059</cell><cell>0.09</cell><cell>0.084</cell><cell>0.369</cell></row><row><cell>apl11Faq2</cell><cell>0.085</cell><cell>0.118</cell><cell>0.115</cell><cell>0.355</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="3,496.89,704.32,3.77,8.96"><head>Table 6 .</head><label>6</label><figDesc>. Index statistics for the 869 MB, 384K article TREC-2002 Arabic collection.</figDesc><table coords="4,79.84,53.95,196.08,98.46"><row><cell></cell><cell># terms</cell><cell>index size</cell></row><row><cell>words</cell><cell>539979</cell><cell>254MB</cell></row><row><cell>3-grams</cell><cell>27016</cell><cell>441MB</cell></row><row><cell>4-grams</cell><cell>225218</cell><cell>766MB</cell></row><row><cell>5-grams</cell><cell>1478593</cell><cell>1157MB</cell></row><row><cell>6-grams</cell><cell>6081618</cell><cell>1691MB</cell></row><row><cell>words + 3/4/5-grams</cell><cell>2876187</cell><cell>2422MB</cell></row><row><cell>words + 3/4/5/6-grams</cell><cell>9714673</cell><cell>4038MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,49.02,432.90,236.08,156.29"><head>Table 7 .</head><label>7</label><figDesc>Official results for Arabic runs (50 topics). The highlighted rows indicate bilingual runs that used only standard translation resources.</figDesc><table coords="4,49.02,432.90,236.08,156.29"><row><cell></cell><cell cols="6">Comparision of Tokenization Types</cell></row><row><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Average Precision</cell><cell>0.05 0.10 0.15 0.20 0.25 0.30 0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>words</cell><cell>3-grams</cell><cell>4-grams</cell><cell>5-grams</cell><cell>6-grams</cell><cell>words+3/4/5-</cell><cell>words+3/4/5/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>grams</cell><cell>6-grams</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,62.77,47.13,473.47,382.15"><head>TREC-2002 Official Arabic Results</head><label></label><figDesc></figDesc><table coords="5,62.77,56.07,473.47,373.20"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>0.30 0.40 0.50 0.60 0.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>apl10ca1 (0.3410)</cell><cell>apl10ce3(0.2658)</cell><cell>apl10ce2 (0.2571)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>apl10ce1 (0.2427)</cell><cell>apl10ce4 (0.1777)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Monolingual Arabic: APL vs. Median Performance</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>26</cell><cell>31</cell><cell>36</cell><cell>41</cell><cell>46</cell><cell>51</cell><cell>56</cell><cell>61</cell><cell>66</cell><cell>71</cell></row><row><cell></cell><cell>-0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Difference</cell><cell>-0.30 -0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Topic Number</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="5,62.77,325.54,3.66,49.87"><head>in Mean Average Precision</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,342.04,544.00,197.96,8.96;5,324.04,555.16,214.43,8.96;5,324.04,566.32,216.04,8.96;5,324.04,577.48,215.94,8.96;5,324.04,588.64,215.97,8.96;5,324.04,599.80,114.57,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,465.72,544.00,74.27,8.96;5,324.04,555.16,104.67,8.96">Term Selection for Searching Printed Arabic</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,457.04,555.16,81.44,8.96;5,324.04,566.32,216.04,8.96;5,324.04,577.48,215.94,8.96;5,324.04,588.64,161.89,8.96">the Proceedings of the25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-2002)</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,342.04,622.12,197.97,8.96;5,324.04,633.28,214.36,8.96;5,324.04,644.44,214.60,8.96;5,324.04,655.60,215.94,8.96;5,324.04,666.76,180.81,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,328.11,633.28,210.29,8.96;5,324.04,644.44,96.39,8.96">Inductive Learning Algorithms and Representations for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,443.67,644.44,94.98,8.96;5,324.04,655.60,215.94,8.96;5,324.04,666.76,144.41,8.96">Proceedings of the 7th International Conference on Information and Knowledge Management (CIKM 98</title>
		<meeting>the 7th International Conference on Information and Knowledge Management (CIKM 98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,42.64,198.01,8.96;6,72.04,53.68,215.97,8.96;6,72.04,64.84,75.81,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,173.92,42.64,114.13,8.96;6,72.04,53.68,100.85,8.96">Trends and controversies: Support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,181.48,53.68,102.06,8.96">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,87.16,197.99,8.96;6,72.04,98.32,215.98,8.96;6,72.04,109.48,214.48,8.96;6,72.04,120.64,113.73,8.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,219.21,87.16,68.82,8.96;6,72.04,98.32,215.98,8.96;6,72.04,109.48,107.79,8.96">Relating the new language models of information retrieval to the traditional retrieval models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
		<idno>TR-CTIT-00-09</idno>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">CTIT Technical Report</note>
</biblStruct>

<biblStruct coords="6,90.04,142.96,197.94,8.96;6,72.04,154.12,216.01,8.96;6,72.04,165.28,215.97,8.96;6,72.04,176.44,121.41,8.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,144.64,142.96,143.34,8.96;6,72.04,154.12,216.01,8.96;6,72.04,165.28,64.67,8.96">Making large-Scale SVM Learning Practical Advances in Kernel Methods -Support Vector Learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<editor>B. Schölkopf and C. Burges and A. Smola</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,198.76,196.33,8.96;6,72.04,209.92,216.01,8.96;6,72.04,221.08,215.94,9.08;6,72.04,232.24,148.65,9.08" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,151.51,198.76,134.87,8.96;6,72.04,209.92,216.01,8.96;6,72.04,221.08,30.90,8.96">Text categorization with support vector machines: learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,129.88,221.20,158.10,8.96;6,72.04,232.36,116.70,8.96">Proc. 10th European Conference on Machine Learning ECML-98</title>
		<meeting>10th European Conference on Machine Learning ECML-98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,254.56,196.43,8.96;6,72.04,265.72,215.94,8.96;6,72.04,276.88,215.97,8.96;6,72.04,288.04,66.33,8.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,143.29,254.56,143.19,8.96;6,72.04,265.72,215.94,8.96;6,72.04,276.88,116.53,8.96">TNO at CLEF-2001&apos;. In Results of the CLEF-2001 Cross-Language System Evaluation Campaign (Working Notes)</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="29" to="40" />
			<pubPlace>Darmstadt, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,310.36,197.97,8.96;6,72.04,321.52,215.94,8.96;6,72.04,332.68,215.98,8.96;6,72.04,343.84,216.01,8.96;6,72.04,355.00,215.94,8.96;6,72.04,366.16,215.96,8.96;6,72.04,377.32,183.81,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,76.62,321.52,211.36,8.96;6,72.04,332.68,215.98,8.96;6,72.04,343.84,31.40,8.96">Improving Stemming for Arabic Information Retrieval: Light Stemming and Co-Occurance Analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,132.32,343.84,155.73,8.96;6,72.04,355.00,215.94,8.96;6,72.04,366.16,215.96,8.96;6,72.04,377.32,21.67,8.96">the Proceedings of the25th Annu al International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-2002)</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,400.00,198.04,8.96;6,72.04,411.16,160.53,8.96" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Dave</forename><surname>Lewis</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>personal communication, TREC 2001 Batch Filtering Task Experiments</note>
</biblStruct>

<biblStruct coords="6,90.04,433.84,197.97,8.96;6,72.04,445.00,216.01,8.96;6,72.04,456.16,215.97,8.96;6,72.04,467.32,215.97,8.96;6,72.04,478.48,216.01,9.08;6,72.04,489.64,215.97,9.08;6,72.04,500.80,85.89,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,235.84,445.00,52.21,8.96;6,72.04,456.16,215.97,8.96;6,72.04,467.32,109.58,8.96">JHU/APL at TREC 2001: Experiments in Filtering and in Arabic, Video, and Web Retrieval</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cash</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,145.72,478.60,142.33,8.96;6,72.04,489.76,90.10,8.96">Proceedings of the Tenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07">2001. July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,523.12,197.98,8.96;6,72.04,534.28,214.41,8.96;6,72.04,545.44,215.97,9.08;6,72.04,556.60,114.90,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,254.97,523.12,33.05,8.96;6,72.04,534.28,132.75,8.96">Scalable Multilingual Information Access</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,218.00,534.28,68.46,8.96;6,72.04,545.44,182.15,9.08">Draft version in the Proceedings of the CLEF-2002 Workshop</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09-20">19-20 September 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,578.92,197.98,8.96;6,72.04,590.08,215.94,9.08;6,72.04,601.24,165.69,9.08" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,152.41,578.92,135.61,8.96;6,72.04,590.08,168.44,8.96">Knowledge -light Asian Language Text Retrieval at the NTCIR-3 Workshop</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,253.60,590.20,34.38,8.96;6,72.04,601.36,135.71,8.96">Working Notes of the 3rd NTCIR Workshop</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.04,623.56,197.99,8.96;6,72.04,634.72,216.01,8.96;6,72.04,645.88,156.30,8.96;6,228.40,644.19,5.04,4.54;6,236.80,645.88,51.25,8.96;6,72.04,657.04,215.94,8.96;6,72.04,668.20,215.97,8.96;6,72.04,679.36,54.09,8.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,282.75,623.56,5.28,8.96;6,72.04,634.72,216.01,8.96;6,72.04,645.88,26.43,8.96">A Hidden Markov Model Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,121.16,645.88,107.18,8.96;6,228.40,644.19,5.04,4.54;6,236.80,645.88,51.25,8.96;6,72.04,657.04,215.94,8.96;6,72.04,668.20,142.54,8.96">the Proceedin gs of the 22 nd International Conference on Research and Development in Information Retrieval (SIGIR-99)</title>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,42.64,198.01,8.96;6,324.04,53.68,214.46,8.96;6,324.04,64.84,215.97,8.96;6,324.04,76.00,215.94,8.96;6,324.04,87.16,215.97,8.96;6,324.04,98.32,114.57,8.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,504.41,42.64,35.63,8.96;6,324.04,53.68,175.80,8.96">Emprical Studies in Strategies for Arabic Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,526.28,53.68,12.22,8.96;6,324.04,64.84,215.97,8.96;6,324.04,76.00,215.94,8.96;6,324.04,87.16,161.89,8.96">the Proceedings of the25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-2002)</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,165.28,173.65,8.96;6,324.04,186.52,215.97,8.96;6,324.04,197.68,215.94,8.96;6,324.04,208.84,215.94,8.96;6,324.04,220.00,216.03,8.96;6,324.04,231.16,121.17,8.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,472.60,197.68,67.38,8.96;6,324.04,208.84,215.94,8.96;6,324.04,220.00,63.62,8.96">We gratefully acknowledge the provision of the research corpus by Reuters Limited</title>
		<ptr target="http://www.almisbar.com/salam_trans.html" />
	</analytic>
	<monogr>
		<title level="m" coord="6,342.04,186.52,62.56,8.96">Reuters Corpus</title>
		<imprint>
			<publisher>English Language</publisher>
			<date type="published" when="1996-08-20">1996-08-20 to 1997-08-19</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>without it, our filtering experiments would not have been possible</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
