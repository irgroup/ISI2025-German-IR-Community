<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,133.65,129.91,328.62,15.49">The University of Amsterdam at TREC 2002</title>
				<funder ref="#_xJdnuZd #_GDBZPu3 #_N4mqC4q">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_WgzSY8F #_EuRegNr #_KUnTJ4B #_t8X3zxD">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Physical Sciences Council</orgName>
				</funder>
				<funder ref="#_8HzjjwY">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,171.59,162.40,74.71,10.76"><forename type="first">Christof</forename><surname>Monz</surname></persName>
							<email>christof@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Language &amp; Inference Technology group ILLC</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.25,162.40,63.92,10.76"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<email>kamps@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Language &amp; Inference Technology group ILLC</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.13,162.40,90.19,10.76"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language &amp; Inference Technology group ILLC</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,133.65,129.91,328.62,15.49">The University of Amsterdam at TREC 2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B0A0CAA18552697309884D990D99F3EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the TREC 2002 Novelty, Question answering, and Web tracks. We provide a detailed account of the ideas underlying our approaches to these tasks. All our runs used the FlexIR information retrieval system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At TREC 2002 we took part in the Novelty, Question Answering, and Web tracks. Our main aims for the Novelty and Web tracks was to set up baseline systems on which we plan to build in future editions of the tracks. Our main aim for the Question Answering track was to test a revised architecture of our knowledge-intensive question answering system Tequesta <ref type="bibr" coords="1,82.73,451.48,15.27,8.97" target="#b15">[16]</ref>, and to experiment with a number of newly added features relating to the document retrieval steps carried out within Tequesta.</p><p>For all three tracks, our experiments exploited the FlexIR information retrieval system developed at the University of Amsterdam <ref type="bibr" coords="1,92.06,511.66,15.27,8.97" target="#b14">[15]</ref>. The main goal underlying FlexIR's design is to facilitate flexible experimentation with a wide variety of retrieval components and techniques. FlexIR is implemented in Perl, and built around the standard UNIX pipeline architecture; it supports many types of pre-processing, scoring, indexing, and term-weighting methods, of which we made good use this year. Depending on the task at hand, we used different weighting schemes; see the detailed descriptions of our efforts for each of the tracks below for the exact settings.</p><p>The rest of this paper is organized as follows. In three (largely self-contained) sections we describe our work for the Novelty, Question Answering, and Web tracks. We also provide a brief concluding section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Novelty Track</head><p>In this section we describe our submissions for the TREC 2002 novelty track. The overall aim of the track is to investigate systems' abilities to locate relevant and new information within the ranked set of documents retrieved in a reply to a search engine query. Thus, systems should return infor-mation that is both new and relevant rather than whole documents containing duplicate and extraneous information <ref type="bibr" coords="1,539.28,285.00,10.58,8.97" target="#b7">[8]</ref>. The novelty task can naturally be divided into two parts. Indeed, the guidelines require that participants identify two lists of documents for a given topic <ref type="bibr" coords="1,441.10,320.87,15.27,8.97" target="#b19">[20]</ref>. The first contains the relevant sentences, and the second one (a subset of the first) contains only those sentences that add new information.</p><p>Our main interest in participating in the novelty track was in exploring the second part of the task: identifying new sentences. However, due to time constraints we had to limit ourselves to fairly straightforward approaches to both parts of the novelty task. We ended up setting a simple baseline, using established IR strategies for the relevance part, and weighted overlap for the novelty part; our aim is to build on this with more linguistically motivated techniques in the near future. The relevance part, which is the most important part of the track as it also has an obvious impact on the performance of the novelty part, requires far more work than we had anticipated.</p><p>The remainder of this section is organized as follows. After recalling some key facts about the experimental set-up, we describe our approaches to the relevance and novelty parts of the novelty task, and then list and briefly discuss our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topics and Documents</head><p>For ease of reference, we briefly highlight some key facts about the documents and topics used in the novelty track; the overview paper provides further details <ref type="bibr" coords="1,475.20,607.32,10.58,8.97" target="#b7">[8]</ref>. Initially, there were 50 topics, taken from TRECs 6, 7, and 8 (topics 300-450); after the evaluation was completed, one topic was removed as it was not found to have relevant sentences. The documents are a subset of the relevant documents for the topics. Participants are provided with a ranked list of relevant documents, with between 10 and 25 relevant documents per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computing Relevance</head><p>We approached the task of identifying relevant sentences in the following manner. For a given topic, the sentences in the relevant documents for that topic were viewed as documents themselves, thus creating a sentences-as-documents collection for each topic. We ran the topic (only using the title and description fields) against this sentences-as-documents collection using our retrieval engine FlexIR. We initially followed Salton and Buckley, who recommend the tfx.nfx weighting scheme for short queries and short documents <ref type="bibr" coords="2,266.11,162.76,15.27,8.97" target="#b17">[18]</ref>, but some informal pre-submision experiments on comparable topics and documents suggested that tfv.nfx was somewhat more effective.</p><p>Three different runs were submitted: one where all documents and topics were porter stemmed <ref type="bibr" coords="2,216.64,223.54,16.60,8.97" target="#b16">[17]</ref> (run identifier UAmsT11ntste), and a second where they were lemmatized using Helmut Schmidt's TreeTagger <ref type="bibr" coords="2,211.03,247.45,16.60,8.97" target="#b18">[19]</ref> (run identifier UAmsT11ntlem); here, each word is assigned its syntactic root through lexical look-up; mainly number, case, and tense information is removed, leaving other morphological processes such as nominalization intact. And in the third run the results of the other two runs were simply merged (run identifier UAmsT11ntcom). Our motivation for the first two runs was to see to which extent morphological normalization has an impact on the relevance and novelty parts of the task. The third run was included to determine the impact on the novelty part of the task of high recall approaches to the relevance part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computing Novelty</head><p>Our approach to the novelty part of the task was based on a non-symmetric weighted overlap score, which we use to provide graded answers to the following question: is the information contained in a sentence entailed by a sentence (or set of sentences) seen before? We say that a sentence is new (within a context) if it is not entailed by the context.</p><p>Assuming the usual definition of idf term weights, we compute the entailment score, entscore(s i , s j ), of two (sets of) sentences s i and s j by comparing the sum of the weights of terms that appear in both s i and s j to the sum of the weights of all terms in the second sentence (or set of sentences) s j :</p><formula xml:id="formula_0" coords="2,42.52,562.44,187.74,29.71">entscore(s i , s j ) = ∑ t k ∈(s i ∩s j ) idf k ∑ t k ∈s j idf k .<label>(1)</label></formula><p>In words: how many of the content-bearing terms in s j occur in s i ? Clearly, entscore(s i , s j ) varies from 0 to 1.</p><p>A few remarks are in order. First, note that our entailment score is not just a notion of similarity: in general, entscore(s i , s j ) = entscore(s j , s i ).</p><p>Second, to work with entscore and conclude that s i entails s j , it may not be sufficient to have a non-zero entailment score: we may need some positive 'entailment threshold. <ref type="bibr" coords="2,269.40,690.50,3.84,8.97">'</ref> In our experiments we used 0.6; this figure was obtained by testing our methods on the 4 samples provided by NIST as training material. The mechanism of entailment thresholds offers a large amount of flexibility for fine-tuning the entailment notion to one's purposes; see below for some discussion on this point.</p><p>To identify the list of new sentences as required by the guidelines, we simply went down our list of relevant sentences, taking the first one as our starting point, and including later ones only if they were not entailed by the ones already included. Our three runs used exactly the same ideas for their novelty parts, and differed only in the list of relevant sentences they took as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results and Discussion</head><p>To assess the results of the relevance and novelty parts of the task, the product of precision and recall (P*R) is used as measure, with separate scores for the two parts of the task. The average of P*R is meaningful even when the judgment sets sizes vary widely, as is the case for the task at hand. One downside of P*R is that in practice the scores tend to be close to 0.</p><p>Table <ref type="table" coords="2,345.51,308.13,4.98,8.97" target="#tab_0">1</ref> shows the results for each of our three runs. Taking the stemmed run as our baseline, we see that both lemmatizing and combining produce significant improvements, for both the relevance and novelty parts.  The definition of the novelty task suggests that a system's performance on the novelty part is, to a large degree, determined by its performance on the relevance part, and the considerable similarity between the plots in Figures <ref type="figure" coords="2,504.11,726.07,4.98,8.97">1</ref> and<ref type="figure" coords="2,528.30,726.07,4.98,8.97">2</ref> confirms this.</p><p>We carried out a number of post-submission experiments, using the golden standards provided by NIST. First of all, we ran some experiments to see whether we used an (almost) optimal value for the entailment threshold for our official submissions. Figure <ref type="figure" coords="3,114.65,521.36,4.98,8.97" target="#fig_1">3</ref> shows the average precision, recall, and P*R scores for our combined run (UAmsT11ntcom) with increasing values of the threshold. The value of 0.6 that we used in the submitted run is close to the optimal one, although  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UAmsT11ntcom</head><p>Recall Upperbound for recall values of 0.7 or higher would have produced slightly higher scores (0.033, +3.1%).</p><p>Furthermore, we determined an upperbound on the performance of the novelty part of our system, to get some understanding of its behavior in absolute terms. If we take the relevance results of our best run (UAmsT11ntcom) and intersect these with the novelty qrels provided by NIST, we get the best possible list of new sentences (given our relevance output). Since the precision for this optimal list is 1, it only makes sense to look at the recall for this list, which turns out to be 0.23, very close to the score actually obtained (0.22); see Figure <ref type="figure" coords="3,354.43,431.74,3.74,8.97">4</ref>.</p><p>In conclusion, while we are especially interested in the novelty part of the novelty track, it seems that the relevance part is the hardest and most important part of the task. We plan to address it more extensively than we have done so far by bringing in linguistic features; it is not obvious, however, how much this will differ from document summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Answering Track</head><p>This section describes our submissions for the question answering track at TREC 2002. Our main focus was on evaluating a basic question answering system that exploits shallow NLP techniques in combination with standard retrieval techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Description</head><p>The system architecture of Tequesta (TExtual QUESTion Answering) is fairly standard; its overall architecture is displayed in Figure <ref type="figure" coords="3,378.88,678.53,3.74,8.97" target="#fig_2">5</ref>. Like most current QA systems, Tequesta is built on top of a retrieval system. The first step is to build an index for the document collection, in this case the AQUAINT collection. Then the question is translated into a retrieval query which is sent to the retrieval system. For retrieval we use the FlexIR system described in the introduction.</p><p>The retrieval system is used to identify a set of documents that are likely to contain the answer to a question posed to Just like the top documents, the question is also parsed. The parsed output is used to determine the focus of the question. Question analysis is explained in Section 3.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Document Retrieval</head><p>For pre-fetching relevant documents that are likely to contain the answer, Tequesta uses FlexIR, which was given a total of 1,033,461 documents to index. All our official runs for TREC 2002 used the Lnu.ltc weighting scheme <ref type="bibr" coords="4,238.96,470.59,11.62,8.97" target="#b2">[3]</ref> to compute the similarity between a question and a document. For the experiments on which we report in this article, we fixed slope at 0.2; the pivot was set to the average number of unique words occurring in the collection.</p><p>To increase precision, we decided to use a lemmatizer; the lemmatizer used is TreeTagger, the same as in our experiments for the novelty track.</p><p>In document retrieval it is common practice to return a ranked list of documents, each item being adorned with the similarity score. Additionally, FlexIR returns a minimally matching span (MSM) for each document. An MSM indicates the starting (s) and ending position (e) of a text excerpt, containing all matching terms, such that there are no positions s or e , s &lt; s and e &lt; e, and neither the span s , e nor the span s, e also covers all matching terms; see also <ref type="bibr" coords="4,253.06,651.48,10.58,8.97" target="#b3">[4]</ref>. In a later stage of the question answering process, MSMs are used to restrict documents to passages which are likely to contain the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Document Analysis</head><p>Document analysis focuses on the top 50 documents that were returned by FlexIR. For each of them, we used the MSM to extract a text passage which was then analyzed further.</p><p>The passage begins with the sentence containing the beginning position of the MSM and ends with the sentence containing the ending position of the MSM. This way we make sure that the passage contains full sentences which can be parsed. Here, we used Dekang Lin's dependency parser MINIPAR <ref type="bibr" coords="4,310.71,162.76,15.27,8.97" target="#b13">[14]</ref>. Identifying sentence boundaries was accomplished by TreeTagger.</p><p>Depending on the question type -see below for more details -a named entity recognizer was applied to identify phrases that are of the same semantic type as the expected answer. This process is guided by the question classification component. For instance, if a question is looking for a numerical expression (such as age, speed, length, etc.) only expressions of that type are annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Question Analysis</head><p>Just like the top 50 documents, the questions themselves were also part-of-speech tagged, morphologically normalized, and parsed. Since there is a significant difference between word order in questions and in declarative sentences, we needed to adjust the tagger for questions. To this end, TreeTagger was trained on a set of 500 questions with part-of-speech tags annotated. We used 300 questions taken from the Penn Treebank II data set together with the 200 TREC-8 questions, which we annotated semi-automatically.</p><p>We used 33 categories to classify the focus or target of a question, some of which are listed in Figure <ref type="figure" coords="4,488.08,435.55,3.74,8.97" target="#fig_3">6</ref>.</p><p>To identify the target of a question, pattern matching is applied to assign one of the 33 categories to the question. In total, a set of 102 patterns is used to accomplish this. Some of the patterns used are shown in Table <ref type="table" coords="4,468.48,484.69,3.74,8.97" target="#tab_4">3</ref>.</p><p>If more than one pattern matches the question, it was assigned multiple targets. The patterns are ordered so that more specific patterns match first. Also, the answer selection component described in the next subsection obeys the order in which questions were categorized to find answers for more specific targets first.</p><p>Questions (2)</p><p>What river is called "China's Sorrow"?</p><p>If none of the matching strategies described so far is able to assign a target to a question, the question is categorized as unknown. As a consequence, none of the answer selection strategies which are particularly suited for the respective question targets can be applied, and a general fall back strategy is used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Answer Selection</head><p>Given the parsed and annotated top documents returned by FlexIR and given the parsed and classified questions, the actual process of identifying the answer starts. Questions of type agent ask for an animate entity, such as a person or organization, being the logical agent of an event described in the question. If the dependency structure from the question matches a dependency structure from a document and there is an animate NP in subject position, or, in case of passive voice, within a PP headed by the preposition by, we take this to be the logical agent. Of course, such an NP is disregarded if it already occurs in the question itself. Questions of type object are dealt with analogously.</p><p>Questions of type what-np are particularly interesting because they are very frequent (at least in the TREC 2002 data, where 14.8% of the questions are of this type) and explicitly require some lexical knowledge base. Questions of type what-np ask for something that is an instance of the np and that fits the further description expressed in the remainder of the question. For example, question 1525, given in (3), asks for something which is a university.</p><p>(3)</p><p>What university did Thomas Jefferson found?</p><p>In (3) university is the focus of the question and the further constraint did Thomas Jefferson found? is the topic of the question. In order to establish the relationship between an entity found in a matching dependency structure and the predicate university it is necessary to access a lexical knowledge base. Tequesta exploits WordNet for this purpose. In particular, WordNet's hyponym relations are used.</p><p>Answer candidates for all remaining question types where identified by named entity extraction where the named entity has to be of the same type as the expected answer.</p><p>Each answer candidate received a matching score depending on its position in the document. Candidates occurring within the MSM passage received a higher score than candidates occurring outside it. If the same candidate was extracted several times, possibly from different documents, their individual scores were summed up. The answer candidates were sorted by score and the answer candidate with the highest score was returned as answer. Answer candidates with identical scores were sorted randomly.</p><p>Since the score of the highest ranked answer candidate can be the sum of several occurrences, possibly from different documents, we take the document which has the largest share in the score as the supporting document, which is returned together with the answer-string.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Confidence</head><p>One of this year's changes in the TREC question answering track was to adorn an answer with a confidence score, indicating the system's trust in the returned answer. We used a rather simple approach to computing confidence. All answer candidates for a question q were ranked with respect to their answer score, yielding a sorted list of answer candidates a 1 , . . . , a n , where score(a i ) ≥ score(a i+1 ) , for 1 ≤ i ≤ n. If two answer candidates have the same score, they are sorted at random. Then, the confidence that the highest ranked answer candidate is indeed the correct answer is computed as follows:  </p><formula xml:id="formula_1" coords="5,319.63,746.20,218.18,24.28">confidence(a 1 ) = a 1 -a 2 if a 1 &gt; a 2 1 m if a 1 = • • • = a m &gt; a m+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The 2002 edition of the main QA task differs from previous years in several aspects. First of all, the document collection has changed from Disks 1-5 of the TIPSTER/TREC collection to the AQUAINT collection covering a more recent period, namely 1998-2000. A total of 500 questions is provided that seek short, fact-based answers. Some questions are not known to have an answer in the document collection. A further restriction, with respect to previous TRECs, is that each participating system is allowed to return only one response per question. A response is either a [answer-string, docid] pair or the string "NIL," The answer-string has to be an exact answer and the docid must be the id of a document in the collection that supports the answer.</p><p>An [answer-string, docid] pair is judged correct or right (R) if the answer-string consists of exactly a correct answer and that answer is supported by the document returned. If the answer-string is responsive and contains a correct answer, but the document does not support that answer, the pair will be judged "unsupported" (U). If the answer-string contains a correct answer and the document supports that answer, but the string contains more than just the answer (or is missing bits of the answer), it is judged as inexact (X). Otherwise, the pair is judged incorrect or wrong (W).</p><p>Finally, the scoring method for a run has changed in order to incorporate the confidence with which a question is answered by a system. Within the submission file the questions should be ordered from most confident response to least confident response. The final confidence-weighted score (CWS) is computed as follows:</p><formula xml:id="formula_2" coords="6,349.20,435.23,164.52,26.45">CWS = ∑ 500 i=1 1 i ∑ i j=1 [[ judgment( j) = R ]]<label>500</label></formula><p>where judgment( j) is the judgment of the NIST assessors for question j, and [[ expression ]] is 1 if expression is true, and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Submitted Runs</head><p>We submitted three runs for the main task (UAmsT11qaM1, M2, and M3).</p><p>The runs differed along 2 dimensions: the number of documents used as input for the answer selection process: either 50 documents (UAmsT11qaM1) or 100 documents (UAmsT11qaM2 and UAmsT11qaM3), and whether questions were sorted with respect to confidence or not: runs UAmsT11qaM1 and UAmsT11qaM2 were sorted with respect to confidence and run UAmsT11qaM3 was simply sorted by question id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Results and Discussion</head><p>Table <ref type="table" coords="6,335.92,701.99,4.98,8.97" target="#tab_6">4</ref> summarizes the confidence-weighted scores (CWS) for each of our three submitted runs (UAmsT11qaM1, M2, and M3) over the 500 questions.</p><p>To investigate the impact of the different judgments for partial correctness of an answer-string, we compared the strict confidence-weighted scores, as defined above, to confidence </p><formula xml:id="formula_3" coords="7,75.77,210.71,192.44,26.45">(R, X) = ∑ 500 i=1 1 i ∑ i j=1 [[ judgment( j) ∈ {R, X} ]]<label>500</label></formula><p>As can be expected, confidence-weighted scores increase as judgments become less strict. In particular, allowing for unsupported answers has a strong impact on the scoring. Comparing run UAmsT11qaM1 (using the top 50 documents) with UAmsT11qaM2 (using the top 100 documents), indicates that using a smaller set of documents for answer selection is to be preferred; although this conclusion is not supported by CWS(R,U,X). Runs UAmsT11qaM2 and UAmsT11qaM3 both use the top 100 documents, but we did not sort the responses in UAmsT11qaM3 with respect to confidence. This was meant to evaluate our confidence score computation algorithm. The results in Table 4 are very inconclusive, as UAmsT11qaM2 scores better for CWS(R,U) and CWS(R,U,X) but worse for CWS(R) and CWS(R,X).</p><p>In addition, we also calculated the precision of each run, neglecting confidence weights. E.g.,</p><formula xml:id="formula_4" coords="7,90.09,454.74,146.36,25.07">Prec(R) = ∑ 500 i=1 [[ judgment( j) = R ]]<label>500</label></formula><p>The average precision scores are displayed in Table <ref type="table" coords="7,249.97,487.66,3.74,8.97" target="#tab_7">5</ref>. As with the confidence-weighted scores, precision also increases as judging becomes less strict. Again, counting unsupported answers as correct has the strongest impact on precision. Note, that UAmsT11qaM2 and UAmsT11qaM3 have the same scores for all judgments since they differ only with respect to confidence sorting. The higher precision scores of UAmsT11qaM2 and UAmsT11qaM3 compared to UAmsT11qaM1, when allowing for unsupported answers, are probably due to the lower number of NIL answers: UAmsT11qaM1 contains 234 questions having NIL as an answer, whereas UAmsT11qaM2 and UAmsT11qaM3 contain only 88 questions having NIL as an answer. Table <ref type="table" coords="7,76.69,726.37,4.98,8.97" target="#tab_8">6</ref> offers a closer look at our primary run for the main task, UAmsT11qaM1, and provides a breakdown in terms of the individual question types. Column 1 lists the question classes as discussed in Section 3.1.3 which have at least one question </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Web Track</head><p>TREC 2002's Web track features two tasks, named page finding and topic distillation, using a recent crawl of the .gov domain (January 2002). For the named-page finding task, we experimented with plain text runs, anchor-text runs, and their combinations. For topic distillation task, we additionally experimented with ways to exploit the link and URL structure in the collection.</p><p>The remainder of this section is organized as follows. After discussing some key facts about the collection and our experimental set-up, we describe our runs for the named pages finding task, and for the topic distillation task, and then discuss our findings on the link structure of the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The .GOV Collection</head><p>The size of the .GOV collection, 1.25 million documents and in total 18 gigabytes, posed a challenge for our FlexIR system. Although CSIRO did a commendable job in preparing this collection, we occasionally stumbled upon binary content, and extremely long strings of characters. We had to implement various modifications to overcome the linux filesize limits. The resulting text-based index is 6 Gb (3.25 Gb for the index and 2.5 Gb for the inverted index).</p><p>We built two separate indexes for the .GOV collection: a text-only index, and an anchor-text index. For the free-text index, we indexed all of the documents' textual contents, decoding special html-characters into plain ASCII, and replacing diacritics with the unmarked characters. We used the Porter stemmer <ref type="bibr" coords="8,108.05,279.23,15.27,8.97" target="#b16">[17]</ref>, and a stoplist of 391 words. Our text index contains 1, 247, 753 documents. We also built a separate anchor-text only index, assigning the anchor-texts to the linked documents. Again, we used the Porter stemmer. Our anchor-text index contains 667, 737 documents, which is 53.51% of the text-based index. For the retrieval runs, we experimented with two weighting schemes, the familiar Lnu.ltc scheme and a scheme, baptized Lnm.ltc, based on minimal matching span (MSM) weighting (see section 3.1.2 for details). We did not use blind feedback in any of our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Named Page Finding Task</head><p>For the named page finding task, there are 150 short queries containing the name of a page. The average query length is 3.81 words or 3.55 words after removing stopwords. There is considerable ambiguity when retrieving a unique page characterized by such a short query. As it turned out, there is a unique relevant page for 132 of the topics, for 16 topics there are two relevant pages, and there are three relevant pages for the remaining 2 topics.</p><p>The precursor of this task was TREC 2001's home page finding task <ref type="bibr" coords="8,91.97,547.84,10.58,8.97" target="#b8">[9]</ref>. For entry page finding, non-content features such as URLs and links provided valuable information <ref type="bibr" coords="8,266.11,559.79,15.27,8.97" target="#b10">[11]</ref>. We did not see a straightforward way to use non-content features for this year's task. An alternative is to use the anchortexts in the collection <ref type="bibr" coords="8,132.76,595.66,10.58,8.97" target="#b4">[5]</ref>. For the named page finding task, we experimented with plain text runs, anchor-text runs, and their combinations. The submitted runs are shown in Table <ref type="table" coords="8,214.35,726.37,3.74,8.97" target="#tab_9">7</ref>. The text and anchor-only runs were combined in the following manner.</p><p>We only considered the first ten results of both runs; following Lee <ref type="bibr" coords="8,93.31,762.24,15.27,8.97" target="#b12">[13]</ref>, the scores are normalized using RSV i = RSV i -min i max i -min i . We assigned new weights to the documents using the summation function used by Fox and Shaw <ref type="bibr" coords="8,497.89,114.94,10.79,8.97" target="#b6">[7]</ref>: The results for our official run are shown in Table <ref type="table" coords="8,528.35,233.70,3.88,8.97" target="#tab_10">8</ref>; the column labeled 'MRR' lists the mean reciprocal rank of the first correct answer (the official measure); the column labeled 'Top 10' lists the number of topics with a correct named pages in the top 10; and the column labeled 'Unknown' lists the number of topics for which no named page was found in the top 50.</p><formula xml:id="formula_5" coords="8,310.71,114.94,242.69,21.85">RSV new = RSV 1 + RSV 2 .</formula><p>The results show that the text runs using Lnu.ltc weighting scheme were more effective than those using the Lnm.ltc scheme. The combined text and anchor-text run performed the best with an MRR of 0.4317. The anchor-text only run, which indexes only half of the documents, scores 77.08% of the text only run. The combination of both runs improves the MRR by 1.48% over the text only run; the number of topics in the top 10 is improved by 20.73% over the text only run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Topic Distillation Task</head><p>For topic distillation, only key resources in the collection will be regarded as relevant. A page can be a key resource solely by its set of links, e.g., a home page of a relevant site. The challenge is to find ways to exploit the additional structure in the documents. There are 50 topics, having on average 3.24 words (2.92 after removing stop words). Although key resources are supposedly much rarer than relevant documents, there turn out to be on average 32.12 key resources per topic. <ref type="foot" coords="8,549.21,531.85,3.69,6.63" target="#foot_0">1</ref>Similar to the named page finding task, we created runs using the text-only and anchors-only collections (see Table <ref type="table" coords="8,548.41,557.81,4.98,8.97" target="#tab_11">9</ref> for an overview of the official runs). We experimented with Realized indegree 3 5. UAmsT02WtAcs Base URL clusters 3 the following approach for exploiting the URL information (indicated as 'base URL clusters' in Table <ref type="table" coords="8,489.76,688.52,3.60,8.97" target="#tab_11">9</ref>). Since there will rarely be more than one key resource per site, we cluster pages by their base URL, and return the page with the lowest URL depth. Specifically, we assign the top 100 documents to the first 10 different base URLs. Next, we return the page with the lowest URL depth or slash-count per cluster.</p><p>We also experimented with the use of the link structure of the documents (indicated as 'realized indegree' in Table <ref type="table" coords="9,274.41,139.27,3.60,8.97" target="#tab_11">9</ref>). There exist approaches that look at the global link structure, i.e., page-rank <ref type="bibr" coords="9,124.38,163.18,10.58,8.97" target="#b1">[2]</ref>, and those that look at the local link structure surrounding an initially retrieved set of documents, i.e., Hyperlink Induced Topic Search (HITS) <ref type="bibr" coords="9,229.15,187.09,15.27,8.97" target="#b9">[10]</ref>. We follow Kleinberg <ref type="bibr" coords="9,102.12,199.04,16.60,8.97" target="#b9">[10]</ref> in considering the local set of pages containing the initially retrieved documents, plus all documents linked from, or linking to documents in this set. For the anchor text runs we used the top 100 results, and for the text runs, the local set is determined by the top 200 documents. We implemented an approach that combines both global and local link structure by comparing how much of the links of a page are present in the local set of initially retrieved documents. Specifically, we calculate the local indegree (the number of a page's incoming links that are in the local set) divided by the page's indegree (the total number of links to a page). This number, which gives an indication of the topicality, is multiplied by the local indegree. The (local) indegree by itself gives an indication of the relative importance of the page <ref type="bibr" coords="9,42.52,366.41,10.58,8.97" target="#b0">[1]</ref>. The resulting new ranking is solely based on the structural link information. The results of our official runs are shown in Table <ref type="table" coords="9,251.63,485.17,8.30,8.97" target="#tab_12">10</ref>. The official measure is precision at 10, at which the text-only run scores best with 0.1755. The anchor-text only run, covering only half the documents, scores 56.98% of the text only run. A text only run using Lnu.ltc weighting, not submitted, scored better than the official run, with a precision at 10 of 0.2102. The run using the base URL clusters fails to improve the anchor-text base run, although it improves precision at 20 and 30. The runs based on link information all perform worse than the underlying base runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Link Structure</head><p>The link structure in .GOV should be a fairly representative sample of the current Internet. 2 Figure <ref type="figure" coords="9,196.06,654.97,4.98,8.97" target="#fig_4">7</ref> shows the link distribution in the .GOV collection on a logarithmic scale. Both the distribution of outlinks, and the distribution of inlinks show a powerlaw behavior as observed by <ref type="bibr" coords="9,189.80,690.83,10.58,8.97" target="#b5">[6]</ref>. The five pages with the highest number of outlinks are:</p><p>• visibleearth.nasa.gov/browse.html (653);</p><p>2 We used the provided links id and id2url files. These contain a few bugs, e.g., G15-52-0622377 is listed as www.lib.noaa.govnewj.htm instead of www.lib.noaa.gov/edocs/..newj.htm.  It is of crucial importance for link-based approaches to be able to distinguish between intrinsic links (links within a site, mainly for navigational purposes) and transverse links (links between sites). The .GOV contains in total 11, 164, 829 links between pages in the collection. We first identified the site of a page as it base URL, with the removal of any prefix starting with www. This results in a set of 2, 413, 054 transverse links (or 22%). This reduced set still contained many within-site links, so we further reduced the set by removing links between base URLs when either is a substring of the other. For example, a link between www.nih.gov and www.nlm.nih.gov regarded as instrinsic, while a link between www.nlm.nih.gov and www.nichd.nih.gov is regarded as transverse. The resulting set of transverse links contains 1, 699, 834 links (or 15% of all links). Arguably, pages that do not receive links from other sites will rarely be key resourses. This motivated experiments with anchor-text only runs on three different indexes:</p><p>First Anchors Index Only extracting complete link descriptions in the collection. This includes all transverse links, and only a small proportion of intrinsic links (which are usually included as relative locations). All unique anchor-texts are assigned to the document to which the link points. Considering the dramatic difference in the number of inlinks discussed above, we decided to remove repeated occurrences of the same anchor-text. This resulted in a set of 313, 562 anchor-texts covering 186, 328 documents, only 15% of the collection.</p><p>Second Anchors Index Here we try to recover as many links as possible, by unfolding relative links based on the URL path of the page in which the link occurs, and simplying the resulting URL paths. This includes both intrinsic and transverse links. We again remove repeated occurrences of the same anchor-texts. The result is a set of 1, 110, 566 anchor-texts covering 667, 737 documents, which is 54% of the collection.</p><p>Third Anchors Index We use the same procedure as for the second anchors index, but now retain all links as they appear in the collection. Thus, if the same anchor-text occurs thousands of times, we include it thousands of times (similar to <ref type="bibr" coords="10,133.07,323.99,10.46,8.97" target="#b4">[5]</ref>). The resulting index is based on 2, 766, 946 anchor-texts covering 667, 737 documents, which is 54% of the collection. The post-submission experiments shown in Table <ref type="table" coords="10,238.39,475.08,9.96,8.97" target="#tab_14">11</ref> show the performance of anchor-text only runs using the three anchortext indexes. The second anchor-text index, which was used for our official runs, shows the best performance. We carried out pre-submission experiments using Kleinberg's HITS <ref type="bibr" coords="10,96.36,535.09,16.60,8.97" target="#b9">[10]</ref> in order to retrieve key resources for the topic distillation task. Table <ref type="table" coords="10,157.94,547.04,9.96,8.97" target="#tab_15">12</ref> shows the results for the test topic 'obesity in the U.S.': the 'Base top 10' are the top 10 results of the text base run; and 'HITS 100' and 'HITS 200' show the top 10 authorities over the top 100 and top 200 documents respectively. Although HITS is successful at isolating key resources, there is a considerable topic drift towards generally good 'authorities.' As is well-known, good authorities and the number of inlinks show considerable correlation <ref type="bibr" coords="10,42.52,642.68,15.77,8.97" target="#b9">[10,</ref><ref type="bibr" coords="10,62.06,642.68,7.19,8.97" target="#b0">1]</ref>. Thus, one can easily image how a loosely-related site with a high indegree can infiltrate in the HITS method. We experimented with a link-based method that tries to avoid such topic drift, by looking at the proportion of inlinks that is in the local set of documents. The top 10 results are also shown in Table <ref type="table" coords="10,109.01,702.46,8.49,8.97" target="#tab_15">12</ref>: 'Realized indegree 100' and 'Realized indegree 200' show the results over the top 100 and top 200 documents of the initial text base run. Informal evaluation shows that our combined approach is much more robust than HITS (by comparing results over different numbers of top documents), for example, when considering the top 500 ini- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we described our participation in the TREC 2002 Novelty, Question answering, and Web tracks. We set up a baseline system for the Novelty track, and showed that both lemmatizing and combining yield significant improvements for the relevance as well as the novely part. We can look at the novelty part of our system in isolation by assuming perfect output from the relevance part of our system. As it turns out, our system's recall scores for the novelty part are very close to the maximal performance. Our results for the relevance part of the task are less impressive. It seems that the relevance part is the hardest and most important part of the task.</p><p>For the question answering track, we experimented with a revised version of our Tequesta system. The main innovation was to introduce document retrieval techniques that were tuned for question answering purposes; in particular, we used high precision settings, together with minimal span matching for each document. In a later stage of the question answering process, MSMs are used to restrict documents to passages which are likely to contain the answer. Our results show considerable differences across question types, which is probably due to quality of the extraction components.</p><p>For the web track, we set up a baseline system using separate text and anchor-text indexes. We experimented with the use of non-content features, such as the URL and link structure in the collection for the topic distillation task. Our results failed to show a positive effect on retrieval effectiveness. For the named page finding task, a genuine needle-ina-haystack task, we experimented with text-only and anchortext only runs, and their combinations. Here, the combined text/anchor-text run slightly improves the mean reciprocal rank, but significantlty improves the number of topics with the named page in the top 10.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,49.74,102.80,219.81,7.46"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Comparison of relevance scores to median by topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,49.74,575.13,196.31,7.46"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Impact of the entailment threshold on novelty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,49.74,102.80,141.61,7.46"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Tequesta system architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,49.80,102.80,217.24,7.46;6,49.80,115.16,45.76,7.46"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Question targets, plus examples from the TREC-11 question set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,317.93,102.80,125.16,7.46"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Link distribution in .GOV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,310.71,361.91,242.68,193.89"><head>Table 1 :</head><label>1</label><figDesc>Summary of the results for the novelty track.</figDesc><table coords="2,449.50,373.25,52.51,8.97"><row><cell>Average P*R</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,310.71,564.75,242.68,122.16"><head>Table 2 :</head><label>2</label><figDesc>Top scores per run.</figDesc><table coords="2,310.71,576.09,242.68,110.82"><row><cell></cell><cell cols="2"># Top P*R Scores (shared, unique)</cell></row><row><cell>Run identifier</cell><cell>Relevance</cell><cell>Novelty</cell></row><row><cell>UAmsT11ntste</cell><cell>25, 3</cell><cell>23, 12</cell></row><row><cell>UAmsT11ntlem</cell><cell>37, 15</cell><cell>37, 26</cell></row><row><cell>UAmsT11ntcom</cell><cell>21, 9</cell><cell>23, 0</cell></row><row><cell cols="3">Figures 1 and 2 plot our P*R scores against the median by</cell></row><row><cell cols="3">topic. They suggest a number of things. First, while we seem</cell></row><row><cell cols="3">to do relatively poorly on the relevance part of the novelty</cell></row><row><cell cols="3">task, our performance on the novelty seems somewhat better.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,310.71,571.01,242.69,92.65"><head></head><label></label><figDesc>of type what-np form a special category. Here we use a dependency parser to identify the appropriate target, symbolized by np in the type. Usually, what-np questions are of the form What NP VP? or What NP PP VP?. After parsing the question, we use the head of the NP as target, which has what, or which as a determiner. For instance, question 1413 from the TREC 2002 question set, shown in (2), is assigned what:river as question target.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,49.19,102.80,392.92,235.23"><head>Table 3 :</head><label>3</label><figDesc>Types for question classification.</figDesc><table coords="5,49.19,114.15,392.92,223.89"><row><cell>Question target</cell><cell>Example patterns</cell></row><row><cell>name</cell><cell>/(W|w)hat( wa| i|\')s the name/</cell></row><row><cell>pers-def</cell><cell>/[Ww]ho( wa| i|\')s [A-Z][a-z]+/</cell></row><row><cell>thing-def</cell><cell>/[Ww]hat( wa| i|\')s an? /, / (was|is|are|were) a kind of what/</cell></row><row><cell>pers-ident</cell><cell>/[Ww]ho( wa| i|\')s the/</cell></row><row><cell>thing-ident</cell><cell>/[Ww](hat|hich)( wa| i|\')s the /</cell></row><row><cell>number</cell><cell>/[Hh]ow (much|many) /</cell></row><row><cell>expand-abbr</cell><cell>/stand(s)? for( what)?\s*?/, /is (an|the) acronym/</cell></row><row><cell>find-abbr</cell><cell>/[Ww]hat( i|\')s (the|an) (acronym|abbreviation) for</cell></row><row><cell>agent</cell><cell>/[Ww]ho /, / by whom[\.\?]/</cell></row><row><cell>object</cell><cell>/[Ww]hat (did|do|does) /</cell></row><row><cell>known-for</cell><cell>/[Ww]hy .+ famous/ /[Ww]hat made .+ famous/</cell></row><row><cell>aka</cell><cell>/[Ww]hat( i|\')s (another|different) name /</cell></row><row><cell cols="2">name-instance /Name (a|one|some|an) /</cell></row><row><cell>location</cell><cell>/[Ww]here(\'s)? /, / is near what /</cell></row><row><cell>date</cell><cell>/([Aa]bout )?(W|w)hen /, /([Aa]bout )?(W|w)(hat|hich) year /</cell></row><row><cell>reason</cell><cell>/[Ww]hy /</cell></row><row><cell>what-np</cell><cell>-</cell></row><row><cell>unknown</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,49.80,126.50,228.12,566.87"><head></head><label></label><figDesc>Where did Golda Meir grow up? name the name of a person or an entity in general.</figDesc><table coords="6,49.80,126.50,228.12,540.97"><row><cell>agent name or description of an animate entity</cell></row><row><cell>(Q-1424): Who won the Oscar for best actor in</cell></row><row><cell>1970?</cell></row><row><cell>aka alternative name for some entity</cell></row><row><cell>(Q-1448): What is the fear of lightning called?</cell></row><row><cell>capital capital of a state or country</cell></row><row><cell>(Q-1520): What is the capital of Kentucky?</cell></row><row><cell>date date of an event</cell></row><row><cell>(Q-1406): When did the story of Romeo and Juliet</cell></row><row><cell>take place?</cell></row><row><cell>date-birth date of birth of some person</cell></row><row><cell>(Q-1880): When was King Louis XIV born?</cell></row><row><cell>date-death date of death of some person</cell></row><row><cell>(Q-1601): When did Einstein die?</cell></row><row><cell>expand-abbr the full meaning of an abbreviation</cell></row><row><cell>(Q-1531): What does NASDAQ stand for?</cell></row><row><cell>location location of some entity</cell></row><row><cell>(Q-1818): number-height height of some entity</cell></row><row><cell>(Q-1802): How tall is Tom Cruise?</cell></row><row><cell>number-length length of some entity</cell></row><row><cell>(Q-1857): What is the length of Churchill Downs</cell></row><row><cell>racetrack?</cell></row><row><cell>number-money monetary value of some entity or</cell></row><row><cell>event</cell></row><row><cell>(Q-1645): How much is the international space sta-</cell></row><row><cell>tions expected to cost?</cell></row><row><cell>object object questions are near-reverses of the</cell></row><row><cell>agent questions. Here, the object of an action de-</cell></row><row><cell>scribed in the question is sought.</cell></row><row><cell>(Q-1590): What do grasshoppers eat?</cell></row><row><cell>pers-ident a person fitting some description ex-</cell></row><row><cell>pressed in the question</cell></row><row><cell>(Q-1769): Who is the owner of the St. Petersburg</cell></row><row><cell>Times?</cell></row><row><cell>thing-ident thing identical to the description ex-</cell></row><row><cell>pressed in the question</cell></row><row><cell>(Q-1547): What is the atomic number of uranium?</cell></row></table><note coords="6,69.73,369.59,208.19,8.97;6,69.73,381.54,50.14,8.97;6,49.80,395.49,216.17,8.97;6,69.73,407.45,180.02,8.97;6,49.80,672.45,217.00,8.97;6,69.73,684.41,198.11,8.97"><p>(Q-1436): What was the name of Stonewall Jackson's horse? number-dist spatial distance between two entities (Q-1876): How far from the earth is the sun? what-np an instance of the np fitting the description (Q-1484): What college did Allen Iverson attend?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,42.52,102.80,242.68,127.51"><head>Table 4 :</head><label>4</label><figDesc>Summary of the CWS for the main task.</figDesc><table coords="7,42.52,114.15,242.68,116.17"><row><cell>UAmsT10qa. . .</cell><cell>M1</cell><cell>M2</cell><cell>M3</cell></row><row><cell>CWS(R)</cell><cell cols="3">0.145 0.101 0.146</cell></row><row><cell>CWS(R,U)</cell><cell cols="3">0.219 0.213 0.197</cell></row><row><cell>CWS(R,X)</cell><cell cols="3">0.151 0.135 0.174</cell></row><row><cell>CWS(R,U,X)</cell><cell cols="3">0.225 0.248 0.226</cell></row><row><cell cols="4">scores where also inexact (X) or unsupported (U) answers</cell></row><row><cell>count as correct. E.g.,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CWS</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,48.90,505.58,229.93,68.53"><head>Table 5 :</head><label>5</label><figDesc>Summary of the avg. precision for the main task.</figDesc><table coords="7,48.90,516.92,229.93,57.19"><row><cell>UAmsT10qa. . .</cell><cell>M1</cell><cell>M2</cell><cell>M3</cell></row><row><cell>Prec(R)</cell><cell cols="3">0.128 0.112 0.112</cell></row><row><cell>Prec(R,U)</cell><cell cols="3">0.170 0.176 0.176</cell></row><row><cell>Prec(R,X)</cell><cell cols="3">0.134 0.132 0.132</cell></row><row><cell>Prec(R,U,X)</cell><cell cols="3">0.176 0.196 0.196</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,310.71,102.80,242.68,445.52"><head>Table 6 :</head><label>6</label><figDesc>Analysis of the scores for UAmsT11qaM1.</figDesc><table coords="7,310.71,114.15,242.68,434.17"><row><cell cols="4">Question class % quest. Prec. CWS CWS diff.</cell></row><row><cell>agent</cell><cell cols="2">5.8% 0.172 0.150</cell><cell>+3.4%</cell></row><row><cell>aka</cell><cell cols="2">3.0% 0.133 0.131</cell><cell>-9.6%</cell></row><row><cell>capital</cell><cell>0.6%</cell><cell>0 0.136</cell><cell>-6.2%</cell></row><row><cell>date</cell><cell cols="2">16.2% 0.160 0.160</cell><cell>+10.3%</cell></row><row><cell>date-birth</cell><cell cols="2">2.0% 0.300 0.164</cell><cell>+13.1%</cell></row><row><cell>date-death</cell><cell cols="2">1.2% 0.833 0.165</cell><cell>+13.7%</cell></row><row><cell>expand-abbr</cell><cell>1.8%</cell><cell>0 0.132</cell><cell>-8.9%</cell></row><row><cell>location</cell><cell cols="2">14.4% 0.097 0.148</cell><cell>+2.0%</cell></row><row><cell>name</cell><cell cols="2">4.8% 0.041 0.133</cell><cell>-8.2%</cell></row><row><cell>number-dist</cell><cell>1.4%</cell><cell>0 0.163</cell><cell>+12.4%</cell></row><row><cell>number-height</cell><cell cols="2">2.0% 0.200 0.131</cell><cell>-9.6%</cell></row><row><cell>number-length</cell><cell>0.4%</cell><cell>0 0.145</cell><cell>±0%</cell></row><row><cell>number-many</cell><cell cols="2">1.0% 0.200 0.158</cell><cell>+8.9%</cell></row><row><cell>number-people</cell><cell>0.8%</cell><cell>0 0.135</cell><cell>-6.8%</cell></row><row><cell>number-money</cell><cell cols="2">0.8% 0.250 0.175</cell><cell>+20.6%</cell></row><row><cell>number-much</cell><cell cols="2">1.8% 0.111 0.132</cell><cell>-8.9%</cell></row><row><cell>number-speed</cell><cell cols="2">0.6% 0.666 0.155</cell><cell>+6.9%</cell></row><row><cell>number-age</cell><cell cols="2">1.2% 0.166 0.155</cell><cell>+6.9%</cell></row><row><cell>object</cell><cell cols="2">1.4% 0.428 0.132</cell><cell>-8.9%</cell></row><row><cell>pers-def</cell><cell cols="2">0.8% 0.250 0.132</cell><cell>-8.9%</cell></row><row><cell>pers-ident</cell><cell cols="2">4.4% 0.090 0.141</cell><cell>-2.7%</cell></row><row><cell>thing-def</cell><cell>0.2%</cell><cell>0 0.136</cell><cell>-6.2%</cell></row><row><cell>thing-ident</cell><cell cols="2">16.2% 0.061 0.133</cell><cell>-8.2%</cell></row><row><cell>what-np</cell><cell cols="2">14.8% 0.121 0.143</cell><cell>-1.3%</cell></row><row><cell>unknown</cell><cell>2.4%</cell><cell>0 0.133</cell><cell>-8.2%</cell></row><row><cell>Total</cell><cell></cell><cell>0.128 0.145</cell><cell></cell></row><row><cell cols="4">in the TREC 2002 question set; column 2 lists the percentage</cell></row><row><cell cols="4">of questions belonging to a particular class. In column 3 the</cell></row><row><cell cols="4">individual precision scores are displayed. Column 4 lists the</cell></row><row><cell cols="4">confidence-weighted scores for each class of questions. The</cell></row><row><cell cols="4">last column records the relative difference between the mean</cell></row><row><cell cols="4">CWS for the class and the overall CWS for the run (shown at</cell></row><row><cell cols="4">the bottom of column 4). All confidence-weighted scores are</cell></row><row><cell cols="3">based on strict evaluation, i.e., CWS(R).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="8,48.90,635.44,229.93,80.09"><head>Table 7 :</head><label>7</label><figDesc>Overview of the named page finding runs.</figDesc><table coords="8,48.90,646.38,229.93,69.14"><row><cell>Run</cell><cell>Type</cell><cell>Weighting</cell></row><row><cell>1. UAmsT02WnTl</cell><cell>Text-only</cell><cell>Lnu.ltc</cell></row><row><cell>2. UAmsT02WnTm</cell><cell>Text-only</cell><cell>Lnm.ltc</cell></row><row><cell>3. UAmsT02WnA</cell><cell cols="2">Anchors-only Lnu.ltc</cell></row><row><cell>4. UAmsT02WnTlA</cell><cell>Combined 1/3</cell><cell></cell></row><row><cell>5. UAmsT02WnTmA</cell><cell>Combined 2/3</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="8,317.09,144.15,229.93,80.09"><head>Table 8 :</head><label>8</label><figDesc>Anchor-text only runs.</figDesc><table coords="8,317.09,155.09,229.93,69.14"><row><cell>Run</cell><cell>MRR</cell><cell>Top 10</cell><cell>Unknown</cell></row><row><cell>UAmsT02WnTl</cell><cell cols="3">0.4254 82 (54.7%) 46 (30.7%)</cell></row><row><cell>UAmsT02WnTm</cell><cell cols="3">0.2601 58 (38.7%) 83 (55.3%)</cell></row><row><cell>UAmsT02WnA</cell><cell cols="3">0.3279 69 (46.0%) 70 (46.7%)</cell></row><row><cell>UAmsT02WnTlA</cell><cell cols="3">0.4317 99 (66.0%) 35 (23.3%)</cell></row><row><cell cols="4">UAmsT02WnTmA 0.3672 81 (54.0%) 59 (39.3%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="8,317.09,587.68,229.93,68.13"><head>Table 9 :</head><label>9</label><figDesc>Overview of the topic distillation runs.</figDesc><table coords="8,317.09,598.62,229.93,57.19"><row><cell>Run</cell><cell>Type</cell><cell>Weighting</cell></row><row><cell>1. UAmsT02WtT</cell><cell>Text</cell><cell>Lnm.ltc</cell></row><row><cell>2. UAmsT02WtTri</cell><cell>Realized indegree 1</cell><cell></cell></row><row><cell>3. UAmsT02WtA</cell><cell>Anchors</cell><cell>Lnu.ltc</cell></row><row><cell>4. UAmsT02WtAri</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="9,48.90,394.24,229.93,80.09"><head>Table 10 :</head><label>10</label><figDesc>Official topic distillation run results.</figDesc><table coords="9,48.90,405.18,229.93,69.14"><row><cell>Run</cell><cell>Prec. at 10, 20, and 30</cell></row><row><cell>UAmsT02WtT</cell><cell>0.1755 0.1245 0.1020</cell></row><row><cell>UAmsT02WtTri</cell><cell>0.0673 0.0582 0.0463</cell></row><row><cell>UAmsT02WtA</cell><cell>0.1000 0.0714 0.0558</cell></row><row><cell>UAmsT02WtAri</cell><cell>0.0633 0.0469 0.0381</cell></row><row><cell>UAmsT02WtAcs</cell><cell>0.0653 0.0786 0.0660</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="10,48.90,372.24,229.93,92.04"><head>Table 11 :</head><label>11</label><figDesc>Anchors only run results.</figDesc><table coords="10,48.90,383.18,229.93,81.10"><row><cell>Run</cell><cell>Index</cell><cell>MRR Prec. at 10</cell></row><row><cell cols="2">UAmsT02WnA' Anchors 1.</cell><cell>0.1391</cell></row><row><cell>UAmsT02WnA</cell><cell>Anchors 2.</cell><cell>0.3279</cell></row><row><cell cols="2">UAmsT02WnA" Anchors 3.</cell><cell>0.3098</cell></row><row><cell>UAmsT02WtA'</cell><cell>Anchors 1.</cell><cell>0.0673</cell></row><row><cell>UAmsT02WtA</cell><cell>Anchors 2.</cell><cell>0.1000</cell></row><row><cell cols="2">UAmsT02WtA" Anchors 3.</cell><cell>0.0837</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="10,317.09,102.80,142.83,7.46"><head>Table 12 :</head><label>12</label><figDesc>Test Topic "obesity in the U.S." consider the link topology, can be effective. If non-relevant documents dominate the initially retrieved set of documents, one cannot expect link-based methods to deliver.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="8,325.06,744.67,228.33,7.17;8,310.71,754.13,242.68,7.17;8,310.71,763.60,25.89,7.17"><p>This is over 49 topics, ignoring Topic 582 for which there were no key resourses in the collection. There are 11 topics with less than 10 key resources.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Jaap Kamps was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs>, grant # <rs type="grantNumber">400-20-036</rs>. <rs type="person">Christof Monz</rs> was supported by the <rs type="funder">Physical Sciences Council</rs> with financial support from <rs type="funder">NWO</rs>, project <rs type="grantNumber">612-13-001</rs>. <rs type="person">Maarten de Rijke</rs> was supported by grants from <rs type="funder">NWO</rs>, under project numbers <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">220-80-001</rs>, and <rs type="grantNumber">612.000.207</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8HzjjwY">
					<idno type="grant-number">400-20-036</idno>
				</org>
				<org type="funding" xml:id="_xJdnuZd">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_GDBZPu3">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_N4mqC4q">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_WgzSY8F">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_EuRegNr">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_KUnTJ4B">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_t8X3zxD">
					<idno type="grant-number">612.000.207</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Base Top 10 www.surgeongeneral.gov/topics/obesity/calltoaction/4_2.htm 4woman.gov/faq/easyread/obesity-etr.htm whi.nih.gov/guidelines/obesity/e_txtbk/intro/intro.htm www.surgeongeneral.gov/topics/obesity/calltoaction/2_0.htm www.surgeongeneral.gov/topics/obesity/calltoaction/fact_glance.htm www.surgeongeneral.gov/topics/obesity/calltoaction/principles.htm www.cdc.gov/nccdphp/dnpa/obesity/trend/maps/ www.nalusda.gov/ttic/tektran/data/000010/76/0000107699.html www.nalusda.gov/ttic/tektran/data/000010/09/0000100959.html www.surgeongeneral.gov/topics/obesity/calltoaction/2_2.htm</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tially retrieved documents HITS authorities appear almost unrelated to the topics, where as the 'realized indegree' method is still on topic.</p><p>Earlier attempts at exploiting link structure (in the ad hoc task) failed to show an improvement of retrieval effectiveness <ref type="bibr" coords="10,332.05,642.68,10.58,8.97" target="#b8">[9]</ref>. Our experiments with HITS and with the 'realized indegree' method show a decrease in precision at 10 (see <ref type="bibr" coords="10,330.36,666.59,37.32,8.97">Table 10)</ref>. A possible explanation could be the topics used for the distillation task. These are more specific than the very general topics used in <ref type="bibr" coords="10,437.19,690.50,15.27,8.97" target="#b9">[10]</ref>, such as 'java,' 'censorship,' 'search engines,' and 'Gates.' Also, after stopping, the test topic 'obesity in the U.S.' results in the one-word query 'obesity.' For such general queries, relevant documents will dominate the top 10, top 100, or even top 200 of initially retrieved documents. Under this assumption, link-based approaches, which ignore the content of documents and solely</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,64.10,750.28,221.10,8.97;11,64.10,762.23,221.10,8.97;11,332.29,102.99,221.10,8.97;11,332.29,114.94,221.11,8.97;11,332.29,126.90,221.10,8.97;11,332.29,138.85,221.10,8.97;11,332.29,150.81,87.44,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,228.69,750.28,56.51,8.97;11,64.10,762.23,221.10,8.97;11,332.29,102.99,60.21,8.97">Does &apos;authority&apos; mean quality? predicting expert quality ratings of web documents</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Amento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,491.99,114.94,61.41,8.97;11,332.29,126.90,221.10,8.97;11,332.29,138.85,217.22,8.97">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Yannakoudakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-K</forename><surname>Leong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</editor>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,173.79,221.10,8.97;11,332.29,185.74,221.10,8.97;11,332.29,197.70,221.10,8.97;11,332.29,209.65,42.34,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,419.78,173.79,133.61,8.97;11,332.29,185.74,113.77,8.97">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,464.24,185.74,89.15,8.97;11,332.29,197.70,168.35,8.97">Proceedings of the 7th International World Wide Web Conference</title>
		<meeting>the 7th International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,232.63,221.10,8.97;11,332.29,244.59,221.10,8.97;11,332.29,256.54,221.10,8.97;11,332.29,268.50,221.10,8.97;11,332.29,280.45,60.60,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,498.54,232.63,54.85,8.97;11,332.29,244.59,141.59,8.97">New retrieval approaches using SMART: TREC 4</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,351.75,256.54,201.64,8.97;11,332.29,268.50,57.74,8.97">Proceedings of the Fourth Text REtrieval Conference (TREC-4)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Fourth Text REtrieval Conference (TREC-4)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="500" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,303.43,221.10,8.97;11,332.29,315.39,221.10,8.97;11,332.29,327.34,62.54,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,497.05,303.43,56.34,8.97;11,332.29,315.39,129.98,8.97">Exploiting redundancy in question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<editor>Kraft et al.</editor>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,350.32,221.10,8.97;11,332.29,362.28,221.10,8.97;11,332.29,374.23,84.12,8.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,517.53,350.32,35.86,8.97;11,332.29,362.28,161.97,8.97">Effective site finding using link anchor information</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<editor>Kraft et al.</editor>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,397.21,221.10,8.97;11,332.29,409.17,221.10,8.97;11,332.29,421.12,122.86,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,511.34,397.21,42.05,8.97;11,332.29,409.17,161.05,8.97">On powerlaw relationships of the internet topology</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,500.81,409.17,52.59,8.97;11,332.29,421.12,26.34,8.97">In ACM SIG-COMM</title>
		<imprint>
			<biblScope unit="page" from="251" to="262" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,444.10,221.10,8.97;11,332.29,456.06,221.10,8.97;11,332.29,468.01,221.10,8.97;11,332.29,479.97,221.10,8.97;11,332.29,491.92,108.49,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,450.26,444.10,103.13,8.97;11,332.29,456.06,32.21,8.97">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,471.59,456.06,81.81,8.97;11,332.29,468.01,113.96,8.97">The Second Text Retrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="500" to="215" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,514.90,221.10,8.97;11,332.29,526.86,91.23,8.97" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,397.83,514.90,155.56,8.97;11,332.29,526.86,21.15,8.97">Overview of the TREC 2002 Novelty Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,549.84,221.10,8.97;11,332.29,561.79,221.10,8.97;11,332.29,573.75,27.40,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,456.63,549.84,96.76,8.97;11,332.29,561.79,61.65,8.97">Overview of the TREC-2001 web track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,415.15,561.79,89.83,8.97">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="25" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,596.72,221.10,8.97;11,332.29,608.68,221.11,8.97;11,332.29,620.64,22.42,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,406.33,596.72,147.06,8.97;11,332.29,608.68,75.84,8.97">Authoritative structures in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,417.58,608.68,76.96,8.97">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,643.61,221.10,8.97;11,332.29,655.57,221.10,8.97;11,332.29,667.52,221.10,8.97;11,332.29,679.48,221.11,8.97;11,332.29,691.43,221.10,8.97;11,332.29,703.39,219.48,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,520.21,643.61,33.18,8.97;11,332.29,655.57,217.11,8.97">The importance of prior probabilities for entry page search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,402.24,679.48,151.16,8.97;11,332.29,691.43,221.10,8.97;11,332.29,703.39,135.88,8.97">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and development in information retrieval</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Myaeng</surname></persName>
		</editor>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.29,726.37,221.10,8.97;11,332.29,738.32,221.10,8.97;11,332.29,750.28,221.10,8.97;11,332.29,762.23,105.56,8.97" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
		<title level="m" coord="11,352.26,738.32,201.13,8.97;11,332.29,750.28,221.10,8.97;11,332.29,762.23,76.77,8.97">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.10,102.99,221.10,8.97;12,64.10,114.94,221.10,8.97;12,64.10,126.90,221.11,8.97;12,64.10,138.85,221.10,8.97;12,64.10,150.81,221.10,8.97;12,64.10,162.76,87.44,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,106.19,102.99,179.01,8.97;12,64.10,114.94,125.78,8.97">Combining multiple evidence from different properties of weighting schemes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,225.01,126.90,60.20,8.97;12,64.10,138.85,221.10,8.97;12,64.10,150.81,217.22,8.97">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Edward</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Ingwersen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raya</forename><surname>Fidel</surname></persName>
		</editor>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.10,182.69,221.10,8.97;12,64.10,194.64,221.10,8.97;12,64.10,206.60,221.10,8.97;12,64.10,218.55,138.91,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,105.63,182.69,179.56,8.97;12,64.10,194.64,87.30,8.97">PRINCIPAR-an efficient, broad-coverage, principle-based parser</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,173.17,194.64,112.03,8.97;12,64.10,206.60,221.10,8.97;12,64.10,218.55,54.03,8.97">Proceedings of the 15th International Conference on Computational Linguistics (COLING-94)</title>
		<meeting>the 15th International Conference on Computational Linguistics (COLING-94)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.10,238.48,221.10,8.97;12,64.10,250.43,221.10,8.97;12,64.10,262.39,221.10,8.97;12,64.10,274.34,221.10,8.97;12,64.10,286.30,208.09,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,189.95,238.48,95.25,8.97;12,64.10,250.43,221.10,8.97;12,64.10,262.39,75.98,8.97">Shallow morphological analysis in monolingual information retrieval for Dutch, German and Italian</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,182.47,274.34,102.73,8.97;12,64.10,286.30,45.89,8.97">Proceedings CLEF 2001, LNCS 2406</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting>CLEF 2001, LNCS 2406</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="262" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.10,306.23,221.10,8.97;12,64.10,318.18,221.10,8.97;12,64.10,330.14,174.95,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,181.00,306.23,104.19,8.97;12,64.10,318.18,202.46,8.97">Tequesta: The University of Amsterdam&apos;s texual question answering system</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,64.10,330.14,88.35,8.97">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.10,350.06,221.10,8.97;12,64.10,362.02,86.62,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,113.66,350.06,127.33,8.97">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,247.43,350.06,33.05,8.97">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.10,381.94,221.10,8.97;12,64.10,393.90,221.10,8.97;12,64.10,405.85,143.41,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,173.53,381.94,111.67,8.97;12,64.10,393.90,104.22,8.97">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,178.88,393.90,106.32,8.97;12,64.10,405.85,49.37,8.97">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.10,425.78,221.10,8.97;12,64.10,437.73,221.11,8.97;12,64.10,449.69,213.66,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,116.04,425.78,169.16,8.97;12,64.10,437.73,53.66,8.97">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,136.60,437.73,148.61,8.97;12,64.10,449.69,184.54,8.97">Proceedings of International Conference on New Methods in Language Processing</title>
		<meeting>International Conference on New Methods in Language Processing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.10,469.61,221.10,8.97;12,64.10,482.82,208.32,7.04;12,64.10,494.78,119.35,7.04" xml:id="b19">
	<monogr>
		<ptr target="http://trec.nist.gov/act_part/guidelines/novelty_guidelines.html" />
		<title level="m" coord="12,64.10,469.61,217.15,8.97">The TREC 2002 novelty track guidelines</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.10,513.45,221.11,8.97;12,64.10,525.40,221.10,8.97;12,64.10,537.36,221.10,8.97;12,64.10,549.31,87.44,8.97" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<title level="m" coord="12,244.82,513.45,40.39,8.97;12,64.10,525.40,221.10,8.97;12,64.10,537.36,221.10,8.97;12,64.10,549.31,58.08,8.97">The Tenth Text Retrieval Conference (TREC 2001). National Institute for Standards and Technology. NIST Special Publication 500-250</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
