<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.80,77.03,333.62,11.66">Some Similarity Computation Methods in Novelty Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,211.92,112.96,76.77,10.80"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
							<email>mftsai@nlg.csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.73,112.96,71.79,10.80"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hh_chen@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.80,77.03,333.62,11.66">Some Similarity Computation Methods in Novelty Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF8FEA648E41588364871FE17FEE95D9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the novelty task, the amount of information of a sentence that can be used in similarity computation is the major challenging issue. Some sort of information expansion methods was introduced to tackle this problem.</p><p>Our approach to relevance identification was to expand the information of a sentence with the context of this sentence using a sliding window method. The similarity was measured by the number of words of a topic description that match the sentences within a window. Besides, WordNet was employed to relax word match operation to inexact match. In the novelty detection part, we first applied a coherent text segmentation algorithm to partition the sentences extracted from the relevance identification part into several coherent segments denoting sub-topics.</p><p>Then we compute the similarity of each sentence with each segment. A sentence was in terms of a sentence-segment similarity vector. Two sentences are regarded as similar if they are related to the same sub-topics.</p><p>In this way, the redundant sentences were filtered out.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information explosion is one of challenging problems in the new information era.</p><p>How to obtain relevant information from a large amount of data collection has become important.</p><p>Current information retrieval (IR) systems only return documents satisfying users' information needs, but they do not locate the relevant sentences. Users have to go through the whole documents to find the relevant information.</p><p>Moreover, traditional IR systems do not tell out which sentences contribute new information. To filter the redundant information and locate the novel information becomes more and more important for many emerging applications like summarization and question-answering. Novelty track, the new task of TREC, aims to locate relevant and new sentences (within context) rather than the whole documents containing duplicate and extraneous information.</p><p>Only few attempts have been made so far on novelty detection problem, because there is little agreement to the definition of novelty and the lack of evaluation data.</p><p>In Topic Detection and Tracking (TDT) project <ref type="bibr" coords="2,320.12,94.96,182.95,10.80" target="#b0">(Allan, Carbonnell and Yamron, 2002)</ref>, link detection task relates news stories on the same topic <ref type="bibr" coords="2,378.00,112.96,105.48,10.80" target="#b2">(Chen and Ku, 2002)</ref> and first story detection tries to find out the first article of a new event. It is some sort of novelty detection on document level. In novelty track of TREC, the basic unit that we confront with is a sentence. The amount of information of a sentence that can be used in similarity computation is the major challenging issue. In multi-document summarization <ref type="bibr" coords="2,165.67,202.96,122.53,10.80" target="#b1">(Chen and Huang, 1999;</ref><ref type="bibr" coords="2,292.55,202.96,101.67,10.80" target="#b3">Chen and Lin, 2000)</ref>, we faced the similar problem. We had to compute the similarity of meaningful units, which contain less information than passages and documents. Word matching and thesaurus expansion were adopted to tell out if two meaningful units touch on the same theme.</p><p>This paper shows how to extract relevant sentences from several known relevant documents, and how to determine new sentences from the extracted relevant sentences. The decision about what information is new depends on the order of the occurrence of the information. In other words, "a novel sentence" means that all of the relevant information in this sentence is never covered by the relevant sentences delivered previously. Section 2 presents the architecture of our system. It uses sliding window to exact relevant sentences and uses relevant segments to exact novel sentences. Section 3 shows the performance of this system and makes some discussions. Section 4 concludes the remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>Figure <ref type="figure" coords="2,124.74,472.96,6.00,10.80">1</ref> shows the architecture of our novelty system. It is composed of two major components, i.e., a relevance detector and a novelty detector. The relevance detector receives a sequence of sentences from known relevant documents, and determines which sentence is on topic. Those relevant sentences will be delivered to the novelty detector and the redundant sentences will be filtered out. The remaining sentences are new (novel) and relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Architecture of Our Novelty System</head><p>The basic idea in our study is to measure the similarity of the sentences in the relevant documents. The following subsections will deal with the similarity model, relevance detector and novelty detector in sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Similarity Model</head><p>Because the basic unit of similarity measure is a sentence instead of the whole document, we have to deal with the problem of less information in a sentence during distinguishing relevant and irrelevant sentences. Predicate-argument structure forms the kernel of a sentence, thus verbs and nouns are important features for similarity measures. All the sentences were parsed using Eric Brill's part-of-speech tagger.</p><p>After tagging, nouns and verbs were extracted. Then we utilized WordNet to find the synonymous terms for inexact matching. Noun and verb taxonomies with hyponymy/hypernymy relations were consulted. The shortest path of each sense of word w 1 to each sense of word w 2 , denoted dist(w 1 , w 2 ), was computed. Figure <ref type="figure" coords="3,499.25,310.96,6.00,10.80">2</ref> demonstrates an example. Each note represents a synset in WordNet. In this example, the distance between universe and sky is 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: An Example of Distance Measurement</head><p>A threshold is employed to decide whether two words are similar or not. If their distance is less than the threshold, then 0.5 is added in the matching score. In summary, our similarity model is shown as follows:</p><p>Nouns in one sentence are matched to nouns in another sentence, so are verbs. The value of 1 is added to the matching score for each exact matching.</p><p>In inexact matching, we set word distance threshold to 4. In other words, if the dist(w 1 , w 2 ) is less than the threshold, the value of 0.5 is added to the matching score.</p><p>Each term is matched only once.</p><p>The similarity of two sentences is in terms of noun-similarity and verb-similarity:</p><formula xml:id="formula_0" coords="4,139.92,76.70,330.89,94.68">ab m s s sim noun = ) , ( _ 2 1 (1) cd n s s sim verb = ) , ( _ 2 1 (2) ) , ( _ ) , ( _ ) , ( 2 1 2 1 2 1 s s sim verb s s sim noun s s sim + = (3)</formula><p>where s 1 and s 2 denote two sentences, respectively; m and n denote the number of matching nouns and verbs, respectively; a and b are the total number of nouns in s 1 and s 2 , respectively; and c and d are the total number of verbs in s 1 and s 2 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevance Detector</head><p>The relevance detector aims to identify those sentences containing the relevant information from the known relevant documents. The approach to determine if a sentence is on topic is to use the above similarity function to measure the similarity of a sentence and the given topic. Its function is similar to traditional information retrieval system. The main difference is that the relevance detector extracts relevant information from sentences. The major problem of calculating similarity of a sentence and a topic is that sentence contains less information for comparison.</p><p>In Section 2.1, we try to augment a sentence with the synonymous terms retrieved from WordNet. We call it within-sentence expansion. Here one more expansion, called between-sentence expansion later, is considered. The context of a sentence is also a cue to determine relevance. In one extreme case, all the sentences surrounding the specific sentence form a context. But the context may be so large that noise may be introduced. In another extreme case, only the specific sentence is considered without adding any other sentence. In other words, it employs the information coming from itself. Trading the two extreme cases off, a sliding window controls how large a context is. Figure <ref type="figure" coords="4,327.56,562.93,6.02,10.84">2</ref> shows a sliding window of size 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: A Sliding Window (window size = 2)</head><p>A predefined relevance threshold, TH relevance , is employed to determine whether sentences within a window are on topic or not. The sentences within a window are on topic if the similarity is larger than the predefined threshold. That is, if the sentences within a window are on topic, then those sentences within a window are identified as relevant sentences and sent to the next component, i.e., novelty detector. The window size and the relevance threshold, TH relevance , are trained from the pre-released sample data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Novelty Detector</head><p>The next step is to detect new information among the sentences extracted by the relevance detector. It would be better to say that we plan to filter the redundant sentences among the relevant sentences. The key issue on the detection of new information is how to differentiate the meaning of sentences accurately. Sentences may contain too less information to distinguish their differences, so that certain information expansion method is required.</p><p>We postulate that the relevant sentences may touch on several particular sub-topics. Under this postulation, a text segmentation algorithm developed by <ref type="bibr" coords="5,90.00,382.93,110.01,10.84" target="#b4">Utiyama, et al. (2001)</ref> was employed to partition the relevant sentences into several segments. Each segment corresponds to a sub-topic. This algorithm finds the maximum-probability coherent segmentation of a given text. The similarity between each sentence and each segment is calculated, and then each sentence is represented as a sentence-segment similarity vector. Two sentences are regarded as similar if they are related to the same sub-topics. In this way, the redundant sentences are filtered out and only the novel sentences are kept. Figure <ref type="figure" coords="5,374.57,490.93,6.02,10.84">4</ref> sketches this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: An Illustration of Novelty Detector</head><p>Assume a sentence s i is represented as a vector (v i,1 , v i,2 , …, v i,n ), where n is the number of segments. Cosine function shown in formula (4) measures the similarities of two vectors.</p><formula xml:id="formula_1" coords="6,119.28,127.38,362.81,46.12">j i n k k j k i j i s s v v s s ⋅ × = ∑ =1 , , ) , cos(<label>(4)</label></formula><p>This value indicates that how similar sentences s i and s j are. From the other point of view, the higher value indicates sentence s i is somewhat redundant relative to sentence s j . A threshold of novelty decision, TH novelty , determines the degree of redundancy. If the similarity score of sentences s i and s j is larger than TH novelty , then one of the two sentences has to be filtered out depending to their temporal order. The remaining sentences are the result of the novelty detector. The novelty threshold, TH novelty , was trained from the pre-released sample data set.</p><p>In the above approach, we employed the relevant data itself to select the new information. Alternatively, we may use a reference corpus and regard each relevant sentence as a query to this corpus. An IR system may retrieve top n documents from the reference corpus for each relevant sentence. Each retrieved document is assigned a weight 1/r, where r is a rank of a retrieved document. In this way, a sentence is still represented as a vector. Cosine function measures the similarity of any two sentences, and the novel sentence is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Traditional precision and recall is counted to measure the performance of our novelty system and the product of precision and recall is also calculated for TREC measure. In the relevance part, we used description 1, description 2 as well as narrative part of the topic to retrieve relevant sentences. WordNet 1.7.1 was employed.</p><p>Tables <ref type="table" coords="6,148.55,544.93,6.02,10.84" target="#tab_0">1</ref> and<ref type="table" coords="6,178.80,544.93,6.02,10.84" target="#tab_1">2</ref> show our official runs at TREC 2002 Novelty Track. Our result of novelty part is not so good in this experiment, because the threshold, TH novelty , is set to 0.97. This setting is according to the observation in pre-released sample set. The novelty sentence is ten percent of relevant sentences, thus we applied high novelty threshold to filter more sentences. After evaluation results were returned, we found that assessor also considered the sentence is novel if the sentence is relevant. Therefore, we applied higher novelty threshold in the latter unofficial experiments. In this way, two sentences should have much higher similarity to pass the threshold if they are similar. The lower the probability two sentences pass the threshold, the higher the probability both sentences are novel. Our unofficial results are shown as follows. The set of sentences randomly selected from the target documents is regarded as a baseline model, its P*R score is 0.006. Table <ref type="table" coords="7,165.54,372.85,6.02,10.84" target="#tab_2">3</ref> lists the performance of relevance detector. The threshold for relevance detector is set to 0.4. Performance of the system (i.e., the P*R value) is improved as window size is increased from 1 to 4. When the window size is increased a little larger after the critical point, the performance starts to decline. The results show that larger window size may incorporate useful context information, but it may also select more irrelevant sentences. We chose the best performance of relevance part to experiment with the next component, Novelty detector. The experimental result is shown in Table <ref type="table" coords="7,453.13,665.41,4.49,10.84" target="#tab_3">4</ref>. In this experiment, the novelty thresholds are set to 0.98 and 0.99. Table <ref type="table" coords="7,429.58,683.41,6.02,10.84" target="#tab_3">4</ref> indicates that more sentences are filtered as TH novlety is lower. The experimental result shows that the performance of revised novelty detector is two times better than that of the original one in the formal run. However, the performance is still not comparable to the human assessors. The major reason is that the result of relevance detector contains irrelevant sentences, so novelty detector false identifies that those irrelevant sentences contain new information. As we mention before, the relevance part is the major difficulty to overcome in this task. In this paper, we proposed an approach to identify sentences that are novel and redundant as well as relevant and irrelevant. The method of matching keywords and related words in sentences may not be appropriate to the relevance part. We presented an information expansion approach to deal with this problem. We postulated that if two sentences have the similar meaning, then their behavior on information retrieval to a reference corpus (relevant sentence segments or an independent corpus) is similar. The current estimators for our approach should be improved, even though they sometimes work well on some topics. The syntactic and semantic analysis of sentences may help distinguish relevant sentence from target corpus.</p><p>To use a similarity function to measure if a sentence is on topic is similar to the function of an IR system. We may use a reference corpus, and regard a topic and a sentence as queries to this corpus. An IR system may retrieve top n documents from the reference corpus for these two queries. Each retrieved document is assigned a relevant weight by the IR system. In this way, a topic and a sentence can be in terms of two weighting vectors. Cosine function measures their similarity, and the sentence with similarity score larger than a threshold is selected. The issues behind this approach include the reference corpus, the IR system, the number of documents reported, the similarity threshold, and the number of relevant sentences extracted.</p><p>The reference corpus consulted should be large enough to cover different themes for references. In the first experiments, the document sets used in TREC-6 text collection were considered as a reference corpus. It consists of 556,077 documents.</p><p>In the initial experiments, Smart system with the basic setting (i.e., tf*idf scheme without relevance feedback) was employed. It had average precision 0.1459 on the TREC topics 301-350.</p><p>We compute the Cosine of a topic vector T and a given sentence vector S i (1 ≤ i ≤ m), where m denotes total number of the given sentences. Assume normal distribution with mean µ and standard deviation σ is adopted to specify the similarity distribution of the given sentences with a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>m S T m</head><formula xml:id="formula_2" coords="9,138.96,115.03,325.13,148.47">i i ∑ = = 1 ) , cos( µ (5) m S T m i i ∑ = - = 1 2 ) ) , (cos( µ σ (6) j i n k k j k i j i s s v v s s ⋅ × = ∑ =1 , , ) , cos(<label>(7)</label></formula><p>The percentage n denotes that top n percentages of the given sentences will be reported. Similarity thresholds (TH relevance ) shown as follows are determined by these percentages.</p><formula xml:id="formula_3" coords="9,138.00,324.80,326.08,51.37">TH relevance = µ + zσ (8) n dy e z z y - = = ∫ ∞ - - 1 2 1 ) ( 2 / 2 π φ (9)</formula><p>Even though the above dynamic approach has better performance, it is still "fixed percentage" for every topic. We consider further how to select "good" percentages for individual topics. <ref type="bibr" coords="9,206.16,418.96,101.85,10.80">Larkey et al. (2002)</ref> showed that only 5% of the sentences contained relevant materials for average topic. From their collection statistics <ref type="bibr" coords="9,90.00,454.96,98.82,10.80">(Larkey et al., 2002)</ref>, we used linear regression as follows to capture the relationship between total number of the given sentences and number of the relevant sentences. n = 47.903 -0.006x <ref type="bibr" coords="9,450.00,490.96,20.08,10.80">(10)</ref> where x is total number of given sentences, and n is the percentage.</p><p>After computing n using Formula (10), we derived z using Formula (9) and finally TH relevance using Formula (8). Table <ref type="table" coords="9,270.68,544.96,6.00,10.80" target="#tab_4">5</ref> summarizes experimental results. For different size of ranked document lists, the performance is more stable (i.e., between 0.71 and 0.81). The best average P×R is 0.081, i.e., 42.41% of human performance. Figure 5 lists the performance of each topic when 45 documents were returned by IR system. Two dotted lines, i.e., one is human performance (0.191) and the other one is baseline performance (0.006), are provided for reference. Performance of our system in 8 topics <ref type="bibr" coords="10,179.23,94.96,211.24,10.80">(358, 364, 365, 368, 397, 414, 433 and 449)</ref> is competitive to that of human judge. In contrast, performance in 6 topics <ref type="bibr" coords="10,353.81,112.96,141.28,10.80;10,90.00,131.44,21.89,10.80">(305, 312, 315, 419, 420, and 432)</ref> is lower than that of random selection. The average P×R of the remaining 36 topics are below human performance, but better than that of baseline model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,130.80,455.44,333.62,10.80"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Average P×R of Relevance Identification for Each Topic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,162.00,77.20,271.17,103.93"><head>Table 1 . Performance of Official Relevance Detection</head><label>1</label><figDesc></figDesc><table coords="7,184.08,95.41,229.94,85.72"><row><cell></cell><cell></cell><cell>Relevance Part</cell><cell></cell></row><row><cell></cell><cell cols="2">Precision (P) Recall (R)</cell><cell>P*R</cell></row><row><cell>ntu1</cell><cell>0.07</cell><cell>0.47</cell><cell>0.037</cell></row><row><cell>ntu2</cell><cell>0.07</cell><cell>0.47</cell><cell>0.033</cell></row><row><cell>ntu3</cell><cell>0.08</cell><cell>0.40</cell><cell>0.037</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,168.24,207.28,258.46,103.93"><head>Table 2 . Performance of Official Novelty Detection</head><label>2</label><figDesc></figDesc><table coords="7,184.08,225.49,229.94,85.72"><row><cell></cell><cell></cell><cell>Novelty Part</cell><cell></cell></row><row><cell></cell><cell cols="2">Precision (P) Recall (R)</cell><cell>P*R</cell></row><row><cell>ntu1</cell><cell>0.07</cell><cell>0.07</cell><cell>0.009</cell></row><row><cell>ntu2</cell><cell>0.06</cell><cell>0.07</cell><cell>0.008</cell></row><row><cell>ntu3</cell><cell>0.09</cell><cell>0.06</cell><cell>0.010</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,138.48,499.12,315.52,122.65"><head>Table 3 . Performance of Relevance Detector (TH relevance = 0.4)</head><label>3</label><figDesc></figDesc><table coords="7,138.48,517.33,300.26,104.44"><row><cell>Window size</cell><cell>Precision (P)</cell><cell>Recall (R)</cell><cell>P*R</cell></row><row><cell>1</cell><cell>0.137</cell><cell>0.211</cell><cell>0.029</cell></row><row><cell>2</cell><cell>0.094</cell><cell>0.393</cell><cell>0.037</cell></row><row><cell>3</cell><cell>0.080</cell><cell>0.474</cell><cell>0.038</cell></row><row><cell>4</cell><cell>0.077</cell><cell>0.532</cell><cell>0.041</cell></row><row><cell>5</cell><cell>0.069</cell><cell>0.565</cell><cell>0.039</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,90.00,149.20,348.74,121.68"><head>Table 4 . Performance of Novelty Detector</head><label>4</label><figDesc></figDesc><table coords="8,90.00,167.41,348.74,103.47"><row><cell>Novelty Threshold</cell><cell>Precision (P)</cell><cell>Recall (R)</cell><cell>P*R</cell></row><row><cell>0.98</cell><cell>0.123</cell><cell>0.132</cell><cell>0.016</cell></row><row><cell>0.99</cell><cell>0.099</cell><cell>0.221</cell><cell>0.022</cell></row><row><cell cols="2">4 Conclusions and Future Work</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,90.00,617.20,423.48,82.39"><head>Table 5 . Performance of Relevance Detection with Dynamic Percentages</head><label>5</label><figDesc></figDesc><table coords="9,90.00,636.79,423.48,62.81"><row><cell>doc-size</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell><cell>55</cell><cell>60</cell><cell>65</cell><cell>70</cell><cell>75</cell><cell>80</cell><cell>85</cell><cell>90</cell><cell>95</cell><cell>100</cell></row><row><cell>P</cell><cell cols="16">0.14 0.14 0.14 0.14 0.14 0.14 0.14 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.12</cell></row><row><cell>R</cell><cell cols="16">0.46 0.48 0.49 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.51 0.51 0.51 0.50</cell></row><row><cell>P×R</cell><cell cols="16">.072 .077 .079 .080 .081 .078 .078 .076 .075 .075 .074 .074 .074 .074 .074 .071</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,90.00,508.96,403.67,10.80;10,117.12,526.96,311.61,10.80" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,396.72,508.96,96.95,10.80;10,117.12,526.96,235.02,10.80">Topic Detection and Tracking: Event-Based Information Organization</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">;</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><forename type="middle">;</forename><surname>Carbonnell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Yamron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,544.96,415.01,10.80;10,117.12,562.96,388.06,10.80;10,117.12,580.96,378.36,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,322.84,544.96,182.17,10.80;10,117.12,562.96,141.39,10.80">A Summarization System for Chinese News from Multiple Sources</title>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi And</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng-Jie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,273.36,562.96,231.82,10.80;10,117.12,580.96,226.16,10.80">Proceedings of the 4th International Workshop on Information Retrieval with Asian Languages</title>
		<meeting>the 4th International Workshop on Information Retrieval with Asian Languages<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,598.96,415.01,10.80;10,117.12,616.96,388.31,10.80;10,117.12,634.96,388.44,10.80;10,117.12,652.96,102.58,10.80" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,321.17,598.96,183.84,10.80;10,117.12,616.96,44.80,10.80;10,185.04,616.96,320.39,10.80;10,117.12,634.96,61.26,10.80">Topic Detection and Tracking: Event-Based Information Organization</title>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi And</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<editor>James Allan, Jaime Carbonnell, Jonathan Yamron</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer</publisher>
			<biblScope unit="page" from="243" to="264" />
		</imprint>
	</monogr>
	<note>An NLP &amp; IR Approach to Topic Detection</note>
</biblStruct>

<biblStruct coords="10,90.00,670.96,415.25,10.80;10,117.12,688.96,388.20,10.80;10,117.12,706.96,215.59,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,325.66,670.96,168.50,10.80">A Multilingual News Summarizer</title>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi And</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuan-Jie</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,117.12,688.96,383.57,10.80">Proceedings of 18th International Conference on Computational Linguistics</title>
		<meeting>18th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="159" to="165" />
		</imprint>
		<respStmt>
			<orgName>University of Saarlandes</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,724.96,414.91,10.80;10,117.12,742.96,329.40,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,285.00,724.96,219.90,10.80;10,117.12,742.96,88.60,10.80">A statistical Model for Domain-Independent Text Segmentation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,219.36,742.96,126.70,10.80">Proceedings of ACL/EACL</title>
		<meeting>ACL/EACL</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="491" to="498" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
