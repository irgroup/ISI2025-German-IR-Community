<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.88,83.28,289.55,13.08">ICT Experiments in TREC-11 QA Main Task</title>
				<funder ref="#_tNjpDgS">
					<orgName type="full">Institute Youth Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,224.64,123.43,46.87,9.16"><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
							<email>hbxu@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.33,123.43,44.79,9.16"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.71,123.43,38.95,9.16"><forename type="first">Shuo</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.88,83.28,289.55,13.08">ICT Experiments in TREC-11 QA Main Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">00F4CD9207C3815F0B9A34CF65A13262</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TREC-QA</term>
					<term>candidate passage ranking</term>
					<term>answer matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the second time we participate in the TREC-QA track. We put emphasis on candidate passage ranking and answer matching. As to named entity tagging, we applied the latest version of GATE and did some succeeding work aiming at our goal. This paper presents our methods in detail.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We took part in the TREC-QA track for the second time this year. Of the main and list subtasks, we still undertook the main subtask. Three QA runs have been submitted for evaluation.</p><p>The document set for TREC-11 QA track has been changed to the new AQUAINT disk set released by AQUAINT Data Set Organization. The QA main task has several differences from previous years' tasks. Each question requires exactly one response, and the question set should be ordered by confidence in the response. The score assigned to each question will be 1 if the judgment is correct, and 0 otherwise. A measure that is an analogue to document retrieval's uninterpolated average precision will be used to score the run as a whole. The measure is computed as:</p><p>[sum for i=1 to 500 (#-correct-up-to-question-i/i)] / 500</p><p>Obviously, this measure will reward systems that correctly rank questions it answered correctly before questions it answered incorrectly.</p><p>Inspired by experiments on web track, we changed the weighting method of SMART to meet the need of TREC documents. We've made many experiments on candidate passages ranking to seek a better method and proper parameters. Another focus of our efforts is to improve the precision of answer extracting and matching. Aiming at the measure of TREC-11, we give priority to questions with types that were processed well in last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Description</head><p>Our TREC-11 QA system is based on last year's. SMART <ref type="bibr" coords="1,375.54,619.93,8.22,6.12" target="#b1">[2]</ref> , pairing sentences module, candidate passage ranking module and GATE <ref type="bibr" coords="1,283.50,635.53,8.22,6.12" target="#b1">[2]</ref> are used to retrieve relevant documents from data set and produce ranked named entities as candidate answers. Question analyzer analyses every question to identify the question type and keywords. Answer extracting and matching module matches the question type with the named entities. Answer outputting module outputs the most credible named entity and orders the question set by confidence in the answer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pairing sentences and Candidate passage ranking</head><p>In experiments on web track, we find that the weighting method of SMART is not so fit for TREC documents, so we modify its weighting module, taking (log(tf)+1)*idf instead of tf*idf. Then we use the revised SMART to retrieve 50 relevant documents for each question from the AQUAINT data set.</p><p>Subsequently, we parse these documents into sentences, and then assemble a candidate passage every two successive sentences that both have keywords in common with the question. The algorithm presented last year in section 4.4 of paper <ref type="bibr" coords="2,343.68,385.93,8.22,6.12" target="#b1">[2]</ref> is still used to rank the candidate passages for each question. Our new experiments show that step6 of the algorithm is of little help. So we devise a new method as an alternative. The score added to a candidate passage P is computed by:</p><formula xml:id="formula_0" coords="2,230.70,448.39,154.94,11.80">β * count_m / (count_q + count_k)</formula><p>Where count_m is the number of matching keywords between the question and the candidate passage P, count_q the number of keywords in the question and count_k the number of keywords in P. β is an experiential parameter.</p><p>To lighten the burden of GATE in next process, for each question we only reserve the top 10 or 20 ranked candidate passages for named entity tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Answer extracting, matching and outputting</head><p>We still use GATE as our Named Entity tagger. The latest version of GATE 2.0 (released on March 15, 2002) realizes the function to process a document set serially, which not only saves the time on loading modules when processing, but also allows us processing more candidate passages for one question. GATE 2.0 also optimizes the identification of type LOCATION, PERCENT, ORGANIZATION and PERSON. As to type NUMBER and MEASUREMENT, we still need to take some succeeding steps to assemble an integrated NUMBER or MEASUREMENT entity.</p><p>As in last year, we use a question analyzer to identify the question type and keywords of each question by two kinds of rules: keyword-based and template-based <ref type="bibr" coords="2,379.44,682.33,8.16,6.12" target="#b1">[2]</ref> . The main difference from last year is that we've made a consistency check on these rules to eliminate the collision when applying them.</p><p>Answer extracting, matching and outputting module compares the question type with the named entities in candidate passages and chooses the most credible named entity as final output.</p><p>To optimize for TREC-11 measure, we should order the question set by confidence in the answer. Without experiments supporting, we intuitively give priority to questions with types that were processed well by our system in last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Analysis</head><p>We have submitted three runs for QA main task. In ICTQA11a and ICTQA11b we produce question answers from the top 10 candidate passages, the difference between them is that the candidate passages are ranked with different strategies. ICTQA11b and ICTQA11c have the same ranking strategies. But in ICTQA11c, question answers are derived from the top 20 candidate passages. Table <ref type="table" coords="3,157.89,217.03,4.39,9.16">5</ref> Table <ref type="table" coords="3,184.60,335.05,4.39,9.16">5</ref>.1 Statistics over all 500 questions of our runs in TREC-11 Our system does badly on most question types, except that the No Answer, DATE and LOCATION types are done a little better. Since only one response is allowed for each question, we think it's the simple answer matching strategy that does so much harm to the performance. Lack of time, many experiments aborted, so we had to give up trying our new answer matching methods. In addition, applying syntactic and semantic parsing technique should be a good approach to solve the problem on answer extracting and matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We've participated in the TREC-QA track for two times. By communicating with friends from China and abroad, we've learned much. We've also realized that there is a long way for us to go on QA research. But we are sure to do better in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,459.18,700.63,46.13,9.16;1,90.00,716.23,139.92,9.16"><head></head><label></label><figDesc>Figure 2.1 illustrates the whole architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,185.64,263.83,245.16,9.16;2,190.08,77.04,236.16,177.12"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Figure 2.1: Architecture of the TREC-11 main QA System</figDesc><graphic coords="2,190.08,77.04,236.16,177.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,91.56,217.03,412.07,107.77"><head></head><label></label><figDesc>.1 shows the evaluation results.</figDesc><table coords="3,91.56,233.49,412.07,91.31"><row><cell>RunID</cell><cell>Confidence-weighted</cell><cell>Wrong</cell><cell>Unsupported</cell><cell>Inexact</cell><cell>Right</cell><cell>Precision of</cell><cell>Recall of</cell></row><row><cell></cell><cell>score</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>recognizing</cell><cell>recognizing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>no answer</cell><cell>no answer</cell></row><row><cell>ICTQA11a</cell><cell>0.091</cell><cell>445</cell><cell>9</cell><cell>4</cell><cell>42</cell><cell cols="2">10/58 =0.172 10/46 =0.217</cell></row><row><cell>ICTQA11b</cell><cell>0.084</cell><cell>440</cell><cell>7</cell><cell>6</cell><cell>47</cell><cell>9/69 =0.130</cell><cell>9/46 =0.196</cell></row><row><cell>ICTQA11c</cell><cell>0.088</cell><cell>435</cell><cell>8</cell><cell>9</cell><cell>48</cell><cell>9/47 =0.191</cell><cell>9/46 =0.196</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is supported by the national <rs type="programName">973 fundamental research program</rs> under contact of <rs type="grantNumber">G1998030413</rs>, the <rs type="funder">Institute Youth Fund</rs> under contact 20016280-9 and the <rs type="funder">Institute Youth Fund</rs> under contact 20026180-24. We give our thanks to all the people who have contributed to this research and development, in particular <rs type="person">Bin Wang</rs>, <rs type="person">Xueqi Cheng</rs>, <rs type="person">Yitao Zhang</rs>, <rs type="person">Sujian Li</rs> and <rs type="person">Huaping Zhang</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tNjpDgS">
					<idno type="grant-number">G1998030413</idno>
					<orgName type="program" subtype="full">973 fundamental research program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,107.04,647.05,398.28,9.16;3,90.00,662.65,207.01,9.16" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,167.05,647.05,250.30,9.16">Overview of the TREC 2001 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,439.98,647.05,65.34,9.16;3,90.00,662.65,91.26,9.16">The Tenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.00,678.25,403.44,9.16;3,90.00,693.85,388.58,9.16" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,355.49,678.25,152.95,9.16;3,90.00,693.85,92.78,9.16">TREC-10 Experiments at CAS-ICT: Filtering, Web and QA</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,202.74,693.85,154.81,9.16">The Tenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">109</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.47,709.45,399.91,9.16;3,90.00,725.05,275.84,9.16" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,181.57,709.45,306.96,9.16">Patterns of Potential Answer Expressions as Clues to the Right Answers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,90.00,725.05,201.68,9.16">The Tenth Text REtrieval Conference (TREC 10)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">293</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.47,740.65,399.87,9.16;3,90.00,756.25,287.24,9.16" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,269.15,740.65,231.45,9.16">Oracle at TREC 10: Filtering and Question-Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alpha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,101.40,756.25,154.81,9.16">The Tenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">423</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
