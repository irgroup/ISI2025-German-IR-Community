<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.40,62.97,408.87,13.01">IBM&apos;s Statistical Question Answering System -TREC-11</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,219.96,94.80,98.42,10.91"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IBM T. J. Watson Research Center</orgName>
								<address>
									<postBox>P.O.Box 218</postBox>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,326.79,94.80,64.80,10.91"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
							<email>roukos@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">IBM T. J. Watson Research Center</orgName>
								<address>
									<postBox>P.O.Box 218</postBox>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,101.40,62.97,408.87,13.01">IBM&apos;s Statistical Question Answering System -TREC-11</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A05535DAFAAAB493B9E0AF237E81B8A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we document our efforts to extend our statistical question answering system for TREC-11. We incorporated a web search feature, and novel extensions of statistical machine translation as well as extracting lexical patterns for exact answers from a supervised corpus. Without modification to our base set of thirty-one categories, we were able to achieve a confidence weighted score of 0.455 and an accuracy of 29%. We improved our model on selecting exact answers by insisting on exact answers in the training corpus and this resulted in a 7% gain on TREC-11 but a much larger gain of 46% on TREC-10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC evaluations in Question Answering provide a useful application benchmark, which allows validation of a number of component technologies for which evaluation criteria are absent by providing a score for the integration of these components. Our approach since TREC-9 has been to investigate a mathematical framework under which a useful solution for question answering could be produced. We will present our model and its novel extensions below. For training our system, we collected a 4K questionanswer corpus based on trivia questions and developed answer patterns for the TREC collection of documents. This corpus was used to drive a number of components we will describe below. This corpus also allowed us to investigate weights on features such as presence of the answer chunk in web documents and lexical patterns found in answers. We also describe our efforts after the evaluation to overcome the inexact answer problem and present results obtained since the evaluation.</p><p>In TREC-8 <ref type="bibr" coords="1,143.31,657.24,127.68,10.91" target="#b10">(Voorhees and Tice, 1999)</ref>, the NLP community began the task of evaluating Question Answering systems and has in subsequent evaluations provided significant chal-lenges to such systems. In TREC-9, the challenge was 50-byte answers and in TREC-10 it was definitional questions and handling rejection. To address these challenges, systems have largely adopted the architecture of predicting the answer tag of the desired answer, using a document retrieval method to select relevant documents and performing answer selection to obtain the target answer. In TREC-8 <ref type="bibr" coords="1,326.51,453.72,111.39,10.91" target="#b9">(Srihari and Li, 1999)</ref> obtained significant gains using an expanded class of entities (66). In TREC-9, improved performance was demonstrated by using boolean retrieval and feedback loops <ref type="bibr" coords="1,343.26,507.96,137.73,10.91" target="#b5">(Harabagiu and et. al., 2000)</ref>. In TREC-10, use of a large number of patterns was shown to perform well for retrieving answers <ref type="bibr" coords="1,508.08,535.08,26.25,10.91;1,315.00,548.64,56.04,10.91" target="#b8">(Soubbotin, 2001)</ref>. In TREC-11, the track agreed to several significant changes</p><formula xml:id="formula_0" coords="1,326.40,588.72,191.65,61.31">• Exact Answers • Single Answers • Confidence-based Ranking of Answers</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Exact Answers</head><p>Systems were required to return answers which had only the desired answer. Extra words were not accepted and their presence caused the answer to be judged inexact. Our approach of handling exact answers was to use our phrases spanned by our thirty named entity categories as well as constituent phrases of the syntactic parse of the answer (Penn Treebank style) which satisfied the answer pattern for the question. The decision to use the syntactic parse based phrases caused our system to output a large number of answers which were judged as inexact. We will describe some experiments where we changed the decision to accept only those phrases which exactly satisfy the answer pattern. Our named entity categories do not capture the differences between dates and years; nevertheless, we decided to evaluate our system without modifying the named entity categories. The named entity tags are broken along five major categories: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Single Answers</head><p>In previous TREC evaluations, systems returned upto 5 answers per questions. In TREC-11, only a single answer was returned for each question. For evaluating single answers, the criteria used in this evaluation was the accuracy of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Confidence-based Ranking of Answers</head><p>NIST changed the metric from the mean reciprocal rank (MRR) of previous TREC Q&amp;A evaluations to the Uninterpolated Mean Average</p><p>Precision, which we shall refer to as the confidence weighted score (CWS) defined as follows,</p><formula xml:id="formula_1" coords="2,330.36,99.97,192.74,30.45">CWS = 1 N N i=1 # correct upto question i i</formula><p>where N is the number of questions. This metric gives more credit to questions answered correctly at the beginning of the list. We made no specific attempt to optimize on this criteria and instead worked mostly on optimizing the accuracy of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TREC 11 System</head><p>We model the distribution p(c|a, q), which attempts to measure the c, 'correctness', of the answer and question. c can take on values of either 0 and 1 indicating either an incorrect or correct answer respectively. We introduce a hidden variable representing the class of the answer, e, (answer tag/named entity) as follows, p(c|q, a) = e p(c, e|q, a) = e p(c|e, q, a)p(e|q, a)</p><p>The terms, p(e|q, a) and p(c|e, q, a) are the familiar answer tag problem and the answer selection problem. Instead of summing over all entities, as a first approximation we only consider the top entity predicted by the answer tag model and then find the answer that maximizes p(c|e, q, a).</p><p>The distribution p(c|e, q, a) is modeled utilizing the maximum entropy framework described in <ref type="bibr" coords="2,327.82,540.72,94.24,10.91" target="#b0">(Berger et al., 1996)</ref>. We built on top of the model we used last year and those features are described in <ref type="bibr" coords="2,375.24,567.84,113.75,10.91" target="#b6">(Ittycheriah et al., 2001)</ref>. The new features we investigated for this year are:</p><p>• Occurrence of the answer candidate on the web</p><p>• Re-ranking of answer candidate window using a statistical MT dictionary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Lexical patterns from supervised training pairs</head><p>This year we submitted 3 runs, two of which measured the effectiveness of the first feature type. The last run was a feedback loop on the first run, where we included the answer string of questions which had sufficient confidence to further improve their confidence. The results are presented below in Table <ref type="table" coords="3,217.90,142.20,4.19,10.91" target="#tab_1">1</ref>. We also provided the output of system 'ibmsqa02a' to another group at IBM for the run labeled IBM-PQSQA. The integration of our system's output with their question answering system, improved their base performance from 33.8% to 35.6%, an improvement of 5.3% in accuracy and in terms of CWS, from 0.534 to 0.586 <ref type="bibr" coords="3,206.64,237.00,89.80,10.91;3,72.00,250.56,23.95,10.91" target="#b3">(Chu-Carroll et al., 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training Data</head><p>We used TREC-8, TREC-9, and 4K questions from our KM database to train the model this year. This corpus represented an order of magnitude increase in size over the training data size we used last year. For each question we developed a set of answer patterns by judging several potential answer sentences in the TREC corpus. Using the answer patterns and sentences derived from the TREC corpus, we automatically labelled chunks as being correct or incorrect. The total number of chunks used in formulating the model was 207K. There were 30K instances of correct answers (though 10K were inexact) and 177K incorrect chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Web Feature</head><p>The web feature was used by a number of groups last year <ref type="bibr" coords="3,115.11,521.76,93.88,10.91" target="#b4">(Clarke et al., 2001)</ref>  <ref type="bibr" coords="3,212.35,521.76,83.93,10.91" target="#b1">(Brill et al., 2001)</ref> and we attempted to measure its impact on our system. We incorporated the feature as two indicators: (1) occurrence of the answer candidate in the top 10 documents retrieved from the web, (2) count of the number times the answer candidate occurred. This feature type and performing no rejection is the difference between the runs ibmsqa02a and ibmsqa02b. Removing rejection the correctly rejected questions, we note only an improvement of 7 questions by using the web based feature. Our systems have traditionally used an encyclopaedia for LCA based expansion and this may explain why the web feature is less effective in our system. We refer to this method of using the web as Answer Verification to differentiate it with other approaches which attempt to answer the question on the web and then look in the target document corpus for the same answer. The latter method can result in unsupported answers. We note that the number of unsupported answers is not significantly different between runs 'ibm-sqa02a' and 'ibmsqa02b' (11 vs. 8) but when we used the answer strings of confident questions as feedback to the run 'ibmsqa02c', the number of unsupported answers went up significantly (18 unsupported answers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Statistical Machine Translation Thesaurus</head><p>Generally, an answer to a fact-seeking question can be decomposed as</p><formula xml:id="formula_3" coords="3,400.32,328.56,139.53,10.91">a = a d + a s (2)</formula><p>where a d is the desired answer and a s is the supporting evidence for the answer. Although words comprising the answer support are generally found in the question, words such as the focus of the question are sometimes deleted in the answer. Following our general approach of learning phenomena from training data, we used our question-answer corpus to train a Model 1 translation matrix <ref type="bibr" coords="3,406.25,461.76,94.97,10.91" target="#b2">(Brown et al., 1993)</ref>. Questions were tokenized with casing information folded and answers were both tokenized and name entity tagged. A question answer pair is presented below before and after the prepreprocessing.</p><p>Q unique sentences in the corpus. This data was split into two and separate translation models were derived. Entries which occurred in both translation models were retained; a few of the more interesting entries are shown below in Table 2. Each word is shown with the 5 top translation candidates. For the word "who", the model prefers to see a named entity tag "person ne" with a relative high probability. Even though the number of translation pairs is small (16.3K pairs), for the question answering application we are interested in only the most common words, which are potentially modified in the translated output of the question; rarer words have to appear identical to the form in the question. Using this additional thesaurus resource, we re-ranked the answer candidate windows (windows of text bounded by the question terms and the answer candidate) and quantized the rank into 5 bins (1,2, high, mid and low) for use in the maximum entropy answer selection module. We have not separately investigated the effect of this ranking, so details will presented in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Patterns</head><p>The approach described in <ref type="bibr" coords="4,208.92,535.32,87.43,10.91" target="#b8">(Soubbotin, 2001)</ref> uses patterns for locating answers. In a related work, <ref type="bibr" coords="4,127.93,562.44,149.48,10.91" target="#b7">(Ravichandran and Hovy, 2002)</ref> has shown how to extract patterns in an unsupervised manner from the web. In this work, we use the supervised corpus of question and answers to extract n-grams occurring in the answer. To specialize the pattern for a particular question type, the question was represented only by the question word and the first word to its right. To generalize the answer candidate window, it was modified to replace all non-stop question words with "&lt;queryTerm&gt;" and the answer candidate with "&lt;answer&gt;". So for the example above, QF: how tall MW: he started with the highest , &lt;answer&gt; &lt;queryTerm&gt; measure ne where QF stands for the question focus and MW stands for the mapped answer candidate window. Ideally, the question would be represented by more than just the word adjacent to the question word but in most cases this suffices. To overcome some of the limitations of this choice, we also chose features relating the predicted answer tag and an answer pattern. An answer pattern consists of 5-grams or larger chosen with a count cutoff. The total number of pattern features incorporated was 8.5K out 15.3K features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answer Selection</head><p>Answer selection was performed as we have in previous years with minor modifications. First, a fast-match technique of selecting answer sentences is used and top 100 sentences are selected. This phase yields sentences which have the answer-pattern in TREC-10 for 80% of the sentences. Considering the approximately 10% of questions which were to be rejected in TREC-10, the error of the sentence selector is about 10% with a list size of 100 sentences.</p><p>In order to select exact answers, we extracted all parse nodes which were noun phrases and together with all phrases which were named entities formed a candidate pool. As mentioned before, our system suffered a great deal of inexact answers in the judgement and these were mostly due to the decision to accept any phrase thus selected which had an answer pattern. Below we discuss some experiments in which a phrase is considered correct only if it contains only the answer pattern. For the training corpus of chunks with their labeled decision of correct or incorrect, we formulated features such as whether the desired named entity was found in the chunk. The features described above were added to the base model described in <ref type="bibr" coords="5,162.30,405.60,113.75,10.91" target="#b6">(Ittycheriah et al., 2001)</ref> and weights were derived using the maximum entropy algorithm. For a typical answer candidate, 50-100 features are able to fire for each decision. The answer candidate that has the highest probability is chosen for the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Rejection</head><p>For questions which are determined to have no answer in the corpus, the system was supposed to return 'NIL' as the document id. To determine which questions to reject, we employed the distribution p(c|q, a) and used a threshold on the distribution. However, the system sometimes encounters events which are not sufficiently represented in the training corpus and to allow some level of control it was useful to smooth this probability with a decreasing function of chunk rank. This smooth estimate was computed as</p><formula xml:id="formula_4" coords="5,74.28,697.83,219.95,10.99">p * = (1 -α)p(c|q, a) + α(1 -0.1(chunk rank))</formula><p>where chunk rank was saturated at 10. This year the alpha was set to 0.2 and the rejection threshold to 0.3. The rejection threshold was optimized on the accuracy of TREC-10 questions using the TREC corpus of documents. We plot in Figure <ref type="figure" coords="5,390.47,405.60,4.19,10.91" target="#fig_0">1</ref>, the cumulutive distribution function of questions with answers in the corpus and also 1.0 minus the cumulutive distribution function for questions which should be rejected. The plot is for TREC-10 questions using the TREC corpus of documents for answers. We expected to reject about 80 answers in our base system and the actual run seems to have done approximately the same. The feedback loop of ibmsqa02c seems to have reduced the number of rejections and thus the precision of rejections has improved from 0.145 to 0.224 while maintaining the recall rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis &amp; Subsequent Experiments</head><p>One method of characterizing a test set is with respect to a set of answer tags. The primary difference between TREC-10 and TREC-11 is in the composition of the answer tags and these are presented for the top set of tags in Figure <ref type="figure" coords="5,315.00,697.92,4.19,10.91">2</ref>. The drastic difference between the test sets is in the number of questions being classified PHRASE (this class represents any question which does not fall into other categories). This result reflects the reduction in definitional questions and the emphasis on exact answer questions; however, calibrating rejection rates and system strategies on TREC-10 is mismatched with the evaluation.</p><p>In order to overcome the excessive number of inexact answers produced by our system, we trained the model indicating only those phrases which exactly matched the answer pattern to be correct. As noted earlier, this is only a partial solution since some answers are now considered incorrect when they seem quite reasonable. For example in the first question of TREC-8, the answer of "Hugo Young" is now considered incorrect since the answer pattern contains only "Young". This exact match reduced the number of correct training instances about 33% (reduced from 30K to 20K where the total number of training instances is 207K); inspection of these instances indicates (a) some exact answers are now labelled incorrect, (b) majority of phrases containing the answer plus extra words are now labelled incorrect. The number of answers of type (a) is relatively small (estimated about 10% of the chunks). We then calibrated the performance of our system on TREC-11 by using the answer patterns and modifying the scoring script to accept the pattern only if if ($answer_str =~/^(\s+)?$p(\s+)?$/i)</p><p>The results of the system using the answer patterns are generally lower and each run seems to suffer about the same amount. Table <ref type="table" coords="6,503.66,398.64,5.39,10.91">5</ref> shows the results of using the new model. We emphasize that these results are obtained using the perl patterns as opposed to human judgments in the evaluation. In order to remove the effect of rejection, we modified the threshold (to 0.22 from 0.3) in the new model to output about the same number of questions rejected so that the improvement in scores is not dominated by getting only rejection questions correct. The results indicate a 46% improvement in the TREC-10 test but only about 7% gain in TREC-11. Investigating this discrepancy will be subject of future work.</p><p>In Table <ref type="table" coords="6,369.76,589.56,4.19,10.91" target="#tab_3">3</ref>, these are answers which were accepted by the evaluation system but are now training examples for the incorrect answers. Examples of system output with the exact answer fix is shown in Table 5: Experimental results using perl-patterns since TREC-11 evaluation. . overall the system was able to produce more answers which satisfied the exact match criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In TREC-11, our method of selecting which candidates were exact answers did not satisfy the exact match criteria of the evaluation. We have since modified our system to extract exact answers and retrained the system. We incorporated two novel concepts (a statistical machine translation thesaurus and lexical patterns derived from supervised question-answer pairs) since last year.</p><p>In TREC-11, although we thresholded the distribution p(c|q, a) to reject answers, this we recognize as being deficient in the following sense. We should recognize a question as not having an answer in the corpus by taking into consideration all the answers found and not just the top ranking answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>This work is supported in part by DARPA under SPAWAR contract number N66001-99-2-8916.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,158.28,278.28,294.75,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TREC-10 scores for normal and rejection questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,315.00,552.85,224.62,155.98"><head>Table 1 :</head><label>1</label><figDesc>Performance on TREC-11. .</figDesc><table coords="3,315.00,552.85,224.62,155.98"><row><cell>: How tall is Mt. Everest?</cell></row><row><cell>A: He started with the highest , 29,028</cell></row><row><cell>-foot Mt. Everest , in 1984</cell></row><row><cell>Q: how tall is mt. everest ?</cell></row><row><cell>A: he started with the highest , 29,028</cell></row><row><cell>-foot mt. everest , in 1984 measure ne</cell></row><row><cell>We had 4K training pairs from the KM trivia</cell></row><row><cell>database, 1.6K pairs from TREC8 and 10.7K</cell></row><row><cell>pairs from TREC9. The latter were derived</cell></row><row><cell>from correct judgements given to questions in</cell></row><row><cell>those evaluations and which also came from</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.00,60.13,409.48,257.74"><head>Table 2 :</head><label>2</label><figDesc>Translation entries for some question words. .</figDesc><table coords="5,130.44,60.13,351.04,202.66"><row><cell>who</cell><cell></cell><cell>haiti</cell><cell></cell><cell>river</cell><cell></cell></row><row><cell>a</cell><cell>t(a|q)</cell><cell>a</cell><cell>t(a|q)</cell><cell>a</cell><cell>t(a|q)</cell></row><row><cell cols="2">person ne 0.125</cell><cell>haiti</cell><cell>0.076</cell><cell>river</cell><cell>0.217</cell></row><row><cell>,</cell><cell>0.010</cell><cell cols="2">port-au-prince 0.048</cell><cell>the</cell><cell>0.081</cell></row><row><cell>the</cell><cell>0.051</cell><cell>miami</cell><cell>0.034</cell><cell>water</cell><cell>0.060</cell></row><row><cell>.</cell><cell>0.046</cell><cell>people</cell><cell>0.021</cell><cell cols="2">location ne 0.039</cell></row><row><cell>"</cell><cell>0.042</cell><cell>haitian</cell><cell>0.018</cell><cell>many</cell><cell>0.028</cell></row><row><cell cols="2">nuclear</cell><cell>tall</cell><cell></cell><cell>team</cell><cell></cell></row><row><cell>a</cell><cell>t(a|q)</cell><cell>a</cell><cell>t(a|q)</cell><cell>a</cell><cell>t(a|q)</cell></row><row><cell cols="2">nuclear 0.183</cell><cell cols="2">measure ne 0.056</cell><cell>team</cell><cell>0.099</cell></row><row><cell cols="2">atomic 0.020</cell><cell>foot</cell><cell>0.041</cell><cell cols="2">organization ne 0.056</cell></row><row><cell>at</cell><cell>0.013</cell><cell>feet</cell><cell>0.027</cell><cell>game</cell><cell>0.030</cell></row><row><cell>soviet</cell><cell>0.010</cell><cell>-</cell><cell>0.017</cell><cell>;</cell><cell>0.029</cell></row><row><cell>site</cell><cell>0.010</cell><cell>i</cell><cell>0.012</cell><cell>their</cell><cell>0.023</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,315.00,643.68,224.69,65.15"><head>Table 3 :</head><label>3</label><figDesc>Table 4 with the older strings as well to demonstrate the nature of the fix. The first two examples show answers which satisfy the answer patterns exactly at test time. The last two example show errors by the system, but Training data instances which are rejected for the exact answer fix. .</figDesc><table coords="7,78.00,78.23,460.38,464.28"><row><cell></cell><cell>300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trec-10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trec-11</cell></row><row><cell></cell><cell>250</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>PHR</cell><cell>PERS</cell><cell>LOC</cell><cell>DATE</cell><cell>MEAS</cell><cell>ORG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Answer Tags</cell><cell></cell></row><row><cell></cell><cell cols="7">Figure 2: Comparison of answer tags between TREC-10 and 11.</cell></row><row><cell></cell><cell cols="5">What canine was made famous by Eric</cell><cell cols="2">Lassie Come -Home</cell></row><row><cell></cell><cell>Knight?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Professor Moriarty was whose rival?</cell><cell></cell><cell cols="2">Sherlock Holmes' neme-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sis</cell></row><row><cell></cell><cell cols="7">What is Francis Scott Key best known for? write the Star Spangled</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Banner</cell></row><row><cell cols="2">Qnum Question</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Old Answer</cell><cell>Answer</cell></row><row><cell>1059</cell><cell cols="3">What peninsula is Spain part of?</cell><cell></cell><cell cols="3">position on the Iberian</cell><cell>Iberian Peninsula</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Peninsula</cell></row><row><cell>1215</cell><cell cols="4">When was President Kennedy shot?</cell><cell cols="3">shot on Nov. 22 , 1963</cell><cell>Nov. 22 , 1963</cell></row><row><cell>1316</cell><cell cols="4">What was the name of the plane Lind-</cell><cell cols="3">Spirit of St. Louis</cell><cell>Charles A.</cell></row><row><cell></cell><cell cols="3">bergh flew solo across the Atlantic?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1348</cell><cell cols="3">How cold should a refrigerator be?</cell><cell></cell><cell cols="3">28 degrees Farenheit</cell><cell>soda ice cold</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.00,559.68,394.29,104.39"><head>Table 4 :</head><label>4</label><figDesc>TREC-10 QA pairs before and after the exact answer fix. .</figDesc><table coords="7,145.08,597.36,321.21,66.71"><row><cell>System</cell><cell>Description</cell><cell cols="3">CWS Right Wrong Rej</cell></row><row><cell>trec10-perl</cell><cell>Base system</cell><cell>0.289 92</cell><cell>408</cell><cell>10/76</cell></row><row><cell>trec10-perl</cell><cell cols="2">Exact answer fix 0.423 127</cell><cell>373</cell><cell>16/92</cell></row><row><cell cols="2">ibmsqa02a-perl Base system</cell><cell>0.438 134</cell><cell>366</cell><cell>12/83</cell></row><row><cell cols="3">ibmsqa02a-perl Exact answer fix 0.469 144</cell><cell>356</cell><cell>4/52</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,445.55,224.85,9.96;8,82.92,456.47,214.02,9.96;8,82.92,467.39,213.89,9.96;8,82.92,478.43,153.55,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,207.13,456.47,89.81,9.96;8,82.92,467.39,178.50,9.96">A maximum entropy approach to natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Della</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietra</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Della</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietra</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,272.95,467.73,23.86,9.18;8,82.92,478.77,92.49,9.18">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,497.87,224.87,9.96;8,82.92,508.79,213.87,9.96;8,82.92,519.71,134.46,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,109.88,508.79,146.34,9.96">Data-intensive question answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,265.02,509.13,31.77,9.18;8,82.92,520.05,61.56,9.18">TREC-10 Proceedings</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,539.15,224.83,9.96;8,82.92,550.19,214.14,9.96;8,82.92,561.11,214.14,9.96;8,82.92,572.03,213.89,9.96;8,82.92,583.07,88.93,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,82.92,561.11,214.14,9.96;8,82.92,572.03,117.79,9.96">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,210.61,572.37,86.20,9.18;8,82.92,583.41,30.91,9.18">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,602.51,224.94,9.96;8,82.92,613.43,214.11,9.96;8,82.92,624.35,213.90,9.96;8,82.92,635.39,213.86,9.96;8,82.92,646.65,37.27,9.18" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,82.92,624.35,213.90,9.96;8,82.92,635.39,81.01,9.96">A multi-strategy and multi-source approach to question answering</title>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Ferruci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>To appear in TREC-11 Proceedings</note>
</biblStruct>

<biblStruct coords="8,72.00,665.75,224.83,9.96;8,82.92,676.67,214.04,9.96;8,82.92,687.71,214.02,9.96;8,82.92,698.63,214.17,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,229.46,676.67,67.50,9.96;8,82.92,687.71,214.02,9.96;8,82.92,698.63,16.66,9.96">Web reinforced question answering (multitext experiments for trec</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Mclearn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,132.78,698.97,92.73,9.18">TREC-10 Proceedings</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="673" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,315.00,61.55,224.91,9.96;8,325.92,72.59,213.86,9.96;8,325.92,83.51,78.39,9.96" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<title level="m" coord="8,466.18,61.55,73.73,9.96;8,325.92,72.59,213.86,9.96;8,325.92,83.85,16.02,9.18">Falcon: Boosting knowledge for answer engines. TREC-9 Proceedings</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="50" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,315.00,103.43,224.84,9.96;8,325.92,114.35,214.01,9.96;8,325.92,125.39,213.73,9.96;8,325.92,136.31,65.07,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,404.68,114.35,135.24,9.96;8,325.92,125.39,106.54,9.96">IBM&apos;s statistical question answering system -trec-10</title>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,441.60,125.73,93.69,9.18">TREC-10 Proceedings</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="258" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,315.00,156.23,225.04,9.96;8,325.92,167.15,214.00,9.96;8,325.92,178.19,213.98,9.96;8,325.92,189.11,147.57,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,325.92,167.15,214.00,9.96;8,325.92,178.19,62.61,9.96">Learning surface text patterns for a question answering system</title>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,408.65,178.53,131.25,9.18;8,325.92,189.45,83.39,9.18">Proceedings of the 40th Annual Meeting of the ACL</title>
		<meeting>the 40th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,315.00,209.03,224.95,9.96;8,325.92,219.95,214.13,9.96;8,325.92,230.99,166.36,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,430.13,209.03,109.82,9.96;8,325.92,219.95,209.56,9.96">Patterns of potential answer expressions as clues to the right answers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,325.92,231.33,93.45,9.18">TREC-10 Proceedings</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,315.00,250.91,225.06,9.96;8,325.92,261.83,213.90,9.96;8,325.92,272.75,119.37,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,464.53,250.91,75.53,9.96;8,325.92,261.83,173.55,9.96">Question answering supported by information extraction</title>
		<author>
			<persName coords=""><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,508.05,262.17,31.77,9.18;8,325.92,273.09,56.53,9.18">TREC-8 Proceedings</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="75" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,315.00,292.67,225.08,9.96;8,325.92,303.71,213.98,9.96;8,325.92,314.63,174.52,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,325.92,303.71,209.68,9.96">The TREC-8 question answering track evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,325.92,314.97,88.30,9.18">TREC-8 Proceedings</title>
		<imprint>
			<date type="published" when="1999-11">1999. Nov</date>
			<biblScope unit="page" from="41" to="63" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
