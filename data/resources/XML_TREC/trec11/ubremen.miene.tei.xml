<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.66,115.96,292.04,12.62;1,151.93,133.89,311.49,12.62">Automatic Shot Boundary Detection and Classification of Indoor and Outdoor Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,173.34,171.56,37.78,8.74"><forename type="first">A</forename><surname>Miene</surname></persName>
						</author>
						<author>
							<persName coords="1,219.28,171.56,49.02,8.74"><forename type="first">Th</forename><surname>Hermes</surname></persName>
						</author>
						<author>
							<persName coords="1,276.65,171.56,52.07,8.74"><forename type="first">G</forename><surname>Ioannidis</surname></persName>
						</author>
						<author>
							<persName coords="1,336.28,171.56,34.75,8.74"><forename type="first">R</forename><surname>Fathi</surname></persName>
						</author>
						<author>
							<persName coords="1,397.99,171.56,44.03,8.74"><forename type="first">O</forename><surname>Herzog</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">TZI -Center for Computing Technologies</orgName>
								<orgName type="institution">University of Bremen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Universitätsallee</orgName>
								<address>
									<addrLine>21-23</addrLine>
									<postCode>D-28359</postCode>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.66,115.96,292.04,12.62;1,151.93,133.89,311.49,12.62">Automatic Shot Boundary Detection and Classification of Indoor and Outdoor Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">30CE66AD077F1079C14C859EC2D5AAF3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our contribution to the TREC 2002 video analysis track. We participated in the shot detection task and in the feature extraction task (features indoors and outdoors). The shot detection approach is based on histogram differences and uses adaptive thresholds. Multiple detected shot boundaries that follow each other within a short temporal interval are grouped together and classified as a gradual change beginning with the first and ending with the last shot boundary in the interval.</p><p>For the feature extraction task we examined whether it is possible to classify indoor and outdoor shots by their color distribution. In order to analyze the color distribution we use first order statistical features. The shots are classified into indoor and outdoor shots using a neural net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Center for Computing Technologies (TZI), University of Bremen, Germany, participated in the video analysis track in the shot detection task and in the feature extraction task (features indoors and outdoors).</p><p>The shot detection approach is based on histogram differences. It is divided into two steps -feature extraction and shot boundary detection. Firstly, the histogram differences are calculated for the entire video in real time. Secondly, shot boundaries are detected. The advantage of this approach is the possibility to set adaptive thresholds for the shot boundary detection considering all extracted features of the complete video sequence. The adaptive threshold is set to a percentage of the maximum of all calculated difference values of the video. In the case of gradual changes, often multiple shot boundaries are detected. Therefore multiple detected shot boundaries that follow each other within a short temporal interval are grouped together and a gradual change is detected beginning with the first and ending with the last shot boundary in the interval. The approach is explained in more detail in section 2.</p><p>To extract the features indoors and outdoors we use a feed forward neural net as a classificator, trained by a backpropagation learning rule. The input is a feature vector describing the color distribution of an image. The output is the probability for each feature (indoors and outdoors) to appear in the image. The approach is discussed in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Shot detection</head><p>Quite a lot of approaches to shot boundary detection were proposed in the literature. An overview is given in <ref type="bibr" coords="2,266.95,194.93,64.13,8.74" target="#b0">[Lienhart, 1999</ref><ref type="bibr" coords="2,331.09,194.93,80.08,8.74" target="#b0">,Yusoff et al., 1998</ref>]. The principle methodology of shot boundary detection is to extract one or more features from every nth frame of a video sequence, to compute the difference of features for consecutive frames, and to compare these differences to a given threshold. Each time the threshold is exceeded a shot boundary is detected. The various approaches differ concerning the used features.</p><p>The shot boundary detection system we used for TREC 2002 is based on the approach presented in <ref type="bibr" coords="2,234.40,278.62,81.83,8.74" target="#b0">[Miene et al., 2001]</ref>. As mentioned before , the approach can be divided into two main parts. The first part is to extract all needed features from a video. The second part is to detect the shot boundaries based on the previously extracted features.</p><p>For the feature extraction part each frame is converted into a grayscale image. Then a histogram H G is created. Subsequently, the squared differences between each two consecutive frames</p><formula xml:id="formula_0" coords="2,134.77,366.49,345.83,60.85">H G Dif f (n, n -1) = 255 i=0 (H G (n)(i) -H G (n -1)(i)) 2 M ax(H G (n)(i), H G (n -1)(i)) (1) are calculated. H G (n)(i) denotes a grayscale histogram value at index i of frame n. M ax(H G (n)(i), H G (n -1)(i))</formula><p>denotes the maximum of both grayscale histogram values H Gray (n)(i) and H Gray (n -1)(i), and is used as a normalization factor. This leads to a feature difference list in order to detect shot boundaries, which is compared to a threshold. To determine the adaptive threshold, the maximum of all calculated difference values of the actual video is calculated. The adaptive threshold for the actual video is specified as a percental value of the maximum:</p><formula xml:id="formula_1" coords="2,172.94,509.84,307.66,22.98">T h = M ax{H G Dif f (1, 0), . . . , H G Dif f (n, n -1)} • T h percentage 100 (2)</formula><p>For gradual changes like dissolves or wipes the shot boundary detection often detects more than one boundary per shot. Therefore, all shot boundaries which belong to the same shot have to be merged into one boundary. This step is illustrated in Figure <ref type="figure" coords="2,229.28,572.43,3.88,8.74" target="#fig_0">1</ref>. Shot boundaries are merged together if the temporal distance between their occurrences is less than a threshold. The minimal frame number of the merged shot boundaries determines the start, and the maximum frame number determines the end of the gradual change. The exact boundary position is set to the maximum feature difference value within the merged shot boundaries.</p><p>Before preparing our results for TREC we tested our shot detection on three videos from the feature developement collection, for which we determined the shot boundaries manually. The results of this experiment are shown in table 2. File denotes the file name of the video, human the amount shot boundaries determined manually, auto the total number of shot boundaries detected by the system, correct the amount of correct detected shot boundaries, false the amount of false alarms and missing the amount of shot boundaries, that were not detected by the system. The last two colums contain the percentage values for precission and recall. For this test we did not distinguish between hard cuts and gradual changes. In the following section we present our approach for the classification of indoor and outdoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Classification of indoor and outdoor scenes</head><p>For the feature extraction task we have examined whether it is possible to classify indoor and outdoor shots by their color distribution. In order to analyze the color distribution, first order statistical features are used, which are extracted from the histograms of the three color channels (RGB) and the grey level histogram. The features calculated from each histogram are average, variance, and amount of peaks, normalized to an interval [0.0, . . . , 1.0]. Therefore we calculate 12 statistical features alltogether. In order to classify the shots into indoor and outdoor shots, a feed forward neural net with backpropagation learning was trained. For this task we used the SNNS <ref type="bibr" coords="3,314.44,620.10,166.15,8.74;3,134.77,632.05,74.07,8.74">(Stuttgart Neural Network Simulator) [SNNSv4.2, 2002]</ref>.</p><p>At the input layer the 12 statistical features are presented, that were obtained from the histograms. The output layer consists of two neurons that take on values between 0.0 and 1.0 measuring the probability for the features indoors or outdoors in the shot. Two hidden layers with 20 neurons each are initialized with random weights. In order to train the neural net, some videos from the feature development collection were chosen. The shots are classified manually to generate 323 training data sets, 178 for indoors, and 145 for outdoors. Figure <ref type="figure" coords="4,475.61,166.81,4.98,8.74" target="#fig_1">2</ref> shows the trained neural net. In order to classify the shots from the feature extraction test collection, a set of n key frames is extracted from each shot. Every kth frame of a shot is used as a key frame, but in order to be more independent of inaccuracies during the shot detection and of gradual changes (e.g., wipes, fades, or dissolves) a number of frames around the shot boundaries is skipped (see Figure <ref type="figure" coords="4,385.13,411.44,3.88,8.74" target="#fig_2">3</ref>). In order to classify a shot, the set of n key frames is presented to the neural net. For each of the two output neurons a list is obtained containing n values, one for each key frame. The median is calculated for each list to obtain the final indoors or outdoors probabilities for the shot. In order to measure the accuracy of the classification result, the difference between the median values of the indoors and the outdoors neuron is calculated. If the difference exceeds a threshold the shot is classified to contain the feature with the higher probability. The difference is also used for the ranking.</p><p>For the next months and also for our next participation in TREC 2003 we will concentrate on the improvement of our shot detection approach concerning the detection of gradual changes. We have also to examine how to obtain better results for the extraction of the indoors and outdoors features. As mentioned before the neural net was trained with indoor and outdoor example frames from the feature development collection. The results from the TREC evaluation of our results shows that there are serious problems with the classification of the material from the feature test collection. At the moment we are working on the analysis of these problems. One major problem appears to be, that we trained the net only with examples of indoor and outdoor scenes. Therefore the results for scenes containing neither indoor nor outdoor scenes are undefined. Therefore, especially artificial scenes lead to a wrong classification.</p><p>In addition we are looking forward to develop further modules for our feature extraction system to be able to extract other features like text or human faces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,160.67,217.81,294.03,7.89;3,148.68,115.83,318.00,87.20"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Merging of multiple detected shot boundaries[Miene et al., 2001].</figDesc><graphic coords="3,148.68,115.83,318.00,87.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,253.39,328.86,108.58,7.89;4,157.23,207.29,300.90,106.80"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Trained neural net.</figDesc><graphic coords="4,157.23,207.29,300.90,106.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,241.14,551.57,133.07,7.89;4,187.28,453.85,240.80,82.95"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Extraction of key frames.</figDesc><graphic coords="4,187.28,453.85,240.80,82.95" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.72,353.80,341.87,7.86;5,142.31,364.76,338.29,7.86;5,142.31,375.71,232.99,7.86;5,134.77,386.67,345.83,7.86;5,142.31,397.63,338.29,7.86;5,142.31,408.59,310.05,7.86;5,134.77,419.55,157.50,7.86;5,328.23,419.55,152.36,7.86;5,142.31,430.51,205.44,7.86;5,373.68,430.51,106.92,7.86;5,142.31,441.47,14.85,7.86;5,176.49,441.47,41.29,7.86;5,237.12,441.47,7.42,7.86;5,263.89,441.47,40.96,7.86;5,357.42,441.47,123.17,7.86;5,142.31,452.43,222.76,7.86;5,134.77,463.39,345.83,7.86;5,142.31,474.34,306.13,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,291.38,353.80,189.21,7.86;5,142.31,364.76,74.43,7.86;5,331.31,364.76,149.28,7.86;5,142.31,375.71,81.44,7.86;5,142.31,397.63,195.63,7.86;5,433.02,463.39,47.57,7.86;5,142.31,474.34,129.49,7.86">Comparison of Automatic Shot Boundary Detection Algorithms</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lienhart ; Lienhart</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Miene</surname></persName>
		</author>
		<ptr target="http://www-ra.informatik.uni-tuebingen.de/downloads/SNNS/SNNSv4.2.Manual.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="5,238.99,364.76,45.99,7.86;5,220.71,408.59,173.16,7.86;5,328.23,419.55,152.36,7.86;5,142.31,430.51,205.44,7.86;5,373.68,430.51,106.92,7.86;5,142.31,441.47,14.85,7.86;5,176.49,441.47,41.29,7.86;5,237.12,441.47,7.42,7.86;5,263.89,441.47,36.41,7.86">SNNS Stuttgart Neuronal Network Simulator User Manual, Version 4.2. University of Stuttgart and University of Tübingen</title>
		<title level="s" coord="5,279.82,474.34,140.53,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Fellner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</editor>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1999. 1999. 2001. 2001. 2002. 1998. 1998</date>
			<biblScope unit="volume">3656</biblScope>
			<biblScope unit="page">1425</biblScope>
		</imprint>
	</monogr>
	<note>A study on automatic shot change detection</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
