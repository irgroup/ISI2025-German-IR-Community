<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.24,75.23,273.42,13.44;1,163.60,91.31,284.70,13.44">CLIR Experiments at Maryland for TREC-2002: Evidence combination for Arabic-English retrieval</title>
				<funder ref="#_2MKbPfg">
					<orgName type="full">DARPA Cooperative Agreement</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,211.36,120.28,80.54,11.40"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
							<email>kareem@glue.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.04,120.28,82.22,11.40"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@glue.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.24,75.23,273.42,13.44;1,163.60,91.31,284.70,13.44">CLIR Experiments at Maryland for TREC-2002: Evidence combination for Arabic-English retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EB7EAEC0380F8D10B048F0F399220465</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The focus of the experiments reported in this paper was techniques for combining evidence for crosslanguage retrieval, searching Arabic documents using English queries. Evidence from multiple sources of translation knowledge was combined to estimate translation probabilities, and four techniques for estimating query-language term weights from document-language evidence were tried. A new technique that exploits translation probability information was found to outperform a comparable technique in which that information was not used. Comparative results for three variants of Arabic "light" stemming are also presented. A simple variant of an existing stemming algorithm was found to result in significantly better retrieval effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical techniques for processing natural language are imperfect, but reliance on multiple sources of evidence can help to mitigate the limitations of any one technique. In this paper, we leverage the combination of evidence in two ways: <ref type="bibr" coords="1,176.20,396.58,12.80,10.57" target="#b0">(1)</ref> to estimate translation probabilities from multiple sources of translation knowledge, and (2) to estimate weights for a query term weights based on the statistics of multiple documentlanguage terms. Five translation resources of three types (machine translation lexicons, a printed bilingual dictionary that had been manually rekeyed, and translation probabilities derived from parallel corpora) were used as a basis for estimating translation probabilities. Four ways to estimate query term weights were tried (translation-based indexing, Pirkola's structured query method, and two newly developed variants of structured queries). The main task in the TREC-2002 CLIR track was retrieval of Arabic documents using English queries, so we also made some refinements to our Arabic text processing techniques. Most importantly, we compared three approaches to "light" (i.e., one-level rule-based truncation) stemming. Light stemming was found to perform well for retrieval of character-coded Arabic text in TREC-2001 <ref type="bibr" coords="1,488.57,510.46,12.00,10.57" target="#b0">[1,</ref><ref type="bibr" coords="1,503.33,510.46,8.00,10.57" target="#b1">2]</ref>, so we limited our experiments to that technique. We begin with a description of the techniques that we tried in the next section. Section 3 then presents our results, and section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, the approaches mentioned in the introduction will be described in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Translation Resources</head><p>Using multiple sources of evidence to guide the translation process can help in two important ways: <ref type="bibr" coords="1,523.85,648.46,12.92,10.57" target="#b0">(1)</ref> No single source provides a comprehensive set of translations, and <ref type="bibr" coords="1,359.45,661.06,12.80,10.57" target="#b1">(2)</ref> No single source provides an accurate indication of translation probabilities. Drawing translation knowledge from diverse sources therefore offers an interesting potential to improve CLIR effectiveness. We had five translation resources of three types available:</p><p>• Two bilingual term lists that were constructed using two Web-based machine translation systems (Tarjim and Al-Misbar [3] <ref type="bibr" coords="2,215.10,113.26,12.01,10.57" target="#b2">[4]</ref>). In each case, we submitted sets of isolated English words found in a 200 MB collection of Los Angeles Times news stories for translation from English into Arabic <ref type="bibr" coords="2,519.52,125.86,11.70,10.57" target="#b3">[5]</ref>.</p><p>Each system returned at most one translation for each submitted word. Together, the two term lists covered about 15% of the unique Arabic stems in the AFP collection (measured by using light stemming on both the term list and the collection). • The Salmone Arabic-to-English dictionary, which was made available for use in the TREC-CLIR track by Tufts University. No translation preference information is provided in this dictionary, but it does include rich markup describing morphology and part-of-speech information. We preprocessed the dictionary without using any of that additional information, thereby creating a bilingual term list.</p><p>The coverage of the resulting term list, measured in the same way, was about 7% of the unique Arabic stems in the AFP collection. • Two translation probability tables, one for English-to-Arabic and one for Arabic-to-English. These tables were constructed from tables provided by BBN, which were in turn constructed from a large collection of aligned English and Arabic United Nations documents using the Giza++ implementation of IBM's model 1 statistical machine translation design. The coverage of the of the Arabic-to-English table, measured in the same way, was 29% of the unique Arabic stems in the AFP collection.</p><p>We combined the evidence from these sources in the following manner: 1. We selected a direction (English-to-Arabic or Arabic-to-English), and then inverted any resources that were originally provided in the other direction. For the translation probability table, we retained the probabilities for each translation pair and then renormalized the inverted table so that the values of the "probabilities" for each source-language term summed to one. This process likely introduced some error, since probabilities for rare events may not have been accurately estimated. 2. A uniform distribution was used to assign probabilities to the translations obtained from machine translation systems and the Salmone dictionary. Tarjim and Al-Misbar returned at most one translation for an English word, but two English words might share a common translation. When n alternatives were known from a single source, each was assigned a probability of 1/n. 3. For translation from English to Arabic, the resulting translation probabilities were then combined across sources by summing the probabilities for a given Arabic translation across the sources in which it appeared and then dividing by the number of sources in which the English term had appeared. For example, if Tarjim, Al-Misbar and Salmone contained the English term, with Tarjim containing some specific translation with probability 1.0, Al-Misbar lacking that translation (i.e., assigning it a probability of 0.0), and Salmone assigning it a probability of 0.5 (because two translations were known), then the resulting combined probability would be 1/3 + 0/3 + 0.5/3 = 0.5.</p><p>Translation probabilities for Arabic-to-English were computed in the same manner, but with the role of each language reversed. The resulting translation resource contained what appeared to us to be reasonable estimates of translation probabilities, and covered 36% of the unique Arabic stems in the AFP collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stemming</head><p>Three Arabic light stemmers were tested. The first was Al-Stem, the stemmer provided by the organizers for the standard resource run, which was developed by the author at Maryland and modified by Leah Larkey from University of Massachusetts (U Mass). The second was a light stemmer described in Larkey's SIGIR 2002 paper, which we will refer to as the U Mass stemmer <ref type="bibr" coords="2,323.34,674.86,11.79,10.57" target="#b1">[2]</ref>. The third was a modified version of the U Mass stemmer in which two additional prefixes identified through failure analysis using the TREC-2001 topics, were removed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combination of Evidence for Alternate Translations</head><p>One of the key challenges in CLIR is what to do when more than one possible translation is known. One principled solution to this problem is Pirkola's structured query method <ref type="bibr" coords="3,381.16,284.02,11.79,10.57" target="#b4">[6]</ref>. Pirkola used InQuery's synonym operator to estimate the weight of each query term based on document language evidence as follows: <ref type="figure" coords=""></ref>and<ref type="figure" coords="3,341.68,392.70,6.14,10.36;3,347.80,396.96,1.95,6.57;3,349.72,392.70,11.57,10.36;3,361.24,396.96,3.11,6.57">T j (D k</ref> ) is the set of known translations for the term D k . The key insight here is that translation is viewed as a process akin to stemming, in which several document-language terms might be treated as if it were the same query language term (in stemming, many morphological variants are treated as if they were the same stem). One limitation of Pirkola's structured query method is that it makes no use of translation probabilities-every possible translation is treated as being equally likely. Consider a case in which one query term has two translations, one of which is highly probable, but which typically has a low term frequency when it appears, while the other is improbable but typically has high term frequencies. Because the improbable translation is so common, documents containing that translation would likely be retrieved ahead of documents that contain the more likely translation. This seems undesirable. Moreover, consider a second case in which an extremely improbable translation appears in many documents, with an almost certain translation appears in only a few. Intuitively, it would seem that we should ignore the extremely improbably translation and assign this term a low document frequency (and thus give it added importance in any retrieval system that relies on inverse document frequency as a measure of term importance). But Pirkola's structured query method does just the opposite. In this paper, we propose two ways of incorporating translation probability information. The simplest approach is to retain the same formulae, but to suppress the contribution of unlikely translations. We implemented this by starting with the most likely translation and adding additional translations in order of decreasing probability until the cumulative probability of the selected translations reached a preset threshold that was determined through experimentation using the TREC-2001 CLIR collection. As an alternative, we also explored three ways of incorporating translation probabilities directly into the formulae: 1. The weighted DF method (WDF): In this method, translation probability estimates are used in the calculation of document frequency, but not term frequency. Formally, TF(Q i ) is computed as in equation ( <ref type="formula" coords="3,85.34,670.90,3.93,10.57">1</ref>), and DF(Q i ) is computed as follows:</p><formula xml:id="formula_0" coords="3,63.40,311.80,480.33,91.74">TF j (Q i ) = ∈ )} ( | { ) ( i k Q T D k k j D TF (1) DF(Q i ) = | } | { | )} ( | { i k Q T D k k d D d ∈ ∈ (2) Where Q i is a query term, D k is a document term, TF j (Q i ) is the term frequency of Q i in document j, DF(Q i ) is the number of documents that contain Q i , d is a document,</formula><formula xml:id="formula_1" coords="3,81.41,674.21,354.91,34.43">DF(Q i ) = [ ] ∈ × )} ( | { ) ( ) ( i k Q T D k k k j D wt D DF (3)</formula><p>Where Q i is a query term, D k is a document term, DF(Q i ) is the number of documents that contain Q i , wt(D k ) is the probability estimate for D k , and T j (D k ) is the set of translations for the term D k . This method addresses the case in which an improbable translation has a high document frequency. Note that the union operator has been replaced by the sum operator, so equations ( <ref type="formula" coords="4,384.51,112.42,4.31,10.57">2</ref>) and (3) will not necessarily produce the same value, even in the case of equiprobable translations. 2. The weighted TF method (WTF): In this method, translation probability estimates are used in the calculation of term frequency, but not document frequency. Formally, DF(Q i ) is computed as in equation ( <ref type="formula" coords="4,85.33,163.06,3.93,10.57">2</ref>), and TF(Q i ) is computed as follows:</p><formula xml:id="formula_2" coords="4,81.40,166.37,354.91,34.43">TF j (Q i ) = [ ] ∈ × )} ( | { ) ( ) ( i k Q T D k k k j D wt D TF (4)</formula><p>Where Q i is a query term, D k is a document term, TF j (Q i ) is the term frequency of Q i in document j, and T j (D k ) is the set of different translations for the term D k . This method addresses the case in which an improbable translation typically has high term frequencies. 3. The weighted TF/DF method (WTF/DF): In this method, translation probability estimates are used in the calculation of both term frequency and document frequency. Equation ( <ref type="formula" coords="4,400.11,255.34,4.31,10.57">3</ref>) is used to compute DF(Q i ), with equation ( <ref type="formula" coords="4,148.67,267.94,4.27,10.57">4</ref>) used to compute TF j (Q i ). This addresses both potential concerns.</p><p>The same approach to preselection of the most likely translation probabilities that we used with Pirkola's structured queries can also be used with any of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Balanced Translation-Based Indexing</head><p>The formulae for Pirkola's method do not depend on any information that can only be computed at query time, so an indexing time implementation is possible. Oard and Ertunc built such an implementation, which they called translation-based indexing. That approach is equivalent to unbalanced document translation, in which every English translation for each Arabic term is indexed once. Unbalanced translation is, however, known to underemphasize the contribution of highly specific terms (which typically have very few possible translations) and to overemphasize the contribution of common terms (which often have many possible translations). The obvious alternative is to balance the translation-based indexing process. To achieve this, we replaced each Arabic term with the five most likely English translations. For terms with fewer than 5 known translations, the most probable translations were replicated in a round-robin fashion until a total of five was reached. We chose the value 5 based on post-hoc experiments with the 25 TREC 2001 topics (we tried 1, 3, 5, 7, and 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Document and Query Expansion</head><p>Any individual document will likely include only a fraction of the words that might be used to describe a topic, so some form of expansion to the representation of a document might help retrieval. We therefore prepared a contrastive condition in which the representation of each document was expanded as follows:</p><p>1. We identified the 20 most descriptive terms in each document, by dividing the frequency with which each term appeared in the document by the number of documents in which that term was found. 2. We then formed a query with one instance of each of those 20 terms and used that query as a basis for ranking the documents in the AFP collection using the InQuery text retrieval system from the University of Massachusetts (the document being expanded was often in this set). We used the Al-Stem stemmer to normalize the representation of Arabic terms in InQuery, both for query and for document processing. 3. We combined the 10 top-ranked documents into a single mega-document and then chose the 20 most descriptive terms in that mega-document, using the same measure of term importance as above. The resulting set of 20 terms was then added to the representation of document that was being expanded.</p><p>One can imagine many variants of this approach, including alternative parameter settings, alternative term importance measures, and alternative ways of combining evidence from the top-ranked documents. Because document expansion was not the principal focus of our experiments, we tried only this one implementation for TREC-2002.</p><p>Because queries are relatively brief, they are even more likely to contain only a small subset of the words that might be used to express the concepts that are important for the topic that they represent. We therefore performed pre-translation query expansion using a similar blind relevance feedback process. We used Associated Press articles from 1994-1998 from the North American News Text Corpus (Supplement) and the orld Stream English Collection from the Linguistic Data Consortium for this purpose Because InQuery contains built-in provisions for query expansion, we used InQuery (with the kstem English stemmer) to search these collections. We configured InQuery to choose the most discriminating terms from the top 10 returned documents for each query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Implementation Details</head><p>To ease work in Arabic, Arabic letters were transliterated to Roman letters. Table <ref type="table" coords="5,445.60,268.78,5.52,10.57">1</ref> shows the mappings between the Arabic letters and their transliterated representations. In preprocessing the text, all diacritics and kashidas were removed, and letter normalization was employed to normalize the letters ya ( ) and alef maqsoura ( ) to ya ( ) and all the variants of alef ( ) and hamza ( ), namely alef ( ), alef hamza (! " ), alef maad (#), hamza ( ), waw hamza ($), and ya hamza (%), to alef ( ).</p><formula xml:id="formula_3" coords="5,206.21,344.05,198.86,131.25">! # " A $ % A &amp; b t ' v ( j ) H * x + d , O - r . z / s 0 P 1 S 2 D 3 T 4 Z 5 E 6 g 7 f 8 q 9 k : l ; m n h w y &lt; p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: English transliteration of Arabic characters</head><p>As for a stop-word list, we used the list that is distributed with Sebawai, which includes 130 particles and pronouns <ref type="bibr" coords="5,107.20,515.14,11.70,10.57" target="#b5">[7]</ref>. Sebawai is a publicly available Arabic morphological analyzer. We used InQuery for our official monolingual runs and for one of our official cross-language runs. For the remaining monolingual runs and for our post-hoc experiments, we used PSE (an IR system developed at Maryland that uses OKAPI BM-25 weights) because we were not able to perform the necessary changes to the term weight computation using InQuery. Our monolingual and cross-language results should therefore not be considered to be strictly comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We submitted five official runs, and we have used the relevance judgments provided by NIST to perform an additional 31 post-hoc runs. In this section we present those results along with a preliminary discussion that we intend to extend in our final paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Arabic Monolingual Runs</head><p>We submitted one official automatic monolingual run, one official manual monolingual run, and we performed three post-hoc automatic monolingual runs. In the official automatic run, queries were formed using every word in the title and description fields of the topic description (stopwords were removed, but no automatic stop structure removal was performed). For the manual run, the title, description, and narrative fields were used, additional query terms were manually added, and stop structure and information about what would cause a document to be judged as not relevant was removed manually. The principal goal of the manual run was to enrich the relevance pools. In both cases, Al-stem was used to stem the documents and the queries, and document expansion was used as described above. Table <ref type="table" coords="6,374.80,191.74,5.52,10.57" target="#tab_1">3</ref> shows the results (results on TREC-2001 data are post-hoc).</p><p>The three post-hoc runs were done using PSE rather than InQuery, with a goal of comparing three variants of light stemming. Document expansion was not used in those experiments. Again, Table <ref type="table" coords="6,502.60,229.78,5.52,10.57" target="#tab_1">3</ref> shows the results. The Al-stem and the U Mass stemmers were found to be statistically indistinguishable by a paired two-tailed t-test (for p&lt;0.05). The modified U Mass stemmer did achieve mean average precision results that were statistically better than the two other stemmers over the full 75-topic set, but the advantage over the unmodified U Mass stemmer was not seen when the 50 TREC 2002 topics used alone. Because the suitability of the TREC-2001 collection for post-hoc experiments is open to some question, we conclude that no reliable differences between the three stemmers were detected by our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Mean </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">English-Arabic CLIR Runs</head><p>We performed three official automatic CLIR runs and 29 post-hoc automatic CLIR runs. In each case, we formed title+description queries in the same manner as for the automatic monolingual run. Pre-translation query expansion was done using blind relevance feedback on Associated Press articles from 1994-1998 using InQuery. The articles were part of the North American News Text Corpus (Supplement) and AP World Stream English Collection from the Linguistic Data Consortium <ref type="bibr" coords="6,348.16,567.70,11.79,10.57" target="#b6">[8]</ref>. For the official CLIR runs we tried these following configurations:</p><p>• Pirkola's Method. We used InQuery, pre-translation query expansion, document expansion, and the thresholded version of Pirkola's method (with a threshold of 0.3, established using post-hoc experiments with the TREC 2001 topics). • Balanced Translation-Based Indexing (TBI). We used InQuery, document expansion, and balanced translation-based indexing. • Weighted TF. We used PSE, pre-translation query expansion, document expansion, and the Weighted TF method (with a threshold of 0.35, again tuned using the TREC-2001 topics).</p><p>For the post-hoc experiments, we used PSE, pre-translation query expansion, one of four methods (Pirkola's method, Weighted TF, Weighted DF, or Weighted TF/DF), and a probability threshold that was varied between 0.1 and 0.7 in increments of 0.1. Post-hoc CLIR results are reported on all 75 topics from TREC 2001 and TREC 2002. We also conducted one additional post-hoc experiment in which we combined the results from our official Weighted TF and translation-based indexing runs in a manner similar to that suggested by <ref type="bibr" coords="7,123.28,112.42,11.79,10.57" target="#b7">[9]</ref>. For that experiment, we renormalized the PSE score for the Weighted TF run to be between 0.4 and 0.5 (a range similar to that seen in InQuery), averaged that normalized score with the translationbased indexing score from PSE, and resorted the ranked list based on that averaged score. The result was statistically better than the Weighted TF run, but not statistically distinguishable from the translation-based indexing run. Table <ref type="table" coords="7,155.44,163.06,5.52,10.57" target="#tab_2">4</ref> shows the results for the official runs and for the one post-hoc experiment that was based on the two official runs. Table <ref type="table" coords="7,230.08,175.66,5.52,10.57" target="#tab_3">5</ref> and Figure <ref type="figure" coords="7,288.40,175.66,5.52,10.57" target="#fig_0">1</ref>  The Weighted TF method and translation-based indexing were both statistically significantly better than Pirkola's method (on the 75 topic set), but statistically indistinguishable from each other. The retrieval effectiveness of Weighted TF/DF was close to the best at every cumulative probability threshold, and it was the only one of the four techniques that we tried that did not exhibit a decrease in effectiveness at high thresholds. It therefore seems to be a good candidate for further study, and an appropriate choice if a method </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability Threshold Mean Average Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pirkola</head><p>Weighted TF Weighted IDF Weighted TF/IDF must be selected without access to a representative collection on which to tune the probability threshold. We do not yet know whether document expansion was helpful because the runs with and without document expansion were done using different retrieval systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented two basic ideas for using a combination of evidence to improve cross-language retrieval effectiveness, demonstrated an ability to produce useful translation probability estimates from multiple sources, and extended Pirkola' s structured query method in ways that exploit that information. Our results suggest that further work on the Weighted TF/DF method would be well justified, and that further work on document expansion is needed before the utility of that technique in this context can be judged. We look forward to the discussions that we will have at the conference, and to the opportunity to continue out exploration of these questions through additional post-hoc experiments and analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,170.92,714.40,269.64,11.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sensitivity to cumulative probability threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,63.42,700.06,466.52,23.29"><head>Table 2 : Prefixes and suffixes removed by each light stemmer.</head><label>2</label><figDesc>Table 2 lists the prefixes and suffixes removed by each stemmer. Of the three, Al-Stem was the most aggressive stemmer. Stem wAl, fAl, bAl, bt, yt, lt, mt, wt, st, nt, bm, lm, wm, km, fm, Al, ll, wy, ly, sy, fy, wA, fA, lA, and bA. ( ) At, wA, wn, wh, An, ty, th, tm, km, hm, hn, hA, yp, tk, nA, yn, yh, p, h, y, A.</figDesc><table coords="3,63.40,87.58,475.08,126.01"><row><cell>Stemmer</cell><cell>Prefixes</cell><cell cols="2">Suffixes</cell></row><row><cell cols="3">Al-(</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>)</cell></row><row><cell>Umass Stemmer</cell><cell cols="2">Al, wAl, bAl, kAl, fAl, and w</cell><cell cols="2">hA, An, At, wn, yn, yh, yp, p, h, and y</cell></row><row><cell></cell><cell>(</cell><cell>)</cell><cell>(</cell><cell>)</cell></row><row><cell>Modified UMass</cell><cell cols="2">--Identical to U Mass, plus</cell><cell>--Identical to U Mass --</cell></row><row><cell>Stemmer</cell><cell>ll, and wll (</cell><cell>)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,131.56,331.42,359.34,141.91"><head>Table 3 : The Arabic Monolingual Results</head><label>3</label><figDesc></figDesc><table coords="6,352.25,331.42,80.88,10.57"><row><cell>Average Precision</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,86.56,175.66,438.96,225.61"><head>Table 4 : CLIR results for official runs (expanded documents).</head><label>4</label><figDesc>together show the other post-hoc results.</figDesc><table coords="7,86.56,201.46,438.96,199.81"><row><cell></cell><cell>Run</cell><cell></cell><cell cols="3">Mean Average Precision</cell><cell></cell><cell></cell></row><row><cell cols="2">Official Runs</cell><cell cols="3">TREC 2002 Topics (50</cell><cell cols="2">TREC 2001 &amp; 2002</cell><cell></cell></row><row><cell cols="2">(Expanded documents)</cell><cell></cell><cell>topics)</cell><cell></cell><cell cols="2">Topics (75 topics)</cell><cell></cell></row><row><cell cols="2">Pirkola's Method</cell><cell></cell><cell></cell><cell>0.202</cell><cell></cell><cell>=&gt; AE2</cell><cell></cell></row><row><cell cols="2">Balanced TBI</cell><cell></cell><cell></cell><cell>0.274</cell><cell></cell><cell>0.279</cell><cell></cell></row><row><cell cols="2">Weighted TF</cell><cell></cell><cell></cell><cell>0.247</cell><cell></cell><cell>0.279</cell><cell></cell></row><row><cell cols="3">Balanced TBI + Weighted TF</cell><cell></cell><cell>=&gt; ABD</cell><cell></cell><cell>=&gt; ?=F</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Cumulative Probability Threshold</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>Pirkola's Method</cell><cell>0.215</cell><cell>0.212</cell><cell>0.230*</cell><cell>0.230</cell><cell>0.207</cell><cell>0.186</cell><cell>0.134</cell></row><row><cell>Weighted TF</cell><cell>0.220</cell><cell>0.226</cell><cell>0.241</cell><cell>0.247*</cell><cell>0.241</cell><cell>0.230</cell><cell>0.192</cell></row><row><cell>Weighted DF</cell><cell>0.211</cell><cell>0.196</cell><cell>0.198</cell><cell>0.180</cell><cell>0.147</cell><cell>0.126</cell><cell>0.091</cell></row><row><cell>Weighted TF/DF</cell><cell>0.220</cell><cell>0.222</cell><cell>0.235</cell><cell>0.234</cell><cell>0.236</cell><cell>0.240</cell><cell>0.245</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.00,405.40,435.96,11.40"><head>Table 5 : Post-hoc CLIR results (Mean Average Precision, 75 topics, no document expansion.</head><label>5</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,69.16,715.31,305.37,9.53"><p>College of Information Studies and Institute for Advance Computer Studies.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by <rs type="funder">DARPA Cooperative Agreement</rs> <rs type="grantNumber">N660010028910</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2MKbPfg">
					<idno type="grant-number">N660010028910</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,99.40,338.02,441.12,10.57;8,99.40,350.62,191.88,10.57" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aljlayl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Beitzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<idno>IIT at TREC-10</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.40,363.22,441.55,10.57;8,99.40,375.94,341.40,10.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,296.80,363.22,244.15,10.57;8,99.40,375.94,196.56,10.57">Improving Stemming for Arabic Information Retrieval: Light Stemming and Co-occurrence Analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,311.92,375.94,51.44,10.57">SIGIR 2002</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.40,401.26,396.77,10.57" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,191.09,401.26,155.97,10.57">ATA Software Technology Limited</title>
		<ptr target="www.almisbar.com" />
		<imprint>
			<pubPlace>North Brentford Middlesex, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.41,413.86,244.44,10.57" xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName coords=""><surname>Nist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,129.41,413.86,111.84,10.57">Text Research Collection</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1997-04">April 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.41,426.58,446.24,10.57;8,99.41,439.18,46.92,10.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,173.21,426.58,289.55,10.57">Ertunc: Translation-Based Indexing for Cross-Language Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,469.85,426.58,24.88,10.57">ECIR</title>
		<imprint>
			<biblScope unit="page" from="324" to="333" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.41,451.78,408.36,10.57;8,99.41,464.50,283.44,10.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,163.37,451.78,245.94,10.57">Building a Shallow Morphological Analyzer in One Day</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Darwish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,423.29,451.78,84.48,10.57;8,99.41,464.50,216.88,10.57">ACL Workshop on Computational Approaches to Semitic Languages</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.41,477.10,388.32,10.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,191.21,477.10,175.94,10.57">North American News Text Supplement</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Macintyre</surname></persName>
		</author>
		<idno>LDC98T30</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,435.53,477.10,18.54,10.57">LDC</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.41,489.82,434.52,10.57;8,99.41,502.42,102.72,10.57" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,168.46,489.82,365.47,10.57;8,99.41,502.42,38.93,10.57">Should we Translate the Documents or the Queries in Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccarley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,149.81,502.42,18.54,10.57">ACL</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
