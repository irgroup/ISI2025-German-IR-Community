<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.71,109.06,332.84,13.98;1,470.92,105.33,5.14,9.25">University of Alicante Experiments at TREC-2002 1</title>
				<funder ref="#_5X8VGUw">
					<orgName type="full">(PROFIT)</orgName>
				</funder>
				<funder ref="#_ZG45wwp">
					<orgName type="full">Spanish Government (CICYT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,204.54,131.53,42.97,9.67"><forename type="first">Jose</forename><surname>Luis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos Universidad de Alicante Apartado</orgName>
								<address>
									<postCode>99. 03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.04,131.53,91.18,9.67"><forename type="first">Fernando</forename><forename type="middle">&amp;</forename><surname>Llopis</surname></persName>
							<email>llopis@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos Universidad de Alicante Apartado</orgName>
								<address>
									<postCode>99. 03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.01,131.53,90.76,9.67"><forename type="first">Antonio</forename><surname>Ferrández</surname></persName>
							<email>antonio@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos Universidad de Alicante Apartado</orgName>
								<address>
									<postCode>99. 03080</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.71,109.06,332.84,13.98;1,470.92,105.33,5.14,9.25">University of Alicante Experiments at TREC-2002 1</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">035C011FC7BADC3BC4D5F2F4479294F5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the architecture, operation and results obtained with the Question Answering prototype developed in the Department of Language Processing and Information Systems at the University of Alicante. This system is based on our TREC-10 approach where different improvements have been introduced. Main modifications reside on the introduction of a filtering stage into paragraph selection and answer extraction modules that allow the treatment of questions with no answer in the document collection. Moreover, WordNet has been enhanced by adding a collection of gazetteers that includes several types of proper nouns (people, organisations, and places) and a large variety of acronyms, measure and money units.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This year, question-answering task has been significantly modified. The organisation has restricted the proposed experiments to main and list tasks. Main task is similar to previous year task but instead of permitting 5 ranked responses for each query and a maximum of 50 bytes as answer length, only a response is allowed and the answer string must contain nothing other than the exact answer. Besides, there is no guarantee that an answer will actually appear in the document collection. The list task consists of answering questions that will specify a number of instances to be retrieved. In this case, it is guaranteed that the collection contains at least as many instances as the question asks for.</p><p>The system presented to TREC-2002 QA task departs from the system presented in past TREC conferences <ref type="bibr" coords="1,139.03,541.54,12.59,9.67">[7]</ref>[8] where new tools have been added and existing ones have been updated and adapted to cope with new specifications. Main enhancements rely on several aspects. First, passage selection and answer extraction stages have been adapted in order to face questions with no answer in the document collection. For this purpose, these stages have been complemented with a filtering module that rejects relevant paragraphs as well as possible answers that do not validate a series of restrictions. This way, when no possible answer remains after applying these restrictions, the system returns NIL as final answer. Second, WordNet has been extended by adding entities included in several gazetteers mainly referring to places (countries, states, cities, etc.) as well as and a large number of different acronyms, measure and money units. In this case, WordNet enrichment tries to minimise, as possible, the lack of a Name-Entity tagger.</p><p>Although our participation has been restricted to main task, this year we tried to face up all the specifications. In fact, it is the first time we manage with no-answer questions. This paper is structured as follows: Section 2 describes system structure and operation and tries to emphasize new contributions. Afterwards, we present and analyse the results achieved and finally, we extract initial conclusions and discuss directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Overview</head><p>Our QA system is structured into four main modules: question analysis, document/passage retrieval, paragraph selection and answer extraction. First module processes questions expressed in open-domain natural language in order to analyse the information requested in the queries. This information is used as input by remaining modules. Document retrieval module accomplishes a first selection of relevant passages by using a passage retrieval system. Afterwards, the paragraph selection module filters these passages in order to select smaller text fragments (paragraphs) that are more likely to contain the correct answer. Finally, the answer selection module processes these fragments in order to locate and extract the final answer. Figure <ref type="figure" coords="2,361.36,279.60,5.37,9.67" target="#fig_0">1</ref> shows system architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Question Analysis</head><p>Question processing module accomplishes several tasks. First, questions are part-of-speech tagged and parsed. This process allows identifying simple noun and verbal phrases (concepts) in the query. Afterwards, this module determines question type and classifies concepts into two categories: key or definition concepts. Finally, these concepts are processed to obtain and represent its semantic characteristics. The resulting structures will be used as main information units for QA purposes. Question analysis process starts with question type detection. This process maps Wh-terms (What, Which, How, etc) into one or several of the categories listed in figure <ref type="figure" coords="3,422.87,119.65,3.98,9.67" target="#fig_1">2</ref>. When no category can be detected by Wh-term analysis, NONE is used (e.g. "What" questions). This module also includes definition as a question type. Definition questions are detected by applying patternmatching techniques.</p><p>Question type categories pertaining to group A are related to WordNet top concepts <ref type="bibr" coords="3,462.27,265.08,11.38,9.67" target="#b2">[2]</ref>. Each of these categories is represented by the vector of WordNet synsets that are semantically related to its corresponding top concept (called QTC-Question type characteristics). These synsets are obtained by extracting from WordNet all hyponyms of each top concept until third level and they are weighted depending on its level into the WordNet hierarchy and the frequency of its appearance into the path. As it can be deduced, NONE questions have an empty QTC.</p><p>Once question type has been obtained, the system selects the noun phrase in the query that expresses the semantic characteristics of the expected answer (definition concept). Definition concepts do not help the system to locate the correct answer into the document collection but they usually add critical information about the kind of information requested by the query. The semantic characteristics of definition concepts are represented by a weighted vector (CT-concept type) that includes the set of synsets they are semantically related to. These synsets are obtained by extracting from WordNet all hyperonyms of each definition concept head term (its path to top concepts) and they are weighted depending on its level into the WordNet hierarchy and the frequency of its appearance into the path towards top concepts.</p><p>QTC and definition concept CT are used to generate the expected answer semantic context (EASC). This context defines the semantic context that the expected answer has to be compatible with. This context is computed by performing exclusive vectorial addition between QTC and CT. As special case, EASC will be equal to CT for NONE questions. By the other hand, group B questions have a different treatment due to the special nature of its expected answers and therefore, at this analysis point only question type assignment is needed.</p><p>Once definition concepts have been detected, remaining question concepts are classified as key concepts. This question processing stage builds the semantic representation of the key concepts expressed into the query (semantic content of a question -QSC). This process consists of obtaining a general semantic representation of the concepts that appear in the question.</p><p>The head of a key concept syntactic structure represents the basic element or idea the concept refers to. Remaining terms pertaining to this structure modify this basic concept by refining the meaning represented by its head. Following this approach, the system tries to obtain and represent the different ways of expressing a concept. This process starts by associating each term pertaining to a concept, with its synonyms and one level search hyponyms and hyperonyms. These relations are extracted from WordNet lexical database. We define the semantic content of a term t (SCt) as a set of terms made up by the term t and all the terms related with it through the synonym and one level search hyponym and hyperonym relations. The SC of a term is represented using a weighted term vector. The weight assigned to each term pertaining to the SC of a term t is the 80%, 50% and 50% of the idf <ref type="bibr" coords="4,156.55,107.41,12.58,9.67" target="#b3">[3]</ref> value of term t for synonyms, hyponyms and hyperonyms respectively. As a concept is made up by the terms included into the same syntactic structure, we define the semantic content of a concept (SCC) as the set of weighted vectors (HSC, MSC) were HSC is the a vector obtained by adding the SC of the terms that made up the head of the concept and MSC is the vector resulting from adding the SC of terms that modify that head into the same syntactic structure. The set of SCCs that stand for the concepts appearing in a question builds the semantic content of a question (QSC). This way, the QSC represent all the concepts referenced into the question and the different ways of expressing each of them. All the described processes and related formulae are widely described and explained in <ref type="bibr" coords="4,233.19,205.92,11.38,9.67">[6]</ref>.</p><p>Figure <ref type="figure" coords="4,132.44,230.52,5.37,9.67">3</ref> sums up these processes using an example question. First, the system identifies and classifies the concepts "company", "manufacture" and "American Girl doll collection" by parsing question. Afterwards, the system generates the expected answer semantic context (EASC) and obtains the semantic content of each key concept to compound semantic content of the question (QSC).</p><p>Question keywords are used for first stage passage retrieval, QSC information will help paragraph selection module to detect the paragraphs that are more likely to contain the answer and finally, EASC (or question type for group B questions) will allow detecting and evaluating possible answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Passage retrieval module</head><p>First stage retrieval applies the passage retrieval approach described in <ref type="bibr" coords="4,434.60,666.21,11.37,9.67" target="#b1">[1]</ref>. This passage retrieval can be applied over all the document collection, but it has only been applied for the 1,000 relevant documents supplied by TREC organisation. Therefore, keywords detected at question processing stage are used for retrieving the 200 most relevant passages from the documents included in this initial list. This process is intended to reduce the amount of text that has to be </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSC HSC MSC HSC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition concept: company</head><p>Question type: NONE <ref type="bibr" coords="4,216.66,550.66,34.88,9.67">Figure 3</ref>. Question analysis processes processed by NLP modules since these passages are made up by text snippets of 15 sentences length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Paragraph selection</head><p>This module processes the 200 first ranked passages selected at passage retrieval stage in order to extract smaller text fragments that are more likely to contain the answer to the query. As all this process is widely described in <ref type="bibr" coords="5,215.45,192.37,12.53,9.67">[7]</ref>[9] we extract here the basic algorithm:</p><p>1. Documents are split into sentences.</p><p>2. Overlapping paragraphs of three sentences length are obtained.</p><p>3. Each paragraph is scored. This value measures the similarity between each paragraph and the question.</p><p>4. Paragraphs are ranked according to this score.</p><p>The score assigned to each paragraph (paragraph-score) is computed as follows:</p><p>a) Each SCC appearing in the question is compared with all the syntactic structures of the same type (noun or verbal phrases) appearing into each relevant paragraph. Each comparison generates a value. As result, each SCC is scored with the maximum value obtained for all the comparisons accomplished through the paragraph. b) The paragraph-score assigned to each paragraph is obtained by adding the values obtained for all SCCs of the question as defined in previous step. c) The value that measures similarity between a SCC and a syntactic structure of the same type is obtained by adding the weights of terms appearing into SCC vectors and the syntactic structure that is being analysed. If the head of this syntactic structure does not appear into the vector representing the SCC head (HSC), this value will be 0 (even if there are matching terms into MSC vector).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Filtering relevant paragraphs</head><p>This module has been added with the intention of managing with questions with no answer. This module filters relevant paragraphs by getting rid of those that contain value 0 for more than one SCC evaluated previously. This way, the system only accepts, as relevant, a paragraph that contains nearly all key concepts expressed in the query. At this stage, best 100 ranked paragraphs are selected to continue with the remaining processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Answer extraction</head><p>This process consists on analysing selected paragraphs in order to detect and evaluate concepts that can be considered probable answers. Among all the candidates the system will select the one it considers the correct answer. Answer extraction processes differ depending question type group.</p><p>For group A questions, the system gets rid of key concepts appearing in the paragraph and selects the concepts that validate lexical restrictions of the expected question (e.g. proper noun for a Who question). All these concepts are considered probable answers of the query. Next, the system computes the semantic context of each possible answer (SCPA) by taking into account the semantic concept types (CT) of the probable answer and its adjacent concepts in the paragraph. This way, The SCPA of a probable answer r is computed as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCPA r = CT (r-1) + CT r + CT (r+1)</head><p>Then, probable answers are filtered and only those that are compatible with the expected answer semantic context (EASC) are selected. For this purpose, each probable answer is assigned a score (probable-answer-compatibility) that measures its compatibility with the expected answer semantic context. Only probable answers with score greater than 0 are maintained. This value is computed as follows:</p><p>Next, compatible probable answers are evaluated by computing a final score (answer-score) that is obtained as follows:</p><p>Intuitively, the answer-score combines (1) the semantic compatibility between the probable answer and the expected answer (probable-answer-compatibility) and ( <ref type="formula" coords="6,398.31,345.83,4.19,9.67">2</ref>) the degree of similarity between question and paragraphs (paragraph-score). Finally, probable answers are ranked on answer-score and the system returns the first one as correct answer or NIL when no compatible probable answers have been found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer extraction manages differently with group B questions (definition, reason and manner).</head><p>The answer to this kind of questions is usually a part of a sentence that defines a concept, reason or a way of performing an action and they are usually expressed via certain sentence syntactic structures. Consequently, our approach performs probable answer detection and extraction by applying syntactic pattern-matching techniques over relevant paragraphs. This way, when no pattern has been successfully validated, the system returns NIL as answer. This approach, as well as a full description of the patterns is described in [6].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>We submitted one single run for main task. This task allowed one answer for each question and the response had to contain only the exact answer string to be considered correct. Figure <ref type="figure" coords="6,477.71,542.62,5.37,9.67" target="#fig_2">4</ref> shows the results obtained. 39 / 46 = 0.848</p><p>Our main objective was to inspect the way that restrictions imposed at paragraph selection and answer extraction stages affected system performance as a whole and, particularly, to the treatment of no-answer questions.</p><p>Result analysis shows two main circumstances to take into account. First, system performance presents a good precision since it has answered correctly a 72.4% of the questions the system considered to have answer in the collection (181 from 250 not NIL answers). Nevertheless, these filters seem to be too restrictive since the system has provided a NIL response for 211 questions with known answer. Second, our filtering approach does not perform correctly the detection of no answer questions. In fact, the precision achieved in this task has been very low (only a 15.6%). Moreover, despite of having answered as NIL a large number of questions (250), seven real NIL questions have not been recognised (a 15.2% of the 46 existing NIL questions).</p><p>Comparison with TREC-9 and TREC-10 results.</p><p>Comparison between our different participations is difficult and has to be analysed carefully since task specifications are significantly different. Nevertheless, we can compare 50-bytes strict results achieved in previous conferences with TREC-2002 results if we focus our attention mainly on the percentage of correct answers ranked in first place (see third column in figure <ref type="figure" coords="7,456.42,329.04,3.90,9.67">5</ref>). From this point of view, our system has achieved a significant improvement in precision since the percentage of correct answers retrieved in first place increases 12,8 points from TREC-10 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Future Work</head><p>As it can be deduced from result analysis, the main objective pursued this year has not been achieved. The filtering processes incorporated to paragraph selection and answer extraction stages have significantly increased system precision, they have failed in detecting questions with no answer in the collection. Consequently, we need to direct our next steps to investigate and test validation techniques that could cope efficiently with no answer questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,242.82,606.69,126.99,9.67"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. System architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,226.74,231.72,148.49,9.67"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Question type categories</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,244.02,681.56,125.67,9.67"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. TREC-2002 results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,175.87,365.25,205.68,164.75"><head>What is the company that manufactures the American Girl doll collection?</head><label></label><figDesc></figDesc><table coords="4,175.87,404.01,205.68,126.00"><row><cell cols="2">Key concept 1:</cell><cell cols="2">Key concept 2:</cell></row><row><cell cols="2">manufactures</cell><cell cols="2">American Girl doll collection</cell></row><row><cell>(no modifiers)</cell><cell>manufacture</cell><cell>American</cell><cell>collection</cell></row><row><cell></cell><cell>invent</cell><cell>Girl</cell><cell>aggregation</cell></row><row><cell></cell><cell>make</cell><cell>doll</cell><cell>accumulation</cell></row><row><cell></cell><cell>create</cell><cell>dolly</cell><cell>group</cell></row><row><cell></cell><cell>….</cell><cell>toy</cell><cell>compendium</cell></row><row><cell></cell><cell></cell><cell>plaything</cell><cell>…..</cell></row><row><cell></cell><cell></cell><cell>…..</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div> <ref type="bibr" coords="1,100.51,715.55,3.15,5.68" target="#b1">1</ref> <p>This work has been partially supported by the <rs type="funder">Spanish Government (CICYT)</rs> with grant <rs type="grantNumber">TIC2000-0664-C02-02</rs> and <rs type="funder">(PROFIT)</rs> with grant <rs type="grantNumber">FIT-150500-2002-416</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZG45wwp">
					<idno type="grant-number">TIC2000-0664-C02-02</idno>
				</org>
				<org type="funding" xml:id="_5X8VGUw">
					<idno type="grant-number">FIT-150500-2002-416</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,87.51,632.93,67.38,10.51" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.16,664.05,4.05,9.67;7,111.28,664.05,402.14,9.67;7,111.31,676.61,401.95,10.51;7,111.31,690.05,278.14,10.51" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,225.54,664.05,203.87,9.67">IR-n: a passage retrieval system at CLEF-2001</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,449.53,664.05,63.88,9.67;7,111.31,676.61,235.63,10.51">proceedings of the second Cross-Language Evaluation Forum</title>
		<title level="s" coord="7,425.30,676.61,87.97,10.51;7,111.31,690.05,83.65,10.51">Lecture Notes in Computer Science</title>
		<meeting>the second Cross-Language Evaluation Forum<address><addrLine>Darmstadt (Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">2001. September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.16,703.16,4.05,9.67;7,111.28,703.16,402.03,9.67;7,111.31,715.40,72.80,9.67" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,191.60,703.16,180.84,9.67">Wordnet: A Lexical Database for English</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,384.50,703.16,128.82,9.67">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.16,727.88,4.05,9.67;7,111.30,727.88,402.07,9.67" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<title level="m" coord="7,186.67,727.88,326.70,9.67">Automatic Text Processing: The Transformation, Analysis, and Retrieval of</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
