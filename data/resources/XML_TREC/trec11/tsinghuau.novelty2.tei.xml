<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,75.72,83.59,443.72,12.19;1,160.44,114.79,269.86,12.19;1,430.32,111.58,4.50,7.85">Expansion-Based Technologies in Finding Relevant and New Information: THU TREC2002 Novelty Track Experiments *</title>
				<funder ref="#_hq8jhm9">
					<orgName type="full">Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_vgEhq2Q">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_SjzTJx8">
					<orgName type="full">Chinese National Key Foundation Research &amp; Development Plan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,95.88,139.03,44.70,9.16"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>zhangmin99@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,148.15,139.03,52.33,9.16"><forename type="first">Ruihua</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.97,139.03,42.94,9.16"><forename type="first">Chuan</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.80,139.03,53.38,9.16"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.38,139.03,39.68,9.16"><forename type="first">Zhe</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.44,139.03,44.70,9.16"><forename type="first">Yijiang</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.10,139.03,41.20,9.16"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,464.26,139.03,35.20,9.16"><forename type="first">Le</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,75.72,83.59,443.72,12.19;1,160.44,114.79,269.86,12.19;1,430.32,111.58,4.50,7.85">Expansion-Based Technologies in Finding Relevant and New Information: THU TREC2002 Novelty Track Experiments *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2FE14B7A321250B246B7EB8152BB4972</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first time that Tsinghua University took part in TREC. In this year's novelty track, our basic idea is to find the key factor that help people find relevant and new information on a set of documents with noise. We paid attention to three points: 1. how to get full information from a short sentence; 2. how to complement hidden well-known knowledge to the sentences; 3. how to make the determination of duplication. Accordingly, expansion-based technologies are the key points. Studies of expansion technologies have been performed on three levels: efficient query expansion based on thesaurus and statistics, replacement-based document expansion, and term-expansion-related duplication elimination strategy based on overlapping measurement. Besides, two issues have been studied: finding key information in topics, and dynamic result selection. A new IR system has been developed for the task. In the system, four weighting strategies have been implemented: ltn.lnu <ref type="bibr" coords="1,163.68,385.93,8.16,6.12" target="#b0">[1]</ref> , BM2500 <ref type="bibr" coords="1,218.58,385.93,8.22,6.12" target="#b1">[2]</ref> , FUB1 <ref type="bibr" coords="1,261.84,385.93,8.16,6.12" target="#b2">[3]</ref> , FUB2 <ref type="bibr" coords="1,305.04,385.93,8.16,6.12" target="#b2">[3]</ref> . It provides both similarity and overlapping measurements, based on term expansion. Comparisons can be made on sentence-to-sentence or sentence-to-pool level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Query Expansion</head><p>In the task, it is most possible that a relevant sentence is mismatched to the query if we only use the original topic words. Therefore proper query expansion (QE) technology is necessary and helpful. Besides thesaurus based QE described in section 1 and 2, we proposed a new statistical expansion approach called local co-occurrence based query expansion, shown in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Using WordNet</head><p>Firstly Wordnet <ref type="bibr" coords="1,138.48,553.93,8.16,6.12" target="#b3">[4]</ref> is used as the thesaurus to expand query words. Totally three kinds of information were observed in our experiments: hyponyms (descendants), synonyms and coordinated words. Figure <ref type="figure" coords="1,103.22,587.83,4.38,9.16">2</ref>.1 shows the effects of QE using WordNet hyponyms. Effects of using WordNet synonyms and coordinated words are shown in Table <ref type="table" coords="1,250.50,603.43,3.94,9.16" target="#tab_0">2</ref>.1. In the figure, hpyo means to expand all hyponyms and sub-hyponyms of each topic word. And hypo_1, hypo_2 and hypo_3 refer to expanding words in the direct one or two or three levels of hyponyms respectively. Hypo_leaf is to expand hyponyms in leaf nodes of WordNet. Baseline result used long query. Results show that the more words expanded, the worse the retrieval performance is. All kinds of hyponyms expansion did not help retrieval. Expanding first level hyponyms (average P*R=0.066) makes trivial improvement to the baseline (average P*R = 0.064). Shown in the table, expansion based on synonyms achieves a little improvement in terms of average P*R while it does not help in terms of F-measure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Using Dr. Lin Dekang's synonyms dictionary</head><p>We also observed the performance by Dr. Lin Dekang's synonyms dictionary <ref type="bibr" coords="2,404.94,260.77,8.22,6.12" target="#b4">[5]</ref> . It provides two kinds of synonym dictionaries, based on dependency and mutual information respectively. This QE approach works better than the baseline in training set, while makes trivial improvement in test data (see Table <ref type="table" coords="2,469.81,294.67,3.84,9.16" target="#tab_0">2</ref>.2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Expansion</head><p>Sometimes, the query mentions a general topic while some relevant documents describe detailed information. For example, the concept of "vehicle" in query is expressed by specific words such as "car" , "truck" and "aircraft" in documents. In this case, (1) QE may take too many useless words because of aimless of expansion; (2) Setting weights for original and expanded terms is one of the main difficulties in QE. Therefore we proposed term expansion in documents (referred as DE) to solve the problem. Other than QE, the concept network in WordNet is definitely helpful. We used three levels of hypernyms (ancestor) and their synonyms, referred as hype_3 in our experiments. The algorithm of document expansion (DE) is as following. For each noun in a relevant document, if its 3-level hypernyms include any keyword in query, then replace the noun with the keyword. By doing this, the documents evolve into expanded documents while the query takes no change. Experimental results in Table <ref type="table" coords="3,433.00,123.43,4.39,9.16" target="#tab_2">3</ref>.1 show that DE got higher performance than QE under the same circumstances. The key point of DE is replacement. The keyword and its hyponyms were represented by an identical word, while the keyword and its hyponym were treated as different words in QE. Essentially DE used the concept space instead of the term space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Overlap Measurement Strategy Based on Term Expansion</head><p>On eliminating repetitive information, rather than concept of similarity, we used the concept of sentence overlapping. It represents the extent of the information taken by one sentence overlapped by another one.</p><p>This overlapping measure is unsymmetrical to the compared two sentences. Our experimental results show it is better than the symmetrical measure of similarity. Eq5.1 shows the overlapping of document B by document A, where A is the document preceding B.</p><formula xml:id="formula_0" coords="4,228.84,124.19,250.24,21.54">B B A B Overlap A I = 5.1</formula><p>Then the overlapping factor of B is max{Overlap B i | document i preceding B}. In repetitive information elimination, term expansion was performed. Suppose the two sentences that should be compared are D 1 and D 2 , the expanded parts of the original sentences are E 1 and E 2 respectively. Then the basic idea of elimination with term expansion (TE) is shown as Eq5.2.</p><formula xml:id="formula_1" coords="4,86.40,216.73,388.18,14.17">OverlapTE(D 1 , D 2 ) = Overlap(D 1 , D 2 )+ ∆ Overlap(E 1 , D 2 ) + ∆ Overlap(E 2 , D 1 ) 5.2</formula><p>Table <ref type="table" coords="4,100.57,239.83,4.36,9.16" target="#tab_5">5</ref>.1 shows the result of eliminating repetitive information by using standard qrels of relevant information as the input of the second step. It seems that the dataset used in TREC2002 is not redundant enough for testing the system ability of finding new information. 6 Special Issues</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Finding Keyword in Topics</head><p>In Novelty track, all the four domains of the topic can be used to retrieval, while the most useful information is taken by only several keywords. Therefore, finding key information from the topic is an important issue. We classified words in the topic into three classes by statistical learning and rule-based learning: useful keywords that contain the most useful words and were used to perform retrieval, general describing words that contain little information and were discarded directly and negative words that were applied to refine retrieval results.</p><p>To remove the topic-free words that contain no more information on describing the topic, two statistical learning methods were performed. Suppose the impact factor of the term is IF i , terms with impact factor lower than a threshold were general description words. IF i can be calculated by the two approaches:</p><formula xml:id="formula_2" coords="4,157.44,570.43,276.64,9.90">IF i = qtf i / sum i 6.1 IF i = tf i / n i 6.2</formula><p>Where qtf i is the term frequency for t i in the topic, sum i is the summation of qtf i in past TREC queries, tf i is the term frequency in relevant documents and n i is the number of documents that the term occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dynamic Result Selection</head><p>In general information retrieval experiments, the system returns fixed number of results to all the topics. In most cases, however, different topic has different number of relevant documents. Therefore, how many is enough is an important issue. We give the algorithm to select the documents whose similarity and rank fit in with the thresholds. Figure <ref type="figure" coords="4,198.12,695.77,4.37,9.16">6</ref>.1 and 6.2 show the effects of dynamic result selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Runs Submitted</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Discussion</head><p>In this year's TREC experiments, we mainly focused on the expansion-related technologies. Besides thesaurus based QE, which made only a little progress, we studied a new statistical expansion approach, called local co-occurrence expansion. The results are extremely good. It made consistent great progress not only in recall but also in precision. Furthermore, we proposed a novel document term expansion (DE) approach. Experimental results proofed encouraging effect of DE. Combinations of QE and DE by topic classification lead to better performance than either approach. On eliminating repetitive information, rather than concept of similarity, we used the concept of overlap with term expansion. Unfortunately however, it did not take improvement in the experiments. However, it seems that the dataset used in TREC2002 is not redundant enough for testing the system ability of finding new information, which may influence the conclusion of effectiveness of different approaches. We still take an optimistic view of redundancy elimination technology based on term expansion and overlap measurement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,79.20,217.03,214.23,9.16"><head>Figure 2 . 1</head><label>21</label><figDesc>Figure 2.1 Effects of QE with WordNet hyponyms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,311.40,76.63,185.85,152.80"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="2,311.40,76.63,185.85,152.80"><row><cell cols="5">.1 Effects of QE using WordNet</cell></row><row><cell cols="5">synonyms and coordinate words</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P*R</cell></row><row><cell>Baseline</cell><cell cols="4">0.2 0.28 0.197 0.064</cell></row><row><cell>Hypo_1</cell><cell cols="4">0.18 0.32 0.197 0.066</cell></row><row><cell>Synset</cell><cell cols="4">0.17 0.32 0.195 0.068</cell></row><row><cell cols="5">Coordinate 0.18 0.29 0.189 0.061</cell></row><row><cell cols="5">P: Average precision R: Average Recall</cell></row><row><cell>F: F-measure</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">P*R: Average Precision*Recall</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,72.00,310.27,451.39,319.30"><head>Table 2 .</head><label>2</label><figDesc>2 Effects of QE by Dr. Lin Dekang's synonyms dictionary</figDesc><table coords="2,112.38,326.35,376.04,303.22"><row><cell></cell><cell>Ave. Precision</cell><cell>Ave. Recall</cell><cell>F-measure</cell><cell>Ave. P*R</cell></row><row><cell></cell><cell>0.18</cell><cell>0.31</cell><cell>0.196</cell><cell>0.067</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">QE with different sources</cell></row><row><cell></cell><cell></cell><cell></cell><cell>baseline</cell><cell>LCE</cell><cell>lindek</cell></row><row><cell></cell><cell></cell><cell></cell><cell>synset</cell><cell>coordinate</cell><cell>hypo_1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.4</cell></row><row><cell></cell><cell>Baseline</cell><cell>LCE</cell><cell>0.3 0.35</cell></row><row><cell cols="2">Ave. Precision 0.20</cell><cell>0.21</cell><cell>0.25</cell></row><row><cell>Ave. Recall</cell><cell>0.28</cell><cell>0.34</cell><cell>0.15 0.2</cell></row><row><cell>F-measure</cell><cell>0.197</cell><cell>0.227</cell><cell>0.05 0.1</cell></row><row><cell>Ave. P*R</cell><cell>0.064</cell><cell>0.081</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Precision Recall F-measure</cell><cell>P*R</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Figure 2.2 Overview of QE experiments</cell></row></table><note coords="2,93.24,364.55,195.51,10.46;2,72.00,386.41,451.39,9.16;2,72.00,402.01,451.34,9.16;2,72.00,417.61,451.33,9.16;2,72.00,433.21,451.24,9.16;2,72.00,448.81,372.57,9.16;2,72.00,464.41,403.95,9.16;2,122.04,495.61,135.22,9.16;2,137.28,511.21,104.70,9.16"><p><p><p><p><p><p>2.3 QE based on local co-occurrence</p>We proposed a new statistical expansion approach, which expands terms highly co-occurred in a fixed window size with any of headwords in the relevant document set, called local co-occurrence expansion (LCE). The results are extremely good. Other than most expansion techniques, LCE made consistent great progress in terms of both recall and precision. Experimental results are shown in Table</p>2</p>.3. By using LCE, we got 15% and 28% improvement in terms of F-measure and average P*R respectively. Figure</p>2</p>.2 gives the overview of query expansion technologies used in our novelty experiments. Table 2.3 Effects of QE by local co-occurrence expansion</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,72.00,185.83,390.66,83.52"><head>Table 3 .</head><label>3</label><figDesc>1 Comparisons between QE and DE</figDesc><table coords="3,72.00,201.91,390.66,67.44"><row><cell>Method</cell><cell cols="2">Ave. Precision Ave. Recall</cell><cell>F-measure</cell><cell>Ave. P*R</cell></row><row><cell>QE (hypo_3)</cell><cell>0.14</cell><cell>0.25</cell><cell>0.179</cell><cell>0.057</cell></row><row><cell>DE (hype_3)</cell><cell>0.18</cell><cell>0.40</cell><cell>0.248</cell><cell>0.079</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,72.00,256.36,451.40,230.40"><head>Combination of QE and DE 4.1 Topic Classification by QE and DE QE</head><label></label><figDesc>and DE are oriented from two aspects of retrieval problem and may work well for different topics. Therefore we classified the topics into two classes according to topic or document characteristics to perform QE or DE respectively, which lead to better performance than either approach.</figDesc><table /><note coords="3,72.00,352.81,451.38,9.90;3,72.00,368.41,451.37,9.90;3,72.00,384.01,451.31,9.90;3,72.00,399.61,307.19,9.90;3,72.00,415.21,451.30,9.16;3,72.00,428.17,359.02,11.80;3,72.00,446.41,451.24,9.16;3,72.00,462.01,451.40,9.16;3,72.00,477.61,409.89,9.16"><p>One intuitive method of classification is topic-oriented. Define fields' similarities in topic: FS td (&lt;title&gt; and &lt;desc&gt;), FS tn (&lt;title&gt; and &lt;narr&gt;) and FS dn (&lt;desc&gt; and &lt;narr&gt;). In our experiments we use the following rules: if FS dn &lt;θ 1 and (FS td +FS dn -2FS tn ) &lt;θ 2 , then the topic should use DE on the topic, otherwise QE is performed. The thresholdsθ 1 andθ 2 are set according to 0.07 and 0.035. The other one is document-oriented. Compute the value of: (# words expanded)/(# words in docs) for each topic. Only when the value is greater thanθ, use DE. In our experiments, θ= 0.058. All the parameters were set according to TREC2002 training examples. It got better performance although the thresholds are not fit for testing data completely. The effects of two approaches are shown in Table 4.1, where TOTC and DOTC means topic similarity and DE oriented topic classification, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,72.00,493.21,451.46,208.74"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table coords="3,72.00,493.21,451.46,208.74"><row><cell></cell><cell cols="2">1 Effects of topic classification</cell><cell></cell></row><row><cell>Method</cell><cell>Ave. Precision</cell><cell>Ave. Recall</cell><cell>Ave. P*R</cell></row><row><cell>QE (LCE)</cell><cell>0.21</cell><cell>0.34</cell><cell>0.081</cell></row><row><cell>DE</cell><cell>0.22</cell><cell>0.28</cell><cell>0.066</cell></row><row><cell>TOTC</cell><cell>0.23</cell><cell>0.34</cell><cell>0.087</cell></row><row><cell>DOTC</cell><cell>0.23</cell><cell>0.374</cell><cell>0.086</cell></row><row><cell>4.2 Result Combination</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">We've tried several different combination strategies. Here are two that work pretty well. One is called</cell></row><row><cell cols="5">re-ranking (Eq4.1), and another one is called combining inversed rank (Eq4.2). We used Eq2.7 in the</cell></row><row><cell cols="3">experiments. The combined approaches are QE(LCE) and DE. λ ≤ 0.3.</cell><cell></cell></row><row><cell cols="4">If Doc i ∈ result list1 &amp; Doc i ∈ list2, then Sim i '=λS 1i , (λ&gt;1) else S'=S 1i</cell><cell>4.1</cell></row><row><cell cols="4">if Doc i ∈ result list1 or Doc i ∈ list2, Sim i '=λ*1/Rank 1i + (1-λ)*1/Rank 2i , (λ&lt;1)</cell><cell>4.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,111.18,286.63,367.22,73.54"><head>Table 5 .</head><label>5</label><figDesc>1 Effects of repetition elimination by using qrels of relevant</figDesc><table coords="4,287.88,302.71,190.52,9.16"><row><cell>Ave precision</cell><cell>Ave recall</cell><cell>Ave P*R</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,72.00,742.57,451.37,24.76"><head>Table 7 .</head><label>7</label><figDesc>1 show the runs we submitted in novelty experiments, where DOTC, TOTC and QE(LCE) have the same definition of Table4.1. Comb_QE_DE is the combining inversed rank of QE and DE. The first step results of above four results are got by Okapi system. And the last result is got by our new system with short query. All the second step results were got by the new system.</figDesc><table coords="5,96.06,123.43,398.26,277.84"><row><cell cols="7">Figure 6.1 Result number deduction</cell><cell cols="6">Figure 6.2 Retrieval performance improvement</cell></row><row><cell></cell><cell></cell><cell cols="4">Result number deduction</cell><cell></cell><cell></cell><cell cols="5">Retriev performance improvement</cell></row><row><cell></cell><cell>26000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.24</cell><cell cols="2">F-measure</cell><cell></cell><cell>P*R</cell></row><row><cell>#result</cell><cell>6000 11000 16000 21000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>performance</cell><cell>0.08 0.12 0.16 0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Finding relevant information Elimination repetitive information</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ave P Ave R</cell><cell cols="2">Ave P*R</cell><cell>Ave P</cell><cell></cell><cell>Ave R</cell><cell>Ave P*R</cell></row><row><cell cols="3">Thunv1. DOTC</cell><cell></cell><cell></cell><cell>0.23</cell><cell>0.34</cell><cell cols="2">0.086</cell><cell>0.22</cell><cell></cell><cell>0.30</cell><cell>0.073</cell></row><row><cell cols="3">Thunv2. TOTC</cell><cell></cell><cell></cell><cell>0.23</cell><cell>0.34</cell><cell cols="2">0.087</cell><cell>0.23</cell><cell></cell><cell>0.29</cell><cell>0.074</cell></row><row><cell cols="6">Thunv3. Comb_QE_DE 0.20</cell><cell>0.41</cell><cell cols="2">0.088</cell><cell>0.20</cell><cell></cell><cell>0.35</cell><cell>0.073</cell></row><row><cell cols="4">Thunv4. QE(LCE)</cell><cell></cell><cell>0.21</cell><cell>0.34</cell><cell cols="2">0.081</cell><cell>0.21</cell><cell></cell><cell>0.28</cell><cell>0.067</cell></row><row><cell cols="4">Thunv5. New System</cell><cell></cell><cell>0.19</cell><cell>0.35</cell><cell cols="2">0.066</cell><cell>0.18</cell><cell></cell><cell>0.31</cell><cell>0.060</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,87.42,408.25,420.55,9.16"><head>Table 7 .</head><label>7</label><figDesc>1 Submitted runs and evaluation results of Tsinghua University in TREC2002 novelty Track</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>* Supported by the <rs type="funder">Chinese National Key Foundation Research &amp; Development Plan</rs> (Grant <rs type="grantNumber">G1998030509</rs>), <rs type="funder">Natural Science Foundation</rs> No.<rs type="grantNumber">60223004</rs>, and National 863 High Technology Project No. <rs type="grantNumber">2001AA114082</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SjzTJx8">
					<idno type="grant-number">G1998030509</idno>
				</org>
				<org type="funding" xml:id="_hq8jhm9">
					<idno type="grant-number">60223004</idno>
				</org>
				<org type="funding" xml:id="_vgEhq2Q">
					<idno type="grant-number">2001AA114082</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,86.19,673.59,411.45,8.74" xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Baeza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,282.34,673.59,119.63,8.74">Modern Information Retrieval</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,86.19,689.19,324.97,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,248.53,689.19,108.82,8.74">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<idno>TREC-8</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.94,704.79,435.41,8.74;5,89.10,720.39,241.71,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,331.78,704.79,191.58,8.74;5,89.10,720.39,182.89,8.74">FUB at TREC-10 Web Track: A probabilistic framework for topic relevance term weighting</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Romano</surname></persName>
		</author>
		<idno>TREC-10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,86.15,735.99,414.39,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,147.30,735.99,148.67,8.74">WordNet: An on-line lexical database</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,302.53,735.99,149.51,8.74">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,86.14,751.59,290.14,8.74" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Miniparser</surname></persName>
		</author>
		<ptr target="http://www.cs.ualberta.ca/~lindek/minipar.htm" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
