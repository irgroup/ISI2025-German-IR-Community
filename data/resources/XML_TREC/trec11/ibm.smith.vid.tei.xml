<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.45,110.66,355.28,15.49">IBM Research TREC-2002 Video Retrieval System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,132.53,147.28,54.78,10.76;1,187.33,144.33,1.41,8.37"><forename type="first">Bill</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.30,147.28,58.76,10.76;1,252.08,144.33,1.88,8.37"><forename type="first">Arnon</forename><surname>Amir</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Almaden Research Center</orgName>
								<address>
									<addrLine>650 Harry Road</addrLine>
									<postCode>95120</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.05,147.28,60.09,10.76;1,318.15,144.33,1.88,8.37"><forename type="first">Chitra</forename><surname>Dorai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>19 Skyline Drive</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.13,147.28,69.33,10.76;1,393.47,144.33,1.88,8.37"><forename type="first">Sugata</forename><surname>Ghosal</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IBM India Research Lab</orgName>
								<address>
									<addrLine>Block 1</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">IIT</orgName>
								<address>
									<postCode>110016</postCode>
									<region>New Delhi</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.45,147.28,91.23,10.76;1,490.68,144.33,1.41,8.37"><forename type="first">Giridharan</forename><surname>Iyengar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,108.03,161.12,83.33,10.76;1,191.37,158.18,1.88,8.37"><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>19 Skyline Drive</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,197.34,161.12,71.39,10.76;1,268.75,158.18,1.88,8.37"><forename type="first">Christian</forename><surname>Lang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>19 Skyline Drive</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.71,161.12,76.71,10.76;1,351.44,158.18,1.88,8.37"><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>19 Skyline Drive</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.40,161.12,73.75,10.76;1,431.16,158.18,1.88,8.37"><forename type="first">Apostol</forename><surname>Natsev</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>19 Skyline Drive</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,437.14,161.12,78.02,10.76;1,515.17,158.18,1.88,8.37"><forename type="first">Milind</forename><surname>Naphade</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>19 Skyline Drive</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,99.16,174.98,77.96,10.76;1,177.13,172.02,1.41,8.37"><forename type="first">Chalapathy</forename><surname>Neti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.11,174.98,73.36,10.76;1,256.48,172.02,1.41,8.37"><forename type="first">Harriet</forename><forename type="middle">J</forename><surname>Nock</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.45,174.98,87.98,10.76;1,350.45,172.02,1.88,8.37"><forename type="first">Haim</forename><forename type="middle">H</forename><surname>Permuter</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Almaden Research Center</orgName>
								<address>
									<addrLine>650 Harry Road</addrLine>
									<postCode>95120</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.41,174.98,93.54,10.76;1,449.97,172.02,1.88,8.37"><forename type="first">Raghavendra</forename><surname>Singh</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IBM India Research Lab</orgName>
								<address>
									<addrLine>Block 1</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">IIT</orgName>
								<address>
									<postCode>110016</postCode>
									<region>New Delhi</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,455.94,174.98,68.09,10.76;1,524.05,172.02,1.88,8.37"><forename type="first">John</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>19 Skyline Drive</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,152.51,188.82,88.49,10.76;1,241.01,185.88,1.88,8.37"><forename type="first">Savitha</forename><surname>Srinivasan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Almaden Research Center</orgName>
								<address>
									<addrLine>650 Harry Road</addrLine>
									<postCode>95120</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.97,188.82,70.72,10.76;1,317.71,185.88,1.88,8.37"><forename type="first">Belle</forename><forename type="middle">L</forename><surname>Tseng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>19 Skyline Drive</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.31,188.82,81.34,10.76;1,473.68,185.88,2.59,8.37"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IBM India Research Lab</orgName>
								<address>
									<addrLine>Block 1</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">IIT</orgName>
								<address>
									<postCode>110016</postCode>
									<region>New Delhi</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Dept. of E.E</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.45,110.66,355.28,15.49">IBM Research TREC-2002 Video Retrieval System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5FCE59AD193C1DFE535D1252AFCCEEAB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the IBM Research system for analysis, indexing, and retrieval of video, which was applied to the TREC-2002 video retrieval benchmark. The system explores novel methods for fully-automatic content analysis, shot boundary detection, multi-modal feature extraction, statistical modeling for semantic concept detection, and speech recognition and indexing. The system supports querying based on automatically extracted features, models, and speech information. Additional interactive methods for querying include multiple-example and relevance feedback searching, cluster, concept, and storyboard browsing, and iterative fusion based on user-selected aggregation and combination functions. The system was applied to all four of the tasks of the video retrieval benchmark including shot boundary detection, concept detection, concept exchange, and search. We describe the approaches for each of the tasks and discuss some of the results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The growing amount of digital video is driving the need for more effective methods for indexing, searching, and retrieving video based on its content. Recent advances in content analysis, feature extraction, and classification are improving capabilities for effectively searching and filtering digital video content. Furthermore, the recent MPEG-7 standard promises to enable interoperable content-based retrieval by providing a rich set of standardized tools for describing features of multimedia content <ref type="bibr" coords="1,257.66,522.87,9.52,8.07" target="#b0">[1]</ref>. However, the extraction and use of MPEG-7 descriptions and the creation of usable fully-automatic video indexing and retrieval systems remains a significant technical challenge.</p><p>The TREC video retrieval benchmark is facilitating the technical advancement of content-based retrieval of video by standardizing a benchmark video corpus along with different video retrieval and detection tasks. The benchmark provides a consistent evaluation framework for assessing progress as researchers experiment with novel video indexing techniques. This year, we participated in the TREC video retrieval benchmark and submitted results for four tasks: (1) shot boundary detection, (2) concept detection, (3) concept exchange, (4) search. We explored several</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Outline</head><p>The outline is as follows: in Section 2, we describe our process for video and speech indexing. In Section 3, we describe the video retrieval system including methods for content-based search, model-based search, speech-based search, and other methods for interactive searching and browsing. In Section 4, we discuss the approaches for each of the benchmark tasks and examine some of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Video indexing system</head><p>The video indexing system analyzes the video in an off-line process that involves video content indexing and speech indexing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Shot boundary detection (SBD)</head><p>Shot boundary detection (SBD) is performed using the real-time IBM CueVideo system <ref type="bibr" coords="2,156.19,115.76,10.45,8.07" target="#b1">[2]</ref> which automatically detects shots and extracts key-frames. This year, we explored several methods for making SBD more robust to poor video quality. Some of the methods include using localized edge gradient histograms and comparing pairs of frames at greater temporal distances. Overall, our 2002 SBD system showed reduction in errors by more than 30% compared to our 2001 SBD system <ref type="bibr" coords="2,184.52,178.23,9.52,8.07" target="#b2">[3]</ref>.</p><p>The baseline CueVideo SBD system uses sampled, threedimensional color histograms in RGB color space to compare pairs of frames. Histograms of recent frames are stored in a buffer to allow a comparison between multiple image pairs up to seven frames apart. Statistics of frame differences are computed in a moving window around the processed frame and are used to compute the adaptive thresholds, shown in Figure <ref type="figure" coords="2,208.08,251.88,4.48,8.07" target="#fig_1">2</ref> as a line above the difference measures (Diff1, Diff3 and Edge1). A state machine is used to detect the different events (states). The SBD system does not require any sensitivity-tuning parameters. More details about the baseline system can be found in <ref type="bibr" coords="2,188.49,293.52,9.71,8.07" target="#b2">[3,</ref><ref type="bibr" coords="2,200.43,293.52,6.47,8.07" target="#b3">4]</ref>. Notice the ground truth (GT) and system output (Sys) plots for this segment of video which has six dissolves (one missed) and twelve cuts.</p><p>Several changes were incorporated to the baseline SBD algorithm to accommodate lower video quality, as was the case for the videos in the TREC-02 data set. Localized edge-gradient histograms were added to overcome color errors. The 512-bin edgegradient histogram counts the number of pixels in each of eight image regions, having similar Ix, Iy derivatives (each derivative is quantized into three bits). Thus it is less sensitive to lighting and color changes. Rank filtering was added in time/space/histogram at various different points along the processing to handle the new types and higher levels of noise. The comparison of pairs of frames at wider distances up to thirteen frames apart was added to overcome the high MPEG-1 compression noise. Several new states were added to the state machine to detect certain types of video errors and to detect very short dissolves that were 2-3 frames long. These changes were tuned based on precision-recall measurements using data subsets from TREC01 test set and TREC02 training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature extraction</head><p>The system extracts a number of descriptors for each video shot. Some of the descriptors, as indicated below, are extracted in multiple ways from each key-frame image using different normalization strategies (see <ref type="bibr" coords="2,369.96,234.42,10.08,8.07" target="#b4">[5]</ref>) as follows: (1) global, (2) 4x4 grid, (3) 5-region layout, and (4) automatically extracted regions. The following descriptors were extracted: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Region extraction</head><p>In order to better extract local features and detect concepts, we developed a video region segmentation system that automatically extracts foreground and background regions from video. The system runs in real-time with extraction of regions from I-frames and P-frames in MPEG-1 video. The segmentation of the background scene regions uses a block-based region growing method based on color histograms, edge histograms, and directionality. The segmentation of the foreground regions uses a spiral searching technique to calculate the motion vectors of I-and P-frames. The motion features are used in region growing in the spatial domain with additional tracking constraints in the time domain. Although we tested MPEG-1 compressed-domain motion vectors, we found them to be too noisy. We also found that combining motion vectors, color, edge, and texture information for extraction of foreground objects did not give significantly better results than using only motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Clustering</head><p>We used the extracted visual descriptors (see Section 2.2) to cluster the video shots into perceptually similar groups. We used a k-means clustering algorithm to generate 20 clusters. We found color correlograms to achieve an excellent balance between color and texture features. The clusters were later used to facilitate browsing and navigation for interactive retrieval (as described in Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Concept detection</head><p>The concept detection system learns from labeled training video content to classify unknown video content (in our case, the feature test and search test data). We have investigated several different types of statistical models including Support Vector Machines (SVM), Gaussian Mixture Models (GMM) and Hidden Markov Models (HMM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Lexicon design</head><p>The first step in designing a semantic concept detection system is the construction of a concept lexicon <ref type="bibr" coords="3,205.53,387.95,13.74,8.07" target="#b9">[10]</ref>. We viewed the training set video and identified the most salient frequently occurring concepts and fixed a lexicon of 106 concepts, which included the 10 concepts belonging to the TREC concept detection task (denoted as primary concepts). Overall, we generated training and validation data and modeled the following 10 primary concepts: Outdoors, Indoors, Cityscape, Landscape, Face, People, Text Overlay, Music, Speech and Monologue. We also modeled the following 39 secondary generic concepts:</p><p>• Objects: Person, Road, Building, Bridge, Car, Train, Transportation, Cow, Pig, Dog, Penguin, Fish, Horse, Animal, Tree, Flower, Flag, Cloud,</p><p>• Scenes: Man Made Scenes, Beach, Mountain, Greenery, Sky, Water, Household Setting, Factory Setting, Office Setting, Land, Farm, Farm House, Farm Field, Snow, Desert, Forest, Canyon,</p><p>• Events: Parade, Explosion, Picnic, Wedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Annotation</head><p>In order to generate training and validation data, we manually annotated the video content using two annotation tools<ref type="foot" coords="3,263.71,620.66,2.99,5.38" target="#foot_0">1</ref> -one produced the visual annotations and the other produced audio annotations. The IBM MPEG-7 Video Annotation Tool (a.k.a. VideoAn-nEx), shown in Figure <ref type="figure" coords="3,154.06,653.66,3.36,8.07" target="#fig_2">3</ref>, allows the shots in the video to be annotated using terms from an imported lexicon. The tool is compatible with MPEG-7 in that the lexicons can be imported as MPEG-7 classification schemes and generates MPEG-7 descriptions of the video based on the detected shots and annotations. The tool also allows the users to directly create and edit lexicons. The second tool, the IBM Multimodal Annotation Tool, provides three modes of annotation: video, audio with video, or audio without video. The audio annotation is based upon audio segments in which the user manually delimits each segment within the audio upon listening and selects from the lexicon those terms that describe the audio content. Multimodal concepts (e.g. Monologues) are annotated using audio with video mode of annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Concept modeling</head><p>Semantic concept detection was investigated using a statistical classification methodology (as described in <ref type="bibr" coords="3,474.25,432.87,14.19,8.07" target="#b10">[11,</ref><ref type="bibr" coords="3,490.50,432.87,11.20,8.07" target="#b11">12,</ref><ref type="bibr" coords="3,503.76,432.87,10.30,8.07" target="#b9">10]</ref>). The system learns the parameters of the classifiers using training data for each concept using statistical methods. We considered two approaches: one based on a decision theoretic approach and the other based on a risk minimization approach.</p><p>Decision theoretic approach In this approach, the descriptors are assumed to be independent identically distributed random variables drawn from known probability distributions with unknown deterministic parameters. For the purpose of classification, we assume that the unknown parameters are distinct under different hypotheses and can be estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural risk minimization</head><p>Unlike the decision theoretic approach, the discriminant approach focuses only on those characteristics of the feature set that discriminate between the two hypotheses of interest. The idea of constructing learning algorithms based on the structural risk minimization inductive principle was proposed in <ref type="bibr" coords="3,362.59,628.88,13.74,8.07" target="#b12">[13]</ref>. In particular, we used Support Vector Machines (SVM)<ref type="foot" coords="3,343.48,637.51,2.99,5.38" target="#foot_1">2</ref> , which map the feature vectors into a higher dimensional space through nonlinear function and constructing the optimal separating hyper-plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and validation</head><p>Training and validation of models was done using the NIST feature training data set. We randomly partitioned the NIST feature training data set into a 19 hour Feature Training (FTR) collection and a 5 hour Feature Validation (FV) collection. We used the FTR collection to construct the models and the FV collection to select parameters and evaluate the concept detection performance. The validation process was beneficial in helping to avoid over-fitting to the FTR collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">Fusion</head><p>Since no single descriptor is powerful enough to encompass all aspects of video content and separate the concept hypotheses, combining information is needed at several levels in the concept modeling and detection processes. We experimented with two distinct approaches involving early fusion and late fusion. For early fusion we experimented with fusing descriptors prior to classification. For late fusion we experimented with retaining soft decisions and fusing classifiers. In addition, we explored various combining methods and aggregation functions for late fusion of search results as described in Section 3.6. Two modeling procedures are used. They use different subsets of visual features. The first procedure utilizes both early and late fusions, while the second procedure uses only late fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature fusion</head><p>The objective of feature fusion is to combine multiple features at an early stage to construct a single model. However, since this increases the dimensionality of the feature space-which makes it sparser-it also makes the classification problem harder and increases the risk of over-fitting the data. This approach is therefore most suitable for concepts that have sufficiently large number of training set examples that would allow the classifier to exploit correlations between the features. We experimented with feature fusion by simply normalizing and concatenating descriptors. Different combinations of descriptors were used to construct models. We used the validation set to choose the best combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier fusion</head><p>In an ideal situation, early fusion should work for all concepts, since all of the information is available to the classifier. However, practical considerations, such as limited number of training examples and the increased risk of over-fitting necessitate an alternate strategy. If the features are fairly de-correlated, then treating them independently is less of a concern. In such situations, we model concepts in each modality or feature space independently, and fuse individual classifier decisions later. We used a separate model (SVM or GMM) for each descriptor, which results in multiple classifications and associated confidences for each shot depending on the descriptor. While the classifiers can be combined in many ways, we explored normalized ensemble fusion to improve overall classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.5">Specialized detectors</head><p>Although we used the above generic approaches for detection of most concepts, for two concepts (monologues and text overlay) we explored specialized approaches as follows:</p><p>Monologue detection For monologue detection, we first performed speech and face detection on each shot. Then, for shots containing speech and face, we further evaluated the synchrony between the face and speech using mutual information and used the combined score thus generated to rank all shots in the corpus. Based on experimental results of a variety of synchrony detection techniques, we used a scheme that models audio and video features as locally Gaussian distributions (see <ref type="bibr" coords="4,472.73,97.92,14.93,8.07" target="#b13">[14]</ref> for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text overlay detection</head><p>We explored two algorithms for extracted overlay text in video and fused the results of the classifiers to produce the final concept labeling. The first method (see <ref type="bibr" coords="4,536.17,142.49,14.33,8.07" target="#b14">[15]</ref>) works by extracting and analyzing regions in a video frame. The processing stages in this system are: (1) isolating regions that may contain text characters, (2) separating each character region from its surroundings and (3) verifying the presence of text by consistency analysis across multiple text blocks. A confidence measure is computed as a function of the number of characters in text objects in the frame. The second method uses macro-block-based texture and motion energy. Layout analysis is used to verify the layout of these character blocks. A text region is identified if the character blocks can be aligned to form sentences or words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Speech recognition and indexing</head><p>As in TREC-2001, we constructed a speech-based system for video retrieval. Significant improvements were made to both the automatic speech recognition (ASR) performance and the speech search engine performance relative to our TREC-2001 submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Automatic speech recognition (ASR)</head><p>A series of increasingly accurate speech transcriptions for the entire corpus were produced in the period leading up to the evaluation. The first set of transcriptions were produced using an IBM real-time transcription system tuned for Broadcast News; this is the same transcription system as was used in TREC-2001 <ref type="bibr" coords="4,541.40,402.92,9.52,8.07" target="#b2">[3]</ref>. Later transcriptions were produced using an off-line, multiple pass transcription system comprising the following stages (see <ref type="bibr" coords="4,526.43,423.75,14.93,8.07" target="#b15">[16]</ref> for more details and citations):</p><p>• Remove silent videos</p><p>• Divide each video into segments using Bayesian Information Criteria (BIC) • Detect "music" and "silence" and transcribe using an IBM 10×Real-Time Broadcast News transcription System • Apply supervised Maximum Likelihood Linear Regression (MLLR) adaptation of speaker-independent HUB4 models using a set of eight (word-level transcribed) videos • Decode "speech-only" segments using interpolated trigram Language Model (LM) • Cluster "speech-only" segments into "speaker-and environment-similar" clusters • Apply unsupervised MLLR adaptation of TREC-2002adapted HUB4 models to each cluster using single global MLLR mean and precision transforms</p><p>The word error rate (WER) of the final transcripts is estimated at 34.6% on a held out set of six videos from Search Test and Feature Test which were manually transcribed <ref type="foot" coords="4,488.88,656.56,2.99,5.38" target="#foot_2">3</ref> . This compares favorably to 39.0% for the best of the publicly-released transcriptions on the same set and represents a 41% improvement over the transcriptions used as the basis for IBM's TREC-2001 SDR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Speech indexing</head><p>Indexes were constructed for SDR from the final most accurate speech transcriptions. Three types of indexes were generated: document-level indexes, an inverse word index, and a phonetic index. No attempt was made to index the set of silent videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level indexes:</head><p>The document-level indexes support retrieval at the document level, where a document is defined to span a temporal segment containing at most 100 words <ref type="foot" coords="5,269.90,185.88,2.99,5.38" target="#foot_3">4</ref> . Consecutive documents overlap by 50 words in order to address boundary truncation effects. Once documents are defined and their associated time boundaries are recorded, the documents are preprocessed using (1) tokenization to detect sentence/phrase boundaries; (2) (noisy) part-of-speech tagging such as noun phrase, plural noun etc; (2) morphological analysis, which uses the partof-speech tag and a morph dictionary to reduce each word to its morph eg. verbs [lands], [landing] and [land] reduce to /land/; (4) "stop" words are removed using standard stop-word lists. After pre-processing, indexes are constructed and statistics (such as word and word pair term-and inverse-document frequencies) are recorded for use during retrieval.</p><p>Inverse word index: the inverse word index supports Boolean search by providing the (video i , time i ) of all the occurrences of a query term in the videos. Preprocessing of transcripts is similar to that above.</p><p>Phonetic index: the phonetic index supports search of out-ofvocabulary words. The (imperfect) speech transcript is converted to a string of phones <ref type="bibr" coords="5,147.38,423.63,13.74,8.07" target="#b16">[17]</ref>. The phonetic index can be searched for sound-like phone sequences, corresponding to out-of-vocabulary query terms such as some acronyms, names of people, places, and so forth <ref type="foot" coords="5,99.65,453.09,2.99,5.38" target="#foot_4">5</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Video retrieval system</head><p>The video retrieval system provides a number of facilities for searching, which include content-based retrieval (CBR), modelbased retrieval (MBR), speech-based search or spoken document retrieval (SDR) and other interactive methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Content-based retrieval (CBR)</head><p>The objective of CBR is to match example query content to target video content using the extracted descriptors (see Section 2.2). The degree of match is determined on basis of feature similarity, which we have measured using Minkowski-form metrics considering values of r = 1 (Manhattan distance) and r = 2 (Euclidean distance) as follows: given descriptors represented as multi-dimensional feature vectors, vq and vt be the query and target vectors, respectively, then</p><formula xml:id="formula_0" coords="5,377.14,116.61,177.06,30.19">d r q,t = ( M -1 m=0 |vq[m] -vt[m]| r ).<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model-based retrieval (MBR)</head><p>Model-based search allows the user to retrieve video shots based on the concept labels produced by the models (see Section 2.5).</p><p>In MBR, the user enters the query by typing label text, or the user selects from the label lexicon. Since a confidence score is associated with each automatically assigned label, MBR ranks the shots using a distance D derived from confidence C using D = 1 -C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Speech-based search (SDR)</head><p>Speech-based search allows the user to retrieve video shots based on the speech transcript associated with the shots. We used multiple SDR systems independently and combined the results to produce the final SDR results for TREC-2002; we refer to the three systems as OKAPI-SYSTEM-1, OKAPI-SYSTEM-2, BOOLEAN-SYSTEM-1. To evaluate different design decisions, a limited ground truth was created for the combined FTR and FV collections by pooling the results and performing relevance assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query development and preprocessing:</head><p>All SDR systems operate using a textual statement of information need. Query strings are pre-processed in a similar manner to the documents: tokenization, tagging and morphing gives the final query term sequence for use in retrieval.</p><p>Video segment retrieval: Given a query, the three SDR systems rank documents or video segments as follows:</p><p>• OKAPI-SYSTEM-1, OKAPI-SYSTEM-2: a single pass approach is used to compute a relevancy score for each document. Each document is ranked against a query, where the relevancy score is given by the OKAPI formula <ref type="bibr" coords="5,508.34,515.00,13.74,8.07" target="#b17">[18]</ref>. The total relevancy score for the query string is the combined score of each of the query terms. The scoring function takes into account the number of times each query term occurs in the document and how rare that query term is across the entire corpus, with normalization based upon the length of the document to remove the bias towards longer documents since longer documents are more likely to have more instances of any given word.</p><p>• BOOLEAN-SYSTEM-1: a Boolean search was applied to Boolean queries. This search also supported phonetic search of out-of-vocabulary words using the phonetic index, in conjunction with in-vocabulary words which can be located in the inverse word index.</p><p>Many SDR systems use the results of first pass retrieval as the basis for automatic query expansion scheme prior to running a second pass of retrieval. Experiments showed little gain from using an LCA-based scheme <ref type="bibr" coords="5,402.17,704.29,14.93,8.07" target="#b18">[19]</ref> on FTR+FV, since the number of relevant documents retrieved per query in the first pass is quite low, so the approach was not investigated further.</p><p>Video segment-to-shot mapping: NIST evaluates video retrieval performance at the level of shots, rather than at the level of documents or video segments which span one or more shots. Thus we must somehow use the scores assigned to documents or video segments by SDR to assign scores at the level of shots <ref type="foot" coords="6,285.75,127.37,2.99,5.38" target="#foot_5">6</ref> . The mappings used in the three component systems are:</p><p>• OKAPI-SYSTEM-1: the score assigned to a document is assigned to the longest shot overlapping that document;</p><p>• OKAPI-SYSTEM-2: the score assigned to a document is assigned to all the overlapping shots. A slightly higher score given to the later shots than to the first ones;</p><p>• BOOLEAN-SYSTEM-1: First, the boundaries of the video segment are determined by the coverage of the relevant words. Then the overlapping shots are scored the same way as with OKAPI-SYSTEM-2.</p><p>The video segment-to-shot mapping is critical to overall SDR performance. Post-evaluation experiments show the schemes above were not optimal choices; for example, since multiple relevant shots often overlap a single document, OKAPI-SYSTEM-1 performance can be improved simply by assigning a document score to all overlapping shots. Our current research is investigating more sophisticated schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion of multiple SDR systems:</head><p>Analysis of the results from the different systems shows that they are often complementary on FTR+FV: no system consistently outperforms the others. Thus we hypothesized fusion of scores might lead to improved overall performance. Whilst various fusion schemes are possible, for TREC-2002 we use a simple additive weighted scheme to combine shot-level, zero-to-one range normalized scores from each of our basic SDR systems. Weights can be optimized on FTR+FV prior to the final run on (held-out) search test data. This combined system is termed "SDR-FUSION-SYSTEM".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Term vector search</head><p>We used term vectors constructed from the ASR text for allowing similarity search based on textual content. Given the entire collection of shots, we obtained a list of all of the distinct terms that appear in the ASR for the collection. The order of this list was fixed to give a one-to-one mapping of distinct terms and dimensions of the vector space. Each shot was then represented by an n-dimensional vector, where the value at each dimension represented the frequency of the corresponding term in each shot. This allows the comparison of two shots based on frequency of terms. We constructed several term vector representations based on ASRtext.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Browsing and navigation</head><p>The system provides several methods for browsing and navigation. For each video a story-board overview image was generated that allowed its content to be viewed at a glance. The system also generated these overview images for each cluster (see Section 2.4) and each model (see Section 2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Iterative fusion</head><p>The interactive fusion methods provide a way for combining and rescoring results lists through successive search operations using different combination methods and aggregation functions defined as follows:</p><p>Combination methods Consider results list R k for query k and results list Q r for current user-issued search, then the combination function R i+1 = F c (R i , Q r ) combines the results lists by performing set operations on list membership. We explored the following combination methods:</p><p>• Intersection: retains only those items present in both results lists.</p><formula xml:id="formula_1" coords="6,414.37,231.92,139.83,9.81">R i+1 = R i ∩ Q r (2)</formula><p>• Union: retains items present in either results list.</p><formula xml:id="formula_2" coords="6,414.37,261.79,139.83,9.56">Ri+1 = Ri ∪ Qr (3)</formula><p>Aggregation functions Consider scored results list R k for query k, where D k (n) gives the score of item with id = n and Q d (n) the scored result for each item n in the current user-issued search, then the aggregation function re-scores the items using the function</p><formula xml:id="formula_3" coords="6,334.67,326.33,119.02,9.87">D i+1 (n) = F a (D i (n), Q d (n)).</formula><p>We explored the following aggregation functions:</p><p>• Average: takes the average of scores of prior results list and current user-search. Provides "and" semantics. This can be useful for searches such as "retrieve items that are indoors and contain faces."</p><formula xml:id="formula_4" coords="6,386.97,394.32,167.22,21.29">D i+1 (n) = 1 2 (D i (n) + Q d (n))<label>(4)</label></formula><p>• Minimum: retains lowest score from prior results list and current user-issued search. Provides "or" semantics. This can be useful in searches such as "retrieve items that are outdoors or have music."</p><formula xml:id="formula_5" coords="6,386.37,466.94,167.82,9.87">D i+1 (n) = min(D i (n), Q d (n))<label>(5)</label></formula><p>• Maximum: retains highest score from prior results list and current user-issued search.</p><formula xml:id="formula_6" coords="6,385.47,510.16,168.73,9.88">D i+1 (n) = max(D i (n), Q d (n))<label>(6)</label></formula><p>• Sum: takes the sum of scores of prior results list and current user-search. Provides "and" semantics.</p><formula xml:id="formula_7" coords="6,394.05,553.38,160.14,9.87">Di+1(n) = Di(n) + Q d (n)<label>(7)</label></formula><p>• Product: takes the product of scores of prior results list and current user-search. Provides "and" semantics and better favors those matches that have low scores compared to "average".</p><formula xml:id="formula_8" coords="6,394.05,612.04,160.14,9.87">Di+1(n) = Di(n) × Q d (n) (8)</formula><p>• A: retains scores from prior results list. This can be useful in conjunction with "intersection" to prune a results list, as in searches such as "retrieve matches of beach scenes but retain only those showing faces."</p><formula xml:id="formula_9" coords="6,411.85,673.13,142.35,9.56">Di+1(n) = Di(n) (9)</formula><p>• B: retains scores from current user-issued search. This can be useful in searches similar to those above but exchanges the arguments.</p><formula xml:id="formula_10" coords="6,411.43,721.38,142.76,9.87">D i+1 (n) = Q d (n) (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Normalization</head><p>The normalization methods provide a user with controls to manipulate the scores of a results list. Given a score D k (n) for each item with id = n in results set k, the normalization methods produce the score Di+1(n) = Fz(Di(n)) for each item n as follows:</p><p>• Invert: Re-ranks the results list from bottom to top. Provides "not" semantics. This can be useful for searches such as "retrieve matches that are not cityscapes."</p><formula xml:id="formula_11" coords="7,157.83,188.21,150.28,9.81">Di+1(n) = 1 -D i (n)<label>(11)</label></formula><p>• Studentize: Normalizes the scores around the mean and standard deviation. This can be useful before combining results lists.</p><formula xml:id="formula_12" coords="7,153.30,238.05,151.08,21.77">D i+1 (n) = Di(n) -µi σ i , (<label>12</label></formula><formula xml:id="formula_13" coords="7,304.38,245.42,3.73,8.07">)</formula><p>where µi gives the mean and σi the standard deviation, respectively, over the scores D i (n) for results list i. • Range normalize: Normalizes the scores within the range 0 . . . 1.</p><formula xml:id="formula_14" coords="7,123.77,313.98,184.34,21.77">D i+1 (n) = Di(n) -min(Di(n)) max(D i (n)) -min(D i (n))<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Shot expansion</head><p>The shot expansion methods allow the user to expand a results list to include for each shot its temporally adjacent neighbors. This can be useful in growing the matched shots to include a larger context surrounding the shots, as in searches such as "retrieve shots that surround those specific shots that depict beach scenes."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Multi-example search</head><p>Multi-example search allows the user to provide or select multiple examples from a results list and issue a query that is executed as a sequence of independent searches using each of the selected items.</p><p>The user can also select a descriptor for matching and an aggregation function for combining and re-scoring the results from the multiple searches. Consider for each search k of K independent searches the scored result S k (n) for each item n, then the final scored result Q d (n) for each item with id = n is obtained using a choice of the following fusion functions:</p><p>• Average: Provides "and" semantics. This can be useful in searches such as "retrieve matches similar to item "A" and item "B".</p><formula xml:id="formula_15" coords="7,153.28,577.84,154.83,26.32">Q d (n) = 1 K k (S k (n))<label>(14)</label></formula><p>• Minimum: Provides "or" semantics. This can be useful in searches such as "retrieve items that are similar to item "A" or item "B".</p><formula xml:id="formula_16" coords="7,158.98,640.91,149.13,14.80">Q d (n) = min k (S k (n))<label>(15)</label></formula><p>• Maximum:</p><formula xml:id="formula_17" coords="7,158.08,671.65,150.03,14.80">Q d (n) = max k (S k (n))<label>(16)</label></formula><p>• Sum: Provides "and" semantics.</p><formula xml:id="formula_18" coords="7,159.46,710.91,148.65,20.52">Q d (n) = k (S k (n))<label>(17)</label></formula><p>• Product: Provides "and" semantics and better favors those items that have low scoring matches compared to "average".</p><formula xml:id="formula_19" coords="7,406.37,119.53,147.81,20.52">Q d (n) = k (S k (n))<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">Relevance feedback search</head><p>Relevance feedback based search techniques enhance interactive search and browsing. The user's feedback on a set of shots is used to refine the search and retrieve in minimum number of iterations the desired matches. The user implicitly provides information about the matches being sought or query concept by marking whether shots are relevant or non-relevant in relation to his/her desired search output. The system utilizes this feedback to learn and refine an approximation to the user's query concept and retrieve more relevant video-clips in the next iteration. We use a robust relevance feedback algorithm <ref type="bibr" coords="7,495.20,270.15,14.93,8.07" target="#b19">[20]</ref> that utilizes non-relevant video-clips to optimally delineate the relevant region from the non-relevant one, thereby ensuring that the relevant region does not contain any non-relevant video-clips. A similarity metric estimated using the relevant video-clips is then used to rank and retrieve database video-clips in the relevant region. The partitioning of the feature space is achieved by using a piecewise linear decision surface that separates the relevant and non-relevant videoclips. Each of the hyper-planes constituting the decision surface is normal to the minimum distance vector from a non-relevant point to the convex hull of the relevant points. With query concepts that can reasonably be captured using an ellipsoid in the feature space. The proposed algorithm gives a significant improvement in precision as compared to simple re-weighting and SVM-based relevance feedback algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tasks and results</head><p>We participated four tasks: shot boundary detection (SBD), concept detection, concept exchange, and search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shot boundary detection (SBD) results</head><p>For the shot boundary detection task, the results of five systems were submitted, one of which was last year's SBD system as a baseline. A large difference in performance relative to last year was anticipated due to the degraded video quality of the TREC '02 data. The other four were different versions of the improved system, mainly applying different logic to the fusion of color histogram and the localized edges histogram information. Three of them performed well and yielded very similar results, while the forth one did not perform as well. Table <ref type="table" coords="7,463.28,610.09,4.48,8.07" target="#tab_1">1</ref> summarizes the evaluation of the baseline system, alm1, and the best new system, sys47, on last year's and this year's TREC video data test sets. The results on TREC-01 data set were computed by us, while the results for the TREC-02 data set are taken from the official NIST TREC 2002 evaluation of those systems. Two additional rows are provided on TREC-02 benchmark that compare our results to the best and average systems, respectively, among the 54 SBD runs submitted by TREC participants.</p><p>As anticipated, the SBD performance on TREC-02 data was lower than on TREC-01 data set. This was very noticeable in other participating systems as well. Never-the-less, the error rates of the new system sys47 were 20-36% lower than of the baseline system alm1 in almost all measures on both data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Concept detection results</head><p>Overall, concept detection results were submitted for ten concept classes. The evaluation results are plotted in Figure <ref type="figure" coords="8,275.02,336.57,3.36,8.07" target="#fig_4">4</ref>, which shows shows Average Precision measured at a fixed number of documents (1000 for the feature test set). The "Average" bars correspond to the performance averaged across all participants. The "Best" bars correspond to the system returning the highest Average Precision. The "IBM" bars correspond to IBM's submitted concept detection run (priority=1). The IBM system performed relatively well on the concept detection task giving highest Average Precision on 6 of the 10 concepts <ref type="foot" coords="8,205.73,418.08,2.99,5.38" target="#foot_6">7</ref> .  </p><formula xml:id="formula_20" coords="8,78.21,443.89,214.57,173.80">¢¡ £ ¢¡ ¤ ¢¡ ¥ ¦ ¢¡ § £ ¢¡ ¦ ¢¡ © £ ¢¡ ¦ ¢¡ £ ¢¡ £ £ ! " ¢# $ % &amp; % ' ( % ) 0 1 $ # ' % 2 # ! $ # ' % 3 % 4 65 7 % ( # 1 8 ' % % $ 9 ! @ % ! A ! ( B % C ED GF IH ¢P RQ ¦S T U V W X Y U `V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Concept exchange results</head><p>Apart from running the primary and secondary detectors on the search test set to assist the search task, we participated in the concept exchange task by submitting results of eight primary detectors on the search test set. We generated shot based MPEG-7 descriptions for this exercise thus permitting easy exchange of the detection results between participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Search results</head><p>The search task required retrieving video shots from the search test collection for a given set of query topics. We investigated both manual and interactive methods of searching. We submitted four runs of all 25 query topics using the content-based, model-based, speech-based, and interactive search methods described above. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Manual CBR</head><p>The manual CBR run consisted of mapping the query topics into one or more content-based or model-based queries and fusing the results in a predetermined fashion. As described in Section 2.2, CBR was based on a variety of descriptors. The manual CBR run was generated by allowing the following operations to answer each query topic:</p><p>1. Issue a content-based search by selecting one or more query examples, a feature type, and a fusion method, as necessary;</p><p>2. Issue a model-based search by selecting one or more concept models, a fusion method, and model weights, as necessary;</p><p>3. Fuse results lists from one or more content-based or modelbased search by selecting a fusion method.</p><p>For example, the following sequence of operations was executed for Query 79: People spending leisurely time at the beach:</p><p>1. Pick examples 0, 1, 4, 8, 12, 23, 29 from query content set 2. Perform CBR search with edge histogram layout using "minimum" fusion (Eq 15)</p><p>3. Combine with "Landscape" model using "intersection" combining method (Eq 2) and "product" aggregation function (Eq 8).</p><p>The exact mapping of query topics into a fixed sequence of the above operations was performed manually by visually optimizing performance over the FTR and/or FV collections without knowledge of the search test collection. Once a query topic was mapped to system operations, the operations were applied to the search collection by a designated person who did not participate in the mapping process or have prior knowledge of the search test collection. Figure <ref type="figure" coords="8,374.54,714.70,4.48,8.07" target="#fig_5">5</ref> shows the results for topic 76, which is looking for shots depicting "James Chandler." As shown, some matches are found in the results list, however, many shots of "James Chandler" are not retrieved using CBR.</p><p>With to performance, it was our experience that the TREC 2002 query topics were at a higher semantic level than what CBR can handle. While CBR and semantic modeling are generally able to capture low-to mid-level semantics, they are fairly limited in the case of only a few query examples or mid-to high-level semantics. We found that purely CBR worked best for refining candidate lists generated from semantically rich sources, such as speech, or explicit semantic models that closely match the query need. For example, refining the face model by cross-comparison with examples images of "James Chandler" did produce a few relevant hits near the top (see Figure <ref type="figure" coords="9,193.51,488.73,3.24,8.07" target="#fig_5">5</ref>). Model-based retrieval on the other hand worked well when the query topic was a close match to an existing model and was built with sufficient training data, such as the "musician" topic. However, in the case of limited example content, such as of query topic looking for "butterflies", or given a lack of closely related explicit semantic models, CBR and MBR techniques alone are not sufficient. In addition, some of the query topics were so general (e.g., beach query) or specific (e.g., Price Tower query) that it is doubtful whether any reasonable discrimination can be done using low-level features alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Manual SDR</head><p>Manual searching using spoken document retrieval (SDR) was based on the indexed speech information. We explored multiple methods of SDR and their fusion, where the SDR queries were developed through interaction with the Feature Training collection.</p><p>Query strings were created manually for each query. Queries derived from the audio and textual statement of information need supplied by NIST were expanded by hand in ad-hoc fashion based on retrieval on the FTR+FV sets <ref type="foot" coords="9,186.35,695.53,2.99,5.38" target="#foot_7">8</ref> . More complicated query strings are used in the Boolean system, since it was hypothesized that the Boolean retrieval would be less susceptible to the effects of query over-tuning on FTR+FV.</p><p>The query terms used in the submitted multiple-SDR fusion system for topic 90 ("Find shots with one or more snow-covered mountain peaks or ridges. Some sky must be visible behind them") were "ice snow covered mountain peaks valley vista". Twenty relevant items were retrieved in the top 100, with Average Precision 0.12. For topic 84 ("Find shots of Price Tower, designed by Frank Lloyd Wright and built in Bartlesville, Oklahoma") the query terms are "Price Tower Frank Lloyd Wright Bartlesville Oklahoma", the top three items recalled are relevant and Average Precision is 0.75.</p><p>Weights for the SDR-FUSION-SYSTEM were optimized using the limited ground truth that was compiled for FTR+FV. As expected, this scheme led to Mean Average Precision (MAP) improvements FTR+FV; more importantly, fusion gave performance improvements (35%) over our best single SDR system on the unseen search test data (as shown in Table <ref type="table" coords="9,462.28,275.32,3.24,8.07">3</ref>). Note that simple postevaluation changes in the video segment-to-shot mapping scheme improved the performance of the individual OKAPI systems (eg. OKAPI-SYSTEM-1 increased to MAP 0.114) and the fusion system performance might be expected to improve further as the component systems improve. The results overall are a significant improvement over those for IBM's speech-only retrieval submission to TREC-2001. The system was ranked second among 27 evaluated manual search results.  <ref type="table" coords="9,339.71,443.40,3.49,8.07">3</ref>: Search test performance of the fusion system and its three components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Manual CBR and SDR</head><p>The combination of CBR and SDR was explored for manual searching, where queries were developed through interaction with the Feature Training collection. An example of (successful) SDR and CBR integration is query topic 86 ("find overhead views of cities -downtown and suburbs; the viewpoint should be higher than the highest building visible"). In the following, we assume that the SDR results and CBR results have been found independently prior to the integrated query:</p><p>1. Retrieve results for SDR query of "view panorama overhead downtown suburbs city town urban"</p><p>2. Expand results list to include adjacent shots (repeat two times) using expand operation (see Section 3.8)</p><p>3. Combine with CBR results using "union" combination method (Eq 3) and "product" aggregation function (Eq 8).</p><p>The final Average Precision improved from CBR 0.0 and SDR 0.039 to CBR+SDR 0.057. A similar approach was used for the other queries with minor differences such as the number of shot expansions and the choice of the combination method and aggregation function, for example, using "intersection" rather than "union" and "sum" rather than "product". However, this approach was not always successful; for example, the same scheme was used for topic 84 ("Price Tower") SDR+CBR but performance was degraded below that obtained using SDR alone. approach to SDR and CBR integration improved 4 of the 25 queries beyond the performance attained with SDR alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Interactive search</head><p>We explored interactive search using CBR and SDR in which the user interacted with the search test collection at query-time, we chose various combinations of these methods and selected among different methods for fusion, multiple examples search, relevance feedback, and browsing. The wall-clock time was measured to gauge the user effort for each interactive query. The following describes the interactive search operations for query topic 89 for "Butterflies", which took just over seven minutes of user time:</p><p>1. Search for shots of butterflies using SDR with terms such as "monarch", "butterfly", "wings", "flower".</p><p>2. View grouping of results by video (clusters shots according to source video) to get idea of which videos contribute which shots 3. Remove two irrelevant shots at top of results list 4. Expand all shots t adjacent shots 5. Results show 5 hits at the top, stop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>We presented the IBM Research video indexing system. The system explores fully-automatic content analysis methods for shot detection, multi-modal feature extraction, statistical modeling for semantic concept detection, and speech recognition and indexing.</p><p>The system supports manual methods of querying based on automatically extracted features, models, and speech information. In this paper we described the system and the experiments runs that are part of the TREC-2002 video retrieval benchmarking effort.</p><p>The results show good performance on tasks such as shot boundary detection, concept detection, and search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,338.30,711.19,195.57,8.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Summary of video content indexing process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,72.00,588.52,236.01,8.07;2,72.00,598.94,236.01,8.07;2,72.00,609.35,236.02,8.07;2,72.00,619.76,16.68,8.07"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Plot of frame-to-frame processing of the SBD algorithm.Notice the ground truth (GT) and system output (Sys) plots for this segment of video which has six dissolves (one missed) and twelve cuts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,318.08,266.39,236.01,8.07;3,318.08,276.81,236.01,8.07;3,318.08,287.22,51.39,8.07;3,319.14,83.71,234.00,168.48"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: VideoAnnEx MPEG-7 video annotation tool. The system enables semi-automatic annotation of video shots and editing of the lexicon.</figDesc><graphic coords="3,319.14,83.71,234.00,168.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,72.00,638.40,236.02,8.07;8,72.00,648.81,57.23,8.07"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of concept detection performance using Average Precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,105.09,331.19,169.87,8.07;9,82.60,84.32,214.98,232.49"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results for topic 76: James Chandler.</figDesc><graphic coords="9,82.60,84.32,214.98,232.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,84.91,236.07,145.34"><head>Table 1 :</head><label>1</label><figDesc>Shot boundary detection results, comparing the new system with last year system on both TREC-01 and TREC-02 video data test sets. If all participating systems are to be ranked by P r All + Rc All then system S-5 would be found the best one, provided here for comparison. System mean reflects the average of all 54 submitted systems.</figDesc><table coords="8,77.98,84.91,225.81,72.90"><row><cell></cell><cell>Video</cell><cell>All</cell><cell cols="2">Cuts</cell><cell cols="2">Gradual</cell><cell cols="2">Frame</cell></row><row><cell>Sys.</cell><cell>Data</cell><cell>Pr</cell><cell>Rc</cell><cell>Pr</cell><cell>Rc</cell><cell>Pr</cell><cell>Rc</cell><cell>Pr</cell></row><row><cell>alm1</cell><cell cols="3">TR-01 .95 .88 .98</cell><cell cols="5">.97 .87 .68 .59 .93</cell></row><row><cell cols="4">sys47 TR-01 .96 .92 .99</cell><cell cols="5">.98 .89 .79 .66 .90</cell></row><row><cell>alm1</cell><cell cols="3">TR-02 .86 .77 .93</cell><cell cols="5">.80 .69 .71 .48 .94</cell></row><row><cell cols="4">sys47 TR-02 .88 .83 .93</cell><cell cols="5">.87 .76 .72 .57 .89</cell></row><row><cell>S-5</cell><cell cols="3">TR-02 .84 .89 .91</cell><cell cols="5">.94 .76 .78 .62 .90</cell></row><row><cell>mean</cell><cell cols="3">TR-02 .76 .79 .86</cell><cell cols="5">.84 .53 .60 .55 .71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,318.08,253.75,225.21,93.10"><head>Table 2 :</head><label>2</label><figDesc>Table 2 summarizes the results for the four search runs. Summary of search results for four submitted runs.</figDesc><table coords="8,347.84,274.16,178.83,52.10"><row><cell>System</cell><cell>Type</cell><cell>Code</cell><cell>MAP</cell></row><row><cell>CBR</cell><cell>Manual</cell><cell>M B M 1</cell><cell>0.006</cell></row><row><cell>SDR</cell><cell>Manual</cell><cell cols="2">M B M-2 2 0.137</cell></row><row><cell cols="2">CBR+SDR Manual</cell><cell cols="2">M B M-3 3 0.093</cell></row><row><cell cols="3">CBR+SDR Interactive I B M-4 4</cell><cell>0.244</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,86.34,725.79,190.99,7.17"><p>Annotation tools are available at http://alphaworks.ibm.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,332.42,725.79,186.14,7.17"><p>We used SVMLight toolkit (http://svmlight.joachims.org/)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,332.42,716.80,221.83,7.17;4,318.08,725.79,57.12,7.17"><p>Note this set does not overlap with the set used in supervised acoustic model adaptation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,86.34,671.24,221.83,7.17;5,72.00,680.24,236.18,7.17;5,72.00,689.23,236.18,7.17;5,72.00,698.22,236.18,7.17;5,72.00,707.21,19.92,7.18"><p>Minor differences in document definition were used in constructing the different indexes, such as whether or not document boundaries are defined at long stretches of silence or music; experiments suggest these differences do not make a significant contribution to the differences in MAP across</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,93.92,707.21,27.24,7.17;5,82.86,715.35,225.31,8.63;5,72.00,725.79,228.30,7.17"><p>systems.<ref type="bibr" coords="5,82.86,715.35,2.99,5.38" target="#b4">5</ref> For this year's queries we found the phonetic index was of limited use: only two queries involved out-of-vocabulary words, which were names.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,86.34,707.80,221.83,7.17;6,72.00,716.80,236.18,7.17;6,72.00,725.79,219.19,7.17"><p>Whilst this procedure might be simplified by defining documents in a fashion more closely related to shot boundaries, our results to date have found this to be less successful than the approaches discussed above.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="8,86.34,689.83,221.83,7.17;8,72.00,698.81,236.18,7.17;8,72.00,707.80,236.18,7.17;8,72.00,716.80,236.18,7.17;8,72.00,725.79,129.84,7.17"><p>Top score is indicated only on five concepts. In our original submission to NIST, we mistakenly submitted the speech detection twice overwriting our instrument detection result. However, the actual Average Precision of our instrument sound detector was 0.686, which was reported through later communication with NIST.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="9,86.34,716.80,221.83,7.17;9,72.00,725.79,183.28,7.17"><p>Later experiments showed that, at least in OKAPI-SYSTEM-1, the gains due to the manual query expansion were negligible.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments: We thank <rs type="person">Prof. Chiou-Ting Hsu</rs>, <rs type="affiliation">National Tsing-Hua University, Hsinchu, Taiwan</rs> and her students for their assistance in annotating the feature training data sets.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,91.91,568.85,216.10,8.07;10,91.92,579.26,216.20,8.07;10,91.92,589.68,48.30,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,201.07,568.85,106.94,8.07;10,91.92,579.26,44.75,8.07">MPEG-7 multimedia description schemes</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Salembier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,144.11,579.26,160.40,8.07">IEEE Trans. Circuits Syst. for Video Technol</title>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.92,603.59,15.93,8.07;10,132.99,603.59,34.69,8.07;10,192.85,603.59,24.58,8.07;10,242.60,603.59,26.81,8.07;10,294.58,603.59,13.44,8.07;10,91.92,614.00,154.11,8.07" xml:id="b1">
	<monogr>
		<ptr target="http://www.almaden.ibm.com/cs/cuevideo/" />
		<title level="m" coord="10,91.92,603.59,15.93,8.07;10,132.99,603.59,34.69,8.07;10,192.85,603.59,24.58,8.07;10,242.60,603.59,26.81,8.07;10,294.58,603.59,10.08,8.07">IBM CueVideo Toolkit Version 2.1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.91,627.92,216.10,8.07;10,91.92,638.32,216.11,8.07;10,91.92,648.73,216.11,8.07;10,91.92,659.14,216.19,8.07;10,91.92,669.56,216.15,8.07;10,91.92,679.97,63.48,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,277.30,638.32,30.73,8.07;10,91.92,648.73,212.95,8.07">Integrating features, models, and semantics for trec video retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ponceleon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,270.90,659.14,37.21,8.07;10,91.92,669.56,103.37,8.07">Proc. Text Retrieval Conference (TREC)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>Text Retrieval Conference (TREC)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="240" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.91,693.88,216.11,8.07;10,91.92,704.29,216.11,8.07;10,91.92,714.70,216.11,8.07;10,91.92,725.11,167.70,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,91.92,704.29,212.18,8.07">What is in that video anyway? in search of better browsing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ponceleon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.00,714.70,198.03,8.07;10,91.92,725.11,26.80,8.07">Proc. of IEEE Intl. Conf. on Multimedia Computing and Systems</title>
		<meeting>of IEEE Intl. Conf. on Multimedia Computing and Systems<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="388" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,87.50,216.10,8.07;10,337.99,97.92,216.15,8.07;10,337.99,108.32,185.72,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,434.77,87.50,119.32,8.07;10,337.99,97.92,92.69,8.07">Feature and spatial normalization for content-based retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,445.99,97.92,108.15,8.07;10,337.99,108.32,45.93,8.07">IEEE Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>Laussane, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,122.59,216.10,8.07;10,337.99,133.01,216.15,8.07;10,337.99,143.42,165.84,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,389.10,122.59,164.99,8.07;10,337.99,133.01,23.30,8.07">Content-based access of image and video libraries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,437.24,133.01,116.90,8.07;10,337.99,143.42,63.53,8.07">Encyclopedia of Library and Information Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Kent</surname></persName>
		</editor>
		<imprint>
			<publisher>Marcel Dekker, Inc</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,157.69,216.10,8.07;10,337.99,168.11,216.15,8.07;10,337.99,178.52,181.83,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,537.66,157.69,16.43,8.07;10,337.99,168.11,124.03,8.07">Spatial color indexing and applications</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,468.26,168.11,85.89,8.07;10,337.99,178.52,59.11,8.07">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="268" />
			<date type="published" when="1999-12">December 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,192.79,216.10,8.07;10,337.99,203.20,216.15,8.07;10,337.99,213.61,128.47,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,492.59,192.79,61.50,8.07;10,337.99,203.20,122.86,8.07">Textural features corresponding to visual perception</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,467.84,203.20,86.31,8.07;10,337.99,213.61,50.28,8.07">IEEE Trans. Syst., Man, Cybern., SMC</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,227.88,216.20,8.07;10,337.99,238.30,150.49,8.07" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,474.79,227.88,54.90,8.07">Machine Vision</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schunck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MIT Press and McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,252.57,216.10,8.07;10,337.99,262.98,216.10,8.07;10,337.99,273.40,216.13,8.07;10,337.99,283.81,108.02,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,534.17,252.57,19.91,8.07;10,337.99,262.98,216.10,8.07;10,337.99,273.40,18.46,8.07">Modeling semnatic concepts to support query by keywords in video</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,374.75,273.40,179.37,8.07;10,337.99,283.81,10.28,8.07">IEEE International Confernce on Image Processing</title>
		<meeting><address><addrLine>Rochester, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">Sep 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,298.08,216.10,8.07;10,337.99,308.49,216.10,8.07;10,337.99,318.90,216.19,8.07;10,337.99,329.31,216.20,8.07;10,337.99,339.73,181.29,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,534.17,298.08,19.91,8.07;10,337.99,308.49,216.10,8.07;10,337.99,318.90,167.49,8.07">Probabilistic multimedia objects (multijects): A novel approach to indexing and retrieval in multimedia systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kristjansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,521.74,318.90,32.44,8.07;10,337.99,329.31,212.40,8.07">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,354.00,216.10,8.07;10,337.99,364.41,216.18,8.07;10,337.99,374.82,216.17,8.07;10,337.99,385.23,34.35,8.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,502.90,354.00,51.19,8.07;10,337.99,364.41,140.62,8.07">A factor graph framework for semantic video indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kozintsev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,486.16,364.41,68.02,8.07;10,337.99,374.82,163.33,8.07">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2002-01">Jan 2002</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="40" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,399.50,216.21,8.07;10,337.99,409.91,96.01,8.07" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="10,389.13,399.50,161.11,8.07">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,424.19,216.11,8.07;10,337.99,434.60,216.18,8.07;10,337.99,445.01,216.11,8.07;10,337.99,455.42,104.56,8.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,467.28,424.19,86.82,8.07;10,337.99,434.60,167.45,8.07">Audio-visual synchrony for detection of monologues in video archives</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,524.99,434.60,29.19,8.07;10,337.99,445.01,216.11,8.07">Proc. of the Intl. Conf. on Acoustics, Speech and Signal Processing</title>
		<meeting>of the Intl. Conf. on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003-04">April 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,469.70,216.11,8.07;10,337.99,480.10,216.10,8.07;10,337.99,490.52,216.17,8.07;10,337.99,500.93,157.83,8.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,463.50,469.70,90.59,8.07;10,337.99,480.10,198.09,8.07">Automatic text extraction from video for content-based annotation and retrieval</title>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dorai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,337.99,490.52,149.21,8.07">IEEE Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="618" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,515.20,216.10,8.07;10,337.99,525.62,216.17,8.07;10,337.99,536.03,216.10,8.07;10,337.99,546.43,216.20,8.07;10,337.99,556.85,102.81,8.07" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,337.99,525.62,138.76,8.07">Context-enhanced video understanding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,493.66,525.62,60.51,8.07;10,337.99,536.03,216.10,8.07;10,337.99,546.43,192.88,8.07">IS&amp;T/SPIE Symposium on Electronic Imaging: Science and Technology -Storage &amp; Retrieval for Image and Video Databases</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-01">2003. January 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,571.12,216.10,8.07;10,337.99,581.53,216.13,8.07;10,337.99,591.95,216.18,8.07;10,337.99,602.36,120.36,8.07" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,476.00,571.12,78.09,8.07;10,337.99,581.53,49.02,8.07">Advances in phonetic word spotting</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,406.70,581.53,147.43,8.07;10,337.99,591.95,188.72,8.07">Proc. of the 2001 ACM CIKM Int. Conference on Information and Knowledge Management</title>
		<meeting>of the 2001 ACM CIKM Int. Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001-11">November 2001</date>
			<biblScope unit="page" from="580" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,616.63,216.10,8.07;10,337.99,627.04,216.10,8.07;10,337.99,637.45,170.37,8.07" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,471.43,627.04,66.68,8.07">OKAPI at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sparck-Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,347.70,637.45,134.32,8.07">Proc. Third Text Retrieval Conference</title>
		<meeting>Third Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,651.72,216.10,8.07;10,337.99,662.14,216.18,8.07;10,337.99,672.55,129.01,8.07" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,409.35,651.72,144.73,8.07;10,337.99,662.14,157.29,8.07">Improving the Effectiveness of Informational Retrieval with Local Context Analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,501.79,662.14,52.39,8.07;10,337.99,672.55,102.77,8.07">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,337.98,686.82,216.10,8.07;10,337.99,697.24,216.10,8.07;10,337.99,707.64,216.18,8.07;10,337.99,718.05,108.02,8.07" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,493.83,686.82,60.26,8.07;10,337.99,697.24,200.41,8.07">Leveraging nonrelevant images to enhance image retrieval performance</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">V</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghosal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,337.99,707.64,175.29,8.07">Proc. ACM Intern. Conf. Multimedia (ACMMM)</title>
		<meeting>ACM Intern. Conf. Multimedia (ACMMM)<address><addrLine>Juan Les Pins, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-12">December 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
