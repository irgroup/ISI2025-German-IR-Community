<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.52,112.08,432.47,15.49;1,244.20,134.04,123.68,15.49">Building a Foundation System for Producing Short Answers to Factual Questions</title>
				<funder ref="#_3ngTjUX">
					<orgName type="full">ARDA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.88,170.75,98.44,10.76"><forename type="first">Sameer</forename><forename type="middle">S</forename><surname>Pradhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Spoken Language Research</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<postCode>80309</postCode>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.47,170.75,71.46,10.76;1,324.00,169.69,1.80,6.40"><forename type="first">Gabriel</forename><surname>Illouz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Currently at LIMSI-CNRS</orgName>
								<address>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.67,170.75,134.99,10.76;1,472.80,169.69,1.80,6.40"><forename type="first">Sasha</forename><forename type="middle">J</forename><surname>Blair-Goldensohn</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,79.44,186.71,131.65,10.76;1,211.20,185.65,1.80,6.40"><forename type="first">Andrew</forename><surname>Hazen Schlaikjer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.51,186.71,80.07,10.76"><forename type="first">Valerie</forename><surname>Krugler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Spoken Language Research</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<postCode>80309</postCode>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.63,186.71,73.96,10.76;1,385.68,185.65,1.80,6.40"><forename type="first">Elena</forename><surname>Filatova</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.63,186.71,86.33,10.76;1,482.04,185.65,1.80,6.40"><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Duboue</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,492.35,186.71,44.96,10.76;1,537.36,185.65,1.80,6.40"><forename type="first">Hong</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,96.84,202.67,116.75,10.76;1,213.72,201.61,1.80,6.40"><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.91,202.67,78.05,10.76"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Spoken Language Research</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<postCode>80309</postCode>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.23,202.67,130.65,10.76;1,443.04,201.61,1.80,6.40"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,452.87,202.67,67.13,10.76"><forename type="first">Wayne</forename><surname>Ward</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Spoken Language Research</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<postCode>80309</postCode>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,153.96,218.63,69.05,10.76"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Spoken Language Research</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<postCode>80309</postCode>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.03,218.63,116.05,10.76;1,349.20,217.57,1.80,6.40"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.52,218.63,86.31,10.76"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Spoken Language Research</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<postCode>80309</postCode>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.52,112.08,432.47,15.49;1,244.20,134.04,123.68,15.49">Building a Foundation System for Producing Short Answers to Factual Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55B886149024F6ACAB02E1362D0F5DC4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe question answering research being pursued as a joint project between Columbia University and the University of Colorado at Boulder as part of ARDA's AQUAINT program. As a foundation for targeting complex questions involving opinions, events, and paragraph-length answers, we recently built two systems for answering short factual questions. We submitted results from the two systems to TREC's Q&amp;A track, and the bulk of this paper describes the methods used in building each system and the results obtained. We conclude by discussing current work aiming at combining modules from the two systems in a unified, more accurate system and adding capabilities for producing complex answers in addition to short ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>such a short period. This gave us the added benefit that each site was able to focus more of their effort on specific components they were interested in, and avoided the need for developing complex protocols for communication between the modules across the sites.</p><p>The paper focuses on our development of our foundation question answering systems for TREC, processing factual questions only and producing short answers. Most of the remainder of the paper (Sections 2 and 3) discusses the two architectures we developed, ways that each departs from standard question answering methodology, and our results on this year's TREC questions. We submitted the results of these two architectures as runs 1 and 2 with the tag cuaq. We conclude with a discussion of our current integration effort and ongoing work on adding advanced components for handling additional question types to our foundation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System A -The Boulder System</head><p>The novel feature of our approach in System A is the use of shallow semantic representations to enhance potential answer identification. Most successful systems first identify a list of potential answer candidates using pure wordbased metrics. Syntactic and semantic information at varying granularity is then used to re-rank those candidates <ref type="bibr" coords="2,72.00,263.32,15.81,8.97" target="#b9">[10,</ref><ref type="bibr" coords="2,91.29,263.32,11.86,8.97" target="#b10">11]</ref>. However, most of these semantic units are quite specific in what they label. We identify a small set of thematic roles-viz., agent, patient, manner, degree, cause, result, location, temporal, force, goal, path, percept, proposition, source, state, and topic-in the candidate answer sentences, using a statistical classifier <ref type="bibr" coords="2,469.19,287.20,10.69,8.97" target="#b8">[9]</ref>. The classifier is trained on the FrameNet database <ref type="bibr" coords="2,217.40,299.20,10.60,8.97" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>The following sequence of actions will be taken in response to an input query:</p><p>1. Classify the question according to type by identifying the Named Entity and Thematic Role of the expected answer type. This also defines a set of answer type patterns, and includes named entity tagging and parsing the question for thematic roles. 2. Identify the focus, i.e., certain salient words or phrases in the question that are very likely to be present in the answer string in one form or the other. 3. Extract a set of query words from the question, and apply semantic expansion to them. <ref type="bibr" coords="2,84.48,442.84,3.77,8.97" target="#b3">4</ref>. Submit the query words to the mg (Managing Gigabytes) <ref type="bibr" coords="2,334.89,442.84,16.66,8.97" target="#b14">[15]</ref> IR engine and get back a rank-ordered set of documents. 5. Keep the top M (approximately 500) documents and prune the rest. <ref type="bibr" coords="2,84.48,481.12,3.77,8.97" target="#b5">6</ref>. Segment documents into paragraphs and prune all but the top N paragraphs (approximately 2,500). 7. Generate scoring features for the paragraphs, including named entity tagging and parsing of paragraphs to add thematic roles. 8. Re-rank documents based on the set of features that we compute, including answer type patterns. Some of the answer type patterns are based on the semantic labels. 9. Compute for each paragraph a confidence measure that it contains some relevant information. This includes N -Best count as one of the features. 10. Send tagged paragraphs that exceed a confidence threshold for extraction of the short answer required for TREC.</p><p>For the problem of question answering, we are more concerned with precision than recall, so we have to be careful in expanding the query words to get answers that are expressed in words quite different from the ones mentioned in the question. Semantic expansion will be performed when the system's confidence in the best candidate answer string without expansion is found to be below a certain threshold. Our mechanism for expansion is: a. Submit original query words to IR engine and get back a rank-ordered set of documents. b. Generate set of target words from top k documents based on the idf values of the words. We are experimenting with k in the range of 1-100. c. Generate a set of target words from WordNet <ref type="bibr" coords="3,278.79,98.80,11.63,8.97" target="#b7">[8]</ref> synsets of original keywords. d. Take the intersection of the two sets and add to the keyword set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features</head><p>Answer Identification We now discuss the features used for ranking the documents. The features are roughly ordered by decreasing salience.</p><p>Answer type -In order to extract the answer to a question, the system needs to identify the expected answer type. Answers for short answer questions generally can be categorized as named entities and/or propositions. Summary information is often required for descriptive and definition questions. The system classifies the answer type by two features: 1) named entity class and 2) thematic role. Named entity class specifies one (or more) of 56 classes as the named entity class of the answer. We use 54 real named entity classes, one class representing the case where the answer is expected to be a named entity but one not known to the system, and one class for cases where the answer is not expected to be a named entity. The thematic role class identifies the thematic role in which the potential answer would tend to be found in the answering sentence.</p><p>Answer surface patterns -Once the question type is known the next step is to identify candidate answers. One technique we use is based on generating a set of expected surface patterns for the answer. Sentences or snippets matching these patterns would get better scores than ones that did not. The patterns specify word and named entity-based regular expressions that are derived from a large corpus annotated with named entities; (simplified) examples include:</p><p>1. Some common question types, e.g., [&lt;PERSON DESCRIPTION&gt;&lt;PERSON NAME&gt;] for questions like "Who is &lt;PERSON NAME&gt;?"; [&lt;ORGANIZATION&gt;, &lt;CITY&gt;, &lt;STATE&gt;, &lt;COUNTRY&gt;] for questions asking about the location or address of an organization. 2. Likely re-phrasings of the question, e.g., [&lt;PERSON&gt; invented &lt;PRODUCT&gt;] for questions like "Who was the inventor of &lt;PRODUCT&gt;?" 3. Occurrence statistics of the pattern in the corpus, e.g., [&lt;PERSON&gt; (&lt;YEAR&gt;-&lt;YEAR&gt;)] for birth dates of people.</p><p>Named entities in answer -In the case of questions that expect a specific named entity (including the unknown named entity type) as the answer, candidates that do not contain that named entity are penalized. In the case that the answer is expected to be an unknown named entity, then candidates that contain an untagged sequence of capitalized words (a strong indicator of unknown named entities) are preferred.</p><p>Presence of focus word -The presence of the focus word is an important feature for the overall score. For our purposes, a focus word is a word in the question that, or its synonym, is very likely to appear in the sentence that contains the answer.</p><p>Thematic role patterns -While surface patterns for answers can provide valuable information when a match is found, the specific nature of the patterns and the limited occurrences of the answer string within the reformulations obtainable from the question do not always guarantee a surface pattern match. We also provide a more general set of expected answer patterns based on thematic roles. We expect that these patterns will have higher coverage than the more specific surface patterns. This feature scores sentences based on the presence of expected thematic roles and named entities existing in specific thematic roles.</p><p>Thematic role patterns help identify false positive answer candidates and help extract the exact answer boundary from the string This can be illustrated with the example in Figure <ref type="figure" coords="3,358.76,641.20,5.03,8.97" target="#fig_0">1</ref> Question This is one of possibly more than one patterns that will be applied to the answer candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>False Positives:</head><p>Note: The sentence number indicates the final rank of that sentence in the returns, without using thematic role patterns.  All the named entities, but only roles pertaining to the target predicate are marked in the sentences.</p><p>Okapi scoring -This is the Okapi BM25 <ref type="bibr" coords="4,266.17,517.00,21.04,8.97" target="#b13">[14]</ref> score assigned by the information retrieval engine to the paragraph extracted in the very beginning.</p><p>N-gram -Another feature that we use is based on the length of the longest n-gram (sequence of contiguous words) in the candidate answer sentences after removing the stopwords from both the question and the answer.</p><p>Case match -Documents with words in the same case as in the question tend to be more relevant than those that have different case, so the former are given a relatively higher score. This is because capitalization helps disambiguate named entities from common words (at least in carefully written queries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Annotation</head><p>Once the likely answer candidates (paragraphs or sentences) are extracted for a question we need to estimate the likelihood of those being good answers. To do this, we have a scheme that annotates each with # wrong (W) # unsupported (U) # inexact (X) # right (R) CW Score NIL Precision NIL Recall 411 7 3 79 0.226 0 / 9 = 0.000 0/46 = 0.00 Table <ref type="table" coords="5,96.33,109.48,3.90,8.97">1</ref>: System A (Boulder) results; numbers are out of 500 questions. some level of confidence. We use a weighted linear combination of N-Best answer count, Named Entity class of the answer, and the N-gram length features to calculate the degree of confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Current Implementation</head><p>In this section we discuss the state of the current implementation for system A, which was used to produce the first run submitted to the TREC 2002 question answering track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC-2002 Database</head><p>The text database comprises non-stemmed, case-folded indexing of the TREC/AQUAINT corpus using a modified version of the mg (Managing Gigabytes) search engine <ref type="bibr" coords="5,386.14,277.12,16.66,8.97" target="#b14">[15]</ref> that incorporates the Okapi BM25 ranking scheme <ref type="bibr" coords="5,137.91,289.12,16.54,8.97" target="#b13">[14]</ref> and an expanded set of characters forming an indexed unit-so as to accommodate hyphenated words, URLs, emails etc. Each indexed document is a collection of segmented sentences forming a paragraph in the original corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Identification</head><p>We currently use a rule-based question classifier that identifies the named entity type and thematic role of the expected answer to each question. For each query, the top N (2500, based on 85% recall threshold) ranked paragraphs are retrieved for processing. A set of documents are carried over from the list of documents retrieved by the IR engine, using gradually diluted boolean filters, until there are no more keywords to drop, or the cumulation of filtered documents exceeds a threshold of n (currently set to 10). We call this the boolean peel-off strategy. The answer named entity type is used to filter out documents that do not contain the required named entity type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Extraction</head><p>For questions in which the answer is a specific named entity or a thematic role, if the top ranking sentence contains only one instance of that element, that instance is returned as the answer. In many cases, however, there is more than one element of the predicted type. In such cases, the system selects the element with the shortest average distance from each of the query words present in that snippet. There are penalties added for some punctuation symbols like commas, semi-colons, hyphens etc. In cases when the required answer is not a known named entity, the answer extractor tries to find a sequence of capitalized words that could be a probable named entity. In case the expected answer is not a named entity, and the thematic role cannot be identified without much ambiguity, then the system tries to find an apposition near the question words, and extracts that as the answer. An example would be definition questions, of the style "What is X?". Failing to get any of the above, the system replies that it does not have an answer to that particular question in the given corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Annotation</head><p>The prior probability of correct answers for a particular question type, along with the nbest count, are used to assign a confidence measure. The system then additionally tests whether the candidate at rank two has more counts in the top ten candidates than the candidate at rank one. If so, it is promoted to rank one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>The results of System A on the TREC-2002 test set are presented in Table <ref type="table" coords="5,374.45,617.32,3.77,8.97">1</ref>. These results are consistent with what we expected based on our TREC 9 and 10 development test set. Note that we answered "no answer" or NIL only if no answer was returned by our standard algorithm, which was rarely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System B -The Columbia System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of System Operation</head><p>Our second foundation system, developed at Columbia University, uses a hybrid architecture that evolved from an initial version that relied solely on the web as a source of answers. We focused on query expansion and candidateanswer scoring in order to perform search in parallel over open (the web) and closed (TREC) collections and then combine the results. For example, as described in more detail in Section 3.5, we experimented with a strategy that interleaves information learned from the web search to re-query the TREC document set.</p><p>In our initial, web-based system we adopted two working assumptions: First, given the size and redundancy of the web, a database of paraphrase patterns where more specific patterns are prioritized would be both necessary and effective for finding relevant documents (i.e., we aimed at precision rather than recall). Second, scores for candidate answers would be a composite function of attributes of the query formulation process, and of the space of candidate answers. For example, more specific queries have higher default scores, and an answer string retrieved from multiple documents is weighted higher than an answer string that appears in relatively few documents. Finally, we assumed that the question of how best to capitalize on these two assumptions would be primarily an empirical one. Thus questions that are superficially similar might require rather distinct queries. For example, questions that contain a noun denoting a leadership position, as in "Who was the first commander of FLEET X?" and "Who was the first coach of TEAM Y?" might both benefit from query expansion rules in which the related verbs appear (e.g., "&lt;TERM&gt; commanded FLEET X" and "&lt;TERM&gt; coached TEAM Y"), but this would not be the case for the structurally similar question "Who was the first general of FORCE Z?".</p><p>The remainder of this section documents four key modules of System B's architecture. There is a pipeline of three modules responsible for distinct phases of the query expansion process: paraphrasing of patterns derived from the question string (Section 3.2 below); modification of queries, e.g., by inserting terms likely to occur with the answer ([1]; Section 3.3); and term prioritization (Section 3.4). At each phase of the query expansion process we modify according to the results of that phase scoring weights that guide how likely that version of the query is to lead to the correct answer. Finally, after a candidate answer set has been assembled, these weights are assembled into a single score (Section 3.5). If, as in the case of our second run submitted to TREC, candidate answer sets for a given question come from distinct document collections, the source collection is considered in ranking potential answers (Section 3.5).</p><p>We conclude this section with a brief discussion of the results obtained on the data and questions of the 2002 TREC Q&amp;A track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Paraphrasing</head><p>In order to generate queries providing high precision coverage of the answer space for a given question, custom rules were developed providing a mapping from a given question type to a set of paraphrasing patterns which would generate alternative queries. For example, the question string "Where is the Hudson River located?" may result in the generation of queries including "Hudson River is located in", "Hudson River is located", "Hudson River is in the", "Hudson River is near", and "Hudson River in". Since we often do not have specific information about the question target, our paraphrasing rules allow changes on articles (definite, indefinite, or no article), number, and function words to maximize our coverage of the collection.</p><p>A two-level scoring scheme was implemented for these queries whereby each was scored based on the specificity of its query string as well as that of the paraphrasing pattern that generated it. Specificity here is defined by the length of the query string (or the length of the shortest possible generated query string in the case of the paraphrasing patterns); longer queries typically return fewer results than shorter (more general) queries. These query scores are used to aid the scoring and subsequent ranking of the returned results along with other factors that rate the results themselves (see Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query Modification</head><p>Knowing the type of the question can be very helpful not only for defining the type of the answer but also for better targeting of the search of the documents which might contain a potential answer. This process can be viewed as connecting the answer and question spaces <ref type="bibr" coords="7,249.73,118.24,10.60,8.97" target="#b2">[3]</ref>. A question like "What is the length of the Amazon?" produces no useful results among the top 20 when submitted to Google, because among the two content words, "length" is not likely to appear directly in the answer, and "Amazon" is highly ambiguous (the river, the mythological female warrior, the online company, to name just the more common senses). "Length" is a very good question word, but needs to be mapped to corresponding answer words such as "miles" and "kilometers". If we expand the query by adding either km or mi, the first hits returned by Google are about the Amazon river and contain the answer.</p><p>In our TREC 2002 system we used query expansion only for questions that required answers consisting of a number plus a measurement unit. All the questions that required a number as an answer were categorized under the general type NUMBER by our question classifier. We built a further classifier that translated some of the question words to subtypes of NUMBER, namely DISTANCE, WEIGHT, SPEED, TEMPERATURE, MONEY, and OTHER. The classifier was constructed by training using RIPPER <ref type="bibr" coords="7,246.54,238.00,11.75,8.97" target="#b5">[6]</ref> to produce a set of rules. For each of the questions classified into the above subtypes, the classifier returns an automatically compiled list of words expressing the appropriate measurement units, which were added to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Query Prioritization</head><p>Query prioritization scores queries so that those with highest predicted answer precision (i.e., number of documents retrieved by a query that contain the correct answer(s) divided by the total number of documents retrieved) will be attempted first during document retrieval. This is because, first, our paraphrasing and modification mechanisms can generate thousands of queries and it is impractical to submit all of those to the search engine, and, second, because we can tell that some query rewriting mechanisms are more likely to be accurate than others.</p><p>For example, given the question "How many soldiers died in World War II?", our system will generate many queries, using the paraphrasing (Section 3.2) and modification (Section 3.3) rules mentioned above. The generated queries include "World War II" (q a ) and "soldiers" (q b ). We want query prioritization to assign q a a higher score since it clearly is more specific and relevant to this question, and thus is likely to have a higher predicted answer precision.</p><p>Unlike previous TREC systems <ref type="bibr" coords="7,217.06,418.96,10.90,8.97" target="#b6">[7,</ref><ref type="bibr" coords="7,230.96,418.96,11.86,8.97" target="#b11">12]</ref>, which mainly applied heuristics for query prioritization, we empirically built our query prioritization algorithm using statistics collected over previous TREC question-answer pairs. We analyzed the relation between a query term (see Sections 3.2 and 3.3 for query term generation) and its answer precision. We considered various features of a query term, including the term's syntactic role (e.g., noun phrase, verb phrase, and head noun), morphological features (e.g., upper case for proper noun), and inverse document frequency (IDF). We found that IDF consistently reflected answer precision.</p><p>The query scoring algorithm for non-paraphrase-based queries relies on two functions:</p><p>The term-scoring function T maps terms to term scores, where terms are strings of one or more words which are part of a query. The term score of a multiword term X is the product of the IDF values of the individual words forming the term. A minimum IDF value of 1.05 is used in this case to ensure that even common terms slightly boost the score of a multiword term. In addition, a suitably high value is used for words with unknown IDF values. We use IDF values computed over a large body of recent general news text.</p><p>The query-scoring function Q maps a set of one or more terms (a query) to a query score. The query score for query Y is the product of the term scores of the query's component terms; given the definitionof term scores above, this means that a query's score is the product of the IDF values for all words in all phrases of the query.</p><p>As mentioned above, the query scores produced are used in document retrieval, answer selection, and answer ordering (for listing first the answers to questions where our system has the greatest confidence). For the latter purpose, scores must be normalized. This is because answer ordering requires us to compare answer confidence across answers to different questions; since part of our confidence is determined by the score of the query leading to the answer (see Section 3.5), we must be able to make a meaningful comparison between query scores for queries produced for two different questions. Therefore, scores for each set of queries produced for a given question are normalized using division by the highest query score for a query generated for that question. Thus, the highest scoring query produced for any question will have a score of 1, while lower scoring queries will have scores between 0 and 1.</p><p>It is important to note that our system scores any paraphrase-based query above any keyword-based query, irrespective of the functions mentioned above. <ref type="foot" coords="8,245.76,158.08,3.48,6.28" target="#foot_0">1</ref> This is because the paraphrasing rules (Section 3.2) were hand-crafted and produced with an eye toward high precision. Thus, the scores produced by this module of the system were only used to differentiate between keyword-based queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Combining Evidence from Multiple Sources</head><p>Even when the answer must be found in a specific collection, as in the TREC evaluation, there are benefits in combining evidence from multiple collections: the answer can be found in a larger or broader collection, in which case the smaller collection can be searched again with that specific answer in mind; and, if no answer can be found in the smaller collection, the confidence in the answer from the larger collection can help determine whether "no answer present in the collection" (NIL) should be returned.</p><p>To this end, we have designed a general mechanism using wrappers that interface our system to different collections and/or search engines using a common API. We have implemented wrappers for the Google, Glimpse <ref type="bibr" coords="8,520.39,309.76,15.33,8.97" target="#b12">[13]</ref>, and mg <ref type="bibr" coords="8,104.13,321.64,16.54,8.97" target="#b14">[15]</ref> search engines, and for TREC-11 we ran system B using a combination of Google on the web (a broad collection) and mg on the TREC/AQUAINT collection (the collection where the answer must be found).</p><p>Our system returns a list of answers extracted from each source (search engine and collection combination), which may include the same string multiple times. Each instance of an answer has an associated confidence score, which depends on the query used to retrieve the answer (see Sections 3.2 and 3.4), the confidence score returned by the search engine (not implemented for our TREC experiments), and the confidence of the answer extractor in selecting an appropriate phrase from the returned document. These instance scores are combined for each potential answer (i.e., across all instances where the same string is returned) using the following formula for computing the cumulative score after n instances of the same string have been processed</p><formula xml:id="formula_0" coords="8,146.64,441.21,318.71,10.33">cumulative score n = 1 -[(1 -cumulative score n-1 ) × (1 -instance score n )]</formula><p>which ensures that answers occurring multiple times are weighted higher, taking into account the evidence for them each time they are found.</p><p>An answer may be found in one or more sources. We employ the following algorithm for calculating a composite score based on the cumulative scores of each string proposed as a potential answer, from the web or the TREC collection. The algorithm distinguishes the following cases:</p><p>No answers found in either collection. Since this probably represents a system failure (it is unlikely that a TREC question would not be answerable from both the web and the TREC collection), we return NIL with zero confidence.</p><p>One or more answers found in the TREC collection, but no answers found from the Web. We return the best answer extracted from the TREC collection but depreciate its confidence by a constant factor because the coverage of the web would make it unlikely that no answers at all could be found there while the TREC collection would be able to provide one.</p><p>No answers found in the TREC collection, but one or more answers found from the Web. This is our prima facie case for a NIL answer; however, since often we got our answer from the web using a modified query, we re-query the TREC collection using the answer identified from the web. If that step succeeds, we report the answer with a combined confidence as in the next case below; otherwise we report NIL, with confidence equal to our confidence in the web answer.</p><p>One or more answers found in each collection, and both top ranked answers are the same. We report that answer, with reinforced confidence according to the formula</p><formula xml:id="formula_1" coords="9,158.64,176.61,319.19,9.96">combined confidence = 1 -[(1 -confidence TREC) × (1 -confidence web)]</formula><p>One or more answers found in each collection, but the top ranked answers are different. We report the TREC answer, but reduce its confidence by the formula combined confidence = confidence TREC × (1 -confidence web)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Results</head><p>For the 500 questions in the TREC-11 question, System B produced 58 correct answers, 8 unsupported, 2 inexact, and 432 wrong. The system's unweighted precision is 11.6%, while its confidence-weighted score is 0.178, representing a significant boost over the unweighted precision. We produced a lot of NIL answers (195, or 39% of our total answers), which indicates that we were too conservative in failing to choose low-confidence answers found in the TREC collection. We did retrieve a third of the NIL answers present in the test set, but overall System B performed less well than System A which obtained a confidence-weighted score of 0.226 while producing very few NILs (1.8% of its total answers). On non-NIL answers, the two systems performance is closer (unweighted precision of 16.1% for system A and 14.1% for system B).</p><p>We attribute the lower performance of System B to two additional factors beyond the excessive production of NILs: First, we used a very simple extractor for selecting the answer out of the sentence where it was found, and the extractor failed to produce the correct phrase in a number of cases where we succeeded in finding the appropriate sentence. Second, our question classifier was not always successful in predicting the correct question type, producing no label for 77 (15.4%) of the questions. We performed significantly worse on those questions than in who, when, where, or what questions with well-extracted types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>We have described two systems that handle questions with short, factual answers. These systems were developed in a very brief time frame, and are our first entry in the TREC Q&amp;A track. They represent for us a starting point for an overall question answering architecture, to which we are adding additional capabilities. In the months since TREC, we have worked on developing detailed XML-based protocols for communication between modules in a common architecture. The system we are building by combining elements of the two systems discussed in this paper utilizes the best-performing components of each of the current systems. The modular architecture allows us to connect additional modules, and actual question answering is done in a distributed fashion, with most of question analysis done at Colorado and most of answer synthesis done at Columbia. Modules communicate with a central server via HTTP, while the architecture offers several communication interfaces at different levels of complexity (for example, one module may request only the high-level question type, while another may examine in detail the semantic parse). A web-based client provides a front end to the integrated system, allowing users in different locations to access the system.</p><p>Our research is moving towards questions with more complex answers, including opinions, events, definitions, and biographies. We recently completed a prototype module for processing definitional questions, and we are currently adding its functionality to our integrated system, so that for questions of the form "What is X?" we produce both a short, TREC-like, answer and an answer spanning several paragraphs. At the same time, we are continuing to tune individual components to enhance interoperability between the two sites; for example, we recently started re-focusing the semantic analysis (done at Colorado) on types of phrases that are likely to impact the processing of opinions and biographies (done at Columbia).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.00,464.80,467.68,8.97;4,72.00,476.80,394.69,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example where thematic role patterns help constraint the correct answer among competing candidates. All the named entities, but only roles pertaining to the target predicate are marked in the sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,78.36,85.96,417.29,100.39"><head>:</head><label></label><figDesc>Who assassinated President McKinley? Parse: [ role=agent Who] [ target assassinated] [ role=patient [ ne=person description President] [ ne=person</figDesc><table coords="4,78.36,111.28,329.26,62.37"><row><cell>McKinley]]?</cell></row><row><cell>Keywords: assassinated President McKinley</cell></row><row><cell>Answer named entity (ne) Type: Person</cell></row><row><cell>Answer thematic role (role) Type: Agent of target synonymous with "assassinated"</cell></row><row><cell>Thematic role pattern: [</cell></row></table><note coords="4,174.00,164.68,321.65,9.67;4,78.36,176.68,293.92,9.67"><p>role=agent [ ne=person ANSWER] ∧ [ target synonym of(assassinated)] ∧ [ role=patient [ ne=person reference to(President McKinley)]]</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="8,86.40,642.56,438.55,7.17"><p>Queries augmented with the query modification techniques discussed in Section 3.3 are considered keyword-based queries in this context.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work, as well as our broader research on question answering, is supported by <rs type="funder">ARDA</rs> AQUAINT contract <rs type="grantNumber">MDA908-02-C-0008</rs>. We would like to thank <rs type="person">Ralph Weischedel</rs> of <rs type="affiliation">BBN Systems and Technologies</rs> for giving us the named entity tagger (Identifinder), and <rs type="person">Daniel Gildea</rs> for the FrameNet parser.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3ngTjUX">
					<idno type="grant-number">MDA908-02-C-0008</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,93.59,258.64,445.99,8.97;10,93.60,270.64,412.20,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,291.05,258.64,248.53,8.97;10,93.60,270.64,79.93,8.97">Learning Search Engine Specific Query Transformations for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,195.83,270.64,238.53,8.97">Proc. of the 10th International World-Wide Web Conference</title>
		<meeting>of the 10th International World-Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.59,282.52,445.90,8.97;10,93.60,294.52,98.86,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,251.87,282.52,126.93,8.97">The Berkeley FrameNet Project</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,400.90,282.52,133.10,8.97">Proceedings of the COLING-ACL</title>
		<meeting>the COLING-ACL<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.59,306.40,446.01,8.97;10,93.60,318.40,446.00,8.97;10,93.60,330.40,310.27,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,358.58,306.40,181.01,8.97;10,93.60,318.40,112.01,8.97">Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,228.19,318.40,311.41,8.97;10,93.60,330.40,196.01,8.97">Proceedings of the 23th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 23th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.59,342.28,446.09,8.97;10,93.60,354.28,75.10,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,277.90,342.28,172.66,8.97">An Algorithm that Learns What&apos;s in a Name</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,463.22,342.28,72.11,8.97">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.59,366.28,445.77,8.97;10,93.60,378.16,175.18,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,186.92,366.28,260.40,8.97">The Anatomy of a Large-Scale Hypertextual Web Search Engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,459.05,366.28,80.31,8.97;10,93.60,378.16,71.73,8.97">Computer Networks and ISDN Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-7</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.59,390.16,446.13,8.97;10,93.60,402.04,109.30,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,157.80,390.16,118.10,8.97">Fast Effective Rule Induction</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,298.43,390.16,241.29,8.97;10,93.60,402.04,79.83,8.97">Proceedings of the Twelfth International Machine Learning Conference (ML-95)</title>
		<meeting>the Twelfth International Machine Learning Conference (ML-95)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.59,414.04,445.99,8.97;10,93.60,426.04,182.37,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,320.67,414.04,199.44,8.97">Web question answering: Is more always better?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,93.60,426.04,98.40,8.97">Proceedings of SIGIR-02</title>
		<meeting>SIGIR-02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,437.92,337.28,8.97" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,183.75,437.92,166.74,8.97">WordNet: An Electronic Lexical Database</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.59,449.92,446.21,8.97;10,93.60,461.92,175.93,8.97" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,207.33,449.92,151.65,8.97">Automatic Labeling of Semantic Roles</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>TR-01-005</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>International Computer Science Institute, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,93.58,473.80,446.11,8.97;10,93.60,485.80,446.00,8.97;10,93.60,497.68,161.51,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,251.24,473.80,288.45,8.97;10,93.60,485.80,71.22,8.97">Answering complex, list and context questions with LCC&apos;s Question-Answering Server</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,186.85,485.80,252.67,8.97">Proceedings of the Tenth Text REtrieval Conference (TREC-10)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC-10)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">November 13-16, 2001</date>
			<biblScope unit="page" from="355" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,509.68,445.74,8.97;10,93.60,521.68,387.56,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,214.94,509.68,183.84,8.97">The Use of External Knowledge of Factoid QA</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,422.21,509.68,117.11,8.97;10,93.60,521.68,130.76,8.97">Proceedings of the Tenth Text REtrieval Conference (TREC-10)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC-10)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">November 13-16, 2001</date>
			<biblScope unit="page" from="644" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,533.56,445.89,8.97;10,93.60,545.56,446.04,8.97;10,93.60,557.56,446.16,8.97;10,93.60,569.44,95.01,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,150.60,545.56,389.04,8.97;10,93.60,557.56,51.67,8.97">SiteQ: Engineering High Performance QA System Using Lexico-Semantic Pattern Matching and Shallow NLP</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">K</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<idno>TREC-10</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,171.53,557.56,213.85,8.97">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">November 13-16, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,581.44,445.90,8.97;10,93.60,593.32,143.33,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,195.81,581.44,215.25,8.97">Glimpse: A tool to search through entire file systems</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Manber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,434.24,581.44,105.25,8.97;10,93.60,593.32,80.60,8.97">Proceedings of the Winter USENIX Conference</title>
		<meeting>the Winter USENIX Conference</meeting>
		<imprint>
			<date type="published" when="1994-01">January 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,605.32,445.89,8.97;10,93.60,617.32,314.79,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,214.79,605.32,106.62,8.97">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,344.56,605.32,194.91,8.97;10,93.60,617.32,58.03,8.97">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">November 17-19, 1999</date>
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,629.20,446.07,8.97;10,93.60,641.20,230.23,8.97" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,238.93,629.20,291.76,8.97">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<publisher>Morgan Kaufmann Publishing</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
