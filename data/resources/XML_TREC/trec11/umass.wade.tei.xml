<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,224.22,87.44,163.65,14.88;1,168.18,105.86,275.57,14.88">UMass at TREC 2002: Cross Language and Novelty Tracks</title>
				<funder ref="#_P4xcUfY #_XFZMABb">
					<orgName type="full">SPAWARSYSCEN-SD</orgName>
				</funder>
				<funder ref="#_3fsTyts">
					<orgName type="full">Advanced Research and Development Activity</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,75.90,123.26,78.47,11.15"><forename type="first">Leah</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
						</author>
						<author>
							<persName coords="1,163.35,123.26,63.70,11.15"><forename type="first">James</forename><surname>Allan</surname></persName>
						</author>
						<author>
							<persName coords="1,235.41,123.26,104.39,11.15"><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
						</author>
						<author>
							<persName coords="1,348.66,123.26,72.40,11.15"><forename type="first">Alvaro</forename><surname>Bolivar</surname></persName>
						</author>
						<author>
							<persName coords="1,452.78,123.26,83.36,11.15"><forename type="first">Courtney</forename><surname>Wade</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Massachusetts</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,224.22,87.44,163.65,14.88;1,168.18,105.86,275.57,14.88">UMass at TREC 2002: Cross Language and Novelty Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E04F45DD2F896AC9039CBA1504AD9E0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>participated in the cross-language and novelty tracks this year. The cross-language submission was characterized by combination of evidence to merge results from two different retrieval engines and a variety of different resources -stemmers, dictionaries, machine translation, and an acronym database. We found that proper names were extremely important in this year's queries. For the novelty track, we applied variants of techniques that have been employed for other problems. In addition, we created additional training data by manually annotating 48 additional topics.</p><p>Ajeeb's Tarjim (http://tarjim.ajeeb.com/ajeeb/). These systems gave one translation (or transliteration) for each term. These were added to the dictionary. TREC Normalization and Stemming -A Perl script written by Kareem Darwish and modified by Leah Larkey was distributed as the standard stemmer for TREC. It is also a light stemmer, but it is not as light as the UMass stemmer.</p><p>TREC bilingual lexicon -We used the bilingual lexicon with probabilities from BBN, derived from the UN parallel corpus. The English words in this dictionary are stemmed with the Porter stemmer, and the Arabic words are stemmed with the TREC standard stemmer. We did not use the bilingual dictionary from Tufts University.</p><p>English Corpus -We used AP news articles from 1994 through 1998 in the Linguistic Data Consortium's NA News corpus for an English background language model, and for English query expansion.</p><p>English stop words and stemming -English stop words are from INQUERY's standard list of 418 stop words. English stop phrases are defined by regular expressions in a script we have used before in TREC (in English). In order to use the BBN probabilistic dictionary we performed Porter stemming on the queries. In sub-runs that did not use the BBN dictionary, we left the English unstemmed, stemming with kstem [9] only when the English word was not found in the dictionary. Acronym Expansion -In the cross-language runs, we looked up in the Acrophile system <ref type="bibr" coords="2,435.30,265.59,16.74,8.74" target="#b11">[12]</ref> any sequences of allcapital letters. The top-ranking expansion for each acronym was added to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Information Retrieval</head><p>We used INQUERY [5]  for the monolingual run and two of the cross-language sub-runs, and language modeling (LM) for the rest of the crosslingual sub-runs. For both English and Arabic, text was broken up into words at any white space or punctuation characters. For Arabic, there were five additional Arabic punctuation characters included in the definition of punctuation. Words of one-byte length (in CP1256 encoding) were not indexed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INQUERY -</head><p>The monolingual run and some of the cross-language runs used a version of INQUERY as the search engine. This version computes the belief function reported in UMass's TREC9 report <ref type="bibr" coords="2,434.84,380.01,10.63,8.74" target="#b1">[2]</ref>. The main difference between this version and "real" INQUERY is that proximity information is not stored in the index, so that INQUERY operators requiring proximity information are not implemented.</p><p>For cross-language INQUERY retrieval, English queries are translated to Arabic as follows. For each English word in a query, do the following: find the set of all translations in the dictionary. If the English word is not found, stem the English word using the kstem stemmer and look it up again. Stem the Arabic translations. If any of the translations consist of an Arabic phrase rather than a single word, enclose the phrase in a #filreq operator. Enclose all the alternative Arabic translations for a single English word under a #syn (synonym) operator. Finally, take all the #syn sets and build a weighted sum query out of all the stemmed translations of the query terms by subsumimg all the synonym sets under a #wsum (weighted sum) operator. Each synonym set was given the weight described in the query expansion section.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Crosslingual Language Modeling (LM) -In language modeling, documents are represented as probability distributions over a vocabulary. Documents are ranked by the probability of generating the query by randomly sampling the document model. The language models here are simple unigram models, similar to those of <ref type="bibr" coords="2,496.74,544.89,16.69,8.74" target="#b13">[14]</ref> . Unigram probabilities in our official run were estimated as a mixture of maximum likelihood probability estimates from the document and the corpus, as follows: where P(Q e |D a ) is the probability of generating the query from the document model, the e's are the English words in the query, P(a|D a ) is the probability of an Arabic word a in the document D a , P(e|a) is the translation probability of seeing the English query word e given the presence of Arabic word a, and P(e|GE) is the probability of the English query word in the English background model. P(a|D a ) is estimated as tf a,Da /|D a | where tf a,Da is the number of occurrences of term a in Arabic document D a and |D a | is the length of document, that is, the number of total term occurrences in the document. The translation probability P(e|a) comes from the bilingual lexicon. If the lexicon was derived from a parallel corpus, these probabilities represent the proportion of the time that Arabic word a was aligned with English word e in the parallel corpus. If the lexicon is a dictionary, and Arabic word a has n different translations into English e 1 ,…e n , then P(e|a) is estimated as 1/n. The background probability P(e|GE) is estimated as</p><formula xml:id="formula_0" coords="2,153.12,583.87,190.04,28.56">( ) ∏ ∑ ∈ ∈       + = e Q e Arabic</formula><formula xml:id="formula_1" coords="3,73.80,81.87,98.26,24.11">( ) ∑ ∈ = C t C t C e df df GE e P , ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|</head><p>where df e,C is the number of English documents in C containing term e, and the summation is over all the terms in the English collection, as in <ref type="bibr" coords="3,324.10,109.47,10.64,8.74" target="#b6">[7]</ref>.</p><p>Query Expansion -We expanded English queries in some of our cross-language sub-runs using the AP news articles from 1994 through 1998 in the Linguistic Data Consortium's NA News corpus. This corpus was indexed without stemming, but normalized to lower case. We retrieved the top 10 documents for each query. Terms from these documents received an expansion score which was the sum across the ten documents of the INQUERY belief score for the term in the document. The 5 terms with the highest expansion score were added to the query. Final term weights were set to 2w o + w e where w o is the original weight in the unexpanded query and w e =1.</p><p>Arabic query expansion was handled in different ways for INQUERY sub-runs and LM sub-runs. For INQUERY sub-runs, Arabic query expansion was just like English query expansion, except the top 10 documents were retrieved from the Arabic corpus, rather than the English corpus, and 50 terms, not 5, were added to the query.</p><p>In language model sub-runs, query expansion was carried out using relevance modeling <ref type="bibr" coords="3,425.53,242.43,15.33,8.74" target="#b12">[13]</ref>. The best matching fifty documents were retrieved and 500 words were selected as the new query. Associated with each word is an estimated probability of observing this word in the relevant documents.</p><p>Combination of Evidence -Sub-runs were combined into submitted runs by normalizing document scores in ranked lists, and then summing lists two at a time. Score normalization was a linear min-max normalization where score norm =(score-min)/(max-min).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Monolingual run</head><p>Our one monolingual run was designated UMassM, and carried out as follows:</p><p>1. Convert queries to CP1256 encoding 2. Extract titles and descriptions from topics.</p><p>3. Remove stop phrases, using a script developed for TREC2001. 4. Stem the query with the UMass light stemmer and remove stop words. 5. Expand the query by adding the best 50 words from the top ranking 10 documents. 6. Retrieve the top 1000 documents using INQUERY.</p><p>Overall average precision on this run was .3619. The per-query comparison among the 18 submitted monolingual runs suggests that our approach favored recall over precision. We performed at or above the median average precision on 34 of the 50 queries, and below the median on 16 queries. In number of relevant documents retrieved in top 1000, we were at or above the median in 44 or the 50 queries, and at the highest number in 28 queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Cross-language runs</head><p>Our cross-language submissions relied heavily on combination of evidence this year. We used two different dictionaries that could not be combined because they were built with different stemmers. In our own dictionary, a composite of a variety of different sources, English was not stemmed, and Arabic words were stemmed using the UMass light stemmer. In the standard probabilistic dictionary, (the bilingual lexicon built at BBN using the UN parallel corpus), Arabic words were stemmed using the Darwish stemmer, and English was stemmed with the Porter stemmer. We ran several independent retrieval sub-runs and combined their ranked lists at the end.</p><p>Each sub-run used one of two different retrieval engines -INQUERY or LM, one of two different resource sets: UMass dictionary and stemmer, or TREC standard probabilistic dictionary and stemmer, one of 3 different query expansion options: English only, Arabic only, or English and Arabic, and one of two different selections from the topic: title and description, or title, description, and narrative.</p><p>The steps were as follows: (For sub-runs) Twelve different crosslingual conditions were run. These sub-runs were combined into four cross language runs that can be briefly described as follows:</p><p>1. UMassX2 -combination of 2 INQUERY cross language sub-runs both using title and description fields. 2. UMassX6 -combination of all 6 cross language sub-runs using title and description fields. 3. UMassX2n -combination of 2 INQUERY sub-runs using title, description, and narrative fields. 4. UMassX6n -combination of all 6 cross language sub-runs using title, description, and narrative fields. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.">Cross-Language Results</head><p>Table <ref type="table" coords="4,98.63,446.79,5.01,8.74" target="#tab_4">2</ref> summarizes the results of our official submissions. Combination of results based on different resources improved performance. Inclusion of narrative words also improved performance a great deal. UMassX6n had the highest mean average precision of all the officially submitted cross language runs in TREC 2002. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.">Post hoc Cross-Language Experiments</head><p>Post hoc experiments were performed first, to correct some errors in our official submissions, second, to provide a "standard resources" run, and third, to explore the role of acronyms, proper names, and stemming of the UN parallel corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.1.">Fixing Errors</head><p>We discovered two problems after submitting our results. First, the Porter stemmer used in processing the BBN bilingual corpus was different from our version of the Porter stemmer, so that many English words were not found in the probabilistic dictionary. For example, contrary was stemmed to contrari by the BBN Porter stemmer, but to contrar by our Porter stemmer. Money became monei under the BBN Porter stemmer, but remained money under our Porter stemmer. When we reran the sub-runs with compatible Porter stemming, results on those sub-runs improved, as did the combinations that included them.</p><p>A second problem resulted from a procedural error, in which the language model runs (but not the INQUERY runs) using the UMass resources were run with an older, smaller version of the dictionary. We therefore reran the affected conditions with the correct dictionary, and obtained the results shown in the last column of Table <ref type="table" coords="5,462.81,138.09,3.77,8.74" target="#tab_4">2</ref>.</p><p>Overall, the greater coverage of the dictionaries and the use of compatible versions of the Porter stemmer improved performance. The overall patterns remained the same -combination of resources improved performance, and retrieval was more effective when the narrative portion was included in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.2.">Standard Resources Run</head><p>Although we did not submit an official standard resources run we ran one later for this report. Recall that one of the two sub-runs that made up UMassX2 and UMassX2n, and three of the six sub-runs that made up UMassX6 and UMassX6n, used the standard parallel corpus dictionary and stemmer. The Standard Resources column of Table <ref type="table" coords="5,530.81,234.75,5.01,8.74" target="#tab_5">3</ref> shows the results when the sub-runs based on the UMass resources and acronym expansion were excluded. Only the sub-runs based on the standard resources were included. Thus, in the Standard Resources column, the UMassX2 and UMassX2n rows show the results of a single sub-run, and UMassX6 and UMassX6n rows each show results based on a combination of three, rather than six, sub-runs. Relative to these three way combinations, the additional resources increased average precision 3 percentage points for title+description queries, and 4 points for title+description+narrative queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.3.">Acronym expansion</head><p>Because this was the first time acronym expansion was used in the TREC cross-lingual track, we assessed its contribution separately. We reran the UMassX6 and UMassX6n runs without expanding acronyms. The results, shown in the No Acro column of Table <ref type="table" coords="5,202.84,359.91,3.77,8.74" target="#tab_5">3</ref>, revealed that acronym expansion added almost nothing. Analysis of individual queries containing acronyms revealed that while acronym expansion helped on some queries, it hurt on others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.4.">Importance of Proper Names</head><p>It had struck us in informal query analyses that proper names were extremely important in these queries. We hypothesized that successful retrieval depended upon having these proper names in the lexicon. In order to test this, we identified all the proper names (people, places, organizations, acronyms) in our queries and in our expanded English queries, and made special versions of the UMass and BBN dictionaries from which these names had been removed. The column labeled No Names in Table <ref type="table" coords="5,277.54,450.51,5.01,8.74" target="#tab_5">3</ref> gives the results. Performance dropped more than 50% when names were not available in the dictionaries. We speculate that one reason dictionaries derived from parallel corpora work so well is that they cover so many more proper names than do static dictionaries. Although the standard parallel corpus dictionary as processed by BBN was a very valuable resource, we found it awkward to fit into the rest of our system because of the Porter stemming used on the English words, and because of the small differences between the standard Arabic stemmer and the UMass light stemmer. We reprocessed the UN corpus, obtained from LDC, using the same configuration Alex Fraser used at BBN, except we used different preprocessing in preparing the English and Arabic input to GIZA++. English words were lower cased, and stop words were removed. Arabic words were stemmed and stop words removed using the UMass (light10_stop) stemmer. Retrieval results comparing the two versions of the parallel corpus dictionary can be seen in Table <ref type="table" coords="5,509.50,683.43,3.76,8.74" target="#tab_6">4</ref>. Table <ref type="table" coords="5,97.32,694.89,5.01,8.74" target="#tab_6">4</ref> contains only sub-runs and combinations that used the parallel corpus dictionary. It does include any runs that used the UMass dictionary. The improvement in the official runs that include all the resources can be seen in Table <ref type="table" coords="6,97.30,86.13,3.76,8.74" target="#tab_5">3</ref>, above, in the Reprocessed UN Corpus column. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Novelty Track</head><p>Our attempts to find relevant and novel information focused on variants of techniques that have been employed for other problems <ref type="bibr" coords="6,134.72,282.45,11.69,8.74" target="#b0">[1]</ref> [3] <ref type="bibr" coords="6,163.08,282.45,10.62,8.74" target="#b3">[4]</ref>. Basically, we looked for relevant sentences by comparing them to the query, and we looked for redundancy by estimating whether a sentence was dissimilar from all prior (relevant) sentences. We found that the task of recognizing relevant sentences was the major challenge and that our errors there account for poor overall performance.</p><p>One notable feature of the CIIR's participation in the novelty track is our creation of additional training data. We hired students to annotate 48 topics in addition to the handful provided by NIST. Details on that process are discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Creating additional training topics</head><p>For the purpose of this evaluation we built our own collection of documents and sentence level relevance judgments. We randomly chose 48 topics from the TREC-7 and TREC-8 ad-hoc retrieval tracks (topics 300-450)-though we were careful not to use the topics set aside for the novelty track evaluation. For each of the 48 topics, we used the INQUERY search engine to find the top-ranked documents in a subset of TREC volumes 4 and 5 (Federal register 1994, LA Times 1989-90, and FBIS 1996). We selected the top 25 known-relevant documents (based on TREC judgments) for each topic.</p><p>We followed the same methodology defined by the novelty track to collect relevance and novelty assessments. We hired undergraduate students, who were otherwise unaffiliated with our research, to read the relevant documents for each topic in rank order. They extracted relevant and novel sentences from each topic in a two step process. First, the assessors were told to read a printed copy of the relevant documents and highlight the relevant sentences. Second, they read the sentences marked as relevant again and flagged the ones that contained novel information. For this process, order is very important. By definition, a sentence is novel if it provides totally new information or further details on previously seen information. Instances that summarize details seen earlier in the document stream were not considered novel.</p><p>Some statistics for the constructed training data are presented in Figure <ref type="figure" coords="6,358.91,569.85,5.01,8.74" target="#fig_0">1</ref> and Table <ref type="table" coords="6,408.61,569.85,3.76,8.74" target="#tab_7">5</ref>. These statistics are consistent with the NIST-provided training topics as well as the evaluation topics. Interestingly, for the average topic less than 5% of the sentences contain relevant material. Further, more than 80% of the relevant sentences on average contain novel information. That is, most of the material is non-relevant and most of the relevant material is novel. The material for reconstructing our data and some additional statistics about the data are available at [http://ciir.cs.umass.edu/downloads/access.html]. That material assumes that TREC volumes 4 and 5 are available to the user.  We were curious about the impact of time on a person's conception of novelty. For instance, we were interested in answering question like the following: How far could an assessor go without losing track of the mental representation of a novel sentence with respect to a particular topic? What is the behavior of the novelty rate as more documents are added to the knowledge base? To answer this questions, one of the assessors was told to read through a bigger set of documents (75 documents) carrying out the relevance and novelty judgments steps as they were explained before. The topic chosen for this experiment was 422 (Figure <ref type="figure" coords="7,353.90,395.25,3.61,8.74" target="#fig_1">2</ref>). The results are presented in Table <ref type="table" coords="7,504.95,395.25,3.76,8.74" target="#tab_8">6</ref>.  The assessor in charge of this task reported that it was not difficult to assess the relevance or novelty of a sentence even with this many documents, because it was fairly easy to maintain a clear sense of the topic definition and when new information about the topic was appearing.</p><p>Table <ref type="table" coords="7,97.26,687.09,5.01,8.74" target="#tab_8">6</ref> shows an increase for the percent of relevant sentences across different assessors and different evaluation set sizes. The difference is presumably accounted for by normal inter-annotator disagreement. Interestingly, though, the proportion of new material decreases with more sentences judged. Our intuition tells us that as more &lt;top&gt; &lt;num&gt; Number: 422 &lt;title&gt; art, stolen, forged &lt;desc&gt; Description: What incidents have there been of stolen or forged art? &lt;narr&gt; Narrative: Instances of stolen or forged art in any media are relevant. Stolen mass-produced things, even though they might be decorative, are not relevant (unless they are massproduced art reproductions). Pirated software, music, movies, etc. are not relevant. &lt;/top&gt; documents are processed and our knowledge base about the topic increases, the tendency to find new sentences should be greatly reduced: the topic should be fully covered. The results from trying one topic in more detail might be explained by a lack of redundancy in the collection or by the intrinsic property of the topic to constantly generate new events related to the same topic (and thus novel sentences). Topics defined the way topic 422 is defined admit constant generation of new events that must create new relevant and novel sentences.</p><p>We were also concerned by the low number of relevant sentences and wondered if this was the result of the annotation instructions. We told one of our assessors to read the relevant documents for topic 327 and topic 417 (Figure <ref type="figure" coords="8,103.96,166.59,4.16,8.74" target="#fig_2">3</ref>) and identify the sentences that do not provide any relevant information whatsoever in relation to the topic. The results are presented in Tables <ref type="table" coords="8,213.53,178.11,5.01,8.74" target="#tab_9">7</ref> and<ref type="table" coords="8,238.01,178.11,3.76,8.74" target="#tab_10">8</ref>.   Table <ref type="table" coords="8,97.31,622.05,5.01,8.74" target="#tab_9">7</ref> shows the inconsistencies between two assessors, one finding relevant and one finding non-relevant. For topic 327, for example, the upper-right cells indicate that 40 sentences were explicitly identified as relevant and as non-relevant. Presumably those reflect inter-annotator disagreement, though we have not adjudicated the results to see whether there are errors. Nineteen of the sentences were judged relevant by both approaches, and almost 86% of the sentences were consistently judged non-relevant.</p><p>Table <ref type="table" coords="8,97.30,685.59,5.01,8.74" target="#tab_10">8</ref> shows for topic 327 that an assessor finding relevant sentences found that 12.9% of the sentences were relevant, whereas an assessor looking for non-relevant implicitly found only 5% of the sentences relevant. The difference is surprising, particularly since we were expecting that the "find non-relevance" assessor would "find" substantially more relevant material. On the other hand, for topic 417 we obtained a more reasonable result with a very low 0.4% of inter-annotator disagreement. These differences among topics may be explained by the information need that the topic describes, which for topic 327 seems to be pretty ambiguous.</p><p>Similar experiments on a larger collection of topics might make clearer what is happening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Experiments finding relevant sentences</head><p>In order to extract the sentences with relevant information, we used the traditional IR ranking approach with three different retrieval models. Pseudo-relevance feedback (PRF) was used with each approach and executed depending on the retrieval model being used.</p><p>1. TFIDF. Here, we tried the traditional TFIDF approach. Given a query q and sentence s:</p><formula xml:id="formula_2" coords="9,172.62,211.14,196.01,29.40">∑ ∈         + + + + = q t t s t q t sf n tf tf s score 5 . 1 log ) 1 log( ) 1 log( ) ( , ,</formula><p>q t tf , and s t tf , are the number of times term t occurs in the query and sentence, respectively, t sf is the number of sentences in which term t appears, and n is the number of sentences in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Simple Language Modeling (KLD).</head><p>Given smoothed language models of a query q and a sentence s, the score is given by the Kullback-Leibler divergence (KLD) between the two mass distributions:</p><formula xml:id="formula_3" coords="9,172.68,298.33,190.10,23.64">) | ( ) | ( log ) | ( ) || ( ) ( s w p q w p q w p s q KLD s score V w ∑ ∈ ⋅ = =</formula><p>3. Two-Stage Smoothing in Language Modeling. The two-stage smoothing method allows the use of different smoothing techniques for the query and the sentence language models. This is used in order to differentiate the two roles that smoothing plays in the retrieval process. For the sentence, smoothing assigns non-zero probabilities to words not present in the sentence. For the query, smoothing "explains away" the common non-topic words in the query. This approach is extensively explained in <ref type="bibr" coords="9,479.68,383.79,15.35,8.74" target="#b14">[15]</ref>.</p><p>Multiple runs were carried out on our training topics in order to tune the multiple parameters in the different retrieval models. Results from the best runs are presented in Figure <ref type="figure" coords="9,345.55,418.29,3.77,8.74" target="#fig_3">4</ref>. Although TFIDF with PRF had the best average precision, its performance is not significantly different (student's t-test) from the other models. For all models, performance of the retrieval process at sentence level was poor and very hard to improve. We also explored the influence of query length on performance. We used "short" queries and "long" queries. For every case, we tried using only some portion of the topic description. For short queries we tried using only the topic title or the topic title and description. As long queries, we tried using the topic title, description, and narrative. Our results were consistent with the results reported in <ref type="bibr" coords="10,274.85,74.61,15.34,8.74" target="#b14">[15]</ref>. On average, it is better to use the complete topic text as opposed to using only portions of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision -Recall</head><p>Some additional experiments were carried out, without much success, to try to improve performance by means of query preprocessing. A standard ad-hoc query algorithm <ref type="bibr" coords="10,302.71,115.11,11.68,8.74" target="#b1">[2]</ref> was run on the query text (topic text) for the different topics. The goal of this algorithm was to get rid of what we believed were query stop words and stop phrases. We believed that words like "narrative" and "description" as well as phrase patterns such as "A document that discusses word [word … word] is considered non-relevant" should not be present in the query. Contrary to our intuition, the use of this algorithm did not improve the retrieval performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Looking for novelty</head><p>Our novelty detection systems take as input the ranked list of relevant sentences for each topic (i.e., the output of the previous step). We decided to use the vector space retrieval model with TFIDF weighting and pseudo-relevance feedback as we found that it was the method that worked best with our training data. Both of our systems (CIIR02tfkl and CIIR02tfnew) use this method to identify the relevant sentences. We determined through trial and error on the training data that our results were best if we used the top 10 percent of relevant sentences, even though we know from the training topics that only about 5% of the sentences are likely to be relevant. These top relevant sentences are re-indexed using Lemur. <ref type="foot" coords="10,226.14,267.23,3.99,6.96" target="#foot_0">1</ref> No stopping or stemming is used at this stage.</p><p>Our two systems assign novelty scores to sentences in different ways:</p><p>CIIR02tfkl. The CIIR02tfkl system uses the KL-divergence between two language models as its scoring method.</p><p>For each topic, the documents are considered in the order given by the task and novelty scores are assigned using the following method. The first relevant sentence of the first relevant document is assigned a maximum score. For the remaining sentences we calculate a collection language model and a sentence language model in Lemur: collLM(i) (the collection language model for sentence i) is a maximum likelihood model built on sentences 1 to (i-1), smoothed using linear interpolation against a maximum likelihood model built on sentences 1 to i. sentLM(i) (the sentence language model for sentence i) is a maximum likelihood model built on sentence i, smoothed using linear interpolation against a maximum likelihood model built on sentences 1 to i.</p><p>Sentence i's score is the KL-divergence between its collection and sentence language models, KLD(sentLM(i) || collLM(i)).</p><p>We set the smoothing parameters for both language models so that almost no smoothing occurs. These parameter settings were chosen because they achieved the best results on our training data.</p><p>CIIR02tfnew. The CIIR02tfnew system assigns novelty scores in a very simple way. For each topic, it considers the documents in the order specified by the task. Each sentence is treated as a set of words and a sentence's score is equal to the number of new words in the set (i.e., words that have not appeared so far in the sentences for that topic).</p><p>We observed that in our training data, approximately 80 percent of the sentences judged relevant by the annotators were also judged new. Therefore, both of our systems return the top 80 percent of the ranked list of novelty scores as new.</p><p>Interestingly, for both the training and test data, when we ran our two systems on the collection of known relevant sentences (i.e., we cheated), CIIR02tfkl performed better than CIIR02tfnew. However, when we ran these systems on our own relevance results (i.e., relevance results with errors), CIIR02tfnew performed better than CIIR02tfkl. We hypothesize that non-relevant sentences are likely to be identified as novel and that CIIR02tfkl models novelty better than CIIR02tfnew and therefore tends to pull those non-relevant sentences towards the top of the novelty rankings. We have not sufficiently investigated this issue, however. It may, for example, be nothing more than a statistical anomaly.</p><p>The following graphs show the effectiveness of the two techniques when only relevant sentences are ranked. Note that even a random ranking of these sentences does quite well, because about 80% of them are novel.</p><p>Training Data (52 topics) Known Relevant Sentences The next graphs provide the same information for when the "relevant" sentences are chosen using one of the approaches described above-i.e., generated by a system. Overall performance drops substantially because the quality of the initial retrieval is poor (the scale of the axes is dramatically different). This final graph shows how the results of our official submissions compared to other submissions of the TREC and shows on a query-by-query basis how our two approaches stacked up. 2   2 This graph compares systems based on precision * recall, the evaluation measure used at the TREC workshop. After the workshop, the evaluation metric was changed to the F measure. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,84.84,308.85,442.25,8.74;7,99.24,320.37,413.54,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Histograms of distribution of (a) the percentage of relevant sentences over the total number of sentences and, (b) the percentage of novel sentences in terms of the number of relevant sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,233.88,521.13,144.18,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Topic 422's description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,189.72,381.69,232.54,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Topic description for topic 327 and topic 417</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,153.42,665.49,305.21,8.74"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results for sentence relevance retrieval on the training topics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.00,207.09,457.95,20.26"><head>Table 1</head><label>1</label><figDesc>lists the resources used in each of the 12 sub-runs, and indicates which sub-runs composed each submitted run. Names of submitted runs are abbreviated e.g. UMassX2 as X2, etc.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,100.50,236.19,410.55,178.36"><head>Table 1 : Definition of sub-runs Sub-run Compo- nent of</head><label>1</label><figDesc></figDesc><table coords="4,268.80,255.21,242.25,20.26"><row><cell>Engine</cell><cell>Dictionary + Stemmer</cell><cell>Expansion</cell><cell>Narrative Included?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,107.28,487.41,393.07,109.00"><head>Table 2 : Monolingual and Cross-language Results: official runs Name of Run Official Mean Average Precision Number of Queries at or Above Median Average Precision Rel Ret in top 1000 Post hoc Average Precision</head><label>2</label><figDesc></figDesc><table coords="4,107.28,541.53,384.58,54.88"><row><cell>UMassM</cell><cell>.3619</cell><cell>34/50</cell><cell>44/50</cell><cell>.3619</cell></row><row><cell>UMassX2</cell><cell>.3538</cell><cell>30/50</cell><cell>40/50</cell><cell>.3589</cell></row><row><cell>UMassX6</cell><cell>.3658</cell><cell>36/50</cell><cell>42/50</cell><cell>.3801</cell></row><row><cell>UMassX2n</cell><cell>.3900</cell><cell>35/50</cell><cell>37/50</cell><cell>.3941</cell></row><row><cell>UMassX6n</cell><cell>.3996</cell><cell>39/50</cell><cell>41/50</cell><cell>.4107</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,72.00,491.13,420.78,117.61"><head>Table 3 : Post hoc average precision: Cross Language runs Name of Run Official Errors Corrected Standard Resources No Acro No Names</head><label>3</label><figDesc></figDesc><table coords="5,439.38,510.15,53.40,20.26"><row><cell>Reprocessed</cell></row><row><cell>UN Corpus</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,112.62,103.71,386.77,120.52"><head>Table 4 : Mean average precision using two dictionaries made from the UN parallel corpus.</head><label>4</label><figDesc></figDesc><table coords="6,170.04,122.73,271.91,101.50"><row><cell></cell><cell></cell><cell>TREC</cell><cell>Reprocessed</cell></row><row><cell></cell><cell>Type of Run</cell><cell>Standard</cell><cell>UN Corpus</cell></row><row><cell></cell><cell></cell><cell>Dictionary</cell><cell></cell></row><row><cell cols="2">INQUERY title+description</cell><cell>.3161</cell><cell>.3413</cell></row><row><cell>LM</cell><cell>title+description</cell><cell>.3464</cell><cell>.3637</cell></row><row><cell cols="2">INQ + LM title+description</cell><cell>.3520</cell><cell>.3678</cell></row><row><cell cols="2">INQUERY title+desc+narrative</cell><cell>.3350</cell><cell>.3614</cell></row><row><cell>LM</cell><cell>title+desc+narrative</cell><cell>.3605</cell><cell>.3804</cell></row><row><cell>INQ+LM</cell><cell>title+desc+narrative</cell><cell>.3734</cell><cell>.3880</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,76.89,74.73,452.50,221.00"><head>Table 5 : Collection statistics summary for training set</head><label>5</label><figDesc></figDesc><table coords="7,76.89,93.46,452.50,202.27"><row><cell></cell><cell>TOPICS</cell><cell></cell><cell cols="3"># ASSESSORS</cell><cell cols="11">DOCS SENT NOVEL REL REL/SENT NOVEL/REL</cell></row><row><cell></cell><cell>48</cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell>1122</cell><cell>84588</cell><cell cols="2">2759</cell><cell>3400</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">AVERAGE/TOPIC 23.38 σ 3.923 797.22 1762</cell><cell cols="2">57.48 46.217</cell><cell>70.83 55.63</cell><cell></cell><cell>4.6% 3.64%</cell><cell></cell><cell cols="2">81.6% 12.74%</cell><cell></cell></row><row><cell cols="8">Percentage of Relevant Sentences in a Topic -</cell><cell></cell><cell cols="8">Percentage of Novel over Relevant Sentences</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Histogram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">in a Topic -Histogram</cell><cell></cell><cell></cell></row><row><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6 8 10 12 Frequency</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frequency</cell><cell>3 4 5 6 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2%</cell><cell>4%</cell><cell>6%</cell><cell>8%</cell><cell>10%</cell><cell>12%</cell><cell>14%</cell><cell>16%</cell><cell></cell><cell>60%</cell><cell>65%</cell><cell>70%</cell><cell>75%</cell><cell>80%</cell><cell>85%</cell><cell>90%</cell><cell>95%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,85.38,539.07,441.20,83.86"><head>Table 6 : Annotation information for topic 422. The first row presents statistics for an assessor who was asked to annotate 75 documents rather than just 25.</head><label>6</label><figDesc></figDesc><table coords="7,128.34,571.89,355.36,51.04"><row><cell cols="8">TOPICID ANNOT DOCS SENT NEW REL REL/SENTS NEW/REL</cell></row><row><cell>422</cell><cell>D</cell><cell>75</cell><cell>3593</cell><cell>164</cell><cell>181</cell><cell>5.0%</cell><cell>90.6%</cell></row><row><cell>422</cell><cell>D</cell><cell>25</cell><cell>2065</cell><cell>89</cell><cell>94</cell><cell>4.6%</cell><cell>94.7%</cell></row><row><cell>422</cell><cell>C</cell><cell>25</cell><cell>2065</cell><cell>75</cell><cell>77</cell><cell>3.7%</cell><cell>97.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="8,71.88,416.73,469.49,92.04"><head>Table 7 : Comparison of assessor results when one assessor located relevant sentences and the other locates non-relevant sentences. Upper right and lower left portions of each table represent disagreement</head><label>7</label><figDesc></figDesc><table coords="8,71.88,445.90,469.49,62.87"><row><cell></cell><cell></cell><cell></cell><cell cols="2">ASSESSOR D</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ASSESSOR D</cell><cell></cell></row><row><cell></cell><cell>TOPIC 327</cell><cell></cell><cell>NOT</cell><cell></cell><cell></cell><cell>TOPIC 417</cell><cell></cell><cell>NOT</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">JUDGED</cell><cell>NOT REL</cell><cell></cell><cell></cell><cell cols="2">JUDGED</cell><cell cols="2">NOT REL</cell></row><row><cell>ASSES. F</cell><cell>RELEVANT NOT JUDGED</cell><cell>19 8</cell><cell>4.2% 1.8%</cell><cell>40 389 85.3% 8.8%</cell><cell>ASSES. A</cell><cell>RELEVANT NOT JUDGED</cell><cell>20 103</cell><cell>0.9% 4.4%</cell><cell>9 2215</cell><cell>0.4% 94.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="8,65.16,529.59,483.33,72.22"><head>Table 8 : Comparison of sentences judged by two assessors in selected topics</head><label>8</label><figDesc></figDesc><table coords="8,65.16,547.59,483.33,54.22"><row><cell cols="2">TOPICID ASSESSOR</cell><cell>*</cell><cell>*/S</cell><cell cols="2">TOPICID ASSESSOR</cell><cell>*</cell><cell>*/S</cell></row><row><cell>327</cell><cell>A D</cell><cell>59 Relevant 429 Not Relevant</cell><cell>12.9% 94.1%</cell><cell>417</cell><cell>F D</cell><cell>29 Relevant 2224 Not Relevant</cell><cell>1.2% 94.8%</cell></row><row><cell>Documents:</cell><cell>11</cell><cell cols="2">Sentences: 456</cell><cell>Documents:</cell><cell>25</cell><cell cols="2">Sentences: 2347</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="10,77.76,699.63,448.54,8.74;10,72.00,711.09,411.72,8.74"><p>Lemur is a toolkit from CMU and UMass Amherst intended to support the use of language modeling techniques for information retrieval. It is freely available for research purposes at http://www.cs.cmu.edu/~lemur.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part the <rs type="institution">Center for Intelligent Information Retrieval</rs>, in part by <rs type="funder">SPAWARSYSCEN-SD</rs> grant numbers <rs type="grantNumber">N66001-99-1-8912</rs> and <rs type="grantNumber">N66001-02-1-8903</rs>, and in part by <rs type="funder">Advanced Research and Development Activity</rs> under contract number <rs type="grantNumber">MDA904-01-C-0984</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.</p></div>
<div><head>4.</head></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_P4xcUfY">
					<idno type="grant-number">N66001-99-1-8912</idno>
				</org>
				<org type="funding" xml:id="_XFZMABb">
					<idno type="grant-number">N66001-02-1-8903</idno>
				</org>
				<org type="funding" xml:id="_3fsTyts">
					<idno type="grant-number">MDA904-01-C-0984</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,93.56,183.81,445.83,8.74;12,93.60,195.33,294.17,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,129.62,183.81,172.23,8.74;12,319.08,183.81,220.31,8.74;12,93.60,195.33,49.56,8.74">Topic detection and tracking: Event-based information organization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<editor>J. Allan</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>Introduction to topic detection and tracking</note>
</biblStruct>

<biblStruct coords="12,93.56,212.79,444.60,8.74;12,93.60,224.31,322.87,8.74" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F.-F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trec-9</forename><surname>Inquery</surname></persName>
		</author>
		<title level="m" coord="12,93.60,224.31,191.94,8.74">The Ninth Text REtrieval Conference (TREC-9)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,241.77,420.75,8.74;12,93.60,253.29,443.28,8.74;12,93.60,264.81,22.58,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,258.30,241.77,143.69,8.74">Temporal summaries of news topics</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Khandelwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,419.10,241.77,95.22,8.74;12,93.60,253.29,396.25,8.74">Proceedings of the 24th annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,282.27,424.92,8.74;12,93.60,293.79,360.91,8.74;12,93.60,305.31,126.46,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,492.42,282.27,26.08,8.74;12,93.60,293.79,222.13,8.74">Topicbased novelty detection: 1999 summer workshop at clsp</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rajman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hoberman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Caputo</surname></persName>
		</author>
		<ptr target="http://www.clsp.jhu.edu" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.57,322.77,425.58,8.74;12,93.60,334.29,226.72,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,262.80,322.77,200.19,8.74">TREC and TIPSTER experiments with INQUERY</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Broglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,471.36,322.77,47.79,8.74;12,93.60,334.29,114.18,8.74">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="343" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.60,351.81,427.39,8.74;12,93.60,363.27,336.90,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,206.21,351.81,314.78,8.74;12,93.60,363.27,160.48,8.74">The TREC-2001 cross-language information retrieval track: Searching Arabic using English, French, or Arabic queries</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,271.26,363.27,24.46,8.74">TREC</title>
		<meeting><address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2001">2001. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.57,380.79,426.83,8.74;12,93.60,392.31,439.40,8.74;12,93.60,403.77,65.90,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,215.28,380.79,305.11,8.74;12,93.60,392.31,63.38,8.74">Relating the new language models of information retrieval to the traditional retrieval models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
		<idno>TR-CTIT-00-09</idno>
		<imprint>
			<date type="published" when="2000-05">May 2000 2000</date>
			<pubPlace>Enschede, The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">CTIT Technical Report</note>
</biblStruct>

<biblStruct coords="12,93.58,421.29,425.69,8.74;12,93.60,432.81,309.17,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,198.30,421.29,85.22,8.74">Stemming Arabic text</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khoja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garside</surname></persName>
		</author>
		<ptr target="http://www.comp.lancs.ac.uk/computing/users/khoja/stemmer.ps" />
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Lancaster, U.K.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computing Department, Lancaster University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.59,450.27,409.77,8.74;12,93.60,461.79,433.13,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,142.53,450.27,179.26,8.74">Viewing morphology as an inference process</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,339.18,450.27,164.17,8.74;12,93.60,461.79,351.01,8.74">Proceedings of the sixteenth annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the sixteenth annual international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="191" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.59,479.31,439.77,8.74;12,93.60,490.77,418.48,8.74;12,93.60,502.29,446.43,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,292.74,479.31,240.62,8.74;12,93.60,490.77,148.88,8.74">Improving stemming for Arabic information retrieval: Light stemming and co-occurrence analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,259.62,490.77,252.46,8.74;12,93.60,502.29,260.16,8.74">SIGIR 2002: The twenty-fifth annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,519.75,403.81,8.74;12,93.60,531.27,107.28,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,227.24,519.75,202.24,8.74">Arabic information retrieval at UMass in TREC-10</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,447.96,519.75,24.43,8.74">TREC</title>
		<meeting><address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,548.79,419.13,8.74;12,93.60,560.25,435.12,8.74;12,93.60,571.77,75.93,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,318.57,548.79,194.14,8.74;12,93.60,560.25,23.07,8.74">Acrophile: An automated acronym extractor and server</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tamilio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,133.86,560.25,265.88,8.74">Digital libraries &apos;00 -the fifth ACM conference on digital libraries</title>
		<meeting><address><addrLine>San Antonio, TX</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.60,589.29,431.89,8.74;12,93.60,600.75,418.54,8.74;12,93.60,612.27,180.00,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,283.31,589.29,124.16,8.74">Cross-lingual relevance models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,425.22,589.29,100.27,8.74;12,93.60,600.75,414.86,8.74">SIGIR 2002: The twentyfifth annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.57,629.79,446.28,8.74;12,93.60,641.24,419.33,8.74;12,93.60,652.76,274.18,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,253.24,629.79,283.04,8.74">Evaluating a probabilistic model for cross-lingual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,104.46,641.24,408.47,8.74;12,93.60,652.76,83.01,8.74">Proceedings of the 24th annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on research and development in information retrieval<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.57,670.28,442.63,8.74;12,93.60,681.74,440.46,8.74;12,93.60,693.26,129.61,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,192.22,670.28,210.82,8.74">Two-stage language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,419.82,670.28,116.37,8.74;12,93.60,681.74,396.25,8.74">SIGIR 2002: The twenty-fifth annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
