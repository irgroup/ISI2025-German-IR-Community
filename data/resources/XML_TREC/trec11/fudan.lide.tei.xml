<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.16,95.66,401.74,14.36">FDU at TREC2002: Filtering, Q&amp;A, Web and Video Tasks</title>
				<funder ref="#_SGwmxys #_sPUXHUt">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_gyVUjFp #_U7JKAyh">
					<orgName type="full">NSF of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.20,130.43,31.77,9.50"><forename type="first">Lide</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.94,130.43,67.48,9.50"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.31,130.43,41.26,9.50"><forename type="first">Junyu</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.32,130.43,42.36,9.50"><forename type="first">Yingju</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.63,130.43,37.97,9.50"><forename type="first">Zhe</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,418.76,130.43,53.41,9.50"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.16,95.66,401.74,14.36">FDU at TREC2002: Filtering, Q&amp;A, Web and Video Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9225555AF3C3FE0FD512EF53C7D945FC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year Fudan University takes part in the TREC conference for the third time. We have participated in four tracks of Filtering, Q&amp;A, Web and Video.</p><p>For filtering, we only participate in the sub-task of adaptive filtering. A novel method is presented, in which a winnow classifier from the description and narrative fields is constructed, and then utilized to assist our previous adaptive filtering system.</p><p>A novel approach to confidence sorting, which is based on Maximum Entropy, is proposed in our Question Answering system. The rank of individual answer is determined by several weighted factors, and the confidence score is the product of the exponent of the weights of every factors. The weight of every factor is assigned during the training of previous questions.</p><p>To return highly relevant key resources for web retrieval, we modified our original search system to make it return higher precision result than before. First, we proposed a novel search algorithm to get a base set of highly relevant documents. Then special post-processing modules are used to expand and re-sort the base set.</p><p>This year we tried a fast manifold-based approach to face recognition in the Video Search Task. It can be used when there are only few different images of a specific person and runs fast. Experiment shows that applying this step will make the face recognition 5-fold faster and with almost no decreasing of performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Filtering</head><p>Our research focuses on how to make use of the narrative and description fields of each topic. Experiment results have shown that these two fields are very important and the proper exploitation of them can enhance the filtering system's performance greatly.</p><p>In this section, we will first introduce the training stage of our adaptive filtering system in details, and then the adaptive filtering stage; some experiment results are also given. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Training of adaptive filtering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Topic processing</head><p>By examining the words in the description and narrative of each topic carefully, we believe that smart use of these words will bring notable gain. Therefore, we construct a winnow classifier [Littlestone88] from these two fields to assist our adaptive filtering system.</p><p>First, we remove the function words from these two fields. Here function words including stop word and those words such as "relevant", "irrelevant", "documents" which bear no content in the topic description. Then we initialize a winnow classifier for each topic using the remaining words and assigning each word with equal weight. After that, we adjust the winnow item's weight during training. The adjustment procedure can be described as below: If one of the positive samples of each topic (3 documents per topic) has not been retrieved by the winnow classifier, we promote weights of the words occur in this positive sample by a coefficient of 1.5.</p><p>If an irrelevant document has been retrieved by the winnow classifier, we demoted the words occur in this document by a coefficient of 0.8.</p><p>After that, we set threshold for each winnow classifier through the whole training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Feature selection</head><p>Since the total number of all words is very large and it costs much time in similarity computation, we decide to select some important words from them. First, we carry out morphological analysis and stopword removing. Then we compute the logarithm Mutual Information between remaining words and topics:</p><formula xml:id="formula_0" coords="2,181.72,522.30,309.49,36.52">( ) ( )       = i j i j i w P T w P T w MI | log ) , ( log (1.1)</formula><p>Where, w i is the ith word and T j is the jth topic. Higher logarithm Mutual Information means w i and T j are more relevant. P(w i ) and P(w i |T j ) are both estimated by maximal likelihood method. For each topic, we select those words with logarithm Mutual Information higher than 3.0 and occurs more than once in the relevant documents. Logarithm Mutual Information is not only used as the selection criterion, but also as the weight of feature words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3">Similarity Computation</head><p>The similarity between the profile and training documents is computed by the cosine formula:</p><formula xml:id="formula_1" coords="3,179.08,79.15,312.14,48.37">) )( ( * ) , ( 2 2 ∑ ∑ ∑ = = k jk k ik k jk ik j i p d p d Cos p d Sim θ . (1.2)</formula><p>Where, p j is the profile of the jth topic and d i is the vector representation of the ith document. d ik , the weight of the kth word in d i , is computed as such:</p><formula xml:id="formula_2" coords="3,248.56,156.03,128.67,16.61">) log( 1 dl avdl tf d ik ik * + =</formula><p>, where tf ik is the frequency of the kth word in the ith document , dl is the average number of different tokens in one document, avdl is the average number of tokens in one document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.4">Creating initial profile and Setting initial threshold</head><p>Each topic's feature vector is the weighted sum of feature vector from positive (relevant) documents and feature vector from pseudo relevant documents with the ratio of 1: 0.25. The pseudo relevant documents are acquired during pseudo feedback procedure, which uses the similarity as selection metric. Those documents that have highest similarity and do not occur in the positive documents are regard to be pseudo relevant.</p><p>After combining the positive and pseudo-positive feature vectors, we obtain the initial profile and then set the initial thresholds to get the largest value of T11SU or T11F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Adaptive stage</head><p>Figure <ref type="figure" coords="3,124.08,373.19,4.39,9.50" target="#fig_0">1</ref>.2 shows the architecture for the adaptive stage. While filtering the input document stream, we first use the winnow classifier to make the initial decision. If winnow classifier has retrieved a document, the system will compute the similarity between this document and the feature vector and make final decision based on the threshold. For each document retrieved, we will see whether it is relevant and do some adaptation accordingly. The adaptations including adjusting the weight of winnow classifier and modifying the feature vectors and threshold. The threshold-adjusting algorithm is the heuristic algorithm we presented at TREC10 <ref type="bibr" coords="3,72.04,467.85,26.23,8.10" target="#b9">[Wu01]</ref>. The winnow's weight adjusting is the same as described in 1.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Effect of Winnow classifier and analysis</head><p>To see the effect of winnow classifier, we investigate the words in both winnow classifier and the vectors gotten from training set and find that only about 36% of the words of winnow classifier has occurred in the vectors. Therefore, there must exist many documents on that winnow classifier and VSM will make different decisions. If these documents happen to be irrelevant, combining winnow classifier with normal VSM will enhance the system's precision. We have done several experiment and found that Winnow classifier's recall is very high (0.5137 in training set and 0.4360 in testing set) but it's precision is very low (0.1406 in training set and 0.0823 in testing set). So combining winnow classifier with VSM will lower the system's recall, but will enhance the system's precision at the same time. Since the system's performance is the function of precision and recall and emphasize on precision, combine normal VSM with winnow classifier will enhance system's performance ultimately. The experiment results have confirmed this assumption. Table <ref type="table" coords="3,449.22,659.27,4.39,9.50" target="#tab_1">1</ref>.1 is the experiment results of filtering system using winnow classifier and Table <ref type="table" coords="3,339.46,674.87,4.38,9.50" target="#tab_1">1</ref>.2 without winnow classifier. We can see the efficiency of using winnow classifier. We also conducted experiments to see the effect of dynamic adjusting winnow's weights during adaptive filtering. Table <ref type="table" coords="4,139.83,567.35,4.39,9.50" target="#tab_1">1</ref>.3 and Table <ref type="table" coords="4,203.45,567.35,4.39,9.50" target="#tab_1">1</ref>.4 show the efficiency of adjusting winnow's weights. We also have try the method that add the words of winnow classifier to topic vectors and try different way to assign weight to words, but the results are not satisfactory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Question Answering</head><p>Since the TREC evaluation for Question Answering begins four years ago, quite a few sites joined in this task. The deep NLP method get succeed during the first three years [Harabagiu99], while the shallow method <ref type="bibr" coords="5,72.04,247.89,51.97,8.10" target="#b6">[Soubbotin01]</ref> get even greater success in last year's evaluation. Their success propels us to focus on the shallow NLP method.</p><p>Our QA system can be divided into four modules: pre-processing and indexing (the offline model), question analysis, sentence searching, and answer finding. Moreover, the last module can be divided into two sub-modules: answer extracting &amp; meriting, and confidence calculation &amp; sorting. Among them, we pay much attention on the second and last modules, which can be shown in the next figure. be some knowledge base to tell the system that such kind of question's answer should be person name. Thus, almost every system has integrated more or less special knowledge for the QA task.</p><p>Our knowledge base includes about 80 question classes, which can be used in the Question Analysis model and Answer Extracting model. Each class includes three parts: question patterns, answer types and context templates. Following is an example: One question class may include one or more patterns, and these patterns determine whether a question belongs to this class or not. The question pattern may include normal words and key concepts. Every key concept is combined with ID and Type, which identify and recognize the concept.</p><p>The answer types are common, such as base NP, person name, location, time, etc. The context template is quite like those of [Soubbotin01], but we use concept matching instead of its pure string matching method. We also divide the template into two types of strict order and lenient order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Analysis</head><p>The pre-processing and indexing model is general. We analysis the morphology and delete the stop words based on the POS tagging. Only the nouns, verbs, adjectives, and adverbs are indexed.</p><p>We divide the key word into two categories: words that must appear and optional words, which are abbreviated as MA-Keywords and OA-Keywords. The OA-Keywords are replaceable words such as verbs. Others are all MA-Keywords.</p><p>When a question is presented, its question class is examined. If one question class's question pattern can match the question, the system classifies the question into this question class. Then the question will be divided into several parts by the question pattern. The system only regard the words belong to key concepts as the candidate keywords, while the others will be discarded as stop words. We then use some structures to discard other stop words, such as "the A of B" and if A is the hypernym of B, then A is the stop word. For example, the phrase "the state of Alaska", only "Alaska" will be tagged as keywords. Then we discard the words except noun, verb, adjective and adverb. Finally, we divide the remaining words into MA-keywords and OA-Keywords by the above principle. Then the system acquires the corresponding context patterns and answer types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer extraction and merit</head><p>There is a change in this year's task. Instead of five 50 bytes (or 250 bytes) answer gobbets, every question should return an exact answer. This change requires us paying more attention to answer itself.</p><p>In order to get a high precision and a moderate recall, we limit the answer from both the inside and outside pattern of the answer. We divide the context template and answer type into two categories: the strict and the lenient. When assembling the context template and answer type, every combination is allowed, except both lenient.</p><p>Each term in the context template, including the answer, is a concept. System first locates all the concepts in the coming sentence, then checks if the sentence matches the pattern, and extract the answer if success.</p><p>One answer's score equals the summation of the score of context template, answer type, and sentence match score and concepts match score. S ans =S ct + S at + S sen + ∑ S c Where, S ans is the score of the candidate answer, S ct is the score of context template, S at is the score of answer type, and S c is score of the concepts.</p><p>Concept match is based on keyword matching: S c =∑ S mk / total keywords in the concepts of the question S sen =∑ S mk / total keywords in the question Where, S mk is the score of matched keywords. If two keywords are matched by original or derivation form, S mk is set to 1.0. If keyword in the question is the hypernym of the keyword in the sentence, , S mk is set to 0.8. Otherwise, the concept cannot be matched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Confidence</head><p>As known to all, another dramatic change in this year's task is that the submitted questions should be ranked by confidence. This means that we should be aware of what we have submitted. We use the ME model to determine every answer's confidence <ref type="bibr" coords="7,234.90,470.87,42.09,9.50" target="#b0">[Berger96]</ref>.</p><p>According to our maximum entropy based sorting method, the rank of different answer is determined by several weighted factors, and the confidence score is the product of the exponent of the weights of every factors. The weight of every factor is assigned during the training of TREC-10 questions.</p><p>As for the non-nil answers, five factors are considered, which are: Score (The score of S ans is dependent on the question, and cannot be used to compare the confidence of different questions directly.)</p><p>Step (The answer can be extracted in step1 or step2.</p><p>Step1 use strict answer patterns, while step2 use lenient patterns, so step1 is better.) BestCount (For each question, a lot of answer snippet can be found, some of which are the same. We use boosting algorithm to find the best answer, and the best answer can be find in several places, the number of which is the BestCount.) BestCount/totalCount (totalCount is the total of possible answer snippets.) Answer Type (Person, Place, Time, etc.) As for the nil answers, five factors are considered, which are:</p><p>The number of key words (Key words are those words which are important to answer finding in the question. They are extracted in our question analysis module.)</p><p>The number of key words which cannot be matched between questions and TREC corpus. The total number of question words (key words as well as optional words)</p><p>The number of all the question words, which cannot be matched between questions and TREC corpus. Answer Type Following are some of our experiment results, where the training data are the 500 questions of TREC-10, and the test data are the 500 questions of TREC-11.</p><p>Sorted </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Web Retrieval</head><p>This is year is the second year that we attend TREC Web task. We have submitted five runs for the topic distillation task: fduwt11b0, fduwt11t1, fduwt11o1, fduwt11t2, fduwt11o2. Detailed information of each run is given in the following table. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Architecture</head><p>The Topic Distillation task of this year requires the retrieval system return "key resources" in the .GOV corpus for certain queries. Compared with last year's relevance retrieval task, the amount of "key resources" the system should return is not as many as relevant documents in older task. However, the quality of the key resources is more important than that of older task. Considering this specialty, we modified our original search system to let it return high precision result. The main idea does not change a lot; we use the basic search algorithm to get a set of relevant documents. Then special post processing modules are used to expand and re-sort the base set using title, link structure and anchor text information. The final result set is a set of desired key resources.</p><p>The process of indexing includes: Transform HTML files in the corpus into plain text. At the same time of transformation, we use special module to extract information of links with anchor text for constructing link database later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index the plain text corpus for basic search algorithm Index the link database</head><p>The process of searching includes: Make morphology analysis of queries. Use basic search algorithm to get base set of relevant documents.</p><p>Use post-processing modules to do expansion and re-sorting upon the base set, get the final result of "key resources".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Improved kernel search algorithm</head><p>This year we have made some modifications on the kernel search algorithm based on "shortest extend". The goal is to improve the precision of the first retried documents. We believe this will make it easy for the post-processing module to expand and re-sort.</p><p>We modified the score equation of calculating the shortest extend. The purpose is trying to avoid the situation that extents of queries longer than two words will get extremely low scores.</p><p>For shortest extent (p, q), its the score I(p, q) is</p><formula xml:id="formula_3" coords="9,160.48,318.26,333.50,70.02">a p q K         + - 1 若 q -p + 1 &gt;= K (3.1) 1 若 q -p + 1 &lt;= K</formula><p>in which, we set 'a' to1, K=16 * (length(Q) -1), length(Q) represents the number of the word in the query Q.</p><p>We noticed that the average length of document in the .GOV corpus is longer than that in older corpus. Thus, we altered the method of getting document score. The basic idea is to lower the score of the document if the document length exceeds the average document length. The equation is show as below:</p><formula xml:id="formula_4" coords="9,94.36,466.64,396.88,76.15">length(D) N AVG_DOC_LE * S(D) S(D) N) AVG_DOC_LE (length(D) if ) , ( 1 * ) , ) , ( ( ) ( = &gt; ∑ ∈ = i q i p w i q D qi pi i p I D S (3.2)</formula><p>in which, length(D) represents the number of the word in document D. AVG_DOC_LEN represents the average length of documents in the corpus, also counted in word number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Post process module</head><p>We use two ways to expand and re-sort the base set of relevant documents. Details are given in the following. 1) Use structured information to improve expand base set Words in different parts of a document have different importance. For example, Words in title are more important than words in content; words of large font are more important than words of small font; words of bold or underlined font are more important than words of normal font. In this year's web track, we used title = ) , ( q p I information to improve our retrieval result.</p><p>We increase the score of a document if the title of this document includes retrieval words. The increment coefficient is between 0 and 1:</p><formula xml:id="formula_5" coords="10,147.64,122.84,337.93,25.08">score_title d =( count_of_match count_of_retrieval_words ) γ (3.3)</formula><p>in which, γ is a parameter greater than 1. 2) Use breadth-first traverse algorithm to find out key resource By breadth-first traverse algorithm, we access all pages of a domain from the homepage of the domain. Then we construct a tree, the root of which is the homepage of the domain.</p><p>Our retrieval procedure has two steps:</p><p>Step 1: Retrieve a batch of documents using our text-only search engine. Every document retrieved is given a score. This score of document d is called score_ text d .</p><p>Step 2: Calculate key resource score of a document score_KR</p><formula xml:id="formula_6" coords="10,148.96,283.81,335.20,29.31">score_KR i = ∑ j∈subtree(i) score_text j γ Δlevel(i,j) (3.4)</formula><p>in whick, γ is parameter greater than 1.</p><p>3) Synthesize score_ text, score_title, score_KR final_score d =α* score_ text d +β* score_title d +γ* score_KR d then we re-sort documents on final_score, rettun the top n documents as our result.</p><p>Because the task of "Topic Distillation" is a totally new task, we cannot use data in the past task for training. We have made a human tagging system to let several students tagging our experiments result at the same time. In this way, we can get some feedback from human.</p><p>Experiment results show that the above algorithm can lead to satisfactory results. The average P@10, P@20, P@30 of 49 queries are 0.45, 0.31, 0.26 respectively, while the average median precision is 0.11. 0.09, 0.08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Video Track</head><p>On Video Track of this year, we paticipated in Shot Segmentation, Feature Extraction and Search task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shot Segmentation</head><p>This year we use most parts of TREC-10 Shot Segmentation System <ref type="bibr" coords="10,406.00,569.37,26.13,8.10" target="#b9">[Wu01]</ref>. FFD (Frame-to-Frame Difference) calculated by Luminance Difference and Color Histogram Similarity are used to detect the Shot Changes. We use two thresholds C θ and G θ , which are calculated automatically according to the FFD value histogram in 500 frames, to detect if there is a clear FFD value change caused by Shot Changes. Then Flashlight Detection and Motion Detection are applied for candidate Shot Changes to remove the false alarms of Cut and Gradual. The parameters used in the system are trained and adjusted based on the TREC-10 Video Library. According to the performance on TREC-10 Video Library, we selected the system parameters to generate the submissions. We add Fade In/Out Detection into our system this year although Shot Segmentation task does not include it. In our submissions, Run02, Run09 and Run10 include Fade Detection. In our system, Fade In/Out Detection is applied to all candidate Gradual Changes. If a black screen chain exists in the candidate duration, we think it is a Fade. Otherwise, it will be labeled as Dissolve. In order to detect the black screen and get the accurate boundary of Fade, a frame is split into several blocks whose size is 8×8 (pixels Evaluation shows that our system has a good balance between precision and recall. Comparing F-Value, the rank of our best result for all the changes, Cut Changes and Gradual Changes is 3, 3 and 9 (out of 54 systems). On Gradual Accuracy, frame-recall of our system is better than frame-precision. Comparing with other submitted systems, our system located at the middle on Gradual Accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Extraction</head><p>According to the FE Task of this year, we developed a new Video Feature Extraction System. It consists of five sub-systems: Outdoor / Indoor Detection, Cityscape / Landscape Detection, Face / People Detection, Text Detection and Speech / Music / Monologue Detection. In each sub-system, a value calculated by whatever methods and features is used for ranking. In the following part, we call it "Ranking Value". Evaluation shows that our system works well on these features: Cityscape, Landscape, Indoor and Music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Outdoor / Indoor Detection</head><p>In our Outdoor / Indoor System, we use K-Nearest Neighbor Classifier. Considering the difference between Outdoor and Indoor, we select Color Histogram and Edge Direction Histogram as the feature <ref type="bibr" coords="12,72.04,130.17,47.85,8.10" target="#b7">[Szummer98]</ref>. A 512-bin color histogram is calculated in Color Space T=R-B. The Edge Direction is obtained by calculating the ratio of the grades on vertical direction and horizontal direction at every edge point. The edge point is got by Canny Edge Detector. In the Edge Direction Histogram, all the directions are separated as 12 bins.</p><p>The training set includes 600 indoor images and 1300 outdoor images. They are selected from Feature Dev Set. For each shot in Feature Test Set, we only apply the classifier to the selected keyframe for a shot. In Run01, the keyframe are divided into 4×4 blocks. The histogram distance is calculated for each block. After summing with weights, we will get the distance for whole keyframe. On the contrary, we only calculate the distance on whole keyframe in Run02. In each run, the Ranking Value is minimum distance between keyframe and the training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Cityscape / Landscape Detection</head><p>Similar with Outdoor / Indoor System, we also use K-Nearest Neighbor classifier in Cityscape / Landscape Classification. However, we only select Edge Direction Histogram as the feature in this system <ref type="bibr" coords="12,72.04,344.97,41.14,8.10" target="#b8">[Vailaya98]</ref>.</p><p>The training set includes 410 cityscape images and 250 landscape images. They are also selected from Feature Dev Set. We think all the cityscape and landscape image should be outdoor image at first. Therefore, the outdoor Ranking Value calculated in Outdoor / Indoor Detection will multiply with the maximum distance output by Cityscape / Landscape Classifier. The product is the Ranking Value in this system. In the submissions, Run01 use the Ranking Value of Outdoor/Indoor Detection Run01 and Run02 use the Ranking Value of Outdoor/Indoor Detection Run02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Face / People Detection</head><p>This system uses the same idea of TREC-10. The method consists of three steps: Skin-Color based Segmentation, Motion Segmentation, and Shape Filtering. Considering the difference between the TREC-10 Video and TREC-11 Video, some new training data selected from Feature Dev Set are added into the Skin-Color template.</p><p>We use the following equation to calculate the Ranking Value for each shot: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Text Detection</head><p>Same with last year's work, there are still three main parts in our Video Text Detection System: Text Block Detection, Text Enhancement and Binarization. In order to reduce the false alarms, we combine Neural Network <ref type="bibr" coords="13,111.52,77.37,23.17,8.10" target="#b2">[Li00]</ref> as a preprocessing in our Text Block Detection.</p><p>System applies 2-level Harr Wavelet Decomposition on the image. The image will be split as four sub-bands at each level: HH, HL, LH and LL. For each N×N window, we can calculate three features: The output of Neural Network is a confidence between 0 and 1. The more the confidence approaches 1, the more possible the window is classified as text. A threshold is used to determine whether it is a text or non-text. We select three-layer BP Neural Network as a classifier to identify text regions. Bootstrap method is used in training. The training data come from Feature Dev Set and TREC-10 Video Library.</p><formula xml:id="formula_7" coords="13,155.20,125.40,315.02,79.69">∑∑ - = - = = 1 0 1 0 2 ) , ( 1 ) ( N i N j j i I N I E (4.6) ∑∑ - = - = - = 1 0 1 0 2 2 2 )) ( ) , (<label>( 1 )</label></formula><formula xml:id="formula_8" coords="13,155.08,174.49,315.13,77.40">( N i N j I E j i I N I µ (4.7) ∑∑ - = - = - = 1 0 1 0 3 2 3 )) ( ) , (<label>( 1 )</label></formula><p>For each shot, the system processes only one frame in every ten. On each processed frame, we use a small window (16×16 pixels) to scan the image and classify each window as text or non-text using trained neural network. When scanning the image, we move the window 4 pixels at a time. If a window is classified as text, all the pixels in this window are labeled as text. Those pixels which are not covered by any text window are labeled as non-text. Then we generate a binary map from original image. The following Text Block Detection is only applied in the region labeled as text. The experiments show that the text detection precision increases about 30% while the recall almost does not decrease.</p><p>We average the output values generated by Neural Network for all the small windows in one frame. This average value is the confidence that a single frame contain text region. We select the maximum frame confidence of all the frames in a shot as the ranking value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Speech / Music / Monologue Detection</head><p>Speech/Music Classification is applied on the 1-second window. The features we used include: Mean and Covariance of Zero-crossing Rate, High Zero-crossing Rate Ratio, Mean and Covariance of Short Time Energy, Low Short Time Energy Ratio, Noise Frame Ratio, Mean and Covariance of Brightness, Spectral Flux, Spectral Roll-off Point, Mean and Covariance of LPC, Mean and Covariance of MFCC, Mean and Covariance of Pitch, Mean and Covariance of Band Spectrum, Mean and Covariance of Band Width [Lu01][ Scheirer97].</p><p>Nearest Neighbor Model and Gaussian Mixture Model are trained by TREC-10 Videos. Applying these trained models on 1-second window, we can get the type of each window. In our submission, Run01 uses NN Model and Run02 uses 16-mixture GMM Model.</p><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Search</head><p>We have submitted four runs in Search Task. Considering the difficulty of search topics, not all of the topics are processed in each run.</p><p>The whole architecture of the Search system is almost same with last year [Wu01]. However, there are some improvements in Face Recognition and Object Search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Face Recognition</head><p>In TREC-11 Search Task, there are four topics concerning about a certain people. Face Recognition is the basis for such topics.</p><p>The eigen-face and Fisher method are commonly used in face recognition especially when the person set is completely known. However, in most cases of video retrieval the complete person set cannot be acquired. This year we tried a fast manifold-based approach to face recognition in TREC-11 Search Task. It can be used when there are only few different images of a specific person and runs fast.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,93.04,536.27,447.16,9.50;1,72.04,551.87,161.99,9.50"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1.1 shows the architecture of the training stage, which includes topic processing, feature vectors extracting and initial threshold setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,233.80,362.49,144.65,8.10"><head>Figure 1 . 1</head><label>11</label><figDesc>Figure 1.1 Architecture of training stage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,260.32,600.69,109.58,8.10;5,185.44,614.73,259.38,8.10;5,72.04,637.70,207.16,11.10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.1 QA online modoles From left to right: question analysis, sentence searching, answer finding 2.1 Building the QA knowledge base</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,167.80,230.11,4.00,10.81;13,214.72,221.29,4.67,6.31;13,215.56,245.05,1.95,6.31;13,229.12,221.29,4.67,6.31;13,230.56,245.05,1.95,6.31;13,299.56,230.11,4.00,10.81;13,286.84,230.11,7.34,10.81;13,267.16,230.11,3.34,10.81;13,256.84,230.11,3.34,10.81;13,247.48,230.11,4.00,10.81;13,196.36,239.71,8.01,10.81;13,172.48,230.11,4.00,10.81;13,155.08,225.62,6.92,16.38;13,450.04,231.11,20.17,9.50;13,90.04,261.77,450.22,11.36;13,72.04,279.11,148.87,9.50"><head></head><label></label><figDesc>can get 24 (2 [level] ×4 [sub-band] × 3 [feature] = 24) features. Eight features are selected as the input of the Neural Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,93.04,388.67,13.98,9.50"><head>Let</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="14,199.24,385.69,3.50,6.31;14,172.72,394.09,3.50,6.31;14,190.72,387.69,7.32,10.79;14,163.60,387.69,7.32,10.79;14,128.20,387.69,7.32,10.79;14,109.84,387.69,7.32,10.79;14,179.80,383.77,8.55,15.62;14,158.20,387.69,3.00,10.79;14,142.36,387.69,3.00,10.79;14,122.80,387.69,3.00,10.79;14,137.32,394.11,3.49,6.29;14,118.24,394.11,3.49,6.29;14,145.72,386.90,12.00,11.97;14,206.08,388.67,334.11,9.50;14,72.04,412.55,116.42,9.50;14,220.84,411.61,5.97,10.74;14,195.52,407.71,22.03,15.55;14,93.04,442.67,30.49,9.50;14,165.28,439.28,12.84,17.98;14,169.72,454.34,3.84,9.11;14,285.28,437.95,6.55,15.55;14,269.32,437.95,6.55,15.55;14,237.64,437.95,6.55,15.55;14,211.84,437.95,6.55,15.55;14,145.96,437.95,6.55,15.55;14,169.96,432.97,3.50,6.31;14,167.20,456.61,1.95,6.31;14,231.52,448.09,1.95,6.31;14,205.00,448.09,1.95,6.31;14,189.64,448.09,1.95,6.31;14,295.00,441.85,5.97,10.74;14,278.68,441.85,3.32,10.74;14,247.60,441.85,7.29,10.74;14,222.52,441.85,7.29,10.74;14,198.64,441.85,6.63,10.74;14,180.64,441.85,7.29,10.74;14,156.76,451.21,5.97,10.74;14,133.60,441.85,7.29,10.74;14,173.20,456.62,3.50,6.29;14,261.52,441.81,5.99,10.79;14,256.84,441.81,3.00,10.79;14,193.36,441.81,3.00,10.79;14,156.64,434.25,5.99,10.79;14,310.24,442.67,153.63,9.50;14,472.36,433.88,18.99,20.64;14,482.20,448.09,1.95,6.31;14,475.84,441.67,64.24,10.81;14,98.32,471.43,5.67,23.90;14,204.88,471.43,5.67,23.90;14,198.04,481.75,6.01,10.81;14,177.40,481.75,6.68,10.81;14,164.08,481.75,6.68,10.81;14,144.16,481.75,7.34,10.81;14,121.00,481.75,7.34,10.81;14,103.00,481.75,7.34,10.81;14,74.08,481.75,10.01,10.81;14,187.36,477.83,8.57,15.65;14,155.68,477.83,6.60,15.65;14,133.36,477.83,6.60,15.65;14,89.08,477.83,6.60,15.65;14,172.24,481.75,3.00,10.81;14,114.52,481.75,3.34,10.81;14,215.32,482.75,77.80,9.50;14,93.04,513.95,30.49,9.50"><head></head><label></label><figDesc>of images of a specific person. Usually n is small and the dimension d is very big, i.e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,133.60,630.21,344.79,72.42"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="4,133.60,630.21,344.79,72.42"><row><cell></cell><cell cols="4">.3 Experiment results of filtering without adjusting the winnow weight</cell></row><row><cell>static winnow</cell><cell>Recall</cell><cell>Precision</cell><cell>T11F</cell><cell>T11SU</cell></row><row><cell>Total</cell><cell>0.1387</cell><cell>0.2471</cell><cell>0.1741</cell><cell>0.2484</cell></row><row><cell>R101 ~ R150</cell><cell>0.2257</cell><cell>0.4461</cell><cell>0.3064</cell><cell>0.3556</cell></row><row><cell>R151 ~ R200</cell><cell>0.0517</cell><cell>0.0481</cell><cell>0.0417</cell><cell>0.1412</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,72.04,201.11,468.22,103.11"><head></head><label></label><figDesc>by question id: 0.315 (this is the baseline) Four factors (open test, answer type is not considered): 0.461 Five factors (open test): 0.434 Four factors (closed test): 0.498 Five factors (closed test): 0.489 Therefore, our maximum entropy method does significantly better than the baseline method. In addition, only consider the first four factors is better.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,104.56,400.77,399.91,104.70"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="8,288.70,400.77,81.66,8.10"><row><cell>.1 Web runs submitted</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,72.04,123.23,468.29,414.99"><head></head><label></label><figDesc>which has the maximum Black Value between Gradual Start Frame and Black Screen Start Point. Black Increase Point: the frame n which has the maximum Black Value between Black Screen End Point and Gradual End Frame. If Black Screen Start Point is equal to the Gradual Start Frame, we regard it Fade In. It starts from Black Screen Start Point and ends at Black Increase Point. On the contrary, if Black Screen End Point is equal to the Gradual End Frame, a Fade Out, which starts from Black Decrease Point and ends at Gradual End Frame, is detected. The third condition is that the black screen chain located in the middle of candidate duration. At this time, a Fade Out followed by a Fade In will be detected. It starts at Black Decrease Point and ends at Black Increase Point.</figDesc><table coords="11,74.20,123.23,465.97,252.21"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>). Maximum Luminance</cell></row><row><cell cols="7">Lum ax m</cell><cell>( ) n</cell><cell cols="5">and Black Value</cell><cell>Black</cell><cell>( ) n</cell><cell>are calculated for each frame n in the candidate duration.</cell></row><row><cell>B</cell><cell cols="2">m</cell><cell>ax</cell><cell cols="2">=</cell><cell cols="4">{ block</cell><cell cols="2">i 1</cell><cell>,</cell><cell>,</cell><cell>( ) set are n ax m } Lum block i 10 = ∑ ten of B Lum blocks inance with ( ) ximum ma k n block ax m k , ∈</cell><cell>inance Lum</cell><cell>value</cell><cell>(4.1)</cell></row><row><cell cols="2">B m</cell><cell cols="2">in</cell><cell>=</cell><cell cols="4">{ block</cell><cell cols="2">i 1</cell><cell>,</cell><cell>,</cell><cell>( ) set are n in m } Lum block i 10 = ∑ ten of B blocks inance with Lum block in m k ∈</cell><cell>( ) nimum mi k n ,</cell><cell>inance Lum</cell><cell>value</cell><cell>(4.2)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Black</cell><cell>( ) n</cell><cell>=</cell><cell>10 Lum ax m</cell><cell>( ) n</cell><cell>×</cell><cell>in Lum m Lum m</cell><cell>( ) ( ) 1 + n n ax</cell><cell>(4.3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="10">Then system finds four points in the candidate duration:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Black Screen Start Point: the first frame n satisfies</cell><cell>Lum</cell><cell>m</cell><cell>ax</cell><cell>( ) black th n &lt;</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Black Screen End Point: the first frame n satisfies</cell><cell>Lum</cell><cell>m</cell><cell>ax</cell><cell>( ) black th n ≥</cell></row></table><note coords="11,412.72,363.71,2.64,9.50;11,111.04,388.31,143.76,9.50"><p>. Black Decrease Point: the frame n</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,109.09,665.51,361.12,48.57"><head></head><label></label><figDesc>Ranking Value of speech, music and monologue are calculated by:</figDesc><table coords="13,137.20,686.25,333.02,27.83"><row><cell>Ranking</cell><cell>Value</cell><cell>music</cell><cell></cell><cell>=</cell><cell cols="2">#</cell><cell cols="3">of</cell><cell cols="3">shot music is type a in whose windows windows of #</cell><cell>(4.10)</cell></row><row><cell>Ranking</cell><cell>Value</cell><cell>monol</cell><cell cols="3">ogue</cell><cell cols="3">=</cell><cell cols="2">ue RankingVal</cell><cell>speech</cell><cell>×</cell><cell>RankingVal</cell><cell>ue</cell><cell>face</cell><cell>(4.11)</cell></row><row><cell>Ranking</cell><cell>Value</cell><cell cols="2">speech</cell><cell>=</cell><cell></cell><cell cols="2">#</cell><cell cols="2">of</cell><cell cols="3">shot speech is type a in whose windows windows of #</cell><cell>(4.9)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was partly supported by <rs type="funder">NSF of China</rs> under contracts of <rs type="grantNumber">69935010</rs> and <rs type="grantNumber">60103014</rs>, as well as the 863 National High-tech Promotion Project of China under contracts of <rs type="grantNumber">2001AA114120</rs> and <rs type="grantNumber">2002AA142090</rs>. We are thankful to <rs type="person">Lin Mei</rs>, <rs type="person">Yuefei Guo</rs>, <rs type="person">Sitong Huo</rs>, <rs type="person">Yi Zheng</rs>, <rs type="person">Xin Li</rs>, <rs type="person">Kaijiang Chen</rs>, <rs type="person">Xiaoye Lu</rs>, <rs type="person">Jie Xi</rs>, <rs type="person">He Ren</rs>, <rs type="person">Li Lian</rs>, <rs type="person">Wei Qian</rs>, <rs type="person">Hua Wan</rs>, <rs type="person">Tian Hu</rs>, <rs type="person">Jiayin Ge</rs>, <rs type="person">Jian Gu</rs>, <rs type="person">Danlan Zhou</rs>, <rs type="person">Zhiyan Tang</rs>, <rs type="person">Xipeng Qiu</rs>, <rs type="person">Lan You</rs>, <rs type="person">Lin Zhao</rs>, <rs type="person">Rongrong Wang</rs>, <rs type="person">Min Jin</rs>, <rs type="person">Jiawei Rong</rs>, <rs type="person">Wanjun Jin</rs> for their help in the implementation.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gyVUjFp">
					<idno type="grant-number">69935010</idno>
				</org>
				<org type="funding" xml:id="_U7JKAyh">
					<idno type="grant-number">60103014</idno>
				</org>
				<org type="funding" xml:id="_SGwmxys">
					<idno type="grant-number">2001AA114120</idno>
				</org>
				<org type="funding" xml:id="_sPUXHUt">
					<idno type="grant-number">2002AA142090</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⊥</head><p>S the orthogonal complement subspace of S. It is easy to see that if</p><p>, where ( )</p><p>is the projection of X on S.</p><p>Based on observation above, we scan the input image with sub-windows of different positions and levels of the image pyramid. Then the distance between the scanning window and the mean of samples in ⊥ S is calculated. The sub-window with the minimum distance may include the person we search for if the minimum distance is below a threshold.</p><p>Experiment shows this approach works quite well even in case where there are only few samples. However, it is quite time-consuming.</p><p>To speed up, we collect a large number of non-face samples</p><p>, where m is very big.  ) th X X P Z &gt; -. Since k is much smaller than d-n, this filtering step is much faster. In our experiment, applying this step will make the face recognition 5-fold faster and with almost no decreasing of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denote</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and the dimension of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Color Histogram Comparison</head><p>Color Histogram similarity is used to compare the Image Example and Key Frame of each shot. It will provide us the similarity on the image between Image Example and Key Frame. We calculate the histogram in RGB and YUV space. During the calculation and comparison, two modes are used:</p><p>Whole Image Mode: For both Key Frame and Image Example, the histogram is calculated on the whole image.</p><p>Block mode: For Key Frame, we split it into several blocks with different size. The histogram is calculated on each block. For Image Example, the histogram is calculated on the whole image. Then the histogram comparison is processed between the histogram of each block and image example. The maximum similarity will be selected as the final similarity. In searching, Block mode is used on the topics which is concerning about a certain object. Such as parrots, butterfly and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Searching</head><p>For each topic, we combine the similarities come from different modules. Such as Face Recognition, Text Recognition, Color Histogram Comparison, ASR Text etc.</p><p>In our submission, Sys1 only use the information get by our own search modules. There is no ASR Text and Feature Extraction results are used. However, Feature Extraction Confidence is useful for some topics. For example, Face Confidence is useful to certain people searching and Cityscape Confidence is useful for Topic 86. So in Sys2 and Sys3, we combined feature extraction confidence into the searching. Sys2 use our own Feature Extraction results and Sys3 use the reference Feature Extraction results provided by IBM and MediaMill. In Sys4, we combine the ASR Results provided by LIMSI. We select some keywords manually for each topic. These selected keywords are used for Text Retrieval on ASR Results.</p><p>NIST's evaluation shows that our searching system is not efficient in several topics. In the future work, we should pay more attention on Image Similarity calculation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="16,136.38,196.89,403.90,8.10;16,136.36,214.89,111.43,8.10" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Adam</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietra</forename></persName>
		</author>
		<title level="m" coord="16,384.28,196.89,155.99,8.10;16,136.36,214.89,76.79,8.10">A Maximum Entropy Approach to Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,136.37,232.89,403.94,8.10;16,136.36,250.89,185.60,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,277.60,232.89,188.10,8.10">FALCON: Boosting Knowledge for Answer Engines</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,472.36,232.89,67.95,8.10;16,136.36,250.89,119.20,8.10">Proceeding of The Eighth Text Retrieval Conference</title>
		<meeting>eeding of The Eighth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1999-11">November, 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,136.35,268.89,403.95,8.10;16,136.36,286.89,278.22,8.10" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="16,184.96,268.89,228.44,8.10">Automatic Processing and Analysis of Text in Digital Video</title>
		<author>
			<persName coords=""><forename type="first">Huiping</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
		<respStmt>
			<orgName>Center for Automation Research, University of Maryland College Park</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="16,136.38,304.89,403.81,8.10;16,136.36,322.89,99.41,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,188.68,304.89,313.37,8.10">Learning quickly when irrelevant attributes abound: a new linear threshold algorithm</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Littlestone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,508.61,304.89,31.59,8.10;16,136.36,322.89,30.82,8.10">Machine Learning</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="285" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,136.34,340.89,403.99,8.10;16,136.36,358.89,297.21,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,287.44,340.89,204.18,8.10">A Robust Audio Classification and Segmentation Method</title>
		<author>
			<persName coords=""><forename type="first">Lie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,498.17,340.89,42.15,8.10;16,136.36,358.89,224.92,8.10">Proc. of the 9th ACM International Multimedia Conference and Exhibition</title>
		<meeting>of the 9th ACM International Multimedia Conference and Exhibition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="103" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,136.32,376.89,404.05,8.10;16,136.36,394.89,223.44,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,235.84,376.89,300.73,8.10">Construction and Evaluation of a Robust Multifeature Music/Speech Discriminator</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,136.36,394.89,71.46,8.10">Proc. of ICASSP&apos;97</title>
		<meeting>of ICASSP&apos;97</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997-04">April 1997</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="1331" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,136.28,412.89,392.05,8.10;16,136.36,430.89,248.34,8.10" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<title level="m" coord="16,265.48,412.89,262.85,8.10;16,136.36,430.89,181.89,8.10">Patterns of Potential Answer Expressions as Clues to the Right Answers, Proceeding of The tenth Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2001-11">November, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,136.34,448.89,403.81,8.10;16,136.36,466.89,287.47,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,206.68,448.89,138.68,8.10">Indoor-Outdoor Image Classification</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,353.56,448.89,186.59,8.10;16,136.36,466.89,245.94,8.10">IEEE International Workshkop on Content-based Access of Image and Video Databases, in conjunction with ICCV&apos;98</title>
		<imprint>
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,136.31,484.89,403.79,8.10;16,136.36,502.89,167.35,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,308.08,484.89,198.95,8.10">On Image Classification: City Images vs. Landscapes</title>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Vailaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,514.59,484.89,25.51,8.10;16,136.36,502.89,42.61,8.10">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1921" to="1936" />
			<date type="published" when="1998-12">Dec. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,136.32,520.89,403.94,8.10;16,136.36,538.89,106.13,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,192.04,520.89,202.97,8.10">FDU at TREC-10: Filtering, Q&amp;A, Web and Video Tasks</title>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,400.84,520.89,139.42,8.10;16,136.36,538.89,39.67,8.10">Proceeding of The tenth Text Retrieval Conference</title>
		<meeting>eeding of The tenth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001-11">November, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
