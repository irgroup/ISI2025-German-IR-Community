<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.74,74.70,262.09,12.99;1,89.60,98.82,432.50,12.99">CLARIT Experiments in Batch Filtering: Term Selection and Threshold Optimization in IR and SVM Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,95.36,136.30,76.37,10.16"><forename type="first">David</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.72,136.30,86.85,10.16"><forename type="first">James</forename><surname>Shanahan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.00,136.30,69.42,10.16"><forename type="first">Norbert</forename><surname>Roma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.19,136.30,77.69,10.16"><forename type="first">Jeffrey</forename><surname>Bennett</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,442.44,136.30,68.97,10.16"><forename type="first">Victor</forename><surname>Sheftel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,136.94,155.20,66.26,10.16"><forename type="first">Emilia</forename><surname>Stoica</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.40,155.20,96.31,10.16"><forename type="first">Jesse</forename><surname>Montgomery</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.23,155.20,66.01,10.16"><forename type="first">David</forename><forename type="middle">A</forename><surname>Hull</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,391.06,155.20,83.74,10.16"><forename type="first">Waibhav</forename><surname>Tembe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.74,74.70,262.09,12.99;1,89.60,98.82,432.50,12.99">CLARIT Experiments in Batch Filtering: Term Selection and Threshold Optimization in IR and SVM Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">46B06C38613D4C80DFE169DAEB0EC867</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Clairvoyance team participated in the Filtering Track, submitting two runs in the Batch Filtering category. While we have been exploring the question of both topic modeling and ensemble filter construction (as in our previous TREC filtering experiments <ref type="bibr" coords="1,243.48,286.58,9.38,8.32" target="#b4">[5]</ref>), we had one distinct objective this year, to explore the viability of monolithic filters in classification-like tasks. This is appropriate to our work, in part, because monolithic filters are a crucial starting point for ensemble filtering, and it is possible for them to contribute substantially in the ensemble approach. Our primary goal in experiments this year, thus, was to explore two issues in monolithic filter construction: (1) term count selection and (2) filter threshold optimization.</p><p>In fact, our pre-TREC experiments were conducted in a brief period and we were unable to complete all the tests we had planned. Our official submissions reflect essentially our first, baseline results. They are overall poor in comparison to other results reported this year.</p><p>However, an additional focus of our work relates to the general problem of exploiting training data, in particular, where there are only a few positive-example documents for a topic. We regard such cases as more realistic (e.g., in commercial settings) than the categorizationoriented tasks we have seen in TREC filtering in the past, e.g., based on the Reuters collection. Thus, in a series of follow-up experiments, we explored the strengths and limitations of classifier-based approaches (using kernel methods) and CLARIT-IR-based ones on the fifty TREC-2002 "Assessor" Topics.</p><p>In our CLARIT-IR-based experiments, we aimed to establish a more accurate baseline than the one reflected in our official submissions. We also sought to vary the term-extraction techniques we used, to optimize performance on a topic-by-topic basis.</p><p>In our kernel-based (SVM) experiments, we used nonmathematical (non-QP) based approaches to learning SVMs. We used both NLP-based features and simple white-space-delimited ones; and we developed a preliminary approach to thresholding the classifier margin.</p><p>In the following sections we first describe our official submitted runs and results and then present in greater detail the post-TREC experiments that we conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Official Batch Filtering Runs</head><p>Our official batch filtering runs reflected a straightforward extraction of term vectors from positive training documents, the setting of thresholds based on calibration of the term vectors over the training data, and the use of the term vectors to score (retrieve/rank) documents in the test collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preparing Filters and Testing</head><p>As a general approach to handling the available training data, we divided the training corpus equally into two parts, one half of which we used for constructing filters (i.e., extracting terms, assigning weights, and determining the optimal term profile cutoff), the other half of which we used for validation (including score-threshold setting). We constructed monolithic filters for each topic automatically, based on the positive examples of each topic. In fact, we used a slightly modified version of the training database: we added two additional, identical positive example "documents" for each topic. These were created by the system from the topic's title and its short and long descriptions, with all meta-language ("Retrieve documents which…," "Find documents that…") automatically removed. We chose to add these artificial documents to increase the number of training documents and to emphasize terms that we anticipated would be especially useful in the filter.</p><p>Terms for all topics in this combination of positive examples and artificial documents were generated by CLARIT NLP (yielding morphologically normalized single words, phrases, and sub-phrases), then weighted, ranked, and selected for extraction (to represent the term profile in the filter) using our thesaurus extraction method "Prob2" (as given in Figure <ref type="figure" coords="1,348.37,659.18,3.52,8.33">1</ref>). We used Prob2 as our default and only term-extraction method based, in part, on our observations of Prob2's overall robust performance compared to other term-extraction methods in our TREC 2001 experiments. While keeping the method of selecting terms for topics constant, we experimented with optimizing the number of terms for a given topic.</p><p>We investigated several techniques for determining how many terms to include in the filter for a given topic. We settled on a method based on the 2 nd derivative of a topic's term weight profile (w″). In short, this approach examines a set of terms ranked according to term weight, and disregards all terms occurring after the point where the term weight profile begins to level off. This point is determined by the condition 0 &gt; w″ &gt; ε. (In our case, we set ε to 0.01.) In this way, all terms that did not show evidence of being particularly characteristic of a topic (according to their rank in the term weight profile) were disregarded. We applied this method of term count selection to construct filters for each topic. We imposed the additional condition that no topic filter use fewer than five terms. For each of our submitted runs, the average number of terms in a filter was 26, and the maximum was 104.</p><p>Once we established which terms (and how many) to use in constructing a filter, it remained for us to determine the threshold for each topic. This was one of the chief issues we wanted to explore, and so we took a different approach for each of our submissions. In our runs CCT11BFC and CCT11BFD, the filter was applied to the entire training corpus. That is, the threshold was optimized over both the first half of the corpus, which we used for constructing the filter, as well as the second half, which had thus far been unused. For CCT11BFC, the filter's threshold was set using the beta-gamma method on normalized linear utility, T11SU, to decrease the likelihood that we would over-fit the training data. (Cf. <ref type="bibr" coords="2,89.05,425.72,27.53,8.32">[13;14]</ref> for discussion of the beta-gamma threshold setting method.) For this run, beta-gamma values were 0.1 and 0.4, respectively. To further decrease the possibility of over-fitting the training data, we employed an additional "global threshold multiplier" that relaxed the optimal threshold a bit further. For run CCT11BFC, this global multiplier was set to 0.95.</p><p>Our other submission, CCT11BFD, was identical to CCT11BFC, with two exceptions. For this run we employed no beta-gamma regulation at all, and instead lowered the global threshold multiplier to 0.85. Finally, all filters for both CCT11BFC and CCT11BFD were run on the full testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Official Test Results</head><p>Table <ref type="table" coords="2,97.10,601.58,5.00,8.33" target="#tab_1">1</ref> presents a summary of various batch filtering runs in terms of normalized linear utility (T11SU) and F-Beta. Row one of this table gives the median of all submitted runs from all groups for TREC-2002 batch filtering. The second and third rows summarize the results for our two submitted runs. The remaining rows show results for other unofficial runs we completed, including an Adatron SVM <ref type="bibr" coords="2,178.48,674.12,30.26,8.33">[1;6;11]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Observations on Official Runs</head><p>On the whole, our official results were unsatisfactory.</p><p>Our failure to perform well may have been due to several factors. We may have selected terms poorly for various topics, and therefore had a poor characterization of these topics. It was also possible that we chose appropriate terms, but too many or too few of them. Finally, we may have correctly chosen our terms and term counts, and still have performed poorly on various topics due to poor thresholding. We did several analyses to see which of these factors actually was responsible for our weak performance.</p><p>As we investigated topics where we performed poorly, we saw little indication that we had grossly erred in our method of choosing which terms to extract from positive examples. Thus, our decision to use a single feature-extraction method (Prob2) that had performed well and robustly in the past does not seem to have harmed our effort significantly.</p><formula xml:id="formula_0" coords="2,329.36,80.38,227.59,23.60">        - + - - + - + - + = 1) R 1 R log( 1) 1 R N 2 R N log( x 1) log(R Prob2(t) t t t t</formula><p>N is the number of documents in the (reference) corpus; N t is the number of documents in the (reference) corpus that contain term t; R is the number of documents (for training or feedback) that are relevant to the topic; R t is the number of documents (for training or feedback) that are relevant to the topic and contain term t; TF is the (raw) frequency of term t in a document; and NTF is the normalized frequency of term t in a document.</p><formula xml:id="formula_1" coords="2,330.44,120.49,157.16,120.06">R (t) NTF x IDF(t) Rocchio(t) DocSet D D ∑ ∈ = R (t) TF x IDF(t) t) RocchioFQ( DocSet D D ∑ ∈ = 2 t t t ) N (R xN 4xR GL2(t) + =</formula><p>Additionally, there was little evidence that our term count optimization was faulty. We conducted postsubmission experiments where we added terms to (poorly performing) topic profiles in which we had originally used relatively few terms. We likewise did experiments where we removed terms from topic profiles in which we had originally used many terms. In neither case did we see a dramatic change in the performance of filters upon the addition or removal of terms from the profile.</p><p>We did see, however, that setting filter thresholds improperly had a remarkable impact upon a filter's performance. In particular, we observed that for many of the topics where we performed poorly, we had set the filter threshold much too low, thereby allowing for the retrieval of many non-relevant documents. Results of our post-submission experiments indicate that the negative effects of poor thresholding far outweigh the positive effects of good term and term count selection.</p><p>We saw this principle at work, for example, in Topic 144, Mountain Climbing Deaths, which was one of the topics on which we performed extremely poorly. Upon examining the actual terms (and number of terms) in the profile, we could see that they were a fair characterization of the topic. Our recall figure was quite high (0.965-we retrieved 55 of 57 total relevant), and initial precision was quite high: roughly the first third of the documents we retrieved were relevant-a good indication that our terms and term weights were on target. The explanation for such poor performance, then, can be found in our set precision figure (0.026): we retrieved far too many non-relevant documents (2048 out of 2104). Furthermore, we observed the positive effect that conservative thresholding can have in overcoming the lesser negative effects of poorly chosen terms or term counts. We saw this in some topics where we performed quite well in comparison to the TREC median (T11SU), even though many of the highly ranked documents were not relevant. Our good performance (relative to the median) is likely the result of choosing terms that were at least adequate, and, especially, having a threshold that was conservative enough to prevent over-delivery of non-relevant documents. Topic 122, Symptoms Parkinson's Disease, was one topic where we observed this behavior.</p><p>The results of our analyses, then, clearly demonstrate that having good terms and term counts is outweighed by setting an improper threshold. On the other hand, accurately choosing a proper threshold helps even in instances where term and term count selection are not especially good. The greater danger lies in using a threshold that is too relaxed rather than setting too conservative a threshold. Thus, our decision to override and lower the threshold for each topic set automatically on the training data was the principal cause of our poor overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Post-TREC Experiments: IR-Based Filters</head><p>We were naturally interested in assessing the problem of threshold setting in our post-TREC follow-up experiments. In particular, we wanted to establish our baseline performance in threshold setting and to look more closely at the problem of term selection.</p><p>We confined our evaluation to the first fifty ("Assessor") topics, because they proved to be the most valuable (and valid) ones in the test suite, and because these topics also seem more realistic than the artificially generated "Intersection" topics. In our subsequent analysis, we report both our post-TREC results and official TREC results on Assessor topics only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revised (Corrected) Term/Threshold Selection</head><p>In our first post-TREC experiment, we repeated our basic TREC runs with "normal" threshold setting. That is, we did not force the threshold (set on the training data) to be more relaxed when running on the test collection. This experiment used only the simplest approach to term selection (based on Prob2 extraction and 2 nd -derivative term-count selection), threshold calibration on the full training database (using betagamma threshold setting), and direct ranking of the test collection. We called this run Prob2-2D.</p><p>In our second post-TREC experiment, we first split the training data into halves and used one half (including approximately half the positive examples) for candidate term selection and the other half for validation. In this approach, we were interested in trying several different term-extraction methods and predicting which method would give the best terms for each topic. Thus, we used each method (and 2 nd -derivative term-count selection) on the positive training documents for a topic in the first half of the training corpus to create a term vector for each topic, and then tested the performance of each vector against the second half of the training corpus. Based on which vector gave the best performance (T11SU score), we chose the termextraction method used to create that vector as the "best" for that topic. We then repeated the procedure in our first post-TREC experiment, but with the termextraction method set to the "best" method for each topic. We called this run Opt-2D.</p><p>The steps in our process are given in Figure <ref type="figure" coords="3,498.51,580.88,3.68,8.33">2</ref>. Note that the split of the training data into halves (or any other arbitrary proportion) to yield a sub-corpus for topic modeling (term extraction) and a sub-corpus for validation (testing a model), is based on a pseudorandom assignment of documents to one or the other portion. This means that, in the case of some topics, there might be very few positive examples of a topic in any one of the training sub-corpora. A paucity of data can lead to poor training, of course, but we decided not to intervene to insure optimal splits in training data precisely because we wanted to assess, as well, the robustness of our generalized topic-modeling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Procedure for creating a CLARIT filter profile</head><p>The formulae for our term-extraction methods-Prob2, Rocchio, RocchioFQ, and GlobalLocal2 (GL2)-are given in Figure <ref type="figure" coords="4,133.66,340.94,3.68,8.33">1</ref>. In both experiments, we used the full training corpus as the reference corpus. Processing time for these filters averaged 17 seconds per topic for training and testing combined. Filter length for Prob2-2D averaged 33.34 terms and for Opt-2D 15.76.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Post-TREC Experiment Results</head><p>As can be seen from the results in Table <ref type="table" coords="4,235.53,423.74,3.77,8.33" target="#tab_2">2</ref>, both Prob2-2D and Opt-2D clearly out-perform our submitted (official) runs. (The values in Table <ref type="table" coords="4,214.98,444.44,5.00,8.33" target="#tab_2">2</ref> for our official runs reflect our performance on the Assessor topics only.) Compared to the median reported for the group on Assessor topics, both Prob2-2D and Opt-2D have lower T11SU scores. However, in terms of F-Beta, both post-TREC runs show rather impressive performance.</p><p>In our Opt-2D runs, the Prob2 extraction method was chosen only 6 times, whereas Rocchio was chosen 30 times, RocchioFQ 8 times, and GL2 6 times. In terms of individual-topic results, Opt-2D gave significantly better performance (&gt;0.10 absolute difference in score) than Prob2-2D on 10 topics for T11SU and 9 topics for F-Beta. In contrast, Opt-2D was significantly worse on 11 topics for T11SU and on 7 for F-Beta. In the aggregate, however, the effect of term-extraction method optimization appears to be negligible. We note, however, that our choice of an optimum method was based on the performance of a candidate filter on half the training corpus. In those cases where we had poor training splits, our choice was not well informed. Clearly, this is an area for further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>The overall strong performance on F-Beta for both runs confirms our hypothesis that the basic method we have used is robust and practical. It also confirms that the poor results in our official runs were due to improper threshold setting, in particular, our decision to relax the threshold values that were determined for filters on the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Post-TREC Experiments: Kernel-Based Filters</head><p>In addition to our IR-based runs, we decided to expand our evaluation of kernel techniques for batch filtering in a series of post-TREC experiments. The essential questions we focused on include how well kernel methods perform on topics with limited training data and how flexible the learned thresholds can be when data is sparse. We explored both our kernel-Adatron and a new version of an SMO algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">General Note on Kernel (SVM) Methods</head><p>Support vector machines (SVM) are a general purpose machine learning approach [ st half ("even" documents) was chosen for validation/optimization.) The prescrambling makes it possible to select any subset with reduced bias. (If one chose a 60/20/20 split for the 10document collection, above, the system would deliver the subsets "0 3 6 9 1.4", "7 2," and "5 8".)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Choose extraction method.</head><p>If the extraction method is fixed (e.g., Prob2), skip this step and go to Step 3.</p><p>For each candidate extraction method (e.g., Prob2, Rocchio, RocchioFQ, and GL2), create a filter using the Training half of the training corpus. Optimize the term count by applying the 2nd derivative method. Choose Max(MinimumTermCount, 2ndDerivTermCount). (The MinimumTermCount used in post-TREC experiments is 10.) Truncate the term vector to the specified term count. Set the threshold using beta-gamma optimization (β=0.1,γ=0.4) over the entire training set. Retrieve over the Validation half of the training set (using the optimized threshold) and compute utility (T11SU). Choose the extraction method with the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Extract final filter.</head><p>Extract terms from the entire training set using the chosen method. Optimize the term count (using 2nd derivative, as described above), subject to the MinimumTermCount. Truncate the vector to that count. Set the threshold on the entire training set using beta-gamma optimization.</p><p>for this hyperplane in the term/word space, thus avoiding the use of more complex feature spaces that can be induced easily using kernel (non-linear similarity) functions. Hyperplane selection is based upon ideas from statistical learning theory <ref type="bibr" coords="5,240.61,115.22,13.96,8.33" target="#b11">[12]</ref>, where the hyperplane that is furthest away (has maximum margin) from all training data and that provides (tolerable) class separation is chosen. Large margin separation has been theoretically shown to lead to improved generalization.</p><p>More formally, SVM models or classifiers denote a separating hyperplane between two classes, whereby datapoints falling on one side of the hyperplane denote one class and datapoints falling on the other denote the other class. In linear kernel-based SVMs, hyperplanes are typically represented in primal form as follows (where &lt;.,.&gt; denotes inner/dot product):</p><p>( )</p><formula xml:id="formula_2" coords="5,90.14,266.73,119.18,10.83">E ; : 6LJQ &amp;ODVVᤡ;ᤢ + =</formula><p>where W is a weight vector, and b is the bias or threshold. See Figure <ref type="figure" coords="5,160.67,298.64,5.00,8.33">3</ref> for a graphic depiction of a hyperplane for a linearly separable dataset. An alternative and more general representation of a hyperplane that is commonly used in SVMs is the following dual representation:</p><formula xml:id="formula_3" coords="5,90.68,357.39,140.25,23.86">      + = ∑ = b X , X y α Sign Class(X) i i L 1 i i</formula><p>Here, the alphas (α i ) denote the Lagrange multiplier associated with each example. This representation permits the learning of such classifiers using wellknown optimization techniques such as quadratic programming. After learning, only a small percentage of the training data will have non-zero Lagrange multipliers. These examples are known as the support vectors. For dot-product (linear) kernels the dual representation of a hyperplane can be mapped to the primal form, thus, yielding a computationally more efficient model, akin to the more traditional information retrieval model. The above dual representation of a hyperplane can be further generalized by considering different forms of the similarity function or kernels such as polynomial, LSI Kernels <ref type="bibr" coords="5,181.18,534.98,9.31,8.33" target="#b3">[4]</ref>, and String Kernels <ref type="bibr" coords="5,272.61,534.98,9.44,8.33" target="#b8">[9]</ref>. For our current purposes of text classification, linear kernels were deemed to be sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning SVMs</head><p>Support vector machines are commonly trained using either mathematical programming (MP) approaches such as quadratic programming or by strategies that avoid the use of the MP techniques. The latter techniques have the added attraction of being easier to implement, while providing similar levels of performance as their MP counterparts. For our experiments, we implemented and evaluated two non-MP based approaches: the kernel-Adatron (KA) algorithm <ref type="bibr" coords="5,260.41,679.70,30.27,8.33">[1;6;11]</ref> and variations of the sequential minimal optimization (SMO) algorithm <ref type="bibr" coords="5,140.31,700.40,21.62,8.33">[10;7]</ref>. Both of the algorithms are outlined briefly below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. A linearly separable dataset in a twodimensional space for a two-class problem (where the "o"s correspond to one class, and the "x"s denote the other)</head><p>Kernel-Adatron Learning Algorithm. One of the simplest strategies for learning a support vector machine is to update the Lagrange multipliers, α, associated with each example iteratively. This approach has been taken in the kernel-Adatron algorithm proposed by various researchers (cf. <ref type="bibr" coords="5,508.44,385.76,10.07,8.32" target="#b5">[6]</ref> and <ref type="bibr" coords="5,320.36,396.02,13.79,8.32" target="#b10">[11]</ref>). The Adatron was originally proposed by Anlauf and Biehl <ref type="bibr" coords="5,360.47,406.46,10.06,8.32" target="#b0">[1]</ref> in the field of statistical mechanics. It is an on-line learning algorithm for learning perceptrons. In <ref type="bibr" coords="5,330.44,427.16,9.44,8.32" target="#b0">[1]</ref>, it was proved that the Adatron converges to a maximum margin solution; that is, the discovered hyperplane is a fixed point of the adaptive algorithm for linearly separable data. In <ref type="bibr" coords="5,426.03,458.12,10.07,8.32" target="#b5">[6]</ref> and <ref type="bibr" coords="5,456.09,458.12,13.95,8.32" target="#b10">[11]</ref>, the Adatron algorithm was extended to learn the dual representation of a separating hyperplane in which the dot product is replaced with the more general kernel, thereby expanding the domain of application of the Adatron to non-linear problems. (A simplified version of the pseudo-code for the kernel-Adatron algorithm is presented in Figure <ref type="figure" coords="5,400.47,530.66,3.53,8.33">4</ref>.) We limited our implementation to training hard-margin SVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMO Learning Algorithm. The Sequential Minimal</head><p>Optimization (or SMO) algorithm is an alternative method for training SVMs <ref type="bibr" coords="5,424.37,582.32,14.11,8.33" target="#b9">[10]</ref>. Traditionally, training an SVM required the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of the smallest possible QP problems, where only two Lagrange multipliers, α I , are optimized at each iteration. Since only two parameters are considered at a time, while all others are fixed, it is possible to derive an analytical solution as opposed to the numerical methods used in MP solutions. This avoids using a time-consuming numerical QP optimization as an inner loop in the algorithm. On each iteration, SMO chooses two Lagrange multipliers to optimize jointly (typically the  examples that have the largest polar error), finds the optimal values for these multipliers analytically, and updates the SVM to reflect the new optimal values. (A simplified version of the pseudo-code for the SMO algorithm is presented in Figure <ref type="figure" coords="7,200.75,115.22,3.46,8.33">5</ref>.) For our experiments, we implemented two variants of the SMO algorithms proposed by Keerthi et al. <ref type="bibr" coords="7,221.11,135.92,10.07,8.32" target="#b6">[7]</ref> that provide better heuristics for determining which pair of Lagrange multipliers to update next and that provide better stopping criteria. For our current study, two variations of the SMO algorithm were implemented and evaluated: SMOK1 and SMOK2, corresponding to modification 1 and modification 2, respectively, as proposed in <ref type="bibr" coords="7,264.21,198.02,9.31,8.32" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Preprocessing</head><p>We examined two representations of documents: one using CLARIT NLP-based (single or multi-word) terms and the other using single white-space-delimited words. The latter approach involved the following steps: replace all numbers and punctuation by spaces; eliminate stopwords such as articles and prepositions, etc. In both preprocessing approaches each term is associated with a TF * IDF weight, where TF denotes the frequency of a term in a document, and IDF is calculated based on the distribution of the term in the training corpus. The TF * IDF weights were then normalized leading to documents vectors of unit length.</p><p>For some of our experiments we chose a subset of terms in the term-space of the training corpus. In such cases, we ranked terms based upon their mutual information with the class label and the k terms with highest mutual information were selected to represent each document. The mutual information MI(x i , c) between a feature, x i , and a category or topic, c, is defined as follows:</p><formula xml:id="formula_4" coords="7,78.98,452.19,215.12,34.28">{ } { } ∑ ∑ ∈ ∈ = [ L L L F L L ᤢ3ᤡFᤢ 3ᤡ[ Fᤢ 3ᤡ[ FᤢORJ 3ᤡ[ Fᤢ 0,ᤡ[</formula><p>Following feature selection, the document vectors were again normalized to unit length.</p><p>A learning step follows where for each topic/class/ category a topic-specific binary classifier is learned from the training data that models the topic (positive class) and the not-topic (or negative class). While it is possible to learn a topic SVM classifier by using all available training data, it is computationally attractive to reduce the number of training data, especially the number of negative examples. We currently achieve this through random sampling of the negative class, though all explicitly labeled negative documents are used. Typically, given n positive training examples for a topic, we chose m * n negative documents. We explore different values of m in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiment Results using SVMs</head><p>For each of our experiments, we trained a linear TF * IDF kernel-based SVM (i.e., linear kernel, where each term is weighted using TF * IDF) for each topic using the training data. For most machine learning processes, with SVM-based approaches being no exception, there are many parameters and decisions that need to be made in order to generate a model that performs well on unseen data. Some of these are domain specific (e.g., text vs. images), while others are algorithm specific (e.g., the upper bound for Lagrange multipliers, C). The domain-specific decision variables for text include the following: (1) the number of terms used to represent each topic; (2) the number of ontopic training documents; (3) the ratio of positive to negative documents; (4) the sampling strategy for the negative class; and (5) the representation of a document using single words or NLP-based terms. In an ideal setting one could potentially chose the optimal configuration for a topic using, for example, n-fold cross validation. However, due to time limitations, we were unable to carry out such experiments in our post-TREC work. Instead, we report results where the different experiment variables are set to equivalent values across all topics for a particular experiment. The decision variables and explored values for our experiments are presented in Table <ref type="table" coords="7,464.47,301.52,3.77,8.33" target="#tab_4">3</ref>.</p><p>The results of the more interesting experiments are presented in Table <ref type="table" coords="7,397.58,332.48,3.77,8.33" target="#tab_5">4</ref>, where each row denotes one experiment on 50 Assessor topics. We report the T11SU and F-Beta measures for each experiment. For our some of our experiments we used different document sampling strategies. One was based upon a sampling of positive to negative documents (denoted as a ratio in Table <ref type="table" coords="7,394.48,394.58,3.52,8.33" target="#tab_4">3</ref>). The other was based on using all labeled documents and fixed sample size of all unlabeled documents in the training size (denoted as an integer in the Class Ratio column in Table <ref type="table" coords="7,501.98,425.72,3.53,8.33" target="#tab_5">4</ref>). Experiments on the 50 Intersection topics were not carried out, apart from one experiment, which was performed with the kernel-Adatron algorithm using all NLP-based terms and a 15,000 sample of the training set, yielding a T11SU performance of 0.328 (and 0.342 on the fifty Assessor topics).</p><p>For each experiment, training a battery of 50 binary classifiers (one classifier corresponding to each Assessor topic) took approximately twenty minutes (or approximately 24 seconds per topic), while evaluation took approximately two to three hours. The CC SVM toolkit is developed in Java and experiments were carried out under Linux and Windows XP on a 866-MHz Pentium III computer with 1 gigabyte of RAM.</p><p>Among the results, the best overall performance was given by an SMOK2 run (SMOK2-θ.45) representing one of our first experiments in thresholding the margin scores given by the SVM. In particular, we found the margin that gave optimal T11U utility on a resample of the training corpus, Margin MaxU , and used the following formula to compute the new threshold, θ Opt, ,, where ThresholdDiscount was set to 0.45 for this run:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(</head><p>)1  Overall, the performance of the learnt SVM classifiers is good compared to the submission results for other groups. The T11SU utility measure is average, while the F-Beta measure is low, apart from the SMOK2θ.45 run with its more reasonable performance of 0.271. The lack of higher performance for the SVMs is partly due to the experimental setup, which did not employ cross-validation; i.e., for each experiment, each topic SVM was trained using the same parameter settings, thereby limiting potential performance of the learnt SVMs. Allowing the determination of a customized setting (potentially optimal) for each topic should lead to improved performance. In addition, our preliminary work on thresholding the margin value of the SVM output has given very encouraging results. This is consistent with results form other groups for this particular dataset <ref type="bibr" coords="9,356.18,583.04,9.31,8.33" target="#b2">[3]</ref>. However, for some other datasets in the past, this did not improve performance <ref type="bibr" coords="9,479.45,593.30,9.45,8.33" target="#b7">[8]</ref>.</p><p>Given the results of our limited experiments, we can make the following observations:</p><p>• Using a simple tokenizing-based representation of a document (in lieu of NLP) actually boosts performance. • Sampling negative class documents does degrade evaluation performance, while it improves the efficiency of classification and learning. • Using all available terms gives the best performance, though sampling terms does not degrade performance substantially, while it improves the efficiency of classification and learning. • The kernel-Adatron algorithm gives a very reasonable performance, though it is a much simpler algorithm that the examined SMO variations. • Using cross-validation for customizing the parameters of learning should improve the quality of the learnt SVM classifiers. • Our simple thresholding results are encouraging.</p><p>Using a principled approach to thresholding (such as beta-gamma or other distribution based approaches) may prove practical and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Thoughts</head><p>As the additional analyses in Figures <ref type="figure" coords="10,221.18,251.66,5.02,8.33">6</ref><ref type="figure" coords="10,226.20,251.66,5.02,8.33">7</ref><ref type="figure" coords="10,226.20,251.66,5.02,8.33">8</ref><ref type="figure" coords="10,231.22,251.66,5.02,8.33">9</ref>show, our experiments on the TREC Assessor topics underscore the comparative strengths and differences in the two filter types we have developed. For IRbased filters, we see responsiveness and delivery of relevant documents even when there are limited training data. They also were very fast to train and run and generally required only a handful of features (cf. Figure <ref type="figure" coords="10,115.12,334.28,7.71,8.33">10</ref>). The IR-based filters failed to return relevant documents in only one case out of fifty topics. However, a measure that rewards the delivery of no documents, such as T11SU, penalizes the IRbased approach. In contrast, we see consistent positive (or neutral) performance from SVM-based filters, giving high precision, if under-delivery, on unseen data. However, for many of the SVM runs (on on more than twenty topics) there were no documents returned at all.</p><p>The challenge in many practical (commercial) applications is limited training data and the need to optimize performance in virtually real time. Some of the best methods for classifier training, such as kernel-based approaches, require significant amounts of data and may depend on sensitive parameter tuning. However, we see in our own experiments that kernel methods can give high precision and accuracy. If we can overcome their high-precision bias using thresholding or unevenmargin-based learning and adapt them to sparse data, they may become an attractive solution. We also see the robustness and generally good performance of IR-based approaches. Perhaps the ideal application will combine features of both and optimize the choice of classifier-IR or SVM-for each topic on a case by case basis </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,71.96,261.02,254.61,8.32"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Partial pseudo-code for Kernel-Adatron algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,71.96,529.40,462.16,8.33;9,71.96,551.72,142.49,8.33"><head>Figure 9 . 4 . 5 .</head><label>945</label><figDesc>Figure 9. Difference from TREC med F-Beta-scores x Topic x Training Data, ranked by Prob2-2D TREC med 4.5. Observations on SVM Filters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,211.11,350.48,323.58,331.97"><head>run. Figure 1. Term-extraction formulae</head><label></label><figDesc></figDesc><table coords="2,325.76,377.66,208.93,73.48"><row><cell cols="2">Run Description</cell><cell>T11SU</cell><cell>F-Beta</cell></row><row><cell cols="2">Median for all Submitted Runs</cell><cell>0.316</cell><cell>0.129</cell></row><row><cell>Submitted</cell><cell>CCT11BFC</cell><cell>0.186</cell><cell>0.147</cell></row><row><cell>Results</cell><cell>CCT11BFD</cell><cell>0.184</cell><cell>0.145</cell></row><row><cell>Unofficial Results</cell><cell>CCT11BFA CCT11BFB Adatron</cell><cell>0.147 0.165 0.328</cell><cell>0.130 0.129 0.035</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,320.36,459.38,202.62,18.76"><head>Table 1 . Results of official and pre-TREC batch experiments (on all 100 topics)</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,71.96,631.10,214.33,79.25"><head>Table 2 . Results of post-TREC-batch experiments</head><label>2</label><figDesc></figDesc><table coords="4,77.36,631.10,208.93,62.69"><row><cell cols="2">Description</cell><cell>T11SU</cell><cell>F-Beta</cell></row><row><cell cols="2">Median for all Submitted Runs</cell><cell>0.377</cell><cell>0.234</cell></row><row><cell>Submitted</cell><cell>CCT11BFC</cell><cell>0.243</cell><cell>0.259</cell></row><row><cell>Results</cell><cell>CCT11BFD</cell><cell>0.243</cell><cell>0.259</cell></row><row><cell>Post-TREC</cell><cell>Prob2-2D</cell><cell>0.309</cell><cell>0.323</cell></row><row><cell>Experiments</cell><cell>Opt-2D</cell><cell>0.315</cell><cell>0.326</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,79.88,88.40,458.61,623.03"><head></head><label></label><figDesc>First, sort (scramble) the document ids. (For a database with 10 docs, such "scrambling" might produce: 0 2 4 6 8 1 3 5 7 9.) Next, apply the desired Training/Validation split. (For the TREC experiments, the split is 50/50, based on choosing every other document for a split.) Pick one split for Training, one for Validation. (In the TREC experiments, the 2 nd half ("odd" documents) was chosen for Training/term-extraction and the 1</figDesc><table coords="4,320.36,610.04,218.13,49.73"><row><cell>2;12], with our interest</cell></row><row><cell>being principally in learning classification models from</cell></row><row><cell>labeled data. Our batch filtering SVM study was limited</cell></row><row><cell>to learning a binary SVM classifier for each topic</cell></row><row><cell>(positive and negative class). This corresponds to</cell></row></table><note coords="4,320.36,661.70,211.70,8.33;4,320.36,672.14,185.59,8.33;4,320.36,682.40,205.17,8.33;4,320.36,692.84,185.55,8.33;4,320.36,703.10,208.73,8.33;4,79.88,88.40,99.16,8.33"><p>searching for (or learning) a hyperplane that provides maximum separation between the positive and negative training examples. Since text classification problems are of high dimensionality (which are generally linearly separable), it is sufficient to search 1. Split the training set.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,71.96,88.63,459.86,560.34"><head>Table 3 . Decision variables and explored values for current experiments using SVM text classifiers 1</head><label>3</label><figDesc>. Given Training data S where each example i is of the form (x i , 1 ,…, x i,n ,y i ), and a learning rate η</figDesc><table coords="6,77.36,492.14,454.46,140.27"><row><cell>Decision Variable</cell><cell>Explored Values</cell></row><row><cell>Learning Algorithm</cell><cell>Adatron, SMO, SVM Light</cell></row><row><cell>C (Upper bound for Lagrange multipliers)</cell><cell>3, 10</cell></row><row><cell>Learning Rate (Adatron)</cell><cell>0.75</cell></row><row><cell>Tolerance</cell><cell>0.001</cell></row><row><cell>Type of kernel</cell><cell>Linear</cell></row><row><cell>Class Ratio</cell><cell>1:4, 1:10 , use all training data</cell></row><row><cell>Sampling Strategy</cell><cell>Random</cell></row><row><cell>Term type</cell><cell>NLP; single words</cell></row><row><cell>Term Ranking Algorithm</cell><cell>• Use all terms</cell></row><row><cell></cell><cell>• Mutual Information: Use k terms that have highest MI for each topic</cell></row><row><cell>Number of terms k</cell><cell>k = 1,000, 10,000, All</cell></row><row><cell>Term weighting</cell><cell>Normalized TF * IDF</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,339.44,696.90,171.07,10.72"><head>Table 4 . Results of SVM experiments on 50 Assessor topics Post-TREC Experiments--Performance on T11SU</head><label>4</label><figDesc></figDesc><table coords="7,339.44,696.90,171.07,10.72"><row><cell>θ Opt</cell><cell>=</cell><cell>iscount ThresholdD</cell><cell>*</cell><cell>Margin</cell><cell>MaxU</cell><cell>+</cell><cell>1</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,71.96,476.12,457.87,251.33"><head>Figure 6. T11SU comparative results for SVM-and IR-filters on topics ranked by TREC median performance Post-TREC Experiments--Performance on F-Beta</head><label></label><figDesc></figDesc><table coords="8,71.96,505.72,457.54,221.73"><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F-beta</cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>R102</cell><cell>R104</cell><cell>R144</cell><cell>R135</cell><cell>R116</cell><cell>R133</cell><cell>R146</cell><cell>R148</cell><cell>R121</cell><cell>R101</cell><cell>R120</cell><cell>R109</cell><cell>R125</cell><cell>R126</cell><cell>R141</cell><cell>R139</cell><cell>R137</cell><cell>R138</cell><cell>R128</cell><cell>R114</cell><cell>R127</cell><cell>R117</cell><cell>R115</cell><cell>R122</cell><cell>R129</cell><cell>R113</cell><cell>R105</cell><cell>R103</cell><cell>R130</cell><cell>R145</cell><cell>R124</cell><cell>R136</cell><cell>R110</cell><cell>R131</cell><cell>R140</cell><cell>R119</cell><cell>R132</cell><cell>R112</cell><cell>R147</cell><cell>R108</cell><cell>R134</cell><cell>R106</cell><cell>R118</cell><cell>R149</cell><cell>R150</cell><cell>R123</cell><cell>R142</cell><cell>R107</cell><cell>R111</cell><cell>R143</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SMO</cell><cell></cell><cell></cell><cell cols="4">Adatron</cell><cell></cell><cell></cell><cell cols="5">TREC-med</cell><cell></cell><cell></cell><cell cols="5">TREC-max</cell><cell></cell><cell></cell><cell cols="4">Prob2-2D</cell><cell></cell><cell></cell><cell cols="3">Opt-2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Figure 7. F-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,123.96,719.12,405.88,8.33"><head>Beta comparative results for SVM-and IR-filters on topics ranked by TREC median performance</head><label></label><figDesc></figDesc><table coords="9,85.28,78.29,442.62,205.22"><row><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R102</cell><cell>R104</cell><cell>R141</cell><cell>R109</cell><cell>R126</cell><cell>R129</cell><cell>R105</cell><cell>R116</cell><cell>R122</cell><cell>R103</cell><cell>R121</cell><cell>R135</cell><cell>R146</cell><cell>R113</cell><cell>R125</cell><cell>R148</cell><cell>R140</cell><cell>R120</cell><cell>R136</cell><cell>R101</cell><cell>R132</cell><cell>R138</cell><cell>R112</cell><cell>R124</cell><cell>R144</cell><cell>R147</cell><cell>R110</cell><cell>R114</cell><cell>R127</cell><cell>R133</cell><cell>R134</cell><cell>R145</cell><cell>R149</cell><cell>R106</cell><cell>R119</cell><cell>R128</cell><cell>R131</cell><cell>R142</cell><cell>R143</cell><cell>R150</cell><cell>R107</cell><cell>R108</cell><cell>R111</cell><cell>R115</cell><cell>R117</cell><cell>R118</cell><cell>R123</cell><cell>R130</cell><cell>R137</cell><cell>R139</cell></row><row><cell cols="50">13 512 0 24 2 0 1 9 17 16 16 15 1 4 1 4 14 13 12 12 12 1 1 9 8 7 7 7 6 6 6 6 5 5 5 5 5 5 5 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,71.96,90.08,371.03,222.82"><head>Topics in Descending Order of Number of Positive Training Documents T11SU</head><label></label><figDesc></figDesc><table coords="9,71.96,90.08,371.03,222.82"><row><cell>SMO</cell><cell>Adatron</cell><cell>TREC-med</cell><cell>TREC-max</cell><cell>Prob2-2D</cell><cell>Opt-2D</cell></row><row><cell>Figure 8.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="9,78.44,304.58,448.60,203.43"><head>Comparison of SVM-based and IR-based filters: T11SU scores x Topic x Training Data</head><label></label><figDesc></figDesc><table coords="9,78.44,331.43,448.60,176.57"><row><cell>0.800</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.600</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SMO</cell><cell></cell><cell></cell><cell cols="2">Adatron</cell><cell></cell><cell></cell><cell cols="3">Prob2-2D</cell><cell></cell><cell cols="3">Opt-2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.400</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R122</cell><cell>R142</cell><cell>R118</cell><cell>R108</cell><cell>R132</cell><cell>R139</cell><cell>R147</cell><cell>R107</cell><cell>R140</cell><cell>R103</cell><cell>R113</cell><cell>R141</cell><cell>R105</cell><cell>R148</cell><cell>R128</cell><cell>R127</cell><cell>R101</cell><cell>R125</cell><cell>R123</cell><cell>R150</cell><cell>R133</cell><cell>R119</cell><cell>R120</cell><cell>R136</cell><cell>R112</cell><cell>R117</cell><cell>R115</cell><cell>R111</cell><cell>R114</cell><cell>R138</cell><cell>R106</cell><cell>R129</cell><cell>R104</cell><cell>R102</cell><cell>R143</cell><cell>R149</cell><cell>R134</cell><cell>R131</cell><cell>R110</cell><cell>R145</cell><cell>R116</cell><cell>R146</cell><cell>R124</cell><cell>R130</cell><cell>R126</cell><cell>R137</cell><cell>R109</cell><cell>R135</cell><cell>R121</cell><cell>R144</cell></row><row><cell>1 5</cell><cell>4</cell><cell>3</cell><cell>3</cell><cell>7</cell><cell>3</cell><cell>6</cell><cell>3</cell><cell cols="6">1 1 1 4 1 2 2 4 1 6 1 2</cell><cell>4</cell><cell>5</cell><cell cols="2">7 1 2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>4</cell><cell>9</cell><cell>8</cell><cell>6</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>4</cell><cell cols="4">1 7 1 2 0 1 3 5 4</cell><cell>5</cell><cell>5</cell><cell>4</cell><cell>5</cell><cell>5</cell><cell cols="2">1 6 1 3</cell><cell>6</cell><cell cols="2">3 1 9</cell><cell>3</cell><cell cols="3">2 0 1 4 1 4</cell><cell>6</cell></row><row><cell>-0.200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-0.400</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-0.600</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,341.96,94.39,158.68,7.33;10,341.96,103.57,178.24,7.33;10,341.96,112.75,33.16,7.33" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,411.82,94.39,88.81,7.33;10,341.96,103.57,72.13,7.33">The adatron: an adaptive perceptron algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Anlauf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Biehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,419.72,103.57,61.51,7.33">Europhys. Letters</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="687" to="692" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.95,128.05,188.41,7.33;10,341.96,137.23,196.28,7.33;10,341.96,146.41,195.60,7.33;10,341.96,155.59,80.33,7.33" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,461.48,128.05,68.89,7.33;10,341.96,137.23,100.26,7.33">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,527.18,137.23,11.06,7.33;10,341.96,146.41,115.19,7.33">th Annual ACM Workshop on COLT</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</editor>
		<meeting><address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.96,170.71,185.81,7.33;10,341.96,180.07,159.90,7.33;10,341.96,189.25,191.02,7.33;10,341.96,198.43,130.18,7.33" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,393.97,189.25,135.94,7.33">Kernel Methods for Document Filtering</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conconi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vinokourov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,341.96,198.43,103.94,7.33">TREC 2002 Notebook Papers</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.96,213.55,194.93,7.33;10,341.96,222.73,178.02,7.33;10,341.96,231.91,96.00,7.33" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,479.29,213.55,57.60,7.33;10,341.96,222.73,25.55,7.33">Latent Semantic Kernels</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shawe-Taylorj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,373.46,222.73,146.52,7.33;10,341.96,231.91,20.67,7.33">Journal of Intelligent Information Systems (JJIS)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.96,247.21,189.28,7.33;10,341.96,256.39,163.84,7.33;10,341.96,265.57,169.29,7.33;10,341.96,274.75,163.01,7.33;10,341.96,283.93,167.98,7.33;10,341.96,293.11,196.32,7.33;10,341.96,302.29,197.70,7.33;10,341.96,311.65,190.14,7.33" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,398.84,265.57,112.41,7.33;10,341.96,274.75,163.01,7.33;10,341.96,283.93,42.61,7.33">Topic-Specific Optimization and Structuring. A Report on CLARIT TREC-2001 Experiments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sheftel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Montogomery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,401.90,283.93,108.04,7.33;10,376.52,293.11,132.57,7.33">The Tenth Text REtrieval Conference</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>U.S. Government Printing Office</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
	<note>EM Voorhees and DK Harman. TREC-2001</note>
</biblStruct>

<biblStruct coords="10,341.96,326.77,169.14,7.33;10,341.96,335.95,196.83,7.33;10,341.96,345.13,189.63,7.33;10,341.96,354.31,165.25,7.33" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,473.53,326.77,37.58,7.33;10,341.96,335.95,196.83,7.33;10,341.96,345.13,133.38,7.33">The kernel adatron algorithm: A fast and simple learning procedure for support vector machines. 15th Intl</title>
		<author>
			<persName coords=""><forename type="first">T-T</forename><surname>Frieß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,501.62,345.13,29.96,7.33;10,341.96,354.31,29.48,7.33">Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.96,369.61,179.67,7.33;10,341.96,378.79,190.74,7.33;10,341.96,387.97,187.97,7.33;10,341.96,397.15,82.67,7.33" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,362.66,378.79,170.04,7.33;10,341.96,387.97,55.25,7.33">Improvements to Platt&apos;s SMO algorithm for SVM classifier design</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krk</forename><surname>Murthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>IISc, Bangalore, India</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept of CSA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="10,341.96,412.45,196.32,7.33;10,341.96,421.63,194.98,7.33;10,341.96,430.81,112.72,7.33" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,348.62,421.63,185.25,7.33">Experiments at KAIST: QA, CLIR and Batch Filtering</title>
		<author>
			<persName coords=""><forename type="first">K-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trec</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,341.96,430.81,68.22,7.33">TREC Proceedings</title>
		<imprint>
			<biblScope unit="page">300</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.96,445.93,182.56,7.33;10,341.96,455.11,178.10,7.33;10,341.96,464.29,178.06,7.33" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,341.96,455.11,128.85,7.33">Text classication using string kernels</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Christianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,476.60,455.11,43.46,7.33;10,341.96,464.29,138.50,7.33">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.96,479.59,195.38,7.33;10,341.96,488.77,195.81,7.33;10,341.96,497.95,173.74,7.33;10,341.96,507.13,152.88,7.33" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,368.41,479.59,168.94,7.33;10,341.96,488.77,112.74,7.33">Fast Training of Support Vector Machines using Sequential Minimal Optimization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,469.04,488.77,68.73,7.33;10,341.96,497.95,122.88,7.33">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.97,522.43,182.96,7.33;10,341.96,531.61,167.00,7.33;10,341.96,540.79,172.84,7.33;10,341.96,549.97,109.67,7.33" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,487.22,522.43,37.71,7.33;10,341.96,531.61,137.76,7.33">Minimising BER in DFE with the Adatron Algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Santamaria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pantaleon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,485.42,531.61,23.54,7.33;10,341.96,540.79,120.83,7.33">Neural Networks for Signal Processing XI</title>
		<meeting><address><addrLine>Falmouth, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.96,565.09,184.55,7.33;10,341.96,574.28,109.30,7.33" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,378.32,565.09,144.30,7.33">The Nature of Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,341.97,589.57,165.64,7.33;10,341.96,598.75,192.34,7.33;10,341.96,607.94,190.16,7.33;10,341.96,617.12,183.38,7.33;10,341.96,626.30,155.59,7.33;10,341.96,635.48,156.50,7.33" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,341.96,598.75,178.24,7.33">Threshold Calibration in CLARIT Adaptive Filtering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,341.96,607.94,107.90,7.33;10,486.68,607.94,45.45,7.33;10,341.96,617.12,128.28,7.33">The Seventh Text REtrieval Conference (TREC-7)</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>U.S. Government Printing Office</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
	<note>EM Voorhees and DK Harman</note>
</biblStruct>

<biblStruct coords="10,341.97,650.78,173.74,7.33;10,341.96,659.96,189.63,7.33;10,341.96,669.14,183.50,7.33;10,341.96,678.32,183.38,7.33;10,341.96,687.50,155.59,7.33;10,341.96,696.68,156.50,7.33" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,341.96,659.96,177.71,7.33">Optimization in CLARIT TREC-8 Adaptive Filtering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,341.96,669.14,107.90,7.33;10,486.67,669.14,38.80,7.33;10,341.96,678.32,128.28,7.33">The Eighth Text REtrieval Conference (TREC-8)</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>U.S. Government Printing Office</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
	<note>EM Voorhees and DK Harman</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
