<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,208.60,77.13,178.28,15.54">NOVELTY TRACK AT IRIT -SIG</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.44,101.04,69.30,12.00"><forename type="first">Taoufiq</forename><surname>Dkaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique de Toulouse</orgName>
								<address>
									<addrLine>118 Rte de Narbonne</addrLine>
									<postCode>31062</postCode>
									<settlement>Toulouse CEDEX</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IUT URS</orgName>
								<address>
									<addrLine>72 Rte du Rhin</addrLine>
									<postCode>67400</postCode>
									<settlement>Strasbourg</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.80,101.04,68.03,12.00"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique de Toulouse</orgName>
								<address>
									<addrLine>118 Rte de Narbonne</addrLine>
									<postCode>31062</postCode>
									<settlement>Toulouse CEDEX</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Institut Universitaire de Formation des Maîtres Midi-Pyrénées</orgName>
								<address>
									<addrLine>56 av de l&apos;URSS</addrLine>
									<postCode>31078</postCode>
									<settlement>Toulouse</settlement>
									<region>CEDEX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.84,101.04,59.98,12.00"><forename type="first">Jérôme</forename><surname>Augé</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique de Toulouse</orgName>
								<address>
									<addrLine>118 Rte de Narbonne</addrLine>
									<postCode>31062</postCode>
									<settlement>Toulouse CEDEX</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,208.60,77.13,178.28,15.54">NOVELTY TRACK AT IRIT -SIG</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1AF9F105E113ADB70C0C69EE0AD97720</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>IRIT developed a new strategy in order to detect the relevant sentences that we did not try in a more general context of document retrieval but did try previously and partially in document categorization. In our approach a sentence is considered as relevant if it matches the topic with a certain level of coverage. This level of coverage depends on the category of the terms used in the texts. Three types of terms have been defined: highly relevant, lowly relevant and no relevant. With regard to the novelty part, a sentence is considered as novel when its levels of coverage with the previously processed sentences and with the bestmatching sentences do not exceed certain thresholds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>«The TREC 2002 novelty track is designed to investigate systems' abilities to locate relevant and new information within the ranked set of documents retrieved in answer to a TREC topic » <ref type="bibr" coords="1,460.72,378.60,61.87,12.00">[trec.nist.gov]</ref>.</p><p>Retrieving relevant texts is traditionally based on computing a similarity between the representation of the information need (or topic) and the texts. This general statement has been applied to full documents as well as chunks of texts (passage retrieval). Intuitively, the same idea can be applied when sentences retrieval is involved. IRIT developed a new strategy in order to detect the relevant sentence that we did not try in a more general context of document retrieval but did try previously and partially in document categorization. In our approach a sentence is considered as relevant if it matches the topic with a certain level of coverage. This level of coverage depends on the category of the terms used in the texts. Three types of terms have been defined: highly relevant, lowly relevant and no relevant. With regard to the novelty part, a sentence is considered as novel when its levels of coverage with the previously processed sentences and with the best-matching sentences do not exceed certain thresholds.</p><p>The results we obtain are quite good for the 'relevant' part. Indeed, we obtain 36 topics (73%) for which the R*P is higher or equal to the average of the 42 runs. With regard to the 'novelty' part, the results are disappointing and our method is situated around the average. This paper is organized as follows: in section 2 we describe the method we used, including the way documents and topics are represented and the strategies we developed for the two sub-tasks (relevant part and novelty part). In section 3 we present the results and comment them. Finally we indicate in the last section the future directions for our work on novelty track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Description of the method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document and topic representation</head><p>In our method, topics and sentences are considered as texts. Each text is pre-processed the same way in order to extract the representative terms. Then, the terms extracted from the topics are categorized into two groups : highly relevant terms (HT) and lowly relevant terms (LT). Finally, each text is represented by these two set of terms, with weights associated to each term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Text processing</head><p>Texts are processed using the following method :</p><p>1. Stop words are removed, 2. The remaining words are normalized using a dictionary that provides a common root for different words. This dictionary contains 21291 entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Topic processing</head><p>A topic is considered as a single text and the representative terms are extracted as explained in the previous section. Each term is then weighted and categorized into 2 groups:</p><p>-Highly relevant terms are terms that get a weight greater than 3, -Lowly relevant terms are terms that get a weight equal to 1 (see below the formula used to compute the term weights).</p><p>Given k T a topic, i t a term and k i tf , the frequency of i t in k T .</p><p>The term weight regarding a topic is computed as follows:</p><formula xml:id="formula_0" coords="2,211.72,348.71,172.56,33.01">otherwise tf if tf T t weight k i k i k i 1 3 ) , ( , , = ≥ =</formula><p>In order to obtain a significant difference -in terms of importance-between the highly relevant terms and the lowly relevant terms the weights of the lowly relevant terms are set to 1.</p><p>Each term is also categorized into one of the groups defined as follows:</p><formula xml:id="formula_1" coords="2,202.84,443.52,191.19,40.39">{ } 1 ) , ( / &gt; ∈ = k i k i i k T t weight and T t t HT { } 1 ) , ( / = ∈ = k i k i i k T t weight and T t t LT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Document processing</head><p>Each sentence of a document is considered as a text and the representative terms are extracted as explained in the section 2.1.1. To each term is associated a weight defined as follows:</p><p>Given j S a sentence, i t a term and j i tf , is the frequency of i t in j S .</p><formula xml:id="formula_2" coords="2,250.24,580.33,94.22,15.18">j i j i tf S t weight , ) , ( =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevant sentences</head><p>In order to decide if a sentence is relevant, we associate three components to each sentence :</p><p>-a score that reflect the sentence -topic matching :</p><p>Given a topic k T and a sentence j S ( )</p><formula xml:id="formula_3" coords="2,177.16,699.55,237.99,54.53">( ) ( ) å å å ∈ ∈ + ⋅ = ⋅ = k i i k i i LT t t j i HT t t k i j i k i j i k j tf tf tf T t weight S t weight T S Score / , / , , ) , ( ) , ( ) , (</formula><p>-and two groups of terms:</p><formula xml:id="formula_4" coords="3,64.48,90.74,299.68,66.53">{ } ) ( / k i i j HT Sj t t HS ∩ ∈ = { } ) ( / k i i j LT Sj t t LS ∩ ∈ = j</formula><p>HS corresponds to the highly relevant terms from the topic that occurs in the sentence, j LS corresponds to the lowly relevant terms from the topic that occurs in the sentence, A given sentence j S is then considered as relevant iff :</p><formula xml:id="formula_5" coords="3,142.24,221.02,306.83,40.11">k j j j k j j j k j LT HS LS HS g HT HS LS LS f T S Score ⋅ ÷ ÷ ø ö ç ç è ae + + ⋅ ÷ ÷ ø ö ç ç è ae + &gt; ) , (</formula><p>where X is the number of elements of X</p><p>In the experiments that correspond to the run sent to TREC, the function ) ( f and ) ( g have been set to:</p><formula xml:id="formula_6" coords="3,216.04,315.79,164.15,34.39">( ) ] ] ( ) ( ) ] ] ( ) 3 . 0 , 1 , 0 ; 85 . 0 0 5 . 1 , 1 , 0 ; 2 0 = ∈ ∀ = = ∈ ∀ = x g x g x f x f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Novelty sentences</head><p>To decide if a sentence p is to be considered as novel, we compute the similarity between the sentence p and the previous processed sentences i p and the similarity between the sentence p and a sentence ' P automatically built from the union of the set of i p :</p><formula xml:id="formula_7" coords="3,45.40,446.88,94.48,35.11">Given { } n p p p P , , , 2 1 K =</formula><p>a set of sentences and</p><formula xml:id="formula_8" coords="3,250.84,465.83,54.87,25.70">{ } 7 K n i i p P , , 1 ' ∈ =</formula><p>, ' P is a sentence made of the set of sentences P , ( )</p><formula xml:id="formula_9" coords="3,47.44,500.72,38.82,13.32">y x Sim ,</formula><p>a function that compute a similarity between x and y and p a sentence for which the system has to decide if it brings something new.</p><p>We first compute the following similarities:</p><formula xml:id="formula_10" coords="3,47.44,562.88,73.11,19.26">( ) p P p Sim α = ' ,</formula><p>and for</p><formula xml:id="formula_11" coords="3,169.96,565.46,139.95,16.49">{ } n i , , 1 K ∈ ( ) i p i p p Sim , , ω =</formula><p>We then consider the q best previous sentences:</p><formula xml:id="formula_12" coords="3,49.48,610.46,101.07,16.49">{ } i p n i for , , , 1 δ K ∈</formula><p>is the series obtained by ordering</p><formula xml:id="formula_13" coords="3,47.56,613.04,386.12,49.13">i p, ω in decreasing order. { } ) , ( , , 1 , å ∈ = q i i p p p Sim K δ β</formula><p>where q=4 in the run sent to TREC.</p><p>p is considered as novel iff:</p><formula xml:id="formula_14" coords="3,171.04,694.32,176.54,40.27">1 T p ≥ α and 2 T p ≥ β where 1 1 = T and 6 . 0 2 = T</formula><p>for the run sent to TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>This section presents the results we obtained with the method we developed and using the parameters that have been described in section 2.</p><p>When comparing the results with the other runs, we can notice that our system is better in finding relevant sentences than in detecting novelty in the sentences. This can be explained by the method we used that does not take into account the order of the sentences in the documents. Additionally most of the parameters have to be tune specifically to take into account the sentence relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relevant sentences</head><p>Figure <ref type="figure" coords="4,78.80,224.16,5.60,12.00" target="#fig_0">1</ref> indicates the number of topics for which our system (or run) has been ranked at the X th position among the 42 runs. For example, our method obtains the best results for 1 topic, the third position for 3 topics, the fourth for 2 topics, etc. and has a rank higher than 30 th for only one topic (see figure <ref type="figure" coords="4,530.81,251.76,3.84,12.00" target="#fig_0">1</ref>.a). Figure <ref type="figure" coords="4,78.68,265.68,4.77,12.00" target="#fig_0">1</ref>.b provides a graph that summarize figure <ref type="figure" coords="4,286.50,265.68,4.42,12.00" target="#fig_0">1</ref>.a by grouping together the results obtained for ranges of ranks. Additionally, the cumulative number of topics per range of system position is provided on the same graph. For example, we obtained a rank between 1 to 6 for 10 topics. The system obtains a rank equal or higher than 24 for 43 topics.</p><p>This clearly shows that our method is better than the average of the results. To be more precise, over the 49 topics, we obtained 36 topics (73%) for which the R*P is higher or equal to the average of the 42 runs. And if we consider the run ranks, we obtained a rank higher than the middle (21) for between 37 and 39 topics (depending how we consider the rank when 2 systems obtained the same value for R*P). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">New sentences</head><p>We present the results obtained in the second subtask the same way (see Figure <ref type="figure" coords="4,420.46,669.48,3.91,12.00" target="#fig_1">2</ref>).</p><p>Over the 49 topics, we obtained 20 topics (about 40%) for which the R*P is higher or equal to the average of the 42 runs. And if we consider the run ranks, we obtained a rank higher than the middle (21) for between 23 and 27 topics (depending how we consider the rank when 2 systems obtained the same value for R*P). The method is not better than the average.</p><p>The results in the second subtask are directly linked to the results obtained in the first subtask. As a result, the groups that obtained high R*P in the first part are more likely to obtain good results in the 'novelty' part. We can consider that the results we obtained for the novelty part are quite disappointing as the results on the relevance part were good. One of the tracks we are going to explore to improve the results is to take into account the order of the sentences as two sentences from a same paragraph are more likely to treat the same subject for example. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The approach we developed leads to relevant results for the first part of the task (relevant sentences). Over the 49 topics, we obtained 36 topics (73%) for which the R*P is higher or equal to the average of the 42 runs. And if we consider the run ranks, we obtained a rank higher than the middle (21) for between 37 and 39 topics. With regard to this sub-task, future work will be devoted first improving the definition of the ) ( f and ) ( g functions, which play the role of thresholds.</p><p>With regard to the second sub-task (novelty), the results are just on the average. This can be explained by the method we used that does not take into account the order of the sentences in the documents. Additionally most of the parameters have to be tune specifically to take into account the sentence relationship (two sentences that are close together in a document are more likely to deal with the same subject). This probably will also improve the fist sub-task of novelty track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">References</head><p>[trec.nist.gov] TREC web site. <ref type="bibr" coords="5,45.40,693.84,31.46,12.00">[Luhn,</ref><ref type="bibr" coords="5,80.24,693.84,14.68,12.00">60]</ref> Luhn, H., Keyword in Context Index for Technical Literature, American Documentation XI (4), 1960, 288-295.</p><p>[Mothe, 02] Mothe J., Chrisment C., Dousset B., Alaux J., DocCube : multi-dimensional visualisation and exploration of large document sets, to appear in JASIST.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,151.12,620.76,293.14,12.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1 : Number of topics per run rank -relevant sentences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,176.92,434.64,241.62,12.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2 : Number of topics per run rank -novelty</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
