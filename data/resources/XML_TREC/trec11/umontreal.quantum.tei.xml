<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.79,102.09,408.44,15.11">The QUANTUM Question Answering System at TREC-11</title>
				<funder>
					<orgName type="full">Bell University Laboratories</orgName>
					<orgName type="abbreviated">BUL</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,188.25,134.57,80.97,10.48"><forename type="first">Luc</forename><surname>Plamondon</surname></persName>
							<email>plamondl@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">RALI/DIRO</orgName>
								<orgName type="department" key="dep2">Succ. Centre-Ville Montréal</orgName>
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<postCode>6128, H3C 3J7</postCode>
									<region>CP, Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.24,134.57,69.52,10.48"><forename type="first">Guy</forename><surname>Lapalme</surname></persName>
							<email>lapalme@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">RALI/DIRO</orgName>
								<orgName type="department" key="dep2">Succ. Centre-Ville Montréal</orgName>
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<postCode>6128, H3C 3J7</postCode>
									<region>CP, Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.41,218.25,71.17,10.48"><forename type="first">Leila</forename><surname>Kosseim</surname></persName>
							<email>kosseim@cs.concordia.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd. West Montréal</addrLine>
									<postCode>H3G 1M8</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,101.79,102.09,408.44,15.11">The QUANTUM Question Answering System at TREC-11</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">869ABE110611FFC2219329DDA2028419</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year, we participated to the Question Answering task for the second time with the QUANTUM system. We entered 2 runs for the main task (one using the web, the other without) and 1 run for the list task (without the web). We essentially built on last year's experience to enhance the system. The architecture of QUANTUM is mainly the same as last year: it uses patterns that rely on shallow parsing techniques and regular expressions to analyze the question and then select the most appropriate extraction function. This extraction function is then applied to one-paragraph long passages retrieved by Okapi to extract and score candidate answers. Among the novelties we added to QUANTUM this year is a web module that finds exact answers using high-precision reformulation of the question to anticipate the expected context of the answer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC-11 marks the second year of existence of QUANTUM, the QUestion ANswering Tool of the University of Montreal. As for last year, we used QUANTUM to participate to the main and the list task, and we did not enter the context task. This year's version of QUANTUM is similar in essence to last year's version, but we enhanced specific modules that provided poor performances last year and we added a module to search for exact answers on the web. Following the conclusions we came to last year <ref type="bibr" coords="1,72.00,677.05,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,87.03,677.05,7.01,8.74" target="#b1">2]</ref>, we decided to drop our own information retrieval system to rely solely on Okapi 1 <ref type="bibr" coords="1,247.16,689.00,10.52,8.74" target="#b2">[3]</ref> since the 1 Okapi-Pack: www.soi.city.ac.uk/˜andym/OKAPI-PACK latter led to clearly better results. Also, we finetuned our answer extraction functions by introducing weights in the computation of answer scores.</p><p>The TREC-10 version of QUANTUM is described in <ref type="bibr" coords="1,321.98,363.15,10.52,8.74" target="#b0">[1]</ref> and an analysis of the results is presented in <ref type="bibr" coords="1,526.72,363.15,9.97,8.74" target="#b1">[2]</ref>. We summarize some sections that are still relevant to the understanding of this year's version, while we delve into the new features in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Components of Questions and Answers</head><p>QUANTUM uses the same technique for question analysis as last year <ref type="bibr" coords="1,401.09,489.93,9.97,8.74" target="#b1">[2]</ref>. To see how we decompose a question, let us consider question #302 -How many people die from snakebite poisoning in the US per year? and its answer. As shown in Fig. <ref type="figure" coords="1,489.01,525.79,3.88,8.74" target="#fig_1">1</ref>, the question is decomposed in three parts: a question word, a focus and a discriminant, and the answer has two parts: a candidate and a variant of the question discriminant.</p><p>The focus is the word or noun phrase that influences our mechanisms for the extraction of candidate answers (whereas the discriminant, as we shall see in Sect. 4.4, influences only the scoring of candidate answers once they are extracted). The identification of the focus depends on the selected extraction mechanism; thus, we determine the focus with the syntactic patterns we use during question analysis. Intuitively, the focus is what the question is about, but we may not need to identify one in every question if the chosen mechanism for answer extraction does not require  it.</p><p>The discriminant is the remaining part of a question when we remove the question word and the focus. It contains the information needed to pick the right candidate amongst all. It is less strongly bound to the answer than the focus is: pieces of information that make up the question discriminant could be scattered over the entire paragraph in which the answer appears, or even over the entire document. In simple cases, the information is found as is; in other cases, it must be inferred from the context or from world-knowledge.</p><p>We use the term candidate to refer to a word or a small group of words, from the document collection, that the system considers as a potential answer to the question. Candidates are usually noun phrases (see Sect. 4.3.1 for a discussion on exact answers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs Submitted at TREC-11</head><p>This year, we participated to the main task and the list task. We developed two versions of QUANTUM for the main task: one version that does not make use of the web, and one that does. For the list task, only the no-web version was ready by the time of the competition. So only one run was submitted for the list task and two for the main task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Architecture of the Core System</head><p>The current version is a continuation of last year's system but with enhancements suggested in our analysis of last year's results and with some changes required by the new specifications of the 11th edition of TREC (exact answer, single answers, etc.). After analyzing last year's performance, we concentrated our efforts in improving the question analysis module to correctly analyze more questions and the scoring module to better score the candidates. In particular, we wanted to better weight the contribution of WordNet and the named-entity tagger depending on the type of question. Last year, our strategy to insert NIL answers in our candidate set dropped our score significantly (from 0.223 to 0.199). We believe that our strategy was sound, but our scoring of the candidates was such that we would have been better off without inserting NIL answers. This is why, this year, we did not attempt to insert NIL answers. Let us step through the 4 basic steps of the system: question analysis, passage retrieval and tagging, candidate extraction and candidate scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question Analysis</head><p>To analyze a question, we use a tokenizer, a partof-speech tagger and a noun-phrase chunker (NPchunker). QUANTUM then applies a set of handmade patterns based on words, on part-of-speech tags and on noun-phrase tags, in order to select the most appropriate function for answer extraction. Last year, only 40 such patterns were used, but they correctly classified 88 % of the 492 TREC-10 questions. To increase the performance of the classification module, we added 20 more patterns to account for last years mistakes and new question formulations. Once the question is classified, an extraction function determines what criteria a group of words from a document should satisfy to constitute a valid answer candidate; for example, a location should begin with a capital letter while a measure should include a number and a measure unit. We also added a synonym extraction function to our last year's set of 11 functions (Table <ref type="table" coords="2,365.46,633.94,4.43,8.74" target="#tab_0">1</ref>) because some questions in the TREC-10 corpus required it. Extraction functions such as definition were less useful this year because the questions were mainly named-entity targeted.</p><p>Like last year, each function triggers a search mechanism to identify candidates in a passage based on the passage's syntactic structure or the semantic relations of its component noun phrases with the question focus. More formally, we have C = f (ρ, ϕ), where f is the extraction function, ρ is a passage, ϕ is the question focus and C is the list of candidates found in ρ. Each element of C is a tuple (c i , d i , s i ), where c i is the candidate, d i is the number of the document containing c i , and s i is the score assigned by the extraction function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Passage Retrieval and Tagging</head><p>The extraction of candidates is a time-consuming task. Therefore, we select the shortest, albeit most relevant, passages of the document collection before we begin answer extraction. To do so, we use the Okapi system to retrieve variable-length passages. Passage retrieval has not changed from last year; however, experiments showed that our own fixedwindow IR did not achieve as good results as Okapi. So this year, we exclusively used Okapi. Okapi is an information retrieval engine that has the ability to return relevant paragraphs instead of whole documents <ref type="bibr" coords="3,107.01,342.00,9.97,8.74" target="#b2">[3]</ref>. We feed it with the question as a query and we set it up so that it returns 30 one-paragraphlong passages (the average length of a passage, or paragraph, is 350 characters).</p><p>Since the answers to the TREC-11 factual questions are usually short noun-phrases (Sect. 4.3.1), we run our NP-chunker on the most relevant passages. Our chunker looks for specific sequences of part-ofspeech tags, which are given by our tagger. In addition, we run a named entity extractor on the passages because the candidates sought by extraction functions such as person(ρ), time(ρ) and location(ρ), are named entities. For this step, we use the Alembic Workbench system developed at Mitre Corporation for the Message Understanding Conferences (MUC). Amongst all the named entity types that Alembic can recognize <ref type="bibr" coords="3,115.75,534.04,9.97,8.74" target="#b3">[4]</ref>, we currently use only person, organization, location, date and time entities.</p><p>The tagged passages are then passed to the previously selected extraction function to identify and score candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Extraction of Candidates</head><p>Given the extraction function f chosen after question analysis, the question focus ϕ and a set of tagged passages ρ j , candidates c i are extracted along with their document number d i and their score s i (Sect. 4.1). To do so, each function is implemented by a set of search strategies that involve words, part-of-speech tags, semantic relations (mainly hypernym/hyponym relations given by WordNet) and named entities identified by Alembic. Table <ref type="table" coords="3,426.95,87.11,4.98,8.74" target="#tab_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>presented earlier shows some examples of what extraction functions look for.</head><p>During the extraction phase, we seek a high recall rate, no matter whether candidates are cited in a context that matches the question discriminant; we shall use a combination of scores to account for the context later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Answer Exactness</head><p>A major difference with the TREC-10 QA track is that answers must be exact. This means there is no limit imposed on the length of an answer string, but the string must not contain an incomplete answer nor must it contain more information than requested by the question. What is considered "too much" or "not enough" was not clearly defined at the time the competition was held; the problem was left to the assessors' judgment. Another consideration for having exact answers, although not relevant to TREC, is that potential users of QA systems are more likely to find the red, green and white flag more pleasant to read than ), that the red, green and white flag of the Ital.</p><p>We decided to keep the strategy we employed last year, which is to extract all the noun-phrases in a retrieved passage and then to test, using the selected extraction function, whether each of them is an interesting candidate. That means that a candidate is always at least a complete NP and it never encompasses more than one NP, except in a limited number of cases that we explicitly foresaw when writing the extraction functions. We are aware that this is not suitable to all questions but we believe it is a simple way to achieve a satisfying level of answer exactness most of the time.</p><p>There are cases where a NP is not long enough (our definition of a single NP does not include conjunctions of embedded NP), typically when (1) a question seeks more than one entity: #1422 -What two European countries are connected by the St. Gotthard Tunnel under the Alps? A: Switzerland and Italy<ref type="foot" coords="3,535.53,567.66,3.97,6.12" target="#foot_0">2</ref> and (2) the answer is a title or a quote: #1832 -What did Walter Cronkite say at the end of every show? A: "and that's the way it was" . There are also cases where a NP can be too long. For example, Louise Veronica Ciccone would be an inexact answer to question #1723 -What is Madonna's last name? A: Ciccone .</p><p>After examining answer patterns for TREC-8, TREC-9 and TREC-10 questions, we estimated that only 2 % of the questions could not be answered by a NP. Therefore, we decided to make no significant change to our system regarding answer exactness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Scoring of Candidates</head><p>The final score of a candidate is computed as in Eq. 1: We dropped the proximity score term that we used last year. This 3rd term was meant to favor candidates that were surrounded by question keywords but it did not have a noticeable influence in the tests we conducted this year.</p><formula xml:id="formula_0" coords="5,74.38,170.38,224.26,20.69">score = α • extraction score + (1 -α) • passage score<label>(</label></formula><p>We found the optimal value α = 0.75 by maximizing the performance of QUANTUM on a subset of TREC-8, TREC-9 and TREC-10 questions. We discarded no-answer questions and questions that QUANTUM does not analyze correctly.</p><p>The final scores given by Eq. 1 range from 0 to 1. Last year, scores awarded by QUANTUM did not have an upper bound, so candidates extracted using different extraction functions were hardly comparable. The comparability of scores seemed a prerequisite for detecting no-answer questions using a threshold (Sect. 4.5) and for a final re-ordering of the questions based on confidence (Sect. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">The Extraction Score</head><p>The extraction score measures how much a candidate satisfies various criteria that all valid candidates should meet. The set of criteria is specific to each type of question, thus to each extraction function. Criteria are weighted and a candidate does not have to satisfy all of them. Eq. 2 shows the criteria for the time function:</p><formula xml:id="formula_1" coords="5,88.40,575.46,212.62,9.65">score time = max(entity, hyponym) • penalty<label>(2)</label></formula><p>with entity = 0.5 if the candidate has been tagged as a time named entity by Alembic (entity = 0 otherwise), hyponym = 0.25 if the candidate is a hyponym of the WordNet synset time period or of another selected synset (hyponym = 0 otherwise), and penalty = 0.75 if the candidate contains one of the question keywords (penalty = 1 otherwise so that score is not reduced). The values of the parameters for the 12 extraction functions were found by maximizing the performance of QUANTUM on the same question subset as described above. An outcome of the optimization is the reduction of the influence of WordNet to the benefit of Alembic for the person function (WordNet: 0, Alembic: 1), for the location function (WordNet: 0, Alembic: 1) and for the time function (WordNet: 0.25, Alembic: 0.75). This was expected because our analysis of last year's results led us to the conclusion that WordNet was rather a source of noise when a named entity extractor could be used instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">The Passage Score</head><p>While the extraction score is concerned only with the form and type of a candidate, the passage score attempts to take into account the supplemental information brought by the question discriminant. It measures how confident we are in the passage where the candidate is found. For this measure, we use the score given to the passage during its retrieval by Okapi. However, this year, we normalize the score of each passage over the score of the best-scoring passage to have a passage score -and thus a final score (Eq. 1) -between 0 and 1. Since the question discriminant is likely to appear in the text under a slightly different form and to be scattered over several sentences around the sought candidate, we believe that an IR engine is the best tool for measuring the concentration of elements from the discriminant in a given passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">No-Answer Questions</head><p>At TREC-10, we measured the score drop between the ranked candidates for a question and we inserted a NIL answer when the score drop was higher than a threshold. This meant the system would rather say there is no suitable answer in the document collection than propose any candidate at a worse rank. We had chosen to use a threshold on normalized score drops instead of a threshold on absolute scores because different extraction functions would use different score scales. The major drawback of that method is the impossibility to propose a NIL answer at first rank (unless QUANTUM finds no candidate at all, which seldom happens because the extraction functions are very permissive).</p><p>This year, we attempted to use scoring methods that produce comparable final scores no matter how the candidates are extracted. Our goal was to set a threshold on the final score below which a NIL answer would be preferred. Unfortunately, because of the small number of criteria used when computing the extraction score and because of their boolean behaviour, the scores of all the candidates tend to cluster around a few values. We observed that the scores of first-rank candidates ranged from 0.5 to 1, with about one third of them at 0.5. Thus, a NIL insertion based on a score threshold would have resulted in a NIL answer for 33 % of the TREC-10 questions, which seemed too far from reality (10 %) to be an improvement to the system. Therefore, we decided not to detect no-answer questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Architecture of the Web Module</head><p>Following several systems from last year, and because TREC-11 questions are of general domain, the web proved to be an interesting source of answers <ref type="bibr" coords="6,276.13,284.63,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="6,290.50,284.63,7.01,8.74" target="#b5">6]</ref>.</p><p>We present here how a new module of QUANTUM makes a simple use of the web to retrieve exact answers.</p><p>Our use of the web differs from most of last year's participants. To use the web, two strategies could be used: either aiming at high recall or high precision. High recall would mean retrieving a large list of candidate answers and using a good scoring strategy to rank the candidates. This is similar in essence to answer redundancy. We decided to go for the other approach: high precision of the answers at the expense of recall.</p><p>We do not seek supplemental documents from the web to complement the set of documents retrieved from the TREC collection, because QUANTUM has enough answer candidates from the TREC collection. The problem is to correctly identify the answer from the list of candidates and score it such that it is positioned at the top of the list. So we use the web to retrieve candidates only through high-precision methods in such a way that the web does not return an answer often, but when it does, the answer is expected to be the correct one. To do so, we look on the web for an exact sentence that could naturally be the formulation of the answer to the question.</p><p>To formulate an answer pattern, the TREC question is turned into its declarative form using a set of hand-made patterns. For instance, #1697 -Where is the Statue of Liberty? is reformulated as "the Statue of Liberty is &lt;LOCATION&gt;". We call this reformulation the expected context of answer because we expect to find this pattern, with the correct answer in place of &lt;LOCATION&gt;, at least once on the web.</p><p>We then use Yahoo! to search for web pages that contain the known part of the expected context (here, the phrase "the Statue of Liberty is") and we identify answer candidates by unification. We do simple validity checks on candidates, such as testing the length of a candidate or whether a candidate for a location begins with a capital letter, but as for now the tests are not as sophisticated as the extraction functions displayed in Table <ref type="table" coords="6,436.45,158.84,3.88,8.74" target="#tab_0">1</ref>.</p><p>The web module can identify about 10 general types of answers (eg. &lt;LOCATION&gt;, &lt;NUMBER&gt;, &lt;CLAUSE&gt;, . . . ). However, by using a conjunction of expected contexts, we can impose more constraints on a candidate. For example, the question #1851 -Which country colonized Hong Kong? is reformulated as "&lt;CLAUSE&gt; colonized Hong Kong", where &lt;CLAUSE&gt; can be any string. To further restrict the candidate, we impose a second context for the candidate to satisfy: "&lt;CLAUSE&gt; is a country". The search thus becomes a conjunction of the two strings "&lt;CLAUSE&gt; colonized Hong Kong" and "&lt;CLAUSE&gt; is a country" (they can be found in separate web pages).</p><p>We score the candidates once they are extracted. A candidate that passes the validity checks starts with a score of 0.65. For each additional occurrence of the candidate found in any of the expected contexts derived from the question, the difference to one is divided by 2, thus raising the score to 0.825, then 0.9125 and so on. A candidate that does not meet the validity criteria but appears where an answer was expected receives a score of 0.1. Each additional occurrence of the candidate boosts the score by 0.1, as long as the resulting score does not exceed 0.6. Therefore, a presumably invalid candidate never has a higher score than a presumably valid candidate.</p><p>After we find an answer on the web, we use Okapi to perform a search in the TREC-11 document collection in order to have a document number to accompany the candidate. For the questions where either no candidate is found on the web or none of them can be matched with a document number, QUAN-TUM proceeds without the web module, as described in Sect. 4.</p><p>To test the performance of the web module, we conducted experiments with TREC-9 and TREC-10 questions. In the TREC-9 and TREC-10 document collection, we seldom find an expected context (we do for only 10 % of the questions). However, when we search on the web, we find at least one occurrence of an expected context for 43 % of the questions. Of these, the answer identified by unification is correct 51 % of the time, and 45 % of them appear at first rank. In clear, 10 % of the TREC questions are cor- rectly answered only by searching for expected contexts of answers on the web and by performing basic validity checks.</p><p>We also tried to run QUANTUM with and without the web module, and then to keep the bestscoring candidate of either version. Results were worse than with the web module alone, probably because scores obtained with expected contexts and with Eq. 1 should be weighted to be appropriately comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Confidence</head><p>Answers for the main task have to be submitted to NIST in decreasing order of confidence. To do so, we simply sort them according to their score. Table <ref type="table" coords="7,296.04,388.07,4.98,8.74" target="#tab_1">2</ref> shows how the sorting affected our runs by comparing their confidence-weighted score to a hypothetical maximum score (all correct answers at the top of the sorted list) and minimum score (all correct answers at the end of the sorted list), given the number of correct answers. Our ordering could be improved considerably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">List task</head><p>The only supplemental difficulty of the list task is the identification of the desired number of items specified in the question. We had to adapt the new question analysis patterns we added this year in the same way that we adapted last year's main task patterns for the list task. Once the system has analyzed a list question with the appropriate set of patterns, it proceeds exactly as for the main task (unfortunately, only the no-web version described in Sect. 4 was ready for TREC-11), except that it keeps the desired number of candidates instead of only the best one. For simplicity, duplicate candidates are eliminated even if they come from different documents (they might not be real duplicates because one candidate might be supported by its document and the other not).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>When we conducted our pre-competition tests on the TREC-10 question corpus, we estimated that the noweb version of QUANTUM could correctly answer 14 % of the questions, and that the web version would perform even better with a correct answer for 17 % of the questions. This was an improvement over last year's version of QUANTUM, which correctly answered 12 % of the questions (this data comes from last year's best run -with 50-character answers, but keeping only 1 answer per question). However, our estimations were made using an automated evaluation procedure that could not detect unsupported answers nor inexact ones (the lenient evaluation).</p><p>Table <ref type="table" coords="7,348.44,253.34,4.98,8.74" target="#tab_2">3</ref> shows the official evaluation details of our 2 main-task runs. We will exclude the effect of confidence weighting from our analysis and we will use Table <ref type="table" coords="7,338.69,289.21,4.98,8.74" target="#tab_3">4</ref> to compare a strict evaluation (the percentage of right answers) against a lenient evaluation that includes inexact and unsupported answers (as an automatic evaluation script would do).</p><p>The lenient evaluation in Table <ref type="table" coords="7,464.38,337.36,4.98,8.74" target="#tab_3">4</ref> is closer to our pre-competition estimations. The web module is clearly an improvement to QUANTUM (15 % of correct answers when using the web and 9 % without). However, because of the important number of web answers that are unsupported (40 unsupported answers when using the web, 2 without), a strict evaluation cuts the performance by half, making the web version even worse than the no-web version (6 % of correct answers when using the web, 7 % without).</p><p>As for answer exactness, the proportion of exact answers over all the supported answers is about 16 % for both runs. We consider this a satisfactory result given the simplicity of the method we have chosen (single NPs only) and given that some of the errors are tagging errors, thus not related to the method itself.</p><p>The list-task run achieved an accuracy of 0.07, while our last year's best run achieved an accuracy of 0.15. Since all the answers that QUANTUM found this year are distinct and very few are unsupported, we attribute the performance decrease to the additional constraints on answer length (candidates are much smaller than 50 characters and some are inexact).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Even though we fine-tuned last year's version of QUANTUM by adding more question analysis patterns, by introducing function-specific weights in the computation of the candidates' scores and by decreasing the influence of WordNet to the benefit of Alembic, the increasing difficulty of the task resulted in a lower performance for this year. We feel that answer exactness can be achieved reasonably well by retrieving single-NP answers only, but candidate scoring, document support and no-answer questions are still challenging issues.</p><p>For the first time, we used the web as a source of answers. We derived from the questions the context in which we expected the answers to appear. Not taking into account answer exactness and document suppport, we found that 10 % of TREC-style questions can be correctly answered this way. We plan to go further in this direction by improving the generation of expected contexts and the validation of the candidates' semantic types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,101.78,86.88,11.79,8.77;2,140.48,86.91,46.37,8.74;2,139.33,104.57,48.66,6.12;2,230.23,86.91,25.97,8.74;2,234.34,104.57,17.76,6.12;2,295.25,86.91,218.85,8.74;2,383.08,104.57,43.19,6.12;2,101.78,125.17,11.84,8.77;2,169.31,125.20,68.26,8.74;2,186.64,142.86,33.59,6.12;2,301.04,125.20,207.26,8.74;2,349.14,142.86,111.07,6.12"><head></head><label></label><figDesc>poisoning in the U.S. per year? discriminant A: About 10 people candidate die a year from snakebites in the United States. variant of question discriminant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,72.00,173.29,468.00,8.74;2,72.00,184.93,264.60,7.86"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of question and answer decomposition. The question is from TREC-9 (# 302) and the answer is from the TREC document collection (document LA082390-0001).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,292.53,182.33,8.49,8.74;5,72.00,194.29,229.01,8.74;5,72.00,206.24,229.02,8.74;5,72.00,218.20,129.31,8.74"><head></head><label></label><figDesc>1) with α, the extraction score and the passage score ranging from 0 to 1 (we describe the extraction score and the passage score below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.00,201.89,468.00,373.96"><head>Table 1 :</head><label>1</label><figDesc>Extraction functions, examples of TREC questions and samples of answer patterns. Hypernyms and hyponyms are obtained using WordNet, named entities are obtained using Alembic and NP tags are obtained using an NP-chunker. When we mention the focus in an answer pattern, we also imply other close variants or a larger NP headed by the focus.</figDesc><table coords="4,78.38,201.89,408.09,319.77"><row><cell>Function</cell><cell cols="3">Example of question and sample of answer patterns</cell></row><row><cell>definition(ρ, ϕ)</cell><cell>Q: #897 -What is an atom?</cell><cell>(ϕ = atom)</cell></row><row><cell></cell><cell cols="3">A: &lt;hypernym of atom&gt;, &lt;atom or hyponym of atom&gt;</cell></row><row><cell></cell><cell cols="3">A: &lt;atom or hyponym of atom&gt; (&lt;hypernym of atom&gt;)</cell></row><row><cell></cell><cell cols="3">A: &lt;atom or hyponym of atom&gt; is &lt;hypernym of atom&gt;</cell></row><row><cell>specialization(ρ, ϕ)</cell><cell cols="2">Q: #1684 -What card game uses only 48 cards?</cell><cell>(ϕ = card game)</cell></row><row><cell></cell><cell>A: &lt;hyponym of card game&gt;</cell><cell></cell></row><row><cell>cardinality(ρ, ϕ)</cell><cell cols="2">Q: #1761 -How many black keys are on the piano?</cell><cell>(ϕ = black keys)</cell></row><row><cell></cell><cell cols="2">A: &lt;number&gt; &lt;black keys or hyponym of black key&gt;</cell></row><row><cell>measure(ρ, ϕ)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>A: Not implemented for TREC</cell><cell></cell></row><row><cell>reason(ρ)</cell><cell cols="2">Q: #902 -Why does the moon turn orange?</cell></row><row><cell></cell><cell>A: Not implemented for TREC</cell><cell></cell></row><row><cell>object(ρ)</cell><cell>Default function</cell><cell></cell></row><row><cell></cell><cell>A: &lt;NP&gt;</cell><cell></cell></row></table><note coords="4,191.10,302.12,244.22,7.89;4,451.83,302.14,68.02,7.86;4,191.10,313.00,306.89,8.21;4,78.38,324.46,56.94,7.86;4,191.10,324.43,172.04,7.89;4,379.65,324.46,43.67,7.86;4,191.10,335.39,80.72,7.89;4,78.38,346.78,58.26,7.86;4,191.10,346.75,223.31,7.89;4,191.10,357.71,170.72,8.14;4,191.10,368.67,169.76,8.14;4,78.38,380.05,39.12,7.86;4,191.10,380.02,226.24,7.89;4,191.10,390.91,107.48,8.21;4,78.38,402.37,30.73,7.86;4,191.10,402.34,185.67,7.89;4,191.10,413.23,96.48,8.21;4,191.10,424.19,118.67,8.21;4,78.38,435.64,43.94,7.86;4,191.10,435.62,198.12,7.89;4,191.10,446.50,116.49,8.21;4,78.38,457.96,44.08,7.86;4,191.10,457.93,178.21,7.89"><p>Q: #1715 -How much vitamin C should you take in a day? (ϕ = vitamin C ) A: &lt;number&gt; &lt;hyponym of unit&gt; of &lt;vitamin C or hyponym of vitamin C &gt; attribute(ρ, ϕ) Q: #1420 -How high is Mount Kinabalu? (ϕ = high) A: Various patterns synonym(ρ, ϕ) Q: #1651 -What is another name for the North Star? A: the North Star , also known as &lt;NP&gt; A: &lt;NP&gt;, also known as the North Star person(ρ) Q: #1424 -Who won the Oscar for best actor in 1970? A: &lt;person named entity&gt; time(ρ) Q: #1676 -When was water found on Mars? A: &lt;time named entity&gt; A: &lt;hyponym of time period &gt; location(ρ) Q: #1483 -Where is the highest point on earth? A: &lt;location named entity&gt; manner (ρ) Q: #1446 -How did Mahatma Gandhi die?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,72.00,74.93,229.03,76.66"><head>Table 2 :</head><label>2</label><figDesc>Confidence-weighted scores of the 2 main-task runs compared to the maximum and minimum possible scores given the number of correct answers.</figDesc><table coords="7,78.38,74.93,217.66,33.36"><row><cell>Run name</cell><cell cols="2"># right Score Max</cell><cell>Min</cell></row><row><cell>UdeMmainNoW</cell><cell>37</cell><cell cols="2">0.080 0.266 0.003</cell></row><row><cell>UdeMmainWeb</cell><cell>29</cell><cell cols="2">0.057 0.222 0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,78.38,74.93,444.90,121.42"><head>Table 3 :</head><label>3</label><figDesc>Detailed evaluation of the 2 main-task runs with their confidence-weighted score (CWS).</figDesc><table coords="8,78.38,74.93,444.90,121.42"><row><cell>Run name</cell><cell>Web</cell><cell># wrong</cell><cell># unsupp'd</cell><cell># inexact</cell><cell># right</cell><cell>CWS</cell></row><row><cell>UdeMmainNoW</cell><cell>no</cell><cell>454</cell><cell>2</cell><cell>7</cell><cell>37</cell><cell>0.080</cell></row><row><cell>UdeMmainWeb</cell><cell>yes</cell><cell>425</cell><cell>40</cell><cell>6</cell><cell>29</cell><cell>0.057</cell></row><row><cell>Run name</cell><cell>Web</cell><cell>Strict</cell><cell>Lenient</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UdeMmainNoW</cell><cell>no</cell><cell>7 %</cell><cell>9 %</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UdeMmainWeb</cell><cell>yes</cell><cell>6 %</cell><cell>15 %</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,72.00,209.19,229.02,30.46"><head>Table 4 :</head><label>4</label><figDesc>Strict (right answers only) and lenient (right, inexact and unsupported answers) evaluations of the 2 main-task runs. The score is the ratio over 500 questions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,326.22,697.89,213.77,6.99;3,310.98,707.35,63.08,6.99"><p>Even though this question has a list-task style, it is part of the main task.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We wish to thank <rs type="person">Louis-Julien Guillemette</rs> whose programming skills gave QUANTUM access to the web. We also wish to express our greatest appreciation to the technical support staff of the <rs type="institution">DIRO department of the U. de Montréal. As Murphy</rs>'s law would have predicted, the lab's file server crashed during the week of the competition. Without their fast and skillful response, we could not have made it on time this year. This project was financially supported by the <rs type="funder">Bell University Laboratories (BUL)</rs> and the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,326.48,185.04,213.52,8.74;8,326.48,197.00,213.53,8.74;8,326.48,208.95,213.53,8.74;8,326.48,220.91,213.53,8.74;8,326.48,232.86,213.52,8.74;8,326.48,244.82,61.99,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,471.72,185.04,68.28,8.74;8,326.48,197.00,193.07,8.74">QUANTUM: A Function-Based Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kosseim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,326.48,208.95,213.53,8.74;8,326.48,220.91,213.53,8.74;8,326.48,232.86,90.76,8.74">Proceedings of the 15th Conference of the Canadian Society for Computational Studies of Intelligence (AI 2002)</title>
		<meeting>the 15th Conference of the Canadian Society for Computational Studies of Intelligence (AI 2002)<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="281" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.48,264.74,213.52,8.74;8,326.48,276.70,213.52,8.74;8,326.48,288.65,213.52,8.74;8,326.48,300.61,213.52,8.74;8,326.48,312.56,47.60,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,522.84,264.74,17.16,8.74;8,326.48,276.70,174.89,8.74">The QUANTUM Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kosseim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,520.89,276.70,19.11,8.74;8,326.48,288.65,213.52,8.74;8,326.48,300.61,44.59,8.74">Proceedings of The Tenth Text Retrieval Conference (TREC-X)</title>
		<meeting>The Tenth Text Retrieval Conference (TREC-X)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="157" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.48,332.49,213.52,8.74;8,326.48,344.44,213.52,8.74;8,326.48,356.40,175.11,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,469.71,332.49,70.29,8.74;8,326.48,344.44,47.72,8.74">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.17,344.44,101.95,8.74">Proceedings of TREC-8</title>
		<meeting>TREC-8<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.48,376.32,213.53,8.74;8,326.48,388.28,213.52,8.74;8,326.48,400.24,213.52,8.74;8,326.48,412.19,213.52,8.74;8,326.48,424.15,213.52,8.74;8,326.48,436.10,128.17,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,462.65,388.28,77.34,8.74;8,326.48,400.24,207.70,8.74">MITRE: Description of the Alembic System as used for MUC-6</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aberdeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vilain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,339.15,412.19,200.84,8.74;8,326.48,424.15,63.74,8.74">Proceedings of the Sixth Message Understanding Conference</title>
		<meeting>the Sixth Message Understanding Conference<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman Publishers</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.48,456.03,213.53,8.74;8,326.48,467.98,213.53,8.74;8,326.48,479.94,213.52,8.74;8,326.48,491.89,213.53,8.74;8,326.48,503.85,213.53,8.74;8,326.48,515.80,96.92,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,429.07,467.98,110.94,8.74;8,326.48,479.94,213.52,8.74;8,326.48,491.89,22.14,8.74">Web Reinforced Question Answering (MultiText Experiments for TREC 2001)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Mclearn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,368.14,491.89,171.87,8.74;8,326.48,503.85,96.10,8.74">Proceedings of The Tenth Text Retrieval Conference (TREC-X)</title>
		<meeting>The Tenth Text Retrieval Conference (TREC-X)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="673" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,326.48,535.73,213.53,8.74;8,326.48,547.68,213.53,8.74;8,326.48,559.64,213.53,8.74;8,326.48,571.59,213.54,8.74;8,326.48,583.55,70.88,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,363.91,547.68,154.55,8.74">Data-Intensive Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,326.48,559.64,213.53,8.74;8,326.48,571.59,69.10,8.74">Proceedings of The Tenth Text Retrieval Conference (TREC-X)</title>
		<meeting>The Tenth Text Retrieval Conference (TREC-X)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
