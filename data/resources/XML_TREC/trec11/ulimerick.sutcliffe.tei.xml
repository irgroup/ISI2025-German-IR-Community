<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,82.77,76.73,446.54,18.59">Question Answering using the DLT System at TREC 2002</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,242.61,109.60,126.71,12.64"><forename type="first">Richard</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,82.77,76.73,446.54,18.59">Question Answering using the DLT System at TREC 2002</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EC5EF920544EEFC70D5B89099F97C3B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This article outlines our participation in the Question Answering Track of the Text REtrieval Conference organised by the National Institute of Standards and Technology. Having not taken part before, our objective was to study the task and build a simple working system capable of answering at least some questions correctly. Only three person weeks was available for the work but this proved sufficient to achieve our goal. The article is structured as follows. Firstly, some preliminaries such as our starting point, tools and strategy are described. After this, the architecture of the Documents and Linguistic Technology Group's DLT system is outlined. Thirdly, the question types analysed by the system are described along with the named entities with which they work. Fourthly, the runs performed are presented together with the results we obtained. Finally, conclusions are drawn based on our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Previous Work</head><p>Our first step was to study previous attempts at the problem. Because of the limited time available, only two articles could be examined in detail. The first was <ref type="bibr" coords="1,314.97,499.13,225.12,9.94;1,72.09,512.09,59.71,9.94" target="#b1">Gaizauskas, Wakao, Humphreys, Cunningham and Wilks (1995)</ref>. This describes the Sheffield LaSIE system and along the way provides many hints regarding techniques for effective information extraction and general text processing. The second was <ref type="bibr" coords="1,72.09,538.01,67.27,9.94" target="#b3">Rennert (2002)</ref>. This excellent but unconventional article describes very concisely a number of key techniques used by the author in his 2001 TREC system. These ideas are highly pragmatic in nature and broken down by question type. In building our system we used Rennert's question taxonomy and methods as a starting point though in fact the final system was somewhat different as will be seen later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Initial Tools</head><p>In computational terms, we started with some modest tools comprising a robust multiple pass parser <ref type="bibr" coords="1,72.09,639.41,74.84,9.94" target="#b5">(Sutcliffe, 2000)</ref> which we have used on a number of projects in Japanese <ref type="bibr" coords="1,422.13,639.41,117.96,9.94;1,72.09,652.37,25.76,9.94" target="#b7">(Sutcliffe and Nashimoto, 2000)</ref> and Spanish <ref type="bibr" coords="1,157.89,652.37,98.24,9.94" target="#b4">(Ruiz-Cascales, 2002)</ref> as well as English, a term recogniser and a general approach to natural language processing. To simplify the task we decided not to index the source documents ourselves but instead to use the TOPDOCS files provided by NIST and comprising the actual text of the top 50 matching documents found by the PRISE information retrieval system for each input query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Overall Strategy</head><p>Beside the articles mentioned above we also devoted a small amount of time to looking at questions from previous years (mainly 2001) and establishing the relationship between them and answers in the corresponding documents. This led to an approximate strategy which has much in common with other QA systems and can be summarised as follows:</p><p>• Identify the type of the question;</p><p>• Based on the question type, search for appropriate named entities in the answer texts;</p><p>• Try to find a named entity which co-occurs with keywords from the question;</p><p>• Return the value of the 'best' such named entity as the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture of the System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Outline</head><p>We summarise here the architecture of the DLT system. Firstly, we identify the query type and hence the relevant named entities for which we will be searching. Secondly, we parse the 50 TOPDOCS Files dividing them into textual units using the markup. Thirdly, we search for instances of appropriate named entities in the textual units and mark them. Fourthly, we identify the winning named entity using one of two possible strategies: highest_scoring or most_frequent. We return this as the answer to the query. These stages are now dealt with in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Type Identification</head><p>Having identified the query types to use in the system, we studied questions of each type and developed simple keyword-based heuristics to recognise them. This is a very crude approach adopted due to shortage of time but it was suprisingly effective (Table <ref type="table" coords="2,341.13,440.09,4.10,9.94" target="#tab_1">2</ref>): 425 of the 500 queries were correctly classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Text File Parsing</head><p>The text files are in XML-compliant form so it was easy to parse them without a Document Type Definition (DTD). Each document to be analysed was divided into a series of segments corresponding to a short passage of text. As with so many corpora, the level of markup varies from document to document and indeed we can not be sure that it has been used consistently. The strategy adopted was thus as follows: First, text within a HEADLINE tag was extracted. Second text within a TEXT tag was extracted and divided up into separate Ps. Finally a P was divided wherever three contiguous blanks were found. This last stage was to approximate sentence recognition. the resulting textual units were used in subsequent processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Named Entity Recognition</head><p>The type of question as identified in the first step determines the type of named entity or entities to be searched for. For example if the question type is what_state the entity is us_state i.e. 'California' etc. Each segment identified in the previous step was therefore inspected and all instances of appropriate named entities were identified. The second column shows a sample question for each type. All are drawn from this year' s data except for those question types which did not occur this year (indicated by an asterisk) where a sample from last year is shown. The third column lists the named entities which are used for answering a question of a particular type. The final column shows sample answers. These are all of appropriate types for the question but are not necessarily correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Answer Entity Selection</head><p>We experimented with two methods for selecting an answer which we call highest_scoring and most_frequent. In the first, we returned the named entity occurring in a textual unit which matched the keywords in the query best, chosen from any of the 50 PRISE documents. In the second, we returned the named entity which most frequently occurred in the vicinity of query keywords, observed across all occurrences of the entity in the 50 PRISE documents. Both strategies are unsophisticated but sometimes one or other of them can perform well on a particular query type.</p><p>In the next section we briefly outline the query types identified, the characteristics of the associated named entities and any special issues which affected processing for particular query forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Type</head><p>Classif.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct Classification Incorrect Classification Run 1</head><p>Run 2 </p><formula xml:id="formula_0" coords="4,81.09,85.72,449.94,231.78">Ru n 1 Run 1 C NC R X U W R X U W R X U W R X U W state_bird 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 state_flower 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 what_city 13 0 2 0 1 10 2 0 1 10 0 0 0 0 0 0 0 0 what_state 1 5 0 0 0 1 0 0 0 1 1 0 0 4 1 0 0 4 what_county 3 0 0 0 0 3 0 0 0 3 0 0 0 0 0 0 0 0 what_country 8 1 2 0 0 6 2 0 0 6 0 0 0 1 0 0 0 1 what_continent 2 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 where 40 3 11 1 1 27 7 0 1 32 0 0 0 3 0 0 0 3 how_many3 5 0 2 0 0 3 2 0 0 3 0 0 0 0 0 0 0 0 how_much 13 0 0 0 0 13 1 0 1 11 0 0 0 0 0 0 0 0 distance 21 0 4 0 0 17 4 0 0 17 0 0 0 0 0 0 0 0 speed 3 0 0 0 0 3 0 0 0 3 0 0 0 0 0 0 0 0 temp 2 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 population 4 0 0 0 0 4 0 0 0 4 0 0 0 0 0 0 0 0 who 54 7 5 0 1 48 5 0 1 48 0 0 0 7 0 0 0 7 when_interval 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 length_of_time 7 0 1 0 0 6 1 0 1 5 0 0 0 0 0 0 0 0 when 92 6 15 1 1 75 15 1 1 75 0 0 0 6 0 0 0 6 colour 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 unknown</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Question Types and Corresponding Techniques</head><p>Given that time was short we were forced to build a basic system based on simple premises. We went about this by trying to identify the question types which occurred frequently in the 2001 question set and which could be answered using the above method. By the time the system had to be frozen for evaluation, there were twenty different question types as summarised in Table <ref type="table" coords="4,376.05,469.61,4.14,9.94" target="#tab_0">1</ref>. Many more types could of course be added We outline here the techniques used for each one.</p><p>state_bird: Each state in the United States has its own state bird -a fascinating fact in itself made even more intriguing by the choice of the same bird by different states in several cases. A list of all such birds was readily drawn up. Any instances of these in documents could therefore be identified. This technique was hindered by two facts. Firstly, texts do not necessarily refer to a state bird by its official name -they might say Carolina Wren or even wren instead of Great Carolina Wren. Secondly, discussions about the bird for a particular state tend to occur close to mentions of other states and other state birds.</p><p>state_flower: The same strategy as for state birds was used, with the same limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>what_city:</head><p>The TIPSTER Gazetteer Version 4.0 from the Consortium for Lexical Research (TIPSTER, 1992) was used to create a recogniser for place names. This gazetteer contains 246,907 entries of various kinds, mostly place names. Unfortunately our recogniser was too inefficient to use, so the list had to be reduced temporarily to a set of 199 US cities together with 167 capital cities for other counties. No heuristics were used for city recognition (e.g. X City is probably a city) due to lack of time. Some experiments with 'where' questions during system development suggested that the highest_scoring strategy was not very satisfactory for places because when one city is mentioned others can be also (e.g. in texts about air travel). This was the reason for developing the most_frequent strategy.</p><p>what_state: A list of all US states was readily obtained. Each name has three official forms (e.g. 'Massachusetts' , 'Mass.' and 'MA' ). Unofficial abbreviations are rare so the standard names are probably sufficient (compare this to state flowers and birds above). State names can appear in isolation or in combination with other units to form a location specifier (e.g. 'Boston, MA').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>what_county:</head><p>The TIPSTER Gazetteer contains a complete list of US counties but this proved too long to handle so a simple heuristic was used: Any name of the form X County was deemed to be a US county. This proved quite effective.</p><p>what_country: A list of 162 non-US countries was used together with a list of nine names for the US itself.</p><p>what_continent: A list of seven continents was used.</p><p>where: Templates were devised which specify the form of a place using various combinations of country, state, county and city. The longest possible specifier (i.e. the most complete one) was used whenever a candidate place was found in a text.</p><p>how_many3: A recogniser was developed for numbers by inspecting a large number of examples. Forms recognised include '1' , '1.1' , '1.1 million' , 'sixty four' , 'sixty four million' . Since tokenisation of the text only joined contiguous sequences of alphabetic characters and left all others separate, parsing numbers was fairly straightforward. Each was converted into a canonical representation for comparison purposes. Initially this was an integer in the case of whole numbers but interestingly some numbers found in the texts are so large that this caused integer overflow. A string representation was thus used instead. For how_many3 questions, the units are picked up from the query and must match those used in the document. For example in 'How many chromosomes' the units are 'chromosomes' and these must follow the number found in the text.</p><p>how_much: In these queries the units are not specified in the query and must be deduced from the text. The word following a number is accepted as the units if it is '$' , '%' or another word longer than two characters which is not a number.</p><p>distance: A distance is a number followed by some distance units. Twelve plural and twelve singular distance units were collected by inspection of TREC texts.</p><p>speed: A speed is a number followed by some speed units. 21 plural and eleven singular speed units were collected from the texts.</p><p>temp: A temperature is a number followed by appropriate units with various premodifiers (e.g. minus, '-' ) and postmodifiers (e.g. 'above Absolute Zero' ). Fifteen basic units, two premodifiers and eight postmodifiers were collected from the texts. Interestingly, 'degrees' with no explicit units implies Fahrenheit in a US text.</p><p>population: A population is considered to be any number of value one million or more which occurs in a text portion matching keywords from the query. The unit is assumed to be 'People'. who: To answer questions about names, a simple name recogniser was constructed. This allows prename titles (e.g. 'Sen.' ) a basic name sequence (e.g. Dwight G. Morgan) and a post-name title (e.g. 'III' ).</p><p>Given names are constrained to come from the MOBY given name word list <ref type="bibr" coords="6,412.89,462.53,57.76,9.94" target="#b8">(Ward, 1996)</ref>. Originally the surname was drawn from the MOBY surnames but this proved too restrictive. Any capitalised word is thus accepted as a surname.</p><p>when_interval: These questions ask about a range of dates e.g. 'When is the hurricane season' . A recogniser was thus built which can handle 'from DATE1 through to DATE2' and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>length_of_time:</head><p>A length of time is simply a number followed by one of nineteen different time units.</p><p>Composed lengths (e.g. 4 min 33 sec) are not handled at present.</p><p>when: These imply the occurrence of a date in the text. Accordingly a recogniser for most of the US date forms was developed (e.g. <ref type="bibr" coords="6,202.89,605.81,162.62,9.94">'Saturday, October 17, 1987' etc.)</ref>. The value of a year specifier was constrained to be less than 10,000 if accompanied by 'AD' etc. (either before or after the date) thus allowing cases like '5,000 B. C.' On the other hand years with no 'AD' etc. were constrained to lie within the range 1700 and 2010. The columns indicate the identity of the run, the number of questions, the time to process all questions, the number of documents, words and characters processed, the time to process one query in minutes, and the numbers of words processed per minute and per second. See text for the system specification. The difference in timings for the two systems is caused merely by the fact that results of the SGML parsing were saved the first time around and therefore did not have to be done again.</p><p>colour: A list of nineteen colours was used. No doubt there are more but a longer list could not be found.</p><p>In any case, no colours came up this year, only lists of colours.</p><p>unknown: This is the default query category. Approximately 156 queries fall into it. By definition these can not be answered by the system. As a crude experiment, all unknown queries were treated as if they were 'where', 'when' or 'who' in that order, accepting the first answer found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Runs and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Two Experiments</head><p>We conducted two experiments each of which resulted in one run. All that varied in the runs was the method used for answer selection relative to each of the 20 query types. In the first run, what_continent, where, how_much and length_of_time were answered using the most_frequent strategy while the remainder were answered using highest_scoring. In the second run, all were answered using highest_scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Results are summarised in Tables <ref type="table" coords="7,232.89,452.21,5.52,9.94" target="#tab_1">2</ref> and<ref type="table" coords="7,263.73,452.21,4.14,9.94" target="#tab_2">3</ref>. In Table <ref type="table" coords="7,320.37,452.21,4.14,9.94" target="#tab_1">2</ref>, the two columns entitled 'Classif.' show the number of queries which were classified correctly (C) and incorrectly (NC) broken down by query type.</p><p>Overall therefore, 425 of the 500 queries (85%) were categorised correctly. This seems quite a good result considering that the method used was based solely on identification of keywords and did not involve any structural analysis of the queries. By far the largest category of course is 'unknown' which accounts for approximately 156 queries (31%). We can conclude from this that 31% of the TREC 2002 questions fall completely out of the scope of this system. The remaining columns give counts of answers falling into the standard Right, ineXact, Unsupported and Wrong categories according to NIST assessors. The figures under 'Correct Classification' refer to queries which were correctly classified. Those under 'Incorrect Classification' refer to queries which should not be in the category. Very occasionally these were also answered correctly by chance.</p><p>Table <ref type="table" coords="7,101.85,607.73,5.52,9.94" target="#tab_2">3</ref> shows the overall performance of the system as a simple percentage of answers which were correct, broken down by question type. The Harsh figures take into account incorrectly classified queries as well as correctly classified ones. Clearly incorrectly classified queries will be answered incorrectly in the vast majority of cases, as shown by the last eight columns of Table <ref type="table" coords="7,403.77,646.61,4.14,9.94" target="#tab_1">2</ref>. The Kind figures leave out incorrectly classified queries. We ignore the confidence rating of the system since we built in no mechanism for this. Dealing with Harsh figures, therefore, the overall performance of the system is 9% in Run 1 and 8% in Run 2. The best level of performance is for types 'what_continent' (50%), 'temp' (50%) and 'how_many3' (40%) but these are not representative because the number of each is small.</p><p>Perhaps the optimum view of the system is suggested by the figure of 26% Harsh (28% Kind) for 'where' queries in Run 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Timings</head><p>To give an indication of performance in terms of times, we used a Dell OptiPlex GX200 running NT4.0 at a clock speed of 733 MHz and having 256 Mb RAM. The whole system was written Quintus Prolog Release 3.4. Run times and related figures are provided in Table <ref type="table" coords="8,378.45,163.25,4.14,9.94" target="#tab_3">4</ref>. Interestingly, the SGML parsing component is the slowest in the system and it is this which accounts for the difference in run times shown in the table. Speeding up this component therefore would be the most effective way of increasing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Our objective was to get started in open domain question answering and to make as much progress in three weeks as possible. By the end of the time our system was up and running, giving a performance of 9% overall in Run 1. It has proved a useful experience and has helped us to focus on possible techniques for answering closed domain questions such as those analysed in <ref type="bibr" coords="8,367.53,294.17,138.55,9.94" target="#b6">Sutcliffe and Kurohashi (2000)</ref>. There are of course many next steps to be taken which we summarise below:</p><p>• Analysing the query in more detail by parsing and hence identifying query type more accurately together with other information (e.g. the units in less simple cases such as 'How many pieces of clothing'); • Tidying up existing named entity recognisers and checking their performance;</p><p>• Adopting answer patterns rather than just using keyword co-occurrence, e.g. in the manner of <ref type="bibr" coords="8,515.85,399.05,24.36,9.94;8,90.09,412.01,52.08,9.94" target="#b2">Hovy et al. (2002)</ref>; • Determining other simple query types which account for the majority of the queries currently classified as unknown;</p><p>• Designing a strategy for identifying queries which have no answers;</p><p>• Developing the ability to recognise ad hoc named entities. For example in 'What type of car is used by Linda Ronstadt' we need to know all the car types despite having no pre-planned recogniser for them;</p><p>• Assigning a confidence rating to responses.</p><p>To finish on a more general note, two issues came to mind during this research. Firstly, a general theme within question answering seems to be the development of increasingly sophisticated pattern matching devices based on more and more detailed classifications of questions. To what extent however, will this lead to general findings about question answering or natural language processing rather than more and more brittle sets of rules?</p><p>Secondly, in TREC we can not always predict what types of question might be asked in the next competition. There were a few surprises this year, two examples being 1641 'Where did ' N Sync get their name?' and 1710 'What are the colors of the Italian flag?' . The first question is open ended in scope while the second involves lists of objects which we had assumed would be restricted to the list sub-track. Perhaps it would be fair for the general characteristics of new query types to be indicated so that some kind of strategy could be worked out for them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,86.73,77.38,441.18,379.29"><head>Table 1 : Question Types used in the DLT system.</head><label>1</label><figDesc></figDesc><table coords="3,86.73,77.38,441.18,353.85"><row><cell>Question</cell><cell>Example Question</cell><cell>Named Entities</cell><cell>Candidate Answer</cell></row><row><cell>Type</cell><cell></cell><cell></cell><cell></cell></row><row><cell>state_bird</cell><cell>1517 What is the state bird of Alaska?</cell><cell>state_bird</cell><cell>Mockingbird</cell></row><row><cell>state_flower</cell><cell>1008* What is Hawaii's state flower?</cell><cell>state_flower</cell><cell>Yellow hibiscus</cell></row><row><cell>what_city</cell><cell>1520 What is the capital of Kentucky?</cell><cell>us_city, non_us_city</cell><cell>Houston</cell></row><row><cell>what_state</cell><cell>1743 Which state has the longest</cell><cell>us_state</cell><cell>Calif.</cell></row><row><cell></cell><cell>coastline on the Atlantic Ocean?</cell><cell></cell><cell></cell></row><row><cell>what_county</cell><cell>1875 What county is St. Paul, Minnesota</cell><cell>us_county</cell><cell>Orange County</cell></row><row><cell></cell><cell>in?</cell><cell></cell><cell></cell></row><row><cell cols="2">what_country 1496 What country is Berlin in?</cell><cell>country</cell><cell>Germany</cell></row><row><cell cols="2">what_continent 1489 What continent is India on?</cell><cell>continent</cell><cell>Asia</cell></row><row><cell>where</cell><cell>1500 Where is Georgetown University?</cell><cell>us_city, non_us_city</cell><cell>Phoenix, Ariz.</cell></row><row><cell></cell><cell></cell><cell>us_state, us_county</cell><cell></cell></row><row><cell></cell><cell></cell><cell>country</cell><cell></cell></row><row><cell>how_many3</cell><cell>1404 How many chromosomes does a</cell><cell>num</cell><cell>two chromosomes</cell></row><row><cell></cell><cell>human zygote have?</cell><cell></cell><cell></cell></row><row><cell>how_much</cell><cell>1571 How much copper is in a penny?</cell><cell>num</cell><cell>20 percent</cell></row><row><cell>distance</cell><cell>1792 How far is it from Buffalo, New</cell><cell>num, distance</cell><cell>100 miles</cell></row><row><cell></cell><cell>York to Syracuse, New York?</cell><cell></cell><cell></cell></row><row><cell>speed</cell><cell>1471 How fast does a cheetah run?</cell><cell>num, speed</cell><cell>60 miles an hour</cell></row><row><cell>temp</cell><cell>1606 What is the boiling point of water?</cell><cell>temp</cell><cell>212 degrees Fahrenheit</cell></row><row><cell>population</cell><cell>1750 What is Mexico's population?</cell><cell>num, population</cell><cell>one hundred million</cell></row><row><cell></cell><cell></cell><cell></cell><cell>people</cell></row><row><cell>who</cell><cell>1395 Who is Tom Cruise married to?</cell><cell>proper_name</cell><cell>Nicole Kidman</cell></row><row><cell cols="2">when_interval 1056* When is hurricane season in the</cell><cell>date, interval</cell><cell>from June 1 to Nov. 30</cell></row><row><cell></cell><cell>Caribbean?</cell><cell></cell><cell></cell></row><row><cell cols="2">length_of_time 1763 How old is the universe?</cell><cell>num, length_of_time</cell><cell>5 billion years</cell></row><row><cell>when</cell><cell>1698 When was Julius Caesar born?</cell><cell>date</cell><cell>100 B.C.</cell></row><row><cell>colour</cell><cell>1193* What color is a giraffe's tongue?</cell><cell>colour</cell><cell>black</cell></row><row><cell>unknown</cell><cell>1641 Where did 'N Sync get their name?</cell><cell>N/A</cell><cell>N/A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,81.09,308.04,449.88,79.47"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="4,81.09,308.04,449.88,20.02"><row><cell></cell><cell>156 53</cell><cell>1</cell><cell>2</cell><cell>1 152</cell><cell>1</cell><cell>2</cell><cell>1 152</cell><cell>4</cell><cell>1</cell><cell>1 47</cell><cell>4</cell><cell>1</cell><cell>1 47</cell></row><row><cell>Totals</cell><cell cols="2">425 75 45</cell><cell>4</cell><cell cols="2">5 371 42</cell><cell>3</cell><cell>7 373</cell><cell>5</cell><cell>1</cell><cell>1 68</cell><cell>5</cell><cell>1</cell><cell>1 68</cell></row></table><note coords="4,149.01,343.18,355.06,10.29;4,108.09,355.03,396.06,8.96;4,108.09,366.79,396.06,8.96;4,108.09,378.55,288.79,8.96"><p>Results by Query Type. The columns C and NC show the numbers of queries of a particular type which were classified correctly and not correctly. Those classified correctly are then broken down into Right, ineXact, Unsupported and Wrong for each of the two runs Run 1 and Run 2. Finally, those classified incorrectly are broken down in the same way.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,108.09,75.34,396.18,345.05"><head>Table 3 :</head><label>3</label><figDesc>Overall Performance. The harsh figure in each case indicates the percentage of queries which were classified as being of that particular type (either rightly or wrongly) and which were answered correctly. The kind figure indicates the percentage of queries which were rightly classified as being of that type and which were answered correctly.</figDesc><table coords="6,169.05,75.34,269.36,284.97"><row><cell></cell><cell>Run 1</cell><cell>Run 1</cell><cell>Run 2</cell><cell>Run 2</cell></row><row><cell></cell><cell cols="4">Harsh % Kind % Harsh % Kind %</cell></row><row><cell>state_bird</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>state_flower</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>what_city</cell><cell>15</cell><cell>15</cell><cell>15</cell><cell>15</cell></row><row><cell>what_state</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>what_county</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>what_country</cell><cell>22</cell><cell>25</cell><cell>22</cell><cell>25</cell></row><row><cell>what_continent</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell>where</cell><cell>26</cell><cell>28</cell><cell>16</cell><cell>18</cell></row><row><cell>how_many3</cell><cell>40</cell><cell>40</cell><cell>40</cell><cell>40</cell></row><row><cell>how_much</cell><cell>0</cell><cell>0</cell><cell>8</cell><cell>8</cell></row><row><cell>distance</cell><cell>19</cell><cell>19</cell><cell>19</cell><cell>19</cell></row><row><cell>speed</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>temp</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell>population</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>who</cell><cell>8</cell><cell>9</cell><cell>8</cell><cell>9</cell></row><row><cell>when_interval</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>length_of_time</cell><cell>14</cell><cell>14</cell><cell>14</cell><cell>14</cell></row><row><cell>when</cell><cell>15</cell><cell>16</cell><cell>15</cell><cell>16</cell></row><row><cell>colour</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>unknown</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell></row><row><cell>Totals</cell><cell>9</cell><cell>11</cell><cell>8</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.09,644.69,468.00,22.90"><head>Table 4 : Timings and Counts for DLT.</head><label>4</label><figDesc>Dates in 'slash' or 'dot' formats (e.g. '02.06.02' or '02/06/02' to mean June 2, 2002 or perhaps February 6, 2002) are not handled.</figDesc><table coords="7,94.17,75.34,418.76,34.41"><row><cell>Run</cell><cell>Qns Time</cell><cell>Docs</cell><cell>Words</cell><cell>Characters Min/Q</cell><cell cols="2">Words/Min Words/Sec</cell></row><row><cell>Run 1</cell><cell>500 11h 0m</cell><cell cols="3">25,000 15,000,000 93,000,000 1.3</cell><cell>23,000</cell><cell>383</cell></row><row><cell>Run 2</cell><cell>500 7h 5m</cell><cell cols="3">25,000 15,000,000 93,000,000 0.85</cell><cell>35,294</cell><cell>588</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,78.08,89.31,84.41,16.49" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.09,115.01,467.96,9.94;9,72.09,127.97,467.98,9.94;9,72.09,140.93,319.72,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,478.65,115.01,61.40,9.94;9,72.09,127.97,286.66,9.94">University of Sheffield: Description of the LaSIE System as used for MUC-6</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wakao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,385.77,127.97,154.30,9.94;9,72.09,140.93,119.89,9.94">Proceedings of the Sixth Message Understanding Conference</title>
		<meeting>the Sixth Message Understanding Conference</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.09,166.85,388.13,9.94;9,72.09,179.81,418.57,9.94" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,173.49,166.85,209.59,9.94">A Typology of over 140 Question-Answer Types</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/natural-language/projects/webclopedia/Taxonomy/taxonomy_toplevel.html" />
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.09,205.73,467.96,9.94;9,72.09,218.69,468.00,9.94;9,72.09,231.65,467.95,9.94;9,72.09,244.61,92.61,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,170.49,205.73,132.41,9.94">Word proximity QA system</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rennert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,72.09,218.69,256.30,9.94">Proceedings of the Tenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2002. 2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.09,270.53,468.00,9.94;9,72.09,283.49,175.69,9.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,189.09,270.53,316.88,9.94">A Specification and Validating Parser for Simplified Technical Spanish</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz-Cascales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Limerick, Ireland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.Sc. Thesis</note>
</biblStruct>

<biblStruct coords="9,72.09,309.41,467.97,9.94;9,72.09,322.51,467.98,11.40;9,72.09,335.57,200.19,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,187.89,309.41,296.21,9.94">Using A Robust Layered Parser to Analyse Technical Manual Text</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,491.61,309.41,48.45,9.94;9,72.09,322.61,88.69,9.94;9,234.81,322.61,305.26,9.94;9,72.09,335.57,66.51,9.94">Número Monográfico: Corpus-based Research in English Language and Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="189" />
			<date type="published" when="2000">2000</date>
			<pubPlace>Spain</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Murcia</orgName>
		</respStmt>
	</monogr>
	<note>Cuadernos de Filología Inglesa</note>
</biblStruct>

<biblStruct coords="9,72.09,361.49,468.00,9.94;9,72.09,374.45,468.00,9.94;9,72.09,387.41,397.28,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,312.81,361.49,227.28,9.94;9,72.09,374.45,202.68,9.94">A Parallel English-Japanese Query Collection for the Evaluation of On-Line Help Systems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,284.01,374.45,256.08,9.94;9,72.09,387.41,160.93,9.94">Proceedings of the Second International Conference on Language Resources and Evaluation</title>
		<meeting>the Second International Conference on Language Resources and Evaluation<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05-31">2000. 31 May -2 June, 2000</date>
			<biblScope unit="page" from="1665" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.09,413.33,468.00,9.94;9,72.09,426.29,467.95,9.94;9,72.09,439.25,224.77,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,269.01,413.33,266.34,9.94">Robust Parsing of Japanese Technical Text: An Initial Study</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,72.09,426.29,421.18,9.94">Proceedings of the Eleventh Irish Conference on Artificial Intelligence and Cognitive Science</title>
		<meeting>the Eleventh Irish Conference on Artificial Intelligence and Cognitive Science</meeting>
		<imprint>
			<date type="published" when="2000-08">2000. August, 2000</date>
			<biblScope unit="page" from="23" to="25" />
		</imprint>
		<respStmt>
			<orgName>National University of Ireland Galway</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.09,465.17,324.86,9.94;9,72.09,491.09,373.22,9.94" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tipster ; Ward</surname></persName>
		</author>
		<idno>Gazetteer 4.0</idno>
		<ptr target="ftp://ftp.dcs.shef.ac.uk/share/ilash/Moby" />
		<imprint>
			<date type="published" when="1992">1992. 1996</date>
		</imprint>
	</monogr>
	<note type="report_type">MOBY Project Wordlists</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
