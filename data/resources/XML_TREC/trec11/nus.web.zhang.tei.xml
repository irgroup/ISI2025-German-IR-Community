<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,74.16,116.63,448.19,21.61;1,113.04,143.51,370.72,21.61">Web based Pattern Mining and Matching Approach to Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,136.32,174.14,52.88,10.51"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
							<email>dell.z@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<address>
									<postCode>S15-05-24</postCode>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University</orgName>
								<address>
									<postCode>117543</postCode>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Singapore-MIT Alliance</orgName>
								<address>
									<addrLine>4 Engineering Drive</addrLine>
									<postCode>E4-04-10</postCode>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<address>
									<postCode>S15-05-24</postCode>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">National University</orgName>
								<address>
									<postCode>117543</postCode>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Singapore-MIT Alliance</orgName>
								<address>
									<addrLine>4 Engineering Drive</addrLine>
									<postCode>E4-04-10</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,393.84,174.14,63.03,10.51"><forename type="first">Wee</forename><forename type="middle">Sun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<address>
									<postCode>S15-05-24</postCode>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University</orgName>
								<address>
									<postCode>117543</postCode>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Singapore-MIT Alliance</orgName>
								<address>
									<addrLine>4 Engineering Drive</addrLine>
									<postCode>E4-04-10</postCode>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<address>
									<postCode>S15-05-24</postCode>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">National University</orgName>
								<address>
									<postCode>117543</postCode>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Singapore-MIT Alliance</orgName>
								<address>
									<addrLine>4 Engineering Drive</addrLine>
									<postCode>E4-04-10</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Science Drive</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<postCode>117576</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Science Drive</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<address>
									<postCode>117576</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,74.16,116.63,448.19,21.61;1,113.04,143.51,370.72,21.61">Web based Pattern Mining and Matching Approach to Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">335870DA597F596CDC4E867AEAC9E8D3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe herein a Web based pattern mining and matching approach to question answering. For each type of questions, a lot of textual patterns can be learned from the Web automatically, using the TREC QA track data as training examples. These textual patterns are assessed by the concepts of support and confidence, which are borrowed from the data mining community. Given a new unseen question, these textual patterns can be utilized to extract and rank the plausible answers on the Web. The performance of this approach has been evaluated also by the TREC QA track data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>What a current information retrieval system or search engine can do is just "document retrieval", i.e., given some keywords it only returns the relevant documents that contain the keywords. However, what a user really wants is often a precise answer to a question. TREC has launched a QA track to support the competitive research on question answering, from 1999 (TREC8). The focus of TREC QA track is to build a fully automatic open-domain question answering system, which can answer factual questions based on very large document. Today, the TREC QA track [V0099][Voo00] <ref type="bibr" coords="1,414.18,501.98,39.68,10.51" target="#b7">[Voo01]</ref> is the major largescale evaluation environment for open-domain question answering systems.</p><p>Most of the recent question answering systems [V0099][Voo00] <ref type="bibr" coords="1,356.75,534.62,39.60,10.51" target="#b7">[Voo01]</ref> require sophisticated linguistic knowledge or tools, such as parser, named-entity recognizer, ontology, WordNet, etc. However, at the TREC10 QA track <ref type="bibr" coords="1,147.86,561.50,37.37,10.51" target="#b7">[Voo01]</ref>, the best performing system just used many textual patterns <ref type="bibr" coords="1,485.54,561.50,30.26,10.51" target="#b4">[SS01]</ref>. The power of textual patterns for question answering looks quite amazing and stimulating to us.</p><p>We describe herein a Web based pattern mining and matching approach to question answering. For each type of questions, a lot of textual patterns can be learned from the Web automatically, using the TREC QA track data as training examples. These textual patterns are assessed by the concepts of support and confidence, which are borrowed from the data mining community. Given a new unseen question, these textual patterns can be utilized to extract and rank the plausible answers on the Web. The performance of this approach has been evaluated also by the TREC QA track data.</p><p>To illustrate our approach, we would like to use the question "Who was the first American in space?" as a running sample in the following sections. This question was the No.21 test question in the TREC8 QA track. Figure <ref type="figure" coords="2,262.32,377.90,4.46,10.51">1</ref>, system architecture.</p><p>As shown in Figure <ref type="figure" coords="2,148.73,397.10,4.44,10.51">1</ref>, the system entails two main functions, one is learning and the other is answering. For both functions, the question has to be pre-processed by the transforming and recognizing module.</p><p>The answering part of the system relies on the textual patterns. A textual pattern can be in either of the following two forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>_Q_ &lt;intermediate string&gt; _A_ &lt;boundary string&gt; &lt; boundary string&gt; _A_ &lt;intermediate string&gt; _Q_</head><p>Here, _Q_ stands for the question key phrase and _A_ stands for the potential answer. The key phrase of a question is a continuous sequence of words in that question, which represents the primary object or event the question asking about. For instance, the key phrase of the sample question would be "the first American in space". A textual pattern actually describes the context of some potential answers to a specific class of questions.</p><p>Such textual patterns can be learned using some question-answer pairs as training examples. For instance, the correct answer to the sample question can be found in the string "In 1961, Alan Shepard became the first American in space ……", this observation suggests that the textual pattern ", _A_ became _Q_" can be used to answer similar questions like "Who was the first U.S. president?", "Who was the second man to walk on the moon?", and so on.</p><p>The learning part of the system will take advantage of the TREC QA track data as training examples. In each year's competition, TREC organizers issue several hundred questions to test the participant systems, and later release the regular expressions indicating the correct answers to each test question. Such TREC questions along with their answer regular expressions are our training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transforming</head><p>The transforming algorithm attempts to guess how the answer to the question may appear in a target sentence, i.e. a sentence that contains the answer, and then transforms the question to that format. We hope there will be more chances to find the answer after transforming the question. Currently two transforming methods are used.</p><p>For questions with an auxiliary do-verb and a main verb, the target sentence is likely to contain the verb in the conjugated form rather than separate verbs. For instance, the answer to the question "When did Nixon visit China?" would more likely to occur in the target sentence as "…… Nixon visited China ……" rather than "…… did Nixon visit China ……". So we transform the question by conjugating the auxiliary do-verb and the main verb. To locate the main verb like "visit" in the question, we have to parse the question using the MEI parser <ref type="bibr" coords="3,302.13,235.34,36.15,10.51" target="#b1">[Cha99]</ref>. To find the converted verb like "visited", we utilize PC-KIMMO <ref type="bibr" coords="3,164.16,248.78,35.11,10.51" target="#b0">[Ant90]</ref>.</p><p>For questions with an auxiliary be-verb and a main verb (past particular), the target sentence is likely to contain these two verbs continuously together. For instance, the answer to the question "When was the telephone invented?" would more likely to occur in the target sentence as "……the telephone was invented ……" rather than "……was the telephone invented ……". So we transform the question by moving the auxiliary be-verb to the place just before the main verb. Again the MEI-parser is used to locate the main verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recognizing</head><p>The recognizing algorithm determines the class of the question and the key phrase of the question. This was done by finding the appropriate template of the question.</p><p>Currently, we have defined 22 question classes, and each class has several templates formulated as regular expressions which indicate the possible appearance of this type questions. For instance, some of the templates in the ACRONYM class are as the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is _Q_ What is the meaning of _Q_ What the (?:acronym|abbreviation) _Q_ (?:stands for|means) What _Q_ (?:stands for|means) What the initials _Q_ (?:stand for|mean) _Q_ is (?:an|the) (?:acronym|abbreviation) for what _Q_ stands for what</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>Assume the set of QA examples is E . Each QA example ( , )</p><formula xml:id="formula_0" coords="3,351.60,561.20,54.20,13.10">i i i e q rea =</formula><p>consists of two parts, the question i q and the regular expression of its correct answer i rea . The following algorithms can learn the textual patterns of a specific question class c from the Web. For each ( ) ( )   ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Construct Snippet Database</head><formula xml:id="formula_1" coords="3,53.04,634.48,417.12,15.09">Given the QA examples of class c , { } ( ) ( , )| ( , ) ^( ) c i i i i i E q rea q rea E class q c = ∈ = ,</formula><formula xml:id="formula_2" coords="3,88.08,675.99,104.86,30.90">c i i q rea E ∈ , do { Submit ( )</formula><p>i kp q as a phrase along with the words in i q to a Web search engine, such as Google (http://www.google.com/). Grab the search result returned by the search engine.</p><p>Extract the text snippets from the search result.</p><p>For each snippet ij s , do { Insert a special symbol "_&lt;_" at the head of ij s .</p><p>Append a special symbol "_&gt;_" at the tail of ij s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replace every</head><p>( ) i kp q with a special symbol "_Q_".</p><p>Find the answer i a using the answer regular expression i rea .</p><p>Replace every i a with a special symbol "_A_".</p><formula xml:id="formula_3" coords="4,53.04,228.38,140.97,38.81">Add ij s to ( ) c S . } }</formula><p>For instance, the sample question "Who was the first American in space?" and its answer regular expression "((Alan (B\. )?)?Shephard)" can be taken as a QA example of the WHO-IS class. The following query will be submitted to Google, and then get the search result.</p><p>"the first American in space" Who was the first American in space?</p><p>A small portation of the snippet database of the WHO-IS class is shown in Figure <ref type="figure" coords="4,437.53,338.78,4.45,10.51">2</ref>.</p><p>Figure <ref type="figure" coords="4,252.48,478.46,4.58,10.51">2</ref>, sample snippet database Two special symbols "_&lt;_" and "_&gt;_" are used to simplify the algorithm, where "_&lt;_" stands for the head of the snippet and "_&gt;_" stands for the tail of the snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Discover Textual Patterns</head><p>Given the snippet database of class c , ( ) c S , the following algorithm can automatically discover the textual patterns of class c , ( ) c P . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>}</head><p>In fact, a textual pattern can be considered as an association rule "context =&gt; answer". The concepts support and confidence are borrowed from the data mining community.</p><p>Some of the assessed textual patterns of the WHO-IS class is shown in Figure <ref type="figure" coords="5,419.58,518.79,4.44,10.51">4</ref>.</p><p>Figure <ref type="figure" coords="5,233.72,634.95,4.58,10.51">4</ref>, sample assessed textual patterns</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answering</head><p>Having the assessed textual patterns of each question, the following algorithm can be employed to answer a new unseen question.</p><p>For a new question new q , determine its class c and its key phrase ( ) new kp q</p><p>, by transforming and recognizing algorithms. </p><formula xml:id="formula_4" coords="6,53.04,317.42,348.76,140.10">q A yet) then { Add jk a to ( ) q A Let confidence( ) confidence( ) jk k a p = } else { Increase confidence( ) jk a by confidence( ) k p } } } }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remove the unreasonable answers in ( ) q</head><p>A using the stop-answers list and class-specific filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sort all the found answers in ( ) q</head><p>A by their confidence value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Return the ordered top-N answers in</head><formula xml:id="formula_5" coords="6,227.04,489.90,21.69,12.56">( ) q A .</formula><p>For instance, the answer to the sample question can be extracted from the snippet "Alan Shepard was the first American in space ……" with confidence 0.11 using a textual pattern of the WHO-IS class "_&lt;_ _A_ was _Q_".</p><p>The list of stop-answers contains the strings which have no chance to be a correct answer in our opinion, such as "he", "today", "http", etc. And for each class of questions, we apply a class-specific filter which can help to remove the answers not for this class, e.g., a date class filter rejects a location string even it matches the textual pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Several experiments have been done to evaluate the performance of this approach, using the data from TREC8, TREC9 and TREC10. The questions with typo mistakes, the definition style questions like "Who is Colin Powell?", the questions which are syntactic rewrites of earlier questions (TREC9 test questions No.701-893), and the questions with no associated answer regular expressions have been removed from the data set. Note all the Web search results were retrieved from Google in the period July --August 2002.  <ref type="table" coords="7,125.51,181.58,4.45,10.51">1</ref>, the performance of this approach on TREC8, TREC9 and TREC10 questions.</p><p>The experiment results are shown in Table <ref type="table" coords="7,258.32,201.02,4.46,10.51">1</ref>. Here t# means the number of test questions, e# means the number of training examples, r# represents the how many questions the system has returned some answers for, c# represents how many questions the system has correctly answered. The MRR (Mean Reciprocal Rank) metric was used in TREC8, TREC9 and TREC10. Here MRR_all represents the MRR score over all test questions, while MRR_ret represents the MRR score over the questions which the system has returned some answers for.</p><p>The MRR score of this approach is not as high as that of the best question answering system in TREC. This discrepancy is due to many reasons. One important factor is that the answer regular expressions provided by TREC are quite limited, many correct answers such as "Alan B. Shepard, Jr." are judged wrong since they do not occur in the TREC specified document collection. Another interesting issue is time, the correct answers to some questions like "Who is the U.S. president?" will change over time. The Web is also messier than the TREC document collection.</p><p>This approach works quite well on simple questions, e.g. questions about ACRONYM, AUTHOR, BIRTHDATE, etc. The overall MRR score (MRR-all) for AUTHOR questions in TREC8 are above 0.55, while using the AUTHOR questions and answers in TREC9 and TREC10 as training examples. The performance of LAMP will dramatically drop down when the length of the question becomes longer, for instance, a TREC8 question averagely contains 9.93 words, but a correctly answered TREC8 question averagely contains 6.75 words.</p><p>The performance of this approach on TREC11 questions is as follows.</p><p>The above experiment results imply that this approach has high precision but low recall, i.e., this approach prefers "no answer" rather than "wrong answer". We think this property is good because "wrong answer" is usually worse than "no answer". And, this property allows this approach to be easily augmented with other approaches.</p><p>To increase recall for our TREC 11 entry, we augmented this approach with a simple answer extractor based on Support Vector Machines (SVM) [CS00] trained using features constructed from words in the question, words in the candidate sentence and the POS (Part-of-Speech) tags of words neighboring a candidate exact answer. To find a supporting document, we return the highest ranked document (in the list returned by the organizers) that contains the answer returned by the system.</p><p>The performance of the hybrid system on TREC 11 questions is as follows.</p><p>CWS (Confidence Weighted Score): 0.396 Precision of recognizing no answer is 0 / 2 = 0.000 Recall of recognizing no answer is 0 / 46 = 0.000 CWS (Confidence Weighted Score): 0.458 Precision of recognizing no answer is 39 / 293 = 0.133 Recall of recognizing no answer is 39 / 46 = 0.848 This approach distinguishes itself by its simplicity. It just uses the snippets in the Web search results, since it is time-consuming to download and analyze the original web documents. It does not require any sophisticated natural language processing on snippets, and does not need any advanced data structure, just simple hash table is enough. Because of this simplicity, LAMP is very efficient, which makes it perfect for online question answering.</p><p>One limitation of this approach is that one textual pattern can include only one question key phrase in the current stage. It does not work for the questions having multiple key phrases, possibly apart from each other. For example, to answer the question "How many calories are there in a Big Mac?", it would be better to use two question key phrases, "calories" and "Big Mac". Another drawback is that the textual patterns cannot handle long-distance relationships between the question key phrase and the answer. For example, the textual pattern "_Q_ became _A_," cannot locate the answer in the text snippet "Alan Shepard, who in 1961 became the first American in space and ……". However, the abundance and variation of the Web information makes it feasible to find answers on the Web with high probability through this approach, because the factual knowledge is usually replicated across the Web, expressed in many different forms <ref type="bibr" coords="8,155.06,326.54,46.11,10.51" target="#b3">[BLB+01]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,476.88,636.97,66.96,10.37;3,53.04,656.17,326.64,10.67;3,391.68,654.07,8.53,6.18;3,394.32,654.07,3.05,6.18;3,384.48,656.17,20.40,10.69"><head></head><label></label><figDesc>the following algorithm can automatically construct the snippet database of class c , ( ) c S .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,195.12,205.82,206.38,10.51;5,53.04,225.26,144.22,10.51;5,53.04,246.86,185.56,10.51;5,249.12,244.70,8.49,6.09;5,251.76,244.70,3.01,6.09;5,241.44,246.60,205.96,10.78;5,459.84,244.94,8.49,6.09;5,462.48,244.94,3.00,6.09;5,452.88,246.85,91.02,10.53;5,53.04,261.74,266.24,10.51;5,331.92,259.74,8.77,6.19;5,334.80,259.74,3.06,6.19;5,324.24,261.59,21.40,10.71"><head></head><label></label><figDesc>Figure 3, sample discovered textual patterns 3.3.3 Assess Textual Patterns Given the textual patterns of class c , ( ) c P , and the snippet database of class c , ( ) c S , the following algorithm can automatically assess the textual patterns in ( ) c P .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,136.32,130.28,324.00,245.28"><head></head><label></label><figDesc></figDesc><graphic coords="2,136.32,130.28,324.00,245.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,61.20,112.92,230.10,516.44"><head>phrase along with the words in new q to a Web search engine, such as Google (http://www.google.com/). Grab the search result returned by the search engine. Extract the text snippets from the search result.</head><label></label><figDesc></figDesc><table coords="5,61.20,112.92,230.10,516.44"><row><cell cols="2">…… , _A_ became _Q_ _&lt;_ _A_ was _Q_ _Q_ was _A_, _A_ made history as _Q_ by _A_ ( _Q_ Submit ( ) new kp q as a For each snippet j s , do {</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">_Q_ , _A_ _&gt;_ Insert a special symbol "_&lt;_" at the head of j s . …… Append a special symbol "_&gt;_" at the tail of j s .</cell></row><row><cell>Replace every</cell><cell>( kp q</cell><cell>new</cell><cell>)</cell><cell cols="2">with a special symbol "_Q_".</cell></row><row><cell cols="4">For each textual pattern</cell><cell>k p P ∈</cell><cell>( ) c</cell><cell>, do {</cell></row><row><cell cols="6">Translate the textual pattern k p into a regular expression ( ) k re p , the special symbol</cell></row><row><cell cols="6">"_A_" are replaced by "(.*?)", means this part can be matched by any string.</cell></row><row><cell cols="5">If j s can match ( )</cell></row><row><cell>……</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>confidence</cell></row><row><cell>, _A_ became _Q_</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.09</cell></row><row><cell>_&lt;_ _A_ was _Q_</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.11</cell></row><row><cell>_Q_ was _A_,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell></row><row><cell cols="5">_&lt;_ _A_ made history as _Q_</cell><cell>1.00</cell></row><row><cell>by _A_ ( _Q_</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.66</cell></row><row><cell>_Q_ , _A_ _&gt;_</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.14</cell></row><row><cell>……</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="6,220.08,287.67,3.00,6.07;6,198.72,281.53,64.78,10.37;6,158.16,300.84,324.46,9.08;6,494.16,306.15,4.92,6.07;6,486.96,300.01,17.61,10.37;6,158.16,320.28,14.55,9.08;6,181.44,325.83,4.92,6.07;6,174.24,319.69,55.19,10.37;6,241.92,317.42,8.97,6.09"><p><p><p>k re p , then { Take the string corresponding to the "(.*?)" part as a plausible answer jk a .</p>If ( jk a is not in</p>( )   </p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,123.11,369.50,420.53,10.51;8,123.12,382.70,202.37,10.51" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,202.56,369.50,299.99,10.51">PC-KIMMO: a two-level processor for morphological analysis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Antworth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Summer Institute of Linguistics</publisher>
			<pubPlace>Dallas, TX</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,123.10,402.14,420.35,10.51;8,123.12,415.34,268.16,10.51" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,187.44,402.14,176.33,10.51">A Maximum-Entropy-Inspired Parser</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
		<idno>CS-99-12</idno>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
		</imprint>
		<respStmt>
			<orgName>Brown University, Computer Science Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,123.12,434.78,420.55,10.51;8,123.12,448.22,244.34,10.51" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,311.52,434.78,226.78,10.51">An Introduction to Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,123.10,467.42,420.46,10.51;8,123.12,480.86,125.12,10.51;8,248.64,478.30,5.77,6.94;8,259.68,480.86,283.99,10.51;8,123.12,494.30,165.68,10.51" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,359.38,467.42,166.23,10.51">Data-Intensive Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,138.24,480.86,110.00,10.51;8,248.64,478.30,5.77,6.94;8,259.68,480.86,176.71,10.51">Proceedings of the 10 th Text Retrieval Conference (TREC10)</title>
		<meeting>the 10 th Text Retrieval Conference (TREC10)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="183" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,123.08,513.50,420.48,10.51;8,123.12,527.18,277.76,10.51;8,401.04,524.38,6.01,6.94;8,413.52,527.18,129.98,10.51;8,123.12,540.38,268.88,10.51" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,325.42,513.50,218.14,10.51;8,123.12,527.18,136.04,10.51">Patterns of Potential Answer Expressions as Clues to the Right Answer</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,287.52,527.18,113.36,10.51;8,401.04,524.38,6.01,6.94;8,413.52,527.18,129.98,10.51">Proceedings of the 10 th Text Retrieval Conference</title>
		<meeting>the 10 th Text Retrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,123.12,559.82,414.33,10.51;8,537.84,557.26,5.77,6.94;8,123.12,573.02,374.02,10.51" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,188.66,559.82,227.33,10.51">The TREC-8 Question Answering Track Report</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,438.24,559.82,99.21,10.51;8,537.84,557.26,5.77,6.94;8,123.12,573.02,165.27,10.51">Proceedings of the 8 th Text Retrieval Conference (TREC8)</title>
		<meeting>the 8 th Text Retrieval Conference (TREC8)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,123.12,592.46,420.66,10.51;8,123.12,605.90,5.84,10.51;8,129.12,603.10,6.01,6.94;8,137.76,605.90,374.20,10.51" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,187.21,592.46,247.25,10.51">Overview of the TREC-9 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,455.28,592.46,88.50,10.51;8,123.12,605.90,5.84,10.51;8,129.12,603.10,6.01,6.94;8,137.76,605.90,122.78,10.51">Proceedings of the 9 th Text Retrieval Conference</title>
		<meeting>the 9 th Text Retrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,123.13,625.10,420.59,10.51;8,123.12,638.54,30.80,10.51;8,154.08,635.98,6.01,6.94;8,164.88,638.54,378.50,10.51;8,123.12,651.98,26.47,10.51" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,187.22,625.10,264.79,10.51">Overview of the TREC 2001 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,472.80,625.10,70.93,10.51;8,123.12,638.54,30.80,10.51;8,154.08,635.98,6.01,6.94;8,164.88,638.54,177.48,10.51">Proceedings of the 10 th Text Retrieval Conference (TREC10)</title>
		<meeting>the 10 th Text Retrieval Conference (TREC10)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="157" to="165" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
