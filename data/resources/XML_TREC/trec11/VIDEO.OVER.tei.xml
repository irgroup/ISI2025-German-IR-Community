<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.33,101.81,257.07,15.39">The TREC-2002 Video Track Report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2003-03-05">March 5, 2003</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,182.21,134.20,87.23,10.99"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
							<email>asmeaton@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing Dublin City University Glasnevin</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<country>Ireland Paul Over</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Retrieval Group Information Access Division</orgName>
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<postCode>20899-8940</postCode>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.33,101.81,257.07,15.39">The TREC-2002 Video Track Report</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2003-03-05">March 5, 2003</date>
						</imprint>
					</monogr>
					<idno type="MD5">5FDEA1E66588B037AB432403EC82DAFD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC-2002 saw the second running of the Video Track, the goal of which was to promote progress in content-based retrieval from digital video via open, metrics-based evaluation. The track used 73.3 hours of publicly available digital video (in MPEG-1/VCD format) downloaded by the participants directly from the Internet Archive (Prelinger Archives) <ref type="bibr" coords="1,72.00,444.71,102.17,9.96">(internetarchive, 2002)</ref> and some from the Open Video Project <ref type="bibr" coords="1,140.10,456.68,86.76,9.96" target="#b7">(Marchionini, 2001)</ref>. The material comprised advertising, educational, industrial, and amateur films produced between the 1930's and the 1970's by corporations, nonprofit organizations, trade associations, community and interest groups, educational institutions, and individuals. 17 teams representing 5 companies and 12 universities -4 from Asia, 9 from Europe, and 4 from the US -participated in one or more of three tasks in the 2001 video track: shot boundary determination, feature extraction, and search (manual or interactive). Results were scored by NIST using manually created truth data for shot boundary determination and manual assessment of feature extraction and search results.</p><p>This paper is an introduction to, and an overview of, the track framework -the tasks, data, and measures -the approaches taken by the participating groups, the results, and issues regrading the evaluation. For detailed information about the approaches and results, the reader should see the various site reports in the final workshop proceedings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">New in TREC 2002</head><p>At the TREC 2001 video track workshop in November 2001, the track set a number of goals for improvement <ref type="bibr" coords="1,337.58,363.73,149.77,9.96" target="#b10">(Smeaton, Over, &amp; Taban, 2002)</ref> and in the subsequent months through cooperative effort met almost all of them. As a result the 2002 track differs from the first running in 2001 in a number of important ways itemized here:</p><p>• There was an increase in the number of participants, up to 17 from last year's 12, and an increase in the data where a total of overview 73 hours of VCD/MPEG-1 data were identified for use in development and testing -up from 11 hours last year.</p><p>• A semantic feature extraction task was added. 10 features (e.g., cityscape, face, instrumental sound, monologue speech) were defined by a group of interested track participants and systems attempted with some success to find shots containing a given feature.</p><p>• Several groups volunteered to extract sets of these features from the test video and share their results with other groups allowing those other groups to use that feature detection in the search task. These feature detections were distributed in an MPEG-7 format developed by IBM.</p><p>• This year the track used a common set of shot definitions, donated by the CLIPS-IMAG group and formatted by Dublin City University whereas previously each group had defined their own shot boundaries. Results for the feature detection and search tasks were reported in terms of these predefined units -allowing for pooling of results.</p><p>• The 25 topics for the search task were developed by NIST rather than by the participants and were released 4 weeks before the search results were due. These were again true multimedia queries as they all had video clips, images, or audio clips as part of the query, in addition to a text description. They reflect many of the various sorts of queries real users pose: requests for video with specific people or types of people, specific objects or instances of object types, specific activities or locations or instances of activity or location types <ref type="bibr" coords="2,168.80,261.25,106.35,9.96" target="#b4">(Enser &amp; Sandom, 2002)</ref>. Unlike last year, where the topics were either known item or general, this year's topics were all general.</p><p>• The very difficult task of fully automatic topicto-query translation was set aside for a future TREC video track. Searching in this year's track could be interactive with full human access to multiple interim search results, or "manual". In manual searches a human with knowledge of the query interface but no direct or indirect knowledge of the search test set or search results was given one chance to translate each topic to what he or she believed to be the most effective query for the system being tested.</p><p>• The shot boundary detection test set was not announced until 3 weeks before the submissions were due at NIST for evaluation. New and revised measures were used to separate a system's ability to detect shot transitions by identifying at least one of the frames in the transition from the accuracy with which a system locates the entire transition (frame-recall and frame-precision).</p><p>• Elapsed search time was added as measure of effort for the interactive search task and groups were encouraged to gather and report data on searcher characteristics and satisfaction.</p><p>Details about each of the three tasks follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Shot boundary detection</head><p>Movies on film stock are composed of a series of still pictures (frames) which, when projected together rapidly, the human brain smears together so we get the illusion of motion or change. Digital video is also organized into frames -usually 25 or 30 per second.</p><p>Above the frame, the next largest unit of video both syntactically and semantically is called the shot. A half hour of video, in a TV program for example, can contain several hundred shots. A shot was originally the film produced during a single run of a camera from the time it was turned on until it was turned off or a subsequence thereof as selected by a film editor. The new possibilities offered by digital video have blurred this definition somewhat, but shots, as perceived by a human, remain a basic unit of video, useful in a variety of ways. Work on algorithms for automatically recognizing and characterizing shot boundaries has been going on for some time with good results for many sorts of data and especially for abrupt transitions between shots. Software has been developed and evaluations of various methods against the same test collection have been published e.g., using 33 minutes total from five feature films <ref type="bibr" coords="2,416.09,302.70,100.98,9.96" target="#b0">(Aigrain &amp; Joly, 1994)</ref>; 3.8 hours total from television entertainment programming, news, feature movies, commercials, and miscellaneous <ref type="bibr" coords="2,347.07,338.56,111.60,9.96" target="#b2">(Boreczky &amp; Rowe, 1996)</ref>; 21 minutes total from a variety of action, animation, comedy, commercial, drama, news, and sports video drawn from the Internet <ref type="bibr" coords="2,349.63,374.43,52.69,9.96" target="#b5">(Ford, 1999)</ref>; an 8-hour collection of mixed TV broadcasts from an Irish TV station recorded in June, 1998 <ref type="bibr" coords="2,360.89,398.34,90.27,9.96" target="#b3">(Browne et al., 2000)</ref>.</p><p>An open evaluation of shot boundary determination systems was designed by the OT10.3 Thematic Operation (Evaluation and Comparison of Video Shot Segmentation Methods) of the GT10 Working Group (Multimedia Indexing) of the ISIS Coordinated Research Project in 1999 using 2.9 hours total from eight television news, advertising, and series videos <ref type="bibr" coords="2,341.47,495.03,198.28,9.96;2,310.98,506.99,22.09,9.96" target="#b8">(Ruiloba, Joly, Marchand-Maillet, &amp; Quénot, 1999)</ref>.</p><p>The shot boundary task is included in the video track as an introductory problem, the output of which is needed for higher-level tasks such as search. Groups can participate for the first time on this task, develop their infrastructure, and move on to more complicated tasks the next year. Information on the effectiveness of particular systems is useful in selecting donated segmentations used for scoring other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The shot boundary test collection for this year's TREC task comprises 4 hours and 51 minutes of video, slightly smaller than last year. The videos are mostly of a documentary/educational nature but were varied in their age, production style, and quality. There were 18 videos encoded in MPEG-1 with a total size of 2.88 gigabytes. The videos contained 545,068 total frames and 2,090 shot transitions (according to the manually created reference data.)</p><p>The reference data was created by a student at NIST whose task was to identify all transitions and assign each to one of the following categories: Software was developed and used to sanity check the manual results for consistency and some corrections were made.</p><p>The freely available software tool 1 was used to view the videos and frame numbers. The collection used for evaluation of shot boundary determination contains 2,090 transitions with the following breakdown as to type:</p><p>• 1466 -hard cuts (70.1%)</p><p>• 511 -dissolves (24.4%)</p><p>• 63 -fades to black and back (3.0%)</p><p>• 50 -other (2.4%) Gradual transitions are generally harder to recognize than abrupt ones. The proportion of gradual transitions to hard cuts in this collection is about twice that reported by <ref type="bibr" coords="3,148.81,542.24,117.96,9.96" target="#b2">Boreczky and Rowe (1996)</ref> and by <ref type="bibr" coords="3,72.00,554.19,49.79,9.96" target="#b5">Ford (1999)</ref>. This is due to the nature and genre of the video collection we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>Participating groups in this task were allowed up to 10 submissions and these were compared automatically to the shot boundary reference data. Each group determined the different parameter settings for 1 The VirtualDub <ref type="bibr" coords="3,150.06,668.96,41.57,7.97" target="#b6">(Lee, 2001)</ref> website contains information about VirtualDub tool and the MPEG decoder it uses. The identification of any commercial product or trade name does not imply endorsement or recommendation by the National Institute of Standards and Technology. each run they submitted. Detection performance for cuts and for gradual transitions was measured by precision and recall where the detection criteria required only a single frame overlap between the submitted transitions and the reference transition. This was to make the detection independent of the accuracy of the detected boundaries. For the purposes of detection, we considered a submitted abrupt transition to include the last pre-transition and first post-transition frames so that it has an effective length of two frames (rather than zero).</p><p>Analysis of performance individually for the many sorts of gradual transitions was left to the participants since the motivation for this varies greatly by application and system.</p><p>As last year, gradual transitions could only match gradual transitions and cuts match only cuts, except in the case of very short gradual transitions (5 frames or less), which, whether in the reference set or in a submission, were treated as cuts. We also expanded each abrupt reference transition by 5 frames in each direction before matching against submitted transitions to accommodate differences in frame numbering by different decoders.</p><p>Accuracy for reference gradual transitions successfully detected was measured using the one-to-one matching list output by the detection evaluation. The accuracy measures were frame-based precision and recall. Note that a system could be very good in detection and have poor accuracy, or it might miss a lot of transitions but still be very accurate on the ones it finds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>As illustrated in Figure <ref type="figure" coords="4,177.18,561.87,4.97,9.96" target="#fig_1">1</ref> and Figure <ref type="figure" coords="4,236.10,561.87,3.87,9.96" target="#fig_2">2</ref>, performance on gradual transitions lags, as expected, behind that on abrupt transitions, where for some uses the problem may be considered a solved one. The numbers in parentheses give the number of runs submitted by each group. Some groups (e.g., CLIPS and RMIT) used their runs to explore a number of precision-recall settings and seem to have good control of this tradeoff. Figure <ref type="figure" coords="4,122.90,657.51,4.97,9.96">3</ref> indicates that at the level of frames in gradual transitions, the best systems have better precision than they do in detecting those transitions but their frame-level recall scores tend to be lower than for simple detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature extraction</head><p>A potentially important asset to help video search/navigation is the ability to automatically identify the occurrence of various semantic features such as "Indoor/Outdoor","People", "Speech" etc., which occur frequently in video information. The ability to detect features is an interesting challenge by itself but it would take on added importance if it could serve as an extensible basis for query formation and search. The high-level feature extraction task had the following objectives:</p><p>• to begin work on a benchmark for evaluating the effectiveness of detection methods for various semantic concepts</p><p>• to allow exchange of feature detection output based on the TREC Video Track search test set prior to the search task results submission date, so that a greater number of participants could explore innovative ways of leveraging those detectors in answering the search task queries.</p><p>The task was as follows. Given a standard set of shot boundaries for the feature extraction test collection and a list of feature definitions, participants were to return for each feature the list, at most the top 1000 video shots from the standard set, ranked according to the highest possibility of detecting the presence of the feature. The presence of each feature was assumed to be binary, i.e., it was either present or absent in the given standard video shot. If the feature was true for some frame (sequence) within the shot, then it was true for the shot. This is a simplification adopted for the benefits it afforded in pooling of results and approximating the basis for calculating recall.</p><p>The feature set was suggested in on-line discussions by track participants. The number of features to be detected was kept small so as to be manageable in this first implementation and the features were ones for which more than a few groups could create detectors. Another consideration was whether the features could, in theory at least,be used in executing searches on the video data using the topics. The topics did not exist yet at the time the features were defined. The feature definitions were to be in terms a human judge could understand.</p><p>Much to the appreciation of the track as a whole, some participating groups made their feature detection output available to participants in the search task and this will be discussed in the section describing the search task.</p><p>The features to be detected were defined as follows for the system developers and for the NIST assessors:</p><p>Outdoors segment contains a recognizably outdoor location, i.e., one outside of buildings. Should exclude all scenes that are indoors or are closeups of objects (even if the objects are outdoors)</p><p>Indoors segment contains a recognizably indoor location, i.e., inside a building. Should exclude all scenes that are outdoors or are close-ups of objects (even if the objects are indoors).</p><p>Face segment contains at least one human face with the nose, mouth, and both eyes visible. Pictures of a face meeting the above conditions count.</p><p>People segment contains a group of two or more humans, each of which is at least partially visible and is recognizable as a human.</p><p>Cityscape segment contains a recognizably city/urban/suburban setting.</p><p>Landscape segment contains a predominantly natural inland setting, i.e., one with little or no evidence of development by humans. For example, scenes consisting mostly of plowed/planted fields, pastures, orchards would be excluded. Some buildings, if small features on the overall landscape, should be OK. Scenes with bodies of water that are clearly inland may be included.</p><p>Text Overlay segment contains superimposed text large enough to be read.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech a human voice uttering words is recognizable as such in this segment</head><p>Instrumental Sound sound produced by one or more musical instruments is recognizable as such in this segment. Included are percussion instruments.</p><p>Monologue segment contains an event in which a single person is at least partially visible and speaks for a long time without interruption by another speaker. Pauses are OK if short. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>This year all result sets from all runs were fully assessed manually to create reference data. Basically, the feature extraction definitions were treated like topics of the form: "I want shots for which this feature is true."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Measures</head><p>The trec eval software, a tool available via trec.nist.gov, was used to calculate recall, precision, average precision, etc., for each result. In experimental terms the features represent fixed rather than random factors, i.e., we were interested at this point in each feature rather than in the set of features as a random sample of some population of features. For this reason and because different groups worked on very different numbers of features, we did not aggregate measures at the run-level in the results pages at the back of the notebook. Comparison of systems should thus be "within feature". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Issues</head><p>It should be noted that in the case of some features (speech, instrumental sound) the number of shots in the feature extraction test set containing the feature approached or exceeded the maximum size of the submitted result set (1,000) and represented a large portion of the entire feature test collection size (1,848 shots) -see Table <ref type="table" coords="6,162.16,453.38,3.87,9.96" target="#tab_0">1</ref>. While the performance of a random baseline was high in these cases, the median performance was still well above it. Where more hits exist than a result can hold, an artificial upper bound on possible average precision scores exists -namely for feature 8 (speech) 0.724 and for feature 9 (instrumental sound) 0.819.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>Figure <ref type="figure" coords="6,103.66,573.52,4.97,9.96" target="#fig_4">5</ref> summarizes the results by feature for all of the runs at the median or above. Included as a dotted line in this figure is the baseline -the average for 100,000 randomly created result sets for each feature. The artificial upper limit on average precision mentioned above is indicated by a white triangle for features 8 and 9.</p><p>Results vary in their dispersion among features as well as in their mean. While the random baseline is high, almost all of the runs are well above it. While there was a lot of overlap in the shots submitted for a given feature, Figure <ref type="figure" coords="6,177.40,705.33,4.97,9.96" target="#fig_3">4</ref> shows the relatively small number of true shots contributed uniquely by a given system -summed over all features. Not all systems submitted results for all features. The large overlap is no doubt due in part to the relatively small size of the test set (1,848 shots) in comparison to the size of the result (1,000 shots).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Search</head><p>The search task in the Video Track was an extension of its text-only analogue. Video search systems, all of which included a human in the loop, were presented with topics -formatted descriptions of an information need -and were asked to return a list of up to 100 shots from the videos in the search test collection which met the need. The list was to be prioritized based on likelihood of relevance.</p><p>4.1 Data to be searched 40.12 hours (176 videos containing 14,524 master shots) were randomly chosen from the identified collection to be used as the search test collection.</p><p>The video data was chosen because it represented an established archive of publicly available material that one can easily imagine being searched for information as well as historically interesting material that could be included in new video products. Publicly available video collections of any significant size are extremely hard to find. While we are not aware of any systematic study of the characteristics of the Internet Archive movie material, some details can be found in individual site papers. Collection characteristics will affect the scope of any conclusions drawn here.</p><p>Groups were allowed to develop their systems with knowledge of the search test collection -the topics being the surprise element. This was designated training pattern A. Other groups preferred to develop their systems without knowledge of the search test set. This training pattern was designated B. Results were labeled with these designations as were the feature extractions donated by some of the groups.</p><p>As was mentioned earlier, two search modes were allowed, fully interactive and manual, though no fully automatic mode was included, a choice which has advantages as well as disadvantages. A big problem in TREC video searching is that topics were complex and designating the intended meaning and interrelationships between the various pieces -text, images, video clips, and audio clips -is a complex one and the examples of video, audio, etc. do not always represent the information need exclusively and exhaus- Understanding what an image is of/about is famously complicated <ref type="bibr" coords="7,169.37,426.14,69.40,9.96" target="#b9">(Shatford, 1986)</ref>.</p><p>The definition of the manual mode allowed a human, expert in the search system interface, to interpret the topic and create an optimal query in an attempt to make the problem less intractable. The cost of the manual mode is terms of allowing comparative evaluation is the conflation of searcher and system effects. However if a single searcher is used for all manual searches within a given research group, comparison of searches within that group is still possible. At this stage in the research, the ability of a team to compare variants of their system is arguably more important than the ability to compare across teams, where results are more likely to be confounded by other factors hard to control (e.g. different training resources, different low-level research emphases , etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topics</head><p>The topics were designed as multimedia descriptions of an information need, such as someone searching a large archive of video might have in the course of collecting material to include in a larger video or to answer questions. Today this may be done largely by searching descriptive text created by a human when the video material was added to the archive. The track's search scenario envisioned allowing the searcher to use a combination of other media in describing his or her need. How one might do this naturally and effectively is an open question. This year 25 topics were created by NIST, who had intended to create 50, but due to time pressures, this was not possible. Each topic contained a text description of the user information need. Examples in other media, e.g., one more video clips, still images, audio files illustrating the information need, were optional. Table <ref type="table" coords="7,310.98,569.60,4.97,9.96" target="#tab_1">2</ref> presents an overview of the topics, their types, and the number of relevant shots found for each topic.</p><p>Comparing the TREC video topic types to distributions of actual queries against video archives is nearly impossible due to lack of published studies, differences in archive content and searcher characteristics, amount of mediation, etc. However, <ref type="bibr" coords="7,499.11,645.55,40.65,9.96;7,310.98,657.51,76.28,9.96" target="#b1">Armitage and Enser (1996)</ref> provide some real world reference points which may be of interest. Comparing the distribution of TREC video track topics types to a sample of 370 submitted to the BBC Natural History Unit and 388 submitted to the British Film Institute's Na- tional Film and Television Archive one sees the same predominance of non-abstract types and roughly the same percentage of type overlap (i.e., multi-category queries). However, the TREC queries have about half as many requests for specific persons and things and two to five times as many requests for generic persons and things. Whether this may be due to any degree to librarian/archivist mediation (e.g, substitution of a request for a known example for a generic request) is unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>The top 50 items (half) of each submitted result set was judged by a NIST assessor. Double judging in TREC 2001 indicated a high degree of assessor agreement for both relevant and non-relevant shots, so NIST did not do double judgments for TREC 2002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Measures</head><p>The trec eval program was used to calculate recall, precision, average precision, etc. The interested reader should see the back of the proceeding results pages for details on the performance of individual runs.</p><p>It should be noted that in the case of topics 82, 86, 93, and 94, as with evaluation in the feature extraction task, the number of relevant shots exceeded the maximum size of the submitted result set (100)see Table <ref type="table" coords="8,354.72,474.58,3.87,9.96" target="#tab_1">2</ref>. Where more relevant shots exist than a result can hold, an artificial upper bound on possible average precision scores exists -namely for topic 82 -0.588, 86 -0.952, 93 -0.621, and 94 -0.330.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Issues</head><p>Because the topics have a huge effect on the results, the topic creation process deserves special attention here. Ideally the topics would have been created by real users against the same collection used to test the systems, but such queries were not available.</p><p>Alternatively, interested parties familiar in a general way with the content covered by a test collection could have formulated questions which were then checked against the test collection to see that they were indeed relevant. This avenue was also not open to us for two main reasons. First, the collection used is so diverse that creating a question that has answers in several videos is next to impossible without detailed knowledge of the collection. Second, NIST had no video search system in place which could be used.</p><p>What was left was to work backward from the test collection with a number of goals in mind. Rather than attempt to create a representative sample, NIST tried to get an equal number of each of the basic types: generic/specific; person/thing/event, though in no way do we wish to suggest these types are equal as measured by difficulty to systems. Another important consideration was the estimated number of relevant shots and their distribution across the videos. The goals here were as follows:</p><p>• For almost all topics, there should be multiple shots that meet the need.</p><p>• If possible, relevant shots for a topic should come from more than one video.</p><p>• As the search task is already very difficult, we don't want to make the topics too difficult.</p><p>The videos in the test collection were viewed and notes made about their content in terms of people, things, and events, named or unnamed. Those that occurred in more than one video became candidates for topics. This process provided a rough idea of a minimum number of relevant shots for each candidate topic. The third goal was the most difficult since there is no reliable way to predict the hardness of a topic.</p><p>In general NIST tried to be sure there were relevant shots with relatively large images of the target person, thing, or event. When choosing examples for the topics, NIST tried to find at least some that seemed to resemble the target shot in shape, color, and/or texture. This was often not possible, nor is it likely the estimate of similarity corresponded in any meaningful way with that of the automatic systems.</p><p>Sometimes words from the audio were incorporated into the wording of the topic. This leaves open the possibility that some topics were in fact generally biased toward approaches using automatic speech recognition. On the other hand some information needs make demands unlikely to be supported by text from the audio e.g., requests for specific relative object/camera motion (98: locomotive approaching the viewer), some events/activities (96: US flags flapping), etc. A full analysis on the presence or absence of topic keywords in the audio track for relevant shots would be required to determine whether this is the case and has yet to be done.</p><p>The nature of the test collection for 2003 and the possible use of a search tool to validate minimal numbers of relevant shots (even if a related system is likely </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results</head><p>The results in terms of mean average precision for the top ten manual runs are presented in Figure <ref type="figure" coords="9,534.84,585.69,4.97,9.96" target="#fig_5">6</ref> and those for the top ten interactive runs in Figure <ref type="figure" coords="9,532.05,597.65,3.87,9.96">7</ref>, each list sorted by mean average precision. Another measure for interactive runs which was gathered was total elapsed time for each topic search. Figure <ref type="figure" coords="9,514.10,633.52,4.97,9.96" target="#fig_6">8</ref> contrasts the two measures. Time spend varied widely from an average of just over 1 minute to just under 30 minutes per topic. No simple relationship between elapsed search time and effectiveness as measured by mean average precision is apparent.</p><p>The number of relevant shots contributed uniquely The search task results in this report are based on manual relevance judgments for the top (most relevant) half (50 shots) in each submitted result set. The bottom half of each result has also been judged manually and this yielded few additional relevant shots except in the case of a couple topics which already had more than the average number of relevant shots. Fourteen of the twenty-five topics had no change in the number of relevant shots. For 8 the number of relevant shots grew 11% or less, for 3 it grew 20 -24% (topics 82 -20%; 94 -21%; 96 -24%). Figure <ref type="figure" coords="10,72.00,510.66,9.94,9.96" target="#fig_1">10</ref> illustrates the distribution of relevant shots in the top versus the bottom half of the result sets.</p><p>Looking underneath the averages at the performance by topic, one can see that considerable variability exists across the set of topics and that some topics were harder than others. Figure <ref type="figure" coords="10,241.12,572.13,9.94,9.96" target="#fig_1">11</ref> and Figure <ref type="figure" coords="10,72.00,584.08,9.94,9.96" target="#fig_8">12</ref> show these together with two covariates: number of relevant shots and relevant videos. Manual results for topics 76, 84, 90, and 97 stand out. Why are they better? No single, simple explanation suffices. Topics with more relevant shots/videos or topics containing video examples from the search test collection (see small vertical arrows in Figure <ref type="figure" coords="10,209.73,655.81,9.21,9.96" target="#fig_8">12</ref>) are not necessarily easier.</p><p>The jury is still out with respect to two important search issues. The reliable usefulness of features in search generally or in specific situations has yet to be Matching the text of the topic against the text derived by automatic speech recognition on the video's audio track usually delivered better overall results than searches based on just the visual elements in the topic or combinations of the text and other elements. It is too early to draw convincing conclusions about either issue, but see the participants' papers for some interesting observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Approaches in brief</head><p>The following is a list of the groups that took part in one or more of the video track tasks and very short self-descriptions of the approaches taken by each participating research group. For detailed information the reader should consult the relevant system-specific paper in the proceedings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Carnegie Mellon University (US)</head><p>The Informedia Project participated in the feature extraction task and both the manual and interactive search tasks. For the feature classification tasks, their standard approach was to hand label the feature training data using a labeling efficient interface, which allowed undergraduates to label one hour of video in 10 minutes for the presence/absence of one For the interactive track CMU used a modified version of the Informedia Digital Video Library System client, which was expanded to incorporate the classifier features and made more efficient to enable rapid display and exploration of large video data sets. It also incorporated an interface to multiple image search engines based on RGB or Munsell color, Texture, with different 3x3, 5x5, 7x7 blocks or QBICstyle image matching. An expert Informedia user, who did not have knowledge of the current TREC video collection, obtained the answers attempting to achieve high recall rather than speedy results. For the manual track, CMU submitted three systems: the first system was quite similar to last year's video track submission, combining speech recognition transcripts and OCR and image information in a linear fashion, while the second and best system extended the first system by incorporating the movie title and description information as text. This second system also added pseudo-relevance feedback for image retrieval as an additional combination module. Finally CMU submitted a third run using only the speech transcripts for text-only queries, without any relevance feedback or query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CLIPS-IMAG Grenoble (France)</head><p>This group used almost the same system for shot boundary detection as the one used for the TREC-2001 evaluation. This system detects "cut" transitions by direct image comparison after motion compensation and "dissolve" transitions by comparing the norms of the first and second temporal derivatives of the images. It also has a special module for detecting photographic flashes and filtering them as erroneous "cuts". Some parameters controlling the existing modules have been tuned using the TREC-2001 SBD corpus and reference segmentation, and a global parameter for the tuning of the recall versus precision compromise has been inserted.</p><p>The CLIPS group extracted only features 3 (faces), 4 (people), 8 (speech) and 10 (monologue). Face and people detection were based on a face detection tool publicly available from CMU run on one keyframe automatically extracted for each shot. The results were ranked according to the presence of a face and its size for feature 3 and according to the presence of at least two faces and the total size for feature 4. For features 8 and 10, they used the output of two different speech recognition systems, one from CLIPS-IMAG (GEOD team) and the other from LIMSI-CNRS, the same output as used by the group from Dublin. For feature 8, the length of detected speech segment within shots was used for ranking the results. For feature 10, the results were ranked using a combination of the length of a speech segment and the presence of a face.</p><p>Finally, CLIPS submitted three manual runs for the search task. One based only on speech transcription, on based only on a combination of donated features, and one based on a combination of both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dublin City University (Ireland)</head><p>DCU submitted results for three of the features from the feature set, namely speech, instrumental sound (music) and faces. Each technique worked directly on the encoded MPEG-1 bitstream. Speech extraction was based on measuring the duration of the rate of energy peaks of the audio signal. The same technique was extended to include rhythm and harmonicity for music detection while skin masks were used to detect the presence of faces. For the Search Task this group developed an interactive video retrieval system which used all 10 features identified earlier, three of which were the result of their own extractions, and the rest were donations from other groups. Twelve test users each ran the full 25 topics by formulating queries, browsing results and submitting results. The group ran two variations of their system, one which used the features plus the ASR transcript provided by LIMSI, and the other which used just the ASR transcript. All topic searches were limited to 4 minutes in total elapsed time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Eurecom (France)</head><p>This group submitted runs under the feature extraction task. Their approach avoided complete decoding of the MPEG stream, basing decisions instead on the classification of the DCR macro-blocks -at some cost to the precision of the analysis. The work can be seen as an exploration of a "low-cost" baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Fudan University (China)</head><p>Fudan University participated in the shot segmentation, feature extraction, and search tasks.</p><p>In the shot segmentation task, Fudan used most parts of their TREC-2001 shot segmentation system. The parameters used in the system were trained and adjusted based on the TREC-2001 video collection.</p><p>According to the performance on TREC-2001 video collection, they selected the system parameters to generate the submissions. They added fade in/out detection to the system this year although the shot segmentation task did not include it. Evaluation showed that the system had a good balance between precision and recall. Comparing F-Value, the rank of the best result for all the changes, cut changes and gradual changes was 3, 3 and 9 (out of 54 systems). On gradual accuracy, frame-recall of the system was better than frame-precision. Compared with other submitted systems, their system was located at the middle in gradual accuracy.</p><p>In the feature extraction task, they developed a new video feature extraction system. It consisted of five sub-systems: outdoor / indoor detection, cityscape / landscape detection, face / people detection, text detection and speech / music / monologue detection. In each sub-system, a value calculated by whatever methods and features were used for ranking. Evaluation showed that the system worked well on these features: Cityscape, Landscape, Indoor and Music.</p><p>In the search task, Fudan submitted four runs. Considering the difficulty of search topics, they did not process all of the topics in each run. The whole architecture of the search system was almost the same as last year. However, there were some improvements in face recognition and object search. Fudan tried a fast manifold-based approach to face recognition in the TREC-2002 Search Task. This can be used when there are only few different images of a specific person and this process runs fast.</p><p>For each search topic, Fudan combined the similarities coming from different modules such as face recognition, text recognition, color histogram comparison, ASR text etc. In their submission, Sys1 only used the information returned by their own search modules. There was no ASR Text and Feature Extraction results used. However, feature extraction confidence was useful for some topics. So in the runs labeled Sys2 and Sys3, they combined feature extraction confidence into the searching. Sys2 used their own feature extraction results and Sys3 used the reference feature extraction results provided by IBM and Me-diaMill. In Sys4, they combined the ASR Results provided by LIMSI. NIST's evaluation showed that their searching system was not effective in several topics. In their future work, Fudan plans to pay more attention to image similarity calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">IBM Research, Almaden and T.J. Watson (US)</head><p>IBM participated in the shot boundary detection, feature extraction and search tasks. This large group explored several diverse methods for video analysis, indexing, and retrieval, which included automatic descriptor extraction, statistical modeling, and multimodal fusion. In the shot boundary detection task, they explored several methods for making SBD more robust to poor video quality. Some of the methods explored include using localized edge gradient histograms and comparing pairs of frames at greater temporal distances. In the feature detection task the IBM group explored several methods for automatic descriptor extraction and statistical modeling and made significant efforts to manually annotate the Feature Training and Validation collections. First, using the Feature Training collection, they built statistical models of the concepts, exploring a variety of descriptors including color histograms, wavelet texture, edge histograms, color correlograms, motion vectors, audio spectrum features, and so on. They also investigated different discriminant modeling methods (e.g., support vector machines). Once the individual statistical models were constructed, they explored different fusion methods for maximizing retrieval effectiveness on the Feature Validation collection. The resulting fused classifiers were then applied to the Feature Test collection. Overall, feature detection results were submitted for all ten feature classes.</p><p>For the search task the IBM group investigated both manual and interactive methods of searching, submitting four runs as follows: (1) Manual searching using content-based retrieval (CBR) without knowledge of the Search Test collection; (2) Manual searching using spoken document retrieval (SDR) based on automatic speech recognition results; (3) A combination of CBR and SDR in manual searching; (4) Interactive use of CBR and SDR;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Imperial College London (United Kingdom)</head><p>Imperial College London used a shot-boundary detection scheme based on a multi-timescale detection algorithm in which colour histogram differences were examined over a range of frames. At each frame they calculated a distance measure for each of a range of timescales, and made decisions on whether a cut or gradual change had occurred according to where coincident peaks occurred in these distance measures.</p><p>For the search task, they took a representative key frame for each shot and derived a number of low-level features including illumination-invariant colour representations, text from ASR and convolution filters. Query images were tested for similarity to a shot in the test set using the k-nearest neighbours approach. A novel relevance feedback system was then employed to allow the user to modify the query and update the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Indiana University (US)</head><p>At Indiana University researchers have developed a system named ViewFinder for the purpose of providing access to video content for a project named the Cultural digital Library Indexing Our Heritage (CLIOH). They took this existing system, made notable modifications, and applied it to the interactive search task, submitting one interactive search run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Lowlands Group (the Netherlands)</head><p>This group participated in the search task by evaluating a probabilistic model for the retrieval of mul-timodal documents. The model was based on Bayes decision theory and combined models for text based search with models for visual search. The textual model, applied to the LIMSI transcripts, was based on the language modeling approach to text retrieval. The visual model, a mixture of Gaussian densities, described keyframes selected from shots. Both models had been proven successful on media specific retrieval tasks. Their contribution was the combination of both techniques in a unified model, ranking shots on ASR-data and visual features simultaneously. To further improve the query, they experimented with query expansion by adding additional example images found using Google image search. While the expansion process needed human involvement, they hoped the results would identify potential benefits of automatic expansion techniques for video search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">The MediaMill Group (the Netherlands)</head><p>The MediaMill Group performed feature extraction by evaluating a system aimed at training models for semantic concepts on a specific collection by active learning. The system was geared to feature classification for specific collections, to exploit characteristics of domain and collection, and to allow for user definition of problem-specific semantic concepts. Using the i-Notation system, annotators provided learning examples to the system in an efficient way. For active learning (i.e. classifier feedback during an annotation session) as well as final classification, a Maximum Entropy classifier was used. Binning was applied to provide the mapping of numerical values to binary values necessary for Maximum Entropy. A fixed pool of sixty visual descriptors was used as input for the Maximum Entropy classifier for all eight visual TREC features, so that extension of the approach to any other visual feature is trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11">Microsoft Research Asia (China)</head><p>This team participated in the shot boundary, features and search tasks. For shot boundary detection, the submission was based on the last year's work but concentrated on improving gradual transition (GT) detection. The main feature for SBD was frame difference, the total difference of the binwise histogram comparison between two consequent frames in the R, G and B channels. Shot boundaries were then determined according to a set of heuristic rules. For feature extraction, multiple key frames were extracted for each shot and feature extraction was performed on these images. For the indoor, outdoor, cityscape and landscape features, trained models were employed based on color moment and edge direction histograms, aggregated over all keyframes from a shot. Face detection from keyframes and text overlay also ran on the multiple keyframes from each shot. The audio feature extraction was based on a support vector machine classifier with inputs based on low-level audio analysis. This group used the Q-Video video retrieval system in the search task. Manual searching was performed using a combination of Color Moment (CM), Dominant Color (DC), HSV Histogram (HSVH), Color Layout (CL), Edge Histogram (EH), Color Texture Moment (CTM), Kirsh Direction Density (KDD), Wavelet feature (WF) and Motion Texture (MT) with different distance metrics employed for different feature sets. For interactive searching, users browsed retrieved shots and their feedback, both positive and negative, was fed into an SVM-based learning procedure for each topic, making it a kind of learning-based relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.12">National University of Singapore (Singapore)</head><p>This group took part in the shot boundary detection task and used an expanded version of their previous temporal multi-resolution analysis (TMRA) work by introducing a new feature vector based on motion, incorporating functions to detect flash and camera/object motion, and selecting automatic thresholds for noise elimination based on the type of video.</p><p>The framework can be used to extract meaningful keyframes and provides a unified approach to detection of gradual transitions and cuts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.13">Prous Science (Spain)</head><p>This company submitted runs under the search task but details have not been provided in writing at the time of writing this summary report. An overview paper describing the approach taken by Prous Science may become available along with other video track site reports, at a later time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.14">The University of Oulu (Finland)</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.15">RMIT University (Australia)</head><p>RMIT participated in the shot boundary detection task, where they used the techniques of query by example (QBE) and ranked results, both often used in content-based image retrieval (CBIR). Each frame in turn was considered as an example query on the image collection formed by the other frames within a moving window. Transitions were detected by monitoring the relative ranks of these frames in the results list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.16">University of Bremen (Germany)</head><p>This group submitted runs under the shot boundary detection and feature detection tasks. The shot detection approach was based on histogram differences. It was divided into two steps -feature extraction and shot boundary detection. Firstly, the histogram differences were calculated for the entire video in real time. Secondly, shot boundaries were detected. The advantage of this approach was the possibility to set adaptive thresholds for the shot boundary detection considering all extracted features of the complete video sequence. The adaptive threshold was set to a percentage of the maximum of all calculated difference values of the video. In the case of gradual changes, often multiple shot boundaries were detected. Therefore multiple detected shot boundaries that followed each other within a short temporal interval were grouped together and a gradual change was detected beginning with the first and ending with the last shot boundary in the interval.</p><p>For the feature extraction task the group examined whether it was possible to classify indoor and outdoor shots by their color distribution. In order to analyze the color distribution, first order statistical features were used, which were extracted from the histograms of the three color channels (RGB) and the grey level histogram. The features calculated from each histogram were average, variance, and amount of peaks, normalized to an interval [0...1]. In order to classify the shots into indoor and outdoor shots, a feed forward neural net with backpropagation learning was trained. At the input layer the 12 statistical features mentioned above were presented. The output layer consisted of two neurons that take on values between 0 and 1 measuring the probability for the features indoors or outdoors to be present in the shot. Two hidden layers each with 20 neurons were initialized with random weights. In order to train the neural net, some videos from the feature development collection were chosen. The shots were classified manually to generate 323 training data sets, 178 for indoors and 145 for outdoors.</p><p>In order to classify the shots from the feature extraction test collection, a set of n key frames was extracted from each shot. Every k-th frame of a shot was used as a key frame, but in order to be more independent of inaccuracies during the shot detection and of gradual changes (e.g., wipes, fades, or dissolves) a number of frames around the shot boundaries were skipped. In order to classify a shot, the set of n key frames was presented to the neural net. For each of the two output neurons a list was obtained containing n values, one for each key frame. The median for each list was calculated to obtain the final probabilities for the shot to be indoors or outdoors. In order to measure the accuracy of the classification result, the difference between the median values of the indoors and the outdoors neuron was calculated. If the difference exceeded a threshold the shot was classified to contain the feature with the higher probability. The difference was also used for the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.17">University of Maryland (US)</head><p>The University of Maryland led a team made up of researchers from INSA Lyon (France) and the Universities of Maryland (US) and Oulu (Finland), and participated in the text feature extraction task and the search task. For search they provided a weighted query mechanism by integrating 1) text (OCR and ASR) content using full text and n-grams through the MG system, 2) color correlogram indexing of shots and images reported last year in TREC, and 3) ranked versions of the extracted binary features. All of the features are normalized, and a variety of distance measures are used to index into the collection. The command line version of the interface al-lowed users to make various queries, store them and use weighted combinations to generate a compound query.</p><p>In their interactive search experiments, most users generated their initial manual queries with the command line interface, and then explored a ranked collection of clips with an interactive interface. The interactive interface treated each video clip as a visual object in a multi-dimensional space, and each "feature" of that clip was mapped to one dimension. The user could visualize in two dimensions by placing any two features on the horizontal and vertical axis. Additional dimensions could be visualized by adding attributes to each object. Color, for example, could be used to represent a third feature dimension, size a fourth and shape a fifth dimension. Dynamic range sliders were provided for all features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summing up and moving on</head><p>This overview of the TREC-2002 Video Track has provided basic information on the track structure, data, evaluation mechanisms and metrics used, and a snapshot of what most of the participants did in their experiments. Further details about a particular group's approach and performance can be found in that group's site report. The raw results for each submitted run can be found in the results section of the final proceedings or under "Publications" on the trec.nist.gov website.</p><p>In 2003 the track will become an independent evaluation with a one-or two-day workshop <ref type="bibr" coords="16,249.79,451.67,51.01,9.96;16,72.00,463.63,23.75,9.96">(TRECVID 2003)</ref> immediately preceding TREC. The guidelines will be developed during the first quarter of 2003. The following are likely:</p><p>• using 120 hours of 1998 news video  in 2003 and more of the same/similar in 2004</p><p>• continuing the three basic tasks: segmentation, feature extraction, search</p><p>• perhaps attempting detection of higher-level segments: stories, scenes</p><p>• keeping most of the features, but adding some appropriate to news</p><p>• striving for better system comparability in the search task</p><p>• creating more topics, perhaps 50, unbiased by detailed knowledge of the test collection</p><p>• significantly increasing the sizes of the search and especially the feature test collections.</p><p>The latest information about the TREC video retrieval evaluation efforts, past and present, is available from the track website at wwwnlpir.nist.gov/projects/trecvid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Authors' note</head><p>We are particularly grateful to Rick Prelinger and Niall O'Driscoll for their help with the Internet Archive data.</p><p>We appreciate Jonathan Lasko's painstaking creation of the shot boundary truth data. The track would not have been possible without the software development work and general collaboration of Ramazan Taban, who has returned home to France and the job market. Our thanks to John Garofolo and Jose Joeman for their helpful suggestions on an earlier draft.</p><p>Finally, we would like to thank all the track participants and other contributors on the mailing list, and especially those groups who provided shot boundary and feature extraction output for use by others. These combined efforts made this running of the track possible. The spirit of the track was again a very positive one.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,178.68,228.80,9.96;3,91.93,190.64,208.89,9.96;3,91.93,202.59,176.46,9.96;3,72.00,223.09,228.77,9.96;3,91.93,235.05,173.22,9.96;3,72.00,255.55,228.80,9.96;3,91.93,267.51,189.29,9.96;3,72.00,288.01,228.80,9.96;3,91.93,299.96,87.68,9.96"><head></head><label></label><figDesc>cut -no transition, i.e., last frame of one shot followed immediately by the first frame of the next shot, with no fade or other combination; dissolve -shot transition takes place as the first shot fades out while the second shot fades in fadeout/in -shot transition takes place as the first shot fades out and then the second fades in other -everything not in the previous categories e.g., diagonal wipes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,342.77,81.41,165.30,9.96;3,317.53,101.89,215.68,158.23"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Precision and recall for cuts</figDesc><graphic coords="3,317.53,101.89,215.68,158.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,72.00,81.41,228.80,9.96;4,78.56,101.78,215.78,158.34"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision and recall for gradual transitions</figDesc><graphic coords="4,78.56,101.78,215.78,158.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,72.00,81.41,228.82,9.96;6,72.00,93.36,69.15,9.96;6,78.55,114.12,215.77,215.53"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The number of true shots contributed uniquely by run</figDesc><graphic coords="6,78.55,114.12,215.77,215.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,203.30,81.41,205.22,9.96;7,72.09,112.17,503.48,269.56"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average precision by feature and run</figDesc><graphic coords="7,72.09,112.17,503.48,269.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,344.79,81.41,161.26,9.96;9,317.54,102.04,215.65,158.09"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Top 10 manual search runs</figDesc><graphic coords="9,317.54,102.04,215.65,158.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,104.66,81.41,163.54,9.96;10,78.56,102.08,215.76,215.64"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: MAP vs mean elapsed time</figDesc><graphic coords="10,78.56,102.08,215.76,215.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,310.98,126.14,228.81,9.96;10,317.53,146.92,215.67,215.52"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Relevant shots contributed uniquely by run</figDesc><graphic coords="10,317.53,146.92,215.67,215.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="11,310.98,81.41,228.80,9.96;11,317.54,102.09,215.76,215.60"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Manual search: average precision by topic</figDesc><graphic coords="11,317.54,102.09,215.76,215.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,81.41,456.06,633.88"><head>Table 1 :</head><label>1</label><figDesc>Features and total hits</figDesc><table coords="5,72.00,633.60,228.80,33.88"><row><cell>23.26 hours (96 videos containing 7,891 standard</cell></row><row><cell>shots) were randomly chosen from the total available</cell></row><row><cell>data, to be used solely for the development of feature</cell></row></table><note coords="5,72.00,669.46,228.78,9.96;5,72.00,681.42,228.81,9.96;5,72.00,693.38,228.80,9.96;5,72.00,705.33,14.97,9.96"><p>extractors. 5.02 hours (23 videos containing 1,848 standard shots) were randomly chosen from the remaining material for use as a feature extraction test set.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,76.15,81.41,289.95,70.26"><head>Table 2 :</head><label>2</label><figDesc>Overview of topics</figDesc><table coords="8,76.15,141.90,80.17,9.78"><row><cell>Topic #</cell><cell>Abbreviated text description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,77.70,114.12,465.61,266.93"><head>of needed information/shot Topic Types Panofsky-Shatford mode/facet categories (minus abstract) after Armitage &amp; Enser 1996 Number of examples in the topic</head><label></label><figDesc></figDesc><table coords="8,77.70,124.77,465.61,256.29"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Shots</cell></row><row><cell></cell><cell></cell><cell>S1 person; group;</cell><cell>Specifc event; S2 location S3 action</cell><cell>S4 linear time;</cell><cell>G1 kind of person;</cell><cell>G2 kind of event;</cell><cell cols="2">Generic kind of G3 place:</cell><cell>G4 cyclical time:</cell><cell>Video</cell><cell>Image</cell><cell>Shots sub-mitted</cell><cell>judged (pooling top 50 from</cell><cell>Shots judged relevant</cell></row><row><cell></cell><cell></cell><cell>tning</cell><cell></cell><cell>date;</cell><cell>thing</cell><cell>action;</cell><cell></cell><cell>geograph-</cell><cell>season;</cell><cell></cell><cell></cell><cell>each</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>period</cell><cell></cell><cell cols="2">condition</cell><cell>ical;</cell><cell>time of</cell><cell></cell><cell></cell><cell>result)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>architec-</cell><cell>day</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tural</cell><cell></cell><cell></cell><cell></cell></row><row><cell>75</cell><cell>Eddie Rickenbacker</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>2</cell><cell>2668</cell><cell>850</cell><cell>15</cell></row><row><cell>76</cell><cell>Raymond H. Chandler</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>0</cell><cell>3036</cell><cell>625</cell><cell>47</cell></row><row><cell>77</cell><cell>pictures of George Washington</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell><cell>2521</cell><cell>931</cell><cell>3</cell></row><row><cell>78</cell><cell>depictions of Abraham Lincoln</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell><cell>2637</cell><cell>1014</cell><cell>6</cell></row><row><cell>79</cell><cell>people spending leisure time at the beach</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell>x</cell><cell></cell><cell>4</cell><cell>0</cell><cell>3109</cell><cell>1055</cell><cell>55</cell></row><row><cell>80</cell><cell>one or more musicians</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>0</cell><cell>2829</cell><cell>860</cell><cell>63</cell></row><row><cell>81</cell><cell>football players</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>0</cell><cell>2311</cell><cell>890</cell><cell>15</cell></row><row><cell>82</cell><cell>women standing in long dresses</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>0</cell><cell>2696</cell><cell>1058</cell><cell>170</cell></row><row><cell>83</cell><cell>Golden Gate Bridge</cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>5</cell><cell>2529</cell><cell>936</cell><cell>33</cell></row><row><cell>84</cell><cell>Price Tower in Bartlesville, OK</cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2409</cell><cell>816</cell><cell>4</cell></row><row><cell>85</cell><cell>Washington Square Park's arch in NYC</cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>0</cell><cell>2708</cell><cell>909</cell><cell>7</cell></row><row><cell>86</cell><cell>overhead views of cities</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell>4</cell><cell>0</cell><cell>3041</cell><cell>1112</cell><cell>105</cell></row><row><cell>87</cell><cell>oil fields, rigs, derricks</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell>1</cell><cell>0</cell><cell>2721</cell><cell>1002</cell><cell>40</cell></row><row><cell>88</cell><cell>map of the continental US</cell><cell></cell><cell>x</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>0</cell><cell>2569</cell><cell>969</cell><cell>72</cell></row><row><cell>89</cell><cell>a living butterfly</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>2325</cell><cell>979</cell><cell>10</cell></row><row><cell>90</cell><cell>snow-capped mountain peaks or ridges</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell>3</cell><cell>0</cell><cell>2785</cell><cell>926</cell><cell>75</cell></row><row><cell>91</cell><cell>one or more parrots</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell><cell>2228</cell><cell>880</cell><cell>17</cell></row><row><cell>92</cell><cell>sailboats, clipper ships, etc. with sails unfurled</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>2</cell><cell>2860</cell><cell>921</cell><cell>47</cell></row><row><cell>93</cell><cell>live beef or dairy cattle</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>0</cell><cell>3622</cell><cell>1003</cell><cell>161</cell></row><row><cell>94</cell><cell>groups of people walking in an urban environment</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell>x</cell><cell></cell><cell>3</cell><cell>0</cell><cell>3168</cell><cell>1175</cell><cell>303</cell></row><row><cell>95</cell><cell>a nuclear explosion with a mushroom cloud</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>0</cell><cell>2658</cell><cell>951</cell><cell>17</cell></row><row><cell>96</cell><cell>one or more US flags flapping</cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>0</cell><cell>2458</cell><cell>1055</cell><cell>31</cell></row><row><cell>97</cell><cell>microscopic views of living cells</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>0</cell><cell>2968</cell><cell>859</cell><cell>82</cell></row><row><cell>98</cell><cell>a locomotive approaching the viewer</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>0</cell><cell>2729</cell><cell>998</cell><cell>56</cell></row><row><cell>99</cell><cell>a rocket or missile taking off</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>0</cell><cell>2438</cell><cell>907</cell><cell>11</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="16,310.98,415.23,228.82,9.96;16,335.89,427.18,203.91,9.96;16,335.89,439.14,203.90,9.96;16,335.89,451.10,112.34,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,473.20,415.23,66.60,9.96;16,335.89,427.18,203.91,9.96;16,335.89,439.14,131.10,9.96">The automatic real-time analysis of film editing and transition effects and its applications</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Aigrain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,474.30,439.14,65.49,9.96;16,335.89,451.10,36.54,9.96">Computers and Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="103" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,310.98,472.08,228.85,9.96;16,335.89,484.04,203.93,9.96;16,335.89,495.99,203.96,9.96;16,335.89,507.94,203.88,9.96;16,335.89,519.90,203.90,9.96;16,335.89,531.85,84.56,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,526.85,472.08,12.98,9.96;16,335.89,484.04,203.93,9.96;16,335.89,495.99,19.93,9.96">Information Need in the Visual Document Domain</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">H</forename><surname>Armitage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G B</forename><surname>Enser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,368.28,495.99,171.57,9.96;16,335.89,507.94,203.88,9.96;16,335.89,519.90,175.08,9.96">Report on Project RDD/G/235 to the British Library Research and Innovation Centre. School of Information Management</title>
		<imprint>
			<publisher>University of Brighton</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,310.98,552.84,228.81,9.96;16,335.89,564.79,203.90,9.96;16,335.89,576.75,203.94,9.96;16,335.89,588.71,203.92,9.96;16,335.89,600.66,203.93,9.96;16,335.89,612.61,72.18,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,487.75,552.84,52.03,9.96;16,335.89,564.79,187.19,9.96;16,487.43,576.75,52.39,9.96;16,335.89,588.71,203.92,9.96;16,335.89,600.66,9.52,9.96">Storage and Retrieval for Still Image and Video Databases IV</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Boreczky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,353.83,600.66,73.26,9.96">Proc. SPIE 2670</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Sethi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</editor>
		<meeting>SPIE 2670<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="170" to="179" />
		</imprint>
	</monogr>
	<note>Comparison of video shot boundary detection techniques</note>
</biblStruct>

<biblStruct coords="16,310.98,633.60,228.80,9.96;16,335.89,645.55,203.92,9.96;16,335.89,657.51,203.91,9.96;16,335.89,669.46,203.96,9.96;16,335.89,681.42,203.91,9.96;16,335.89,693.38,203.92,9.96;16,335.89,705.33,183.03,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,502.92,645.55,36.89,9.96;16,335.89,657.51,203.91,9.96;16,335.89,669.46,114.58,9.96">Evaluating and Combining Digital Video Shot Boundary Detection Algorithms</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berrut</surname></persName>
		</author>
		<ptr target="www.cdvp.dcu.ie/Papers/IMVIP2000.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,482.70,669.46,57.15,9.96;16,335.89,681.42,203.91,9.96;16,335.89,693.38,46.32,9.96">IMVIP 2000 -Irish Machine Vision and Image Processing Conference</title>
		<meeting><address><addrLine>Belfast, Northern Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,74.49,228.83,9.96;17,96.90,86.44,203.94,9.96;17,96.90,98.40,203.97,9.96;17,96.90,110.36,203.90,9.96;17,96.90,122.31,203.90,9.96;17,96.90,134.27,203.90,9.96;17,96.90,146.22,39.31,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,261.69,74.49,39.14,9.96;17,96.90,86.44,203.94,9.96;17,96.90,98.40,44.26,9.96">Retrieval of Archival Moving Imagery -CBIR Outside the Frame</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G B</forename><surname>Enser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Sandom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,163.33,110.36,137.47,9.96;17,96.90,122.31,98.86,9.96">Image and Video Retrieval, International Conference</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Eakins</surname></persName>
		</editor>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002-07-18">2002. 2002. July 18-19, 2002</date>
			<biblScope unit="page">2383</biblScope>
		</imprint>
	</monogr>
	<note>CIVR</note>
</biblStruct>

<biblStruct coords="17,72.00,166.15,228.82,9.96;17,96.90,178.10,203.95,9.96;17,96.90,190.06,203.94,9.96;17,96.90,202.01,203.92,9.96;17,96.90,213.96,203.93,9.96;17,96.90,225.93,203.92,9.96;17,72.00,245.85,16.79,9.96;17,110.03,245.85,34.97,9.96;17,166.23,245.85,32.94,9.96;17,220.43,245.85,26.22,9.96;17,267.89,245.85,32.94,9.96;17,96.90,257.80,22.89,9.96;17,135.75,257.80,21.86,9.96;17,202.17,257.80,30.38,9.96;17,277.14,257.80,23.78,9.96;17,96.90,269.76,144.88,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,185.28,166.15,115.54,9.96;17,96.90,178.10,181.65,9.96;17,132.42,202.01,168.40,9.96;17,96.90,213.96,87.66,9.96;17,72.00,245.85,16.79,9.96;17,110.03,245.85,34.97,9.96;17,166.23,245.85,32.94,9.96;17,220.43,245.85,26.22,9.96;17,267.89,245.85,32.94,9.96;17,96.90,257.80,22.89,9.96;17,135.75,257.80,17.49,9.96">A Quantitative Comparison of Shot Boundary Detection Metrics</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Ford</surname></persName>
		</author>
		<ptr target="http://www.archive.org/movies/" />
	</analytic>
	<monogr>
		<title level="j" coord="17,192.34,213.96,87.16,9.96">Proceedings of SPIE</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Yueng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B.-L</forename><surname>Yeo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3656</biblScope>
			<biblScope unit="page" from="666" to="676" />
			<date type="published" when="1999">1999. 2002</date>
			<pubPlace>San Jose, California, USA</pubPlace>
		</imprint>
	</monogr>
	<note>The Internet Archive Movie Archive home page</note>
</biblStruct>

<biblStruct coords="17,72.00,289.68,228.84,9.96;17,96.90,301.64,117.77,9.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="17,160.46,289.68,100.43,9.96">VirtualDub home page</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="www.virtualdub.org/index" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,321.57,228.81,9.96;17,96.90,333.52,170.51,9.96" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="17,191.36,321.57,109.45,9.96;17,96.90,333.52,43.95,9.96">The Open Video Project home page</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</author>
		<ptr target="www.open-video.org" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,353.45,228.85,9.96;17,96.90,365.40,203.94,9.96;17,96.90,377.36,203.93,9.96;17,96.90,389.31,150.87,9.96;17,267.59,389.31,33.25,9.96;17,96.90,401.26,203.95,9.96;17,96.90,413.22,203.94,9.96;17,96.90,425.18,246.80,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,206.97,365.40,93.87,9.96;17,96.90,377.36,203.93,9.96;17,96.90,389.31,146.21,9.96">Towards a Standard Protocol for the Evaluation of Video-to-Shots Segmentation Algorithms</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiloba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Quénot</surname></persName>
		</author>
		<ptr target="URL:clips.image.fr/mrim/georges.quenot/articles/cbmi99b.ps" />
	</analytic>
	<monogr>
		<title level="m" coord="17,285.19,389.31,15.65,9.96;17,96.90,401.26,203.95,9.96;17,96.90,413.22,73.43,9.96">European Workshop on Content Based Multimedia Indexing</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,445.10,228.80,9.96;17,96.90,457.05,203.93,9.96;17,96.90,469.02,166.93,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,163.30,445.10,137.50,9.96;17,96.90,457.05,128.88,9.96">Analyzing the Subject of a Picture: A Theoretical Approach</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,235.78,457.05,65.05,9.96;17,96.90,469.02,101.32,9.96">Cataloging and Classification Quarterly</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="39" to="61" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,488.93,228.82,9.96;17,96.90,500.89,203.94,9.96;17,96.90,512.85,203.94,9.96;17,96.90,524.81,203.92,9.96;17,96.90,536.76,72.06,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,283.68,488.93,17.14,9.96;17,96.90,500.89,118.50,9.96">The trec-2001 video track report</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Taban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,209.20,512.85,91.64,9.96;17,96.90,524.81,81.32,9.96">The Tenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>TREC-2001</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
