<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,68.20,81.73,475.64,12.64">Question Answering Approach Using a WordNet-based Answer Type Taxonomy</title>
				<funder>
					<orgName type="full">Korea Science and Engineering Foundation (KOSEF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.72,118.16,74.88,10.80"><forename type="first">Seung-Hoon</forename><surname>Na</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering Division Pohang University of Science and Technology (POSTECH) and Advanced Information Technology Research Center (AlTrc) Question Classification Entity Classification</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.37,118.16,52.74,10.80"><forename type="first">In-Su</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering Division Pohang University of Science and Technology (POSTECH) and Advanced Information Technology Research Center (AlTrc) Question Classification Entity Classification</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.89,118.16,69.67,10.80"><forename type="first">Sang-Yool</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering Division Pohang University of Science and Technology (POSTECH) and Advanced Information Technology Research Center (AlTrc) Question Classification Entity Classification</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.78,118.16,79.70,10.80"><forename type="first">Jong-Hyeok</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering Division Pohang University of Science and Technology (POSTECH) and Advanced Information Technology Research Center (AlTrc) Question Classification Entity Classification</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,68.20,81.73,475.64,12.64">Question Answering Approach Using a WordNet-based Answer Type Taxonomy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">82D1DA8CA3818BA03A3BCE8F86A6270D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In question answering (QA), answer types are semantic categories that questions require. An answer type taxonomy (ATT) is a collection of these answer types. ATT may heavily affect the performance of QA systems, because its broadness and granularity provides coverage and specificity of answer types. Cardie <ref type="bibr" coords="1,345.76,307.84,11.67,8.96" target="#b0">[1]</ref> used 13 categories for entity classification, and obtained large performance improvement, compared with the method using no categories. Also, according to Pasca et al. <ref type="bibr" coords="1,57.88,343.84,10.64,8.96" target="#b2">[3]</ref>, the more categories a system uses, the better performance the system shows. For example, consider two answer type taxonomies, A={PERSON}, and B={PRESIDENT, ENGINEER, SINGER, PERSON}. Given a question "Who was the president of Vichy France?", we know that the more specific answer type of this question is not PERSON, but PRESIDENT.</p><p>Thus, if we use ATT B, a set of candidate answers from documents can be reduced to a set of PRESIDENT entities, by excluding the other PERSON entities such as ENGINEER and SINGER. This is not the case with ATT A. However, since ATT A cannot distinguish hypernyms of PERSON, the QA system should consider much more candidate answers.</p><p>Thus far, most QA systems rely on small-scale ATTs, with the number of semantic categories ranging from 20 to 100.</p><p>Normally, these ATTs are created from a beginning set of frequently-asked answer types like person, organization, location, number, etc., and then they are incrementally extended to include unexpected answer types from new questions. However, these ad-hoc ATTs may raise the following problems in QA. First, it is nontrivial to manually enlarge a small ATT to a large one, as new answer types appear. Second, ad-hoc ATTs do not allow easy adaptation for processing questions asking new answer types. For such questions, the system needs to modify an existing IE module to classify entities into new answer types. Third, previous ATTs do not have sufficient broadness and granularity, where they are expected characteristics of ATT for open-domain QA. Therefore, at this year's TREC, we have taken a question answering approach that uses WordNet itself as ATT. In other words, our QA system maps an answer type into a concept node called a synset in WordNet. WordNet provides sufficient diversity and density to distinguish specific answer types for most questions. By using such an ontological taxonomy, we do not have the above problems with small ad-hoc ATTs. This paper is organized as follows. Section 2 describes each module of our QA system, and section 3 shows TREC-11 evaluation results, and concluding remarks are given in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Description</head><p>Figure <ref type="figure" coords="2,273.16,391.12,3.74,8.96">1</ref>. System Architecture Our QA system consists of components as illustrated in figure <ref type="figure" coords="2,313.49,423.04,3.75,8.96">1</ref>. Question Classification analyzes a question to determine its answer type which maps to a synset in WordNet. Passage Retrieval formulates a vector query from the question, and retrieves relevant passages. Entity Classification processes top N passages from Passage Retrieval to classify each entity into its semantic category which as well corresponds to a synset in WordNet. A final answer is obtained from Answer Extraction by processing two sub sequent steps: Determination of Candidate Answers and Ranking Candidate Answers. Determination of Candidate Answers semantically recognizes all candidate answers in relevant passages by matching a taxonomic relation between the answer type of a question and the entity type of each entity, and regard matched entities as candidate answers. In the matching process, if any entity type is a hypernym of the answer type, since the entity does not have sufficient evidence to be selected as a candidate answer, thus in the case, Entity Feedback is invoked to collect clue words indicating more specific type from whole document collection. After Entity Feedback, the entity can be classified to more specific semantic category by Entity Classification. Unfortunately, in TREC-11, we did not perform experiment on the Entity Feedback. Ranking of Candidate Answers ranks candidate answers by calculating the similarity between a question and the passage containing each candidate answer, and generates a top candidate answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Classification</head><p>For each question, Question Classification determines its answer type using a set of 20 question pattern rules that were created from analyses of previous TREC questions. First, a question is tagged with part-of-speech tags using Tree-Tagger <ref type="bibr" coords="3,57.88,100.84,10.67,8.96">[6]</ref>. Next, our NP-chunker performs NP chunking and marks a linguistic head of a noun phrase. The NP-chunker was designed by a simple rule-based scheme. After chunking, question pattern rules are applied to the question to determine an answer type of a question.  <ref type="table" coords="3,102.86,351.76,4.98,8.96" target="#tab_0">1</ref> shows some examples of question pattern rules, where lexical and semantic constraints describe preconditions to be met before the question pattern is matched against a question. {X} indicates a WordNet synset of a lexical word corresponding to X. Since there are several senses for a lexical word, {X} is currently assumed to be the most frequent sense. '{X} &lt; {Y}' means that {X} be a hyponym of {Y}. For example, suppose a question "What is the governor of Colorado?". After performing np-chunking for this question, the question is converted into "What is [the governor] NP of <ref type="bibr" coords="3,57.88,441.76,46.00,8.96">[Colorado]</ref> NP ?". This is matched with the first rule in table 1, because it satisfies a lexical constraint 'What (be) NP', and a synset of the NP head GOVERNOR synset , is a hyponym of ENTITY synset . In this example, therefore, the answer type is a synset GOVERNOR synset in WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical constraint</head><p>On the other hand, to deal with questions starting with 'how', we defined an additional semantic relation Unit_of into WordNet as follows.</p><formula xml:id="formula_0" coords="3,183.88,544.59,244.31,34.12">DISTANCEsynset -Unit_of → LINEAR_UNITsynset FINANTIAL_CONDITIONsynset -Unit_of → MONETARY_UNITsynset</formula><p>This relation is also used in question pattern rules. For example, given a question "How far would you run if you participate in a marathon?", it satisfies the lexical constraint of the third pattern in table <ref type="table" coords="3,416.46,604.84,3.71,8.96" target="#tab_0">1</ref>, where JJ is a part-of-speech tag for an adjective. Because a synset FAR synset for 'far' has a Value_of relation with a synset DISTANCE synset , the variable X in the semantic constraint becomes DISTANCE synset . Since, we defined that DISTANCE synset has a Unit_of relation with LINEAR_UNIT synset in WordNet, the variable Y becomes LINEAR_UNIT synset . Therefore, the answer type of the question is set to LINEAR_UNIT synset .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval</head><p>Passage Retrieval consists of two processing steps: document retrieval and passage ranking. For document retrieval, we have developed an IR system based on a vector space model. To extract terms, we use a stop word list provided by the SMART system <ref type="bibr" coords="4,124.54,145.84,10.69,8.96" target="#b4">[5]</ref>, from which 30 words such as first, second are eliminated because they provide important clues in QA.</p><p>We do not use stemming, but use Tree-Tagger [6] to obtain root forms which are used as terms. To weight each term, we employ the weighting formula nxx.bfx introduced by Salton and Buckley <ref type="bibr" coords="4,351.32,181.84,10.69,8.96" target="#b3">[4]</ref>.</p><p>From top 1000 documents generated by a document retrieval module, passage ranking identifies all legitimate passages in each document and rank them, using cover density ranking by Clarke et al. <ref type="bibr" coords="4,415.99,217.83,10.64,8.96" target="#b1">[2]</ref>. A passage unit is the minimal number of subsequent sentence including maximal number of query terms. No three passages are taken from the same document and no passages can contain more than five sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Entity Classification</head><p>From the top 15 passages obtained by passage retrieval, Entity Classification classifies each entity occurring in the passages into a semantic category that maps to a synset in WordNet. First, we access entity databases. There are three kinds of entity databases: person names, location names, and organization names. If an entity is found in one of these databases, then the type of the entity is set to the type of the entity database. Otherwise, the entity is looked up in WordNet, and its corresponding synset is selected as the entity type. If the entity is not found even in WordNet, a clue word related to the entity is searched within the document containing the entity. Here, a clue word means a word that can indicate a semantic category of the entity. To recognize a clue word, we use a set of 30 type indicator patterns, which was empirically constructed. Table <ref type="table" coords="4,136.78,442.83,4.98,8.96" target="#tab_1">2</ref> shows some examples of type indicators. For instance, appositive constructions may provide type words for entities. For entities like "Nancy Powers" and "Thomas" in table 2, we know that their entity types are DIRECTOR synset and DETECTIVE synset respectively, since the entity and its type word are grouped into an appositive construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type indicator Examples</head><p>Appositive </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Determination of Candidate Answers</head><p>Determining whether an entity is a candidate answer or not is based on taxonomic relations in WordNet between an entity</p><p>type and the answer type of a question. Figure <ref type="figure" coords="5,249.03,100.84,4.98,8.96">2</ref> shows three possible taxonomic relationships between the entity type and the answer type.</p><p>The first is the case where the answer type is a hypernym of an entity type. The second is the case where an entity type is a hypernym of the answer type. The third includes all the other cases. As an example of the first case, suppose that the answer type is PERSON synset and an entity type is ENGINEER synset . Then, since PERSON synset is a hypernym of ENGINEER synset , the entity becomes a candidate answer. As an example of the third case, suppose that the answer type is ENGINEER synset and an entity type is PRESIDENT synset . In this example, because the entity is not an ENGINEER synset , it cannot become a candidate answer. As an example of the second case, if the answer type is PRESIDENT synset and an entity type is PERSON synset , it is not clear whether the given entity is a PRESIDENT synset or not. So, candidate answer determination fails. In this case, an entity feedback module is fired to acquire another clue words from the whole document collection, which may contain the more specific semantic category for the same entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Ranking of Candidate Answers</head><p>After recognizing candidate answers, each candidate answer is ranked by the formula <ref type="bibr" coords="5,401.83,550.84,10.64,8.96" target="#b0">(1)</ref>. <ref type="figure" coords="5,300.85,607.72,207.23,8.96">----------------------------------------------------------(1)</ref> In formula <ref type="bibr" coords="5,103.25,676.84,10.66,8.96" target="#b0">(1)</ref>, Type_score is determined by WordNet taxonomic relationships between the answer type of the question and the entity type of the candidate answer. When the relationship belongs to the first case in figure <ref type="figure" coords="5,441.03,694.84,3.75,8.96">2</ref>, its Type_score is 1, and in q t e t </p><formula xml:id="formula_1" coords="5,98.80,580.31,202.05,71.08">set term order Second T w w score _ Context score _ Context score _ Type Score T ) t , t ( t , t , d ) t , t ( , q j i j i j i = × = × = ∑ ∈ ) ( -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Taxonomic Relationships between an Answer Type and an Entity Type</head><p>the second case, Type_score set to 1/2 since currently our QA system does not support entity feedback module.</p><p>Context_score is a score calculated from proximity between the candidate answer and the question words in the passage where the candidate answer appears. To compute a context score, first we convert the passage into a secondorder vector (SOV) that is different from the traditional first-order vector (FOV). The difference is that a term in FOV consists of a single lexical word and a term in SOV consists of two lexical words. For example, when a vocabulary set is V ={A, B, C}, the possible term set in SOV is T = {AB, AC, BC}, where each element is called by a second-order term.</p><p>To weight a second-order term, we assume a proximity hypothesis that the closer two participating lexical words of the second-order term is in a passage, the more important the term is. According to this hypothesis, given any two lexical words t i and t j in second order term set, we calculate the weight of the second-order term (t i ,, t j ) by formula <ref type="bibr" coords="6,510.52,226.84,10.67,8.96" target="#b1">(2)</ref>, where  </p><formula xml:id="formula_2" coords="6,99.52,294.28,411.99,65.01">w = ------------------------------------------------------------------------------------(2)      + × =</formula><formula xml:id="formula_3" coords="6,99.52,420.64,152.23,32.86">i j i j i proximity idf idf w ×         + =<label>2</label></formula><p>---------------------------------------------------------------------(3) NIL answers are generated as follows. If either a proper noun in a question does not appear in the top passages, or the confidence value for the top answer is below a threshold 0.1, then our system generates the NIL answer. The confidence value for NIL generation is calculated by formula (4). Finally we rank all the top answers with their confidence values. <ref type="figure" coords="6,305.89,636.40,203.33,8.96">----------------------------------------------------------(4</ref>)</p><formula xml:id="formula_4" coords="6,160.60,460.99,173.09,61.05">       &lt; = otherwise dist _ pos if dist _</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑ ∑</head><formula xml:id="formula_5" coords="6,98.92,619.90,206.97,47.27">∈ ∈ × = T ) t , t ( t , t , q T ) t , t ( t , t , d t , t , q j i j i j i j i j i w w w value _ Confidence ) ( ) ( ) ( -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation Results</head><p>We obtained the following evaluation results at TREC-11. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Our QA system used WordNet as ATT for open-domain question answering. Question pattern rules were employed to determine an answer type of a question. In addition, in order to map entities in relevant passages into an entity type that corresponds to a synset in WordNet, we devised type indicator patterns. For each entity, taxonomic relationships between the answer type and its entity type are checked to qualify candidate answers. Unqualified entities are passed to an entity feedback module which provides several clue words to determine the more specific type for the problematic entity. A final answer is obtained from a ranking method, which is based on a second-order vector representation for relevant passages.</p><p>In our unpublished experiments on 300 questions in TREC-8,9,10, our system showed about 10% performance improvement by using entity feedback. But on TREC-11 questions, we did not yet perform experiments. From TREC-11 results, we had recorded a bad performance for non-basic entities like CAR synset , WEAPON synset. . In future, we plan to refine the entity classification techniques to determine types of these non-basic entities effectively, and to apply the entity feedback method on such a classification method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,77.92,173.56,459.15,187.16"><head>Table 1 . Question Pattern Rules Table</head><label>1</label><figDesc></figDesc><table coords="3,82.12,173.56,454.95,136.64"><row><cell></cell><cell>Semantic constraint</cell><cell>Answer type</cell><cell>Example</cell></row><row><cell></cell><cell></cell><cell></cell><cell>What is the president of</cell></row><row><cell>What (be) NP</cell><cell>{head of NP} &lt;= ENTITY synset</cell><cell>{head of NP}</cell><cell>U.S. ?</cell></row><row><cell>How many NP</cell><cell>{head of NP} &lt; UNIT synset</cell><cell>{head of NP}</cell><cell>How many miles ...?</cell></row><row><cell>How JJ</cell><cell>Y ∃ Value_of ({JJ}, X) and Unit_of (X,Y)</cell><cell>Y</cell><cell>How fast does a cheetah run?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Who was the lead singer for</cell></row><row><cell>who</cell><cell></cell><cell>PERSON synset</cell><cell>the Commodores?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,131.44,551.92,358.41,120.56"><head>Table 2 . Examples of Type Indicators</head><label>2</label><figDesc></figDesc><table coords="4,131.44,551.92,358.41,102.08"><row><cell></cell><cell>1) Nancy Powers, a sales director for a medical company</cell></row><row><cell></cell><cell>2) Steve Thomas, the public detective</cell></row><row><cell>Role</cell><cell>1) Mrs. Aquino, 2) New York city</cell></row><row><cell>Relative clause</cell><cell>1) Kenneth Starr, who</cell></row><row><cell>Preposition</cell><cell>1) in Washington</cell></row><row><cell>Unit</cell><cell>1) 8,000 miles, 2) 8 miles per hour</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,57.88,119.32,496.40,432.09"><head>Table 3 . Performance by Answer Types</head><label>3</label><figDesc>We selected randomly 100 of 500 questions, and evaluated the performances for some well-known answer types. The results are shown in table3.  Here, an answer type includes all subtypes of the answer type. We had a promising performance in the case of an answer type DATE synset , but we did not generate correct answers in the case of other entity types such as HORSE synset , CAR synset .</figDesc><table coords="7,80.80,119.32,397.19,413.37"><row><cell>Number of wrong answers</cell><cell>399</cell><cell></cell></row><row><cell>Number of unsupported answers</cell><cell>5</cell><cell></cell></row><row><cell>Number inexact answers</cell><cell>10</cell><cell></cell></row><row><cell>Number right answers</cell><cell>86</cell><cell></cell></row><row><cell>Confidence-weighted score</cell><cell>0.298</cell><cell></cell></row><row><cell>Precision of recognizing no answer</cell><cell>0.161 (=15 / 93)</cell><cell></cell></row><row><cell>Recall of recognizing no answer</cell><cell>0.326 (=15 / 46)</cell><cell></cell></row><row><cell>Answer type</cell><cell>Percentage</cell><cell>Precision</cell></row><row><cell>person</cell><cell>19%</cell><cell>21.05%</cell></row><row><cell>location</cell><cell>23%</cell><cell>8.69%</cell></row><row><cell>organization</cell><cell>4%</cell><cell>25%</cell></row><row><cell>unit</cell><cell>7%</cell><cell>14.28%</cell></row><row><cell>count</cell><cell>4%</cell><cell>0%</cell></row><row><cell>date</cell><cell>17%</cell><cell>58.82%</cell></row><row><cell>fullname</cell><cell>1%</cell><cell>0%</cell></row><row><cell>other entities</cell><cell>25%</cell><cell>0%</cell></row><row><cell>Total</cell><cell>100%</cell><cell>18%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">Korea Science and Engineering Foundation (KOSEF)</rs> through the <rs type="institution">Advanced Information Technology Research Center (AITrc)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,86.14,298.84,468.26,8.96;8,86.20,316.84,344.81,8.96;8,431.08,314.65,5.04,5.83;8,441.64,316.84,112.53,8.96;8,86.20,334.84,167.82,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,279.24,298.84,275.16,8.96;8,86.20,316.84,225.63,8.96">Examining the Role of Statistical and Linguistic Knowledge Sources in a General-Knowledge Question-Answering System</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,340.16,316.84,90.86,8.96;8,431.08,314.65,5.04,5.83;8,441.64,316.84,112.53,8.96;8,86.20,334.84,89.95,8.96">Proceedings of the 6 th Applied Natural Language Processing Conference</title>
		<meeting>the 6 th Applied Natural Language Processing Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="180" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,86.15,352.84,468.14,8.96;8,86.20,370.84,252.67,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,321.26,352.84,224.65,8.96">Relevance Ranking for One to Three Term Queries</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Tudhope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,86.20,370.84,162.67,8.96">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="311" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,86.18,388.84,468.03,8.96;8,86.20,406.83,453.55,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,236.82,388.84,163.46,8.96">High Performance Question / Answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,424.26,388.84,129.95,8.96;8,86.20,406.83,376.24,8.96">Proceedings of the 24rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,86.20,424.83,468.00,8.96;8,86.20,442.83,140.10,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,204.60,424.83,228.10,8.96">Term-weighting Approaches in Automatic Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,443.32,424.83,110.88,8.96;8,86.20,442.83,49.66,8.96">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,86.19,460.83,203.33,8.96" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration" coords="8,86.19,460.83,62.25,8.96">SMART system</orgName>
		</author>
		<ptr target="ftp://ftp.cs.cornell.edu/pub/smart/" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
