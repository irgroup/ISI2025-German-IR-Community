<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.03,112.68,287.81,12.63">TREC11 Web and Interactive Tracks at CSIRO</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,193.34,133.81,62.71,9.94"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Enterprise Search</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.38,133.81,69.38,9.94"><forename type="first">David</forename><surname>Hawking</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Enterprise Search</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.89,133.81,55.37,9.94"><forename type="first">James</forename><surname>Thom</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,189.98,146.53,66.44,9.94"><forename type="first">Trystan</forename><surname>Upstill</surname></persName>
							<email>trystan.upstill@anu.edu.au</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.98,146.53,67.37,9.94"><forename type="first">Ross</forename><surname>Wilkinson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Technologies for Electronic Documents CSIRO Mathematical and Information Sciences {Nick.Craswell; David.Hawking</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.45,146.53,59.96,9.94"><forename type="first">Mingfang</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Technologies for Electronic Documents CSIRO Mathematical and Information Sciences {Nick.Craswell; David.Hawking</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.12,209.76,63.96,9.94"><forename type="first">Mingfang</forename><surname>Wu}</surname></persName>
						</author>
						<author>
							<persName coords="1,432.41,209.76,42.64,9.94"><forename type="first">Csiro</forename><surname>Au</surname></persName>
						</author>
						<title level="a" type="main" coord="1,162.03,112.68,287.81,12.63">TREC11 Web and Interactive Tracks at CSIRO</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A58AD959EA7D4148A0475D2CACDC1B8A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our web track participation was a preliminary exploration of forms of evidence which might be useful for named page finding and topic distillation. For this reason, we made heavy use of evidence other than page content in our runs.</p><p>In the interactive track, we continue to focus on answer organization issues, aiming to investigate the usefulness of the knowledge about "organizational structure" in organizing and delivering the retrieved documents. For the collection of the US government (.gov domain) web documents, we used their level two domain labels and their corresponding organization names to categorize the retrieved documents. For example, documents from the "nih.gov" domain will be put into the "National Institutes of Health (nih)" category. We compared this delivery method with the traditional ranked list. The preliminary results indicate that subjects achieved a significantly better performance with the category interface at the end of fifteen minutes search, however, there is no significant difference between the two methods during the first five or ten minutes. The experiment result also shows that the category interface assisted subjects answer the more complex topics as time increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The web track</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Topic distillation</head><p>In topic distillation we used the following forms of evidence: • BM25 on content. Pages returned should be relevant. We indexed the .GOV corpus and applied BM25, sometimes with stemming sometimes without.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• In-link counting and filtering. We expected pages with more in-links to be potentially better answers, and we differentiated between on-host and off-host links. We also eliminated many results on the grounds that they had insufficient in-links. • URL length. We expected short URLs to be better answers than long URLs • BM25 score aggregation. We expected sites with many BM25-matching pages to be better than those with few.</p><p>Table <ref type="table" coords="2,120.58,170.77,5.52,9.94" target="#tab_0">1</ref> reports the results for our topic distillation runs. Our (non-submitted) content-only achieved better performance than any of the submitted runs that included "distillation evidence". In this year's topic distillation task, the focus on local page content relevance ("BM25 content only") was probably too high for our non-content and aggregation methods to succeed (our "distillation evidence"). We expected most correct answers to be shallow URLs of sites containing much useful content. In fact, correct answers were deeper, and our aggregation method for finding sites rich with relevant information was actually quite harmful (runs 3 and 4). The focus on page content is borne out by the improvement in effectiveness achieved when we apply simple BM25 in an unofficial run (csiro02unoff). To perform better in this year's task, we should have put less (or no) emphasis on distillation evidence and far more emphasis on relevance. However, we also believe that in some Web search situations, the distillation evidence would be more important than it was in this year's task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Named Page Finding</head><p>In our named page finding experiments we used the following forms of evidence:</p><p>• BM25 on content and/or anchor text. We indexed the .GOV corpus and applied BM25 to document content and to surrogate documents that contained all anchor text pointing to a page. Stemming of query terms was also employed. • Extra Title Weighting. To bias our results towards what we thought would be page naming text we put further emphasis on document titles. • PageRank. To see whether link recommendation could be used to improve results we incorporated this link popularity measure <ref type="bibr" coords="2,292.36,601.89,11.68,9.94" target="#b3">[3]</ref>.</p><p>Table <ref type="table" coords="2,117.87,620.61,5.52,9.94" target="#tab_1">2</ref> shows the results for the named page finding runs. The BM25 content-only submission performed the best. We tried combining content evidence with anchor-text and PageRank but both combinations harmed retrieval effectiveness. Prior to submission we generated 20 training queries and found content with extra title weighting performed best. We expected page titles to be important evidence in named page finding, however this appeared not to be the case -in fact extra title weighting for the TREC queries appeared to reduce effectiveness (run 1 vs run 3). While there was some anchor text evidence present for the query set (run 2) when we combined this evidence with content (runs 4 and 16) results were noticeably worse than for the content-only run (run 1). PageRank harmed retrieval effectiveness (run 16 vs run 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The interactive track</head><p>On the Internet, the information source and its information provider indicate not only the quality and credibility of the information, but also the type and content of the information. When people try to access information from an organization's website, they very often try to match their mental model about that organization with their information needs. They can usually identify a few related departments in that organization, and search the information within these departments.</p><p>We can consider the whole worldwide web as the web site of a global organization with a hierarchical structure. Documents in this space could be categorized by their "functional departments" corresponding to their domain names. For example, the level one domain labels can categorize the documents into government (.gov), university (.edu), military (.mil), and commercial (.com) etc. (In fact, they should be the level two domain labels, with the level one label of .us) ; the level two domain label can be used to further categorize the documents within the first level domain.</p><p>In this year's interactive track, all documents in the collection are gathered from the US government domain (.gov). The test topics also cover various areas, such as government policy, medicine/health and travel. To this collection and the topic set, our intuition was to organize and delivery the retrieved documents according to the US government functional (or departmental) structure. We intent to use this dynamically generated organizational structure to organize the distributed documents retrieved from the web, and guide users to focus their attentions on the information sources and/or information providers. We hypothesized that this structure (called categorization structure) would serve as a better guide for a user to locate relevant and authoritative information than the traditional ranked list, thus improving the user's performance with the search tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Delivery interfaces</head><p>The Panoptic <ref type="bibr" coords="3,156.32,701.36,12.65,9.94" target="#b1">[1,</ref><ref type="bibr" coords="3,168.97,701.36,8.43,9.94" target="#b2">2]</ref> is used as the back-end search engine in both delivery methods. In the categorization delivery method, the categoriser classifies the retrieved documents according to the level two domain labels. Each category label is obtained by expanding the domain label into its owner's organizational name through the "whois" server (http://www.whois.nic.gov). For example, all documents from the "nih.gov" domain will be put into the "National Institutes of Health" category. The documents in a category are ranked according to their original rank in the returned ranked list, and the categories are ranked according to the original rank of the first document of each category. The category interface shows the first category by default.</p><p>The interfaces for the two different delivery methods are shown in Figure <ref type="figure" coords="4,432.00,156.37,5.52,9.94">1</ref> and Figure <ref type="figure" coords="4,494.32,156.37,4.15,9.94">2</ref>. We have been trying to keep the two interfaces as consistent as possible, differing only in their presentation of the alternate structures. Both interfaces are divided into three areas: the top area shows the current search topic and provides three buttons for the subjects to save answer and move on to the next topic. The middle area is the query area that has a query box and information on query word matching. The bottom area is the main area that shows either the ranked list or the categorized result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Experimental procedures</head><p>During the experiment, all subjects are asked to follow the following procedures.</p><p>• Subjects filled in the pre-search questionnaire about their demographic information and their search experience. • Subjects were then shown the two experimental interfaces, and were free to ask any question related to the use of the two interfaces. • Subjects were assigned to the experimental design that was used by all participant groups in the interactive track. In this experimental design, subjects searched four topics on each interface, the sequence of interface and topics varied among subjects. A complete such a design requires a group of 16 subjects. • Prior to each interface, subjects had hands-on practice with an example topic, and got familiar with each interface. • Prior to each interface, the query "information retrieval" was issued by the corresponding system automatically to calibrate the difference between two systems' response time.</p><p>Subjects were asked to click the "Next Topic" button when they saw the search result appeared. The average response times are 6.8 seconds for the ranked list interface and 8.3 seconds for the category interface. • Prior to the search of each topic, subjects were required to fill in a pre-search questionnaire about their familiarity with the topic. After the search of the topic, subjects filled in a postsearch questionnaire about their experience of that particular search topic. • Subjects filled in a post-system questionnaire after each interface.</p><p>• Subjects filled in an exit questionnaire in the end of the experiment.</p><p>At any time during a topic search, subjects could move on to the next topic whenever they found the required answer and were satisfied with what they have found. We encouraged our subjects to find answers to a topic within ten minutes, however they could have an extra five minutes in case they could not find the required answer in the first ten minutes and want to continue their search.</p><p>Transaction logging, questionnaire, and screen recording are the main methods to collect data. During each search session, every significant event -such as document read, the instance saved and the supporting source document and the query sent -was automatically captured. Questionnaires are those common to all participant groups in the interactive track. Screen recording was used to capture the search process for further detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Subjects</head><p>All our sixteen subjects were university students. These subjects came from various backgrounds, such as computer science, media study, law and mechanical engineering. Of the sixteen subjects, fourteen are male and two are female. Fifteen of them are in the age group 18-27 years, only one is in the age group 38-47 years. Table <ref type="table" coords="6,266.85,282.36,5.52,9.94" target="#tab_2">3</ref> lists subjects' responses to the selected questions from the pre-search question (all are on 7-point Likert scale). From the table, we can see that our subjects search the web very often (Q1, mean=5.81), can usually find what they are looking for (Q5, mean=5.38), and generally regard themselves as experienced searcher (Q10, mean=4.73). Comparatively, subjects use the search box (Q6, mean=5.19) more often than browsing mechanism (Q7, mean=4.06). These subjects very often search for information related to assignments (Q8-1, mean=5.38) and entertainment (Q8-6, mean=5.19), while search less on shopping (Q8-2, mean=3.19), government policy (Q8-5, mean=3.06), and traveling (Q8-3, mean=2.94), and least on medical/health (Q8-4, mean=1.94). (While our test topics cover the government policy, traveling, and medical/health.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Performance with two interfaces</head><p>The effectiveness of the two interfaces is measured by the success rate: the ratio of the correctly saved instances. There are two types of topics in this year's interactive track. Type I topic is "find X instance of …". Type II topic is "find a website that is a good resource on Y". For the topics of type I (topic 1, 2, 4, 5 and 6), each instance correctly identified and supported by a document will be given a score 1/n, where n is the number of required instances for the search topic. Type II topic can be regarded as a special case of the type I where the required instance is 1. So for the topics of type II (topic 3, 7 and 8), the score is binary: 1 -for the correctly identified website, and 0 -for a website that does not give information on the topic. (A 5-or 7-point Likert scale could be used to judge the degree of "goodness" of the saved website. However, this kind of judgement might be too subjective to reach consistence. So we adopted the binary score.)</p><p>Table <ref type="table" coords="6,118.96,667.04,5.52,9.94" target="#tab_3">4</ref> shows the subjects' search performance at three period cut-off -after five minutes, ten minutes, and fifteen minutes. On the average, the performance with the category interface is  lower than that with the ranked list interface at the end of the five minutes, higher than the ranked list interface at the end of the ten minutes, and significantly outperform the ranked list interface at the end of fifteen minutes. (two tailed, paired t-text) Before the search of each topic, subjects were asked about their level of familiarity with the topic on a 7-point Likert scale. On the average, our subjects have low familiarity with all topics (Ranked list: Mean = 2.14, Std = 0.70; Category: Mean = 2.19, Std = 0.72). Although the correlation between the success rate with the ranked list and the familiarity (r = 0.51) is higher than the correlation between the success rate with category and the familiarity (r = -0.0004), nevertheless, neither of the correlations is significant.</p><p>Figure <ref type="figure" coords="7,121.70,543.09,5.52,9.94" target="#fig_1">3</ref> shows the breakdown of the success rate according to the sessions in which a question is either "not answered", "partially answered", or "completely answered" at three cut-off periods.</p><p>At the end of the first five minutes, the number of "not answered" sessions with the category interface is more than the number of those with the ranked list interface. However, with the increase in time spent, the number of "not answered" sessions with the category interface decreases, although the difference is not significant at each cut-off period.</p><p>At the end of each cut-off period, the number of "partially answered" session with the category interface is always less than the number of those with the ranked list interface, although the difference is not significant either.</p><p>At the first cut-off period, subjects have less "completely answered" sessions with the category interface than that with the ranked list interface (not significant). However, at the second and third cut-off period, subjects have significantly more "completely answered" sessions using the category interface (p &lt; 0.05 at tenth minute, and p &lt; 0.01 at the fifteenth minute). Looking at topic by topic at the fifteenth minute, the category interface is performing better for 7 out of 8 topics. In the only exceptional topic -the topic 3, the two interfaces performed the same with the same number of "completely answered" sessions). For the topics 1, 2, and 6, the number of "completely answered" sessions with the category interface is twice that with ranked list interface. Here the topic 1, 2 and 6 are all of the type I.</p><p>We had assumed that the type I topics might need to gather instances from multiple documents, but this is not always the case -sometimes a document may contain enough information to cover all required instances. Table <ref type="table" coords="8,217.87,168.97,5.52,9.94" target="#tab_4">5</ref> shows the distribution of the "completely answered" sessions from either the multiple documents or one document only. In four out of five such type I topics, there are more sessions with the category interface in which the saved answers come from multiple documents. This may suggest that the category interface is more helpful for the more complicated tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Subject's effort</head><p>The subject's effort for getting an answer is measured by the time, the number of documents read, and the number of queries sent in order to get a complete answer or reach the end of each session.</p><p>Table <ref type="table" coords="8,119.56,296.16,5.52,9.94" target="#tab_5">6</ref> shows the average time spent in order to get a complete answer by the two quickest subjects using each interfaces. On the average, the quickest two subjects using the category interface took less time than the quickest two using the list interface, but the difference is not significant here. If we look topic by topic, the two subjects are quicker using the category interface only for three topics -the topic 1, 2 and 4; there three topics are of type I.</p><p>Table <ref type="table" coords="8,118.00,367.43,5.52,9.94" target="#tab_6">7</ref> shows the interaction between the subject and the interface. On the average, subjects read more documents with the category interface (Mean=4.81) than with the ranked list interface (Mean=4.74), but sent less queries with the category interface (Mean=3.0) than with the ranked list interface (Mean=3.54). This indicate that the ranked list interface may encourage subjects to rephrase queries, while the category interface may encourage subjects to browse the answer structure, thus read more documents.</p><p>Usually subjects read and saved documents high in rank from the ranked list interface -the average rank of the read and saved documents is 4.97 and 4.87 respectively. While the average rank of the read and saved documents from the category interface is 19.10 and 16.5 respectively. This may indicate that the category interface may be able to bring relevant (or related) documents in a category; these documents may scatter in the ranked list, while a subject may not go that far to get that relevant document with the ranked list interface.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Subject's satisfaction</head><p>After the search of each topic, subjects filled in a post-search questionnaire that was to get the subject's satisfaction of that particular search topic. Table <ref type="table" coords="9,349.97,406.91,5.52,9.94">8</ref> shows the subjects' response to each question. For all questions, the average response from the subjects using category interface is lower than that from the subjects using the ranked list interface, although no significant difference is found between the two interfaces for any questions.</p><p>We checked the correlation between the each question and the success rate, significant positive correlation is found only between the PS3 (satisfaction) and the success rate (in both interfaces, r = 0.69, significance at 0.05). That may be truism: if subjects saved more answers, they are getting more satisfied.</p><p>In the exit questionnaire, when the subjects were asked about which of the systems they like the best overall, 11 subjects chose the category interface, 3 subjects chose ranked list interface, while the remaining 2 thinking there is no difference between the two interfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>Our experimental results indicate that the users may be able to find the answer quicker with the ranked list interface for those easy search tasks where the search engine is able to bring the relevant documents on the top of the ranked list. However, for more complicated tasks where an answer is to be synthesized from multiple documents, and those documents are scattered along the ranked list, the user may perform better with the category interface. This performance is achieved by spending longer reading or browsing time. One possible reason might be related to the categorization structure itself: the current one-level flat structure may not be very clear to the subjects. It could be enhanced by having a multi-level hierarchical structure closely reflecting the US governmental structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,196.34,594.47,219.20,8.96;4,122.79,266.86,366.45,316.89"><head>Figure 1 Figure 2</head><label>12</label><figDesc>Figure 1 The delivery interface for the ranked list</figDesc><graphic coords="4,122.79,266.86,366.45,316.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,200.42,385.09,211.08,8.96"><head>Figure 3</head><label>3</label><figDesc>Figure 3 The completeness of the saved answers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,121.59,71.87,368.73,291.46"><head></head><label></label><figDesc></figDesc><graphic coords="5,121.59,71.87,368.73,291.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,112.59,204.02,398.15,103.52"><head>Table 1 Runs for topic distillation</head><label>1</label><figDesc></figDesc><table coords="2,112.59,217.70,398.15,89.84"><row><cell>Run</cell><cell cols="2">P@10 BM25 content</cell><cell>BM25 content</cell><cell>In-link counting</cell><cell>URL length</cell><cell>BM25</cell></row><row><cell></cell><cell></cell><cell>only</cell><cell>and anchors</cell><cell>and filtering</cell><cell></cell><cell>aggregation</cell></row><row><cell cols="2">csiro02td1 0.1000</cell><cell>y</cell><cell></cell><cell>y</cell><cell>y</cell></row><row><cell cols="2">csiro02td2 0.0714</cell><cell></cell><cell>y</cell><cell>y</cell><cell></cell></row><row><cell cols="2">csiro02td3 0.0184</cell><cell>y</cell><cell></cell><cell>y</cell><cell>y</cell><cell>y</cell></row><row><cell cols="2">csiro02td4 0.0184</cell><cell></cell><cell>y</cell><cell>y</cell><cell></cell><cell>y</cell></row><row><cell cols="2">csiro02td5 0.0939</cell><cell>y (stem)</cell><cell></cell><cell>y</cell><cell>y</cell></row><row><cell cols="2">csiro02unoff 0.1959</cell><cell>y</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,101.19,74.43,396.95,126.91"><head>Table 2 Runs for named page finding</head><label>2</label><figDesc></figDesc><table coords="3,101.19,88.11,396.95,113.23"><row><cell>Run</cell><cell cols="2">ARR S@10</cell><cell>BM25</cell><cell>Stemming</cell><cell>Extra Title</cell><cell>Small Crawl</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Weighting</cell><cell>PageRank</cell></row><row><cell>csiro02np01</cell><cell>0.573</cell><cell>0.77</cell><cell>Content</cell><cell></cell><cell></cell><cell></cell></row><row><cell>csiro02np02</cell><cell>0.241</cell><cell>0.34</cell><cell>Anchor text</cell><cell></cell><cell></cell><cell></cell></row><row><cell>csiro02np03</cell><cell>0.416</cell><cell>0.59</cell><cell>Content</cell><cell></cell><cell>y</cell><cell></cell></row><row><cell>csiro02np04</cell><cell>0.318</cell><cell>0.51</cell><cell>Content and anchor text</cell><cell>y</cell><cell>y</cell><cell></cell></row><row><cell>csiro02np16</cell><cell>0.307</cell><cell>0.49</cell><cell>Content and anchor text</cell><cell>y</cell><cell>y</cell><cell>y</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,98.79,414.70,411.03,60.58"><head>Table 3 The selected questions 1 from the pre-search questionnaire.</head><label>3</label><figDesc></figDesc><table coords="6,98.79,430.80,411.03,44.48"><row><cell></cell><cell>Q1</cell><cell>Q5</cell><cell>Q6</cell><cell>Q7</cell><cell cols="2">Q8-1 Q8-2 Q8-</cell><cell>Q8-</cell><cell>Q8-5 Q8-6 Q10</cell><cell>Q11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>4</cell></row><row><cell cols="4">Mean 5.81 5.38 5.19</cell><cell cols="2">4.06 5.38</cell><cell cols="3">3.19 2.94 1.94 3.06</cell><cell>5.19</cell><cell>4.73</cell><cell>4.53</cell></row><row><cell>Std</cell><cell cols="3">1.05 0.96 1.83</cell><cell cols="2">1.98 1.09</cell><cell cols="3">1.64 1.06 1.18 1.65</cell><cell>1.42</cell><cell>1.34</cell><cell>1.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,101.31,74.43,405.57,297.57"><head>Table 4 Subjects' search performance per topic at three period cut-off</head><label>4</label><figDesc></figDesc><table coords="7,101.31,88.47,405.57,283.53"><row><cell>Topics</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell cols="2">Mean Std</cell><cell>p &lt;</cell></row><row><cell>5Min</cell><cell></cell><cell cols="9">List 0.38 0.08 0.75 0.44 0.42 0.21 0.13 0.63 0.38</cell><cell>0.24 0.26</cell></row><row><cell></cell><cell></cell><cell cols="9">Cate 0.38 0.04 0.63 0.22 0.25 0.42 0.00 0.63 0.32</cell><cell>0.24</cell></row><row><cell cols="11">10Min List 0.38 0.25 1.00 0.75 0.79 0.42 0.38 0.88 0.61</cell><cell>0.28 0.48</cell></row><row><cell></cell><cell></cell><cell cols="9">Cate 0.58 0.33 1.00 0.53 1.00 0.63 0.25 0.88 0.65</cell><cell>0.29</cell></row><row><cell cols="11">15Min List 0.38 0.54 1.00 0.84 0.92 0.50 0.50 0.88 0.69</cell><cell>0.24 0.05</cell></row><row><cell></cell><cell></cell><cell cols="9">Cate 0.75 0.63 1.00 0.75 1.00 0.88 0.63 1.00 0.83</cell><cell>0.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">the ranked list interface</cell></row><row><cell>The averge number of sessions</cell><cell>0 1 2 3 4 5 6 7</cell><cell cols="2">5Min 10Min</cell><cell>15Min</cell><cell></cell><cell cols="4">the category interface 5Min 10Min 15Min</cell><cell cols="2">5Min 10Min</cell><cell>15Min</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Not Answ ered</cell><cell></cell><cell cols="3">Partially Answ ered</cell><cell></cell><cell cols="2">Com pletely Answ ered</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,180.51,533.27,251.06,74.36"><head>Table 5 The source of the complete answers (M: from multiple documents; S: from one document only</head><label>5</label><figDesc></figDesc><table coords="8,427.23,548.75,4.34,8.96"><row><cell>)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,94.47,631.30,423.00,47.00"><head>Table 6 The average time (in minute) spent to get a complete answer by the two quickest subjects</head><label>6</label><figDesc></figDesc><table coords="8,513.37,631.30,4.10,8.96"><row><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,108.39,95.19,396.63,254.58"><head>Table 7</head><label>7</label><figDesc></figDesc><table coords="9,108.39,95.19,396.63,254.58"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Subject-interface interaction</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean</cell><cell>Std</cell><cell>P &lt; (2 tail t-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>test)</cell></row><row><cell cols="3">Number of documents read</cell><cell>List</cell><cell>4.74</cell><cell>2.52</cell><cell>NS</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Category</cell><cell>4.81</cell><cell>1.54</cell><cell></cell></row><row><cell></cell><cell cols="2">Number of queries</cell><cell>List</cell><cell>3.54</cell><cell>1.93</cell><cell>NS</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Category</cell><cell>3.00</cell><cell>1.39</cell><cell></cell></row><row><cell></cell><cell cols="2">The ranking of the read</cell><cell>List</cell><cell>4.97</cell><cell>2.05</cell><cell>0.0007</cell></row><row><cell></cell><cell>documents</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Category</cell><cell>19.10</cell><cell>7.94</cell><cell></cell></row><row><cell cols="3">The ranking of the saved</cell><cell>List</cell><cell>4.87</cell><cell>2.62</cell><cell>0.04</cell></row><row><cell></cell><cell>documents</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Category</cell><cell>16.5</cell><cell>14.32</cell><cell></cell></row><row><cell></cell><cell cols="5">Table 8 Subjects' response to the post-search questionnaire</cell><cell></cell></row><row><cell></cell><cell>PS1</cell><cell>PS2</cell><cell>PS3</cell><cell>PS4</cell><cell>PS5</cell><cell>PS6</cell></row><row><cell></cell><cell>(easy to start)</cell><cell>(east to</cell><cell>(satisfaction)</cell><cell>(timeliness)</cell><cell>(knowledge</cell><cell>(learn</cell></row><row><cell></cell><cell></cell><cell>search)</cell><cell></cell><cell></cell><cell>helped?)</cell><cell>something</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>new)</cell></row><row><cell>List</cell><cell>4.59</cell><cell>4.24</cell><cell>4.49</cell><cell>5.22</cell><cell>2.22</cell><cell>4.13</cell></row><row><cell>Cate</cell><cell>4.11</cell><cell>3.97</cell><cell>4.25</cell><cell>4.19</cell><cell>2.19</cell><cell>3.69</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,95.79,711.82,330.56,8.96"><p>Questions are listed in the Appendix I. All responses are on a 7-point Likert scale.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,94.53,74.96,64.96,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.23,94.09,178.60,9.94" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Csiro</surname></persName>
		</author>
		<ptr target="http://www.panopticsearch.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.23,112.69,406.74,9.94;10,115.23,125.29,406.71,9.94;10,115.23,138.01,236.32,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,338.43,112.69,183.54,9.94;10,115.23,125.29,67.73,9.94">Efficient and Flexible Search Using Text and Metadata</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<ptr target="http://www.ted.cmis.csiro.au/~dave/TR2000-83.ps.gz" />
	</analytic>
	<monogr>
		<title level="m" coord="10,468.49,125.29,48.11,9.94">TR2000-83</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.23,156.61,406.80,9.94;10,115.23,169.33,406.77,9.94;10,115.23,181.93,199.84,9.94" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,418.73,156.61,103.30,9.94;10,115.23,169.33,196.37,9.94">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<ptr target="http://dbpubs.stanford.edu:8090/pub/1999-66" />
		<imprint/>
		<respStmt>
			<orgName>Standford University Database Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
