<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.16,148.91,277.23,15.11">Overview of the TREC-2002 Web Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2003-04-16">April 16, 2003</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,210.87,181.39,70.30,10.48"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<email>nick.craswell@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution">CSIRO Mathematical and Information Sciences</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.82,181.39,78.85,10.48"><forename type="first">David</forename><surname>Hawking</surname></persName>
							<email>david.hawking@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution">CSIRO Mathematical and Information Sciences</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.16,148.91,277.23,15.11">Overview of the TREC-2002 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2003-04-16">April 16, 2003</date>
						</imprint>
					</monogr>
					<idno type="MD5">B90FD1CF90E931EE3CE45D2E8B689D88</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC-2002 Web Track moved away from non-Web relevance ranking and towards Webspecific tasks on a 1.25 million page crawl ".GOV". The topic distillation task involved finding pages which were relevant, but also had characteristics which would make them desirable inclusions in a distilled list of key pages. The named page task is a variant of last year's homepage finding task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC-2002 Web Track activities centred on two tasks: A Topic Distillation Task and a Named Page Finding Task. Both made use of an 18 gigabyte, 1.25 million document 2002 partial crawl of the .gov domain, distributed on CD-ROM as the .GOV collection. 2. To formulate Web-specific search tasks, which are representative of common Web search activities, leading to new evaluation methods and new effective Web retrieval algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Guidelines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">This Year's Aims</head><p>3. To conduct topic distillation experiments, in order to understand the selectivity required to generate a short top-N list, even when a very large set of on-topic documents are available.</p><p>4. To conduct named page experiments, to find if there are particular forms of ranking evidence which help us to find specific Web documents (last year's experiments found that URL type and anchor text were useful for finding homepage documents).</p><p>5. To make available the first set of reusable relevance judgments for the new .GOV test collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset</head><p>The .GOV corpus is a crawl of Web sites in the .gov domain from early 2002. That makes it 5 years newer than previous TREC Web collections, all of which were based on a 1997 Internet Archive crawl.</p><p>Although we hope that the most useful of the Web search techniques would work on 1997 crawls as well as 2002, it is also highly desirable to have a dataset representative of the current Web. Some properties of .GOV are listed in Table <ref type="table" coords="1,194.48,730.76,3.88,8.74" target="#tab_0">1</ref>. The crawl included binary and text mime types, and was stopped after 1 million HTML pages. The HTML and text, plus extracted text of other document types, gives a total of 1.25 million documents. Total data size was 35 gigabytes, which we considered too great an increase in corpus size (over WT10g), so a 100 kilobyte cutoff was applied to all documents, reducing the total size to 18 gigabytes.</p><p>The .GOV dataset is distributed by CSIRO <ref type="bibr" coords="2,287.73,381.26,9.97,8.74" target="#b2">[3]</ref>. Note that the standard distribution includes the HTML documents plus text extracted from other formats such as PDF. The original PDFs and other binary files such as images were collected and are potentially available. However, the full crawl including binaries is 67 gigabytes almost four times the size of the collection as distributed (and it would not compress as well).</p><p>Docids are 14 characters and of the form G09-04-2395783, meaning that this document is in the bundle G00/04.gz at byte offset 2395783. All .GOV documents can be located in this way via their docid. The collection was distributed on seven CDs, with an eighth containing tables of URLtoID, links, duplicates and redirects. The URLtoID table lists all valid .GOV docids and their corresponding URLs. The link table is useful for link-based ranking experiments, and a potentially more complete picture of link structure could be built in conjunction with the provided duplicate and redirect tables. These give information on link target URLs visited by the crawler but not included in .GOV because they contained duplicate content or forwarded users to another URL.</p><p>The .GOV corpus has fewer documents than WT10g, but has a much larger average document size (15k vs 7k), reflecting changes in Web authoring over the space of five years (perhaps the prevalence of navigation bars and scripting in more recent pages). Compared to WT10g, .GOV also has the strictest file-type checking of any Web collection so far, leading to very few binaries in the corpus.</p><p>We chose to crawl .gov for several reasons. It is a commercially interesting domain, meaning that important services are provided based on precisely this sort of crawl. It is also a crawl of manageable size, in that it can be distributed on CD and is within the data size limitations of most TREC systems. By contrast the crawl of a large search engine would be perhaps 30 terabytes, well beyond the bounds of manageability using current technologies (the 100 gigabyte VLC2 is still considered large relative to the storage media and systems available to researchers). Luckily, many smaller crawls such as .GOV are conceivable, which are of manageable size and of significant research and commercial interest. The .GOV crawl is also of a size which allows sufficiently complete relevance judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Topic Distillation Task</head><p>Topics: 551-600 Example: &lt;top&gt; &lt;num&gt; Number: 600 &lt;title&gt; highway safety &lt;desc&gt; Description: Find documents related to improving highway safety in the U.S. &lt;narr&gt; Narrative: Relevant documents include those related to the improvement of safety of all vehicles driven on highways, including cars, trucks, vans, and tractor trailers. Ways to reduce accidents through legislation, vehicle checks, and drivers education programs are all relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/top&gt;</head><p>The premise of the topic distillation task is that some quality, in addition to relevance, is desirable in Web search results lists. This quality has been called authority, quality, definitiveness and many other names in previous studies <ref type="bibr" coords="3,185.82,265.44,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="3,199.34,265.44,7.75,8.74" target="#b0">1,</ref><ref type="bibr" coords="3,210.09,265.44,7.01,8.74" target="#b1">2]</ref>. Assuming it exists, systems will have to find evidence which indicates its presence, and strike a balance between relevance and "quality" in search algorithms. This balancing act is analogous to the balance between newness and relevance when searching a news archive: it is desirable to return documents which are both relevant and new (if any).  In our non-TREC research, we have found some evidence that query-independent evidence can indicate desirability, based on analysis of hand-made URL lists. We sorted all .GOV URLs in reverse URL length order and link indegree order. Then we found examples of "quality URLs" in .GOV, by identifying those which are hand-listed in the Yahoo! and DMOZ Web directories. Web directories are an alternative Web search technology, where within a category hierarchy, each category has a list of URLs created by a human editor. Figure <ref type="figure" coords="3,169.93,621.61,4.98,8.74" target="#fig_0">1</ref> shows that the URL and link orderings were good predictors of hand-listing. If they predict hand-listing, then these characteristics might also be good predictors of which search results would be preferred by Web search users (who, after all, are also the audience of the Web directories).</p><p>In the topic distillation task we evaluate systems in terms of their ability to return relevant "key pages". A key page is one which the relevance assessor would find worthy of including in a short list of important URLs (the sort of choice made by Web directory editors). The "relevant key pages" found by assessors should thus be relevant and posses that special quality which makes pages worthy of inclusion in a short list. We did not go further than this in defining what makes a page list-worthy, since it has not been agreed in the research community what the definition is (quality, authority, definitiveness etc), and we did not want to bias assessments. The main measure is precision at 10. The objective in the named page finding task is to find a particular Web page in .GOV, given a query which describes it by name. For example, the query "America's Century Farms" might lead to a particular .GOV Web page describing farms that have remained in one family's hands for over 100 years. The assessment task was simply to identify any duplicate URLs for the named page, since a page can appear at more than one URL. The main measure was the reciprocal rank of the first correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Named Page Finding Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Indexing Restrictions</head><p>There were none. Participants were permitted to index all of each document or exclude certain fields as they wished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Submissions and Judgments</head><p>Runs were received from a total of 23 groups: ajou, chinese academy, city-pliers, cmu lti, csiro, cuny, dgic stokoe, fudan, glasgow, hummingbird, ibm-haifa, iit, illinois chicago, irit, kasetsart, lit singapore, neuchatel, tsinghua, umbc-cost, umelbourne, uva, waterloo, yonsei.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Topic Distillation Task</head><p>Seventy-one official runs were submitted from seventeen participating groups. The number of pages judged was 56,650 of which 1574 were judged to meet the criteria. Figure <ref type="figure" coords="4,406.44,480.45,4.98,8.74" target="#fig_5">2</ref> shows the distribution of numbers of key resources found per topic. For a few topics the number of such resources is very much higher than expected.</p><p>While pages hand-listed in Web directories tended to have short URLs and high indegree (Figure <ref type="figure" coords="4,513.92,516.32,3.88,8.74" target="#fig_0">1</ref>), key resources from this year's track did not show such tendencies as strongly (Figure <ref type="figure" coords="4,445.88,528.27,3.87,8.74" target="#fig_6">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Named Page Finding Task</head><p>Seventy official runs were submitted from eighteen participating groups.</p><p>Only one correct answer was identified for most of the 150 topics, but there were three correct answers to two topics (9 and 145) and two for 16 topics (1, 8, 14, 24, 26, 50, 51, 63, 66, 67, 68, 85, 89, 128, 138, and 146).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Distillation Task</head><p>Full official results for the Topic Distillation task are reported in Appendix 1. Results on a per-group basis are presented in Table <ref type="table" coords="4,175.31,697.52,3.88,8.74" target="#tab_1">2</ref>. Here we briefly summarize the information available about the experiments conducted by the top five groups   TsingHua University TsingHua used Okapi with stemming and Fox stoplist but no query expansion or feedback.</p><p>They explored:</p><p>1. techniques based on link structure and link text, especially the use of out-degree to find key resources; 2. the roles of different HTML fields in ranking content;</p><p>3. post-processing of retrieval results, namely a site uniting approach 4. A genetic algorithm based dynamic parameter learning approach.</p><p>They found that anchor text was useful but out-degree was not. They also found that site uniting methods which worked well on the small number of training examples improved average precision but not P@10. Parameter settings learned on past Web tasks did not improve performance this year.</p><p>City University, London It is significant that the second best performing run (pltr02wt2 from City University, London) was a straightforward content retrieval run based on Okapi BM25 (with nondefault parameter for parameter b and stemming but no relevance feedback.</p><p>Chinese Academy No details available.</p><p>IBM Haifa Query expansion via lexical affinities. Knowledge Agents and Knowledge Bases incorporating content scores, anchor text, Kleinberg Hub and Authority scores and SALSA scores. Site compression. Title filtering -eliminate documents which have no query word in their title (beneficial). Duplicate elimination based on textual similarity (harmful).</p><p>Glasgow University Experimentation focused on:</p><p>1. A probabilistic framework for combining link and content, called the Absorbing Model, based on Markov chains and applicable in either static or dynamic forms;</p><p>2. A spreading activation method (either query independent or query dependent) for detecting site entry points;</p><p>3. Anchor text.</p><p>4. A genetic algorithm based dynamic parameter learning approach.</p><p>They found that body only indexing and link analysis without anchors work well while spreading activation on sites was equivocal and query expansion and PageRank were detrimental.</p><p>IBM Haifa identified a scoring problem in that no penalty was applied to runs which included multiple duplicates or near duplicates.  <ref type="table" coords="7,230.32,533.86,3.88,8.74" target="#tab_2">3</ref>. Here we briefly summarize the information available about the experiments conducted by the top five groups TsingHua University They built a collection of surrogate documents comprising keywords, titles and incoming anchor text. Ranks obtained with these surrogates were combined with ranks from the original documents, using S = a * 1/rank1+(1-a) * 1/rank2. (Note that the original collection was divided into two sub-collections: html and non-html and the results merged using a novel procedure (see the TsingHua paper for details). The combined score outperformed the original score which in turn outperformed the surrogate score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Named Page Finding Task</head><p>CMU LTI Their basic model was a generative language model, where the language model for the document was a linear interpolation of several language models (title, in-link text, full text, meta tag text, image alt text, url text, large fonts). Using document structure in this way did improve performance over just using a simple language model.</p><p>They were unable to find useful prior probabilities for this task, either on training data created locally or on the test data. They tried in-link count, document length, document file type, and url length.</p><p>Yonsei No details available.</p><p>Glasgow University Anchor text proved to be more useful than link analysis, significantly improving results.</p><p>They found that body only indexing and link analysis without anchors work well while spreading activation on sites was equivocal and query expansion and PageRank were detrimental.</p><p>U. Neuchatel A second representation of each document in the .GOV collection was created, comprising the documents title and all its incoming anchor text. Okapi scores were computed for both representations and linearly combined αS content + (1 -α)S anchortitle (without normalisation). The best results were obtained with α = 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The .GOV corpus provided an interesting and realistic dataset for the purposes of the track. No significant problems were reported in working with it. The Named Page Finding task was an interesting variant on earlier Home Page Finding evaluations. Unsurprisingly, URL-type analyses did not bring improvement in performance. However, several leading participants reported an improvement in performance by adding anchor text and structural information to a content-only run. In 2003, it is anticipated that a mixed Home Page / Named Page task might prove interesting.</p><p>The Topic Distillation task proved difficult to explain to both participants and assessors and there was considerable disparity between the interpretations of these two groups. It is not clear what, if any, conclusions can be drawn at this stage. The task is worth repeating in 2003 but more explanatory effort is needed.</p><p>Appendix 1 -Topic Distillation runs P@10 results for all topic distillation run submitted. The codes D, A, L indicate the use of document structure (D), Anchor text (A) and Link structure (L) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,84.18,497.19,441.37,8.74;1,96.91,509.14,147.39,8.74"><head>1 .</head><label>1</label><figDesc>To begin work with a new (early 2002) crawl of an important Web domain (.gov). Past TREC Web experiments used data from 1997.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,91.64,539.35,414.26,7.86"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Query-independent properties of .GOV pages predict which will be listed in Web directories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,158.65,402.46,276.15,7.86"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Topic distillation task: Number of key resources per topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,72.00,638.08,453.54,7.86;5,72.00,649.04,85.43,7.86"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Query-independent properties which were good predictors in Figure 1 are less useful when predicting this year's key pages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,118.41,119.43,360.73,164.85"><head>Table 1 :</head><label>1</label><figDesc>Salient properties of the .GOV corpus. (Mime types as reported by the servers.)</figDesc><table coords="2,203.68,144.91,187.12,139.37"><row><cell>Number of pages</cell><cell>1,247,753</cell></row><row><cell>Number of pages by mime type:</cell><cell></cell></row><row><cell>text/html</cell><cell>1,053,110</cell></row><row><cell>application/pdf</cell><cell>131,333</cell></row><row><cell>text/plain</cell><cell>43,753</cell></row><row><cell>application/msword</cell><cell>13,842</cell></row><row><cell>application/postscript</cell><cell>5,673</cell></row><row><cell>other (containing text)</cell><cell>42</cell></row><row><cell>Average page size</cell><cell>15.2 kB</cell></row><row><cell>Number of hostnames</cell><cell>7,794</cell></row><row><cell>Total number of links</cell><cell>11,164,829</cell></row><row><cell>Number of cross-host links</cell><cell>2,470,109</cell></row><row><cell>Average cross-host links per host</cell><cell>317</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,118.93,453.55,248.83"><head>Table 2 :</head><label>2</label><figDesc>P@10 results for the best topic distillation run submitted by each participating group. The codes D, A, L indicate the use of document structure (D), Anchor text (A) and Link structure (L).</figDesc><table coords="6,108.70,155.38,376.83,212.37"><row><cell>Rank</cell><cell>P@10 Group</cell><cell cols="6">Best Run Run type D? A? L? #Runs</cell></row><row><cell>1.</cell><cell>0.2510 tsinghua</cell><cell>thutd5</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell></cell><cell>5</cell></row><row><cell>2.</cell><cell>0.2408 city-pliers</cell><cell>pltr02wt2</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>3.</cell><cell>0.2306 chinese academy</cell><cell>icttd1</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>4.</cell><cell>0.2286 ibm-haifa</cell><cell>ibmhaifapr</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell>L</cell><cell>4</cell></row><row><cell>5.</cell><cell>0.2224 glasgow</cell><cell>uog05tad</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell>L</cell><cell>2</cell></row><row><cell>6.</cell><cell>0.2163 irit</cell><cell>mercah</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>7.</cell><cell>0.1959 neuchatel</cell><cell>uninedi5</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell></cell><cell>5</cell></row><row><cell>8.</cell><cell>0.1939 fudan</cell><cell>fduwt11t1</cell><cell>realistic</cell><cell>D</cell><cell></cell><cell>L</cell><cell>3</cell></row><row><cell>9.</cell><cell>0.1939 umelbourne</cell><cell>mu525</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell><cell>5</cell></row><row><cell>10.</cell><cell>0.1755 uva</cell><cell>uamst02wtt</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell><cell>5</cell></row><row><cell>11.</cell><cell>0.1510 yonsei</cell><cell>yedi01</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell>L</cell><cell>1</cell></row><row><cell>12.</cell><cell>0.1143 umbc-cost</cell><cell>carrot2a</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>13.</cell><cell>0.1082 cuny</cell><cell>pirc2wd2</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell>L</cell><cell>2</cell></row><row><cell>14.</cell><cell>0.1041 illinois chicago</cell><cell>uic0104</cell><cell>realistic</cell><cell></cell><cell></cell><cell>L</cell><cell>2</cell></row><row><cell>15.</cell><cell>0.1000 csiro</cell><cell>csiro02td1</cell><cell>realistic</cell><cell></cell><cell></cell><cell>L</cell><cell>1</cell></row><row><cell>16.</cell><cell>0.0714 dgic stokoe</cell><cell>tdwsdtfidf</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>17.</cell><cell>0.0571 ajou</cell><cell>ajouai0210</cell><cell>realistic</cell><cell></cell><cell></cell><cell>L</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,220.79,453.55,321.80"><head>Table 3 :</head><label>3</label><figDesc>MRR results for the best named page finding run submitted by each participating group. The codes D, A, L indicate the use of document structure (D), Anchor text (A) and Link structure (L).</figDesc><table coords="7,72.00,257.25,453.55,285.35"><row><cell cols="2">Rank MRR Group</cell><cell>Run</cell><cell cols="4">Run Type D? A? L?</cell></row><row><cell>1</cell><cell>0.719 tsinghua</cell><cell>thunp3</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell></cell></row><row><cell>2</cell><cell>0.676 cmu lti</cell><cell>lmralleq</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell></cell></row><row><cell>3</cell><cell>0.671 yonsei</cell><cell>yenp01</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell>L</cell></row><row><cell>4</cell><cell>0.654 glasgow</cell><cell>uog07cta</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell></cell></row><row><cell>5</cell><cell>0.636 neuchatel</cell><cell>uninenp1</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell></cell></row><row><cell>6</cell><cell>0.626 hummingbird</cell><cell>hum02pd</cell><cell>realistic</cell><cell>D</cell><cell></cell><cell></cell></row><row><cell>7</cell><cell cols="2">0.613 chinese academy ictnp6</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell></cell></row><row><cell>8</cell><cell>0.587 iit</cell><cell>iit02b</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>0.578 lit singapore</cell><cell>litlink</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell>L</cell></row><row><cell>10</cell><cell>0.576 umelbourne</cell><cell>mu106</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell></cell></row><row><cell>11</cell><cell>0.573 csiro</cell><cell>csiro02np01</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>12</cell><cell>0.564 illinois chicago</cell><cell>uicnp03</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>13</cell><cell>0.535 waterloo</cell><cell>uwmtbw2</cell><cell>realistic</cell><cell></cell><cell>A</cell><cell>L</cell></row><row><cell>14</cell><cell>0.432 uva</cell><cell>uamst02wntla</cell><cell>realistic</cell><cell></cell><cell>A</cell><cell></cell></row><row><cell>15</cell><cell>0.418 city-pliers</cell><cell>pltr02wt9</cell><cell>realistic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>16</cell><cell>0.263 cuny</cell><cell>pirc2wnp1</cell><cell>realistic</cell><cell>D</cell><cell>A</cell><cell></cell></row><row><cell>17</cell><cell>0.132 ajou</cell><cell>ajouai0204</cell><cell>realistic</cell><cell>D</cell><cell></cell><cell></cell></row><row><cell>18</cell><cell>0.010 kasetsart</cell><cell>kuhpf0201</cell><cell>realistic</cell><cell></cell><cell>A</cell><cell></cell></row><row><cell cols="7">Full official results for the Named Page Finding task are reported in Appendix 2. Results on a per-</cell></row><row><cell cols="2">group basis are presented in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We are very grateful to <rs type="person">Charlie Clarke</rs> of the <rs type="institution">University of Waterloo</rs> for providing the crawler software used to collect the .GOV data and for supervising the crawl. We are also grateful to <rs type="institution">Ed Fox of Virginia Tech</rs> for providing the machine and network connection to support the crawl and to <rs type="person">Ian Soboroff</rs> of <rs type="institution">NIST</rs> for facilitating the crawl and also for his invaluable assistance in organising the track.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,87.50,527.38,438.04,8.74;8,87.50,539.33,242.58,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,216.50,527.38,304.31,8.74">Improved algorithms for topic distillation in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bharat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Henzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,99.95,539.33,131.06,8.74">Proceedings of ACM SIGIR&apos;98</title>
		<meeting>ACM SIGIR&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.50,559.26,438.04,8.74;8,87.50,571.21,438.05,9.02;8,87.50,583.89,18.46,8.30" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,179.64,559.26,264.46,8.74">The anatomy of a large-scale hypertextual Web search engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<ptr target="http://www7.scu.edu.au/programme/fullpapers/1921/com1921.htm" />
	</analytic>
	<monogr>
		<title level="m" coord="8,464.47,559.26,61.07,8.74;8,87.50,571.21,30.17,8.74">Proceedings of WWW7</title>
		<meeting>WWW7</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.50,603.09,338.09,9.02" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,125.81,603.09,131.16,8.74">TREC Web Tracks home page</title>
		<author>
			<persName coords=""><surname>Csiro</surname></persName>
		</author>
		<ptr target="www.ted.cmis.csiro.au/TRECWeb/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.50,623.02,438.04,8.74;8,87.50,634.98,89.11,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,181.04,623.02,232.83,8.74">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,429.04,623.02,90.01,8.74">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
