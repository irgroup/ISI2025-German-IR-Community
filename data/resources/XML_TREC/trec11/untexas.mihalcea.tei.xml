<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.92,87.54,293.06,12.91">Classifier Stacking and Voting for Text Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,252.96,119.05,97.23,10.76"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<email>rada@cs.unt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Texas Denton</orgName>
								<address>
									<postCode>76203-1366</postCode>
									<settlement>Texas</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.92,87.54,293.06,12.91">Classifier Stacking and Voting for Text Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">99F0E8C2DBF928D655D1A4457B52C6A7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarizes the approach and the results of the TextCat system participating in the Filtering track in the Text Retrieval Conference 2002. The system relies primarily on statistical methods, and was designed with the main purpose of having a backbone system in which we can further integrate semantic components, and evaluate their relative performance as compared to traditional statistical approaches. The system is therefore simple, and is based on techniques for keywords extraction, and various classifier combinations including stacking and voting. TextCat participated in the Batch and Routing tasks. In the Batch task, it achieved a score of 39.02% normalized utility, and 26.37% F-measure respectively, averaged over all topics. The averaged uninterpolated precision for our best routing submission was 14.16%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Filtering track has quite a long history in the Text Retrieval Conference (TREC) series. The goal of the track is to measure the ability of systems to classify new documents as relevant or irrelevant with respect to a given topic. While there are three different tasks organized within the Filtering track -adaptive, batch, and routing -our Text Categorization (TextCat) system participated only in the last two tasks. Few changes in the system would have probably allowed us to run TextCat on the adaptive filtering data as well; however, we decided to focus on the classification capabilities of the system, rather than on its adaptability to new incoming data. This is mainly because the purpose for building TextCat was to have a backbone text classification system, in which we can further integrate semantic modules and evaluate their relative performance as compared to the simple statistical approach. This follows up on our previous work in semantic-based Information Retrieval <ref type="bibr" coords="1,60.72,583.69,74.38,9.82" target="#b2">(Mihalcea, 2002)</ref>, where various degrees of semantic knowledge where integrated into an existing Information Retrieval system (SMART <ref type="bibr" coords="1,214.51,610.81,77.72,9.82;1,60.72,624.37,22.49,9.82" target="#b5">(Salton and Lesk, 1971)</ref>). To extend this work to the text classification problem, we needed in the first place a basic text categorization system, which could then be expanded with more sophisticated modules. Since we were not able to find such a tool (reliable, free for download, with complete source code), we started building our own text categorization system, which ultimately resulted in the UNT TextCat system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">UNT TextCat</head><p>As stated in the title, TextCat relies on combinations of simple text classifiers, which includes stacking and voting. Starting with a basic ngram-based classifier and a rule-based classifier, we generate a range of new classifiers by making simple changes in the value of their input parameters. First, stacking is done by applying the rule-based classifier on the output produced by the ngram-based classifier. Second, classifier voting is performed using various degrees of inter-classifier agreement. In turn, different voting schemes generate new classifiers.</p><p>For the Batch task, we had a total of thirteen stacked classifiers, which were then fed to the voting scheme, such that ten more combined classifiers were generated. Out of this total number of 23 classifiers, one was chosen according to its performance during cross validation runs performed on the training data. This tuning on training data was done separately for the normalized utility measure and for the F-measure, resulting in two different submissions, UNTextCatSU (run optimized for the T11SU measure), and UN-TextCatF (run optimized for the T11F measure).</p><p>For the Routing task, we used a single combination of the thirteen stacked classifiers (run UN-TextCatR), and a combination of the thirteen ngram-based classifiers, with no prior stacking (run UN-TextCatR1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data</head><p>This year, the text collection for the Filtering track consisted in the Reuters documents published during August 1996 -August 1997. To speed-up the classification process, and avoid the overhead associated with the repetition of some initial transformation procedures, there is a pre-processing phase where all documents are transformed and saved in a format suitable for the text classifiers. During this phase, words are stemmed using Porter stemmer <ref type="bibr" coords="2,211.37,231.01,57.96,9.82" target="#b4">(Porter, 1980)</ref>, and common words are eliminated based on a list of about five hundreds words that comes with the SMART system package. The output of this stage is a single large training file that includes all training documents, one document per line, each line being preceded by the document identifier. Similarly, there is one large test file that includes all test documents. Even though the pre-processing procedure was designed for the specific format of Reuters documents, we expect to be able to easily adapt it to new data formats, with a minimal number of changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Ngram-Based Classifier</head><p>The first classifier consists of a simple ngrambased classification scheme. Specifically, we have a module that generates candidate ngram keywords starting with the training files. So far, the candidate keywords consisted only of unigrams and bigrams. In future work, we plan to investigate the impact of longer ngrams on the quality and efficiency of the TextCat classifier.</p><p>Shortly, the ngram-based classifier proceeds as follows. First, we select an initial large set of ngrams from the training files, based on their frequency in the documents considered relevant for the given topic. Next, for each such ngram, we calculate several parameters, including frequency in each relevant document, ratio between relevant and irrelevant documents that can be extracted with the ngram, correlation coefficient, number of relevant documents that include the ngram. A fixed number of ngramkeywords is selected from this large list, based on their relative value with respect to some threshold values set for their parameters. Finally, if more than a certain number of keywords fulfill the minimum requirements, the highest ranked ngrams are selected. This list of ngram-keywords is then adjusted through several loops, where the number of keywords may be increased or decreased, based on the total number of test documents that are extracted. This is based on the intuition that a fixed number of keywords may not be satisfactory for all topics. An efficient classification for a certain topic may be performed with only 5 keywords, whereas another topic may need as many as 15 keywords or even more. We also set a maximum over the number of loops that may be executed, to avoid excessive running times for certain topics. Finally, once the list of ngram-keywords is selected, all documents from the test set that contain any of these keyword are classified as relevant, and all remaining documents are classified as irrelevant.</p><p>There are several parameters that may influence the number and the ranking of the ngram keywords. Consequently, the settings made for these parameters may also influence the number of documents classified as relevant or irrelevant to the given topic. In TREC 2002, TextCat included thirteen different settings, which therefore resulted in thirteen different classifiers. The list below details the various parameters that may be set for the ngram-based classifier.  <ref type="bibr" coords="3,128.43,187.45,69.67,9.82" target="#b3">(Ng et al., 1997)</ref>:</p><formula xml:id="formula_0" coords="3,94.30,220.75,200.30,35.88">¢¡ ¤£ ¦¥ §¡ ¤© ¡ ¤£ © ¡ ¤¥ ¡ ¢¡ !£ ¦¥ #" $¡ ¤£ © % &amp; '¡ ¤¥ (" $¡ ¤)© &amp; ¢¡ !£ ¦¥ 0" 1¡ ¤¥ 2 ¢¡ ¤£ 3© 0" 1¡ ¤)© % (1)</formula><p>where </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Rule-Based Classifier</head><p>The rule-based classifier used by TextCat is Ripper, an off the shelf system available from AT&amp;T <ref type="bibr" coords="3,522.16,137.65,14.91,9.82;3,310.56,151.09,45.88,9.82" target="#b1">(Cohen, 1995)</ref>. The reason for choosing Ripper as our rule-based classifier was twofold. First, it was previously shown that Ripper is an efficient classification scheme for text categorization problems <ref type="bibr" coords="3,510.05,191.77,31.85,9.82;3,310.56,205.33,74.26,9.82" target="#b0">(Cohen and Singer, 1996)</ref>. Second, Ripper handles set-valued features, and therefore we can feed the entire document as a single attribute, and let the machine learning algorithm decide upon the contribution of various keywords for the relevance classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Classifiers Stacking</head><p>The first method that we employ for classifier combination is stacking, where the rule-based classifier is applied on the output produced by the ngrambased classifier. The documents considered relevant by the ngram-based classifier are therefore the only documents seen and classified by the rule-based learner. This stacking procedure is also meant as a speed-up in the classification of large text collections, since the rule-based learner does not handle very well collections that exceed a certain size. Systems participating in the filtering task had to deal with collections of over 800,000 documents, and consequently the single use of the rule-based learner was not a feasible solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Classifiers Voting</head><p>Besides stacking, additional classifier combinations are performed through a simple voting scheme, where the number of inter-classifier agreements is collected, starting with a set of thirteen base classifiers. Basically, for each document that is considered relevant by any of these base classifiers, we count the number of votes the document receives from the remaining classifiers. Next, a minimum acceptable value is set for this vote, and only those documents that have a total vote exceeding the given threshold remain in the final classification. There are ten different threshold values considered in the TREC 2002 experiments, ranging from 0 (meaning that at least one base classifier should find a document to be relevant, for the document to be considered relevant in the final classification) to 10 (meaning that a document is classified as relevant only if at least ten of the base classifiers give a relevance vote to that particular document). These ten different threshold values resulted in ten additional classifiers. Hence, the final number of classifiers used in this task was 23, that is thirteen base classifiers, plus ten combined classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">UNT TextCat at TREC 2002</head><p>In TREC 2002, TextCat participated in the Batch and Routing. All filtering systems were evaluated based on: (1) normalized utility measure, which relates the relevant and irrelevant documents in the set of retrieved documents (4 5 " and 4 7 " ); the normalized utility is intended as a measure of the number of irrelevant documents that a user can tolerate in a given set of retrieved documents; (2) the F-measure, which is a standard measure in Information Retrieval <ref type="bibr" coords="4,60.72,327.37,100.96,9.82" target="#b6">(Van Rijsbergen, 1979)</ref>, and combines the precision and recall figures obtained for a certain topic. Precision and recall were also measured individually for each topic.</p><p>In the Batch task, TextCat achieved a score of 39.02% normalized utility, and 26.37% F-measure, averaged over all topics. The normalized precision for our best routing submission was 14.16%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Batch task</head><p>In the Batch task, thirteen base classifiers are built by varying the relative ratio between relevant and irrelevant training documents. The first classifier uses all training documents provided in the QRels judgment file, with their corresponding relevance judgments. The second classifier adds to this initial set of documents an equal share of irrelevant documents. The third, and all subsequent classifiers, double each time the number or irrelevant documents, up to the thirteen classifier, which trains on all N documents listed in the QRels file, plus an additional number of 2048*N irrelevant documents. As a rule of thumb, classifier includes all 4 documents listed in the QRels file, plus an additional set of ¡ £¢ ¥¤ §¦ 4 irrelevant documents, all of them extracted from the training collection. If not enough irrelevant documents are found in the training set, the ratio ¡ £¢ ¥¤ reflects a maximum, rather than the actual rapport between relevant and irrelevant documents. In all these classifiers, all parameters except the RATIO RELEVANT IRRELEVANT were used with their default values. New classifiers can be easily generated by changing the values of these input parameters.</p><p>Next, the rule-based classifier is combined with each of these initial classifiers through stacking. As noted earlier, ten additional classifiers are built by combining the thirteen base classifiers using a voting scheme.</p><p>To select one classifier out of the total set of 23 different classifiers, cross validation runs were performed on the initial training set, with the ratio between training and test documents set to 90%-10%. There were two TextCat submissions in the Batch task, one optimized for the utility measure -UN-TextCatSU -where the classifier leading to the highest normalized utility score during cross validation runs is the one that is selected, and one optimized for the F-measure -UNTextCatF.</p><p>Table <ref type="table" coords="4,352.74,353.05,5.39,9.82">1</ref> lists the thirteen base classifiers and the ten combined classifiers, with their corresponding average utility and F-measure, as well as the averaged precision and recall. Moreover, the last two columns list the number of times each classifier was selected in the cross validation phase, for each run (optimized for normalized utility or for F-measure).</p><p>As seen in the last two columns in Table <ref type="table" coords="4,534.01,448.09,4.07,9.82">1</ref>, the classifier selection is quite uniformly distributed among the 23 different classifiers; however, base classifiers one and thirteen seem to be selected more often, as compared with the selection frequency for the other classifiers. In terms of performance, base classifier thirteen has an utility measure of 33.87%, which is 5% smaller than the score for the final classifier; in real world applications, this relatively small difference may not fully justify the additional running time brought by the cross validation phase, and therefore in some applications this could be the only classifier employed for the filtering task. The maximum F-measure that can be achieved with a single classifier (base or combined, with no selection) is 18.28%.</p><p>Figure <ref type="figure" coords="4,357.05,651.49,4.07,9.82">3</ref>.1. plots the scores obtained by all classifiers for the four different measures. For the base classifiers, a steady growing tendency is observed for the utility measure, which suggests that the larger the Table <ref type="table" coords="5,173.71,416.65,4.19,9.82">1</ref>: Individual results obtained with each base/combined classifier number of irrelevant documents, the better. The recall is relatively constant across different base classifiers, with the only exception being classifier one, which has a significantly higher recall. Precision and F-measure achieve a maximum for base classifier eleven, followed by a decrease in classifiers twelve and thirteen. In the case of combined classifiers, there is a peak in utility, F-measure, and precision, for combined classifiers four through six, followed again by a sharp decrease. As expected, the highest recall is obtained with combined classifier one. What this figure suggests is that new system settings may eventually lead to even higher scores for various measures (e.g. larger number of irrelevant documents for higher utility score, larger number of base classifiers in the voting scheme for higher recall, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Routing task</head><p>In the Routing task, two different TextCat runs were submitted. The first submission, UNTextCatR, consists in the top 1000 documents returned by the combined stacked classifier with a minimum vote of one (i.e. combined classifier one). The second submission, UNTextCatR1, consists in the top 1000 documents returned by a combined classifier, again with a minimum vote of one, but this time with no prior stacking (that is, only the ngram-based classifier is employed during this run). The average normalized precision was 14.16% for UNTextCatR, and 8.15% for UNTextCatR1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>This paper has described the approach and the results obtained with TextCat -a simple text filtering system that relies on various classifier combination schemes. In the Batch task, the average utility score achieved with TextCat was 39.02%, and the average F-measure was 26.37%. In the Routing task, the two TextCat submissions achieved an average uninterpolated precision of 14.16% and 8.15% respectively. As a next step, we plan to integrate semantic modules into TextCat, and evaluate their relative performance as compared to the simple statistical-based approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,121.44,287.77,359.20,9.82"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Utility, F-measure, Precision and Recall obtained with various classifiers</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.97,454.69,71.63,10.76;6,60.72,473.29,231.03,9.82;6,70.56,486.85,221.53,9.82;6,70.56,500.41,221.54,9.82;6,70.56,513.97,221.56,9.82;6,70.56,527.53,221.76,9.82;6,70.56,540.97,19.99,9.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,216.70,473.29,75.05,9.82;6,70.56,486.85,177.63,9.82">Context-sensitive learning methods for text categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,272.69,486.85,19.40,9.82;6,70.56,500.41,221.54,9.82;6,70.56,513.97,221.56,9.82;6,70.56,527.53,92.39,9.82">Proceedings of the 19th Annual International ACM SI-GIR, Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19th Annual International ACM SI-GIR, Conference on Research and Development in Information Retrieval<address><addrLine>Zurich, CH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-07">1996. July</date>
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,60.72,555.25,231.38,9.82;6,70.56,568.81,221.43,9.82;6,70.56,582.37,164.18,9.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,146.74,555.25,124.72,9.82">Fast effective rule induction</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,70.56,568.81,216.72,9.82">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Tahoe City, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-07">1995. July</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,60.72,596.65,231.39,9.82;6,70.56,610.09,221.54,9.82;6,70.56,623.65,221.51,9.82;6,70.56,637.21,151.45,9.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,156.67,596.65,135.45,9.82;6,70.56,610.09,221.54,9.82;6,70.56,623.65,26.07,9.82">Going beyond explicit knowledge for improved semantic based information retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,105.51,623.65,186.56,9.82;6,70.56,637.21,68.42,9.82">International Journal on Tools with Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002-12">2002. December</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,60.72,651.49,231.31,9.82;6,70.56,665.05,221.50,9.82;6,70.56,678.61,221.51,9.82;6,70.56,692.17,221.31,9.82;6,320.40,333.01,221.68,9.82;6,320.40,346.57,131.92,9.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,259.58,651.49,32.44,9.82;6,70.56,665.05,221.50,9.82;6,70.56,678.61,120.02,9.82">Feature selection, perceptron learning, and a usability case study for text categorization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,211.12,678.61,80.95,9.82;6,70.56,692.17,221.31,9.82;6,320.40,333.01,221.68,9.82;6,320.40,346.57,26.91,9.82">Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">1997. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,310.56,360.61,231.13,9.82;6,320.40,374.17,110.88,9.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,392.13,360.61,145.46,9.82">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,320.40,374.17,36.04,9.82">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,310.56,388.33,231.13,9.82;6,320.40,401.89,221.31,9.82" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,450.45,388.33,91.24,9.82;6,320.40,401.89,141.16,9.82">Computer evaluation of indexing and text processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Lesk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="143" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,310.56,429.49,231.36,9.82;6,320.40,443.05,36.64,9.82;6,373.00,443.05,169.10,9.82;6,320.40,456.61,198.48,9.82" xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<ptr target="http://www.dcs.gla.ac.uk/Keith/Preface.html" />
	</analytic>
	<monogr>
		<title level="j" coord="6,442.91,429.49,99.01,9.82;6,320.40,443.05,36.64,9.82;6,373.00,443.05,54.59,9.82">Information Retrieval. London: Butterworths</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
