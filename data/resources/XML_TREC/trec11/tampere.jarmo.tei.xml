<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,119.52,172.49,370.79,18.60;1,231.36,197.33,147.34,18.60">Example Based Text Matching Methodology for Routing Tasks</title>
				<funder>
					<orgName type="full">TEKES</orgName>
				</funder>
				<funder ref="#_vggHefV">
					<orgName type="full">National Technology Agency of Finland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.60,237.52,46.00,12.91"><forename type="first">Ari</forename><surname>Visa</surname></persName>
							<email>ari.visa@cs.tut.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postBox>P.O. Box 553</postBox>
									<postCode>FIN-33101</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.88,237.52,89.20,12.91"><forename type="first">Jarmo</forename><surname>Toivonen</surname></persName>
							<email>jarmo.toivonen@cs.tut.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postBox>P.O. Box 553</postBox>
									<postCode>FIN-33101</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.87,237.52,78.44,12.91"><forename type="first">Tomi</forename><surname>Vesanen</surname></persName>
							<email>tomi.vesanen@cs.tut.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postBox>P.O. Box 553</postBox>
									<postCode>FIN-33101</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.22,237.52,85.90,12.91"><forename type="first">Jarno</forename><surname>Mäkinen</surname></persName>
							<email>jarno.makinen¡@cs.tut.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postBox>P.O. Box 553</postBox>
									<postCode>FIN-33101</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.40,345.16,72.89,12.91"><forename type="first">Barbro</forename><surname>Back</surname></persName>
							<email>barbro.back@abo.fi</email>
							<affiliation key="aff1">
								<orgName type="institution">Åbo Akademi University</orgName>
								<address>
									<addrLine>Lemminkäisenkatu 14 A</addrLine>
									<postCode>FIN-20520</postCode>
									<settlement>Turku</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.72,434.80,106.32,12.91"><forename type="first">Hannu</forename><surname>Vanharanta</surname></persName>
							<email>hannu.vanharanta@pori.tut.fi</email>
							<affiliation key="aff2">
								<orgName type="department">Pori School of Technology and Economics</orgName>
								<address>
									<postBox>P.O. Box 300</postBox>
									<postCode>FIN-28101</postCode>
									<settlement>Pori</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,119.52,172.49,370.79,18.60;1,231.36,197.33,147.34,18.60">Example Based Text Matching Methodology for Routing Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5AE9681F6B421AFF4C2CB38398408459</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present two variations of a prototype based text matching methodology used in the Routing Sub-Task of TREC 2002 Filtering Track. The methodology examines text on the word level. It is based on word coding and examines the distributions of these codes using document histograms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A common approach to topic detection and tracking is the usage of keywords, especially, in context of Dewey Decimal Classification <ref type="bibr" coords="2,369.41,170.87,12.94,10.76" target="#b1">[2,</ref><ref type="bibr" coords="2,385.58,170.87,9.95,10.76" target="#b0">1]</ref> that is used in United States to classify books. The approach is based on assumption that keywords given by authors or indexers characterize the text well. This may be true, but then one neglects the accuracy. There are also many automatic indexing approaches. A more accurate method is to use all the words of a document and the frequency distribution of words, but the comparison of frequency distributions is a complicated task. Some theories say that the rare words in the word frequency histograms distinguish documents <ref type="bibr" coords="2,222.34,271.91,12.67,10.76" target="#b5">[5]</ref>. Traditionally, information retrieval has roughly been based on a fixed list of index terms <ref type="bibr" coords="2,292.94,286.43,12.94,10.76" target="#b5">[5,</ref><ref type="bibr" coords="2,310.08,286.43,8.62,10.76" target="#b3">3]</ref>, or vector space models <ref type="bibr" coords="2,445.27,286.43,12.94,10.76" target="#b9">[9,</ref><ref type="bibr" coords="2,462.41,286.43,8.62,10.76" target="#b8">8]</ref>. The latter ones miss the information of co-occurrences of words. There are techniques that are capable of considering the co-occurrences of words, as latent semantic analysis <ref type="bibr" coords="2,152.35,329.75,13.90,10.76" target="#b6">[6]</ref> but they are computationally heavy.</p><p>Commonly in filtering, documents are preprocessed with tokenizers, stemmers and stopword lists. Using these methods the processing of the documents become more simple for document classification methods. Next step is to construct feature vectors for documents. The value of the feature is usually based on its significance in the document. Traditionally this is done by using term frequencies and inverse document frequencies. Last year results of TREC 2001 filtering track show that using Support Vector Machine (SVM) for classification can give good results for the routing tasks <ref type="bibr" coords="2,192.75,445.31,12.94,10.76" target="#b7">[7,</ref><ref type="bibr" coords="2,208.68,445.31,8.62,10.76" target="#b4">4]</ref>.</p><p>In this paper, we present our methodology briefly and concentrate on tests of content-based topic classification, which is highly attractive in text mining. The evolution of the methodology has been earlier discussed in several publications <ref type="bibr" coords="2,110.88,503.15,18.93,10.76" target="#b10">[10,</ref><ref type="bibr" coords="2,133.17,503.15,14.97,10.76" target="#b12">12,</ref><ref type="bibr" coords="2,151.61,503.15,14.19,10.76" target="#b11">11]</ref>. In the second chapter the applied methodology is described. In the third chapter the experiment with the Reuters database and execution times are described. Finally, the methodology and the results are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The methodology used in our runs examines now the documents on the word level. The runs were designed so that the basic principles were kept the same. On the detailed level variation in the methods was added in order to test the robustness of the basic ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Filtering</head><p>The original text was first preprocessed, extra spaces and carriage returns were omitted, and single words were separated with single spaces. With the Reuters database, the preprocessing included selecting the allowed XML fields and removal of the XML tags. For the Visa1T11 run a stopword list was created. Words which were common to the most of the topics where chosen into the stopword list. If the word occurred at least in 75 different topic it was chosen to the list. These words were regarded meaningless to the topic identification. For the Visa2T11 run the text was stemmed with the Porter stemmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word quantization in Visa1T11</head><p>The filtered text was translated into a suitable form for encoding purposes. The encoding of words is a wide subject and there are several approaches for doing it. The word can be recognized and replaced with a code. This approach is sensitive to new words. The succeeding words can be replaced with a code. This method is language sensitive. Each word can be analyzed character by character and based on the characters a key entry to a code table is calculated. This approach is sensitive to capital letters and conjugation if the code table is not arranged in a special way.</p><p>The last alternative was selected, because it is accurate and suitable for statistical analysis. A word was transformed into a number in the following manner:</p><formula xml:id="formula_0" coords="3,264.20,445.45,235.10,35.46">¡ £¢ ¤ ¦¥ ¨ § © ¤ ¦¥<label>(1)</label></formula><p>where is the length of the character string (the word), is the ASCII value of a character within a word , and is a constant.</p><p>Example: word is "c a t".</p><p>¡ £¢ "! $# &amp;% (' )' 10 2 43 65 "! $# % 7' )' 10 28 ¦3 65 9! $# % 7' )' 10 A@ B3</p><p>The encoding algorithm produces a different number for each different word, only the same word can have an equal number. After each word has been converted to a code number, we consider the distribution of the code numbers of the words. The representation of word coded numbers was floating point number. Floating point numbers in our system use a radix of two. Mantissa can have values from C ED GF IH GP RQ )C . The representation of floating point number:</p><p>S 8 UT V@ XW `Y aY b8 c ed gf ih Xp rq ts uf is Rv </p><formula xml:id="formula_2" coords="4,182.00,258.15,317.30,70.25">£¢ ¥¤ §¦ ©Ä8 ¦Y Y T S ¤ ¢ T "5 0 S a3 T ¢ 0 ¡ § ! s Rv " #" %$ D GF IH 3 ed S ¢ ¡ f 2h Xp rq ts Rf 2s 4v ¡ P (4)</formula><p>where T is the mantissa class number of the word and S is the exponent class number of the word.</p><p>is the quantization accuracy of the mantissa, ¡ is the quantization step of the exponent and ¡ is the word coded to floating point number with formula 1.</p><p>Following example shows how the word coding and word class number generation is done to the word "trec". First the word is converted with word coding formula 1 to a number. Number is represented in floating point format where the radix is two. With formula 4 the word number is converted to a word class number, now is 35000, ¡ is 24, and is 256.</p><formula xml:id="formula_3" coords="4,116.60,473.35,382.70,117.20">¡ ¢ &amp; ! # % (' )' 0 @ B3 65 ! # % (' ¦' 10 ¤ 3 65 § "! $# &amp;% (' )' 10 3 65 ! # % (' ¦' 10 i R3 ¢ D F (' D )' 10 §2 Q uD )' D Q 32 42 1' )5 d " ed &amp; § £¢ ¥¤ §¦ ©Ä8 ¦Y Y T S ¤ ¢ 0 ¡ ¥ 6 s Rv " 7" 8$ D F IH 3 d ¥ 5 0 ¡ f 2h Xp rq ts uf is Rv @9 ¡ 3 ¢ d 4A 5 A Q 5 0 #B H D D D Q 3 ¢ 5 B 5 A Q (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word quantization in Visa2T11</head><p>In the Visa2T11 run the word coding is a variation of the Visa1T11 word coding. Now, the alphabet of the training data is first determined from filtered and Porter stemmed training documents. The letters are put into order of their frequencies in the training data. The most frequent letter gets letter code 1, the second 2, and so on. If the letter does not appear in the training documents, it is given letter code 0. These letter codes are now used in formula 1 to replace the ASCII values.</p><p>Next, all the words of the training data are converted to word codes and their frequencies are counted. The word-frequency list is sorted according to the word code number. The word codes are classified to % classes using a simple classifi- cation scheme. The biggest gap between two succeeding word codes is first found and a class boundary is put between them. Then the sum of frequencies of words in the two new classes are counted. The class with most words is divided into two classes where the gap between two succeeding word codes is the biggest. This method is repeated until there are % classes. The class boundary information and the word codes are used in creating class numbers for the words of the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Test document to histogram</head><p>When examining a single test document, we create a histogram of the word code numbers of the document. The filtered text from a test document is encoded word by word. Each word number is quantized using the word quantization method of the run. The quantization value is determined, an accumulator corresponding to the value is increased, and thus a word histogram ! ¡ is created. The histogram ! ¢ is finally normalized by the length of the histogram vector. The process of converting a document to a histogram is illustrated in Fig. <ref type="figure" coords="5,410.44,433.07,4.49,10.76" target="#fig_1">1</ref>. The histogram contains information about the words of the document in a numerical form. This histogram is used in the TREC Routing process to find the best topic for each test document. The diffence or distance between a single test document histogram and the histogram representing the topic can be calculated using different metrics. Among the most simple and effective metrics there are the Euclidean distance and the cosine distance.</p><p>With the histograms derived from all the documents in the test database we can compare and analyze the text of the single documents on the word level against the relevant texts of each topic. Note, that it is not necessary to have any prior knowledge of the actual text documents to use these methods. No linguistic methods, other than the Porter stemming, are used in the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test document</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filtering, Stemming</head><p>Filtered text </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Codes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class numbers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word coding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Histogram</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Histogram creation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs with Reuters database</head><p>All the relevant documents to a certain topic from the training data were concatenated to one topic document. This document consists of all given relevant text documents classified to that topic. This document was used to define the topic. The information of irrelevant documents to the topic were not used in runs. Every topic document was converted to a normalized topic histogram. All test documents were also transformed to individual histograms and normalized to vector length one. In the two runs we used the methods described in sections 2.2 and 2.3. Every test document histogram was compared with every topic histogram. The distance metric used in run Visa1T11 was the Euclidean distance. In run Visa2T11 the used distance metric was cosine distance. A topic best-match file was created for each topic. Each test document's four best matching (smallest distance) topics were determined. For these four topics, the ID number of the test document and its distance to the topic were put in the best-match file. From these files the top 1000 documents with the closest distance to the topic were selected for the result file. Our methods gave results which where close to the average level of all participating methods. Visa2T11 gave slightly better results than Visa1T11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Execution times</head><p>The applied methodology is very fast even with a database as large as the Reuters database. In table <ref type="table" coords="7,196.93,245.27,5.99,10.76" target="#tab_0">1</ref> we present the execution times we calculated for the two runs. Making histograms execution time consists of creating the word histograms for the test documents. The comparing execution times are the times that it took to compare the test histograms with the topic histograms and to find the four closest topics for each test histogram. The computer used in the experiments was a PC with a Intel R 550 MHz Pentium R III processor and 128 Mb of memory. The operating system was Slackware Linux 7.0.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion on results</head><p>There were some general difficulties when using the methodology on the Reuters database. The selection of documents for the given training set turned out to be disadvantageous. Firstly, it seemed that the set was too unevenly distributed in topics for our methodology. When some topics have under ten relevant documents and some hundreds, statistical methods are in trouble. There is not enough information in just few short relevant documents for this type of methods to be successful. Uneven division in topics also lead to give more weight to topics that have more relevant documents.</p><p>Secondly, because the training set was from a period of two months, the vocabulary in the relevant documents does not vary enough. The type of methodol-ogy we used requires a good set of representative word samples from the whole database. The training set vocabulary was restricted in the sense of yearly cycle, to two months in autumn of 1996. This type of difficulties are, on the other hand, very common in real life tasks. Also, we had difficulties with the topics 151-200. Our methodology was not doing well in finding relevant test documents for these topics. This was perhaps partly due to our decision of emphasizing accuracy more than generalization. It may also be due to the nature of the artificial topic construction process.</p><p>Our runs were designed so that only a basic form of the methodology was used. The methods used are very fast and it seems that we are improving with the accuracy of the methodology. Visa2T11 had 10000 different classes for the words whereas Visa1T11 had about 6000. There was no training for the classification of words in Visa1T11. Because of smaller number of word classes Visa1T11 had one hour faster comparing time. The drawback was slightly poorer results. One interesting issue in advancing even more is how to use the information of the non-relevant documents for a topic to improve the process. Non-relevant documents seemed to be special cases of relevant documents topics. Our methodology can notll use the information of non-relevant document, because only few words can make distinction between relevant and non-relevant document. In future, this could maybe be achieved by giving negative weight to those kind of words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,485.40,664.07,13.90,10.76;4,110.88,129.59,388.11,10.76;4,110.88,143.99,388.10,10.76;4,110.88,158.51,388.11,10.76;4,110.88,172.91,388.10,10.76;4,125.28,187.31,247.03,10.76;4,376.50,182.30,18.58,17.60;4,392.47,187.31,106.68,10.76;4,110.88,201.83,388.10,10.76;4,110.88,216.23,388.12,10.76;4,110.88,230.75,388.13,10.76"><head>( 3 )</head><label>3</label><figDesc>Our word coding gives only positive numbers so sign is always positive. The mantissa has information of the beginning of the word and the exponent has information about the length of the word. The quantization of the words uses the values of the mantissa and the exponent. The range of the mantissa is divided to equal size classes. The exponent is divided to size ¡ classes. The mantissa class number and the exponent class number are used in the calculation of the word class number. Possible number of the mantissa classes of the word varies from 1 to . The actual word class number is calculated in the following manner:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,171.96,434.39,266.20,10.76"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Process of converting document to histogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,154.08,337.55,295.84,67.88"><head>Table 1 :</head><label>1</label><figDesc>Execution times rounded up to the nearest hour.</figDesc><table coords="7,154.08,365.75,295.84,39.68"><row><cell></cell><cell cols="3">Making histograms Comparing Altogether</cell></row><row><cell>Visa1T11</cell><cell>1 h</cell><cell>6 h</cell><cell>7 h</cell></row><row><cell>Visa2T11</cell><cell>6 h</cell><cell>7 h</cell><cell>13 h</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This research is supported by <rs type="funder">TEKES</rs>, the <rs type="funder">National Technology Agency of Finland</rs> (grant number <rs type="grantNumber">40943/99</rs>). The support is gratefully acknowledged.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vggHefV">
					<idno type="grant-number">40943/99</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,136.65,471.47,361.92,9.82;8,136.68,485.03,362.05,9.82;8,136.68,498.59,51.50,9.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,191.52,471.47,307.05,9.82;8,136.68,485.03,142.27,9.82">A Classification and subject index for cataloguing and arranging the books and pamphlets of a library</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dewey</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1876</biblScope>
			<pubPlace>Case, Lockwood &amp; Brainard Co., Amherst, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,136.65,511.67,363.96,9.82;8,136.68,525.23,361.78,9.82;8,136.68,538.79,18.92,9.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,187.56,511.67,308.52,9.82">Catalogs and Cataloguing: A Decimal Classification and Subject Index</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dewey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,148.54,525.23,294.65,9.82">U.S. Bureau of Education Special Report on Public Libraries Part I</title>
		<imprint>
			<biblScope unit="page" from="623" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,158.36,538.79,180.12,9.82" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">S G P O</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">1876</biblScope>
			<pubPlace>Washington DC, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,136.65,551.99,361.82,9.82;8,136.68,565.55,361.76,9.82;8,136.68,578.99,219.10,9.82" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,198.93,551.99,299.54,9.82;8,136.68,565.55,193.54,9.82">Automatic indexing: an approach using an index term corpus and combining linguistic and statistical methods</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lahtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Finland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of General Linguistics, University of Helsinki</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="8,136.65,592.19,361.85,9.82;8,136.68,605.75,361.92,9.82;8,136.68,619.31,361.77,9.82;8,136.68,632.87,361.94,9.82;8,136.68,646.43,295.90,9.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,203.38,592.19,295.12,9.82;8,136.68,605.75,61.21,9.82">Applying Support Vector Machines to the TREC-2001 Batch and Routing Tasks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,388.79,605.75,109.81,9.82;8,136.68,619.31,329.15,9.82">Proceedings of the Tenth Text REtrieval Conference (TREC 2001), NIST Special Publication 500-250</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text REtrieval Conference (TREC 2001), NIST Special Publication 500-250<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">November 13-16 2001</date>
			<biblScope unit="page" from="286" to="292" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,136.65,130.31,364.97,9.82;9,136.68,143.87,255.91,9.82" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,283.69,130.31,217.93,9.82;9,136.68,143.87,30.16,9.82">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,136.65,156.95,363.96,9.82;9,136.68,170.51,260.60,9.82" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,280.49,156.95,179.43,9.82">A conceptual framework for text filtering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</author>
		<idno>CS-TR3643</idno>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,136.65,183.71,362.00,9.82;9,136.68,197.27,361.90,9.82;9,136.68,210.83,361.66,9.82;9,136.68,224.39,361.81,9.82;9,136.68,237.95,202.48,9.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,288.05,183.71,184.05,9.82">The TREC 2001 Filtering Track Report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,300.63,197.27,197.95,9.82;9,136.68,210.83,250.72,9.82">Proceedings of the Tenth Text REtrieval Conference (TREC 2001), NIST Special Publication 500-250</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text REtrieval Conference (TREC 2001), NIST Special Publication 500-250<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">November 13-16 2001</date>
			<biblScope unit="page" from="26" to="37" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,136.65,251.03,270.86,9.82" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,184.31,251.03,113.25,9.82">Automatic Text Processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,136.65,264.23,361.70,9.82;9,136.68,277.79,231.73,9.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,295.24,264.23,198.62,9.82">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,136.68,277.79,122.85,9.82">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,136.64,290.87,361.95,9.82;9,136.68,304.43,361.89,9.82;9,136.68,317.99,361.95,9.82;9,136.68,331.55,363.82,9.82;9,136.68,345.11,337.09,9.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,440.04,290.87,58.55,9.82;9,136.68,304.43,199.13,9.82">Validation of Text Clustering Based on Document Contents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vesanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vanharanta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.19,304.43,65.38,9.82;9,136.68,317.99,361.95,9.82;9,136.68,331.55,363.82,9.82;9,136.68,345.11,23.25,9.82">Proceedings of MLDM 2001, the Second International Workshop on Machine Learning and Data Mining in Pattern Recognition, number 2123 in Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Perner</surname></persName>
		</editor>
		<meeting>MLDM 2001, the Second International Workshop on Machine Learning and Data Mining in Pattern Recognition, number 2123 in Lecture Notes in Artificial Intelligence<address><addrLine>Leipzig, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">July 25-27 2001</date>
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,136.64,358.19,364.13,9.82;9,136.68,371.75,364.12,9.82;9,136.68,385.31,242.97,9.82" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,375.35,358.19,125.42,9.82;9,136.68,371.75,304.41,9.82">Contents Matching Defined by Prototypes -Methodology Verification with Books of the Bible</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vanharanta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,454.58,371.75,46.22,9.82;9,136.68,385.31,146.38,9.82">Journal of Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="87" to="100" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,136.64,398.51,365.00,9.82;9,136.68,412.07,361.93,9.82;9,136.68,425.63,361.65,9.82;9,136.68,439.19,361.67,9.82;9,136.68,452.75,307.64,9.82" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,373.14,398.51,128.50,9.82;9,136.68,412.07,94.31,9.82">Tampere University of Technology at TREC 2001</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vesanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mäkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,417.07,412.07,81.55,9.82;9,136.68,425.63,356.81,9.82">Proceedings of the Tenth Text REtrieval Conference (TREC 2001), NIST Special Publication 500-250</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text REtrieval Conference (TREC 2001), NIST Special Publication 500-250<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">November 13-16 2001</date>
			<biblScope unit="page" from="495" to="501" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
