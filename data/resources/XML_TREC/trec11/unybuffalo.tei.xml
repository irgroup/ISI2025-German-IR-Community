<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.60,114.55,328.63,15.68">UB at TREC-11: Batch and Adaptive Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,190.92,148.42,60.68,10.99"><forename type="first">M</forename><surname>Srikanth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering State</orgName>
								<orgName type="institution">University of New York at Buffalo</orgName>
								<address>
									<postCode>14228-2567</postCode>
									<settlement>Amherst</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.56,148.42,33.34,10.99"><forename type="first">X</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering State</orgName>
								<orgName type="institution">University of New York at Buffalo</orgName>
								<address>
									<postCode>14228-2567</postCode>
									<settlement>Amherst</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.87,148.42,50.13,10.99"><forename type="first">R</forename><surname>Srihari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering State</orgName>
								<orgName type="institution">University of New York at Buffalo</orgName>
								<address>
									<postCode>14228-2567</postCode>
									<settlement>Amherst</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.60,114.55,328.63,15.68">UB at TREC-11: Batch and Adaptive Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AA1722E42C72FFA26AD2C646EE30DD34</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first time we participated in TREC filtering track. We submitted four runs: two for adaptive filtering, and two for batching filtering. And these runs come from two separate efforts with very different approaches. One effort treats the filtering problems as standard text categorization problems and solves them using Support Vector Machines (SVM). The second effort is a Language Modeling approach to information filtering. Among other things we wanted to use filtering tasks as large scale test cases for two separate frameworks we have been working on for information retrieval. Significant time was spent on putting the components together and limited time on pre-submission performance evaluation.</p><p>2 Weighted Margin SVM for Information Filtering</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SVM in text categorization</head><p>The standard text categorization problem can be stated as following: given a set of category-labeled documents, the goal is to classify a new document into the predefined categories. Typically each document can belong to multiple categories or no category at all. This is a supervised machine learning problem. And further more, when the categories form a flat structure, each category can be treated as a separated dichotomy problem.</p><p>SVM are based on Structural Risk Minimization (SRM) principle from statistical learning theory <ref type="bibr" coords="1,92.73,520.78,10.86,10.91" target="#b6">[7]</ref>. In contrast to the Empirical Risk Minimization (ERM) principle which try to find a hypothesis h from a structural complexity fixed hypothesis space, H, that minimizes the training error, the SRM try to find a hypothesis h where the true error of the classifier is minimized. To achieve this, SRM usually tries to shrink the complexity of the hypothesis space H while maintaining a fixed training error. Compare to ERM, SRM is more suited when the training data set is limited.</p><p>SVM, the simplest linear form of SRM, is nothing but a maximum margin linear classifier. Given an example (x i , y i ), and decision hyperplane (w, b), the (functional) margin of example with respect to hyperplane is defined as</p><formula xml:id="formula_0" coords="1,251.64,648.34,288.15,11.41">γ i = y i (&lt; w • x i &gt; +b).<label>(1)</label></formula><p>Note γ i implies correct classification of (x i , y i ) if γ i &gt; 0. The (functional) margin of training set S ( with l examples ) with respect to decision hyperplane (w, b) is defined as</p><formula xml:id="formula_1" coords="1,240.72,700.30,299.07,15.67">γ = min 0≤i&lt;l y i (&lt; w • x i &gt; +b),<label>(2)</label></formula><p>Geometric margin is the functional margin derived by w . The maximum margin hyperplane given training set S is thus defined as the hyperplane with respect to which the training set has maximum geometric margin.</p><p>There are two major advantages of SVM. First the learning ability of SVM is independent of the dimensionality of the feature space thus immune from the so-called curse of dimensionality. SVM learning process typically focuses on these hard to classify patterns automatically and thus can ignore the extra noise introduced by each additional dimension. Thus by using SVM, the usually computational expensive feature selection steps are not needed. Second, with so-called kernel tricks, SVM can be used to learn different discriminant functions by using different kernel functions. So it can learn a linear classifier, polynomial classifier, or radial basis function with just one line change in your source code.</p><p>SVM has been used for text categorization <ref type="bibr" coords="2,308.38,223.78,11.37,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,325.14,223.78,8.50,10.91" target="#b1">2,</ref><ref type="bibr" coords="2,339.04,223.78,8.38,10.91" target="#b4">5]</ref> and shown to outperform all classical learning methods including neural networks, linear discriminant function and KNN. And this claim is further verified by Y. Yang in her well-cited comparison papers <ref type="bibr" coords="2,383.28,250.90,11.49,10.91" target="#b8">[9,</ref><ref type="bibr" coords="2,398.01,250.90,7.58,10.91" target="#b7">8]</ref>. The reason for its superior performance can be best explained by its compatibility to text categorization problems: the typical document representation method like term frequency and inverted document frequency (TFIDF) weighted vectors results in huge and sparse vector for each document and most text categorization problems are linearly separable (as suggested by the fact that all ohsumed categories and most Reuters categories are linearly separable).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Filtering as a Text Categorization Problem</head><p>While last year batch filtering track can be easily cast into a text categorization problem, it is not possible this year. For one, we don't have enough training examples available for most all the topics, and it is even worse in adaptive filtering track where we have only three positive examples for each topic. Second, the rich information contained in the topic description/narrative that help to shape topic boundary is not readily available for any statistical based machine learning methods. And actually it is our guess that the ability of approaches to make use of the topic description/narrative will differentiate their performance. In the final result, the fact that people can do much better in first 50 categories than in the second 50 categories can be considered as a manifestation of our guess. Since the quality of topic description/narrative is much better in first 50 categories is much better than these of the second 50 categories.</p><p>SVM is the winner of the last year batch filtering track. And it is considered as one of major component of our package, to really test out our SVM implementation, we use it to handle the filtering track problem. To do that, there are two problem we have to deal with. First, we have to turn the information stored in the topic description/narratives into some usable information for our SVM learner. Since the only information that SVM learner can make use of is the examples, we have to device a way to generate pseudo examples using topic description/narrative. Second, how our SVM learner make use of these pseudo examples?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generating Pseudo Examples</head><p>The way we generate pseudo examples from the topic description and narrative is very simple. And there are two different steps. First, we use the description/narrative get the top 30 closest document in TFIDF sense, and label them as probable positive example. Note that they definitely can't be considered as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Weighted Margin SVM</head><p>With newly generated probable examples, the question now is how to use them in SVM learning. To be able to use the training set where label is associated with an observation weight, the SVM training and classification algorithms has been modified to handle these additional information available to the learner: and the result of such modification is the Weighted Margin SVM (WMSVM) machine.</p><p>The difficulty a classical SVM classifier faces when presented with weighted observation can be best illustrated by the figure <ref type="figure" coords="3,211.29,466.06,4.19,10.91" target="#fig_0">1</ref>. We have four pieces of labeled data: two positive and two negative. The size of the circle represents the label reliability. The dashed line represents the hyperplane found by SVM, and the solid line the hyperplane by the Weighted Margin SVM.</p><p>In the results submitted for the filtering track, we were using a weaker version of the WMSVM implementation that can handle the weighted soft margin. Since then, we have a stronger WMSVM implementation that can handle both hard margin and soft margin correctly, but we are still working the experiments with this strong version on filtering track. But even with the weaker version WMSVM, along with the simple pseudo example generation, we are able to apply SVM learning on a data set which for some cases, has only a few positive examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language Modeling Approach</head><p>A model-based approach to information filtering was explored for the second set of submissions to TREC 2002 batch and adaptive filtering tasks. Language models are associated with both documents and queries. The initial query model was generated using the topic description. We viewed the training documents for a given query as relevance feedback documents. A new query model is estimated based on the initial query model and the language models estimated for the feedback documents. This method was used for both batch and adaptive filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Modeling and Information Retrieval</head><p>Statistical Language Modeling (SLM) has been used in many Natural Language Processing (NLP) tasks including Speech Recognition, Machine Learning and Information Extraction. Recently, Ponte and Croft <ref type="bibr" coords="4,123.51,122.50,11.49,10.91" target="#b5">[6]</ref> proposed language modeling approach to information retrieval. Each document in a document collection is associated with a language model and given a query, documents are ranked based on the probability of their language model generating the query text. Alternatives to query likelihood model have been proposed. Of specific interest here is the method proposed by Lafferty and Zhai <ref type="bibr" coords="4,173.35,176.62,11.49,10.91" target="#b3">[4]</ref> where they associate language models to both documents and queries and rank documents based on their model's similarity with the query model. Model similarity was computed using the Kullback-Leibler divergence measure.</p><p>The motivation for our language modeling approach to TREC-11 batch and adaptive filtering is Zhai and Lafferty's paper on incorporating relevance feedback in language modeling approaches to information retrieval <ref type="bibr" coords="4,174.26,244.42,15.99,10.91" target="#b9">[10]</ref>. Their proposal was in the Query-Document Model similarity approach to information retrieval Given a set of feedback documents </p><formula xml:id="formula_2" coords="4,355.73,257.98,97.50,11.41">F = {d 1 , d 2 , • • • , d n }</formula><formula xml:id="formula_3" coords="4,535.26,299.86,4.59,10.91">)</formula><p>where α is the interpolation parameter. Two different strategies were proposed for feedback model estimation: a generative model of feedback documents and a model with minimum divergence over feedback documents. We used the later in our language modeling approach to information filtering. Zhang and Callan <ref type="bibr" coords="4,165.21,360.10,16.88,10.91" target="#b10">[11]</ref> used a method similar to the generative model for feedback documents in their TREC 2001 adaptive filtering submission. They used language modeling techniques in updating terms and term weights in their query representation.</p><p>In the divergence minimization approach, the feedback model is estimated to satisfy two conditions: (1) that it is "closer" to the feedback documents and (2) it is "farther" from the corpus model. The second condition ensures that the effect of language and domain characteristics common to feedback documents do not generalize the new query model and move it off topic. The feedback model is selected to be the one which minimizes</p><formula xml:id="formula_4" coords="4,192.24,473.07,347.44,33.92">D e (θ; F, C) = 1 |F | n i=1 D(θ|| θd i ) -λ D(θ||p(•|C))<label>(4)</label></formula><p>where p(•|C) is the corpus probability distribution and λ is the feedback parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Modeling approach to Information Filtering</head><p>Given a query Q, two language models are estimated: (1) positive or on-topic language model, θP , and ( <ref type="formula" coords="4,119.03,580.78,4.59,10.91" target="#formula_1">2</ref> </p><formula xml:id="formula_5" coords="4,174.60,645.58,365.16,30.49">D n (θ Fp ; F p , C) = 1 |F p | d∈Fp D(θ Fp || θd ) -λ D(θ Fp ||p(•|C)<label>(5)</label></formula><p>While the negative language model θN can be used instead of the corpus probabilities in (4), we have used the corpus model since it is a better representation of what is not in topic. A negative feedback model is generated by minimizing</p><formula xml:id="formula_6" coords="5,178.32,93.82,361.52,30.49">D n (θ Fn ; F n , C) = 1 |F n | d∈Fn D(θ Fn || θd ) -λ D(θ Fn || θP )<label>(6)</label></formula><p>where</p><formula xml:id="formula_7" coords="5,105.07,134.74,292.31,13.04">F n = {d n 1 , d n 2 , • • • , d n |Fn| } is the set of negative examples.</formula><p>Here one is interested in a negative feedback model that is "closer" to negative examples by "farther" from positive language model. Unlike <ref type="bibr" coords="5,142.31,161.86,16.88,10.91" target="#b9">[10]</ref> who used a Dirichlet smoothing in estimating document language model, θd , we used a mixture model with fixed weights for document and corpus statistics.</p><p>p(w| θd ) = γ p(w|d) + (1 -γ) p(w|C) <ref type="bibr" coords="5,525.85,197.86,13.77,10.91" target="#b6">(7)</ref> where γ was set to 0.6. The positive and negative topic models are updated by</p><formula xml:id="formula_8" coords="5,229.92,253.54,309.93,32.53">θP = (1 -α 1 ) θP + α 1 θ Fp (8) θN = (1 -α 2 ) θN + α 2 θ Fn .<label>(9)</label></formula><p>Given a test document, its language model is first estimated using <ref type="bibr" coords="5,409.32,297.22,12.75,10.91" target="#b6">(7)</ref>. Its score is determined by the ratio of its divergence from positive and negative models score( θd ; θP , θN ) = D( θd || θP )/D( θd || θN ). <ref type="bibr" coords="5,520.54,333.22,19.16,10.91" target="#b9">(10)</ref> Document scores are thresholded to make the binary classification decision. Thresholds were estimated based on score distribution in the training set similar to the method used by <ref type="bibr" coords="5,475.00,369.22,10.86,10.91" target="#b0">[1]</ref>. The score of relevant documents are assumed to be normally distributed and the top non-relevant documents are exponentially distributed. The utility score is optimized to obtain a closed form solution for the threshold.</p><p>For adaptive filtering, there are no negative examples and hence the initial negative model is the corpus model. The above method is followed to classify documents. When a document is deemed relevant for a query by the system, its relevance judgment is fetched to update the language models. If the document was judged relevant to the topic, the positive model is updated and if it was deemed not relevant the negative model was updated. While the models can be updated irrespective of the relevance of the document, for our TREC submission, we only updated one model at a time. The score threshold is updated before moving on to the next document. Some implementation specific details and observation on our system's performance are given here. In our implementation,</p><p>• Document and queries were stemmed and stop words were removed.</p><p>• Only the topic description was used in generating the initial topic model generation • While computing the score, terms with probability less than 0.0001 were ignored.</p><p>• Instead of top non-relevant document scores, all non-relevant document scores were used in computing the threshold.</p><p>• In adaptive filtering, the corpus statistics were not updated as test documents are processed.</p><p>The training document collection was used to estimate the corpus model.</p><p>• In adaptive filtering, documents whose relevance is not known is assumed to be not relevant to the topic and is used in updating the negative topic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Observations and Conclusion</head><p>The results we submitted to filtering track using either methods is not impressive. There are couple reasons for that. With regards to our Weighted SVM submission, besides the fact we didn't use a stronger version WMSVM, the naive way of making use of topic description/narrative probably killed us. This is partially supported by the different performance difference between us and best performance for topic: in the first 50 topic where description/narrative is rich in content, we lag far behind the best performer. But on the second 50 topic, where the topic description/narrative is not as good, we are a little bit closer to the best performer overall. With regards to the language modeling approach, our initial analysis suggests that the thresholding technique used seems to favor high recall taking our system closer to an "allow-all" classifier. This could have been due to the characteristics of the measure we used for scoring documents. Using all non-relevant documents in our threshold computation seems to have affected the thresholding processes. This was compounded by the assumption of documents with unknown relevance as non-relevant.</p><p>Either methods, we believe, have scope for further improvement with respect to their application in information filtering. We expect the stronger version of WMSVM to perform better. In addition we are exploring better ways to generate pseudo examples from topic description/narratives. At same time, a couple parameter, such as the observation weight for each pseudo examples and the number of pseudo examples, can be tuned to improve filtering performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,206.16,276.46,199.35,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SVM vs Weighted Margin SVM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,123.61,577.90,415.99,13.79;4,72.00,594.34,467.02,10.91;4,72.00,607.78,238.58,10.91;4,311.04,612.03,4.32,7.16;4,319.32,607.78,23.01,10.91;4,342.36,612.03,7.92,8.18;4,351.36,607.78,10.42,10.91;4,361.80,612.03,7.92,8.18;4,370.80,607.78,31.49,10.91;4,402.36,612.03,17.88,8.78;4,421.32,607.78,118.47,10.91;4,72.00,621.94,459.49,13.79"><head></head><label></label><figDesc>) negative or off-topic language model, θN . The initial positive and negative models are estimated from the topic description. These models are updated based on the training data available for each query. The positive examples F p = {d p 1 , d p 2 , • • • , d p |Fp| } are used to update the positive model θP using the feedback model generated by minimizing (5) which is similar to (4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.00,257.98,467.84,52.79"><head></head><label></label><figDesc>for query Q, they estimate a feedback model θF based on the feedback document F and use it to update the query model θQ to θQ by θQ = (1 -α) θQ + α θF (3</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,94.41,387.94,444.88,10.91;6,94.44,401.50,444.73,10.91;6,94.44,415.06,221.22,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,400.28,387.94,139.02,10.91;6,94.44,401.50,254.21,10.91">Incrementality, half-life, and threshold optimzation for adaptive document filtering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Van Der Weide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,133.23,415.43,54.78,10.05">TREC 2000</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.41,437.02,444.72,10.91;6,94.44,450.58,303.04,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,152.99,437.02,290.09,10.91">Classification of news stories using support vector machines</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cooley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,471.11,437.39,68.02,10.05;6,94.44,450.95,171.07,10.05">Proceedings of IJCAI&apos;99 Workshop on Text Mining</title>
		<meeting>IJCAI&apos;99 Workshop on Text Mining<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.41,472.66,444.72,10.91;6,94.44,486.22,444.65,10.91;6,94.44,499.78,444.83,10.91;6,94.44,513.22,113.05,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,195.26,472.66,343.87,10.91;6,94.44,486.22,77.06,10.91">Text categorization with support vector machines: learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,417.64,486.59,121.45,10.05;6,94.44,500.15,223.24,10.05">Proceedings of ECML-98, 10th European Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Claire</forename><surname>Nédellec</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Céline</forename><surname>Rouveirol</surname></persName>
		</editor>
		<meeting>ECML-98, 10th European Conference on Machine Learning<address><addrLine>Chemnitz, DE; Heidelberg, DE</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.41,535.30,444.73,10.91;6,94.44,548.86,267.18,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,211.56,535.30,327.58,10.91;6,94.44,548.86,96.74,10.91">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,213.95,549.23,39.19,10.05">SIGIR01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.41,570.82,444.72,10.91;6,94.44,584.38,444.62,10.91;6,94.44,597.94,69.34,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,162.57,570.82,376.56,10.91;6,94.44,584.38,22.45,10.91">Applying support vector machines to the trec-2001 batch filtering and routing tasks</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,145.23,584.75,357.41,10.05">NIST Special Publication 500-250: The Tenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="286" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.41,619.90,444.89,10.91;6,94.44,633.46,233.95,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,249.81,619.90,267.21,10.91">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,94.44,633.83,39.19,10.05">SIGIR98</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.41,655.54,444.45,10.91;6,94.44,669.10,104.43,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,198.26,655.91,252.75,10.05">The nature of statistical learning theory, 2nd Edition</title>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>Heidelberg, DE</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.41,691.06,444.80,10.91;6,94.44,704.62,271.50,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,208.36,691.06,237.95,10.91">A re-examination of text categorization methods</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,476.67,691.43,62.53,10.05;6,94.44,704.99,94.24,10.05">22nd Annual International SIGIR</title>
		<meeting><address><addrLine>Berkley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.41,74.74,444.47,10.91;7,94.44,88.30,138.77,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,170.13,74.74,298.14,10.91">An evaluation of statistical approaches to text categorization</title>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,482.24,75.11,56.64,10.05;7,94.44,88.67,40.62,10.05">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="69" to="90" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.40,110.86,444.72,10.91;7,94.44,124.42,218.69,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,209.63,110.86,329.49,10.91;7,94.44,124.42,60.10,10.91">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,177.31,124.79,26.55,10.05">CIKM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.40,146.86,444.62,10.91;7,94.44,160.42,427.50,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,211.91,146.86,279.39,10.91">The bias problem and language models in adaptive filtering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<editor>E. M. Voorhees and D. K. Harman</editor>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="78" to="83" />
			<pubPlace>Gaithersburg, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
