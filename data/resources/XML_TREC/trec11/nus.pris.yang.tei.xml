<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,64.50,60.29,482.13,11.48">The Integration of Lexical Knowledge and External Resources for Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,240.00,80.07,43.67,10.13"><forename type="first">Hui</forename><surname>Yang</surname></persName>
							<email>yangh@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.54,80.07,65.21,10.13"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<email>chuats@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,64.50,60.29,482.13,11.48">The Integration of Lexical Knowledge and External Resources for Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">44E8F7D0B2CE8F2B6E152A39A524FD75</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the short, factoid questions in TREC, the query terms we get from the original questions are either too brief or often do not contain most relevant information in the corpus. It will be very difficult to find the answer (especially exact answer) in a large text document collection because of the gap between the query space and the document space. In order to bridge this gap, there is a need to expand the original queries to include the terms in the document space. In this research, we investigate the integration of both the Web and WordNet in performing local context and lexical correlations to bridge the gap. In order to minimize the noise introduced by the external resources, we explore detailed question classes, fine-grained named entities, and successive constraint relaxation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>We are participating in this year's Question Answering (QA) main task and it's our first time to take part in TREC. Question answering has recently received attention from many natural language processing communities <ref type="bibr" coords="1,491.36,253.59,11.52,8.78" target="#b0">[1]</ref>[2] <ref type="bibr" coords="1,514.40,253.59,15.37,8.78" target="#b18">[19]</ref>. Our goal is to retrieve the exact answers for the short, factoid questions in TREC. In our system, several modules have been developed. They are question processing, external resources adoption, document retrieval, candidate sentence selection and exact answer extraction.</p><p>During question parsing, the detailed question classes, answer types, original content query terms and NLP roles of the query terms are analyzed. We derive detailed question class ontology that corresponds to fine-grained named entities. This enables us to extract exact answer from the candidate sentences more accurately.</p><p>The original query terms can be used as the basis to locate potential answer candidates in the corpus. However, one major problem of doing this is that the query terms do not have sufficient coverage to locate most answer candidates. This is known as the semantic gap between the query space and document space. In order to bridge this gap, we use the knowledge of both the Web and lexical resources to expand the original query. We first use the original query to search the Web for top N web documents and then extract terms that co-occur frequently in the local context of the N-gram query terms. We next use WordNet to find other terms in the retrieved documents that are lexically related to the expanded query terms. The new query therefore contains terms that are related to the local context in the Web and the lexical context through WordNet. Finally, we use the expanded query to search for answer candidates through the MG system <ref type="bibr" coords="1,426.75,423.09,14.99,8.77" target="#b19">[20]</ref>.</p><p>Candidate answer sentences are selected from the top returned documents and are ranked based on certain criteria to maximize the answer recall and precision. NL analysis is performed on these candidate sentences to extract POS, base Noun Phrases, Named Entities, etc. Answer selection is done by matching the expected answer type to the NL results. The nearest string with the expected answer type in the candidate sentence is returned as the final answer. Figure <ref type="figure" coords="1,507.99,472.59,4.88,8.78">1</ref> gives the overview of our system architecture Figure <ref type="figure" coords="1,265.18,690.84,3.77,8.78">1</ref>: Overview of System Architecture In this system, we focus on the techniques to expand the original query to locate most answer candidates. The resulting approach is efficient and has been found to be effective. Our experiments on TREC QA main task show good results when combining both local context and lexical information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Formulation Question Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Processing</head><p>The purpose of our question processing is to find the specific nature of each question and to make full use of all the information in the question in order to find the best answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Question Classification</head><p>Question classification in our system is based on question focus and answer type. A rule-based question classifier is developed to determine the question focus and question class. There are seven main question classes in our system. They are: HUM (Human), LOC (Location), TME (Time), NUM (Number), OBJ (Object), DES (Description) and UNKNOWN (Unknown). The last type UNKNOWN is used to group questions that cannot be categorized into the other classes. Different types of the questions are treated slightly differently in the following answer extraction module.</p><p>Ã˜ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Question Parsing</head><p>Besides question classes, some important information are also extracted when we parse the question. They are crucial for the later processes. Detailed analysis is performed here in order to get as much useful information as possible. There are several kinds of word groups we extract from the original question. They are:</p><p>(1) Content Words: These include nouns, adjectives, numbers, and some non-trivial verbs, which appear in the question string. Part of Speech tagging is performed before we select the content words. For example: " What mythical Scottish town appears for one day every 100 years?", the content word vector will be q (0)</p><p>: (mythical, Scottish, town, appears, one, day, 100, years) (2) Basic Noun Phrases: we use noun phrase recognizer to identity all basic noun phrases appear in the question. For the above example, the noun phrase vector n : ("mythical Scottish town") (3) Head of the First Noun Phrase: It refers to the noun follows the question header (e.g. what, which, how, etc) and carries the main meaning of the question focus. It can be the next noun or the last word in the next noun phrase after the question header. For the above example h : (town). Usually, there is only one such head for each question sentence. (4) Quotation Words: For some of the questions, quotations appear in the question string. They should be given special treatment. The string inside quotation marks usually is longer than a noun phrase, sometimes it could be a full sentence. For example, "What Broadway musical is the song " The Story is Me " from ?" The quotation word vector will be u : ("The Story is Me ")</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Query Expansion</head><p>After question processing, we need to locate the relevant documents and sentences from the TREC corpus. The most common way is to apply information retrieval techniques to find the relevant documents and candidate answer sentences.</p><p>For the short, factual questions in TREC, the query terms we get from the original questions are either too brief or do not fully cover the terms used in the corpus.</p><p>Given a short query, q</p><formula xml:id="formula_0" coords="3,150.00,515.14,85.87,14.33">(0) = [q 1 (0) q 2 (0) â€¦q k<label>(0)</label></formula><p>] usually with k&lt;=4, the problem for retrieving all the documents relevant to q (0) is that the query does not contain most of the terms used in the document space to represent the same concept. Thus there is thus a need to expand the original query to bridge the gap between the query space and document space.</p><p>We use general open resources to overcome this problem. The external general resources that can be readily used include the Web, WordNet, Knowledge bases, and Query Logs. Many groups working on QA have recently used the Web <ref type="bibr" coords="3,144.99,582.84,15.86,8.77" target="#b17">[18]</ref> and WordNet <ref type="bibr" coords="3,228.02,582.84,12.47,8.77" target="#b6">[7]</ref>[10] <ref type="bibr" coords="3,257.11,582.84,16.62,8.77" target="#b10">[11]</ref>[16] <ref type="bibr" coords="3,290.36,582.84,16.62,8.77" target="#b16">[17]</ref> as resources for question answering. In our system, w e integrate the external resources to expand the query. The new query is then used to look for the relevant documents and sentences in the QA Text Collection.</p><formula xml:id="formula_1" coords="3,57.75,582.84,87.24,8.77">[3][4][5][6][8][9][12]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Using Web as the Generalized External Resource</head><p>The Web is the most rapidly growing and complete knowledge resource in the world now. The terms in the relevant documents retrieved from the Web are likely to be similar or even the same as those in the QA Text Collection since they are both news articles.</p><p>Original content words in the question are passed to the online search engine, e.g. Google, to search for documents in the Web. The terms in the relevant web documents that are highly correlated with the original query terms will be considered as candidates to expand the context of the original query. The steps are: a) Get original query q</p><formula xml:id="formula_2" coords="3,75.75,713.14,203.47,28.48">(0) = (q 1 (0) , q 2 (0) ,â€¦, q k (0) ). b) For q (0)</formula><p>, retrieve the top N documents from the Web. c) âˆ€q i (0) âˆˆ q (0)</p><p>, extract w i , which contains non-trivial words in the same sentence or within p words away from q i (0) in the retrieved web documents. d) Rank all w ik âˆˆ w i by computing its probability of co-occurrence with q i (0) as:</p><formula xml:id="formula_3" coords="4,168.75,101.94,333.29,28.29">) ( ) ( ) ( ) 0 ( ) 0 ( i ik s i ik s ik q w d q w d w pr âˆ¨ âˆ§ = (1)</formula><p>where, d s (w ik /\ q i (0)</p><p>) is the number of instances that w ik and q i (0) appear together, and d s (w ik \/ q i (0)</p><p>) is the number of instances that either w ik or q i (0)</p><p>appears.</p><p>e) Merge all w i to form C q for q (0)</p><p>. Therefore, C q contains the list of words that are highly correlated with the original query from web documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Use WordNet as the Generalized External Resource</head><p>The Web can only provide us the words that occur frequently with the original query terms in the local context. It however, lacks information on lexical relationships between these terms. To overcome this problem, we look up WordNet to find words that are lexically related to the original query terms. The glosses, synonyms, and hypernyms are considered to be useful in relating words. In this work, we consider glosses and synonyms only to relate terms. For example, from the glosses, Ã˜ Definition of plant: A living organism lacking the power of locomotion Ã˜ Definition of animal: A living organism characterized by voluntary movement</p><p>The common concept here is living organism, which will link concept plant to concept animal.</p><p>From WordNet, we can find gloss words G q and synset words S q for q (0)</p><p>. If we expand the query by appending all the terms in the glosses and synsets, it tends to be too general and contain too many terms out of context. In general, we need to restrict G q and S q to those terms found in the web documents, i.e., those found in C q . Thus we circumvent this problem by using gloss and synset relations to increase the weights of appropriate context terms w k âˆˆ C q by:</p><formula xml:id="formula_4" coords="4,75.75,372.65,428.54,31.56">Ã˜ if w k âˆˆ G q , increase w k by Î± Ã˜ if w k âˆˆ S q , increase w k by Î², (0&lt;Î²&lt;Î±&lt;1)<label>(2)</label></formula><p>The final weight for each term in C q is normalized for ranking. The new query is formed as q</p><p>+ {top m terms from C q whose weights are below the selection threshold }</p><p>Currently, we plan to use the Semantic Perceptron Net approach <ref type="bibr" coords="4,319.48,444.09,16.83,8.77" target="#b13">[14]</ref> to derive semantic groups in C q , G q and S q in order to derive a structured approach to utilize external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Document &amp; Candidate Answer Sentence Retrieval</head><p>We use the MG tool <ref type="bibr" coords="4,143.25,489.84,16.44,8.77" target="#b19">[20]</ref> in our system to index the documents. We choose Boolean retrieval because of the short queries and the need to maximize precision. After performing Boolean retrieval by using q <ref type="bibr" coords="4,390.75,498.64,8.12,6.07" target="#b0">(1)</ref> to retrieve the top M documents (M = 50), if q <ref type="bibr" coords="4,93.00,512.14,8.12,6.08" target="#b0">(1)</ref> does not return sufficient number of relevant documents, we reduce the extra terms added and repeat the Boolean search. Therefore, we successively relax the constraints to ensure precision in document retrieval. The sentence is chosen as the basic unit for processing in our system. After performing sentence boundary detection, we use the following criteria to rank the relevance of a sentence to the question: (Recall from query processing, we extracted q (0) , n, h, u). For each Sentence Sent j , we match it with</p><p>â€¢ quotation words: W uj = % of term overlap between u and Sent j â€¢ noun phrases: W nj = % of phrase overlap between n and Sent j â€¢ head of first noun phrase: W hj = 1 if there is a match and 0 otherwise â€¢ original content words: W cj = % of term overlap between q (0) and Sent j â€¢ expanded content words: W ej = % of term overlap between q (1-0) and Sent j , where q</p><formula xml:id="formula_7" coords="4,428.25,635.89,61.37,13.48">(1-0) = q (1) -q (0)</formula><p>The final score for the sentence is , where âˆ‘a i =1, W ij âˆˆ { W uj , W nj , W hj , W cj , W ej }. The top K sentences are then selected as the candidate answer sentences based on S j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Answer Extraction</head><p>Finally we perform the tagging of fine-grained named entities <ref type="bibr" coords="4,308.27,707.34,16.85,8.77" target="#b14">[15]</ref> on the top K sentences extracted from the previous steps. From these sentences, we extract the string that matches the Question classes (answer target) as the answer. Once an answer is found within the top i th sentence, the system will terminate the search for the rest of (K-i) sentences. When there is more</p><formula xml:id="formula_8" coords="4,198.00,657.73,53.49,17.87">âˆ‘ = i j S ij i W a</formula><p>than one matching strings in a single sentence, we will choose the string that is nearest to the original query terms. For example: for question " Where did Dr. King give his speech in Washington?", we get:</p><p>â€¢ Q-class: LOC_BASIC â€¢ &lt;LOC_BASIC WASHINGTON&gt; KING-DREAM _ &lt;LOC_BASIC WASHINGTON&gt; _ In the &lt;NUM_PERIOD 35 years&gt; since Dr . &lt;HUM_PERSON Martin Luther King&gt; Jr . delivered his `` I Have a Dream '' speech at the &lt;LOC_BASIC Lincoln Memorial&gt; , how have economic and social conditions changed for &lt;LOC_CONTINENT African&gt; Americans ? For question class LOC_BASIC, we look for all the sub categories under LOC and we will get WASHINGTON, WASHINGTON, Lincoln Memorial and African as answer candidates. Among them, Lincoln Memorial is the nearest string to original content word speech, and hence is picked as the exact answer.</p><p>For some questions, we cannot find any answer. Our solution is to reduce the number of the expanded query terms and repeat the document/sentence retrieval and answer extraction process for up to m iterations (m=5). If we still cannot find an exact answer, NIL is returned as the answer. We call this method successive constraint relaxation, which helps to increase the recall while preserving precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Result Analysis</head><p>We answered 290 questions correctly with un-interpolated average precision of 0.61. Figure <ref type="figure" coords="5,430.04,255.84,4.88,8.78">2</ref> shows that our system works well for mo st of the easy questions (right side of the figure), and has reasonable performance for the difficult ones. We also found that the accuracy of the exact answers differ for different type of questions (see Figure <ref type="figure" coords="5,467.02,678.09,3.61,8.78">3</ref>). For some question classes, like Time, Location and Human, our system gives quite high performance. For Description, Number and Object questions, we still need to find better techniques to improve the performance.</p><p>Another problem is that we have too many questions with NIL answers. The precision for recognizing NIL answer is low: 41 / 170 = 0.241, although the recall for NIL answer is satisfactory: 41 / 46 = 0.891. As a result, the overall system recall (consider both questions with non-NIL and those with NIL answer) is not satisfactory as compared to precision. This is because we use the boolean search to look for relevant TREC documents. Only the documents containing all the query terms are returned. This restriction might be too strict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Future Work</head><p>We are currently refining our approach in several directions. First, we are refining our terms correlation by considering a combination of local context, global context and lexical correlations. Second, we are working towards a template-based approach on answer selection that incorporates some of the current ideas on question profiling and answer proofing, etc. Third, we will explore the structured use of external knowledge using the semantic perceptron net approach <ref type="bibr" coords="6,516.40,150.09,15.21,8.78" target="#b13">[14]</ref>. Our longer-term research plan includes Interactive QA, and the handling of more difficult analysis and opinion question types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,222.00,489.09,167.90,8.77;5,165.75,499.50,234.00,161.25"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Question Difficulty Distribution</figDesc><graphic coords="5,165.75,499.50,234.00,161.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,48.00,180.84,529.50,554.61"><head>Table 1 :</head><label>1</label><figDesc>Question Classes</figDesc><table coords="2,48.00,180.84,529.50,499.11"><row><cell></cell><cell cols="4">Example 1: "Which city is the capital of Canada? " (Q-class: LOC)</cell></row><row><cell cols="5">Ã˜ Example 2: "Which province is the capital of Canada in? " (Q-class: LOC)</cell></row><row><cell cols="5">Obviously, both of the questions belong to the type of LOC (Location) and their content words are almost the same, i.e.,</cell></row><row><cell cols="5">capital and Canada. However, they are expecting different answers, which should fall in different categories, i.e., city or</cell></row><row><cell cols="5">state. The first question's answer will be Ottawa but the second's answer should be Ontario. In order to detect the subtle</cell></row><row><cell cols="5">differences in the questions, we further classify the first 6 major question classes into 54 sub classes (see table 1 below).</cell></row><row><cell cols="5">Under each main class, there is a special sub-class called XXX_BASIC, which is designed for questions that fall in the major</cell></row><row><cell cols="5">class but do not suit any of the sub-classes. Our question classification is similar to the learning classifier developed by Li</cell></row><row><cell cols="5">and Roth [13]. Currently, our rule -based classifier can reach an accuracy of over 98%.</cell></row><row><cell>Q-Class</cell><cell>Q-Sub-Class</cell><cell cols="2">#Trec11q #Trec10q</cell><cell>Example</cell></row><row><cell></cell><cell>HUM_PERSON</cell><cell>43</cell><cell>19</cell><cell>Who is the governor of Colorado ?</cell></row><row><cell>HUM</cell><cell>HUM_ORG</cell><cell>11</cell><cell>4</cell><cell>What car company invented the Edsel ?</cell></row><row><cell></cell><cell>HUM_BASIC</cell><cell>41</cell><cell>35</cell><cell>Who is Tom Cruise married to ?</cell></row><row><cell>LOC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>LOC_BASIC</cell><cell>50</cell><cell>29</cell><cell>Where is Devil 's Tower ?</cell></row><row><cell></cell><cell>NUM_WEIGHT</cell><cell>0</cell><cell>2</cell><cell>What is the average weight of a Yellow Labrador ?</cell></row><row><cell></cell><cell>NUM_DEGREE</cell><cell>3</cell><cell>7</cell><cell>What is the boiling point of water ?</cell></row><row><cell>NUM</cell><cell>NUM_AGE</cell><cell>9</cell><cell>7</cell><cell>How old was Nolan Ryan when he retired ?</cell></row><row><cell></cell><cell>NUM_RANGE</cell><cell>2</cell><cell>0</cell><cell>What is the range for the number of passengers a Boeing 747 airplane can carry ?</cell></row><row><cell></cell><cell>NUM_SPEED</cell><cell>3</cell><cell>5</cell><cell>How fast does a cheetah run ?</cell></row><row><cell></cell><cell>NUM_FREQUENCY</cell><cell>1</cell><cell>0</cell><cell>How often does the United States government conduct an official population census ?</cell></row><row><cell></cell><cell>NUM_SIZE</cell><cell>2</cell><cell>1</cell><cell>What 's the capacity of the Superdome ?</cell></row><row><cell></cell><cell>NUM_AREA</cell><cell>1</cell><cell>1</cell><cell>How much area does the Everglades cover ?</cell></row><row><cell></cell><cell>NUM_BASIC</cell><cell>8</cell><cell>5</cell><cell>How much vitamin C should you take in a day ?</cell></row><row><cell></cell><cell>TME_YEAR</cell><cell>24</cell><cell>15</cell><cell>What year was Alaska purchased ?</cell></row><row><cell>TME</cell><cell>TME_MONTH TME_DAY</cell><cell>2 8</cell><cell>0 9</cell><cell>In what month are the most babies born ? What day did Neil Armstrong land on the moon ?</cell></row><row><cell></cell><cell>TME_BASIC</cell><cell>65</cell><cell>23</cell><cell>When was the telegraph invented ?</cell></row></table><note coords="2,89.25,339.37,50.52,7.43;2,190.50,338.85,4.50,8.10;2,237.75,338.85,4.50,8.10;2,268.50,338.85,302.26,8.10;2,89.25,349.87,40.42,7.43;2,188.25,349.35,9.00,8.10;2,235.50,349.35,9.00,8.10;2,268.50,349.35,127.50,8.10;2,89.25,361.12,65.45,7.43;2,190.50,360.60,4.50,8.10;2,237.75,360.60,4.50,8.10;2,268.50,360.60,111.79,8.10;2,89.25,372.37,58.68,7.43;2,188.25,371.85,9.00,8.10;2,237.75,371.85,4.50,8.10;2,268.50,371.85,97.51,8.10;2,89.25,383.62,54.23,7.43;2,190.50,383.10,4.50,8.10;2,237.75,383.10,4.50,8.10;2,268.50,383.10,115.54,8.10;2,89.25,394.87,45.81,7.43;2,190.50,394.35,4.50,8.10;2,237.75,394.35,4.50,8.10;2,268.50,394.35,219.02,8.10;2,89.25,405.37,60.16,7.43;2,190.50,404.85,4.50,8.10;2,237.75,404.85,4.50,8.10;2,268.50,404.85,135.03,8.10;2,89.25,416.62,45.70,7.43;2,190.50,416.10,4.50,8.10;2,237.75,416.10,4.50,8.10;2,268.50,416.10,262.40,8.10;2,89.25,427.87,45.04,7.43;2,190.50,427.35,4.50,8.10;2,237.75,427.35,4.50,8.10;2,268.50,427.35,143.28,8.10;2,89.25,439.12,41.42,7.43;2,190.50,438.60,4.50,8.10;2,237.75,438.60,4.50,8.10;2,268.50,438.60,139.50,8.10;2,89.25,450.37,64.59,7.43;2,190.50,449.85,4.50,8.10;2,237.75,449.85,4.50,8.10;2,268.50,449.85,274.50,8.10;2,89.25,460.87,47.45,7.43;2,190.50,460.35,4.50,8.10;2,237.75,460.35,4.50,8.10;2,268.50,460.35,202.55,8.10;2,89.25,472.12,50.31,7.43;2,190.50,471.60,4.50,8.10;2,237.75,471.60,4.50,8.10;2,268.50,471.60,154.52,8.10;2,89.25,494.62,49.75,7.43;2,188.25,494.10,9.00,8.10;2,235.50,494.10,9.00,8.10;2,268.50,494.10,195.01,8.10;2,89.25,505.87,44.53,7.43;2,190.50,505.35,4.50,8.10;2,237.75,505.35,4.50,8.10;2,268.50,505.35,216.77,8.10;2,89.25,516.37,57.96,7.43;2,190.50,515.85,4.50,8.10;2,237.75,515.85,4.50,8.10;2,268.50,515.85,174.01,8.10;2,89.25,527.62,62.41,7.43;2,188.25,527.10,9.00,8.10;2,235.50,527.10,9.00,8.10;2,268.50,527.10,152.26,8.10;2,89.25,682.87,61.68,7.43;2,190.50,682.35,4.50,8.10;2,237.75,682.35,4.50,8.10;2,268.50,682.35,132.11,8.10;2,89.25,694.12,44.99,7.43;2,190.50,693.60,4.50,8.10;2,237.75,693.60,4.50,8.10;2,268.50,693.60,209.25,8.10;2,89.25,705.37,51.08,7.43;2,190.50,704.85,4.50,8.10;2,237.75,704.85,4.50,8.10;2,268.50,704.85,119.27,8.10;2,89.25,716.62,43.71,7.43;2,190.50,716.10,4.50,8.10;2,237.75,716.10,4.50,8.10;2,268.50,716.10,153.77,8.10;2,54.75,682.12,14.39,7.43;2,89.25,727.87,44.43,7.43;2,190.50,727.35,4.50,8.10;2,237.75,727.35,4.50,8.10;2,268.50,727.35,163.52,8.10"><p>LOC_PLANET 1 2 Which planet did the spacecraft Magellan enable scientists to research extensively ? LOC_CITY 18 16 What is the capital city of Algeria ? LOC_CONTINENT 3 2 What continent is Scotland in ? LOC_COUNTRY 18 3 What country is Berlin in ? LOC_COUNTY 3 2 What county is Elmira , NY in ? LOC_STATE 3 4 Which state has the longest coastline on the Atlantic Ocean ? LOC_PROVINCE 2 2 What province is Calgary located in ? LOC_TOWN 2 0 The Hindenburg disaster took place in 1937 in which New Jersey town ? LOC_RIVER 3 3 What river is called "China 's Sorrow" ? LOC_LAKE 2 2 What is the deepest lake in the world ? LOC_MOUNTAIN 1 2 What is the name of the volcano that destroyed the ancient city of Pompeii ? LOC_OCEAN 2 1 What body of water does the Colorado River flow into ? LOC_ISLAND 3 1 What is the world 's second largest island ? NUM_COUNT 11 12 How many chromosomes does a human zygote have ? NUM_PRICE 5 1 How much does it cost to register a car in New Hampshire ? NUM_PERCENT 4 8 What percent of the U.S . is African American ? NUM_DISTANCE 22 16 What is the height of the tallest redwood ? OBJ_CURRENCY 2 5 What is the currency used in China ? OBJ_MUSIC 8 2 What was Aaron Copland 's most famous piece of music ? OBJ_ANIMAL 5 9 What is the state bird of Alaska ? OBJ_PLANT 2 5 What is the major crop grown in Arizona ? OBJ OBJ_BREED 1 1 What breed was Roy Rogers ' horse Trigger ?</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,75.75,194.34,369.53,8.78" xml:id="b0">
	<monogr>
		<title level="m" coord="6,75.75,194.34,131.50,8.78;6,241.57,194.34,199.39,8.78">Mining Answers from Text and Knowledge Bases</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>AAAI Spring Symposium Series</note>
</biblStruct>

<biblStruct coords="6,75.75,206.34,283.35,8.78" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><surname>Acl-Eacl</surname></persName>
		</author>
		<title level="m" coord="6,158.71,206.34,195.73,8.78">Workshop on Open-domain Question Answering</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,217.59,478.22,8.78;6,75.75,229.59,365.95,8.78" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,284.16,217.59,269.81,8.78;6,75.75,229.59,41.94,8.78">Learning search engine specific query transformations for question answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,139.58,229.59,214.28,8.78">Proceedings of the 10th World Wide Web Conference</title>
		<meeting>the 10th World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,240.84,462.05,8.78" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,257.20,240.84,142.46,8.78">Data-intensive question answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Text REtrieval Conference</note>
</biblStruct>

<biblStruct coords="6,75.75,252.09,477.81,8.78;6,75.75,264.09,358.38,8.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,250.80,252.09,223.66,8.78">An analysis of the AskMSR question-answering system</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,496.27,252.09,57.29,8.78;6,75.75,264.09,290.57,8.77">Proceedings of 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,275.34,467.82,8.77;6,75.75,286.59,332.94,8.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,163.50,275.34,380.06,8.77;6,75.75,286.59,43.84,8.78">Using grammatical relations, answer frequencies and the World Wide Web for TREC question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,142.73,286.59,206.85,8.77">Proceedings of the Tenth Text Retrieval Conference</title>
		<meeting>the Tenth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2002. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,298.59,460.79,8.77;6,75.75,309.84,469.34,8.77;6,75.75,321.09,56.26,8.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,498.09,298.59,38.45,8.78;6,75.75,309.84,240.65,8.77">Question answering: CNLP at the TREC-10 question answering track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Diekema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Taffet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mccracken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">E</forename><surname>Ozgencil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Yilmazel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Liddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,338.04,309.84,207.05,8.77">Proceedings of the Tenth Text Retrieval Conference</title>
		<meeting>the Tenth Text Retrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2001">2002. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,333.09,473.85,8.77;6,75.75,344.34,145.64,8.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,266.16,333.09,142.32,8.77">Web reinforced question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,430.80,333.09,118.79,8.77;6,75.75,344.34,86.08,8.77">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2001">2002. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,355.59,466.36,8.77;6,75.75,367.59,430.92,8.77;6,75.75,378.84,96.14,8.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,264.47,355.59,183.30,8.77">Exploiting redundancy in question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,469.88,355.59,72.24,8.77;6,75.75,367.59,430.92,8.77">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,390.09,457.79,8.77;6,75.75,402.09,438.35,8.77;6,75.75,413.34,129.14,8.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,113.24,402.09,220.80,8.77">FALCON: Boosting knowledge for question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Morarescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,354.77,402.09,159.33,8.77;6,75.75,413.34,46.07,8.78">Proceedings of the Ninth Text Retrieval Conference</title>
		<meeting>the Ninth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,424.59,454.98,8.77;6,75.75,436.59,192.30,8.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,254.71,424.59,176.77,8.77">The use of external knowledge in factoid QA</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,456.90,424.59,73.83,8.77;6,75.75,436.59,132.86,8.77">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2002. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,447.84,473.88,8.77;6,75.75,459.09,184.45,8.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,249.71,447.84,155.86,8.77">Scaling question answering to the Web</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,429.96,447.84,119.67,8.77;6,75.75,459.09,138.17,8.77">Proceedings of the 10th World Wide Web Conference (WWW&apos;10)</title>
		<meeting>the 10th World Wide Web Conference (WWW&apos;10)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="150" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,471.09,443.51,8.77;6,75.75,482.34,130.54,8.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,169.76,471.09,119.75,8.77">Learning Question Classifiers</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,309.20,471.09,210.06,8.77;6,75.75,482.34,105.00,8.77">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,493.59,445.49,8.77;6,75.75,505.59,395.87,8.78" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,169.65,493.59,207.70,8.77">Building semantic perceptron net for topic spotting</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,398.41,493.59,122.83,8.77;6,75.75,505.59,217.06,8.78">Proceedings of 37th Meeting of Association of Computational Linguistics (ACL 2001)</title>
		<meeting>37th Meeting of Association of Computational Linguistics (ACL 2001)<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07">Jul 2001</date>
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,516.84,469.10,8.77;6,75.75,528.09,102.69,8.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,150.46,516.84,250.46,8.77">Fine-Grained Proper Noun Ontologies for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,412.72,516.84,132.13,8.77;6,75.75,528.09,75.76,8.77">SemaNet&apos;02: Building and Using Semantic Networks</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,540.09,440.67,8.77;6,75.75,551.34,468.70,8.77;6,75.75,562.59,35.77,8.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,248.64,540.09,154.50,8.77">High performance question/answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,424.75,540.09,91.67,8.77;6,75.75,551.34,407.47,8.77">Proceedings of the 24th AnnualInternational ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th AnnualInternational ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,574.59,477.07,8.77;6,75.75,585.84,445.04,8.77;6,75.75,597.09,96.14,8.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="6,291.89,574.59,182.63,8.77">Question answering by predictive annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,495.62,574.59,57.20,8.77;6,75.75,585.84,445.04,8.77;6,75.75,597.09,54.14,8.77">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;2000)</title>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,609.09,451.35,8.77;6,75.75,620.34,462.18,8.77;6,75.75,631.59,205.62,8.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="6,450.35,609.09,76.75,8.77;6,75.75,620.34,153.53,8.77">Mining the web for answers to natural language questions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,251.11,620.34,286.82,8.77;6,75.75,631.59,165.77,8.77">Proceeding of the 2001 ACM CIKM: Tenth International Conference on Information and Knowledge Management</title>
		<meeting>eeding of the 2001 ACM CIKM: Tenth International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,643.59,478.31,8.77;6,75.75,654.84,103.34,8.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="6,143.88,643.59,222.56,8.77">Overview of the TREC 2001 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,390.99,643.59,163.07,8.77;6,75.75,654.84,46.35,8.78">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference<address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.75,666.09,336.05,8.77" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="6,215.05,666.09,85.51,8.77">Managing Gigabytes</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
