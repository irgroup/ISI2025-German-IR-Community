<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,58.92,72.44,491.42,16.59">Rutgers Filtering Work at TREC 2002: Adaptive and Batch</title>
				<funder>
					<orgName type="full">State University of New Jersey</orgName>
				</funder>
				<funder ref="#_h7RZj5v">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_DwTmBRq">
					<orgName type="full">Office of Naval Research</orgName>
				</funder>
				<funder ref="#_UKq6F9U">
					<orgName type="full">Advanced Research and Development Activity (ARDA)&apos;s Advanced Question Answering for Intelligence (AQUAINT) Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,59.16,118.02,100.02,11.06"><forename type="first">Andrei</forename><surname>Anghelescu</surname></persName>
							<email>angheles@cs.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,170.52,118.02,66.21,11.06"><forename type="first">Endre</forename><surname>Boros</surname></persName>
							<email>boros@rutcor.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,246.72,118.02,63.90,11.06"><forename type="first">David</forename><surname>Lewis</surname></persName>
							<email>ddlewis@worldnet.att.net</email>
						</author>
						<author>
							<persName coords="1,324.36,118.02,87.89,11.06"><forename type="first">Vladimir</forename><surname>Menkov</surname></persName>
							<email>vmenkov@aplab.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,423.48,118.02,55.52,11.06"><forename type="first">David</forename><surname>Neu</surname></persName>
							<email>djneu@acm.org</email>
						</author>
						<author>
							<persName coords="1,509.52,118.02,61.77,11.06"><forename type="first">Paul</forename><surname>Kantor</surname></persName>
							<email>kantor@scils.rutgers.edu</email>
						</author>
						<title level="a" type="main" coord="1,58.92,72.44,491.42,16.59">Rutgers Filtering Work at TREC 2002: Adaptive and Batch</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">57D1A6BE51E6E0EE0A611A1BD479C909</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year at TREC 2002 we participated in the adaptive filtering sub-task of the filtering track with some models for training a Rocchio classifier. Results were poorer than average on the utility type measures. Using simple feature selection produced better than average results on an F-type measure. The key to our approach was the use of pseudojudgments, and an approach to threshold updating. We also participated in the batch filtering sub-task of the filtering track and investigated the use of rank based feature selection techniques in conjunction with a very simple classification rule.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the adaptive filtering sub-task of the filtering track, systems utilize a training set consisting of a small set of documents which are labelled either relevant or irrelevant . This is supplemented by a training set, from which one may draw inferences about the corpus, and may hazard some conjectures as to the relevant documents. In the work reported here, a simple version of "pseudo-relevance feedback" is used to expand the terms appearing in the 3 relevant documents, and the original topic statement.</p><p>Our approach in preparing to study the problem of adaptive filtering attempts to:</p><p>-Incorporate major techniques common to high-scoring AF approaches in recent TRECs.</p><p>-Allow easy modification of aspects we are likely to be doing experiments on.</p><p>-Be efficient enough to do many tuning runs.</p><p>-Be as simple as possible to implement given the above constraints.</p><p>We prepared the AP 1988-1990 data, which served as a "sandbox" for the selection of parameters in the adaptive Rocchio model. We used the LEMUR <ref type="bibr" coords="1,468.87,324.48,14.15,8.97" target="#b10">[10]</ref> toolkit to manage the text, build indices, etc.</p><p>We introduced a number of parameters controlling how many examples will be pseudo-labeled and with what weights:</p><p>In the batch filtering sub-task of the filtering track, systems utilize a training set consisting of documents which are labelled either relevant or irrelevant for a given information needs to develop static classifiers which attempt to distinguish the documents labelled relevant from those labelled irrelevant. In our opinion, efforts to attack this problem are often complicated by several characteristics of textual data.</p><p>Textual data is generally represented by using the terms in the text as features. Such data is inherently highly dimensional -the number of features being potentially equal to the number of words in the English language. In addition, misspellings, the improper, or colloquial use of words, and the fact that many very common words (e.g. "a", "and", "the", etc.) are virtually useless for distinguishing relevant documents from irrelevant ones, regardless of the information need, lead textual data to be noisy. Finally, most terms, even those not considered "noise" under the previous description, are not needed to distinguish relevant documents from irrelevant documents.</p><p>The aforementioned characteristics of textual data indicate that it might be possible to represent a document collection using only a subset of the original feature set which is much smaller than the original feature set, yet possesses properties which serve to facilitate the process of distinguishing relevant documents from irrelevant documents.</p><p>The idea we pursued in the batch filtering sub-task of the filtering track was to employ a heuristic designed to generate such feature subset and to then train an extremely simple classifier on the training set represented only in terms of the selected feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ADAPTIVE FILTERING: BUILDING A CLASSIFIER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Initialization</head><p>Initial training for each topic uses the training set (on which relevance status with respect to the topic is known only for three of the documents), and the topic description. B4. Call the "pseudolabeling algorithm" to run the linear model trained in Step B3 against all training documents. It will return some portion of the training documents, and will have associated with them positive or negative pseudolabels, and fractional weights. Essentially, the documents that achieve a high score on vector retrieval with the initial query and 3 positive documents are taken as "relevant", those with low score are taken as "not relevant". Our algorithm actually has a number of parameters that control (a) the dividing line between "pseudo-relevant" and "pseudoirrelevant" documents (which we refer to, together, as the "pseudo-labelled" documents) (b) the fractions of each class that are sampled into the updated Rocchio classifier and (c) the weights that each type of "pseudo" document are assigned in step B5. Eventually these weights are expressed in terms of the number of "equivalent documents" that the pseudo-labelled documents represent.</p><p>B5. Call the classifier learning algorithm, which changes the query, a la Rocchio, and selects a threshold that maximizes the target score, on the training set.</p><p>Numerous implementation details are not described here. Note that the idea of an "outer loop" over topics represents just one way to approach the problem, which may not be optimal for specific choices of the learning algorithm.</p><p>For further analytical work we have since modified the code to save the classifiers or the internal state of the training algorithm to persistent storage after initial training. This will potentially be useful for multiple experiments with the same starting point, as well as for comparative experiments (studying improvement of classifier over time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive Phase of Training</head><p>In adaptive filtering, we run through the test documents in the specified order, applying classifiers, getting judgments only for documents judged relevant, and updating the classifiers.</p><p>FOR EACH Topic FOR EACH Test Document C1. Apply current classifier for topic to test document, computing score and determining if score is above threshold.</p><p>IF score is ≥ threshold C2.1. Pass document ID, topic ID, score, and label ("relevant") to routine that writes output for evaluation C2.2. Pass document ID and topic ID to judging routine, which will return label (Relevant vs. Nonrelevant vs. Unjudged). ELSE C3.1 Label = Unknown C4. Pass current classifier, document ID, Label, and a weight of 1.0 to Learner (which for the baseline will be an object that in turn calls Rocchio and TROT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">the Rocchio Algorithm</head><p>The Rocchio algorithm <ref type="bibr" coords="2,420.18,69.95,9.59,8.97" target="#b9">[9]</ref> produces a linear model, which must then be specified with a threshold. The basic inputs to the algorithm are:</p><p>1. An initial "query" vector 2. A set of document vectors. Each vector is accompanied by a weight and a label.</p><p>3. The Rocchio weighting parameters (α, β, γ) 4. Feature weighting parameters 5. Feature selection parameters and rules The Rocchio algorithm <ref type="bibr" coords="2,423.66,164.16,9.59,8.97" target="#b9">[9,</ref><ref type="bibr" coords="2,435.77,164.16,7.07,8.97" target="#b4">4]</ref> is a batch algorithm. It produces a new weight vector w from an existing weight vector w1 and a set of training examples. The jth component wj of the new weight vector is:</p><formula xml:id="formula_0" coords="2,355.56,210.72,200.39,22.53">wj = αw1,j + β i∈C xi,j nC -γ i ∈C xi,j n -nC<label>(1)</label></formula><p>where Typically, classifiers produced with the Rocchio algorithm are restricted to having nonnegative weights, so that instead of using the raw w from Equation ( <ref type="formula" coords="2,459.93,323.76,3.57,8.97" target="#formula_0">1</ref>), one uses w where</p><formula xml:id="formula_1" coords="2,388.44,340.32,90.09,19.41">w = w if w &gt; 0 0 otherwise.</formula><p>This is turned into a classifier by the relatively expensive process of recomputing the threshold after each new judgment is received on a submitted document. The computation of the threshold can be somewhat accelerated with a full Rocchio model, but we have not found a way to accelerate it meaningfully when a non-linear step such as the selection of a number of "top features" is included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Retaining only the top 30 terms in a query</head><p>To improve performance, we limited the number of terms appearing in a query.</p><p>The specific algorithm is given in pseudocode as Algorithm 1: Query term selection Require:</p><formula xml:id="formula_2" coords="2,316.80,523.56,238.88,92.73">query vector Q, k 1: for t ∈ Q do 2: if t &lt; 0 then 3: t = 0 4: end if 5: end for 6: S = reverse(sort(Q)) Ensure: S[1 : min(|S|, k)], the top k positive components of Q</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ADAPTIVE TRAINING HEURISTICS</head><p>To find a Rocchio classifier we started at "plausible" values for all of the parameters in the model, and conducted a "greedy" search on each of the parameter values separately.</p><p>Original results concentrated on the utility based measures, and were terrible. This led to the development of a "TREC-specific" feature, which stops sending examples for judgment if the rate of success falls too low. One such heuristic is to stop when the number of consecutive negatives exceeds the total accumulated positive judgments obtained. Such heuristics have no meaning in the real world situations to which adaptive filtering will be applied.</p><p>An alternative heuristic, which can be justified for real applications, is to reduce the number of components in the updated Rocchio vector to a very small number. In one such run the number of components is reduced to 30. These 30 components are selected on the basis of their individual explanatory power, with regard to the specific measure of performance under considerations. In the submitted run, this was an F-measure.</p><p>Since F measures can be rewritten as</p><formula xml:id="formula_3" coords="3,213.36,191.60,79.41,15.58">1 β 1 p +(1-β) 1 R they are</formula><p>very sensitive to finding any relevant documents. If (g, G) are the numbers of relevant documents (found, in the collection) respectively, and n documents are returned,</p><formula xml:id="formula_4" coords="3,73.32,256.08,199.86,8.97">F = 1/(βn/g + (1 -β)G/g) = g/(βn + (1 -β)G)</formula><p>. So a system that "hangs in there" and eventually produces even a single relevant document will score better than a more discriminating system that returns no relevant documents, and quits sooner.</p><p>The results of our early experiments show only that we have set up a workable laboratory for exploring a host of possible combinations of the five key ingredients of an adaptive algorithm: these ingredients are a compression rule; a representation rule; a matching scheme, a learning scheme, and a fusion or selection scheme for combining multiple approaches to each of these five components. As is well known in the information retrieval community, the adaptive filtering task is extremely difficult, but we are optimistic that previously unexplored combinations of approaches may yield meaningful improvements in performance. The results are showin in Table <ref type="table" coords="3,121.89,439.92,3.54,8.97" target="#tab_2">2</ref>, which appears at the end of the paper. The meaning of the row and column labels is as follows.</p><p>1. label of the run, which is composed of 3 parts -the value of the weight of the unjudged documents (parameter thres.unjWt -U-xx → thres.unjWt=xx) followed by the name of the parameter that is changed and the utility that is optimised (for example "best.f" means that the f-beta utility is optimised)</p><p>The parameter related labels have the following meanings:</p><formula xml:id="formula_5" coords="3,86.88,566.52,62.72,23.13">• A+ : α = 2.0 • A-: α = 0.5</formula><p>• C+ : γ = 0.25</p><p>• C-: gamma = 0.0625</p><p>• ND+ : neg density s.t. 2000 pseudo negatives are selected</p><p>• ND-: neg density s.t. 500 pseudo-negatives are selected</p><p>• PD-: pos density = 0.5, corresponding to 10 pseudo-positive documents</p><p>• PW+ : pos weight of 5</p><p>• PW-: pos weight of 1</p><p>• NW+ : neg weight of 10</p><p>• NW-: neg weight of 2</p><p>• def : default values, α = 1.0, β = 1.0, γ = . the number of topics for which at least one document was sent to the 11. the "giveup threshold" for which these results were obtained oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ratio Based Scoring</head><p>In order to provide variety, we also used an alternate scoring scheming in which documents are ordered by a measure of the ratio of their similarities to the centroids of the positive and negative examples. Thus it builds on the relevance feedback information available to Rocchio, with a key difference. Scores are calculated using the (regularized) ratio of distances between normalized vectors. Specifically, if p, n are the unit vectors corresponding to the centroids of the positive and negative examples, and d is the unit vector corresponding to the document being scored, then</p><formula xml:id="formula_6" coords="3,398.28,498.47,157.66,20.85">sC(d) = 1 -(n, d) 1 -(p, d)<label>(2)</label></formula><p>If the denominator vanishes, the value 10 6 is used as a default.</p><p>In practice this was more effective with a "Quitting" rule that cust off submission if, after the first 50 documents are submitted, we have not achieved a postive utility score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BATCH FILTERING: BOOLEAN MODEL</head><p>Assume that there are n &gt; 0 distinct terms in the document collection and associate an index in V = {1, 2, . . . , n} with each of these terms. Letting B = {0, 1}, we represent each document in the collection as an n-dimensional Boolean vector x ∈ B V . Each component of x corresponds one of the distinct terms in the document collection, with xi = 1 if the i th term is present in the document and xi = 0 if the i th term is absent from the document.</p><p>For a subset S ⊆ V , and vector a ∈ B V , we shall let a[S] ∈ B S denote the projection of a onto S and for X ⊆ B V we shall write X[S] as the projection of X on S, that is,</p><formula xml:id="formula_7" coords="4,53.76,57.00,96.44,8.97">X[S] = {a[S] | a ∈ X}.</formula><p>For a subset S ⊆ V let us denote by χ S ∈ B n its characteristic vector, i.e.</p><formula xml:id="formula_8" coords="4,126.12,84.72,88.53,19.41">χ S j = 1 if j ∈ S,<label>0 otherwise.</label></formula><p>We shall refer to the set of relevant documents as T and the set of irrelevant documents as F and shall assume that T ∩ F = ∅, that is, there do not exist vectors a ∈ T and b ∈ F such that a = b.</p><p>A set S ⊆ V is said to be a support set for T and F if it has the property that T [S] ∩ F [S] = ∅. That is, S is a support set if each relevant document represented in terms of the selected features subset can be distinguished from each irrelevant document represented in terms of the selected features subset.</p><p>The document model described above does not preserve information about the order in which terms appear in the document and therefore is often referred to as the bag-ofwords representation. In addition, the Boolean nature of this representation lies in contrast to a popular representation known in the information retrieval literature as the vector space model, in which the components xi correspond to the (relative) frequency of the term in the document. (3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Measure of Separation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ranking Functions</head><p>For each i ∈ V , each of the ranking functions presented here utilizes the following four values</p><p>• ai ≡ the number of relevant documents containing the i th term</p><p>• bi ≡ the number of irrelevant documents containing the i th term</p><p>• ci ≡ the number of relevant documents which do not contain the i th term</p><p>• di ≡ the number of irrelevant documents which do not contain the i th term</p><p>For each i ∈ V , the relationship between ai bi, ci and di and the document collection is given by the following 2 × 2 contingency table</p><formula xml:id="formula_9" coords="4,70.08,678.12,206.04,42.33">y ∈ T y ∈ F xi = 1 ai bi ai + bi = θi xi = 0 ci di ci + di = θi ai + ci = |T | bi + di = |F | m</formula><p>where the marginals θ and θi represent the number of documents containing the i th term and the number of documents which do not contain the i th term respectively, and y ∈ B is defined as</p><formula xml:id="formula_10" coords="4,392.16,103.32,82.41,19.41">y = 1 if x ∈ T, 0 otherwise.</formula><p>Obviously, the marginals |T | and |F | are constant for all terms while the marginals θi and θi vary for each term. The total number of documents in the collection is m = ai + bi + ci + di which is obviously also a constant.</p><p>For the simplicity of notations, we shall view all ranking functions as functions of the four parameters a, b, c and d, though clearly there are only two independent values among these.</p><p>In <ref type="bibr" coords="4,338.51,214.92,9.59,8.97" target="#b2">[2]</ref> we analyzed and compared a number of possible ranking functions, and based on that study, we selected 5 such functions for this TREC experiment:</p><p>Function</p><formula xml:id="formula_11" coords="4,364.30,246.24,191.65,35.37">α α = a a + c - b b + d = |ad -bc| |T ||F | (4)</formula><p>is the absolute value of the difference between the number of relevant-irrelevant document pairs in the training collection which provide evidence that the i th term is a good classifier of relevant documents and the number of relevant-irrelevant document pairs which provide evidence that the i th term is a good classifier of irrelevant documents, normalized by the total number (i.e. both correctly distinguished and incorrectly distinguished) of relevant-irrelevant document pairs. Function</p><formula xml:id="formula_12" coords="4,329.52,372.24,226.43,36.09">β β = ad + bc (a + c)(b + d) = ad + bc ab + ad + bc + cd = ad + bc |T ||F |<label>(5)</label></formula><p>is the total number of relevant-irrelevant document pairs correctly distinguished by the i th term, normalized by the total number of relevant-irrelevant document pairs in the training collection.</p><p>Function</p><formula xml:id="formula_13" coords="4,364.30,456.96,127.33,36.09">γ γ = ad (a + c)(b + d) = ad |T ||F |</formula><p>is an obvious variant of both α and beta.</p><p>Function</p><formula xml:id="formula_14" coords="4,333.36,510.24,222.59,39.69">δ δ = |ad -bc| (a + b)(c + d)(a + c)(b + d) = ad -bc θθ|T ||F | (6)</formula><p>is the absolute value of the Pearson Product Moment Correlation coefficient or simply the correlation coefficient for the Boolean variables xi and y as defined above. It measures the degree to which these two variables have a linear relationship.</p><p>Function</p><formula xml:id="formula_15" coords="4,340.68,611.40,215.27,38.73">ρ ρ = (a + b + c + d) (ad -bc) 2 (a + b)(c + d)(a + c)(b + d) = m(ad -bc) 2 θθ|T ||F | (7)</formula><p>is the χ 2 statistic for the Boolean variables xi and y as defined above and provides another measure of association for these two variables. Note that ρ is a onotone funcion of δ, so that our precedure, as described below, effectively gives a "double weight" to this particular measure of effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training the Batch Classifier</head><p>This section describes the feature selection method and the simple classifier used in the batch filtering sub-task of the filtering track.</p><p>The set of unique terms in the training set T ∪ F was ranked by each of the five ranking functions α, β, γ, δ, ρ described in §4.2. Five intermediate feature sets, Sα, S β , Sγ , S δ , Sρ, were constructed using the top ranking K = 50 terms of the corresponding ranking functions. Letting S = Sα ∪ S β ∪ Sγ ∪ S δ ∪ Sρ we assigned a score ψ ∈ {1, • • • , 5} to each of the terms in tiledS, defined as the number of sets S ξ , ξ ∈ {α, β, γ, δ, ρ} in which the term appeared. The final feature set S was constructed by selecting the K = 50 terms with the highest ψ scores.</p><p>Next, to each term in i ∈ S we assigned the weight</p><formula xml:id="formula_16" coords="5,142.32,242.60,59.63,29.14">ω(i) = a i +0.5 a i +c i +1 b i +0.5 b i +d i +1</formula><p>which can be seen to be the Bayesian weight of evidence, and to each document y ∈ T [S] ∪ F [S] we assigned the score</p><formula xml:id="formula_17" coords="5,127.20,305.76,92.28,18.57">Ω(y) = j∈S log(ω(i))yj.</formula><p>That is, each document projected onto the selected feature set S is assigned a score equal to the sum of the logarithms of the Bayesian weights of evidence for the terms it contains.</p><p>The batch filtering task requires the definition of a static classification rule which specifies whether each document in the test set should be considered relevant and retrieved, or irrelevant and ignored. The rule we utilized specifies that y ∈ T [S] ∪ F [S] will be retrieved if and only if Ω(y) ≥ τ for some τ ∈ R. The threshold τ was selected so as to optimize the utility measure T U 11 = 2R -I over the training set, where R is the number of relevant documents retrieved by the system and I is the number of irrelevant documents retrieved by the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Filtering Results</head><p>Our training results, using a variety scoring measures, for a great variety of training runs, are shown at the end of the paper in Table <ref type="table" coords="5,114.93,535.08,3.54,8.97" target="#tab_2">2</ref>. In the final analysis, our results at TREC were in the middle of the pack.</p><p>These are summarized in Table <ref type="table" coords="5,192.17,555.96,3.54,8.97" target="#tab_1">1</ref>. The best results were achieved by the run submitted as dimacs11a30Q. This was a Rocchio method, trained on a set of documents similar to the one used at TREC. The " 30" indicates that only the top 30 terms, that is, the 30 terms with highest weight in the updated query vector were included. " AP" includes only the terms with positive weight are retained. " Q" indicates that for our final submission we cut off submission if we did not achieve a positive score after submitting 50 documents for judgment. " P1Q" used a ratio scoring scheme, together with the " quit at 50 if score is negative rule. This is, of course, a " TREC strategy" and not a procedure that would be useful in a real world application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We have subsequently learned that with proper learning parameters, as chosen by the group from the Chinese Academy of Sciences, it is possible for a Rocchio approach similar to ours to achieve very good results. We are not certain as to which steps of our approch blocked us from realizing this high level of performance. One possibility is that even the small number of pseudo-negative cases that we introduce into the training is sufficient to keep us away from the region of good performance. Another is that the space of parameters is too large, and the dependence of the learning too complex, to be successfully explored "one variable at a time", which was essentially the heuristic used. Other inhibiting factors may have included the heursitcs used to cut off submission if we did not achieve a positive score after the first 50 judgments. Nonetheless, our submission that did use this heuristic fared better than those that did not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Batch Results</head><p>On the assessor judged topics our TU11 score was less than the median fifteen times, equal to the median twenty times, and greater than the median fifteen times and never attained the maximum.</p><p>On the intersection topics our TU11 score was less than the median once, equal to the median six times, and greater than the median forty-three times and attained the the maximum twenty-one times. Unfortunately, for many of these topics, submitting no documents at all was an effective TREC strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>This work is part of a larger effort to develop an array of approaches to filtering problems, and to integrate or fuse them for greater effectiveness. In this first effort it would appear that we have adopted tools that are capable of "state of the art" perfromance on the adaptive filtering task, but have not yet learned how to ensure that this level of performance is achieved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,62.76,101.40,74.48,8.97;2,62.76,111.84,108.52,8.97;2,62.76,122.28,230.00,8.97;2,53.76,132.72,204.22,8.97;2,62.76,143.16,230.00,8.97;2,53.76,153.72,239.00,8.97;2,53.76,164.16,99.77,8.97"><head></head><label></label><figDesc>FOR EACH Topic B1. Read topic description B2. Read initial 3 positive training examples for this topic. Give each of these examples a weight of 1.0. B3. Call scoring model learning algorithm (Rocchio) to produce linear model based on the topic description and the initial positive examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,353.15,238.68,202.66,8.97;2,316.80,249.12,238.78,8.97;2,316.80,259.56,238.99,8.97;2,316.80,270.12,238.53,8.97;2,316.80,280.56,238.99,8.97;2,316.80,291.00,237.68,8.97"><head></head><label></label><figDesc>n is the number of training examples, C = {1 ≤ i ≤ n : yi = 1} is the set of positive training examples (i.e., members of the class of interest), and nC is the number of positive training examples. The parameters α, β, and γ control the relative impact of the original weight vector, the positive examples, and the negative examples, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,62.76,322.68,230.03,8.97;4,53.76,331.73,215.75,10.36;4,275.28,333.12,17.62,8.97;4,53.76,342.29,10.55,8.01;4,65.16,343.56,227.46,8.97;4,53.76,354.00,238.86,8.97;4,53.76,364.56,239.06,8.97;4,53.76,375.00,239.00,8.97;4,53.76,385.44,160.68,8.97;4,62.76,395.88,230.02,8.97;4,53.76,406.32,160.77,8.97;4,227.28,411.32,34.80,6.82;4,264.96,406.32,27.95,8.97;4,53.76,419.04,238.90,8.97;4,53.76,429.48,70.15,8.97;4,104.52,448.92,43.06,8.97;4,161.04,443.16,4.55,8.97;4,151.32,455.04,23.99,8.97;4,178.08,461.81,28.91,5.68;4,209.40,448.92,32.74,8.97"><head></head><label></label><figDesc>For a subset S ⊆ V , we measure the distance between the projections T [S] and F [S] of the sets T and F ∈ B V onto B S , by the so called average Hamming distance. The use of Hamming distance based separation, rather than measures based on the l1, l2 or l∞ norms, as is often the practice when the employing the vector space model, is suggested by the Boolean nature of our document model.The Hamming distance between the vectors a[S] ∈ T [S] and b[S] ∈ F [S] is defined as dS(a, b) = j∈S:a j =b j 1. The average Hamming distance between the sets T [S] and F [</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,53.76,573.84,238.93,94.21"><head>Table 1 :</head><label>1</label><figDesc>TREC 2002 Results for the Assessor topics, various runs</figDesc><table coords="5,116.28,573.84,113.95,63.81"><row><cell></cell><cell>Mean T11</cell></row><row><cell>dimacsddl02a</cell><cell>0.110</cell></row><row><cell>dimacs11aAPQ</cell><cell>0.142</cell></row><row><cell>dimacsddl02b</cell><cell>0.293</cell></row><row><cell>dimacs11aP1Q</cell><cell>0.272</cell></row><row><cell>dimacs11a30Q</cell><cell>0.337</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,316.80,580.08,240.84,71.73"><head>Table 2 :</head><label>2</label><figDesc>.75 ND+.best.f 17 -47 473 9.8540 0.1990 0.2970 32 24 48 231 U-0.75 ND-.best.f 16 -47 430 8.9580 0.1980 0.2940 32 24 48 212 U-0.75 NW+.best.f 12 -48 377 7.8540 0.1900 0.2930 29 23 48 256 U-0.75 NW-.best.f 13 -47 314 6.5420 0.1910 0.2860 32 25 48 169 U-0.75 PD-.best.f 13 -54 273 5.8090 0.1640 0.2980 27 18 47 195 U-0.75 PW+.best.f 14 -67 181 3.6200 0.1930 0.2830 34 23 50 206 U-0.75 PW-.best.f 15 -26 322 6.7080 0.1850 0.2930 30 24 48 170 U-0.75 def.best.f 16 -47 427 8.8960 0.1980 0.2940 32 24 48 212 Utility scores of running Rocchio with different parameters</figDesc><table coords="5,316.80,580.08,240.84,71.73"><row><cell>This research is supported in part by the National Sci-</cell></row><row><cell>ence Foundation, which is not, however, responsible for any</cell></row><row><cell>positions expressed in this report. We thank our colleagues</cell></row><row><cell>in the Monitoring Message Streams project: Fred Roberts</cell></row><row><cell>(Principal Investigator), David Madigan, Ilya Muchnik, S.</cell></row><row><cell>Muthukrishnan, Rafi Ostrovsky, and Martin Strauss for help-</cell></row><row><cell>ful conversations.</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>* Research supported in Part by the <rs type="funder">National Science Foundation</rs> under Grant <rs type="grantNumber">NumberEIA-0087022. PBK</rs> and KBN are supported in Part by <rs type="funder">Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program</rs> under contract number <rs type="grantNumber">2002-H790400-000</rs>, the <rs type="projectName">HITIQA</rs> project, of <rs type="affiliation">SUNY Albany and Rutgers</rs>. EB is supported in part by the <rs type="funder">Office of Naval Research</rs> (Grant <rs type="grantNumber">N00014-92-J-1375</rs>). The views expressed in this article are those of the authors, and do not necessarily represent the views of the sponsoring agency. Author Affiliations: d <rs type="institution">= c Division of Computer Sciences, Rutgers</rs>, the <rs type="funder">State University of New Jersey</rs>; i Rutgers Center for Operations Research; ( m ). Independent Consultant, Chicago IL. a . Independent Consultant, Penticton, <rs type="institution">British Columbia Canada</rs>; s SCILS and <rs type="institution">DIMACS, Rutgers</rs>, the <rs type="funder">State University of New Jersey</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_h7RZj5v">
					<idno type="grant-number">NumberEIA-0087022. PBK</idno>
				</org>
				<org type="funded-project" xml:id="_UKq6F9U">
					<idno type="grant-number">2002-H790400-000</idno>
					<orgName type="project" subtype="full">HITIQA</orgName>
				</org>
				<org type="funding" xml:id="_DwTmBRq">
					<idno type="grant-number">N00014-92-J-1375</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,321.29,665.15,96.47,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.64,679.20,207.32,8.97;5,335.65,689.64,206.36,8.97;5,335.65,700.08,192.06,8.97;5,335.65,710.64,199.61,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,510.94,689.64,31.06,8.97;5,335.65,700.08,146.16,8.97">Finding essential attributes from binary data</title>
		<author>
			<persName coords=""><forename type="first">Endre</forename><surname>Boros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Takashi</forename><surname>Horiyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Toshihide</forename><surname>Ibaraki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kazuhisa</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mutsunori</forename><surname>Yagiura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,489.12,700.38,38.59,8.26;5,335.65,710.94,155.37,8.26">Annals of Mathematics and Artificial Intelligence</title>
		<imprint/>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct coords="6,72.59,57.00,208.15,8.97;6,72.60,67.56,205.06,8.97;6,72.60,78.00,147.61,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,202.93,57.00,77.81,8.97;6,72.60,67.56,130.03,8.97">Rank based feature selection in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Endre</forename><surname>Boros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">J</forename><surname>Neu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>RUTCOR, Rutgers University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="6,72.59,89.40,212.73,8.97;6,72.60,100.14,175.25,8.26;6,72.60,110.40,151.19,8.97" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">B</forename><surname>Frakes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<title level="m" coord="6,72.60,100.14,175.25,8.26;6,72.60,110.70,41.84,8.26">Information Retrieval: Data Structures and Algorithms</title>
		<imprint>
			<publisher>Prentice-Hall PTR</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,121.80,212.84,8.97;6,72.60,132.24,203.02,8.97;6,72.60,142.68,211.73,8.97;6,72.60,153.24,193.24,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,140.34,121.80,145.09,8.97;6,72.60,132.24,93.56,8.97">Relevance feedback and other query modification techniques</title>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,194.55,142.98,89.79,8.26;6,72.60,153.54,126.35,8.26">Information Retrieval: Data Structures and Algorithms</title>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Frakes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="241" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,175.08,220.06,8.97;6,72.60,185.52,215.19,8.97;6,72.60,195.96,197.31,8.97;6,72.60,206.52,179.01,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,126.21,175.08,166.44,8.97;6,72.60,185.52,92.58,8.97">Feature Selection and Feature Extraction for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,184.23,185.82,103.56,8.26;6,72.60,196.26,111.57,8.26">Proceedings of Speech and Natural Language Workshop</title>
		<meeting>Speech and Natural Language Workshop<address><addrLine>San Mateo, California</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="212" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.60,217.92,206.02,8.97;6,72.60,228.36,20.74,8.97" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m" coord="6,146.36,218.22,70.54,8.26">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,239.88,212.31,8.97;6,72.60,250.32,175.64,8.97" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,162.64,240.18,122.26,8.26;6,72.60,250.62,78.46,8.26">Introduction to Statistics: The Nonparametric Way</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gottfried</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Noether</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,261.72,192.57,8.97;6,72.60,272.28,206.70,8.97;6,72.60,282.72,214.96,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,225.35,261.72,39.81,8.97;6,72.60,272.28,100.75,8.97">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,180.76,272.58,98.54,8.26;6,72.60,283.02,125.60,8.26">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,294.12,209.96,8.97;6,72.60,304.56,193.54,8.97;6,72.60,315.42,179.71,8.26;6,72.60,325.56,209.10,8.97;6,72.60,336.00,165.38,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,144.78,294.12,137.78,8.97;6,72.60,304.56,31.94,8.97">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,214.45,304.86,51.69,8.26;6,72.60,315.42,179.71,8.26;6,72.60,325.86,83.52,8.26">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<editor>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood Cliffs, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall, Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,347.40,201.94,8.97;6,72.60,357.96,205.17,8.97;6,72.60,368.40,127.00,8.97;6,72.60,378.84,159.73,8.97" xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thi</forename><forename type="middle">Nhu</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fangfang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lemur</surname></persName>
		</author>
		<ptr target="http://www-2.cs.cmu.edu/lemur" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
