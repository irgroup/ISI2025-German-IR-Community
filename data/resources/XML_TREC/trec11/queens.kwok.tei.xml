<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.00,87.46,432.01,12.64">TREC2002 Web, Novelty and Filtering Track Experiments using PIRCS</title>
				<funder ref="#_ASnq2jz">
					<orgName type="full">Space and Naval Warfare Systems Center San Diego</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,203.04,121.38,50.60,10.80"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.10,121.38,35.76,10.80"><forename type="first">P</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.64,121.38,44.00,10.80"><forename type="first">N</forename><surname>Dinstl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,366.87,121.38,41.99,10.80"><forename type="first">M</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.00,87.46,432.01,12.64">TREC2002 Web, Novelty and Filtering Track Experiments using PIRCS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0A8EE377E57615A410775EA7419F871C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In TREC2002, we participated in three tracks: web, novelty and adaptive filtering. The Web track has two tasks: distillation and namedpage retrieval. Distillation is a new utility concept for ranking documents, and needs new design on the output document ranked list after an ad-hoc retrieval from the web (.gov) collection. Novelty track is a new task that involves identifying relevant sentences to a question, and to remove duplicate or nonnovel entries in the answer list. The third track is adaptive filtering. We revived a filtering program that was functional at TREC-9 with some added capability. Sections 2, 3, 4 describe our participation in these tracks respectively. Section 5 has our conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Web Track</head><p>This year the web track involves two tasks: topic distillation and named-page finding. Named-page finding is similar to last year's home page finding <ref type="bibr" coords="1,179.08,517.78,12.91,9.94" target="#b0">[1]</ref> except that an answer page may be a sub-site address containing what the user wants that is named in the query. Topic distillation is new, and is concerned with locating the most useful pages (out of many) that best and comprehensively describe a user's topic, either by content or via links. Previous investigations on topic distillation such as <ref type="bibr" coords="1,185.11,619.06,12.64,9.94" target="#b1">[2,</ref><ref type="bibr" coords="1,197.75,619.06,8.42,9.94" target="#b2">3,</ref><ref type="bibr" coords="1,206.17,619.06,8.42,9.94" target="#b3">4,</ref><ref type="bibr" coords="1,214.59,619.06,8.42,9.94" target="#b4">5]</ref> mostly tie the process to 'quality' identification. They employed Kleinberg's HITS <ref type="bibr" coords="1,222.28,644.26,12.94,9.94" target="#b5">[6]</ref> algorithm as the primary method, and added content weighting as secondary improvement. Authority and hub pages found were identified with topic distillation answers. In this experiment, we employ page content weighting (including anchor texts) as our primary process, and add out-link content weight to help determine answers. This is based on the description of the task as given in the Guidelines for TREC-2002 Web Track (http://trec.nist.gov).</p><p>The collection for this years' web task is the .gov collection, a recent crawl (early 2002) on the government domain web pages. It consists of nearly 1.3 million pages totaling about 10 GB. The file was processed to our internal format and broken up into about 3 million sub-documents. A dictionary of over 5.8 million terms was produced including some 2-word phrases. This was truncated to about 1.4 million by ignoring terms with frequency 2 or less, or greater than 600,000. As usual, 50 topics (later truncated to 49) were used for retrieval. We experimented with short queries employing only the title section of each topic as queries. They averaged to ~3.5 terms after stemming and removal of stop-words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Improved Web Retrieval</head><p>Over the last two TREC conferences, PIRCS has provided about average performance in the Web track. The Web10g collection scale is much larger than the 2 GB that we have been accustomed to in previous ad hoc tracks, and the web page genre is very different from newspaper type. We spent some effort to try to analyze the situation, and test various parameter settings in order to understand the problem and to improve retrieval results for 10-GB scale web collection. It turns out that the major cause of lackluster web retrieval performance with PIRCS is due to a wrong setting of the high Zipf threshold that is used for screening out high frequency indexing terms -so called statistical stop-words. This threshold was previously set at 180,000 (about 18% of the number of documents) in order to gain better efficiency with our network implementation of PIRCS. After upgrading our system with 512 MB of memory and setting this threshold at a high 500,000 to include more terms, mean average precision (MAP) improved substantially for both short and long queries for Trec-9 and Trec-2001 web experiments as tabulated in Table <ref type="table" coords="2,262.65,200.98,4.61,9.94" target="#tab_0">2</ref>.1 due to this single parameter change. Loss of indexing terms is a major cause for unsatisfactory results. Additional gains were observed, when pseudo-relevance feedback parameters were optimized, for example. The improved procedures are employed for this year's web tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distillation Task</head><p>According to the track description, the purpose of topic distillation is to find the 'key resource' page(s) for a given topic. The concept of 'key resource' has been described in the Guideline for TREC-2002 Web Track (http://trec.nist.gov). Examples may be a page with outstanding content, or one with outlinks to good content pages on the topic. Content may be less important than useful links in a page, and in general answers are diversified so that a relevant host site may not have many distillation page(s).</p><p>Our strategy for this task is to a) first find the best content pages for a topic; and b) add link processing to find diversified key resources among these pages. The first step makes use of our normal ad-hoc retrieval ranking since it is content-oriented. The second step involves identifying the importance of linked content for each page. These steps are described below.</p><p>To make use of the structured property of web data, we create four different collections by separating each web page into four objects identified by the same DocID: title, text, meta and href objects. 'Title', 'text', and 'meta' (whose metadata content is usually not for display) are obtained from the appropriate tag fields of the page. For the 'href' collection, each document is composed of anchor texts from different pages that link to one particular URL. This URL is then mapped to a unique DocID using the 'url2id' file provided. 'href' therefore defines a page based on its in-link anchor content irrespective of what the page itself may contain. The 'text' and 'href' collections are processed with Porter's stemming, while 'title' and 'meta' are left unstemmed. The purpose is to obtain higher precision with the latter two shorter documents.</p><p>We form a query from only the title field of a topic. This is a required submission. The query (stemmed or un-stemmed) is used to rank items from each of the four collections using our PIRCS system, and four ad-hoc retrieval lists are obtained.</p><p>To satisfy the desired diversified key resource property, we form host groups. A host group contains pages having the same host address. The DocID of each retrieved page is converted to URL. URL addresses allow us to merge and categorize pages into a set H of host groups each with varying number of pages. Since relevant content documents usually occur in the top part of a retrieval list, we limit key resource finding to the top 100 pages of each of the 4 lists except for 'meta', which is limited to the top 10. The 'meta' collection may be less reliable than the others. These form a best-page candidate pool and organized into host groups.</p><p>Each unique page has four normalized retrieval status value values (RSV) (including zero when it does not appear on some retrieval lists). Each RSV is normalized to lie between 0 and 1 by dividing by the sum of the top 1000 RSV's. Later, another normalization based on transformation by the function g(RSV) = exp(a+b*RSV)/[1+exp(a+b*RSV)] was tried and it performs better. We combine the normalized RSV values to form a weight called A-wt (content) for a page according to the following criteria:</p><p>If (page-type== graphic ('giff', etc.))</p><p>A-wt=0 else if (page-type==HTML) A-wt = 0. </p><p>We assume that the A-wt can characterize roughly how content-relevant a page is to the retrieval topic. Another weight called B-wt (link) is also assigned to each page based on its out-links and defined as follows:</p><formula xml:id="formula_1" coords="3,126.00,454.04,152.59,11.19">B-wt = Σ out-links (A-wt) (<label>2</label></formula><formula xml:id="formula_2" coords="3,278.60,454.78,4.31,9.94">)</formula><p>The sum is over links pointing within the candidate pool only, not to the collection. We assume the B-wt can characterize roughly how strong a page's link content is and its contribution to its distillation power.</p><p>Each member h of H is also assigned a weight equal to the Σ pages-in-h (A-wt)/sqrt(n) for all the n pages within the host. Thus, host groups can be ranked for content. Within each group, pages are ranked by their combined (A-wt + B-wt), which we call page weight. Thus, a page may have little content (i.e. small A-wt), but if it points to many useful pages, its B-wt can be large, ranking it higher among peer pages within a group based on its page weight. A picture of the candidate pool organized as weighted host groups is shown in  To form the answer list for the distillation task, we adopt two strategies resulting in our submissions pirc2Wd1 and pirc2Wd2 (the tag has the meaning: pircs-year02-webdistillation-run#). For pirc2Wd1, the top one page from each of the best 10 host groups are listed first, followed by padding it with other pages sorted by page weights. The second submission pirc2Wd2 uses the top 2 pages from each of the top 10 host groups, sorted by page weight to define the top 20 answers, then followed by padding as in pirc2Wd1.</p><p>Our approach of forming weighted host groups and then using page weight to sort pages within a group, is designed to find key resource page(s) within the most contextually relevant hosts. Forming the answer list by selecting top page(s) from each group is designed to allow diversification in our distillation answer list. Other methods to form answer lists may also be employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussions</head><p>Table <ref type="table" coords="3,346.79,542.74,4.61,9.94" target="#tab_0">2</ref>.2 presents official evaluation of our distillation experiments using precision at 10, 20 and 30 documents retrieved values. It is seen that the second approach of selecting 2 top pages from each host group has much better performance, especially at P10 (0.1082 vs. 0.0816). The first approach suffers from too much diversification because: a) it is risky to assume that statistical ranking can always position key resource pages to the top of each group; b) many queries do have multiple same-host answers, and output of single page from each host artificially diminishes the chance of putting key resources in top 10. After results are known, we fix a bug in our program and employ better RSV normalization to attain a P10 value of 0.1204 as shown in Table <ref type="table" coords="4,179.65,215.62,4.15,9.94" target="#tab_0">2</ref>.3. We had thought that 'title' and 'href' retrieval would be more accurate and weigh them higher in <ref type="bibr" coords="4,264.64,240.94,11.78,9.94" target="#b0">(1)</ref>. In reality, 'text' retrieval remains far superior. When the coefficients for combining RSV's among the four collections to define A-wt were set to 0.65 ('text'), 0.15 ('href'), 0.15 ('title'), and 0.05 ('meta'), and also normalizing the B-wt by the number of outlink edges, the P10 value jumped to 0.1673. When 3 pages are selected from each host (instead of 2), or with no host restriction (just use A-wt + B-wt for ranking), P10 values continue to improve to 0.1735 and 0.2204 respectively. However, when only the content weight (A-wt) is used, also ignoring host groups, distillation result is very similar to using A-wt + B-wt. It seems that a) out-link content (B-wt) is not necessary for key resource detection (using A-wt only performs almost as well); and b) host grouping leads to worse performance. The latter point should be viewed in the context that out of 1574 key resource answers for the <ref type="bibr" coords="4,205.63,506.62,11.06,9.94">49</ref>  host groups would depress the chance of getting relevant answers within the top 10 retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Named-Page Task</head><p>The objective of the named-page task is to retrieve an appropriate page(s) that contains answers to wanted item(s) named in a query.</p><p>There are 150 topics and they all vary between two to six words long. We submitted two runs for this task based on the processing methodology of the distillation task called: pirc2Wnp1 and pirc2Wnp2. The first method outputs 50 top documents from the collections 'title' and 'href' (total 100). A-wt is defined for each page, and the top 50 according to Awt is returned as the answer list. The second method selects top 10 from the 'meta' collection, top 100 from each of 'title', 'text' and 'href' collections (total 310). These are grouped into hosts as in distillation task. Top 5 pages from each of top ten hosts are selected; these are sorted by page weight and returned as the answer list. The lackluster result can be traced again to our wrong emphasis on the 'title' and 'href' collections only. After results are known, we change our processing to include 50 documents each from the collections except 'meta', use the modified combination coefficients to define A-wt as discussed in Section 2.2, and output the top 50 according to A-wt. The MRR value doubled to 0.525, and 96 queries had correct answers in top 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Novelty Track</head><p>A new track called novelty task is defined this year. Given a query, its objective is to first rank and detect relevant sentences from a given set of sentences (that have been obtained from relevant documents of the query). The system next tries to identify among these sentences in an ordered fashion, those that contain novel information --those not novel are removed from the list. This is done after sorting the relevant sentences by document and sentence# order. The objective of this task has similarity to previous work done such as duplicate document removal in IR <ref type="bibr" coords="5,106.45,602.14,11.80,9.94" target="#b6">[7]</ref>, first-story detection in TDT <ref type="bibr" coords="5,264.75,602.14,11.80,9.94" target="#b7">[8]</ref>, or redundancy detection in adaptive filtering <ref type="bibr" coords="5,277.72,614.74,11.77,9.94" target="#b8">[9]</ref>.</p><p>For this experiment, we employ all sections of a topic to form long queries for retrieval because the 'documents' are actually short sentences. The queries average to 19.14 unique terms. Since the sentences come from relevant documents of TREC-8, we use the TREC-8 dictionary to provide better statistics for processing and retrieval. However, the high Zipf threshold has been reset to 400,000 to include more high frequency terms as discussed in Section 2.1.</p><p>Only initial retrieval without pseudorelevance feedback was performed. Based on experimentation with the four training topics, we test two RSV threshold (tr) values on the ranked retrieval list to help decide on the relevance of retrieved sentences: submission pircs2N0{1,2} employ tr=1.25, and pircs2N0{3,4} use tr=1.5. Thus, retrieved sentences with RSV &gt; tr are considered relevant.</p><p>This set of relevant sentences is sorted according to DocID and sentence#. For each sentence, every one of its un-stemmed words is expanded with synonyms by consulting with WordNet. All senses of the noun type are used. The resultant set of words is sorted, and duplicates removed. A double loop passes down the sentence list, and a novelty coefficient based on the Dice formula is evaluated for each pair of sentences Si and Sj: <ref type="figure" coords="5,437.78,444.34,79.62,9.94">----------------(3)</ref> |Si union Sj| If v &lt; a threshold tv, Sj is considered novel with respect to Si, otherwise Sj is removed. pircs2N01 and pircs2N03 employ a threshold tv=0.35 (originally documented as 0.3), and pircs2N02, pircs2N04 use tv=0.5. In addition, a fifth submitted run pircs2N05 does not use synonyms, just raw words, and acts as control with thresholds set to tr=1.5, tv=0.3.</p><formula xml:id="formula_3" coords="5,330.61,431.62,166.27,22.66">| Si interset Sj| v = Novelty coeff. = -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussions</head><p>Five runs were submitted to the novelty track labeled as pircs2N??, where ?? range from 01 to 05.  Fig. <ref type="figure" coords="6,121.64,480.46,4.46,9.94">3</ref>.1 plots the variation of P and R vs. threshold tr. Although P*R is not the same as ΣPq*Rq, one can nevertheless gains some idea of the result. P and R have very good linear fit for this retrieval environment. It shows that as RSV threshold tr changes from 1.25 to 1.5, P*dR/d(tr) drops faster than R*dP/d(tr) rises, leading to a fall of P*R value. If tr were set to 1.1 (or less), ΣPq*Rq improves to a value of 0.81.</p><p>After relevance determination, the set of sentences is passed to novelty processing. Results of using two novelty thresholds: tv = 0.35 and 0.5 are shown in Table <ref type="table" coords="6,253.04,659.26,4.14,9.94" target="#tab_5">3</ref> instead of 0.3 as documented during submission; 2) the submitted run pircs2N05 (which was supposed to be WordNet free) was actually identical to run 03. The correct run denoted as 05* was not submitted, but its result is shown in Table <ref type="table" coords="6,429.66,281.74,4.15,9.94" target="#tab_5">3</ref>.2. From the table, it is seen that pircs2N02 has the better result among the 5 submissions. For our system, it seems preferable to increase the novelty threshold tv to 0.5 (rather than 0.35) so that two sets of sentence words (appropriately expanded with WordNet synonyms) need to have larger overlap before they are considered similar and not novel (3). This, together with an RSV threshold of tr=1.25 produces a ΣPq*Rq value of 0.069. We have plotted the variation of novelty precision and recall values against the threshold tv in Fig, <ref type="figure" coords="6,412.17,613.30,4.47,9.94">3</ref>.2 for three relevance thresholds 1.1, 1.25 and 1.5. It is seen that novelty precision value P is practically constant over a large range of tv values, and they do not vary too much with respect to the tr threshold: 0.14 to 0.17. Apparently, as the tv threshold is changed, correctly identified novel sentences and incorrect ones are included at the same rate. However, as more sentences are accounted, novelty recall improves. This suggests one should set the relevance threshold tr low (like 1.1 or lower) to recall more relevant sentences, and also set the novelty threshold tv high (like 0.9) to include more sentences as novel. At tr=0.9 and tv=0.9, the official measure ΣPq*Rq evaluates to a value of 0.76, an improvement of nearly 12% over our best submitted results. This is achieved based on high recall values. Precision ratios are low at 0.14 to 0.17.</p><p>The last line in Table <ref type="table" coords="7,216.43,239.74,4.61,9.94" target="#tab_5">3</ref>.2 (pircs2N05*) shows novelty detection of sentences without WordNet expansion of terms. The unstemmed words were used for overlap calculation <ref type="bibr" coords="7,142.31,291.22,11.78,9.94" target="#b2">(3)</ref>. It returns a value for ΣPq*Rq of 0.66, about 18% better than pircs2N03, showing that WordNet expansion is not good at these parameters. However, at the better parameters of (tr, tv)=(0.9, 0.9) they all return a ΣPq*Rq value of 0.76. If stemmed words were used, slightly worse performance was observed.</p><p>As an example of WordNet expansion, we illustrate (for Query 305 "most dangerous vehicles") with sentence #20 of document LA031689-0177: "stresses safe driving". These three words expand to: {emphasis, accent, tension, tenseness, stress, focus, strain}, {condom, rubber, safety, safe, prophylactic} and {drive, driving}. Thus good synonyms are brought in as well as many bad ones. A filter needs to be built to screen out unwanted senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptive Filtering Track</head><p>This year's adaptive filtering task makes use of the topics numbered R101-R200 to select documents in date order from the Reuter collection for the period October 1, 1996 to July 31, 1997. Adaptive filtering is difficult. A possible approach is to use a two-step strategy. At start when little knowledge is known, a simple adaptive threshold adjustment and profile re-weighting method is used. Later when sufficient relevant data is available, expand and train the profile to increase the prospect of selecting only the relevant ones. We also employ the dictionary from last year's Q&amp;A collection (in addition to those from training documents) as a basis for processing to ensure that most terms from the test collections are included.</p><p>Many considerations are needed for adaptive filtering. These include defining an initial profile together with an initial selection threshold to start the process, dynamically adapt the threshold to select or not to select a document for examination, adaptively train and expand the profile to tailor to the type of documents seen so far, determine how often these changes are to be made, and at the same time attempt to maximize the utility value. Apparently the adaptation of the filtering profile and that of the threshold are both useful. Improved profile does a better job in separating the relevant documents from the irrelevant ones, based on the probability or the RSV values assigned. Threshold adjustment helps to achieve a utility target for the selected documents. These are performed periodically after a number of documents have gone through the process.</p><p>Initial profile is defined using the raw topic and the three judged relevant documents from the training set. Once the filtering process begins, statistics of term usage is kept for all documents passing through. Moreover, for the documents selected, whether relevant or not, they are identified as a separate retrieval collection for threshold adjustment. We recompute the RSV of those documents based on the current profile and then adjust the threshold to provide us with the maximum utility in regard to the filtered documents. We then use that threshold to filter future incoming documents.</p><p>As more relevant documents are selected, we expand the profile by adding terms that have higher frequency in the filtered relevant. A maximum of 30 is set as a limit for the number of expanded terms.</p><p>We also keep track of precision values, both the global and local ones. Global precision is the precision from the start of the filtering to the current point while the local one contains only the precision for the last two update cycles. We think relevant documents are not distributed uniformly over the course of time but are clustered over certain regions in the timeline of the document stream. If the current local precision is significantly higher than the global one, we feel that we are in a region with relevant documents clustered and the filtering threshold should be lowered so that more relevant documents can be selected. On the other hand, if the global precision is significant higher. It means we are in a region where very few documents are relevant and one should tighten the threshold so that fewer irrelevants will be selected.</p><p>Lastly, a query term co-occurrence filtering method was implemented in addition to statistical filtering to aim at achieving better precision. Query term pairs were formed from the original topic using the title or description fields. During filtering, the presence of a query term pair in a document sentence is considered as evidence for selection even if RSV is somewhat less than the current threshold. Assume the current RSV threshold is T. Normally documents with RSV &gt; T will be selected for the user. This is now modified as follows:</p><p>If (docRSV &gt;= 1.5*T OR (0.9*T &lt; docRSV &lt; 1.5*T &amp;&amp; has-co-occurrence)) select-document; else reject-document;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>We submitted 4 runs pirc2F{01,02,03,04}. pirc2F03 and pircs2F04 are base runs without phrase filtering but using different initial parameters. pirc2F01 and pirc2F02 are based on pirc2F03 but with phrase filtering using a window of three sentences and whole document respectively. Results were not good, especially for the intersection topics. For example, the better run is pirc2F01 with mean scaled T11U = 0.154 for the 50 assessor topics and 0.047 for the 50 intersection topics.</p><p>Phrase filtering seems useful compared to not using it: average score for the two base runs is only about half of the two runs with phrase filters. The experimental results were low and we suspect programming bugs in some of our procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed an approach to finding answer pages for topic distillation in a collection of web documents based on the properties of 'key resource': emphasis on content, link information and host diversity in answer list. In novelty task, we employ a large dictionary with TREC-8 statistics to aid our retrieval with short sentences, and WordNet to help expand words with synonyms for evaluating similarity among sentences. A phrase filtering procedure was tested for the adaptive filtering task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,708.58,33.95,9.94"><head>Fig. 2</head><label>2</label><figDesc>.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,318.84,188.62,181.26,7.13"><head>Fig. 2 . 1</head><label>21</label><figDesc>Fig.2.1 Weighted Pages within Weighted Host Groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,130.56,454.11,138.20,5.33"><head>Fig. 3 . 1 :P</head><label>31</label><figDesc>Fig.3.1: Variation of P, R vs Relevance Threshold tr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,361.08,562.03,125.52,5.00"><head>Fig. 3 . 2 :</head><label>32</label><figDesc>Fig. 3.2: Variation of P,R vs Novelty Threshold tv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,91.80,315.36,201.42,142.53"><head>Table 2 .1 Improved Web Retrieval Results</head><label>2</label><figDesc></figDesc><table coords="2,100.80,315.36,181.27,116.72"><row><cell>Web track</cell><cell>Short</cell><cell>Long (all</cell></row><row><cell>Trec-</cell><cell>(title)</cell><cell>sections)</cell></row><row><cell>2001 (old</cell><cell>0.1742</cell><cell>0.1715</cell></row><row><cell>Zipf threshold</cell><cell></cell><cell></cell></row><row><cell>2001 (new</cell><cell>0.2039</cell><cell>0.2054</cell></row><row><cell>Zipf threshold)</cell><cell></cell><cell></cell></row><row><cell>9 (old</cell><cell>0.1750</cell><cell>0.2209</cell></row><row><cell>Zipf threshold)</cell><cell></cell><cell></cell></row><row><cell>9 (new</cell><cell>0.1818</cell><cell>0.2448</cell></row><row><cell>Zipf threshold)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,108.96,87.82,167.18,74.74"><head>Table 2 .2: Web Distillation Results (Submitted)</head><label>2</label><figDesc></figDesc><table coords="4,109.80,87.82,165.24,36.34"><row><cell></cell><cell>P10</cell><cell>P20</cell><cell>P30</cell></row><row><cell>pirc2Wd1</cell><cell cols="3">.0816 .0765 .0633</cell></row><row><cell>pirc2Wd2</cell><cell cols="3">.1082 .0857 .0741</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,90.00,506.62,205.20,198.70"><head>Table 2 .3: Web Distillation Results (Post- Evaluation)</head><label>2</label><figDesc>topics, 432 have unique host, 112 have duplicate hosts, 55 have three, and the rest (~48%) share four or more same host. Restricting result list from diverse</figDesc><table coords="4,90.00,570.58,201.24,96.33"><row><cell></cell><cell>P10</cell><cell>P20</cell><cell>P30</cell></row><row><cell>bug fix, better RSV</cell><cell cols="3">.1204 .1000 .1075</cell></row><row><cell>normalization</cell><cell></cell><cell></cell></row><row><cell>better combination</cell><cell cols="3">.1673 .1296 .1381</cell></row><row><cell>coeffs., normaliz B-wt</cell><cell></cell><cell></cell></row><row><cell cols="4">3 top pages each host .1735 .1551 .1367</cell></row><row><cell cols="4">No host: A-wt + B-wt .2204 .1816 .1517</cell></row><row><cell>No host: A-wt</cell><cell cols="3">.2184 .1837 .1490</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,316.80,416.85,205.18,291.49"><head>Table 2 .4: Web Named-Page Results (submitted) Results and DiscussionsTable 2 .</head><label>22</label><figDesc><ref type="bibr" coords="5,127.89,99.82,4.61,9.94" target="#b3">4</ref> summarizes results of the two runs.</figDesc><table coords="4,316.80,416.85,205.18,252.72"><row><cell></cell><cell cols="2">Pirc2Wnp1 Pirc2Wnp2</cell></row><row><cell>#of topics having</cell><cell>30</cell><cell>3</cell></row><row><cell>answer ranked 1</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>5</cell><cell>4</cell></row><row><cell>3</cell><cell>6</cell><cell>3</cell></row><row><cell>4</cell><cell>3</cell><cell>5</cell></row><row><cell>5</cell><cell>6</cell><cell>4</cell></row><row><cell>6</cell><cell>1</cell><cell>4</cell></row><row><cell>7</cell><cell>2</cell><cell>2</cell></row><row><cell>8</cell><cell>1</cell><cell>2</cell></row><row><cell>9</cell><cell>6</cell><cell>2</cell></row><row><cell>10</cell><cell>1</cell><cell>2</cell></row><row><cell>MRR</cell><cell>0.263</cell><cell>0.077</cell></row><row><cell>#topics with ans.</cell><cell cols="2">61(40.7%) 31(20.7%)</cell></row><row><cell>≤rank 10</cell><cell></cell><cell></cell></row><row><cell>#topics with ans.</cell><cell cols="2">95(63.3%) 65(43.3%)</cell></row><row><cell>≤rank 50</cell><cell></cell><cell></cell></row><row><cell>#topics with ans.</cell><cell cols="2">55(36.7%) 85(56.7%)</cell></row><row><cell>not found</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,316.81,646.66,205.35,60.58"><head>Table 3 .1: Relevant Sentence Decision Results (submitted)</head><label>3</label><figDesc></figDesc><table coords="5,316.81,646.66,205.35,60.58"><row><cell>Except for pircs2N05, all runs employ</cell></row><row><cell>WordNet to find synonyms to words in the</cell></row><row><cell>retrieved relevant sentences to decide for</cell></row><row><cell>novelty. Results of the submitted experiments</cell></row><row><cell>concerning decision on relevance is shown in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,90.00,75.22,427.97,644.49"><head>Table 3 .2: Novel Sentence Decision Result (submitted except for the corrected *05)</head><label>3</label><figDesc></figDesc><table coords="6,90.00,659.26,205.41,60.46"><row><cell>.2. Two</cell></row><row><cell>corrections need to be pointed out: 1) book-</cell></row><row><cell>keeping of the files during submission were</cell></row><row><cell>mixed up and the tv threshold for runs</cell></row><row><cell>pircs2N01 and 03 should have been 0.35</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by the <rs type="funder">Space and Naval Warfare Systems Center San Diego</rs>, under grant No. <rs type="grantNumber">N66001-1-8912</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ASnq2jz">
					<idno type="grant-number">N66001-1-8912</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,320.94,457.42,4.14,9.94;8,340.91,457.42,180.94,9.94;8,316.80,470.02,205.19,9.94;8,316.80,482.74,205.16,9.94;8,316.80,495.34,205.16,9.94;8,316.80,508.06,84.24,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,316.80,470.02,184.41,9.94">Overview of the TREC-2001 Web Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<idno>TREC 2001. NIST SP 500-250</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,316.80,482.74,205.16,9.94;8,316.80,495.34,93.51,9.94">Information Technology: The Tenth Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="61" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.94,533.26,201.06,9.94;8,316.80,545.98,205.23,9.94;8,316.80,558.58,205.31,9.94;8,316.80,571.30,205.15,9.94;8,316.80,583.90,53.49,9.94;8,370.32,581.49,5.40,6.26;8,378.36,583.90,103.79,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,359.08,558.58,163.02,9.94;8,316.80,571.30,205.15,9.94;8,316.80,583.90,15.50,9.94">Automatic resource compilation by analyzing hyperlink structure and associated text</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,338.95,583.90,31.34,9.94;8,370.32,581.49,5.40,6.26;8,378.36,583.90,53.92,9.94">Proc. 7 th WWW Conf</title>
		<meeting>7 th WWW Conf</meeting>
		<imprint>
			<date type="published" when="1998">1998a</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.94,609.22,201.08,9.94;8,316.80,621.82,205.28,9.94;8,316.80,634.54,205.20,9.94;8,316.80,647.14,205.19,9.94;8,316.80,659.86,178.64,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,424.18,634.54,97.82,9.94;8,316.80,647.14,46.41,9.94">Experiments in topic distillation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,377.78,647.14,144.21,9.94;8,316.80,659.86,172.73,9.94">ACM SIGIR&apos;98 Post Conf. Workshop on Hypertext IR for the Web</title>
		<imprint>
			<date type="published" when="1998">1998b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.94,685.06,200.98,9.94;8,316.80,697.78,198.12,9.94;9,90.00,74.50,205.10,9.94;9,90.00,87.10,80.76,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,316.80,697.78,198.12,9.94;9,90.00,74.50,110.44,9.94">Improved algorithm for topic distillation in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bharat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Henzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,209.74,74.50,85.36,9.94;9,90.00,87.10,19.87,9.94">Proc. ACM SIGIR 1998</title>
		<meeting>ACM SIGIR 1998</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.15,112.42,200.99,9.94;9,90.00,125.14,205.35,9.94;9,90.00,137.74,205.17,9.94;9,90.00,150.34,165.45,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,90.00,125.14,205.35,9.94;9,90.00,137.74,200.11,9.94">Does &quot;authority&quot; mena quality? Predicting expert quality ratings of web documents</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Amento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L &amp;</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,90.00,150.34,104.64,9.94">Proc. ACM SIGIR 2000</title>
		<meeting>ACM SIGIR 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.15,175.66,201.05,9.94;9,90.00,188.38,199.88,9.94;9,289.92,185.97,5.40,6.26;9,90.00,200.98,56.05,9.94;9,162.38,200.98,52.48,9.94;9,230.99,200.98,11.04,9.94;9,258.36,200.98,36.82,9.94;9,90.00,213.58,52.87,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,204.08,175.66,91.12,9.94;9,90.00,188.38,130.68,9.94">Authoritative sourcs in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,244.85,188.38,45.03,9.94;9,289.92,185.97,5.40,6.26;9,90.00,200.98,56.05,9.94;9,162.38,200.98,52.48,9.94;9,230.99,200.98,11.04,9.94;9,258.36,200.98,36.82,9.94;9,90.00,213.58,48.07,9.94">Proc. of 9 th ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>of 9 th ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.14,238.90,201.11,9.94;9,90.00,251.62,205.09,9.94;9,90.00,264.22,205.15,9.94;9,90.00,276.94,94.08,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,191.81,251.62,103.27,9.94;9,90.00,264.22,166.50,9.94">Collection statistics for fast duplicate document detection</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Mccabe</surname></persName>
		</author>
		<idno>ACM TOIS 20</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="171" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.14,302.14,201.04,9.94;9,90.00,314.86,205.15,9.94;9,90.00,327.46,88.63,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,194.00,314.86,101.15,9.94;9,90.00,327.46,84.19,9.94">Topic detection and tracking pilot study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>&amp; Yamron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.14,352.78,201.02,9.94;9,90.00,365.38,205.20,9.94;9,90.00,378.10,195.64,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,90.00,365.38,205.20,9.94;9,90.00,378.10,34.46,9.94">Novelty and redundancy detection in adaptive filtering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,131.06,378.10,82.02,9.94">Proc. ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
