<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.88,81.29,288.36,12.57;1,98.52,97.25,415.05,12.57">Extracting Answers from the Web Using Knowledge Annotation and Knowledge Mining Techniques</title>
				<funder ref="#_dnREZM9">
					<orgName type="full">Air Force Research Laboratory</orgName>
				</funder>
				<funder ref="#_4uh46et">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,75.00,126.35,63.55,10.47"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@ai.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Artificial Intelligence Laboratory 200 Technology Square Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,152.56,126.35,100.11,10.47"><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Artificial Intelligence Laboratory 200 Technology Square Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.68,126.35,63.67,10.47"><forename type="first">Boris</forename><surname>Katz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Artificial Intelligence Laboratory 200 Technology Square Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.17,126.35,96.89,10.47"><forename type="first">Gregory</forename><surname>Marton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Artificial Intelligence Laboratory 200 Technology Square Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,454.96,126.35,86.52,10.47"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Artificial Intelligence Laboratory 200 Technology Square Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.88,81.29,288.36,12.57;1,98.52,97.25,415.05,12.57">Extracting Answers from the Web Using Knowledge Annotation and Knowledge Mining Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">71E145CB9E013F41494993421AB194D4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aranea is a question answering system that extracts answers from the World Wide Web using knowledge annotation and knowledge mining techniques. Knowledge annotation, which utilizes semistructured database techniques, is effective for answering large classes of commonly occurring questions. Knowledge mining, which utilizes statistical techniques, can leverage the massive amounts of data available on the Web to overcome many natural language processing challenges. Aranea integrates these two different paradigms of question answering into a single framework. For the TREC evaluation, we also explored the problem of answer projection, or finding supporting documents for our Web-derived answers from the AQUAINT corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aranea, MIT's entry to the TREC Question Answering track, focused on extracting answers from the World Wide Web. Our system was organized around a modular framework that integrates two different paradigms of question answering: knowledge annotation using annotated structured and semistructured resources and knowledge mining using statistical techniques that leverage data redundancy (cf. <ref type="bibr" coords="1,72.00,521.41,90.04,8.78">(Lin and Katz, 2003)</ref>).</p><p>Aranea's approach to question answering is motivated by an observation about the empirical distribution of user queries, which quantitatively obey Zipf's Law-a small fraction of question types account for a significant portion of all question instances. Certain natural language questions tend to occur much more frequently than others, an observation that is confirmed by many different sources: TREC queries <ref type="bibr" coords="1,141.63,623.17,49.43,8.78">(Lin, 2002)</ref>, logs from the Start question answering system <ref type="bibr" coords="1,195.23,634.45,55.01,8.78" target="#b10">(Katz, 1997)</ref>, and logs of a commercial search engine <ref type="bibr" coords="1,210.84,645.73,55.90,8.78" target="#b15">(Lowe, 2000)</ref>. Furthermore, many questions ask for the same type of information and differ only in the specific object questioned, e.g., "What is the population of (the United States, Mexico, Canada,. . . )?" We can group these questions into a single class (or type), i.e., "What is the population of x?" where x can be any country, and find the answer in a database. Knowledge annotation is a question answering strategy that allows heterogeneous sources on the Web to be accessed as if it were a uniform database. By connecting natural language queries to this "virtual" database, Aranea can answer large classes of commonly-occurring questions.</p><p>Typically, Zipf curves have broad tails where individual instances are either unique or account for an insignificant fraction of total instances. This observation also holds true for the distribution of user questions: in addition to asking large classes of commonly-occurring questions, users also pose a significant number of unique questions that cannot be easily classified into common categories or grouped by simple patterns, e.g., "What format was VHS's main competition?" To answer these questions, Aranea employs what we call redundancybased knowledge mining techniques.</p><p>For the TREC evaluation, the extraction of answers from the Web necessitated an extra step in the question answering process, usually known as answer projection. For every Web-derived answer, our system had to find a supporting document from the AQUAINT corpus, even though the corpus itself was never used in the question answering process. This additional component had a significant impact on the overall performance of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overall Framework</head><p>The general architecture of the Aranea system is shown in Figure <ref type="figure" coords="1,387.27,578.17,3.90,8.78" target="#fig_0">1</ref>. User questions are routed to two separate components, one that employs the knowledge annotation strategy (Section 3) and one that utilizes the knowledge mining strategy (Section 4). Both components consult the World Wide Web to generate candidate answers, and the results of both components are piped through a knowledge boosting and cleanup module (Section 5), which check the answer candidates against a number of heuristics to ensure their validity. Finally, the answer projection module (Section 6) finds an article from the AQUAINT corpus that adequately supports the answer derived from the Web. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledge Annotation</head><p>Although the Web consists largely of unorganized pages, pockets of structured and semistructured knowledge exist as valuable resources for question answering. For example, the CIA World Factbook provides political, geographic, and economic information about every country in the world; 50states.com contains numerous properties related to US states from state bird to land area; Biography.com has collected profiles of over twenty-five thousand famous (and not-so-famous) people; the Internet Movie Database stores entries for hundreds of thousands of movies, including information about their cast, production staff, and dozens of other properties.</p><p>To effectively use these existing resources for question answering, the plethora of knowledge sources must be integrated, or federated, under a common interface or query language. Database concepts and techniques provide the tools to accomplish just that. In fact, since many of these sources are part of the "deep" or "invisible" Web, they are inaccessible to search engines and can only be modeled as "virtual" databases. We have developed a schema-based technique called knowledge annotation by which natural language queries can be connected to semistructured knowledge sources.</p><p>Our knowledge annotation strategy provides an effective mechanism for answering natural language questions. Because users frequently ask the same types of questions, a few well-chosen knowledge sources are sufficient to provide good knowledge coverage. For example, we have verified that ten Web sources can provide answers to 27% of TREC-9 and 47% of TREC-2001 questions from the QA track <ref type="bibr" coords="2,315.00,459.49,48.42,8.78">(Lin, 2002)</ref>. In addition, other researchers <ref type="bibr" coords="2,513.48,459.49,26.64,8.78;2,315.00,470.77,51.20,8.78" target="#b6">(Hovy et al., 2002)</ref> have noticed the importance of external knowledge sources for question answering.</p><p>The knowledge annotation component of Aranea is a simplified implementation of the system used by the Start <ref type="bibr" coords="2,381.96,518.77,55.58,8.78" target="#b9">(Katz, 1988;</ref><ref type="bibr" coords="2,442.70,518.77,52.81,8.78" target="#b10">Katz, 1997)</ref> and Omnibase <ref type="bibr" coords="2,346.10,530.05,86.11,8.78">(Katz et al., 2002a;</ref><ref type="bibr" coords="2,436.05,530.05,83.78,8.78">Katz et al., 2002b)</ref> systems. Start is a natural language understanding system, and Omnibase is a virtual database that provides uniform access to heterogenous and distributed Web sources via a wrapper-based framework. A simplified version of natural language annotation technology <ref type="bibr" coords="2,347.02,597.61,55.23,8.78" target="#b10">(Katz, 1997)</ref> is employed in database access schemata to mediate between natural language and database queries.</p><p>Since it came on-line in December 1993, Start has engaged in exchanges with hundreds of thousands of users all over the world, supplying them with useful knowledge. However, because the system provides users with paragraph-sized answers that often contain multimedia fragments such as pictures and audio clips, they are not suitable for a TRECstyle evaluation. There is evidence to support that Figure <ref type="figure" coords="3,103.91,284.66,3.90,8.78">2</ref>: The knowledge annotation component of Aranea paragraph-sized chunks form the most suitable unit of response to a user question, because it provides not only the exact answer, but additional contextual information that may help with interpretation and analysis <ref type="bibr" coords="3,129.73,367.21,73.78,8.78">(Lin et al., 2003)</ref>. Because this year's TREC QA track accepted only exact answers, we found it inappropriate to directly evaluate Start and Omnibase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Database Access Schemata</head><p>A collection of database access schemata and wrappers comprise the knowledge annotation component of Aranea (Figure <ref type="figure" coords="3,158.52,455.29,3.88,8.78">2</ref>). Each schema is composed of two connected parts: the question signature and the database query. A question signature is a collection of regular expressions that match a specific class of user questions, e.g., requests for birth dates of people. 1 These patterns are paired with unfilled database queries that are dynamically instantiated with bindings extracted from the question signature. Consider a typical database access schema:</p><p>When was x born? What is the birthdate of x? . . . → (biography.com x birthdate)</p><p>In this example, questions that ask for the birth dates of various people are translated into an objectproperty-value database query <ref type="bibr" coords="3,208.13,641.41,84.55,8.78">(Katz et al., 2002a)</ref>.</p><p>1 These question signatures are a simplified version of natural language annotations used by Start, which are parsed into and stored as ternary expressions. Because matching occurs at the level of the parsed structures, powerful linguistic machinery can be employed to handle different linguistic phenomena, e.g., synonymy, hyper/hyponymy, syntactic alternations, etc.</p><p>These queries specify the data source where the answer could be found (biography.com), the object in question (x), and the property sought after (birthdate). The value of the object's property typically answers the user's question.</p><p>The knowledge annotation component of Aranea operates by matching the user question against schemata stored in the knowledge base and executing database queries generated by the matched schemata.</p><p>The Aranea database engine is responsible for retrieving the actual answers by executing the database queries. The retrieval of such information depends on the type of the data source: Some sources are stored locally, and may translate into a file lookup. Other sources are stored on remote Web sites behind a CGI interface; executing database queries on these sources requires dynamically reconstructing an HTTP request and properly parsing the resulting HTML document. More details on the process of structuring a knowledge source for database access is discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Engineering</head><p>Teaching Aranea's knowledge annotation component to answer different classes of natural language questions involves three separate steps borrowed from Start and Omnibase: identifying question classes and knowledge sources, writing the database access schemata, and writing wrappers for the data sources.</p><p>The first step in the knowledge engineering process is to identify the class of questions to be answered and an adequate knowledge source that provides the answer. Empirical analysis of the question distribution provides hints on the effectiveness of any effort. We have noticed, for example, that users frequently asked about the demographics and economics of countries. These questions can be answered by writing a schema that uses information found within the CIA World Factbook.</p><p>Once a question class and a knowledge source have been determined, regular expression patterns that capture the general form of the question must be written. Usually, such patterns take into account various alternative formulations of the same query. These patterns must clearly indicate the noun phrase that can be parameterized. For example, in the question class "What is the GDP of x?", x stands as a generic placeholder for country names. The mapping between the natural language patterns and the database query must also be specified, e.g., the x extracted from the previous question pattern fills the x slot in the database query (cia-factbook x gdp)</p><p>After a database access schema has been crafted, a wrapper must be written for the knowledge resource. These wrappers supply the actual procedures used to execute queries of a specific form. Although Aranea provides a generic framework for organizing the queries and convenient libraries of often-used functionality, specific implementations for accessing various data sources must be provided separately. Typically, executing a query involves either looking up the information in a locally stored database (ranging in complexity from tab-delimited flat files to full SQL databases), or executing a CGI request to retrieve a dynamically generated page from a remote Website and performing additional postprocessing to extract only the relevant fragments.</p><p>The Aranea system deployed for the TREC competition included twenty-eight schemata that access seven different data sources. Here are two examples:</p><p>• Biography.com This source provides information about the lifespan, birth dates, and death dates of various well-known people. Answering questions about such properties involves dynamically retrieving pages from biography.com (via CGI) and performing simple pattern matching on the HTML document to extract exact dates.</p><p>• CIA World Factbook This resource provides various useful facts about countries, e.g., population, area, capital, etc. This information was download and structured in a locally-stored tabdelimited file. Questions about various properties of world countries are translated into simple file lookups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Knowledge Mining</head><p>The knowledge mining approach to question answering is based on the observation that as the size of a text collection increases, occurrences of the correct answer tend to also increases. Specifically, Breck et al. <ref type="bibr" coords="4,91.80,487.57,84.48,8.78" target="#b0">(Breck et al., 2001</ref>) noticed a correlation between the number of times an answer appeared in the TREC corpus and the average performance of TREC systems on that particular question. This result verifies intuition: the more times an answer appears, the easier it is to find it. The knowledge mining component of Aranea extends this insight to the World Wide Web, and leverages the Web's massive size for question answering.</p><p>As a text collection, the Web is larger in size than any research corpus by several orders of magnitude. An important implication of this size is the amount of data redundancy inherent in the text collection; potentially, each item of information has been stated in a variety of ways, in different documents. However, data redundancy is counterbalanced by the poor quality of individual documents.</p><p>A question answering system can utilize massive amounts of Web data in two ways: as a surrogate for sophisticated natural language techniques and as a method for overcoming poor document quality.</p><p>Consider the question "When did Wilt Chamberlain score 100 points?" Here are two possible answers:</p><p>( The answer could be more easily extracted from sentence (1) than from passage (2). In general, the task of answering a question is not very difficult if the document collection contains the answer stated as a simple reformulation of the question. In these cases, simple techniques, e.g., keywords or regular expressions suffice to achieve state-of-the-art performance. As the size of the document collection grows, the more likely it is that question answering systems can find statements that answer the question by matching a simple reformulation.</p><p>Without the luxury of massive amounts of data, a question answering system may be forced to extract answers from passages in which they are not obviously stated, e.g., passage (2). In these cases, sophisticated natural language processing may be required to relate the answer to the question, e.g., recognizing syntactic alternations, resolving anaphora, making commonsense inferences, performing relative date calculations, etc.</p><p>The World Wide Web is so big that simple pattern matching techniques can often replace the need to understand both the structure and meaning of language. The answer to a question could be extracted by searching directly for an anticipated answer form, e.g., in the above example, by searching for the string "Wilt Chamberlain scored 100 points on" and extracting words occurring to the right. Naturally, this simple technique depends crucially on the corpus having an answer formulated in a specific way. Thus, the larger the text collection is, the greater the probability that simple pattern matching techniques will yield the correct answer. Data redundancy enables a simple trick to overcome many troublesome issues in natural language processing, e.g., alternations, anaphora, etc.</p><p>Despite the apparent advantages of massive amounts of data, the process of answering questions using the Web is complicated by the low average quality of individual documents. Due to the low barrier of entry in Web publishing, many documents are poorly written, barely edited, or simply contain incorrect information. As a result, text extracted from a single document cannot be trusted as the correct answer. This problem can also be alleviated through data redundancy. A single instance of a candidate answer may not provide sufficient justification, but multiple occurrences of the same answer in different documents lends credibility to the proposed answer. 2  The tremendous amounts of information on the World Wide Web would be useless without an effective method of data access. Providing the basic infrastructure for indexing and retrieving text at such scales is a tremendous engineering task. Fortunately, such services already exist, in the form of search engines. For example, Google, the largest of the Web search engines, boasts over 3 billion documents in its index. 3 Using existing search engines as information retrieval backends, we can focus our efforts on answer extraction.</p><p>Many of the knowledge mining techniques described above have been implemented in previous systems <ref type="bibr" coords="5,109.86,277.93,80.25,8.78" target="#b1">(Brill et al., 2001;</ref><ref type="bibr" coords="5,194.42,277.93,69.93,8.78" target="#b3">Buchholz, 2001;</ref><ref type="bibr" coords="5,268.78,277.93,28.40,8.78;5,72.00,289.21,50.29,8.78" target="#b5">Clarke et al., 2001;</ref><ref type="bibr" coords="5,125.05,289.21,78.01,8.78" target="#b11">Kwok et al., 2001;</ref><ref type="bibr" coords="5,205.80,289.21,86.20,8.78;5,72.00,300.49,53.75,8.78" target="#b16">Soubbotin and Soubbotin, 2001;</ref><ref type="bibr" coords="5,130.91,300.49,77.67,8.78" target="#b2">Brill et al., 2002)</ref>. The introduction of redundancy-based question answering using the Web <ref type="bibr" coords="5,94.68,323.05,78.63,8.78" target="#b1">(Brill et al., 2001)</ref> at last year's TREC conference has generated a new set of techniques for attacking the question answering problem. We have taken advantage of previous experiences to refine many techniques within a better engineered framework. In particular, our infrastructure supports a modular architecture that allows specific functionality to be encoded into manageable components. This not only allows for faster development cycles, but facilitates glass-box testing to properly determine the effectiveness of various techniques.</p><p>The data flow in the knowledge mining component of Aranea is shown in Figure <ref type="figure" coords="5,198.61,458.17,3.90,8.78" target="#fig_1">3</ref>. In the following sections, we describe each module in detail. Each module accepts an Aranea XML data structure as input and returns a structure of the same type as output; the functionality of each module is implemented as internal transformations on the XML data structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Formulate Requests</head><p>The first step in answering natural language questions in the knowledge mining component is to translate them into Aranea queries, or requests. These requests specify the textual context in which answers are likely to be found, and are analogous to queries posed to information retrieval systems. However, because Aranea relies on Web search engines to fulfill these requests, fine-grained control over the result set is impossible. Aranea instead relies on quantity to make up for lack of quality.</p><p>Two types of queries are generated by this module: exact (or reformulation) queries and inexact (or back-  An inexact query indicates that an answer is likely to be found within the vicinity of a set of keywords. They are composed by treating the natural language question as a bag of words.</p><p>An exact query specifies the location of a potential answer in more detail, e.g., the answer to "When did the Mesozoic period end?" is likely to appear within ten words and fifty bytes to the right of the exact phrase "the Mesozoic period ended". Exact queries in Aranea are generated by approximately a dozen pattern matching rules based query terms and their part-of-speech tags; morpho-lexical pattern matches trigger the creation of reformulated exact queries. As an example, the previous query was generated by the rule "wh-word did . . . verb → . . . verb+ed". An internal lexicon ensures that the generated verb remains properly inflected.</p><p>As a complete example, the requests generated in response to the question "When did the Mesozoic period end?" are shown in Figure <ref type="figure" coords="5,453.30,612.97,3.90,8.78" target="#fig_2">4</ref>. Aranea generates two inexact and one exact requests; each request is assigned a basic score, which establishes the relative importance of the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Execute Requests</head><p>The request execution module is responsible for retrieving textual "snippets" that honor the constraints set forth in each request. Currently, the Google search engine is used to mine text from the Web. In the case of inexact requests, the entire summary provided by Google is extracted for further processing. For exact queries, the request execution module performs additional pattern matching to ensure that the correct positional constraints are satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generate N-Grams</head><p>This module exhaustively generates all possible unigrams, bigrams, trigrams, and tetragrams from the text fragments generated by the request execution module. These n-grams, which are given initial scores equal to the weight of the request from which they derive, serve as the raw candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Vote</head><p>The voting module collates the n-grams generated by the previous module. The new score of each answer candidate is equal to the sum of the scores of all occurrences of that particular n-gram. This module has the effect of promoting text fragments that occur frequently (in the context of query terms), and are hence more likely to answer the user question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Filter Candidates</head><p>In this stage of the processing, a coarse-grained filter in applied to answer candidates:</p><p>• Candidates that begin or end with stopwords are discarded.</p><p>• Candidates that contain words found in the user question are discarded. The only exception to this rule is question focus words, e.g., a question beginning with "how many meters..." can be answered by an expression containing the word meters.</p><p>In addition, this stage encodes a few heuristics that can potentially decrease the number of answer candidates. For example, the answer to "how far", "how fast", "how tall", etc., questions must contain a numeric component (either numeric digits or numerals); thus, we can safely discard all answer candidates that do not fit these criteria. We have also noticed that "who" and "where" questions usually cannot be answered with expressions that contain tokens consisting of numeric digits; Aranea can similarly reduce the number of answer candidates based on this criterion. The general principle embodied in this module is to filter with high confidence, erring on the side of being too lenient. False positives can always be sorted out by later modules, but the system will not be able to recover from false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Combine Candidates</head><p>In this module, shorter answers are used as evidence to boost the score of longer answers. If a portion of a candidate answer appears itself as a candidate answer, then the score of the shorter answer is added to the score of the longer answer. For example, if "de Soto" appears on the list of candidate answers along with "Hernando de Soto", the score of the shorter candidate would be added to the score of the longer one. This module counteracts the tendency of the ngram generation and voting modules to favor shorter answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Score Candidates</head><p>The score of each answer candidate is multiplied by the following factor:</p><formula xml:id="formula_0" coords="6,393.60,410.28,69.04,27.81">1 |A| w∈A log( N w c )</formula><p>A is a set of keywords in the candidate answer; N is the total number of words in the AQUAINT corpus; w c is the number of occurrences of word w in the AQUAINT corpus. Each answer candidate is scaled by the average of an idf-like value of its component keywords. This scoring balances the effect of individual keywords having different (unconditioned) priors. Since the exact distribution of unigrams on the Web can not be easily obtained in a reliable manner, Aranea uses statistics from the AQUAINT corpus as a surrogate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Get Support</head><p>This module performs a final sanity check on the candidate answers. It verifies that final candidate answers actually appear in the original text snippets mined from the Web. Occasionally, the various modules within the knowledge mining component of the system will assemble a nonsensical answer; this module ensures that such answers are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Answer Boosting and Cleanup</head><p>Results from both the knowledge annotation and knowledge mining components of Aranea are subjected to a series of heuristic checks. These heuristics may employ external knowledge resources to verify the candidate answers.</p><p>The answer boosting module of Aranea contains heuristics specifically dedicated to verifying geographic locations. We have gathered large lists of known geographic entities, e.g., world cities, US cities, etc.; these lists allow us to "boost" the score of certain answer candidates in response to "what city", "what state", "what country", "what province", etc. questions.</p><p>Questions requiring dates as answers similarly receive special treatment. Named entity detectors allow us to promote dates over other noun phrases. Knowledge of dates also helps Aranea extract the exact answers. For example, a candidate answer to a "what year" question often contains extra information such as the month and day; Aranea removes such extraneous information.</p><p>Beyond a few simple heuristics, Aranea also performs part-of-speech tagging on the answer candidates to ensure that they are full constituents (NP or VP). Extra leading or trailing words are trimmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Answer Projection</head><p>The final step in the preparation of an answer derived either from knowledge annotation or knowledge mining is answer projection, during which each Web-extracted answer is paired with a document from the AQUAINT corpus to form the basic [answer, docid ] response unit. Answer projection was accomplished in a two step process: first, a set candidate documents was gathered; then, a modified passage retrieval algorithm scanned the documents to pick the best document.</p><p>We experimented with three different methods of retrieving a candidate set of documents on which to project our Web-derived answers:</p><p>• NIST documents. The top fifty documents supplied by NIST served as the baseline set of candidate documents for answer extraction. • MultiText passages. We have implemented the passage retrieval algorithm described by <ref type="bibr" coords="7,91.92,557.89,87.49,8.78" target="#b4">Clarke et al. (2000)</ref>. A set of passages generated by this algorithm serves as the candidate documents for answer projection. • PC3 MultiText passages. We have augmented the MultiText passage retrieval algorithm by a backoff procedure we call pc3. Our algorithm applies a series of controlled query expansion loops, which successively broadens the query terms (e.g., by including different inflections and synonyms of the keywords) until an adequate set of candidate passages have been found.</p><p>After a set of candidate documents has been gathered, the answer projection module applies a mod-ified window-based passage retrieval algorithm to score the documents. Each 140-byte window is given a score equal to the number of times keywords from both the question and candidate answer appears, with the restriction that at least one keyword from the question must appear in the particular passage. The score of a document is simply the score of the highest scoring passage. The highest scoring document is paired with the Web-derived candidate answer as the final response unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Confidence Ordering</head><p>This year's TREC evaluation required participants to sort answers according to confidence, motivated by the importance of a system knowing when it is likely to be right or wrong. Although this was certainly an interesting aspect of the question answering task, due to time constraints, we were unfortunately not able to devote much attention to it.</p><p>For the deployed TREC system, we employed a crude algorithm:</p><p>• All when questions were placed before all who and where questions, which were ordered before all what questions. All other questions were placed after. We discovered through ad-hoc experimentation that Aranea generally performed better on certain types of questions; the confidence ordering reflected our experiences.</p><p>• Within each type of question, answers derived from knowledge annotation were always placed before answers derived from knowledge mining.</p><p>• Answers were sorted by the document score produced by the answer projection algorithm</p><p>• Any further ties were broken by scores generated by the knowledge mining component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>The official TREC results are shown in Table <ref type="table" coords="7,532.17,519.02,3.90,8.78">1</ref>.</p><p>The only difference between our three runs was the method used to generate the initial set of candidate documents for answer projection:</p><p>• aranea02a used only the top fifty NIST-supplied documents.</p><p>• aranea02pbq used the top fifty NIST-supplied documents and passages derived from the Mul-tiText algorithm.</p><p>• aranea02pc3 used the top fifty NIST-supplied documents and passages derived from the pc3 variant of the MultiText algorithm.</p><p>In addition, we analyzed Aranea's performance without taking into account answer projection. We felt that this particular instance of answer projection is an artifact of the TREC evaluation, and not aranea2002a aranea2002pbq aranea2002pc3 aranea2002  inherent in the question answering task itself. We rescored the unsupported judgments of aranea02a either as inexact or correct, careful to adhere to the same standards of judgment as the other runs. This result is shown in the last column of Table <ref type="table" coords="8,261.26,576.49,3.90,8.78">1</ref>.</p><p>In the formal TREC runs, our system answered approximately thirty percent of the questions correctly. Disregarding answer projection, Aranea provided exact, correct answers for nearly thirtyseven percent of the questions. Out of five hundred questions, 42 (8.4%) answers were contributed by Aranea's knowledge annotation component; the knowledge mining component accounted for the rest, or 458 (91.6%) questions.</p><p>Approximately 15% of answers judged as correct were derived from knowledge annotation techniques. We believe that this performance is remarkable, con-sidering that our system contained only twenty-eight data access schemata over seven sources, representing no more than a few person-days worth of knowledge engineering effort. Our experiences with Start and Omnibase have helped us streamline the knowledge engineering process, allowing us to rapidly structure knowledge sources to answer English questions. These results also verify that analysis of the typical distribution of user questions can help guide the knowledge engineering effort. Our database access schemata were geared towards answering the most frequently occurring questions from the previous TREC evaluations; many of the same question types also appeared in this year's evaluation.</p><p>Overall, we noticed that answer projection was the obvious weak link in Aranea. For approximately twenty percent of our Web-derived answer, our sys-tem was unable to find an adequate supporting document, which resulted in a drastic reduction of our overall TREC score. Our passage retrieval algorithm was not sophisticated enough to ignore documents that contained keywords from the question and answer, but in fact did not answer the question. More future research is required to obtain better answer projection performance.</p><p>Individual analysis of each Aranea component is shown in Table <ref type="table" coords="9,142.23,178.21,3.90,8.78" target="#tab_3">2</ref>. In general, the database component achieves much higher accuracy than the knowledge mining component, due to the knowledge engineering effort involved in creating database access schemata. However, projecting answers derived from database access appears more difficult than answers derived from knowledge mining. Once again, we believe that Aranea demonstrates the validity and effectiveness of knowledge engineering in the question answering process. Knowing when to apply manual effort and selectively using human labor can translate into a big payoff in terms of performance enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Contributions</head><p>The Aranea system presents two different paradigms for approaching the question answering problem. In the knowledge annotation approach, natural language questions can be translated into database queries, which then extract answers from the Web. In the knowledge mining approach, data redundancy on the Web can be leveraged to overcome many difficult problems in natural language processing.</p><p>Aranea smoothly integrates both the knowledge annotation and knowledge mining approach into a uniform framework. With knowledge about the types of questions that users ask, we were able to utilize each paradigm effectively.</p><p>Another insight we gained in developing Aranea is to let the analysis of user questions guide our knowledge engineering effort. By correctly anticipating the types of questions users typically ask, we were able to construct effective database access schemata with reasonable amounts of manual labor.</p><p>We believe that Aranea provides a well-engineered platform for experimenting with various Web-based question answering techniques. In the future, we will continue to refine existing technology and develop new methods for answering natural language questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,147.12,315.98,317.85,8.78;2,165.60,72.14,280.92,229.33"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall Architecture of the Aranea question answering system.</figDesc><graphic coords="2,165.60,72.14,280.92,229.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,315.00,335.54,225.06,8.78;5,315.00,346.81,75.92,8.78"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Data flow in the knowledge mining component of Aranea</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,78.72,254.42,211.68,8.78"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Typical requests generated by Aranea.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,334.92,106.53,185.21,110.50"><head></head><label></label><figDesc>1) Wilt Chamberlain scored 100 points on March 2, 1962 against the New Yorks Knicks.</figDesc><table coords="4,334.92,130.29,185.21,86.74"><row><cell>(2) On December 8, 1961, Wilt Chamber-</cell></row><row><cell>lain scored 78 points in a triple overtime</cell></row><row><cell>game. It was a new NBA record, but War-</cell></row><row><cell>riors coach Frank McGuire didn't expect it to</cell></row><row><cell>last long, saying, "He'll get 100 points some-</cell></row><row><cell>day." McGuire's prediction came true just a</cell></row><row><cell>few months later in a game against the New</cell></row><row><cell>York Knicks on March 2.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,201.12,503.54,209.82,8.78"><head>Table 2 :</head><label>2</label><figDesc>Performance of individual components.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="10">Acknowledgements</head><p>This research is funded by <rs type="funder">DARPA</rs> under contract number <rs type="grantNumber">F30602-00-1-0545</rs> and administered by the <rs type="funder">Air Force Research Laboratory</rs>. Additional funding is provided by the <rs type="projectName">Oxygen</rs> Project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4uh46et">
					<idno type="grant-number">F30602-00-1-0545</idno>
				</org>
				<org type="funded-project" xml:id="_dnREZM9">
					<orgName type="project" subtype="full">Oxygen</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,315.00,92.30,225.07,8.78;9,324.96,103.46,215.07,8.78;9,324.96,114.74,215.00,8.78;9,324.96,126.01,215.16,8.78;9,324.96,137.29,214.97,8.78;9,324.96,148.57,215.01,8.77;9,324.96,159.73,215.12,8.77;9,324.96,171.01,128.53,8.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,486.09,114.74,53.87,8.78;9,324.96,126.01,215.16,8.78;9,324.96,137.29,93.11,8.78">Looking under the hood: Tools for diagnosing your question answering engine</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brianne</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mats</forename><surname>Rooth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Thelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,439.56,137.29,100.37,8.77;9,324.96,148.57,215.01,8.77;9,324.96,159.73,215.12,8.77;9,324.96,171.01,123.70,8.77">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL&apos;01) Workshop on Open-Domain Question Answering</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics (ACL&apos;01) Workshop on Open-Domain Question Answering</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,315.00,183.61,225.01,8.78;9,324.96,194.89,215.10,8.78;9,324.96,206.17,215.12,8.78;9,324.96,217.33,159.49,8.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,493.43,183.61,46.58,8.78;9,324.96,194.89,91.08,8.78;9,452.09,194.89,87.97,8.78;9,324.96,206.17,62.27,8.78">Data-intensive question answering</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,409.56,206.17,130.51,8.77;9,324.96,217.33,92.88,8.77">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
	<note>Susan Dumais, and Andrew Ng</note>
</biblStruct>

<biblStruct coords="9,315.00,229.93,225.29,8.78;9,324.96,241.21,215.41,8.78;9,324.96,252.49,215.02,8.78;9,324.96,263.77,215.03,8.77;9,324.96,274.93,87.61,8.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,324.96,241.21,215.41,8.78;9,324.96,252.49,27.87,8.78">An analysis of the AskMSR question-answering system</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,373.92,252.49,166.06,8.77;9,324.96,263.77,215.03,8.77;9,324.96,274.93,13.23,8.77">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,315.00,287.53,225.07,8.78;9,324.96,298.81,215.08,8.78;9,324.96,310.09,215.07,8.78;9,324.96,321.37,210.37,8.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,430.66,287.53,109.41,8.78;9,324.96,298.81,215.08,8.78;9,324.96,310.09,117.50,8.78">Using grammatical relations, answer frequencies and the World Wide Web for question answering</title>
		<author>
			<persName coords=""><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,462.96,310.09,77.07,8.77;9,324.96,321.37,143.74,8.77">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,315.00,333.85,225.15,8.78;9,324.96,345.13,215.26,8.78;9,324.96,356.41,215.08,8.78;9,324.96,367.69,215.00,8.78;9,324.96,378.97,130.21,8.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,453.68,345.13,86.53,8.78;9,324.96,356.41,215.08,8.78;9,324.96,367.69,38.13,8.78">Question answering by passage selection (multitext experiments for TREC-9)</title>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Derek</forename><surname>Kisman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,387.12,367.69,152.84,8.77;9,324.96,378.97,78.86,8.77">Proceedings of the Ninth Text REtrieval Conference</title>
		<meeting>the Ninth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,315.00,391.45,225.09,8.78;9,324.96,402.73,215.11,8.78;9,324.96,414.01,215.18,8.78;9,324.96,425.29,215.17,8.77;9,324.96,436.57,214.95,8.77;9,324.96,447.85,63.01,8.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,384.05,402.73,156.02,8.78;9,324.96,414.01,41.39,8.78">Exploiting redundancy in question answering</title>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,394.92,414.01,145.22,8.77;9,324.96,425.29,215.17,8.77;9,324.96,436.57,214.95,8.77">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>SIGIR-2001</note>
</biblStruct>

<biblStruct coords="9,315.00,460.33,225.06,8.78;9,324.96,471.61,214.99,8.78;9,324.96,482.89,215.01,8.78;9,324.96,494.17,215.03,8.77;9,324.96,505.45,172.33,8.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,455.03,471.61,84.92,8.78;9,324.96,482.89,156.72,8.78">Using knowledge to facilitate factoid answer pinpointing</title>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,504.12,482.89,35.85,8.77;9,324.96,494.17,215.03,8.77;9,324.96,505.45,167.00,8.77">Proceedings of the 19th International Conference on Computational Linguistics (COLING-2002)</title>
		<meeting>the 19th International Conference on Computational Linguistics (COLING-2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,315.00,517.93,225.06,8.78;9,324.96,529.21,215.01,8.78;9,324.96,540.49,215.03,8.78;9,324.96,551.77,215.14,8.78;9,324.96,563.05,215.13,8.78;9,324.96,574.21,215.07,8.77;9,324.96,585.48,212.89,8.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,512.30,540.49,27.69,8.78;9,324.96,551.77,215.14,8.78;9,324.96,563.05,81.25,8.78">Omnibase: Uniform access to heterogeneous data for question answering</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alton</forename><surname>Jerome Mc-Farland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baris</forename><surname>Temelkuran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,427.80,563.05,112.29,8.77;9,324.96,574.21,215.07,8.77;9,324.96,585.48,146.55,8.77">Proceedings of the 7th International Workshop on Applications of Natural Language to Information Systems</title>
		<meeting>the 7th International Workshop on Applications of Natural Language to Information Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,315.00,598.08,225.28,8.78;9,324.96,609.36,215.03,9.23;9,324.96,620.64,215.01,8.78;9,324.96,631.80,215.02,8.77;9,324.96,643.08,175.93,8.77" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<title level="m" coord="9,324.96,609.36,215.03,9.23;9,324.96,620.64,175.59,8.78;9,520.92,620.64,19.05,8.77;9,324.96,631.80,215.02,8.77;9,324.96,643.08,119.40,8.77">The START multimedia information system: Current technology and future directions</title>
		<imprint>
			<publisher>MIS</publisher>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
	<note>Proceedings of the International Workshop on Multimedia Information Systems</note>
</biblStruct>

<biblStruct coords="9,315.00,655.68,225.05,8.78;9,324.96,666.96,215.01,8.78;9,324.96,678.24,214.95,8.77;9,324.96,689.40,126.37,8.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,400.78,655.68,139.27,8.78;9,324.96,666.96,40.70,8.78">Using English for indexing and retrieving</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,388.32,666.96,151.65,8.77;9,324.96,678.24,214.95,8.77;9,324.96,689.40,69.13,8.77">Proceedings of the 1st RIAO Conference on User-Oriented Content-Based Text and Image Handling</title>
		<meeting>the 1st RIAO Conference on User-Oriented Content-Based Text and Image Handling</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>RIAO &apos;88</note>
</biblStruct>

<biblStruct coords="9,315.00,702.00,225.04,8.78;9,324.96,713.28,215.09,8.78;10,81.96,75.26,214.93,8.77;10,81.96,86.53,188.41,8.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,395.76,702.00,144.28,8.78;9,324.96,713.28,97.93,8.78">Annotating the World Wide Web using natural language</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,444.60,713.28,95.45,8.77;10,81.96,75.26,214.93,8.77;10,81.96,86.53,184.42,8.77">Proceedings of the 5th RIAO Conference on Computer Assisted Information Searching on the Internet (RIAO &apos;97)</title>
		<meeting>the 5th RIAO Conference on Computer Assisted Information Searching on the Internet (RIAO &apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,98.29,224.94,8.78;10,81.96,109.45,215.05,8.78;10,81.96,120.73,214.98,8.78;10,81.96,132.01,161.41,8.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,112.87,109.45,178.40,8.78">Scaling question answering to the Web</title>
		<author>
			<persName coords=""><forename type="first">Cody</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,95.40,120.73,201.54,8.77;10,81.96,132.01,96.86,8.77">Proceedings of the Tenth International World Wide Web Conference</title>
		<meeting>the Tenth International World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,143.77,225.12,8.78;10,81.96,155.05,215.10,8.78;10,81.96,166.21,92.53,8.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,238.90,143.77,58.22,8.78;10,81.96,155.05,194.56,8.78">Question answering techniques for the World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,81.96,166.21,88.34,8.77">EACL-2003 Tutorial</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,177.97,225.16,8.78;10,81.96,189.25,215.08,8.78;10,81.96,200.53,215.15,8.78;10,81.96,211.81,215.00,8.78;10,81.96,222.97,215.05,8.77;10,81.96,234.25,53.05,8.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,145.99,200.53,151.12,8.78;10,81.96,211.81,67.56,8.78">The role of context in question answering systems</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dennis</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vineet</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karun</forename><surname>Bakshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,170.76,211.81,126.20,8.77;10,81.96,222.97,215.05,8.77;10,81.96,234.25,48.55,8.77">Proceedings of the 2003 Conference on Human Factors in Computing Systems (CHI 2003)</title>
		<meeting>the 2003 Conference on Human Factors in Computing Systems (CHI 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,246.01,225.10,8.78;10,81.96,257.29,215.13,8.78;10,81.96,268.57,214.98,8.77;10,81.96,279.73,215.00,8.77;10,81.96,291.01,27.01,8.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,162.18,246.01,134.92,8.78;10,81.96,257.29,195.42,8.78">The Web as a resource for question answering: Perspectives and challenges</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,81.96,268.57,214.98,8.77;10,81.96,279.73,174.90,8.77">Proceedings of the Third International Conference on Language Resources and Evaluation</title>
		<meeting>the Third International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,302.77,225.01,8.78;10,81.96,314.05,215.07,8.78;10,81.96,325.33,214.92,8.77;10,81.96,336.49,215.07,8.77;10,81.96,347.77,134.89,8.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,171.10,302.77,125.92,8.78;10,81.96,314.05,47.86,8.78">What&apos;s in store for question answering?</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,217.44,314.05,79.59,8.77;10,81.96,325.33,214.92,8.77;10,81.96,336.49,215.07,8.77;10,81.96,347.77,129.21,8.77">Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</title>
		<meeting>the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>invited talk</note>
</biblStruct>

<biblStruct coords="10,72.00,359.53,225.13,8.78;10,81.96,370.81,215.16,8.78;10,81.96,382.09,215.07,8.78;10,81.96,393.25,210.37,8.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,109.13,370.81,187.99,8.78;10,81.96,382.09,112.66,8.78">Patterns of potential answer expressions as clues to the right answers</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergei</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,217.80,382.09,79.23,8.77;10,81.96,393.25,143.74,8.77">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
