<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,55.80,69.30,500.28,14.76">Information Filtering, Novelty Detection, and Named-Page Finding</title>
				<funder ref="#_q3Z9vNY #_U7nZr7g">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.88,84.49,102.05,11.03"><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.00,84.49,48.73,11.03"><forename type="first">Paul</forename><surname>Ogilvie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.31,84.49,35.81,11.03"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,392.35,84.49,51.87,11.03"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,55.80,69.30,500.28,14.76">Information Filtering, Novelty Detection, and Named-Page Finding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">84C76B44370A2C8866BC97F61415A8D4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Two evaluation measures were used in this year's adaptive filtering track: T11U=2*R + -N + 1 and T11F=1/(1/recall+4/precision)), where R + is the number of relevant documents delivered, and N + is the number of nonrelevant documents delivered. T11U can be optimized if we can estimate precision, and the corresponding optimal strategy is: deliver if P(relevant) &gt; 0.33. T11F can be optimized only if we can estimate both precision and recall.</p><p>When filtering, we have several training documents represented as ((x 1 ,y 1 )(x 2 ,y 2 ),...(x t ,y t )), where x i is the score of a delivered document with user feedback, and y i =0 if document i is not relevant, otherwise y i =1. We also order the tuples according to the constraint x 1 &lt;x 2 &lt;...x i &lt;x i+1 &lt;...&lt;x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimized</head><p>for T11U Optimized for T11F NE Yes Yes ML Yes Yes Empirical Optimal Yes No Logistic Regression Yes No Bayesian Error Model Yes No Greedy Search Yes No Table 1: Candidate Thresholding Algorithms</p><p>We tried six threshold-setting algorithms (Table <ref type="table" coords="1,249.08,512.05,3.72,11.03">1</ref>):</p><p>• NE (Normal-Exponential): Model the scores of the relevant documents with a Normal distribution and model the top ranking non-relevant documents with an exponential distribution as described in [1].</p><p>• ML: Maximum Likelihood Estimation as described in <ref type="bibr" coords="1,316.79,559.93,15.32,11.03" target="#b19">[19]</ref>. Use the same model as NE, but explicitly model the sample bias while estimating model parameters. This is a modified version of the Maximum Likelihood Estimation thresholding algorithm, because the early stage (when the number of relevant documents or non-relevant documents in the training set is smaller than 4, the threshold is the optimal one calculated by the model, smoothed with the old threshold and the average relevant document score using linear interpolation.</p><p>• EO (Empirical Optimal). Let all candidate threshold be θ i =x i +x i-1 /2 (i=1…t). Set the real threshold at θ j , where the evaluation measure we want to optimize achieved the empirical optimal value on training data among all of the candidate points.</p><p>• Logistic Regression: This is a strict implementation of widely known logistic regression algorithm. Although the thresholding algorithm described in <ref type="bibr" coords="1,245.96,676.93,16.73,11.03" target="#b12">[12]</ref> is also based on logistic regression, it is a modified version using calibration to fit the filtering task and data, and would probably get a better result.</p><p>1 The exact official evaluation for utility is T11SU=(max(T11U/MaxU, MinNU)-MinNU)/(1-MinNU), where MaxU=2*(Total Number of relevant documents), MinNU=-0.5. T11SU is normalized T11U.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In TREC 11, our group participated in the Novelty track, Filtering track, and the Named-Page Finding task of the Web track. This paper describes our approaches, experiments, and results. As the approach for each task is quite different, the paper contains a section for each of the tasks. The following section describes our experiments in adaptive filtering, Section 3 describes named-page finding, and section 4 discusses the Novelty track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Adaptive Filtering</head><p>In the adaptive filtering track, we used the same system as in TREC9 and TREC10. The Rocchio algorithm is used for anytime profile updating. More detailed information about profile updating and the system structure is available in <ref type="bibr" coords="1,539.00,238.21,15.42,11.03" target="#b17">[17]</ref>. This year, we focused on comparing different thresholding methods, and also did some experiments on using language model to improve initial query profiles.</p><p>• Bayesian Error Model: Use uniform error model P(y=1|x,t,e)=e+(1-2e)Φ(x-t), where Φ(x-t)=0 if x&lt;t, and Φ(x-t)=1 if x&gt;=t. The system use a Beta distribution p(e) ~ β(α 1 ,α 2 ) to model the prior of error e. Bayesian estimation of P(y=1|x, training data) is used to estimate the precision. More information about this is available in <ref type="bibr" coords="2,492.44,78.61,10.68,11.03" target="#b1">[1]</ref>.</p><p>• Greedy Search: This is a greedy algorithm that increases the threshold if a relevant document is delivered, while decrease the threshold if a non-relevant document is delivered. While the step size depends on 1) the difference between the score of the document and the current threshold and 2) how many changes the system has made before. So the strategy will modify the threshold if given a feedback for document d i using: t new =t old -2δ 1 *max(δ 2 ,score di -t old )</p><p>if d i is relevant, otherwise t new =t old +δ 1 *max(δ 2 ,score di -t old ). Where δ 1 decreases as number of feedback increases, δ 2 is set to 0.005 arbitrarily.</p><p>Notice that Logistic Regression and the Bayesian Error Model are focused on model P(y|x) and do not model the marginal distribution P(x). These two models can help estimate precision, but are not capable of estimating the recall. Theoretical optimization for T11F is not possible for them without getting an estimate of P(x). This is a general problem for using discriminative models for thresholding, and one solution is to use the empirical distribution of x as described in <ref type="bibr" coords="2,510.39,207.85,16.74,11.03" target="#b12">[12]</ref> to help estimate recall, but it requires a lot of computation. We compared these different thresholding algorithms on TREC8 and TREC9 filtering track data, using T11U as the evaluation measure. Maximum Likelihood Estimation works consistently the best on both data sets. The system was tuned using the TREC8 and TREC9 filtering track data. After submitting our results, we did more experiments on TREC11 data.</p><p>The final results on the different datasets are shown in Table <ref type="table" coords="2,300.72,381.36,3.76,11.03" target="#tab_0">2</ref>. Using the Normal and Exponential model to model the score distribution worked well on all three dataset, although their performance was not good on TREC10 data(not reported here. <ref type="bibr" coords="2,54.00,404.28,15.00,11.03" target="#b18">[18]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modifying an initial profile without training documents</head><p>NIST provides topics that contain title, description and narrative fields to describe the user's information interests, but what should be the initial profile description is unknown. If we look at the following sample topic from TREC8:</p><p>Query: q353 We can see that the title is a better description but contains only 2-4 words. The Description and the Narrative are too long and noisy. We tried a mixture model to model how a user generates a description/narrative. Assuming a query is generated by a mixture of profile independent general query model M g and profile dependant core query model M q . Words such as "Identify" or "Documents" are more likely to be generated by M g , while the words "Antarctica" is more likely to be generated from M q . Using algorithms described in <ref type="bibr" coords="2,223.38,611.17,15.41,11.03" target="#b21">[21]</ref>, we can find the core query model M q and use it to do feature selection or reweighing for the initial query. Thus for each profile, we have 7 options for setting initial queries based on how we use title, description and narrative fields provided by NIST. The results on the TREC8 and the TREC11 dataset are shown in Table <ref type="table" coords="2,550.70,634.21,3.76,11.03" target="#tab_2">3</ref>.</p><p>Using the mixture model helped for the TREC8 dataset, while we didn't see significant improvement on TREC11 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Filtering Track Results</head><p>This year, we submitted four very similar runs. UDESC and FDESC used description and title fields, while Uml and Fml used the mixture model for term reweighting. Table <ref type="table" coords="3,269.50,371.53,4.97,11.03" target="#tab_3">4</ref> shows our results compared with other systems. For most topics, our system performance is above the median performance.</p><p>Figure <ref type="figure" coords="3,82.70,400.57,4.97,11.03" target="#fig_0">1</ref> shows the results for one of the 4 runs on each topic, compared with Max, Median and Baseline (the performance of a system that delivers nothing). The system performance on the first 50 topics is much better than the performance on the last 50 topics. The first 50 are created and annotated by NIST annotators, while the last 50 topics were intersections of Reuters categories. Figure <ref type="figure" coords="3,164.15,435.12,4.97,11.03" target="#fig_1">2</ref> compares the first 50 topics with the last 50 topics by sorting the topics according to the number of relevant documents in that topic. Although the number of relevant documents is similarly distributed for the first 50 and the last 50 topics, the best performance is quite different. When we look at some relevant documents for NIST annotated topics and "intersection" topics, we feel it is hard to learn "intersection" profiles according to training documents with a bag of word model (Rocchio). More detailed research and analysis is needed to fully understand the difference between them.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial query TREC8 TREC11</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Named-Page Finding</head><p>The language modeling approach to information retrieval typically makes the assumption that the query is representative of the relevant documents. In most previous research, language modeling places equal weight on all parts of the document. This may work well for ad-hoc document retrieval on newspaper corpora, but we do not feel that this necessarily makes the best use of information present in the document. For example, it does not leverage document structure, such as markup present in HTML documents. For named-page finding we hypothesize that the user's query is what the user believes to be a reasonable estimate of the "name" of the page she is seeking. Therefore, when estimating a language model, we do not want to estimate the language model of the entire document, but instead we want to estimate a model for the page's "name". Given some document structure, we can form a variety of document representations. We can weight these representations according to how characteristic they are of the page's "name". For example, we may want to weight the title of a document more than the rest of the text. Or in a hypertext environment, such as the Internet, we may want to incorporate the text of the links pointing to the page for which we are estimating a language model.</p><p>Language modeling suggests that we try to estimate a new language model from language models created from the document representations. This new language model for a document should be designed so that it closely models what we would expect a user to write as a query when requesting the document. For named-page finding, we would like to estimate a language model for the "name" of the page from language models produced by various document representations. We explore creating this new language model by taking a linear interpolation of the other models. Note that this is different from doing a linear combination of scores from different systems; we directly estimate the probability of a word given the differing language models. This is also different from directly weighting the term frequencies. Isolating the fields into different language models allows for smoothing each representation with a collection language model based on that representation only. This explicitly models the fact that the language usage in different document representations or fields is different, and adjusts the probabilities accordingly.</p><p>As mentioned above, we form language models from different document representations of the document. For example, one representation of a document may be its title. Another document representation may consist of the text contained in larger fonts. These representations do not need to be partitions, or even non-overlapping. We can still use the entire content of a document as a representation, while including other representations such as the document's title. From these different representations, we form a language model, using like representations from other documents for the backoff language model. We combine the language models from different representations using linear interpolation to form a new language model. The representations used and the linear interpolation weights chosen can be fine-tuned to a specific task. For example, we may expect that the document's title is more important for named-page finding than it is for a general ad-hoc relevance task. If so, then for named-page finding we would use a higher weight for the language model formed from the title than when performing an ad-hoc relevance search.</p><p>In this report, we investigate combining document representations to improve retrieval performance for named-page finding.</p><p>We also explore how much information is needed to achieve performance that is similar to using the full document and inlink text available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Modeling for Named-Page Finding</head><p>Two primary models for ranking documents are used in Information Retrieval: Kullback-Leibler divergence <ref type="bibr" coords="4,506.98,506.17,16.73,11.03" target="#b15">[15]</ref> and the generative language model <ref type="bibr" coords="4,164.68,517.69,15.43,11.03" target="#b10">[10]</ref>. Under common assumptions we make for this task, these approaches are equivalent <ref type="bibr" coords="4,527.91,517.69,10.77,11.03" target="#b9">[9]</ref>. So we arbitrarily choose to use a generative language model.</p><formula xml:id="formula_0" coords="4,91.80,541.37,125.20,27.51">( ) ( ) ( ) ∏ ∈ = = Q w D D w P Q D Q θ θ P P</formula><p>In the above equation, Q is a query (a sequence of words), D is a document, and θ is a language model. For the generative model, the query is treated as a sample from the document's language model, and we must then compute the probability that the document's language model produced the query sequence. Note that the probability of the term is in the product as many times as the term occurs. That is, we represent the query as a sequence of words.</p><p>To complete the specification of the retrieval method described above, we need to estimate the language models. The methods we use here are discussed and compared in <ref type="bibr" coords="4,269.73,641.04,15.35,11.03" target="#b16">[16]</ref>. The simplest way to estimate a unigram language model given a chunk of text is to use a maximum likelihood estimate:</p><formula xml:id="formula_1" coords="4,92.16,665.89,98.91,30.00">( ) ( ) T T w count w P T MLE ; = θ</formula><p>Here, T denotes the text we are using to estimate the language model. The probability of the word given the text's model is simply the number of times the word occurs in the text divided by the length of the text. This has the advantage of being easy to compute, but it has the problem that many words have zero probability or are poorly estimated if the length of the text is small. This technique is often used to estimate a background language model, such as the probability of a word given the entire collection. The collection is typically large enough to give estimates. To address the problem zero counts, linear interpolation is often used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( ) ( )</head><formula xml:id="formula_2" coords="5,92.16,108.58,89.15,25.78">∑ = = k i i i LIN w P w P 1 θ λ θ</formula><p>Where k is the number of language models we are combining, and i λ is the weight on the model i θ . To ensure that this is a valid probability distribution, we must place these constraints on the lambdas: 0 , 1 for and 1</p><formula xml:id="formula_3" coords="5,91.80,176.14,129.33,25.78">1 ≥ ≤ ≤ = ∑ = i k i i k i λ λ</formula><p>We often use a linear interpolation of a text model and a collection model. One specific form of this is to use Dirichlet prior smoothing. This technique has worked quite well in ad-hoc retrieval experiments <ref type="bibr" coords="5,386.18,220.08,15.89,11.03" target="#b16">[16]</ref>[9] <ref type="bibr" coords="5,414.00,220.08,15.89,11.03" target="#b14">[14]</ref>. Dirichlet prior smoothing has one parameter, µ, which is typically chosen close to the average length of the text chunks being estimated. For this smoothing method, we have 1 θ as the text model, 2 θ as the collection language model, ( ) <ref type="figure" coords=""></ref>and<ref type="figure" coords="5,102.84,254.86,3.27,20.61">(</ref> )</p><formula xml:id="formula_4" coords="5,488.28,243.34,69.81,14.52">µ λ + = T T 1 ,</formula><formula xml:id="formula_5" coords="5,70.08,260.26,68.72,14.52">µ µ λ + = T 2 .</formula><p>A final technique we will use to estimate a language model is a character based n-gram model. An n-gram model considers the context of the token. Specifically, it estimates the probability of the token given the previous n -1 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( ) ( ) (</head><p>)</p><formula xml:id="formula_6" coords="5,92.16,312.94,197.73,30.82">T i n i n i T i i n i n i T i n i n i i c c c P c c c c P c c c c P θ θ θ 1 2 1 1 2 1 1 2 1 , - + - + - - + - + - - + - + - = K K K</formula><p>The n-gram model does not specify how the probabilities on the right hand side of the above equation should be estimated, but they can be estimated using any of the previously described techniques. Using backoff where the background models are based on shorter sequences of tokens is a common method for estimating these probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Specifics</head><p>We use the Lemur toolkit <ref type="bibr" coords="5,161.57,409.57,11.71,11.03" target="#b7">[7]</ref> for document indexing and retrieval. For document tokenization we used Inquery's stopword list and the Porter stemmer. The URLs were tokenized on punctuation (., /) and were not stemmed. A shorter stopword list was used for URLs ("http", "www", "com", "gov", "html", etc.). Each document had as many as seven document representations, outlined in Table <ref type="table" coords="5,192.57,444.01,3.75,11.03" target="#tab_0">2</ref>. For every representation except the URL, we formed language models using a backoff model with Dirichlet prior smoothing. The Dirichlet prior parameter was chosen to be close to twice the average length of the representation. The probability of a word given the document's URL was computed treating the URL and word as a character sequence, then computing a character-based trigram generative probability. The numerator and denominator probabilities in the trigram expansion were estimated using a linear interpolation with the collection model (all URLs in the corpus).</p><p>The linear interpolation parameters for the .GOV corpus were trained using 80 queries we created locally for the named-page finding task. We trained the lambda parameters by performing retrieval separately on each of the representations. The weights were then taken as the scaled mean reciprocal rank of the system. The normalization was performed to ensure that the weights summed to one. This training procedure did not yield better results than assigning equal weight to each representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Named-Page Finding Results</head><p>We briefly describe experiments on the WT10G testbed (Table <ref type="table" coords="5,321.68,600.49,3.63,11.03" target="#tab_4">5</ref>). Without the use of document priors, our system has respectable performance. This technique is as strong as any reported in TREC10 that did not use a form of document prior <ref type="bibr" coords="5,54.00,623.53,10.69,11.03" target="#b3">[3]</ref>. The best performing single document representation, document in-link text, had a MRR of 0.515, so combining the document representations significantly improves performance. Additionally, we tried using the document URL priors described in <ref type="bibr" coords="5,106.12,646.45,11.71,11.03" target="#b4">[4]</ref> as a re-ranking strategy for the top 1000 documents. The use of document priors did improve performance for all evaluation measures used for the task.</p><p>For this year's TREC, we wished to investigate two main questions: whether we get good performance gains by combining document representations, and what performance we can get when operating under a variety of system constraints. First we look at the performance of the individual document representations. This is in Table <ref type="table" coords="6,404.39,392.17,3.76,11.03" target="#tab_5">6</ref>. The full text, in-link text, and title text were the best document representations for the named-page finding task; full text yielded the greatest performance with a mean-reciprocal rank of .469.</p><p>Our official results are summarized in Table <ref type="table" coords="6,248.75,432.60,3.75,11.03">7</ref>. Two of our official submissions combined all of the representations: LmrAllEq and LmrAllEst, which respectively had mean-reciprocal ranks of .676 and .667. LmrAllEq used equal weighting parameters and LmrAllEst used parameters from our naïve training procedure evaluated on our 80 query training set. Both runs had much better performance on all measures than any of the individual methods. LmrAllEst performed slightly worse than LmrAllEq, but we are not sure whether this difference is significant.</p><p>The other three runs investigated combining document representations under different scenarios. LmrNoStruct used only the full text and the link text. The name is a little misleading, as it did keep the full text and link text separate as different language models. Combining these two best representations yielded a MRR of .611, which is not quite as good as combining all of the document representations. LmrSmall was an attempt to estimate what level of performance can be maintained while indexing only small amounts of text. Only the font, title, and link representations were used for this run. This resulted in indexing only 43 million terms, where the full text index contains around 945 million terms. The MRR of .589 for this run was not bad, but the failure rate at 50 documents was quite high, and the number of topics with the right answer in the top 10 was also lower than for other runs. The other run LmrDocStruct, looked at document representations from the document only, ignoring the URL and the in-link text. Incremental indexing of in-link text can be a complicated operation, and it may be too expensive to scan the entire corpus on a regular basis to compute each document's in-link text. This run was an attempt to measure how well a system could perform without the use of this information. The MRR for this run was .567, which suggests that in-link text is an important document representation for named-page finding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Conclusions</head><p>We explored the use of document structure in the named-page finding task and the homepage finding task. We found that combining different document representations worked very well within the language modeling framework. We feel that the use of language modeling provides an effective mechanism for combining information from different document representations. We found that document in-link text is important for named-page finding, confirming previous results in homepage finding. We also demonstrated that good MRR performance can be achieved with a very small index, but that in order to get a high percentage of documents found in the top ten answers and to preserve a low failure rate, the full text of the Table <ref type="table" coords="6,178.43,347.68,4.18,8.96">7</ref>: Official results of the named-page finding task on the .GOV testbed document is needed. We also showed that by adding more representations, we can improve performance, even though the content of the new representations may overlap with other existing representations.</p><p>The largest issue that we failed to adequately address was the training of the linear interpolation weights for the combination of document representation language models. The training method we used did not seem to provide any gain. We would like to explore more sophisticated techniques for training the parameters in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Novelty Track: Finding Relevant and Redundant Sentences</head><p>The problems of finding relevant and redundant sentences are related to several other well-studied IR problems, with important differences. Finding specific relevant sentences is similar to some aspects of open-domain question-answering, in that we are looking for specific statements, and not just entire documents, that satisfy the query. We therefore include more surface features of sentences, such as punctuation and named entity types, in our analysis. However, the nature of the answers is much less specific than that typically seen in open-domain QA applications. Another similar problem is that of topic-level novelty and redundancy detection, as described by Zhang et al. <ref type="bibr" coords="7,355.76,200.16,15.42,11.03" target="#b20">[20]</ref>, in which the authors use statistical models to perform adaptive filtering to find documents that are not only relevant, but also novel (or equivalently, not redundant). The novelty problem is also related to multi-document, query-specific summarization, in that we seek to find a set of representative, maximally informative sentences. However, the criteria for "maximally informative" are different for each problem: summarization seeks to obtain good coverage of the various aspects of the relevant information while keeping within a size/length constraint. For our problem, we have no length constraint and the definition of "novel" is extended to allow more subtle differences between sentences.</p><p>Our general approach is to view both relevant and redundant sentences as simple statistical translations of the query. For performance reasons, we currently only apply this type of model to the redundant sentence computation, and use a tf.idfbased approach to obtain relevant sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Finding Relevant Sentences</head><p>We examined the performance of tf.idf-based retrieval using sentences as "documents" and found that, with pseudo-relevance feedback, using all sentences with a non-zero score gave high recall of relevant sentences (typically 70-80%). However, not surprisingly, the precision was extremely low (usually 10% or less), so this led to the following method.</p><p>1. Retrieve a set of candidate sentences using straightforward tf.idf-based retrieval with query expansion, based on a query constructed from the TREC description 2. Extract a set of features from the resulting candidate sentences 3. Use these features to classify each candidate sentence and remove those that are more likely to be non-relevant.</p><p>The classification in step 3 can be based on one of several different methods. In this study, we examined these three:</p><p>Simple_Threshold: use the tf.idf score as the only feature, and apply a threshold; Decision_Tree (DT): extract a much wider set of features and build a decision tree; and Proximity: simple model using proximity to highly relevant sentences as the main criterion.</p><p>We treated each sentence as a separate document, and indexed all the sentences from all relevant documents using Lemur <ref type="bibr" coords="7,542.73,518.64,10.63,11.03" target="#b7">[7]</ref>.</p><p>We created a query from the title, description, and narrative fields in the topic and use this to score each sentence using tf.idf weighting. We performed query expansion using pseudo-relevance feedback, using the top 10 terms from the top 20 sentences. This produced an initial "candidate list" of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple_Threshold Rule</head><p>The tf.idf scores were normalized so that the highest scoring sentence received a score of 1.0. This rule was mostly useful as a baseline, and as a way for finding highly relevant sentences for the Proximity rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision Tree Rule</head><p>In this approach, we extracted a "base set" of about 15 to 20 surface and semantic features from a document sentence. This base set was extended to include features based on a small history window of previous sentences and "difference" features based on the query sentences. All of these were then used to build a decision tree using C4.5 <ref type="bibr" coords="7,445.04,663.60,16.75,11.03" target="#b11">[11]</ref> to predict non-relevant sentences. The intent was to find the most highly discriminative features, and hopefully increase precision by removing sentences from the candidate list which likely to non-relevant based on the decision of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proximity Rule</head><p>This method is a simple form of clustering, where we assume that most relevant sentences occur in close proximity to "highly" relevant sentences. Based on our examination of the data, we found that a high proportion of sentences with a high tf.idf score were relevant, but there was very little overall correlation between tf.idf score and relevancy. On the other hand, there was a high correlation between a sentence's relative distance to a highly relevant sentence and its relevancy. This rule uses the tf.idf results, but unlike the other two methods is not restricted to that list when looking for sentences. We scan all the sentences in a document, using the tf.idf scores and relative distances as input. The proximity model can indicate that a sentence is relevant even if it received a zero score in the tf.idf model.</p><p>Here is one example of a simple proximity model which calculates a relevance score R(i) for the i-th sentence in a document. It uses a window of nearby sentences with indexes {i -L, …, i + K}, and scores {S(i -L), … , S(i + K)}, where S(i) is the tf.idf score for sentence i, and K and L are small positive integers (typically 2 or 3).</p><p>∑</p><formula xml:id="formula_7" coords="8,214.08,201.72,183.87,32.89">+ - = ⋅ = K i L i j j j i j S i S g i i S f i R ) , ), ( ), ( ( ) ), ( ( ) (</formula><p>Here, the function f is used to weight the contribution of sentence i based on its score and, optionally, its position in the document. The function g j models the pair-wise relationship between sentences within the window, based on their scores and relative distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Finding Redundant Sentences</head><p>In this evaluation we focused on comparing pairs of sentences rather than more general sets of sentences. There are two main reasons for this. First, dealing with pairs is simpler and gives a good starting point before looking at the more general case. Second, when asked to find redundant facts in lists of sentences, most human assessors <ref type="bibr" coords="8,440.50,317.53,16.72,11.03" target="#b20">[20]</ref> (and we suppose, users) tend to focus on sentence pairs instead of complex subsets.</p><p>We view one redundant sentence as being a statistical translation of another. If we can build a good translation model in the language then we should be able to detect when two sentences are translations of the same thing. The task is simplified a little by the fact that the source and target sentences are in the same language. The methods we adopted for TREC use WordNet to estimate similarity for words and short phrases, and shallow parsing to help extract and compare sentence structures.</p><p>Given two sentences to compare, the algorithm has the following stages. First, we obtain parse trees for each sentence. For the TREC evaluation these were all pre-computed since parsing can be a time-consuming process. Second, we convert each parse tree into a graph that describes the modification structure of the terms. Third, we perform a simple graph matching algorithm that compares the terms from each sentence, weighted by their possible importance. The end result is a similarity measure that estimates how much one sentence is a translation of the other in the same language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A Statistical Method for Estimating Word Semantic Similarity</head><p>Before looking at sentence structure, we estimate a similarity score for each word or short phrase. We do this by comparing the contexts in which they occur. We use a specialized set of contexts: those derived from each of the basic relation types available for a word or phrase in WordNet.</p><p>Given two words or short phrases to compare, we first construct a unigram model of the context for each word. Each unigram model is a linear combination of unigram sub-models. There is one sub-model for each relation we used from WordNet, which were synonyms, hyponyms, hypernyms, and coordinate terms. For some words, some of these submodels will be empty if no relation exists. Each submodel is built from the terms appearing in the word lists and, optionally, the glossary entries for that relation. A set of mixture weights is used to combine the probabilities from the submodels to calculate the final unigram model. The weights are trained from a training set of redundant, semi-redundant, and unrelated sentences.</p><p>If we visualize the enormous graph of words that comprise WordNet, each word has a set of synonyms and other related words. These together form a subgraph associated with that word. To compare two words we are essentially measuring the weighted overlap between their two subgraphs.</p><p>We compute the distributional similarity of the two overall unigram models using skew divergence. Skew divergence is described by Lee <ref type="bibr" coords="8,128.24,670.44,11.71,11.03">[6]</ref> and has the advantage of competitive predictive performance on statistical language tasks similar to ours, without requiring sophisticated smoothing. If D(r || q) represents the KL divergence between distributions r and q, the skew divergence is:</p><formula xml:id="formula_8" coords="8,236.52,715.59,138.51,17.97">) ) 1 ( || ( ) , ( r q r D r q s α α α - + =</formula><p>where α is a smoothing parameter and is set to α=0.99 in our application.</p><p>Once the skew divergence is calculated, the score must be normalized. This is done by calculating a similarity score relative to a small fixed set of "unfamiliar" words that are extremely unlikely to all be similar to the target word. The final score is a real number between zero (identical match) and an arbitrary upper bound of 500 (maximum dissimilarity). Table <ref type="table" coords="9,525.01,106.09,4.97,11.03" target="#tab_6">8</ref> shows scores for the word "astronaut" compared to various words. Note that words like "orbit" share similar co-occurrence distributions with "astronaut" but, correctly, do not get low translation distance scores. Using distributional similarity may be seen as a type of query expansion. Unlike typical scenarios for query expansion, the terms being compared are coming from documents already deemed relevant, so the same word found in two different sentences is less likely to be used with two very different senses, making sense disambiguation less of a problem. There are other methods described to estimate the substitutability of words, e.g. the confusion probability <ref type="bibr" coords="9,439.76,371.04,10.68,11.03" target="#b2">[2]</ref>. We do not explore those here; Lee does a comparison of some of these in a recent paper <ref type="bibr" coords="9,308.16,382.56,10.64,11.03" target="#b5">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sentence Parsing and Modifier Graphs</head><p>After obtaining word translation probabilities, we analyze sentence structure by obtaining a parse of all sentences to be examined either in the document set or in the TREC description and narrative fields. There are currently two purposes for this parse. First, we pre-process surface features to be used by the relevancy classifier, and second, we derive a dependency graph from the parse tree to estimate headwords and modifier relations, and the relative "importance" of terms in a sentence, both of are used our simple translation model. For the actual parsing we used the Apple Pie Parser, an easy-to-use corpusbased probabilistic parser written in C and developed by S. Sekine <ref type="bibr" coords="9,322.33,475.56,15.43,11.03" target="#b13">[13]</ref>.</p><p>The algorithm for converting a parse tree to a dependency graph can be defined recursively, starting at the leaves of the tree:</p><p>1. A terminal node (leaf) depends only on itself and has itself as a headword.</p><p>2. Each possible type of non-terminal node has a rule to decide on the headword for that constituent, given the headwords derived from its leaves. The "winning" headword then becomes a new node in the dependency graph, with the "losing" headwords pointing to the new node and thus becoming dependants of the new node. For example, for the noun phrase NP → n 1 n 2 n 3 , we will choose n 3 (the last noun) as the headword, creating a new node for it, and creating edges pointing from n 1 and n 2 to n 3 . 3. This process is continued up the tree until the root is reached.</p><p>With this dependency graph and the word similarity scores, we can compute the final translation probability of two sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Graph Matching</head><p>We use a very simple graph matching step to model the fact that not all words in a sentence are equally "central". Some words or short phrases express the core ideas in a sentence, and other words act to modify them. Therefore, it seems reasonable to weight any matches between the core ideas in two sentences more highly than matches between other words. We currently define the graph weight of a node as its in-degree.</p><p>Our graph matching is currently "greedy": for each word W i with graph weight A i in the source sentence, we select the word V(i) with graph weight B V(i) with lowest similarity distance S i,V(i) in the target sentence. This is constrained by a limit of at most K matches to any target word, where K = 2 in our implementation. Once a target word has reached its match limit, the word with the next-lowest distance is used instead. The weighting factor for the i-th observation is A i •B i , and so the matching score between the sentences is:</p><formula xml:id="formula_9" coords="10,243.00,86.06,124.36,30.46">∑ = = N i i V i i V i S B A B A M 0 ) ( , ) ( ) , (</formula><p>where N is an adjustable parameter (typically between 6 and 10) to be used when the source words are sorted in descending order of influence.</p><p>We show an example below for two sentences A and B taken from the TREC sample documents. The similarity score of Sentence A against Sentence B is the weighted mean described above: 14.821. Since this is below our threshold of 15, these sentences would be considered redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence A:</head><p>Some of the best shots, released this month by the US space agency Nasa, show parts of the universe billions of light years away -and therefore billions of years in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence B:</head><p>The images sent back this year, after astronauts repaired the telescope's defective mirror, show a myriad of astronomical objects too distant to be seen with the most powerful Earth-bound observatories. There is still plenty of work to do on the best features to represent in the graph, and the most appropriate, theoretically justified matching algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Novelty Track Results</head><p>We now give a brief summary of our official results, for both relevance and novelty, for the five runs we submitted. The bestperforming runs are shown in boldface.</p><p>The runs are labeled as 'relevance algorithm + novelty algorithm'. The relevance algorithms Simple_Threshold, Decision_Tree (DT), and Proximity are those described previously. The novelty algorithms LowSameDoc and HighDiffDoc use the simple statistical translation approach. These labels refer to, respectively, a low threshold (sentences must be very similar in meaning) comparing sentences only within the same document, or a high threshold (more relaxed redundancy definition) comparing sentences only across different documents.</p><p>The official TREC scores we achieved for each run are shown in Table <ref type="table" coords="10,348.54,607.08,8.27,11.03" target="#tab_8">10</ref>, which gives average precision (Ave P), average recall (Ave R), and average P•R scores. Our best average P•R score for relevance (0.058) was achieved with a simple tf.idf threshold on the ranked list of sentences. Our best average P•R novelty score (0.047) was achieved by selecting highly relevant sentences with the Proximity rule and then accepting all such candidates as novel, with our statistical approach performing marginally worse when applied to all sentences within the same document. Comparative results, relative to the median P•R across all systems, are given in Table <ref type="table" coords="11,402.86,175.45,8.32,11.03" target="#tab_9">11</ref>. The results are given as a fraction of the total number of queries (49). Because the scoring scale is continuous, we label as "at the median" any P•R score within ±0.01 of the median P•R score. Three of the runs used the same relevance algorithm (Proximity) and so these are collapsed into one entry. Overall, 4 out of 5 of our novelty runs had more than 50% of the scores at or above the median. Our best run, Proximity + All Novel, had 42.9% of scores above the median, and 91.8% of scores at or above the median. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Conclusions</head><p>The problem of finding specific relevant sentences seems quite difficult. Easier and perhaps more helpful would be to find precise zones of relevance which include more context. In any case, our simple proximity model gave better performance than a more sophisticated decision tree method. The decision tree did not include the same proximity model, so there is some chance that combining the two methods might give better performance: the representation may have made the difference here. The very high proportion of sentences judged as novel made it easy for the trivial "accept everything as novel" algorithm to do well. As a result, overall system performance on this track was dominated by the ability to find relevant sentences.</p><p>We described a word semantic similarity measure based on comparing word contexts from WordNet. Other queryexpansion-type techniques, such as LSI might work as well or better. Wordnet is interesting because it allows some flexibility in how different similarity "features", such as synonyms, hyponyms, coordinate terms, and so on, are combined. Unfortunately, calling Wordnet and building language models is very slow, so pre-computing the LSI matrix might be a good compromise.</p><p>The greedy graph-matching approach is a first step in a direction we think is promising. It's clear that using only the indegree of word nodes is not a sufficient indication of their importance in many cases. The current algorithm does tend to find good, similar sentences, but is still too tolerant of differences in the lesser-weighted areas of the graph. Among other things, named entities could use special treatment. With more work we think it should be possible to create a much more accurate alignment model for redundant sentences and passages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,61.20,700.09,237.46,11.04;3,61.20,711.61,237.60,11.03;3,61.20,723.13,60.93,11.03;3,61.20,518.40,233.88,175.56"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Compare our result (CMUDIR) with the best performance (Max) and the Median Performance (Med) for each Topic.</figDesc><graphic coords="3,61.20,518.40,233.88,175.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,316.56,700.09,237.45,11.04;3,316.56,711.61,237.59,11.03;3,316.56,723.13,42.93,11.03;3,319.56,518.40,233.88,175.56"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: TREC annotators create the first 50 topics, and the last 50 topics are created by intersection of Reuter's categories.</figDesc><graphic coords="3,319.56,518.40,233.88,175.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,164.40,240.76,283.15,99.56"><head>Table 2 :</head><label>2</label><figDesc>T11U performance with different thresholding algorithms.</figDesc><table coords="2,164.40,240.76,283.15,80.84"><row><cell></cell><cell>TREC8</cell><cell>TREC9</cell><cell>TREC11</cell></row><row><cell>NE</cell><cell>0.324</cell><cell>0.360</cell><cell>0.365</cell></row><row><cell>ML</cell><cell>0.340</cell><cell>0.363</cell><cell>0.373</cell></row><row><cell>Empirical Optimal</cell><cell>0.213</cell><cell>0.344</cell><cell>0.315</cell></row><row><cell>Logistic Regression</cell><cell>0.212</cell><cell>0.300</cell><cell>0.289</cell></row><row><cell>Bayesian Error Model</cell><cell>0.268</cell><cell>0.303</cell><cell>0.311</cell></row><row><cell>Greedy Search</cell><cell>0.051</cell><cell>0.155</cell><cell>0.090</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,150.36,71.17,311.72,242.04"><head>Table 3 :</head><label>3</label><figDesc>T11U performance with different initial profile settings.</figDesc><table coords="3,150.36,71.17,311.72,242.04"><row><cell>Title</cell><cell></cell><cell></cell><cell cols="2">0.3185</cell><cell>0.3621</cell></row><row><cell>Title + Description</cell><cell></cell><cell></cell><cell cols="2">0.3297</cell><cell>0.3732</cell></row><row><cell cols="3">Title + Description + Mixture model</cell><cell cols="2">0.3405</cell><cell>0.3736</cell></row><row><cell cols="2">for reweighing words</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Title + Description + Mixture model</cell><cell cols="2">0.3524</cell><cell>0.3668</cell></row><row><cell>for feature selection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Title + Description + Narrative</cell><cell></cell><cell cols="2">0.3559</cell><cell>0.3669</cell></row><row><cell cols="2">Title + Description + Narrative +</cell><cell></cell><cell cols="2">0.3424</cell><cell>0.3741</cell></row><row><cell cols="3">Mixture model for reweighing words</cell><cell></cell><cell></cell></row><row><cell cols="2">Title + Description + Narrative +</cell><cell></cell><cell cols="2">0.3402</cell><cell>0.3673</cell></row><row><cell cols="3">Mixture model for feature selection</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">UDESC</cell><cell>FDESC</cell><cell>Uml</cell><cell>Fml</cell></row><row><cell>= Max</cell><cell></cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>&gt; Med</cell><cell></cell><cell>97</cell><cell>90</cell><cell>92</cell><cell>88</cell></row><row><cell>&lt; Med</cell><cell></cell><cell>3</cell><cell>10</cell><cell>8</cell><cell>12</cell></row><row><cell>Topic</cell><cell>T11U</cell><cell>0.445</cell><cell cols="3">0.431 0.447 0.433</cell></row><row><cell>1-50</cell><cell>T11F</cell><cell>0.422</cell><cell cols="3">0.401 0.410 0.396</cell></row><row><cell>Topic</cell><cell>T11U</cell><cell>0.291</cell><cell cols="3">0.293 0.290 0.292</cell></row><row><cell cols="2">51-100 T11F</cell><cell>0.041</cell><cell cols="3">0.038 0.034 0.035</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,221.64,320.89,168.98,11.04"><head>Table 4 :</head><label>4</label><figDesc>Performance of our official runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,96.00,61.60,420.28,154.88"><head>Table 5 :</head><label>5</label><figDesc>Results of the homepage finding task on the WT10G testbed</figDesc><table coords="6,96.00,61.60,420.28,154.88"><row><cell cols="2">Configuration</cell><cell>MRR</cell><cell>% TOP 10</cell><cell>% FAIL</cell></row><row><cell cols="2">Equal lambdas</cell><cell>.676</cell><cell>83.4</cell><cell>5.5</cell></row><row><cell cols="2">Equal lambdas + URL length prior</cell><cell>.799</cell><cell>91.7</cell><cell>3.4</cell></row><row><cell cols="2">Representation Description</cell><cell>MRR</cell><cell>% TOP 10</cell><cell>% FAIL</cell></row><row><cell>Alt</cell><cell>Image alternate text</cell><cell>.194</cell><cell>28.0</cell><cell>66.7</cell></row><row><cell>Font</cell><cell>Changed font sizes and headings</cell><cell>.191</cell><cell>25.3</cell><cell>68.0</cell></row><row><cell>Full</cell><cell>Full document text</cell><cell>.469</cell><cell>66.7</cell><cell>16.7</cell></row><row><cell>Link</cell><cell>In-link text</cell><cell>.455</cell><cell>58.0</cell><cell>32.0</cell></row><row><cell>Meta</cell><cell>Meta tags (keyword, description)</cell><cell>.144</cell><cell>21.3</cell><cell>75.3</cell></row><row><cell>Title</cell><cell>Document title</cell><cell>.407</cell><cell>56.0</cell><cell>35.3</cell></row><row><cell>URL</cell><cell>Character trigram on URL</cell><cell>.131</cell><cell>19.3</cell><cell>68.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,86.64,224.17,438.99,113.28"><head>Table 6 :</head><label>6</label><figDesc>Performance of individual document representations on the named-page finding task</figDesc><table coords="6,86.64,245.56,438.99,91.88"><row><cell>Run</cell><cell>Configuration</cell><cell>MRR</cell><cell>% TOP 10</cell><cell>% FAIL</cell></row><row><cell>LmrAllEq</cell><cell>Alt+font+full+link+meta+title+url</cell><cell>.676</cell><cell>88.0</cell><cell>3.3</cell></row><row><cell></cell><cell>(equal parameters)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LmrAllEst</cell><cell>Alt+font+full+link+meta+title+url</cell><cell>.667</cell><cell>86.7</cell><cell>3.3</cell></row><row><cell></cell><cell>(trained parameters)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LmrSmall</cell><cell>Link+font+title</cell><cell>.589</cell><cell>73.3</cell><cell>16.7</cell></row><row><cell cols="2">LmrDocStruct Alt+font+full+meta+title</cell><cell>.567</cell><cell>72.7</cell><cell>15.3</cell></row><row><cell>LmrNoStruct</cell><cell>Full+link</cell><cell>.611</cell><cell>84.0</cell><cell>8.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,66.84,158.64,478.47,165.95"><head>Table 8 :</head><label>8</label><figDesc>Semantic similarity "distances" of various words from the word "astronaut", as measured by normalized skew divergence of Wordnet-based unigram mixture models.</figDesc><table coords="9,229.32,158.64,164.51,131.03"><row><cell>astronaut</cell><cell>astronaut</cell><cell>0.000</cell></row><row><cell>astronaut</cell><cell>cosmonaut</cell><cell>0.002</cell></row><row><cell>astronaut</cell><cell>man</cell><cell>9.051</cell></row><row><cell>astronaut</cell><cell>explorer</cell><cell>14.372</cell></row><row><cell>astronaut</cell><cell>commander</cell><cell>20.877</cell></row><row><cell>astronaut</cell><cell>pilot</cell><cell>33.503</cell></row><row><cell>astronaut</cell><cell>traveler</cell><cell>49.312</cell></row><row><cell>astronaut</cell><cell>watermelon</cell><cell>153.548</cell></row><row><cell>astronaut</cell><cell>orbit</cell><cell>283.162</cell></row><row><cell>astronaut</cell><cell>rocket</cell><cell>294.722</cell></row><row><cell>astronaut</cell><cell>committee</cell><cell>302.601</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,57.12,293.44,497.67,162.19"><head>Table 9 :</head><label>9</label><figDesc>Comparison of word pairs from Sentence A and B, in order of influence. Only the top six word pairs, as sorted by Sentence A graph weight, are used for this example. Each word from Sentence A is paired with the word from Sentence B with the lowest similarity distance.</figDesc><table coords="10,101.76,293.44,400.88,115.64"><row><cell>A</cell><cell>Graph</cell><cell>Sentence B</cell><cell>Graph</cell><cell>Similarity Distance</cell></row><row><cell></cell><cell>weight</cell><cell>(Most similar word)</cell><cell>weight</cell><cell>S i</cell></row><row><cell>past</cell><cell>A i 6</cell><cell>year</cell><cell>B i 4</cell><cell>2.456</cell></row><row><cell>years</cell><cell>3</cell><cell>year</cell><cell>4</cell><cell>0.0258</cell></row><row><cell>released</cell><cell>4</cell><cell>show</cell><cell>2</cell><cell>53.631</cell></row><row><cell>Nasa</cell><cell>5</cell><cell>powerful</cell><cell>1</cell><cell>68.240</cell></row><row><cell>show</cell><cell>2</cell><cell>show</cell><cell>2</cell><cell>0.000</cell></row><row><cell>billions</cell><cell>3</cell><cell>myriad</cell><cell>1</cell><cell>0.152</cell></row><row><cell cols="2">Weighted mean: 14.821</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,86.88,56.92,434.43,106.52"><head>Table 10 :</head><label>10</label><figDesc>Official Novelty Track Results by Run.</figDesc><table coords="11,86.88,56.92,434.43,80.96"><row><cell></cell><cell></cell><cell>Relevance</cell><cell></cell><cell></cell><cell>Novelty</cell><cell></cell></row><row><cell></cell><cell>Ave P</cell><cell>Ave R</cell><cell>Ave P•R</cell><cell>Ave P</cell><cell>Ave R</cell><cell>Ave P•R</cell></row><row><cell>Proximity + LowSameDoc</cell><cell>0.13</cell><cell>0.31</cell><cell>0.052</cell><cell>0.12</cell><cell>0.30</cell><cell>0.046</cell></row><row><cell>Proximity + HighDiffDoc</cell><cell>0.13</cell><cell>0.31</cell><cell>0.052</cell><cell>0.12</cell><cell>0.16</cell><cell>0.025</cell></row><row><cell>Proximity + All Novel</cell><cell>0.13</cell><cell>0.31</cell><cell>0.052</cell><cell>0.12</cell><cell>0.31</cell><cell>0.047</cell></row><row><cell>DT + LowSameDoc</cell><cell>0.10</cell><cell>0.13</cell><cell>0.019</cell><cell>0.10</cell><cell>0.13</cell><cell>0.018</cell></row><row><cell>Simple_Threshold + HighAllDoc</cell><cell>0.17</cell><cell>0.23</cell><cell>0.058</cell><cell>0.16</cell><cell>0.18</cell><cell>0.043</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="11,86.04,247.24,439.90,210.19"><head>Table 11 :</head><label>11</label><figDesc>Comparative Novelty Track Results for Ave P•R scores, as a fraction of the total number of queries Runs labeled "at median" have an average P•R score within 0.01 of the median.</figDesc><table coords="11,150.84,247.24,310.33,163.64"><row><cell></cell><cell>Below</cell><cell>At</cell><cell>Above</cell><cell>At or</cell></row><row><cell>(relevance + novelty algorithm</cell><cell>Median</cell><cell>Median</cell><cell>Median</cell><cell>Above</cell></row><row><cell>shown)</cell><cell></cell><cell></cell><cell></cell><cell>Median</cell></row><row><cell>Relevance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Simple_Threshold</cell><cell>0.367</cell><cell>0.347</cell><cell>0.286</cell><cell>0.633</cell></row><row><cell>Decision_Tree (DT)</cell><cell>0.694</cell><cell>0.265</cell><cell>0.041</cell><cell>0.306</cell></row><row><cell>Proximity</cell><cell>0.204</cell><cell>0.510</cell><cell>0.286</cell><cell>0.796</cell></row><row><cell>Novelty</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Simple_Threshold + HighAllDoc 0.286</cell><cell>0.388</cell><cell>0.327</cell><cell>0.714</cell></row><row><cell>Proximity + LowSameDoc</cell><cell>0.796</cell><cell>0.204</cell><cell>0</cell><cell>0.204</cell></row><row><cell>Proximity + HighDiffDoc</cell><cell>0.347</cell><cell>0.490</cell><cell>0.163</cell><cell>0.653</cell></row><row><cell>Proximity + All Novel</cell><cell>0.082</cell><cell>0.490</cell><cell>0.429</cell><cell>0.918</cell></row><row><cell>DT + LowSameDoc</cell><cell>0.469</cell><cell>0.408</cell><cell>0.122</cell><cell>0.531</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Acknowledgements</head><p>This material is based on work supported by <rs type="funder">NSF</rs> grants <rs type="grantNumber">IIS-0096139</rs> and <rs type="grantNumber">EIA-9983253</rs>. Any opinions, findings, conclusions, or recommendations expressed in this material are the authors', and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_q3Z9vNY">
					<idno type="grant-number">IIS-0096139</idno>
				</org>
				<org type="funding" xml:id="_U7nZr7g">
					<idno type="grant-number">EIA-9983253</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,59.85,116.52,85.37,12.99" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,132.85,481.55,11.03;12,72.00,144.37,464.83,11.04;12,72.00,155.89,66.99,11.03" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,339.61,132.85,213.93,11.03;12,72.00,144.37,253.07,11.03">KUN on the TREC-9 Filtering Track: Incrementality, decay, and threshold optimization for adaptive filtering systems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H A</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">P</forename><surname>Van Der Weide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,342.60,146.44,194.23,8.96;12,72.00,157.96,37.20,8.96">Proceedings of Ninth Text REtrieval Conference (TREC-9)</title>
		<meeting>Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,171.36,466.06,11.04;12,72.00,182.88,428.65,11.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,200.67,171.36,228.06,11.03">Generalizing automatically generated selectional patterns</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Sterling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,448.53,171.36,89.53,11.04;12,72.00,184.96,244.16,8.96">the Proceedings of the 15th. International Conference on Computational Linguistics</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="742" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,198.36,466.22,11.04;12,72.00,209.88,176.67,11.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,195.58,198.36,161.65,11.03">Overview of the TREC-2001 Web Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,377.52,200.44,160.70,8.96;12,72.00,211.96,93.21,8.96">Proceedings of the Tenth Text REtrieval Conference (TREC-10)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC-10)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="61" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,225.36,435.90,11.03;12,72.00,238.84,87.55,8.96;12,159.60,236.65,5.03,5.83;12,167.16,238.84,369.42,8.96;12,72.00,248.41,149.46,11.04" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,251.30,225.36,238.98,11.03">The Importance of Prior Probabilities for Entry Page Search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,72.00,238.84,87.55,8.96;12,159.60,236.65,5.03,5.83;12,167.16,238.84,369.42,8.96;12,72.00,250.48,91.54,8.96">Proceedings of the 25 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2002)</title>
		<meeting>the 25 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2002)</meeting>
		<imprint>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,263.77,468.13,11.03;12,72.00,275.41,114.78,11.04" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,102.94,263.77,316.26,11.03">On the Effectiveness of the Skew Divergence for Statistical Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,425.89,263.77,114.24,11.03;12,72.00,277.48,36.03,8.96">In Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="65" to="72" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,290.77,461.76,11.04;12,72.00,302.41,187.59,11.04" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,102.95,290.77,148.49,11.03">Measures of Distributional Similarity</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.63,290.77,265.12,11.04;12,72.00,304.48,104.96,8.96">the Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,317.89,391.62,11.03" xml:id="b7">
	<monogr>
		<ptr target="http://www.cs.cmu.edu/~lemur" />
		<title level="m" coord="12,72.00,317.89,261.04,11.03">The Lemur toolkit for language modeling in information retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,333.25,439.84,11.03;12,72.00,344.89,60.48,11.03" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<ptr target="http://www.stat.cmu.edu/~minka/papers/minka-threshold.ps.gz" />
		<title level="m" coord="12,114.27,333.25,174.44,11.03">Bayesian Analysis of a Threshold Classifier</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,360.25,483.83,11.04;12,72.00,371.89,135.65,11.03" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,174.61,360.25,151.25,11.03">Experiments Using the Lemur Toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,345.48,362.32,205.94,8.96">Proceedings of the Tenth Text Retrieval Conference</title>
		<meeting>the Tenth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,387.25,438.93,11.04;12,510.96,387.13,4.32,5.83;12,517.80,389.32,28.79,8.96;12,72.00,398.77,482.58,11.04;12,72.00,410.41,17.58,11.03" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,175.47,387.25,230.63,11.03">A Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,423.24,389.32,87.69,8.96;12,510.96,387.13,4.32,5.83;12,517.80,389.32,28.79,8.96;12,72.00,400.84,432.12,8.96">Proceedings of the 21 st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998)</title>
		<meeting>the 21 st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998)</meeting>
		<imprint>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,425.89,324.97,11.04" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m" coord="12,129.72,427.96,154.13,8.96">C4.5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,441.37,381.44,11.04" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,139.79,441.37,150.20,11.03">Threshold setting in adaptive filtering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,309.12,443.44,103.53,8.96">Journal of Documentation</title>
		<imprint/>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct coords="12,72.00,456.97,324.13,11.03" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<ptr target="http://www.cs.nyu.edu/cs/projects/proteus/app/" />
		<title level="m" coord="12,114.80,456.97,83.83,11.03">The Apple Pie Parser</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,472.33,472.31,11.03;12,72.00,483.97,347.45,11.03" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,251.25,472.33,260.44,11.03">Retrieving Web Pages Using Content, Links, URLs, and Anchors</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,532.09,472.33,12.22,11.03;12,72.00,486.04,254.13,8.96">the Proceedings of the Tenth Text REtrieval Conference (TREC-10)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="663" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,499.33,404.71,11.04;12,476.76,499.21,6.47,5.83;12,485.76,501.40,28.90,8.96;12,72.00,510.85,482.23,11.04;12,72.00,522.37,17.58,11.03" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,153.11,499.33,218.97,11.03">Cluster-based language models for distributed retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,389.04,501.40,87.67,8.96;12,476.76,499.21,6.47,5.83;12,485.76,501.40,28.90,8.96;12,72.00,512.92,431.88,8.96">Proceedings of the 22 nd Annual International Conference ACM SIGIR on Research and Development in Information Retrieval (SIGIR 1999)</title>
		<meeting>the 22 nd Annual International Conference ACM SIGIR on Research and Development in Information Retrieval (SIGIR 1999)</meeting>
		<imprint>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,537.84,485.65,11.03;12,72.00,549.25,98.35,11.04;12,170.40,549.13,5.03,5.83;12,177.96,551.32,369.42,8.96;12,72.00,560.89,159.41,11.04" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,170.64,537.84,383.10,11.03">A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,82.80,551.32,87.55,8.96;12,170.40,549.13,5.03,5.83;12,177.96,551.32,369.42,8.96;12,72.00,562.96,91.54,8.96">Proceedings of the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2001)</title>
		<meeting>the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2001)</meeting>
		<imprint>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,576.37,446.92,11.04" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,171.64,576.37,68.79,11.03">YFilter at TREC9</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,261.72,578.44,229.94,8.96">Proceeding of Ninth Text REtrieval Conference (TREC-9)</title>
		<meeting>eeding of Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,591.97,460.01,11.04" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,171.65,591.97,73.87,11.03">YFilter at TREC10</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,266.76,594.04,190.93,8.96">Proceeding of Tenth Text REtrieval Conference</title>
		<meeting>eeding of Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,607.33,418.12,11.04;12,490.20,607.21,5.03,5.83;12,497.76,609.40,28.90,8.96;12,72.00,620.92,436.40,8.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,154.77,607.33,230.24,11.03">Maximum Likelihood Estimation for Filtering Thresholds</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,402.48,609.40,87.64,8.96;12,490.20,607.21,5.03,5.83;12,497.76,609.40,28.90,8.96;12,72.00,620.92,377.32,8.96">Proceedings of the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,634.33,479.72,11.03;12,551.76,634.21,5.03,5.83;12,72.00,647.80,467.73,8.96;12,72.00,657.37,51.07,11.03" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,216.37,634.33,228.35,11.03">Novelty and Redundancy Detection in Adaptive Filtering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,464.04,636.40,87.68,8.96;12,551.76,634.21,5.03,5.83;12,72.00,647.80,463.45,8.96">Proceedings of the 25 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2002)</title>
		<meeting>the 25 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2002)</meeting>
		<imprint>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,672.73,477.28,11.04;12,72.00,686.44,246.09,8.96" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,186.41,672.73,235.28,11.03">Exact Maximum Likelihood Estimation for Word Mixtures</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,439.56,674.80,109.72,8.96;12,72.00,686.44,190.65,8.96">Text Learning Workshop in International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
