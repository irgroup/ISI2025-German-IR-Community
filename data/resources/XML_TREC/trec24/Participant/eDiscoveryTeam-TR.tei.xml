<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.68,76.39,287.09,12.79">e-Discovery Team at TREC 2015 Total Recall Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,90.23,110.27,71.53,11.03;1,161.76,111.78,0.72,3.98"><forename type="first">Ralph</forename><forename type="middle">C</forename><surname>Losey</surname></persName>
							<email>ralph.losey@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">National e-Discovery Counsel Sr. Discovery Services Consultants</orgName>
								<orgName type="institution">Jackson Lewis P.C. Kroll Ontrack, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.23,110.27,58.37,11.03"><forename type="first">Jim</forename><surname>Sullivan</surname></persName>
							<email>jsullivan@kollontrack.com</email>
							<affiliation key="aff0">
								<orgName type="department">National e-Discovery Counsel Sr. Discovery Services Consultants</orgName>
								<orgName type="institution">Jackson Lewis P.C. Kroll Ontrack, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.41,110.27,99.65,11.03"><forename type="first">Tony</forename><surname>Reichenberger</surname></persName>
							<email>treichenberger@krollontrack.com</email>
							<affiliation key="aff0">
								<orgName type="department">National e-Discovery Counsel Sr. Discovery Services Consultants</orgName>
								<orgName type="institution">Jackson Lewis P.C. Kroll Ontrack, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.68,76.39,287.09,12.79">e-Discovery Team at TREC 2015 Total Recall Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D75F4BC19B4BE36143C7364CE4FCB4B9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 Information Search and Retrieval: Search process, relevance feedback, supervised learning, best practices Hybrid Multimodal</term>
					<term>AI-enhanced review</term>
					<term>predictive coding</term>
					<term>predictive coding 3.0</term>
					<term>electronic discovery</term>
					<term>e-discovery</term>
					<term>legal search</term>
					<term>active machine learning</term>
					<term>continuous active learning</term>
					<term>CAL</term>
					<term>Computer-assisted review</term>
					<term>CAR</term>
					<term>Technology-assisted review</term>
					<term>TAR</term>
					<term>relevant irrelevant training ratios</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 2015 TREC Total Recall Track provided instant relevance feedback in thirty prejudged topics searching three different datasets. The e-Discovery Team of three attorneys specializing in legal search participated in all thirty topics using Kroll Ontrack's search and review software, eDiscovery.com Review (EDR). They employed a hybrid approach to continuous active learning that uses both manual and automatic searches. A variety of manual search methods were used to find training documents, including high probability ranked documents and keywords, an ad hoc process the Team calls multimodal.</p><p>In the one topic (109) requiring legal analysis the Team's approach was significantly more effective than all other participants, including the fully automated approaches that otherwise attained comparable scores. In all topics the Team's hybrid multimodal method consistently attained the highest F1 values at the time of Reasonable Call, equivalent to a stop point. In all topics the Team's multimodal human machine approach also found relevant documents more quickly and with greater precision than the fully automated or other methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="46" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="47" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="48" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="49" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="50" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="51" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="52" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="53" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="54" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="55" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="56" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="57" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="58" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="59" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="60" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="61" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="62" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="63" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="64" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="65" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="66" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="67" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="68" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="69" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="70" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="71" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="72" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="73" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="74" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="75" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="76" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="77" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="78" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="79" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="80" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="81" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="82" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="83" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="84" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="85" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="86" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="87" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="88" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="89" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="90" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="91" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="92" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="93" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="94" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="95" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="96" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="97" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="98" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="99" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="100" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="101" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="102" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="103" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="104" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="105" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="106" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="107" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="108" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="109" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="110" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="111" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="112" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="113" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="114" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="115" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="116" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The e-Discovery Team participated in all thirty Total Recall Track topics in the Athome group where both manual and automatic methods were permitted. The Team is composed of three practicing attorneys who specialize in legal search. They used Kroll Ontrack's search and review software, eDiscovery.com Review ("EDR"), employing what they call a hybrid multimodal method. <ref type="bibr" coords="1,128.32,564.03,3.53,6.40">1</ref> They attained high recall and precision in most of the thirty topics. The few exceptions appear derived from the fact that the attorneys are accustomed to self-defining the ground truth, and, in some topics, their opinions on relevance differed significantly from the TREC assessors. In later topics the attorney Team learned to turn off their own judgments and rely primarily on their software's automated processes, which generally led to improved scores better matching the TREC relevance assessments. The Team's manual efforts, as measured by time expended and number of documents manually reviewed, were very low by legal search standards.</p><p>The fully automatic methods employed by the Sandbox group participants in the Total Recall Track attained comparable high recall and precision in most topics. The Team's hybrid multimodal method did, however, consistently attain the highest F1 values at the time of Reasonable Call, equivalent to a training stop point, which is very important to legal search. One of the thirty topics, 109 -Scarlet Letter Law -required a small amount of legal knowledge and analysis to understand relevance (most of the others required none). On this topic our legal team, as you would expect, attained significantly better results than the fully automated methods that contained no base legal knowledge.</p><p>The e-Discovery Team's hybrid multimodal method is a type of continuous active learning text retrieval system that employs supervised machine learning and a variety of manual search methods. 2, <ref type="bibr" coords="2,140.30,205.79,3.53,7.76">3</ref> The Team attained very high recall and precision rates in most, but not all, of the thirty Total Recall topics. The Team's F1 scores at the time of Reasonable Call ranged from a perfect score of 100% in one topic (3484), to 91% to 99% in eight topics, and 82%-87% in five others. Although, of course, not directly comparable, these scores are far higher than any previously recorded in the six years of TREC Legal Track (2006-2011) or any other study of legal search. One reason for this may be that the thirty topics in the 2015 Total Recall track presented relatively simple information needs by legal search standards, with one exception (Topic 109 -Scarlet Letter Law). Another may be improved software and the Team's improved hybrid multimodal method that includes continuous active learning. The e-Discovery Team was able to find the target relevant documents in all thirty topics with relatively little human effort and almost no legal analysis. Only Topic 109 required legal knowledge and analysis, with four others -101, 105, 106, 107 -requiring some small measure of analysis.</p><p>A total of 16,576,798 documents were classified in thirty topics. Of these documents 70,414 were predetermined by TREC assessors to be relevant. The e-Discovery Team found these relevant documents by manual review of only 32,916 documents. The other 37,498 relevant documents were found with no human review of these documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Total Recall Track Description -Athome and Sandbox.</head><p>The Total Recall track offered 30 different pre-judged topics for search in two different divisions, Athome and Sandbox. Our Team only participated in the Athome experiments. In the Athome experiments the data was loaded onto the participants' own computers. There were no restrictions on the types of searches that could be performed. The setup allowed the e-Discovery Team to use a slightly modified version of our standard Hybrid Multimodal method, which, as mentioned, employs both ad hoc manual review and machine learning.</p><p>The Sandbox participants were only permitted to use fully automated systems and the data remained on TREC administrator computers. They searched the same three datasets as Athome, plus two more not included in the Athome division due to confidentiality restrictions. The Sandbox participants were prohibited from any manual review of documents or ad hoc search adjustments. <ref type="bibr" coords="2,148.48,581.87,3.53,7.76">4</ref> Even after the submissions ended, the Sandbox participants reported at the Conference that they never looked at any documents, even the unrestricted Athome shared datasets. They never made any effort to determine where their software made errors in predicting relevance, or for any other reasons. To these participants, all of whom were academic institutions, the ground truth itself was of no relevance.</p><p>Three different datasets were searched in both the Athome and Sandbox events, with the same ten topics in each. Even though the data searched and topics overlapped in the two divisions, none of the participants in one division participated in the other division. This is unfortunate because it makes direct comparisons problematic, if not impossible, especially as to the software systems used. It is hope that some participants will participate in both events in future Total Recall tracks.</p><p>The e-Discovery Team participated in all thirty of the Athome topics. We were the only manual participant to do so, with all others completing ten or fewer topics. The lack of participation by others in the Athome group also make meaningful comparisons very difficult or impossible, but we note that the e-Discovery Team's scores were consistently higher than any other Athome participants.</p><p>At Home participants were asked to track and report their manual efforts. The e-Discovery Team did this by recording the number of documents that were human reviewed and classified.</p><p>Virtually all documents human reviewed were also classified, although all documents classified were not used for active training of the software classifier. Moreover 53% of the relevant documents used for training were never human reviewed. We also tracked effort by number of attorney hours worked as is traditional in legal services.</p><p>The Team used Kroll Ontrack's software, known as eDiscovery.com Review, or EDR, which includes active machine learning features, a/k/a predictive coding in legal search. EDR employs a proprietary probabilistic type of logistic regression algorithm for document classification and ranking.</p><p>The At Home participants used their own computer systems and software for search, and then submitted documents to the TREC administrator that they considered relevant. TREC set up a "jig" whereby instant feedback was provided to a participant as whether each document submitted as relevant was in fact previously judged to have been relevant by TREC assessors. When a participant determined that a reasonable effort had been made to find all relevant documents required, which is important in legal search and represents a stopping point for further machine training and document review, they would notify TREC of this supposition and "Call Reasonable." Continued submissions were made after that point so that all documents were classified as either relevant or irrelevant. The goal as we understood it was to submit as many relevant documents as possible before the Reasonable call, and thereafter to have all false negatives appear in submissions as soon after the Reasonable Call as possible.</p><p>Most of the thirty topics presented only simple, single-issue information needs suitable for single-facet classification. Further, only a few of the topics required any legal analysis for relevance identification. These two factors, plus the omission of metadata, was, we think, a disadvantage to the e-Discovery Team of lawyers. Conversely, it appears that these same factors made it simpler for the academic Sandbox participants to perform well in most topics using fully automated methods. It should also be noted that although our lawyer Team was practiced and skilled in complex information needs requiring extensive legal analysis, and had long experience with projects using SME defined ground truths, none had any prior experience using machine learning for the types of searches presented in the 2015 Recall Track.</p><p>The one exception that brought in legal analysis with beneficial SME analysis, was Topic 109, Scarlett Letter Law. It required some legal knowledge, albeit very rudimentary, to begin locating relevant documents. The keywords alone -"Scarlett Letter Law" -would only find relevant documents with this word combination and similar text patterns. These words were just the nickname of the proposed and eventually enacted Florida Statute. Any attorney would know that to find relevant information they would not only have to search the name, but they would also have to search the various house and senate bill numbers for this law. These numbers would not often appear in the same document as the nickname, and since the machine did not know to search for these numbers, it did not realize the significance. Eventually the automated machine learning did see the connection, after many relevance feedback submissions. These submissions and instant feedback of relevant, or not, would, of course, not happen in real legal search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Governor Bush Email</head><p>The first set of Athome Topics searched a corpus of 290,099 emails of Florida Governor Jeb Bush. Most of the metadata of these emails and associated attachments and images had been stripped and converted to pure text files. This increased the difficulty of the Team's search, which normally includes a mixture of metadata specific searches.</p><p>A significant percentage of the Bush emails were form type lobbying emails from constituents, which repeated the same language with little of no variance. The unusually high prevalence of near-duplicate emails made search of many of the Bush topics easier than is typical in legal search.</p><p>The ten Bush email topics searched, and their names, which were the only guidance on relevance provided to either the Athome or Sandbox participants, are shown below. E-Discovery Team leader, Ralph Losey, a lifelong Florida native, personally searched each of these ten Topics. In about half of the topics his personal knowledge of the issues was helpful, but in several others it was detrimental. He had definite preconceptions of what emails he thought should be relevant and these sometimes differed significantly from the TREC assessors. In all of the Bush Topics Losey was at least somewhat assisted by a single "contract review attorney." <ref type="bibr" coords="4,136.12,495.99,3.53,6.52">5</ref> The contract attorneys in most of these ten Topics did a majority of the document review under Losey's very close supervision, but had only limited involvement in initial keyword searches, and no involvement in predictive coding searches or related decisions.</p><p>All participants in the 2015 Recall Track were required to complete all ten of the Bush Email Topics. Completion of the other twenty Topics in the two other data collections was optional. Several participants started review of the Bush Topics, but did not finish, and thus were not permitted to submit a report or attend the TREC Conference. Only one other Athome participant, Catalyst, completed all ten Bush Topics. No other Athome participants even attempted the other twenty topics, and thus comparisons with the e-Discovery Team's results are limited to the fully automatic participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Black Hat World Forums.</head><p>The second set of Athome Topics searched a corpus of 465,149 posts taken from Black Hat World Forums. Again, almost all metadata of these posts and associated images had been stripped and converted to pure text files. The ten topics searched, and their names, which again were the only guidance initially provided on relevance, are shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HYBRID MULTIMODAL APPROACH</head><p>The e-Discovery Team approach includes all types of search methods, with primary reliance placed on predictive coding and the use of high-ranked documents for continuous active training. In that way it is similar to the approach used by Grossman and Cormack, 11 but differs in that the Team uses a multimodal selection of search methods to locate suitable training documents, including high ranking documents, some mid-level ranked uncertain documents, and all other search methods, including keyword search, similarity search, concept search and even occasional use of linear review and random searches. The various types of searches usually included in the Team's multimodal approach are shown in the search pyramid, below.</p><p>The standard eight-step workflow used by the Team in legal search projects is shown in the diagram below. A step by step descriptions of the workflow can be found in e-Discovery Team writings. <ref type="bibr" coords="7,128.70,526.19,7.08,7.76">12</ref> The application of this methodology can be seen the Team's description of their work in each of the thirty Topics that is included in the Appendix. Our usual steps One, Three and Seven had to be omitted or severely constrained to meet the TREC experiment format.</p><p>Standard steps Three and Seven of the workflow were omitted to meet the time requirements of completing every review project in 1.5 days. Skipping these steps allowed us to complete 30 review projects in 45 days in the Team's spare time, but had a detrimental impact.</p><p>Our usual first step, ESI Discovery Communications, is where our information needs are established. This had to omitted to fit the format of the Recall Track Athome experiments. The only communication under the TREC protocol was a very short, often just two-word description of relevance, plus instant feedback in the form or yes or no responses as to whether particular documents submitted were relevant. In the e-Discovery Team's typical workflow discovery communications typically involve: 1) detailed requests for information contained in court documents such a subpoenas or Request For Production; 2) input from a qualified SME, who is typically a legal expert with deep knowledge of the factual issues in the case and how the presiding judge in the legal proceeding will likely rule on borderline relevant issues; and, 3) dialogues with the client, witnesses, and with the party requesting the production of documents to clarify the search target.</p><p>The Team never receives a request for production with just two or three word descriptions as encountered in the TREC experiments. When the Team receives vague requests, which is common, the Team seeks clarification in discussions (Step One). In practice if there is disagreement as to relevance between the parties, which is also common, the presiding judge is asked to make relevance rulings. Again, none of this was possible in the TREC experiments.</p><p>All of our usual practices in Step One had to be adjusted to the submissions format of the 30 Athome Topics. The most profound impact of these adjustments was that the attorneys on the Team often lacked a clear understanding as to the intended scope of relevance and the rationale behind the automated TREC relevance rulings on particular documents. These protocol changes had the impact of minimizing the importance of the SME role on the active machine learning process. Instead, this role was often shifted almost entirely to the analytics of the EDR software. The software analytics could often see patterns, and correctly predict relevance, that the human attorney reviewers could not (often, but not always, because the human reviewers disagreed with the TREC assessors human judgment of ground truth in several topics, and otherwise could not follow or see any logic to the documents returned as relevant).</p><p>This minimization of the importance of the SME role is not common in legal search where attorney reviewers always have some sort of understanding of relevance. The role of the SME in the Team's decades of experience in legal search has always been important to help ensure high quality, trustworthy results. Contrary to the unfortunate popular belief among laypersons going back to the time of Shakespeare, 13 the vast majority of legal professionals maintain very high standards of ethics and trustworthiness. In spite of the alleged negative influences of the centuries old adversarial tradition of the common law, attorneys are dedicated to uncovering the truth, the whole truth, and nothing but the truth, regardless of the particular case impact. Any notion of inherent bias by attorneys is misplaced. It is, after all, attorneys who control the discovery process and define relevance, and attorneys, not robots or scientists, who make the production of relevant documents to the other side. <ref type="bibr" coords="9,322.18,232.67,7.08,7.76">14</ref> Scientific research is better served when driven by reason and objective measurements, not prejudices and assumptions about an entire profession and our common law system of justice, based as it is on an adversarial truth seeking process. The e-Discovery Team will continue to look for ways to improve quality control, and guard against inadvertent errors, which always exists in any human endeavor, and identify intentional errors, which rarely exist in legal search, but, we concede may sometimes take place. For that reason we will explore greater reliance on automated process in our future research and other quality control techniques. <ref type="bibr" coords="9,445.04,326.75,7.08,7.76">15</ref> We will not, however, abandon a hybrid approach where a human remains, if not in control, then at least as an active partner, out of any subjective prejudices against lawyers. We also refuse to accept the unproven assumption that our adversarial system is inherently suspect, encourages bias, and otherwise requires that humans be removed from e-discovery and replaced by robots. Conversely, we do not naively assume lawyers are automatically superior to machines. We have long advocated against the current legal standard of only using manual review of every document. The Team's hybrid approach aims for a proportional balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND DISCUSSIONS</head><p>The e-Discovery Team sought to answer the three previously listed Research Questions in its experiments at the 2015 TREC Total Recall Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">First and Primary Research Question.</head><p>What Recall, Precision and Effort levels will the e-Discovery Team attain in TREC test conditions over all 30 Topics using the Team's Predictive Coding 3.0 hybrid multimodal search methods and Kroll Ontrack 's software, eDiscovery.com Review (EDR).</p><p>We primarily measured effort by the number of documents that were actually humanreviewed and coded relevant or irrelevant. The Team human-reviewed only 32,916 documents to classify 16,576,798 documents. As an additional measure of effort, we estimated our total time spent on all Topics. The Team spent 45 days doing all of the work, with an estimated average of 8 hours per day total expended by the Team. (All Team members carried on their normal employment activities on only a somewhat reduced basis during the 45 days of the review, and TREC work was also reduced on most weekends.) The estimated total hours spent by Team members for both analysis and review is thus approximately 360 hours.</p><p>It is typical in legal search to try to measure the efficiency of a document review by the number of documents classified in an hour. For instance, a typical contract review attorney can classify an average of 50 documents per hour. Here using Predictive Coding 3.0 our Team classified 16,576,798 documents in 360 hours. That is an average speed of 46,047 files per hour.</p><p>In legal search it is also typical, indeed mandatory, to measure the costs of review and bill clients accordingly. If we here assume a high attorney hourly rate of $500 per hour, then the total cost of the review of all 30 Topics would be $180,000. That is a cost of less than $0.01 per document. In a traditional legal review, where a lawyer reviews one document at a time, the cost would be far higher. Even if you assume a low attorney rate of $50 per hour, and review speed of 50 files per hour, the total cost to review would be $16,576,798. That is a cost of $1.00 per document, which is actually low by legal search standards. <ref type="bibr" coords="10,368.29,152.27,7.08,7.76">16</ref> Analysis of project duration is also very important in legal search. Instead of the 360 hours expended by our Team using Predictive Coding 3.0, traditional linear review would have taken 331,536 hours (16,576,798/50). In other words, what we did in 45 days, taking 360 hours, would have taken a team of two lawyers using traditional methods over 45 years.</p><p>Complete details and descriptions of the ad hoc methods employed in all thirty topics are included in the Appendix. 4.2 Research Question No. 2.</p><p>How will the Team's results using its semi-automated, supervised learning method compare with other Recall Track participants using semi automated supervised learning methods.</p><p>Unfortunately no other Athome participants completed all thirty topics and only one completed all ten Bush email topics. The lack of participation by others in the Athome group makes meaningful comparisons very difficult or impossible, but we note that the e-Discovery Team's scores were consistently higher than any other Athome participants.</p><p>The Sandbox participants' work included the same three datasets as AtHome, but none of them also participated in the Athome division. This is unfortunate because it makes direct comparisons problematic, if not impossible, especially as to the software systems used. Still, with some caveats, a few limited comparisons are possible between the two divisions because the same topics and datasets were searched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Research Question No. 3.</head><p>What are the ideal ratios, if any, for relevant and irrelevant training examples to maximize effectiveness of active machine learning with EDR.</p><p>The Team experimented with various positive and negative training ratios using the predictive coding training features of their software. Most of these experiments were post hoc, but some were carried out during the initial TREC submissions. In some of the thirty topics our review work would have been concluded earlier but for these side experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Research Question No. 1.</head><p>The TREC measured results demonstrated high levels of Recall and Precision with relatively little human review efforts using the e-Discovery Team's methods and EDR. The three-man attorney Team was able to review and classify 16,576,798 documents in 45 days under difficult TREC test conditions. They attained total Recall of all relevant documents in all 30 Topics by human review of only 32,916 documents. They did so with two-man attorney teams in the 10 Bush Email Topics, and one-attorney teams in the 20 other Topics. In Topic 3484, which searched a collection of 902,434 News Articles, the Team attained both 100% Recall and 100% Precision. On many other Topics the Team attained near perfection scores. In total, very high scores were recorded in 18 of the 30 topics with good results obtained in all, especially when considering the low human efforts involved in the supervised learning. Moreover, the Team's F1 scores at the time of Reasonable Call ranged from a perfect score of 100% in Topic 3484, to 91% to 99% in eight topics, and 82%-87% in five others.</p><p>Considering the limited human effort put into the reviews, and the speed of the reviews, we consider the results in all Topics to be excellent. As shown by the comparisons with traditional review discussed above, these results are far superior to the typical linear legal document review done by law firm attorneys and contract review attorneys.</p><p>The efforts by number of documents human reviewed in all thirty topics are shown in the below chart Figure <ref type="figure" coords="11,176.21,141.84,4.14,10.82">1</ref>. As you can see, the Team reviewed 32,916 documents to attain total recall of the 70,414 documents predetermined by TREC as relevant in all 30 Topics from out of a total of 16,576,798 documents. The average number of documents reviewed to attain total Recall in each topic was 1,097. The figure ranged from a low of 19 documents reviewed in Topic 2134 (PayPal), which had 252 relevant documents, to a high of 7,203 in Topic 103 (Manatee Protection), which had 5,725 relevant documents. The Team's attainment of high levels of Recall and Precision in multiple projects confirms the hypothesis that EDR software and the Team's Predictive Coding 3.0 hybrid multimodal methods are effective in most projects at attaining high levels of Recall and Precision with minimal human efforts.</p><p>The below charts summarize for each of the three datasets the Precision results obtained in each topic at 70% or higher Recall levels. Precision is shown on the left and Recall levels attained by submissions are shown on the bottom. A different colored line shows each Topic. Although Precision was not the focus of the efforts in the Team's Recall Track participation, instead the focus was on Recall and effort, still the measurements of Precision across the Recall levels provide valuable insights into the overall work. Figure <ref type="figure" coords="12,332.16,89.16,5.48,9.74">2</ref> below shows the results of the 10 Topics in Jeb Bush Email collection of 290,099 emails. Figure <ref type="figure" coords="12,329.98,102.60,5.48,9.74">3</ref> shows the results of the 10 Topics in BlackHat World Forum collection of 465,149 posts, and Figure <ref type="figure" coords="12,368.81,116.04,5.48,9.74">4</ref> shows the results of the News Articles collection of 902,434 articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>A quick exam of the results of the Bush Email Topics shows that four of the ten Topics had significantly less Precision in attaining 80% or higher Recall than the others. They are: Topic 104 New Medical Schools, shown in purple; Topic 100 School and Preschool Funding, shown in blue; Topic 102 Capital Punishment, shown in green; and, Topic 108 Manatee County. Topic 108 was probably the most error-filled of all of the Topic standards, and this may explain part of the outlier results for that topic and others in this low performing group. Investigation of the outliers showed that the primary cause of these results was disagreement by to the Team's lead attorney for the Bush email, a Florida life-long resident who is used to serving as the SME defining ground truth, and the TREC assessors' relevance determinations. Also, these ten Bush topics were carried out at the beginning of the project before the Team adopted mitigating counter strategies of greater reliance on machine ranking to mitigate the impact of the personal judgment disagreements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2</head><p>Analysis of the results of the ten Topics in BlackHat World also indicated that the relevance disagreements accounted for most of the discrepancies.</p><p>It appears that errors and inconsistencies in the TREC standard judging explain most of the Precision differences among the Topics, especially the Topics in the BlackHat World data set. In several of these Topics the Team often had difficulty detecting any logical pattern to the relevance scope. They instead, as mentioned, had to rely almost entirely on the EDR relevance predictions. Only the Team software in some of these Topics could detect any connectivity and pattern to the TREC relevant standards.</p><p>The results on the local News dataset of 902,434 articles (Figure <ref type="figure" coords="13,414.38,472.74,5.48,10.88">4</ref> below) again shows significant divergences in Precision, although less than the differences seen in Bush Email or BlackHat World datasets. Analysis of the results of the ten News Articles Topics again shows considerable disagreement on relevance judgments in some topics. Inherent difficulty of the various issues in the Topics may also explain some of the differences. The size of the relevance pool also has a direct relationship on the Precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3</head><p>The following results are highlights of the Team's top 18 topics where at least seventy-five percent of the target documents (Recall 75%+) were found with a Precision rate of 80% or higher. The Top-18 Projects of the Team are ranked by us, somewhat arbitrarily, as follows, starting with a previously unheard of perfect score. 1. In Topic 3484 (Paul &amp; Kathy Martin), the e-Discovery Team (Jim Sullivan) attained a perfect score of 100% Precision and 100% Recall. All 23 of the target documents were found in the first 23 documents submitted. Sullivan then called Reasonable after the 23 rd relevant document was submitted and so played the perfect game. He predicted that the remaining 902,411 articles in the News collection would be irrelevant. Sullivan was right. The effort expended for perfection was his personal review of 73 news reports out of the total collection of 902,434. 100% Recall with 100% Precision in a large search project was previously thought impossible by most text retrieval experts. 2. In Topic 3431 (Kingston Mills Murders), 100% Recall was attained by the Team (Tony Reichenberger) with 82.3% Precision. He attained 97.5% Recall with a Precision of 98.9%, and 95% Recall with 99% Precision. The effort expended to reach 100% Recall was his personal review of 332 news reports out of the total collection of 902,434. 3. In Topic 106 (Terry Schaivo), which had the highest prevalence of any topic (5.9%), 98.47% Recall was attained by the Team (Ralph Losey) with 97.22% Precision. At that time, after submitting 2,025 documents, he called reasonable. The F1 measure then attained was 97.84%. The effort, or number of documents reviewed and coded by Losey to attain this result, was 2,025 Bush emails, out of the total collection of 290,099, and total relevant of 17,135. A contract review attorney, whose standard billing rate is one-tenth that of Losey's, assisted in the review effort. Losey also attained 99.7% Recall in this Topic with a Precision of 70%. 4. In Topic 2158 (Using TOR), the Team (Jim Sullivan) attained 97.5% Recall of the target while maintaining a Precision of 95%. He attained 95% Recall with a Precision of 98.4%, and 90%</p><p>Recall with 99% Precision. The effort expended to reach 97.5% Recall was his personal review of 1,332 BlackHat Forum posts, out of the total collection of 465,149. 5. Topic 103 (Manatee Protection), which had the third highest Prevalence of 1.97%, the Team (Ralph Losey) attained 97.5% Recall with a Precision of 90.6%, 95% Recall with a Precision of 98.8%, and 90% Recall with 99.3% Precision. The effort expended to reach 97.5% Recall was his personal review of 7,203 Bush emails, out of the total collection of 290,099. Again he was assisted by a contract review attorney. The high review count here is due to the fact this is one of two projects where the Predictive Coding 3.0 second step of random sampling was included. This is also the first project undertaken. 6. In Topic 109 (Scarlett Letter Law), the Team (Ralph Losey) attained 97.5 % Recall with 84.4% Precision, 95% Recall with 95.4% Precision, and 90% Recall with 96% Precision. The effort expended to reach 97.5% Recall was his personal review of 753 Bush emails, again out of the total collection of 290,099. One contract review attorney assisted. 7. In Topic 3378 (Rob McKenna), the Team (Tony Reichenberger) attained 100% Recall after the submission of only 192 documents and review of only 200 documents. This was a low prevalence Topic with only 66 relevant out of the total collection of 902,434. For these reasons the Precision was 34.31%, even though only 192 documents were submitted to attain 100% Recall.</p><p>The Team results exceeded expectations, where our Recall goal was 90%, in many additional Topics: 8. In Topic 3481 (Fracking), the Team (Jim Sullivan) attained 95% Recall with 95.2% Precision by reviewing only 367 news articles. 9. In Topic 105 (Affirmative Action), the Team (Ralph Losey) attained 90% Recall with 99.7% Precision by reviewing only 582 mails (one contract review attorney assisted). 10. In Topic 3089 (Pickton Murders), the Team (Joe White) attained 90% Recall with 97.9% Precision by reviewing only 779 articles. A 99.61% Recall level was attained with 54.98% Precision, again with review of only 799 articles. 11. In Topic 3226 (Traffic Cameras), the Team (Jim Sullivan) attained 90% Recall with 95.9% Precision by his personal review only 18 forum posts. 12. In Topic 101 (Judicial Selection), which had the second highest Prevalence rate of 2%, the Team (Ralph Losey) attained 90% Recall with 87.8% Precision by reviewing 6,895 emails (one contract review attorney assisted). 13. In Topic 3357 (Occupy Vancouver), the Team (Tony Reichenberger) attained 90% Recall with 82.4% Precision by reviewing only 920 news articles. 14. In Topic 107 (Tort Reform), the Team (Ralph Losey) attained 90% Recall with 80.9% Precision by reviewing only 1,164 emails (one contract review attorney assisted).</p><p>Four additional Topics also did quite well, and attained Recall levels over 75% with high Precision rates: 15. In Topic 2225 (Rootkits) the Team (Ralph Losey) attained 80% Recall with 88% Precision by reviewing only 186 forum posts. 16. In Topic 2333 (Article Spinner) the Team (Ralph Losey) attained 80% Recall with 79% Precision by reviewing only 228 forum posts. 17. In Topic 2052 (Paying for Book Reviews) the Team (Jim Sullivan) attained 80% Recall with 73.4% Precision) by reviewing 1,960 forum posts. 18. In Topic 3133 (Pacific Gateway) the Team (Ralph Losey) attained 76.99% Recall with 89.69% Precision by reviewing only 49 News Articles.</p><p>Figure <ref type="figure" coords="15,120.56,705.65,5.48,11.01">5</ref> below shows the recall and precision of these top 18 projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5</head><p>The Team's lower performance in the other 12 projects was, according to our analysis, primarily caused by the fact that the attorney Team members are accustomed to self-defining the ground truth, and their opinions on relevance differed significantly from the TREC assessors. In later topics the attorney Team learned to turn off their own judgments and rely primarily on their software's automated processes, at which point their scores improved. In all topics the machine learning of the Team's EDR software was able to find documents that TREC would consider relevant, even where the human team members could see no connection. But in some topics the human searchers would be completely bewildered by the zig zag relevance scope shown by TREC's response to submissions. The attorneys would not see any kind of logical connecting pattern to some of the documents that TREC determined to be relevant. Sometimes the attorneys only saw wrong answers and inconsistencies. Even though the attorneys could not see any pattern, they learned that their EDR software could often still find the patterns and correctly predict which documents TREC would label relevant. When this happened they would in effect turn all submission decisions over to EDR and only submit the highest-ranking documents. The cut-off point of ranking for submissions, be it top 5% or top 100 documents, or some other scheme, was still determined by the human in charge. That is part of the Team's hybrid design.</p><p>There are probably other explanations for the bottom twelve scoring topics aside from questionable TREC assessor adjudications, including: the data itself; the difficulty of the issues addressed in the Topic; relative performance of human reviewers; and, the impact of the omission of Steps Three and Seven from the Team's standard workflow to meet the 45 day time limitation, and the radical change to Step One. See: Concept Drift and Consistency: Two Keys to Document Review Quality, e-Discovery Team (Jan. 20, 2016). All of the Team's inconsistencies were not caused by differences of opinion on TREC relevance adjudications, only some. We appreciate the difficulty of creating interesting topics for such a diverse group of participants, most of whom used fully automated CAL approaches. We understand the inherent difficulties in setting a ground truth for prejudged relevance where the traditional TREC pooling methods could not be used. 17 In spite of our criticisms here, we overall have high praise and thanks for the TREC administrators' tireless efforts and agree with the majority of the assessments they made under difficult, time constrained conditions.</p><p>Regardless of these issues and metric inconsistencies, the Team's manual efforts, as measured by time expended and number of documents manually reviewed were consistently very low in all topics. More than half of the relevant documents found were not manually reviewed. Instead, the Team was routinely able to delegate relevance coding to the EDR software, either by choice and convenience, or sometimes, as discussed, by necessity in the topics where the ground truth of relevance was unknown and incomprehensible to the attorneys. This result should shatter once and for all the already weakened legal search myth that all documents must be manually reviewed for relevance.</p><p>Although not directly comparable due to different test conditions, different searches, etc., the e-Discovery Team's scores were far higher than any previously recorded in the six years of <ref type="bibr" coords="17,90.23,212.91,102.03,6.71">TREC Legal Track (2006</ref><ref type="bibr" coords="17,192.26,212.91,29.77,6.71;17,222.09,205.79,7.08,7.76">-2011) 18</ref> or any other study of legal search. <ref type="bibr" coords="17,383.57,205.79,7.08,7.76">19</ref> The results of Blair and Maron and TREC from 2007 to 2011 are summarized below in Figure <ref type="figure" coords="17,398.47,226.35,5.48,6.71">6</ref> with F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6</head><p>This is not a listing of the average score per year, such scores would be far, far lower. Rather this shows the very best effort attained by any participant in that year in any topic. These are the highest scores from each TREC year. Note how they compare with the Team's high scores in 2015, Figure <ref type="figure" coords="17,148.10,491.07,4.15,6.71">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7</head><p>One reason for this significant jump in high scores may be that many of the thirty topics in the 2015 Total Recall Track presented relatively simple information needs by legal search standards, with one major exception, Topic 109 -Scarlet Letter Law. It required some legal knowledge and analysis. There were also four other minor exceptions -Topics 101, 105, 106, 107 -that required some measure of legal analysis. Another explanation may be improved software and the Team's hybrid multimodal method that includes continuous active learning. The later is strongly suggested because the results in Topic 109, as well as Topics 101, 105, 106 and 107, are close to typical legal search type projects and the Team's results in these topics were all consistently high: Topic 109 (Scarlett Letter Law) -95% F1 at Reasonable Call; Topic 101 (Judicial Selection) -87% F1 at Reasonable Call; Topic 105 (Affirmative Action) -95% F1 at Reasonable Call; Topic 106 (Terri Schiavo)-98% F1 at Reasonable Call; Topic 107 (Tort Reform) -84% F1 at Reasonable Call. This is shown in Figure <ref type="figure" coords="18,268.70,221.99,5.48,11.07">8</ref> below. The Team attained very high recall and precision rates in most, but not all, of the thirty Total Recall topics. The Team's F1 scores at the time of Reasonable Call ranged from a perfect score of 100% in one topic (3484), to 91% to 99% in eight topics, and 82%-87% in five others.</p><p>Although, of course, not directly comparable, these scores are far higher than any previously recorded in the six years of TREC Legal Track (2006-2011) or any other study of legal search. One reason for this may be that the thirty topics in the 2015 Total Recall track presented relatively simple information needs by legal search standards, with one exception (Topic 109 -Scarlet Letter Law). Another may be improved software and the Team's hybrid multimodal method that includes continuous active learning.</p><p>Since most of the thirty topics presented only simple, single-issue information needs suitable for single-facet classification, they had somewhat limited value for purposes of legal search experimentation. Further, only a few of the topics required any legal analysis for relevance identification. This again limited the use of these experiments for purposes of legal search research. These two factors, plus the omission of metadata, was a disadvantage to the e-Discovery Team of lawyers who are practiced in more complex information needs requiring extensive legal analysis and SME defined ground truths. Further, their methods and EDR software are designed to utilize full metadata derived from native files. Conversely, it appears that these same factors made it simpler for the Sandbox participants to perform well in most topics.</p><p>The one exception was Topic 109, Scarlett Letter Law, which, as mentioned, was the only topic requiring legal analysis and some very rudimentary knowledge to begin locating relevant documents. The keywords alone -"Scarlett Letter Law" -would only find relevant documents with this word combination and similar text patterns. These words were just the nickname of the proposed and eventually enacted Florida Statute. Any attorney would know that to find relevant information they would not only have to search the name, they would have to search the various house and senate bill numbers for this law. These numbers would not often appear in the same document as the nickname, and since the machine did not know to search for these numbers, it did not realize the significance. Eventually the automated machine learning saw the connection, after many relevance feedback submissions. These submissions would, of course, not happen in real legal search, and even if they did, this imprecision would equate to substantial additional human reviews and thus expense.</p><p>Somewhat surprisingly to us, the fully automatic methods employed by the Sandbox participants attained recall and precision scores comparable to that of the e-Discovery Team in most of the topics. Moreover, there were few differences between the various fully automated approaches. Still, the highest F1 values at the time of Reasonable Call were attained by the e-Discovery Team in twenty of the thirty topics, and the second or third best F1 scores in four others. This is shown in Figure <ref type="figure" coords="19,227.27,343.05,5.48,10.97" target="#fig_2">9</ref> below. The Team F1 rankings for each topic are shown in the third column. In Topic 109, Scarlet Letter Law, where some legal knowledge and analysis was required to understand relevance, the Team attained significantly better results -96% F1 -at the time of Reasonable Call than did the automatic runs. In the Sandbox automatic runs the F1 values at the time of Reasonable Call ranged from 0% to 29%. Moreover, at the 1R point in Topic 109, the e- Discovery Team had attained over 95% recall, whereas all of the automated methods were still less than 1% recall. This is shown in the chart below, Figure <ref type="figure" coords="20,356.49,86.86,9.25,12.04">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 10</head><p>The Team's multimodal human machine approach also consistently found more relevant documents at the start of a search, and did so with greater precision than the fully automated approaches. Further, the hybrid man-machine approach was consistently more effective at determining a stop point, referred to by the Recall Track as a "Reasonable Call." An example of this is shown in the Figure <ref type="figure" coords="20,208.88,427.90,11.05,12.04">11</ref> for Topic 109. The dark green line represents the Reasonable Call point, recall is shown in the vertical, and horizontal is the number of documents submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 11</head><p>Another way to evaluate the performance of the multi-modal approach is to consider how precise the coding suggestions were during the course of review. This would indicate an efficient review, which is critical in legal search to cost savings. As to the Athome109 topic, the below Figure <ref type="figure" coords="21,120.56,114.90,11.05,10.88">12</ref> contrasts precision percentage on the Y-axis, with recall percentage on the X-axis. Precision does not begin to drop until approximately 95% Recall. Note that the green line representing percent of the database submitted barely moves off the baseline. Figure <ref type="figure" coords="21,474.97,141.78,11.05,10.88">13</ref> shows the actual document counts reviewed and submitted in order to obtain the various precision thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 12</head><p>Figure <ref type="figure" coords="21,168.82,702.33,11.75,10.49">13</ref> For further comparison Figure <ref type="figure" coords="22,226.98,73.48,11.06,11.98">14</ref> below (prepared by the Total Recall administrators) plots the average Athome3 precision by recall results. The e-Discovery Team results (barely visible on top) follow a curve very similar to the Athome109 topic. The Team's results outperformed the automated runs for most of the duration of the process, demonstrating a consistent efficiency in results. While various automated runs experienced comparable results in the Athome1 and Athome2 sets, the consistently high level of the multimodal approach corroborates a consistent efficient process across all data sets.</p><note type="other">Figure 14</note><p>5.3 Research Question No. 3. The Team's experiments with different positive negative training ratios showed that training using a 50/50 ratio of relevant to irrelevant documents performed consistently better than any other ratios. This result is believed to be specific to the proprietary type of logistic regression algorithm used in Kroll Ontrack's EDR. It may not have applications beyond this software, or even other more complex projects. Our work on this question continues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>The results in Topic 109 and other topics indicate that hybrid man-machine learning by skilled attorneys is, at the current time, significantly more effective at meeting complex legal search needs than fully automated approaches. This seems obvious, but more experiments on this issue are needed before this can be accurately quantified. The surprising success of the Sandbox participants using fully automated search, even though limited to non-legal topics and situations with only simple information needs, suggests that greater reliance on automated methods could be placed in legal search where the cases and needs are simple. The relatively low effort involved in automated learning, and thus low expense, is compelling, especially in view of the proportionality analysis required by law under the December 2015 Amendments to the Federal Rules of Civil Procedure. The Team has begun and will continue post hoc analysis and experiments using various hybrid methods that adjust the balance between man and machine.</p><p>We are experimenting with methods that place greater reliance on machine learning in all topics, including, but not limited to, topics with lesser complexity and information needs. We will also further investigate the use of both fully automated methods, and hybrid methods, in legal search quality control, fraud detection, and in the prediction of future wrongful conduct. <ref type="bibr" coords="23,486.35,111.95,7.08,7.76">20</ref> The 2015 TREC Total Recall Track results also suggest that even when information needs are simple and require no complex analysis or background knowledge, as was true of most of the topics, that a hybrid method outperforms fully automated methods in two ways: one, at finding relevant documents quickly and with high precision; and two, at making better stop decisions. These two considerations are very important in legal search where attorneys must find a proportional balance between recall and effort/expense. The results in all topics, even the simple ones, thus caution against over-reliance at this time on machine learning alone without proper expert supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>The e-Discovery Team would like to thank Kroll Ontrack, Inc. and Jackson Lewis P.C. for their generous support of this project. We would also like to thank the many employees at Kroll Ontrack who pitched in behind the scenes, often late at night and on weekends, to help make this happen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">REFERENCES (Endnotes)</head><p>[1] Losey, R., Predictive Coding 3.0, part two (e-Discovery Team, 10/18/15); also see Predictive Coding Articles by Ralph Losey, (collection of over 50 articles by Ralph Losey further describing the hybrid multimodal approach).</p><p>[2] The e-Discovery Team's hybrid multimodal approach is similar to the method promoted by the Total Recall Track administrators, Maura Grossman and Gordon Cormack, in that they both use continuous active learning (CAL) in legal search as part of a technologyassisted review (TAR). It is, however, fundamentally different from Grossman and Cormack's current methods in two ways. First, our approach relies upon and encourages participation of skilled reviewers in the search process, the hybrid approach, whereas the Grossman and Cormack approach seeks to eliminate the role of the skilled user, namely trained attorneys. The rationale for their automation goal is the unsubstantiated claim that the adversarial context of legal search makes attorneys untrustworthy. They claim that inherent user bias means fully automated approaches are the only reliable methods of legal search. Grossman &amp; Cormack, Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review, CoRR abs/1504.06868 at pg. 1 (2015) ("In eDiscovery, the review is typically conducted in an adversarial context, which may offer the reviewer limited incentive to conduct the best possible search.") Obviously the Team disputes this assumption and conclusion. We do not endorse the view of the inherent bias and untrustworthiness of attorneys. In Ralph Losey's experience as a practicing attorney since 1980 such bias is the rare exception, not the norm, and should not be the basis of a legal search strategy. The better solution to this minor issue of trustworthiness is educational, to train more attorneys in search and in professional ethics. Since our core assumptions on process and attorney honesty are fundamentally different, so too are our methods and goal. Our aim is augmentation of skilled attorneys to perform legal search, not automation, not replacement.</p><p>Second, our Team uses a variety of search methods, a multimodal approach, whereas the Grossman and Cormack approach relies solely upon the use of high-ranking documents to train a classifier. This is consistent with their aim to fully automate and eliminate attorneys from the legal search process, again based on the premise we dispute of attorney bias. In their words: "For the reasons stated above, it may be desirable to limit discretionary choices in the selection of search tools, tuning parameters, and search strategy." Id. We disagree and seek to empower attorneys with a variety of search tools, including the one search method that they endorse of reliance on highranking documents. Also see and the discussion and citations in Endnote 19.</p><p>[3] In these respects the e-Discovery Team follows the teachings of Gary Marchionini, Dean of the School of Information and Library Sciences of U.  <ref type="table" coords=""></ref>and<ref type="table" coords="24,480.50,334.63,4.42,5.95">3</ref>) changing your search goal as you go along and learn things along the way. This may seem fairly obvious when stated this way, but, in fact, many searchers erroneously think they will find everything they want in just one place, and second, many information systems have been designed to permit only one kind of searching, and inhibit the searcher from using the more effective berrypicking technique." Also see: White &amp; Roth, Exploratory Search: Beyond the Query-Response Paradigm (Morgan &amp; Claypool, 2009).</p><p>[4]</p><p>The Total Recall Track fully automated method follows the Track Administrator's preferred methodology of fully automated monomodal search (high ranking only) and their recently announced goal to eliminate attorney review in favor of full automation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grossman &amp; Cormack, Autonomy and Reliability of Continuous Active Learning for</head><p>Technology-Assisted Review, supra at pg. 1 (2015): "Our goal is to fully automate these choices, so that the only input required from the reviewer is, at the outset, a short query, topic description, or single relevant document, followed by an assessment of relevance for each document, as it is retrieved." They call the method "Autonomous TAR." Id. at pg. 6. The protocols of the fully automated division of the Total Recall Track were apparently designed in part by Cormack and Grossman to test this premise, and the results they attained as participants in this division, along with all of the other fully automated participants from Universities around the world, are very impressive. Still, the e-Discovery Team, who did not participate in the 2015 automated division, notes that many of the protocols in this experiment are based on fictions and conditions not found in the real world of legal search, where the Team's methods were developed. The differences include, but are not limited to: the existence of an omnipotent SME that instantly provides perfectly correct judgmental feedback as to relevance of all documents selected by the automated processes as probable relevant; simple, single-facet issues; relatively simple datasets stripped of most native metadata; and, perhaps most importantly, issues requiring little or no legal analysis or background legal knowledge. Note, in post hoc runs the e-Discovery Team ran a few fully automated runs on Kroll Ontrack systems and EDR. We used the same high ranking only Autonomous TAR training method and obtained the same results as all of the other fully automated division participants.</p><p>[5] "Contract review attorney," or simply "contract attorney," is a term now in common parlance in the legal profession to refer to licensed attorneys who do document review on a project-by-project basis. Their pay under a project contract is usually by the hour and is at a far lower rate than attorneys in a law firm, typically only $50 to $75 per hour. Their only responsibility is to review documents under the direct supervision of law firm attorneys who have much higher billing rates. See PreSuit.com where the e-Discovery Team's proposal is outlined to monitor the IT systems of large organizations with advanced analytics and other search methods to predict and avoid future illegal conduct. This man-machine hybrid type of early warning system includes safeguards to protect both individual privacy rights and confidential corporate information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E-Discovery Team 89-Page Narrative Report of all 30 Topics</head><p>This Appendix Narrative Report describes the search of all thirty Total Recall topics in TREC 2015 using the e-Discovery Team's Hybrid Multimodal method. The report follows the chronological order in which the searches were conducted. The first project started on July 14, 2015. It was Topic 103 Manatee Protection. The last Topic 3089 Pickton Murders concluded on August 28, 2015. At the beginning of each Topic the results are reported for that Topic. Each has the same form and discloses metrics at the times when: (1) the Reasonable call was made; and, (2) the point where 97.5% Recall was attained. They are summarized along with a variation of a standard Confusion Matrix, a/k/a Contingency Table <ref type="table" coords="28,324.23,231.82,3.53,5.01">1</ref> The Confusion Matrix itself is highlighted in blue. It is followed by a list of the key the values attained: Recall, Precision, F1 Measure, Accuracy, Error, Elusion and Fallout.</p><p>Work on multiple topics was conducted at the same time. Sullivan, who worked on eight topics, Reichenberger, who worked on four, and White, who did one, each worked on a single topic at a time. They did, however, work concurrently with Losey and each other. Losey, who worked on seventeen topics, and had the assistance of a contract review attorney on the ten Bush Email Topics, typically worked concurrently on multiple topics at the same time. All Topics were a Team effort, but the attorneys identified as running each Topic controlled the review work for that Topic. Consultation was common, especially at first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic 103 Manatee Protection Confusion Matrix -Topic 103</head><p>Total Documents: 290,099 Total Relevant: 5,725 Total Prevalence: 1.97% The Team found this Topic challenging for a variety of reasons, including the fact that the Bush collection of 290,099 emails had been stripped of its original metadata, images, and attachments. Further, we found some inconsistencies in judging this topic, although not many.</p><p>Overall we found Topic 103 had one of the best gold-standards of the ten Bush Email Topics.</p><p>Ralph Losey is a native Floridian and Florida attorney for 35 years. He was somewhat knowledgeable about all of the Bush Email issues, certainly far more so than the average person, but he did not consider himself a bona fide subject matter expert (SME) on any of them. Losey's knowledge and interest on Manatee Protection issues was, however, higher than the other Bush Topics. For that reason it was chosen as the first topic. Losey's assistant, Bottolene, had lived in Florida for several years and also had some background with the Manatee Protection issue. They generally considered their familiarity with the issue to be an asset in the search of Topic 103. The same cannot be said of other Bush Email Topics. Based on the sample prevalence we predicted a spot projection for prevalence in Topic 103 of 5,175 documents (95% +/-2.5% confidence levels). In fact, the total relevant documents in Topic 103 proved to be 5,725, well within the 2.5% margin of error. Based on the length of time needed for random sample review, and our desire to complete all thirty topics in 45 days, we decided to skip this step for ensuing reviews. (Topic 101 Judicial Selection was started shortly after Topic 103, and also included Step Three Random Baseline.) As mentioned, we also skipped most of the procedures in Step 7-" Zero Error Numerics" concerning quality control in this and all 30 Topics.</p><p>After Bottolene completed the random sample review on July 20th she assisted Losey on July 21st and 22nd in his work on Step Five Multimodal Search Review. At that time submission to TREC had already begun and the Team was evaluating the confirmed relevant and irrelevant documents from TREC.</p><p>A total of 24 document submissions were made to TREC in this Topic: four document submissions on July 20 th , one of July 21 st , and the remaining nineteen submissions were made on July 22, 2015. In between most of these submissions the Team conducted Steps Four, Five and Six of its standard workflow. These are the predictive coding steps that iterate. In Topic 103 the use of predictive coding ranked based searches was severely constrained. This was due to initial configuration setup errors, where input parameters for the learning engine were set incorrectly. These setup errors were detected and corrected by July 22, 2015, and thereafter Mr. EDR was of great assistance. Still, as a result of the delays and early errors, this Topic relied much more heavily than any other on keyword searches and human linear reviews. Similarity searches were also used extensively. Basically the predictive coding assistance in this Topic did not begin until the 14 th submission. Losey called Reasonable after the 15 th submission.</p><p>In the TREC experiments most, but not all, of the documents returned as relevant or irrelevant by TREC were included in training (Step Six). In that way their ranking impact was evaluated (Step Four) before the next submission. Training also included various irrelevant documents that were not TREC adjudicated, but were thought to be obviously irrelevant. Experiments were made as to the impact of varying the number of irrelevant documents in the hope that some ideal range or ratio could be determined to maximize Mr. EDR efficiency. These experiments are still underway. Our conclusions as of late December 2015 are stated in the body of this report.</p><p>After a total of 15 submissions that presented 4,806 documents to TREC for adjudication, Losey called Reasonable and stopped work on July 22, 2015, a week after the Topic started. Thereafter an additional 9 submissions were made to TREC to submit the remaining 285,293 emails (98.34% of the 290,099 total). There was Training in between most of the remaining seven submissions based on the TREC adjudications, but no further human input. The first two postcall submissions were critical to the Team's excellent performance on this Topic.</p><p>Losey called Reasonable at the point he thought that a reasonable human effort had been made to find relevant documents. Losey and his assistant Bottolene had personally reviewed and coded as relevant or irrelevant 7,203 documents. (Additional documents had been coded without review.) In fact, by the time Losey had submitted 2,309 documents to TREC for adjudication (the 14 th submission) he had completed all individual document review (7,203 documents), and had completed all searches other than predictive coding ranking searches where document content is not reviewed. At that time (after the 14 th submission) he essentially turned the process over to Mr. EDR, who had by then just recovered from an earlier technical illness and had not been functional before.</p><p>At the time Losey called Reasonable he had submitted a total of 4,806 documents. Of those, 4,780 had been adjudicated as relevant. This was an incredible Precision rate of 99.46%. This was the most Precise production that Losey thinks he has ever made. He also thought that he may have attained as high as a 90% Recall, but, in fact the later submissions showed that at the time Reasonable was called he had attained a Recall of 83.5%. This is still considered a high Recall level in legal search, and the combined F1 measure of 90.8% is, in legal search, like any other, a very outstanding effort.</p><p>The next submissions after Reasonable was called were always the documents that were highest ranked by Mr. EDR, which is why we call this an automated function. As we understand the game set up by TREC for the Recall Track, the actual scoring is not impacted by the Reasonable call. The scoring continues for all submissions until all documents have been returned. The Reasonable call is merely an indication of efforts. The same goes for the 70%, 80% recall calls, when and if they are made before the Reasonable effort call, except they are of even less interest. These calls were not supposed to have an impact on scoring.</p><p>In the first two submissions after the call in Topic 103, the 16 th and 17 th submissions, Mr. EDR identified and highly ranked 661 additional relevant documents, bringing the total relevant found to 5,467 out of the total 5,725. We were thereby able to attain in that submission a Recall of 90% with Precision of 99.33%, a Recall of 95% with Precision of 98.8%, and 97.5% Recall with a Precision of 90.57%! As far as Losey knows, these statistics represent his personal best efforts, especially considering that he did so with very little reliance on predictive ranking.</p><p>What makes this 97.5% Recall, 90.6% Precision all the more remarkable for legal search is that it was accomplished by only one expert attorney assisted by one contract review attorney. The measured effort to attain these high levels was remarkably low, especially considering that a significant amount of time in Topic 103 was spent reviewing the base line sample (Step Three).</p><p>Together the two attorneys only reviewed 7,203 documents out of the total corpus of 290,099 emails (2.5%). In legal search it is common for attorney review teams to consist of dozens or even hundreds of attorneys. Moreover, even when predictive coding is used, a far higher percent of the corpus is typically reviewed than 2.5%, and Recall levels of 97.5% are unheard of, much less precision in excess of 90%.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>(Please note, that the graph is not to scale as the graph is based on individual submissions. We thought this a better depiction than by proportionally showing progress because in most cases a proportional graph would be a line virtually straight up from the start and flat going over).</p><p>The next chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Manatee Protection topic, by the time 97.5% Recall had been attained only 2.12% of the corpus, 6,163 documents, had been submitted for adjudication. This is a triumph for the search pyramid foundation, especially keyword search, that supports AI training. The last portion of the graph thus represents the submission of the remaining 97.88% or 283,936 documents.</p><p>The chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. The reason for the delay in completion is that the Team encountered difficulties in understanding the initial TREC adjudications to their first submissions. Neither Losey, nor the other attorney Team members consulted, could understand the relevance pattern behind TREC's initial submission responses. Due to the initial EDR configuration error, predictive coding was not available to assist at first in ascertaining the relevance scope. After several days of struggling with this project, Losey put this Topic on hold until July 29 th at which time Losey returned to the Topic to finish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>______________________________________</head><p>As a general comment the Team found all of the BlackHat World Forum posts challenging to search, more difficult than a typical search of corporate ESI. That is in part because almost all metadata of these posts, and all associated imagery, had been stripped by TREC and the ESI converted to text files. Also the language and issues (all non-legal) in the Black Hat World Forums were obscure. Even though our attorney searchers were all familiar with forums and had knowledge of most of the technologies and sometimes illegal, nearly always unethical, marketing practices discussed in Black Hat World, they still found the slang-filled posts difficult to review and analyze. The challenges were compounded by significant inconsistencies, and apparent illogic of the TREC judging in many of these topics. Still, the Team was able to overcome these challenges and, after we learned not to try to understand any relevance rules, we overall did quite well in review of the ten BlackHat World Forum Topics. Based on the elusive (to humans) relevance standard, we found that these topics required greater reliance on Mr. EDR than the Bush Emails and News Articles. Even though we continued to use a multimodal approach in Forum topics, our emphasis was on the AI features of ranking and probability. The Team readily admits that its own human intelligence, without the considerable AI enhancements of Mr. EDR, was not up to the task of matching TREC relevance calls for the Forum Topics. But with the help of predictive coding (Me. EDR) we overcame the difficulties and attained relatively high recall levels.</p><p>On July 31, 2015, after making 22 document submissions to TREC providing a total 1,505 documents, Losey had found a total of 580 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 2,101 documents. In fact, Losey had stopped document review after the 21 st submission. His 22 nd submission was entirely based on document rankings without review. After the 22 nd TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 88.41% had been attained. There were seven additional submissions to TREC after the Reasonable call point. In the next, 23 rd submission, 95% Recall was attained after submitting only 2,130 additional documents.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>(Please note, that this graph, and all others like it, are not to scale as the graphs are based on individual submissions. We thought this a better depiction than by proportionally showing progress because in most cases a proportional graph would be a line virtually straight up from the start and flat going over).</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the CAPTCHA Services topic, by the time 97.5% Recall had been attained only 1.34% of the corpus, 6,225 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 98.66% or 458,922 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. Due to the same mentioned initial configuration setup errors the AI features did not work until near the end of this Topic. Losey instead relied heavily on Keyword, linear, and a new type of Similarity search the Team invented out of necessity during TREC events. It is anticipated that the new similarity search feature will be included in future Mr. EDR releases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>______________________________________</head><p>Review of the random sample of 1,534 Bush emails found 30 that were relevant. That suggested a prevalence of 1.96% and a spot projection of 5,673 documents. The actual relevant count of 5,834 and prevalence of 2.01% was very close to the projection. Note this is the second and last Topic in which a full Step Three random sample was implemented.</p><p>On July 25, 2015, after making 15 document submissions to TREC providing a total 5,683 documents, Losey had found a total of 5,026 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 6,895 documents. In fact, Losey had stopped document review after the 14 th submission, as his 15 th submission was entirely based on document rankings without review. After the 15 th TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 86.15% had been attained with a Precision of 88.44%. There were an additional 8 submissions to TREC after the Reasonable call point. In the next, the 16 th there was a submission of 652 documents, 345 of which were relevant. 95% Recall with 82.7% Precision was attained after submitting only 6,705 documents (1,022 after Reasonable call). 97.5% Recall with 70.6% Precision was attained after submitting only 8,052 documents (2,369 after Reasonable call).</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Judicial Selection topic, by the time 97.5% Recall had been attained only 2.78% of the corpus, 8,052 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 97.22% or 282,047 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. Reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>______________________________________</head><p>Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal, was done by Losey with assistance at first of Bottolene. Losey handled all of the AI related searches in Step Five, including the probability and ranking related searches. His assistant focused on keyword searches and also made suggestions of documents to submit. All final submittal decisions were made by Losey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observations on the Errors of Relevance Judgments in This and Other Topics</head><p>This was the most frustrating of all of the TREC Recall Topics for the Team to work on because the judgments on relevance contained more obvious errors and inconsistencies than any other. This Topic was Manatee County, as opposed to Topic 103, which was Manatee Protection, which of course referred to the endangered mammal. Unfortunately, as a life long Florida attorney, Losey has substantial independent knowledge of Manatee County and manatees. Bottolene had also been a Florida resident for several years and an attorney. Their direct personal knowledge of Florida proved to be a significant disadvantage in this Track (and, to a lesser extent, in other Tracks, especially ones that contained obvious errors in relevance) because TREC adjudications were not tied to actual facts and reality (obviously no one at TREC was a Florida SME) and were otherwise surprising.</p><p>For instance in Topic 108, even though the subject was the County of Manatee, a political entity, sometimes, but not always, an email with mere mention of the mammal manatee would be considered relevant, even though there was no mention of location or the county. Also, many references to Manatee Park were considered relevant to TREC, even though that park is, as any Floridian would know, especially Losey who lives in Central Florida, not located in Manatee County and otherwise has no connection to the county. Also, almost all email addresses that had manatee in the name were called relevant by TREC, even if the email had nothing to do with the County of Manatee.</p><p>There may well be some pattern to the so-called gold standard used in this Topic, but if so, it was not logical and not known to Bottolene or Losey. It appeared to these Floridians, after the fact, to be lack of expertise on the part of TREC. Other team members reviewed these adjudications later agreed. One example we were later able to figure out: a well-known Florida law firm (Holland &amp; Knight) has a home office in Bradenton, Florida, and the attorneys there would often write to the governor. As part of post-hoc analysis we saw that almost all of these emails were considered relevant by TREC assessors to this topic simply because the office city was in their standard signature line address, even though the content of the emails has nothing to do with Manatee County.</p><p>Since Losey is used to directing legal search as an SME, or direct SME surrogate, his usual approach to legal search involves using his knowledge and understanding to differentiate relevant from irrelevant. As mentioned, in legal search understanding of relevance is critical, in fact, it is a legal duty and responsibility of the attorney searchers. Thus his position as an actual Florida SME served as a disadvantage in many of the Bush email Topics, including this one.</p><p>The Team later encountered other Topics with inconsistencies and mistakes like Topic 108. In such cases we eventually learned to step out of the process and stop trying to understand or look for a rational basis for the TREC relevance calls. We would put aside our traditional SME role, which is otherwise the firmly established norm in legal search. Instead, when we found ourselves in this situation (and this happened in a little less than half of the Topics), we would basically turn the search and submission decisions over to Mr. EDR. In those situations we did not even try to see any pattern or consistency to the adjudications. When we adopted this approach in later topics we did quite well, in spite of defects we saw in the TREC gold standards. This suggests that TREC's selection of relevant documents in some of the Topics suffered from over-delegation to computer selection without adequate SME based quality controls. It is unknown what software was used by TREC to create the relevant gold standard document set, but like any predictive coding software today, it obviously can be led astray without adequate human supervision and quality control safeguards. This is why the e-Discovery Team adopts a hybrid approach, computer and human, including SMEs, and why in normal circumstances</p><p>Step Seven for quality control is so important under their Predictive Coding 3.0 method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic 108 Description</head><p>On July 23, 2015, after making 10 document submissions to TREC providing a total 746 documents, Losey had found a total of 734 relevant documents (Precision of 98.4%). The effort, or number of documents reviewed and coded by Losey to attain this result, was 696 documents. After the 10 th TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 31% had been attained. The decision to call Reasonable proved to be a big mistake because the TREC adjudications were not limited to Manatee County relevance as the Team had assumed. As mentioned, the error was based upon the Team's construction of relevance in a much narrower manner than TREC. The divergence was not known because the Team did not do enough exploration of irrational constructions and so did not detect the, to our mind, outlier nature of TREC's approach to this Topic.</p><p>The Team should have been less precise (its submissions had a Precision of 98.4%), and should have presented more documents for submission, even though the Team did not personally consider them to be relevant. It should have better tested its relevance concept. But as mentioned, as an SME Losey was used to setting the scope of relevance, and as lawyers, the entire Team was used to rational adjudications of relevance along lines that make sense to them.</p><p>This was an early topic for us in the process and we had not yet learned to mistrust our own assessments.</p><p>There were 6 additional submissions to TREC after the Reasonable call point. In retrospect, this was also an error. The Team should have submitted multiple smaller submissions after they started to discover the outlier nature of the TREC adjudications, with training between each submission where Mr. EDR could take over in an automated fashion. This was another gametype lesson learned the hard way by this Topic, which proved to be the Team's worst performance. Even in the worst case with multiple mistakes the Team still managed to attain 78% Recall with review of only 696 documents, and submission of only 60,817 of the total 290,099 documents.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the Reasonable recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Manatee County topic, by the time 97.5% Recall had been attained 90.95% of the corpus, 263,843 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 9.05% or 26,256 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correction of the Gold Standard Relevance Set in Topic 108</head><p>Since the Team is considering use of the Bush email set in further testing, training and research, they wanted to try to correct the many deficiencies they saw in TREC's determination of the gold standard for this Topic. They also wanted to better understand why the score on this Topic was so out of range from their other scores. With this in mind they re-reviewed the TREC adjudications and set up a three-attorney peer review of all errors spotted in the relevancy determinations. A conservative approach was taken and deference was given to the TREC adjudications where a rational, consistent basis could be found. Losey's personal, narrow view of what should be relevant was not followed, if there was a reason seen to follow TREC's adjudications. (Note, the Team and others in the filed of Legal Search, have observed over many projects that SMEs typically take a more narrow view of relevance than non-SMEs who, by definition, do not understand the subject as well.) Losey accepted all adverse rulings against his own positions as part of this process. Also note that suggestions to revise TREC adjudications came from all three Team members, not just Losey, and were all subject to multiple reviews and objections.</p><p>After the re-review and re-adjudication process was completed, 1,264 documents adjudicated as relevant by TREC were changed to Irrelevant. Further, 3 documents adjudicated as irrelevant by TREC were changed to relevant. Below are the corrected metrics of the Team's review under the improved adjudications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confusion Matrix (Adjusted) -Topic 108</head><p>Total Documents: 290,099 Total Adjusted Relevant: 1,114 (was 2,375) (1,264 changed to Irrelevant, 3 Changed to Relevant) Total Adjusted Prevalence: 0.38% (was 0.82%) After the 10 th TREC submission, when Losey decided to call Reasonable, Losey had found a total of 736 relevant documents (an increase of 2 documents) under the adjusted gold standard. This was a Recall of 66.07% and Precision of 98.66% under the adjusted standard. The F1 measure was 79.14%. Note that these metrics are much more inline with the other 29 projects, although the adjusted 66% Recall is still the Team's second to lowest Recall score at the Reasonable call point. Under the corrected standard the Team attained 94.43% Recall with review of only 696 documents, and submission of only 60,817 of the total 290,099 documents.</p><p>A graph mapping how the review by Recall attained after number of documents submitted is shown below with both the original TREC standard (blue) and the Team adjusted standard (red). Sullivan has a background in computers and programming. He has substantial experience in forums to understand the unique characteristics present in forum communications. While he considers himself far more knowledgeable than the average person, he has no experience with the unethical world of Blackhat Forums and does not consider himself to be a bona fide subject matter expert (SME) on any of them.</p><p>All forum topics presented a unique challenge of identifying variations of terms and understanding use of slang. While this proved to be easy to overcome, it certainly played a vital role in the process in a way not necessary in the News topics, where spelling errors were largely non-existent.</p><p>On the first day, Sullivan started with Step Three, Random Baseline and reviewed a random sample of 1,534 documents. This was used both as a method to estimate prevalence and a means of gaining better understanding of the dataset for this and future topics in AtHome2. This random sample yielded 1 relevant document. Based on the sample prevalence we predicted 303 relevant documents existed in the dataset (95% confidence level with 2.5% margin of error). We would later discover the dataset contained 265 relevant documents, which is well within the margin of error. Given the amount of time necessary to complete this random sample, and the little value gained, Step Three was omitted from all subsequent topics reviewed by Sullivan. Day two was spent running keyword searches to find documents for seeding into the predictive coding algorithm and submitting documents to get a better understanding the TREC standard for relevance. At the end of day two, 273 documents had been submitted, with 204 being returned as relevant. This provided an adequate seed set to being relying more heavily on predictive coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@</head><p>On day three, Sullivan developed a strategy which he relied heavily in future topics. Rather than relying on Mr. EDR alone and reviewing the documents that were given high scores by the machine, he used the multi-modal approach to prioritize documents for review. Starting with all variations of "Amazon" w/5 "Review," he worked down reviewing and categorizing the highest scoring documents first. When he hit a point where few relevant documents were being found, he iteratively expanded the scope of his review universe. He moved to all variations of "Amazon" w/10 "Review, then "Amazon" w/25 "Review," and "Amazon" AND "Review." He expanded into "Amazon" and ("Review" or "Book" or "Feedback" or "Purchase") and eventually to any document containing a variation of "Amazon."</p><p>As previously mentioned, the unique characteristics of the forums required more creative searches than necessary in other datasets. Using the Concept Searching tool as a guide, it was determined that almost all reasonable variations of "Amazon" could be found using the following search: ("amazon*" OR "@mazon" OR "@maz0n" OR "azmon*" OR "azmn*" OR "amzn*"). This method proved effective in eliminating issues of missed documents due to slang or misspelling.</p><p>Using this method, Sullivan was able to identify 257 of the 265 relevant documents at the time he called Reasonable effort. 2,325 total documents had been reviewed, included the 1,534 documents in the initial random sample.</p><p>After calling Reasonable effort, Sullivan continued by submitting all documents that contained any variation of the term "Amazon" in order of priority score descending. 100% recall was obtained through this method. All remaining documents were then submitted in descending priority order, with no more relevant documents being returned.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, slightly darker line signifies 80% Recall call and the dark green line the Reasonable Recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Paying for Amazon Book Reviews topic, by the time 97.5% Recall had been attained only 0.21% of the corpus, 976 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.79% or 464,171 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% recall using the multimodal hybrid model of training EDR. The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Rootkits topic, by the time 97.5% Recall had been attained only 0.69% of the corpus, 3,188 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.31% or 461,959 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>______________________________________</head><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal, was handled with the assistance, at first, of Jensen. Losey performed all of the AI related searches in Step Five, including the probability and ranking related searches. His assistant focused on keyword searches and also made suggestions of documents to submit. Again, all final decisions on submittal were made by Losey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>______________________________________</head><p>On July 28, 2015, after making 20 submissions to TREC, and training after almost every submission, Losey had provided a total 1,071 documents to TREC and confirmed a total of 941 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 1,493 documents. After the 20 th TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 57.94% had been attained with a Precision of 87.86%, so his call proved to be early. There were only 3 additional submissions to TREC after the Reasonable call point, which we later learned was a mistake. We learned later that higher Recall and overall TREC scoring comes from multiple, smaller submissions, with training after each. This is another Topic in which we found many of the TREC judgments inconsistent and incomprehensible. Still, even with these problems and errors, a Recall of 70% was attained after a total of only 7,785 documents had been submitted out of 290,099, and only 1,493 documents had been reviewed.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall Call, and the dark green line the Reasonable Recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Capital Punishment topic, by the time 97.5% Recall had been attained 94.11% of the corpus, 273,010 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 5.89% or 17,089 documents. Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal was handled with the assistance at first of Bottolene. Losey performed all of the AI related searches in Step Five, including the probability and ranking related searches. His assistant focused on keyword searches and also made suggestions of documents to submit. Again, all final decisions on submittal were made by Losey. This review process went longer than other because this proved to be the highest prevalence Topic (5.91%).</p><p>On August 2, 2015, after making 25 submissions, with training after most of these, Losey had submitted a total 17,354 documents. A total of 16,872 of these submissions were confirmed relevant by TREC, for a Precision rate of 97.22%. The effort, or number of documents reviewed and coded by Losey to attain this result, was 2,025 documents. After the 25 th TREC submission, Losey decided to call Reasonable. It was later determined that an incredible Recall of 98.47% had been attained. The F1 measure was 97.84%. That is the Team's best result on any of the Bush Email Topics. Further, Losey believes this may be a personal best for Recall and F1 scores. There were 7 additional submissions to TREC after the Reasonable call point. In the 29th submission, 99.7% Recall was attained after submitting only 7,060 additional documents. The Precision was 70%.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the Reasonable Recall Call. The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Terri Schiavo topic, by the time 97.5% Recall had been attained only 5.90% of the corpus, 17,120 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 94.10% or 272,979 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@</head><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal, was performed with the assistance at first of Jensen. Losey handled all of the AI related searches in Step Five, including the probability and ranking related searches. His assistant focused on keyword searches and also made suggestions of documents to submit. Again, all final decisions on submittal were made by Losey.</p><p>On July 30, 2015, after making 23 document submissions to TREC providing a total 3,418 documents, Losey had found a total of 3,353 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 674 documents. After the 23 rd TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 92.24% had been attained, with Precision of 98.1%, and F1 of 95.08%. There were 7 additional submissions to TREC after the Reasonable call point. In the 27 th submission, after submitting only 3,427 additional documents (total 6,845), 95% Recall was attained. This was attained after submission of only 2.36% of the total documents.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the Reasonable Recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Affirmative Action topic, by the time 97.5% Recall had been attained only 2.90% of the corpus, 8,423 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 97.10% or 281,676 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. The initial submissions on the first day were to test the outlines of the category. The initial search of "Occupy" AND "Vancouver" identified a series of protests in Vancouver about economic income inequality. Documents were selected based on a varying of content, including "Occupy" movements in other cities, riots/protests that took place in the same area (but not same time) as the Occupy Vancouver protests, and generic stories about "Occupy" protests that reference protests in Vancouver but do not specifically name them as "Occupy Vancouver." Various sources were also tested, such as Letters to the Editor, stories sourced in other cities and so forth. Results helped formulate an anticipated rule on relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>______________________________________</head><p>After training EDR and receiving priority scores, relevant documents on subsequent submissions were confirmed by these rules and their priority scores. In fact, of the five irrelevant documents found in the last 2 submissions on July 29 th , three scored over 97% and contained substantial and direct references to Occupy Vancouver; these may be TREC coding errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A modified</head><p>Step Three, Random Sample of 1,000 documents was taken after Step Two was complete. The first 500 contained 50 "training" documents to focus on, while the second 500 documents contained 250. All documents hitting on "Occupy" OR "Vancouver" OR "Ashlie Gough" (a student who died at the protests) OR "Robson Square" (location of the protests) were reviewed, while all others mass trained as irrelevant. The last TREC submission on July 29 th was from the 1,000 random documents. Of the 1,000 documents, 33 were identified as relevant, confirmed by submission. On the second day, the 30 th, submissions by documents containing search terms and escalated as relevant were reviewed and submitted in priority order. In the first submission of the day, 123 were submitted as relevant and 118 came back as confirmed relevant. Of the five irrelevant in that set, four were documents that had the exact same relevant text as documents TREC previously confirmed as relevant. This is another example of the kind of "gold standard" inconsistencies the Team encountered in most of the Topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@</head><p>In the next set of submissions, documents escalated as relevant by Mr. EDR included stories sourced in the Vancouver paper on Occupy movements elsewhere, and sports stories with the word "occupy" in the article (e.g. "Another Vancouver player occupied the penalty box"). Once those documents were removed as irrelevant, all others were submitted and confirmed as relevant on submission. Some additional "gray area" documents were submitted (e.g. "Occupy Christmas" which was an offshoot of the protests, or campaign questions posed to candidates about the Occupy Vancouver protests).</p><p>As the Mr. EDR ranking scores decreased, the precision dropped. Prior to the final submissions, all documents with "Occupy" and "Vancouver" with relevance probability scores over 0.1% had either been submitted or reviewed, and all documents with scores over 75% without those terms had also been reviewed.</p><p>After the final Reasonable call was made the remaining documents were submitted in the following groups in descending priority order: 1) all documents currently coded as irrelevant by the human reviewer not yet submitted (2,212 documents, of which 45 were found to be relevant); 2) anything remaining with "Occup!" AND "Vancouver" (493 documents, all these had scores below 0.1%, of which 8 were found to be relevant); and then 3) all else (no relevant documents found in this set).</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Occupy Vancouver topic, by the time 97.5% Recall had been attained only 0.18% of the corpus, 1,584 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.82% or 900,850 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. Sullivan's computer background proved to be helpful in another uncommon forum topic. He considers himself more knowledgeable on this topic than the average person, but does not consider himself to be a subject matter expert on TOR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>______________________________________</head><p>Day 1 of this topic started with concept searching to find other keywords relating to TOR and anonymous browsing. Many previously unknown terms came to light, such as vpn, torbrowser, proxy, and ip. This process of using concept searching at the beginning of every topic became standard process for all remaining reviews done by Sullivan. The results of this exercise were used in future keyword searches as well as database-wide keyword highlighting.</p><p>Next, Sullivan started manually reviewing some of the hits on terms he felt would be most likely to yield responsive documents. Starting with 102 documents that hit on "TOR" and "anonym*" and moving on to hits on "TOR Browser," then "TOR" and "Prox*." It was not difficult to find a relatively high quantity of relevant documents. 108 relevant documents and 100 irrelevant documents were trained for predictive coding when the first learning session was run.</p><p>After the first learning session completed, Sullivan manually reviewed the highest scoring documents that contained the term "TOR" and found almost all to be relevant. Day 2 consisted of many iterations of learning sessions and evaluating search results. Similar to how Sullivan reviewed Topic 2052, he started with a narrow list of keyword searches and broadened the terms iteratively. For each set, he reviewed the documents with the highest predictive coding scores. Starting the day with "TOR" and "prox*," he moved to "Try TOR," "Try using TOR," and "Use TOR." Eventually he moved to all documents that contained "TOR" or "T0R." Every document he determined to be relevant was submitted to TREC.</p><p>At the end of the exercise, Sullivan had submitted 1,339 documents, with 1,244 being returned as relevant and 95 being returned as not relevant according to the TREC standard. At this point he called his shot at Reasonable Recall.</p><p>Day 3 started with the submission of all remaining documents that contained the term "TOR" as a method to catch any documents potentially missed. No additional relevant documents were returned.</p><p>All remaining documents in the database were submitted in order of descending predictive coding score. 14 more relevant documents were returned. Evaluation of these documents led to finding spectacular errors in the TREC standard. All 14 contained "*tor*" in some context, but none had any even marginal links to the current topic. A majority of the missed documents contained the term "hostigator.com." Evaluation of these 14 documents resulted in a determination that all 14 were caused by an error in the TREC classification system.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable Recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Using TOR for Anonymous Internet Browsing topic, by the time 97.5% Recall had been attained only 0.28% of the corpus, Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal, was performed with the assistance at first of Jensen. Losey handled all of the AI related searches in Step Five, including the probability and ranking related searches. His assistant focused on keyword searches and also made suggestions of documents to submit. Again, all final decisions on submittal were made by Losey.</p><p>On August 3, 2015, after making 8 document submissions to TREC providing a total 199 documents, Losey had found a total of 157 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 1,091 documents. After the 8 th TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 69.16% had been attained, with Precision of 78.89%, and F1 of 73.71%. He made the call decision a little prematurely on this Topic. In the next submission of only 20 documents, Losey brought the Recall level up to 71.37% with Precision of 73.97%. In the next submission of 781 documents he brought the Recall level to 77.97%. There were a total of 7 additional submissions to TREC after the Reasonable call point. After submitting a total of 1,611 documents, which is only 0.56% of the total documents, and reviewing only 1,091 documents, an 80% Recall was attained.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable Recall call. The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the New Medical Schools topic, by the time 97.5% Recall had been attained 82.16% of the corpus, 238,331 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 17.84% or 51,768 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@</head><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR.</p><p>Topic 109 was run by Losey with the assistance of a review attorney, Bottolene. The work to search the 290,099 Bush Emails started on August 3, 2015 and concluded on August 11, 2015. The project commenced with Losey and his assistant beginning Step Two, Multimodal Search Reviews.</p><p>Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal, was performed with the assistance at first of Bottolene. Losey handled all of the AI related searches in Step Five, including the probability and ranking related searches. His assistant focused on keyword searches and also made suggestions of documents to submit. Again, all final decisions on submittal were made by Losey.</p><p>On August 11, 2015, after making 26 submissions to TREC providing a total 510 documents, Losey had found a total of 485 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 953 documents. After the 26 th TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 95.85% had been attained, with Precision of 95.1%. There were 14 additional submissions to TREC after the Reasonable call point. In the next submission after the call of only 121 documents a Recall of 98.62% was attained. Recall of 100% was attained three submissions later after submitting only 1,074 documents, 0.37% of the total, and review of only 953 documents.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the Reasonable Recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Scarlet Letter Law topic, by the time 97.5% Recall had been attained only 0.20% of the corpus, 585 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.80% or 289,514 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal, was performed with some assistance at first of Jensen. Losey handled all of the AI related searches in Step Five, including the probability and ranking related searches. His assistant focused on keyword searches and also made a couple of suggestions of documents to submit. Again, all final decisions on submittal were made by Losey.</p><p>On August 6, 2015, after making 44 submissions to TREC providing a total 2,537 documents, Losey had found a total of 2,441 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 651 documents. After the 44 th TREC submission, Losey decided to call Reasonable. This proved to be a premature call. It was later determined that a Recall of 53.74% had been attained, with Precision of 96.22%, and F1 of 68.96%. There were 19 additional submissions to TREC after the Reasonable call point. After submitting a total of 7,541 documents, which is only 2.6% of the total documents, and reviewing only 651 documents, a 70% Recall level was attained. A Recall of 80% was attained after submitting 6.28% of the total documents, and Recall of 90% after submitting 7.92%.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the reasonable recall call. The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the School and Preschool Funding topic, by the time 97.5% Recall had been attained only 31.20% of the corpus, 90,524 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 68.80% or 199,573 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@</head><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR.</p><p>Topic 107 was run by Losey with the limited assistance of a review attorney, Jensen. The work to search the 290,099 Bush Emails started on August 5, 2015 and concluded on August 15, 2015. The project commenced with Losey and his assistant beginning Step Two, Multimodal Search Reviews.</p><p>Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal, was performed with some assistance at first of Jensen. Losey handled all of the AI related searches in Step Five, including the probability and ranking related searches. His assistant focused on keyword searches and also made a couple of suggestions of documents to submit. Again, all final decisions on submittal were made by Losey.</p><p>On August 14, 2015, after making 48 submissions to TREC providing a total 2,259 documents, Losey had found a total of 1,950 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 1,164 documents. After the 48 th TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 82.31% had been attained, with Precision of 86.32%, and F1 of 84.27%. There were 31 additional submissions to TREC after the Reasonable call point. After submitting a total of 2,648 documents, which is only 0.91% of the total documents, and reviewing only 1,164 documents, a 90% Recall level was attained with 80.55% Precision. Recall of 95% was attained after submitting 3,963 documents, 1.37% of total. Recall of 98% was attained after submitting 5,843 documents, 2.01% of total.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable Recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Tort Reform topic, by the time 97.5% Recall had been attained only 2.01% of the corpus, 5,843 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 97.99% or 284,256 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR.</p><p>______________________________________ evaluation determined the not relevant document was an error in the TREC standard. Next, 9 documents were found which contained "hydrofracking" in the title. All 9 were returned as relevant. He then continued with slight variations until submitting all documents that contain 2 or more hits on the term "fracking." After 1 hour and manual review of 29 documents, 746 documents had been submitted with 745 being returned as relevant.</p><p>Sullivan continued manually reviewing the documents with a single hit on fracking to sort out the false positives. After reviewing a couple sets of documents, he initiated his first predictive coding learning session for this topic.</p><p>On the start of Day 2, Sullivan believed he had found nearly all relevant documents for this topic. However, after reviewing documents with high predictive coding scores, he quickly realized that "fracturing" was another key term he hadn't previously considered. The use of predictive coding helped him quickly find an additional 400 relevant documents that would have been lost if using keyword searching alone.</p><p>Reasonable Recall was called after submitting 2,077 documents, with 1,893 returned as relevant.</p><p>The remaining documents were submitted in order of descending predictive coding scores, and 73 more relevant documents were returned. An evaluation of the returned documents contained many errors in the TREC standard, as well as a fair number of relevant documents that were not properly captured due to Sullivan's lack of knowledge of fracking and related mining terms.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Fracking topic, by the time 97.5% Recall had been attained only 0.27% of the corpus, 2,439 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.73% or 899,995 documents. The initial submissions on the first day were to test the outlines of the category. The initial search of "Kingston" AND "murder" identified a sensationalized murder story about a man with the last name "Shafia" murdering his daughters in an "honor killing." Documents containing the information in various forms (headline, text, "clickbait" link reference at end of article) were submitted. Results helped formulate an anticipated rule on relevance.</p><p>After training Mr. EDR and receiving relevance priority scores, a search on the specific victim names or "Shafia" were sorted by prioritization order. Samples of 10 documents above 90%, 10 between 80-90%, 10 between 60-80%, 10 between 25-60% and 10 below 25% showed that documents above 60% were very likely relevant. In fact, documents scoring over 90% all had multiple name hits and were specifically on point; documents in the middle ranges were usually indirectly related (e.g. about "honor killing," or domestic abuse, or more of a casual reference to the Kingston Mills murders); and those documents below 5% were almost always irrelevant. As a test, the second submission contained all documents with a score over 90%, along with samples of several documents at various scores greater than 50%, cutting the submission off at 200 documents even. With only 111 documents reviewed eyes on to this point, Reichenberger had a 98.5% precision on 205 documents submitted.</p><p>Of the 205 documents submitted to this point, the only 3 irrelevant documents all had the same trait: "Shafia" appeared in the header but there was no reference to it in the text. Similar documents were mass coded as irrelevant going forward. Likewise, people with names similar to the victims were found in the 40-60% probability range but were "false positive" documents. names the same as one of the victims. Searches were done on those specific names and masstagged as irrelevant. After a machine learning session, the scores adjusted dropping those false positive names to the bottom. At this point, a sampling of key term hits showed everything over 20% scores were relevant, and everything below 1% were irrelevant. Everything in between were low quality references to the murders with some irrelevant documents mixed in. As such, the next submission was for everything with a key term over 25% relevant score (456 documents) of which 449 were found relevant. The 7 documents found irrelevant were misclicks by Reichenberger (human error). In one case a document was primarily about a different murder, but later in the article there was relevant discussion of the target murder. Mr. EDR picked this up, but it was apparently missed by TREC's relevance scope adjudications. The 70% Recall call was then made having reviewed only 209 documents. It turned out that Recall was actually 58.6% with Precision at 98.5%.</p><p>The next submission consisted largely of documents containing a single line of "clickbait" link text found by TREC to be relevant. Other documents considered were documents with key terms that had scores raise above 20% following the machine learning session from the previous set and documents with scores above 50% with no key terms. While documents with key terms were largely found to be relevant, most of the documents without the terms were found to be irrelevant. In fact, documents scoring above 70% were often tangential to the issues in the murder (domestic violence mostly) but not relevant, while those 50-70% had no semblance of relevance at all, and were being escalated based on coincidental "clickbait" text advertisement lines at the end of the article. Another 459 documents were submitted with 456 were found relevant. The three irrelevant documents all were on the low end scores within the submission and were only passing references to the case. At this point the 80% recall call was made. Recall was actually at 99.64% with a precision at 99.34%. Only 272 documents were reviewed eyes on to this point, and 1120 relevant documents had been found. All documents with scores over 70% had been reviewed or submitted, and all those with key terms and scores over 20% had been reviewed or submitted.</p><p>Following the subsequent machine learning session, 30 documents were escalated to consider. One borderline document was considered potentially relevant and submitted, returned as irrelevant, while the rest all marked irrelevant. The Reasonable call was made.</p><p>After the Reasonable call was made documents were submitted in the following groups in descending priority score order: 1) three documents potentially relevant found while pending results of the previous submission (one was found to be relevant) 2) all documents reviewed eyes on anticipated to be irrelevant, but not yet submitted (199 documents, of which two were relevant and the only relevant text within these two documents were contained in a document previously submitted to TREC and returned as irrelevant); 3) anything mass-coded as irrelevant (this resulted in one relevant document, of which there does not appear to be any relevant material within it and may be yet another TREC coding error); and 4) anything remaining (all irrelevant).</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the Reasonable call. The initial submissions were to test the outlines of the category. The first submission was nine documents with varying discussions about Bitcoin (e.g. bitcoin exchanges, whether bitcoin was accepted, bitcoin mining, etc). All nine came back as irrelevant. A second submission of nine returned five relevant documents but no noticeable commonality among them except that "accept bitcoin" was relevant and "accept bitcoins" was not. The next 25 documents submitted also followed this trend, with singular "accept bitcoin" being relevant, those in the plural being irrelevant. All documents with "accept w/3 bitcoin" were submitted in the following two submission sets; however, having that text was not indicative of relevance, as some still came back irrelevant. Likewise, a variation of bitcoin ("BTC") was submitted (15 relevant, 5 irrelevant, no consistent thread).</p><p>After a machine learning session, the submitted documents were revisited and it appeared using bitcoin for legal activity or someone vouching for a forum user tended to be relevant, while illegal or immoral activity were irrelevant. For the next submission, the 60 highest scoring documents were submitted and anticipated as relevant/irrelevant based on the purpose of the transaction. While not perfect, this largely correlated with the results. (10 expected relevant, end result was 13). The next submission contained all documents with a 90% or higher probable relevant score and containing the term "vouch*". Of the 122 documents, 94 were relevant.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Surely Bitcoins can be Used topic, by the time 97.5% Recall had been attained only 3.66% of the corpus, 17,007 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 96.34% or 448,140 documents. During the initial part of the submission process, White trained on all documents deemed relevant or irrelevant by TREC. This helped create additional separation in the model and rankings. In one instance he left one obvious TREC mistake trained as relevant (a duplicate of another document that had been adjudicated relevant) in order to ensure he would find any others like it.</p><p>During the predictive analysis and training, White found it was most helpful to review certain sets of documents from the bottom-up, to analyze the least-likely candidates in cases where relevance seemed clear. In other sets of documents, where relevance seemed less certain, White reviewed from the top-down. After additional analysis was completed and 99 documents had been submitted to TREC, White predicted there would be 200 -250 relevant documents in total. (In the end, he would learn there were 255 total relevant documents in this topic, so the early prediction turned out to be quite close.) White also used random sampling in one instance, to train a set of 100 documents that seemed clearly irrelevant. These documents assisted Mr. EDR in separating irrelevant docs from relevant ones at a point early in the process when only relevant documents had been trained. This was part the Team's experimentation of the ideal ratios of irrelevant to relevant in training models.</p><p>As is almost always the case with an iterative training process, as the training and learning commenced, additional relevant subject areas came to light. While almost all of these areas were somewhat apparent from the start, fascinating and subtle nuances emerged. News stories on the case took little turns and spawned entirely new areas of relevance unto themselves. White thought the biggest challenge with these documents wasn't as much about whether they existed or how to locate them, but about whether TREC would see them as relevant or not. He found that it helped to track each pocket of relevance as a separate subject area, to utilize keywords for each subject area to create small seed sets, and to then utilize the predictive rankings within each subject area to dive deeper and ensure that each was adequately explored.</p><p>White made a total of 56 document submissions to TREC in this topic: 6 submissions between Aug. 6 th and 12 th , encompassing 184 documents, 22 submissions between Aug. 21 and 27 th , encompassing 284 documents, and the remaining 28 submissions on Aug. 28 th , encompassing 901,966 documents. In between most of these submissions he conducted iterative steps Four, Five and Six of the standard workflow, utilizing predictive ranking, search, and training.</p><p>After 218 documents had been submitted and additional priority-ranked documents and top keyword sets had been evaluated, White called 70%. There was still a fair quantity of suspected borderline documents in-hand, but his intuition was that he had probably surpassed 70% by a fair margin and so needed to call the shot. Actual Recall at this point turned out to be 83.53%.</p><p>White then studied closely the suspected borderline documents before he decided to submit them. He was attempting to determine the scope of relevance for these subject areas. After locating what he believed to be the full extent of the subject, and having found 23 more relevant documents, he called the 80% shot. White believed he was even farther along than 80%, given the ranked results he was seeing. As it turned out the actual Recall at this point was 92.55%.</p><p>After submitting 8 more documents that he thought might be considered relevant, but were close questions and probably would not, White called Reasonable. This was with 251 total documents submitted, 236 of them relevant, and only 779 documents reviewed. Actual Recall at this point was still 92.55%.</p><p>Having called Reasonable and finding nothing new that looked relevant, White turned to his pool of remaining documents that looked irrelevant, to allow the predictive ranking to help him being submitting them. Indeed, Mr. EDR helped see things he could not, and soon found 18 additional documents that contained an oblique reference to a subject related to the case. While these documents seemed just as oblique as others that were deemed irrelevant, the fact that the predictive rankings caught them quickly was reassuring. After an additional round of training and predictive ranking turned up no additional documents, the submissions continued.</p><p>Finally, at the 2,000 th document submitted, a "relevant" document was discovered that completed the 255-doc set. This document appeared to be a clear mistake, as it was only a reference to an unrelated London, UK murder. After that, all remaining documents submitted were confirmed as irrelevant. A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable Recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Pickton Murders topic, by the time 97.5% Recall had been attained only 0.05% of the corpus, 457 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.95% or 901,977 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR.</p><p>He finished his review of 902,434 News Articles on Aug. 15, 2015 after 5.0 total hours of effort.</p><p>Sullivan's background and knowledge in host sites was expected to be helpful in this topic, but in reality it worked against him. While he does not consider himself to be a subject matter expert on this topic, he has a solid level of knowledge with host sites. This proved difficult, because he thought he knew what documents should be considered relevant, but the TREC gold standard disagreed with most of his determinations.</p><p>Per his standard process, Sullivan started with concept searching to identify popular keywords to use as highlighting and future searches. This generated a long list of terms relating to different hosting sites and VPNs.</p><p>Sullivan continued with the next step of finding some documents to seed for predictive coding and get an understanding of the TREC line for relevance. He found 8 documents that hit on "offshore host* site*" and contained clearly relevant content by his definition. TREC determined all 8 to be not relevant. He then found 5 documents that relate to specific offshore hosting sites, such as hosting panama and anon hoster. TREC returned 1 relevant and 4 not relevant. He continued to try different variations of terms relating to hosting is specific countries and documents with different types of content and could not find any logic to the TREC relevance standard. Frustrated, he initiated a learning session and took a break.</p><p>Upon returning, he decided to try a test submission of 29 top scoring documents that contained the text "offshore" w/2 "host" without looking at any of the documents. To his surprise, 26 of the documents were returned by TREC as relevant. In a review of the documents, he saw no difference between the content of the TREC relevant documents and the documents he found and submitted that were returned as not relevant. The only general correlation he was able to identify is the TREC standard appeared to favor smaller sized documents with a higher proportion of content dedicated to offshore host sites. A document with a single line discussing offshore host sites was more likely to be relevant than a document with 50 lines and 10 references.</p><p>Being unable to determine any reasonable connection between content and relevance, Sullivan had no choice but to continue riding Mr. EDR's suggestions for documents to submit. This process consisted of many iterations of learning sessions and searching. Similar to how Sullivan reviewed Topic 2052 and 3481, he started with a narrow list of keyword searches and broadened the terms iteratively. For each set, he submitted the documents with the highest predictive coding scores. Starting with "offshore" w/2 "host*," he moved to "offshore" and "host," "offshore" and "web," and "offshore" and "vpn." Eventually he moved to all documents that contained "offshore" or "hosting." The difference between this process and what was used in prior reviews is Sullivan did not actually look at any of the documents. As he found his judgment to be out of line with the TREC standard, documents were submitted without review.</p><p>Results of a search would be taken and the top documents would be submitted. If most were determined to be relevant, lower sets of documents from the result would be submitted until a low amount of relevant documents were returned. He would then move on to the next search and repeat.</p><p>After exhausting all of the all key terms, Sullivan submitted all remaining documents in descending priority order. signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Offshore Host Sites topic, by the time 97.5% Recall had been attained only 0.37% of the corpus, 1,735 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.63% or 463,412 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% recall using the multi-modal hybrid model of training EDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>______________________________________</head><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Rooster Turkey Chicken Nuisance topic, by the time 97.5% Recall had been attained only 1.93% of the corpus, 17,414 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 98.07% or 885,020 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable Recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Article Spinner Spinning topic, by the time 97.5% Recall had been attained only 3.16% of the corpus, 14,698 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 96.84% or 450,449 documents. While he counts himself among Facebook's 1.5 billion active users, Sullivan does not consider himself more knowledgeable on this topic than the average person.</p><p>Day 1 on this topic started like all Sullivan topics with concept searching to find keywords relating to Facebook accounts for searching and highlighting. Specifically, variations of Facebook spelling and slang were investigated to ensure all common variants are identified. Many previously unexpected variations of facebook were identified, such as fbook. All variations were added to the highlighting list and documented for future searches.</p><p>Sullivan spent 2.5 hours on Day 1 trying to define relevance according to the TREC standard. He started with 8 documents that contained clear references to facebook accounts, and only 1 of the documents was returned as relevant according to the TREC standard. He continued by isolating documents that contained "Facebook account*" in the title as well as a number of common variants. At the end of the day, Sullivan was no closer to cracking the Facebook puzzle and was barely able to exceed 50% precision even though he was only submitting documents that were certain to be relevant by any objective standard.</p><p>Facing what appeared to be a dead-end, Sullivan started Day 2 by relying on the priority scores generated by Mr. EDR, and started to see much better results. While Sullivan was unable to identify which documents would be returned as responsive by TREC, Mr. EDR seemed to be able to find the pattern. As such, he stopped looking at the documents, and just started submitting all documents that had a high priority score that contained the term Facebook or any known variation, with learning sessions being run periodically to update the scores based on new learning. Once those documents were exhausted, all remaining documents were submitted in descending priority score order. He spent 2.75 hours submitting and evaluating the results, for a total of 5.25 hours spent on this topic.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Facebook Accounts topic, by the time 97.5% Recall had been attained only 0.54% of the corpus, 2,489 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.46% or 462,658 documents.</p><p>Topic 3357 was run by Reichenberger. The work to search the 902,434 News Articles database started on August 22, 2015, and was completed on August 23, 2015.</p><p>The initial submissions on the first day were to test the outlines of the relevance scope. It was ascertained in the first two submissions that documents relating to McKenna as a candidate were relevant, and those related to his job as Attorney General were irrelevant. Borderline documents were those associated with his Attorney General job that could be pretext to a political campaign (e.g. filing a suit related to Obamacare implementation). The third submission was made with the next 65 documents based on prioritization without looking at the content; the results largely confirmed the anticipated parameters (43 relevant, 22 irrelevant, with the borderline documents skewing to the irrelevant) The 70% call was made following the return of results.</p><p>After looking at what was being promoted by prioritization and containing "McKenna," the next 13 documents were submitted. Most of these appeared to be borderline, only 4 were adjudicated relevant by TREC. The 80% recall call was made at that point. One more set of 14 documents was submitted and only 3 came back responsive. The decision was then made to call Reasonable, and thereafter the final submissions were made.</p><p>The post call submissions were made by the following groups in descending priority score order: 1) all documents reviewed that were currently anticipated to be irrelevant, but had now been submitted (129 documents, of which 7 were relevant); 2) anything remaining with "McKenna" (695 documents, all irrelevant; and then 3) all else (all irrelevant).</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable call. Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal search began, including predictive coding features, with iterated training.</p><p>On August 25, 2015, after making 24 submissions to TREC, and training after almost every submission, Losey had provided a total of 12,799 documents to TREC and confirmed a total of 8,060 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 195 documents. After the 24 th TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 79.45% was attained by submission of only 12,799 documents, which is 2.8 % of the total documents. This was accomplished by review of only 0.04% of the total collection.</p><p>There were 21 additional submissions to TREC after the Reasonable call point. In the next submission after Reasonable call, the 25 th , 1,000 documents were submitted and they all came back relevant. Obviously an error in gamesmanship had been made and the call was made a little too early. After that 25 th submission, the Recall level rose to 89.31% and the Precision increased to 65.66%.</p><p>A 90% Recall was attained after submitting 14,477 documents. A 95% Recall was attained after submitting 16,983 documents, and 97.5% Recall attained after 19,821 documents were submitted, which was only 4.35% of total of 456,147 collection of BlackHat World Forum posts.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable Recall call. The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Web Scraping topic, by the time 97.5% Recall had been attained only 4.35% of the corpus, 19,821 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 95.65% or 436,326 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@</head><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR.</p><p>This Topic was run by Sullivan who started on August 24, 2015. He completed his review of 902,434 documents on August 25, 2015. The entire Team observed his final submissions and cheered on his perfect handling of this search project.</p><p>This topic was completely unknown to Sullivan prior to this exercise. His only knowledge came from a quick Google search on the topic.</p><p>Sullivan started late on Day 1 and began with a simple search using the following keywords: ((martin w/3 paul) AND cathy) OR ((martin w/3 cathy) AND paul). This search returned 26 documents. A quick review of the documents yielded 22 clearly relevant documents and 1 marginally relevant. Sullivan submitted the 22 relevant documents, which were all returned as relevant by TREC and quit for the night after 15 minutes of effort.</p><p>On Day 2, Sullivan went back to his standard process of using concept searching to find relevant keywords for highlighting and searches. As with all topics in dataset 3, spelling errors were non-existent, which removed the requirement of broad searching to account for slang or spelling issues.</p><p>Broad searches were run using all relevant keywords and the results were sampled. Next predictive coding scores were used to identify additional potentially relevant documents. A large number of false positives were encountered when it was discovered a popular hockey player and Prime Minister shared the same names as the parties. These were quickly identified and excluded from the potentially relevant set. After 90 minutes of work, Sullivan conceded that he was unable to find any additional relevant documents.</p><p>In reviewing the single marginally relevant document found on Day 1, it was determined this document was very likely to be relevant, so it was submitted to TREC and was in fact returned relevant. At this point, Sullivan called reasonable recall and submitted all remaining documents in descending order of priority score.</p><p>After all documents were submitted, it was discovered that Sullivan in fact had attained 100% recall and 100% precision at the point the reasonable call was made. Additionally, 95.7% recall was attained, with 100% precision, after only 15 minutes. In all, he was able to achieve a perfect game with only 1.75 hours committed to this topic! A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Paul and Cathy Lee Martin topic, by the time 97.5% Recall had been attained only 0.00% of the corpus, 23 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 100.00% or 902,411 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% recall using the multi-modal hybrid model of training EDR.</p><p>As a regular PayPal user for about 10 years, Sullivan has a high level of knowledge regarding this topic. This advanced knowledge proved to be a burden on this topic because his understanding of what should be relevant did not match with the TREC gold standard. He was able to overcome this burden by relying on a variety of advanced methods rather than using his own judgment in review of the documents.</p><p>Sullivan started this topic with his usual process of running concept searches to find similar and related keyword terms for highlighting and future searching. As with all forum topics, he spend some time identifying common variants based on misspelling or slang. All variations were added to the database for highlighting.</p><p>While using a number of methods to identify documents he felt were clearly relevant, Sullivan quickly realized he was unable to make any logic of the TREC relevance standard. Documents with similar or identical content were seemingly arbitrarily designated as relevant or not relevant. Rather than spend a considerable time evaluating the documents himself, as was done in Topic 2129 Facebook Accounts, he went straight to Mr. EDR for help.</p><p>Similar to the method developed in Topic 2129, Sullivan relied heavily on the predictive coding and did very little review on any documents. He would iteratively submit the highest scoring documents to TREC for analysis, and train the documents with the relevancy determination returned. In addition to using a continuous active learning approach, he started using the "Find Similar" feature much more to find documents that contained similar characteristics to documents already determined to be relevant. He started with documents that contained a variation of PayPal in the subject line, then moving to documents that contained the term anywhere in the text. Using this multimodal method he was able to work his way through the entire dataset with almost no actual review of the documents. In all, Sullivan was able to complete the review for this topic in less than 4 hours.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Rob Ford Cut the Waist topic, by the time 97.5% Recall had been attained only 3.89% of the corpus, 35,096 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 96.11% or 867,338 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR.</p><p>beginning Step Two, Multimodal Search Reviews.</p><p>Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal search began, including predictive coding features, with iterated training.</p><p>On August 28, 2015, after making 7 submissions to TREC, and training after almost every submission, Losey had provided a total of 97 documents to TREC and confirmed a total of 87 relevant documents. The effort, or number of documents individually reviewed and coded by Losey to attain this result, was 49 documents. After the 7 th TREC submission, Losey decided to call Reasonable. That call proved to be a little premature. It was later determined that a Recall of 76.99% was attained with Precision of 89.69%. In the 6 th automatic submission after the call, a Recall of 94.69% was attained after submitting only 693 documents total, which is 0.07% of the total of 902,434.</p><p>There were 24 submissions to TREC after the Reasonable call point. Total 100% Recall was attained after submitting 103,189 documents, which is 11.43% of the total.</p><p>A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Pacific Gateway topic, by the time 97.5% Recall had been attained only 11.35% of the corpus, 102,446 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 88.65% or 799,988 documents. Sullivan has some prior experience as a criminal defense attorney, with experience with traffic laws, but he has no prior experience with traffic enforcement cameras, which were not in use at the time he was practicing.</p><p>As usual, Sullivan started his investigation with his standard process of using keyword and concept searches to formulate a list of related keywords for highlighting and future searching. For this exercise, nothing extraordinary was discovered, but he was able to generate a good list of terms relating to traffic cameras, red light cameras, and traffic tickets.</p><p>Day 1 was a short day and started with submitting the results of the most popular keyword searches with minimal review. After 30 minutes of work, 76 documents were submitted with 50 being returned as relevant.</p><p>Using the documents identified on Day 1, Sullivan was able to start utilizing the predictive coding to supplement his searches on Day 2. He was able to progressively make his way through the review set using a combination of predictive coding scores and keyword hits.</p><p>He used this multimodal approach to submit large sets of documents with minimal, if any, manual review. He believed he had found all relevant documents after submitting only 5,347 total documents with 2,061 relevant. After submitting all of the remaining documents in descending order by predictive coding priority score, it was discovered he only missed 33 of the relevant documents in the dataset after submitting 0.6% of the documents! Because he minimized the amount of manual review on this topic, he was able to complete this topic after 3.0 hours on Day 2, for a total of 3.5 hours on this topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="18,133.69,462.46,36.28,12.04;18,90.23,489.34,133.23,12.04;18,108.23,502.78,412.54,12.04;18,90.23,516.22,431.07,12.04;18,90.23,530.63,375.77,11.07;18,108.23,544.07,413.37,11.07;18,90.23,557.51,430.56,11.07;18,90.23,570.95,418.25,11.07;18,90.23,584.39,410.37,11.07;18,90.23,596.86,429.32,12.04;18,90.23,611.27,159.81,11.07;18,108.23,624.47,376.91,11.07;18,90.23,637.91,412.78,11.07;18,90.23,651.35,390.47,11.07;18,90.23,664.79,415.49,11.07;18,90.23,677.26,428.61,12.04;18,90.23,690.70,407.03,12.04;18,90.23,704.14,392.93,12.04;18,135.66,246.79,341.15,213.79"><head></head><label></label><figDesc>Figure 8 5.2 Research Question No. 2.The Team attained very high recall and precision rates in most, but not all, of the thirty Total Recall topics. The Team's F1 scores at the time of Reasonable Call ranged from a perfect score of 100% in one topic (3484), to 91% to 99% in eight topics, and 82%-87% in five others.Although, of course, not directly comparable, these scores are far higher than any previously recorded in the six years ofTREC Legal Track (2006-2011) or any other study of legal search. One reason for this may be that the thirty topics in the 2015 Total Recall track presented relatively simple information needs by legal search standards, with one exception (Topic 109 -Scarlet Letter Law). Another may be improved software and the Team's hybrid multimodal method that includes continuous active learning.Since most of the thirty topics presented only simple, single-issue information needs suitable for single-facet classification, they had somewhat limited value for purposes of legal search experimentation. Further, only a few of the topics required any legal analysis for relevance identification. This again limited the use of these experiments for purposes of legal search research. These two factors, plus the omission of metadata, was a disadvantage to the e-Discovery Team of lawyers who are practiced in more complex information needs requiring extensive legal analysis and SME defined ground truths. Further, their methods and EDR</figDesc><graphic coords="18,135.66,246.79,341.15,213.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="19,90.23,646.50,34.01,9.47"><head>Figure 9</head><label>9</label><figDesc>Figure 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="62,161.66,125.98,289.08,12.04;62,90.23,152.86,134.05,12.04;62,90.23,169.42,117.67,8.92;62,90.23,182.86,97.14,8.92;62,90.23,196.30,106.91,8.92;62,90.23,424.54,403.06,8.92;62,90.23,434.86,250.06,12.04"><head></head><label></label><figDesc>run by Sullivan who also started on July 29, 2015. He finished his review of 465,149 forum posts in BlackHat World on July 31, 2015</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="64,90.23,74.68,401.54,10.78;64,90.23,88.12,335.91,10.78;64,90.23,369.88,423.59,10.78;64,90.23,383.32,366.18,10.78;64,228.78,701.28,154.86,11.51;65,233.10,74.56,146.18,10.90;65,90.23,101.44,128.47,10.90;65,90.23,115.74,117.67,10.04;65,90.23,129.18,88.82,10.04;65,90.23,142.62,106.91,10.04;65,90.23,370.86,428.97,10.04;65,90.23,382.30,425.66,12.04;65,90.23,395.74,402.51,12.04"><head>1</head><label></label><figDesc>,294 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.72% or 463,855 documents. The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr. EDR. run by Losey with the assistance of a review attorney, Jensen. The work to search the 290,099 Bush Emails started on July 31, 2015 and concluded on August 4, 2015. The project commenced with Losey and his assistant beginning Step Two, Multimodal Search Reviews.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="69,228.78,644.40,154.84,11.51;70,214.98,75.55,182.40,9.91;70,90.23,102.43,128.47,9.91;70,90.23,115.87,119.44,9.91;70,90.23,129.31,99.15,9.91;70,90.23,142.75,109.07,9.91;70,90.23,371.16,430.88,9.74;70,90.23,382.30,431.80,12.04;70,90.23,395.74,395.49,12.04;70,90.23,409.18,38.77,12.04"><head></head><label></label><figDesc>run by Losey with the limited assistance of a review attorney, Jensen. The work to search the 290,099 Bush Emails started on August 4, 2015 and concluded on August 8, 2015. The project commenced with Losey and his assistant beginning Step Two, Multimodal Search Reviews.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="77,90.23,329.04,410.49,11.51;77,90.23,341.76,334.54,11.51;77,228.78,642.48,154.86,11.51;78,215.80,74.84,180.80,10.62;78,90.23,101.72,134.05,10.62;78,90.23,115.16,119.44,10.62;78,90.23,128.60,99.15,10.62;78,90.23,142.04,109.07,10.62;78,90.23,368.86,420.75,12.04;78,90.23,385.52,293.65,8.82"><head></head><label></label><figDesc>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% recall using the multi-modal hybrid model of training EDR. run by Reichenberger. The work to search the 902,434 News Articles database started on August 4, 2015, and was completed on August 5, 2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="80,90.23,282.38,404.78,9.96;80,90.23,295.82,396.85,9.96;80,90.23,309.26,406.02,9.96;80,90.23,322.70,424.44,9.96;80,90.23,336.14,285.49,9.96;80,90.23,607.34,423.55,9.96;80,90.23,620.78,305.14,9.96;82,90.23,74.48,395.06,10.98;82,90.23,86.86,434.77,12.04"><head></head><label></label><figDesc>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Kingston Mills Lock Murders topic, by the time 97.5% Recall had been attained only 0.12% of the corpus, 1,096 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.88% or 901,338 documents. The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the Multimodal Hybrid model of training Mr. EDR. Topic 2130 was run by Reichenberger. The work to search the 465,147 documents in the BlackHat World Forums database started on August 7, 2015 and was completed August 13, 2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="99,90.23,649.14,404.79,10.88;99,90.23,662.58,375.98,10.88;99,90.23,676.02,415.11,10.88;99,90.23,689.46,428.61,10.88;99,90.23,702.90,335.92,10.88;100,90.23,360.46,423.56,12.04;100,90.23,375.82,305.14,12.04;100,228.78,687.60,154.86,11.51;101,248.93,73.42,114.50,12.04;101,90.23,100.30,199.29,12.04;101,90.23,115.92,117.67,9.86;101,90.23,129.36,102.71,9.86;101,90.23,142.80,106.91,9.86;101,90.23,371.04,408.12,9.86;101,90.23,382.30,416.52,12.04;101,90.23,395.74,268.83,12.04"><head></head><label></label><figDesc>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying Recall thresholds. On the Rob McKenna Gubernatorial Candidate topic, by the time 97.5% Recall had been attained only 0.02% of the corpus, 169 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.98% or 902,265 documents. The last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the Multimodal Hybrid model of training Mr. EDR. run by Losey who also started on August 22, 2015. He finished his review of 465,149 forum posts in BlackHat World on August 25, 2015. The project commenced as usual with Losey beginning Step Two, Multimodal Search Reviews.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,115.06,207.73,382.35,278.80"><head></head><label></label><figDesc></figDesc><graphic coords="7,115.06,207.73,382.35,278.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,132.78,72.23,346.88,251.85"><head></head><label></label><figDesc></figDesc><graphic coords="8,132.78,72.23,346.88,251.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="12,90.23,152.80,432.00,254.45"><head></head><label></label><figDesc></figDesc><graphic coords="12,90.23,152.80,432.00,254.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="13,90.23,85.66,432.00,256.65"><head></head><label></label><figDesc></figDesc><graphic coords="13,90.23,85.66,432.00,256.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="14,90.23,72.23,432.00,269.10"><head></head><label></label><figDesc></figDesc><graphic coords="14,90.23,72.23,432.00,269.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="16,90.23,86.16,317.31,198.70"><head></head><label></label><figDesc></figDesc><graphic coords="16,90.23,86.16,317.31,198.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="17,170.42,233.37,307.08,207.85"><head></head><label></label><figDesc></figDesc><graphic coords="17,170.42,233.37,307.08,207.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="17,177.35,511.46,294.14,192.45"><head></head><label></label><figDesc></figDesc><graphic coords="17,177.35,511.46,294.14,192.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="20,90.23,112.52,417.70,239.74"><head></head><label></label><figDesc></figDesc><graphic coords="20,90.23,112.52,417.70,239.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="20,113.23,467.06,385.20,238.79"><head></head><label></label><figDesc></figDesc><graphic coords="20,113.23,467.06,385.20,238.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="21,127.23,193.08,357.80,238.70"><head></head><label></label><figDesc></figDesc><graphic coords="21,127.23,193.08,357.80,238.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="21,139.78,455.23,332.90,245.90"><head></head><label></label><figDesc></figDesc><graphic coords="21,139.78,455.23,332.90,245.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="22,109.52,114.69,468.01,327.61"><head></head><label></label><figDesc></figDesc><graphic coords="22,109.52,114.69,468.01,327.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="29,239.93,399.23,278.05,201.85"><head></head><label></label><figDesc></figDesc><graphic coords="29,239.93,399.23,278.05,201.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="32,141.23,233.37,329.74,165.80"><head></head><label></label><figDesc></figDesc><graphic coords="32,141.23,233.37,329.74,165.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="32,157.23,520.72,297.62,198.60"><head></head><label></label><figDesc></figDesc><graphic coords="32,157.23,520.72,297.62,198.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="33,149.48,138.31,313.40,231.35"><head></head><label></label><figDesc></figDesc><graphic coords="33,149.48,138.31,313.40,231.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="35,152.43,72.23,307.58,183.10"><head></head><label></label><figDesc></figDesc><graphic coords="35,152.43,72.23,307.58,183.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="35,142.53,362.76,327.37,218.45"><head></head><label></label><figDesc></figDesc><graphic coords="35,142.53,362.76,327.37,218.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="36,140.41,72.23,331.65,244.82"><head></head><label></label><figDesc></figDesc><graphic coords="36,140.41,72.23,331.65,244.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="38,90.23,72.23,318.93,196.40"><head></head><label></label><figDesc></figDesc><graphic coords="38,90.23,72.23,318.93,196.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="38,90.23,376.16,313.05,208.90"><head></head><label></label><figDesc></figDesc><graphic coords="38,90.23,376.16,313.05,208.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="39,90.23,72.23,312.93,231.00"><head></head><label></label><figDesc></figDesc><graphic coords="39,90.23,72.23,312.93,231.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="41,146.88,238.23,299.95,217.65"><head></head><label></label><figDesc></figDesc><graphic coords="41,146.88,238.23,299.95,217.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="43,90.23,72.23,332.99,222.20"><head></head><label></label><figDesc></figDesc><graphic coords="43,90.23,72.23,332.99,222.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="43,90.23,361.57,332.90,245.75"><head></head><label></label><figDesc></figDesc><graphic coords="43,90.23,361.57,332.90,245.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="45,90.23,85.66,380.15,215.33"><head></head><label></label><figDesc></figDesc><graphic coords="45,90.23,85.66,380.15,215.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="45,129.23,368.80,353.89,215.20"><head></head><label></label><figDesc></figDesc><graphic coords="45,129.23,368.80,353.89,215.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="48,90.23,166.23,312.90,208.80"><head></head><label></label><figDesc></figDesc><graphic coords="48,90.23,166.23,312.90,208.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="48,90.23,442.17,335.00,247.30"><head></head><label></label><figDesc></figDesc><graphic coords="48,90.23,442.17,335.00,247.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="50,90.23,72.23,335.48,174.35"><head></head><label></label><figDesc></figDesc><graphic coords="50,90.23,72.23,335.48,174.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="50,90.23,354.01,317.10,211.60"><head></head><label></label><figDesc></figDesc><graphic coords="50,90.23,354.01,317.10,211.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="51,90.23,72.23,320.17,236.35"><head></head><label></label><figDesc></figDesc><graphic coords="51,90.23,72.23,320.17,236.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="52,90.23,407.93,342.41,187.65"><head></head><label></label><figDesc></figDesc><graphic coords="52,90.23,407.93,342.41,187.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="53,90.23,72.23,307.95,205.49"><head></head><label></label><figDesc></figDesc><graphic coords="53,90.23,72.23,307.95,205.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="53,90.23,358.30,313.94,231.75"><head></head><label></label><figDesc></figDesc><graphic coords="53,90.23,358.30,313.94,231.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="55,156.23,72.23,299.72,178.25"><head></head><label></label><figDesc></figDesc><graphic coords="55,156.23,72.23,299.72,178.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="55,146.48,357.62,319.20,213.00"><head></head><label></label><figDesc></figDesc><graphic coords="55,146.48,357.62,319.20,213.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="56,147.03,72.23,318.40,235.04"><head></head><label></label><figDesc></figDesc><graphic coords="56,147.03,72.23,318.40,235.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="57,90.23,354.22,302.38,170.60"><head></head><label></label><figDesc></figDesc><graphic coords="57,90.23,354.22,302.38,170.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="58,90.23,72.23,316.20,211.00"><head></head><label></label><figDesc></figDesc><graphic coords="58,90.23,72.23,316.20,211.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="58,90.23,350.87,316.58,233.70"><head></head><label></label><figDesc></figDesc><graphic coords="58,90.23,350.87,316.58,233.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="60,90.23,475.07,331.39,194.05"><head></head><label></label><figDesc></figDesc><graphic coords="60,90.23,475.07,331.39,194.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="61,90.23,152.80,327.37,218.45"><head></head><label></label><figDesc></figDesc><graphic coords="61,90.23,152.80,327.37,218.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="61,142.48,438.94,327.49,241.75"><head></head><label></label><figDesc></figDesc><graphic coords="61,142.48,438.94,327.49,241.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="63,90.23,448.21,304.60,181.15"><head></head><label></label><figDesc></figDesc><graphic coords="63,90.23,448.21,304.60,181.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="66,90.23,72.23,354.85,210.90"><head></head><label></label><figDesc></figDesc><graphic coords="66,90.23,72.23,354.85,210.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="66,90.23,404.58,344.90,230.15"><head></head><label></label><figDesc></figDesc><graphic coords="66,90.23,404.58,344.90,230.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="67,90.23,72.23,340.00,250.99"><head></head><label></label><figDesc></figDesc><graphic coords="67,90.23,72.23,340.00,250.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="68,90.23,367.64,298.75,185.15"><head></head><label></label><figDesc></figDesc><graphic coords="68,90.23,367.64,298.75,185.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="71,90.23,72.23,320.44,177.05"><head></head><label></label><figDesc></figDesc><graphic coords="71,90.23,72.23,320.44,177.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="71,90.23,356.71,281.21,187.65"><head></head><label></label><figDesc></figDesc><graphic coords="71,90.23,356.71,281.21,187.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="72,90.23,72.23,280.14,206.80"><head></head><label></label><figDesc></figDesc><graphic coords="72,90.23,72.23,280.14,206.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="73,90.23,381.07,287.15,190.29"><head></head><label></label><figDesc></figDesc><graphic coords="73,90.23,381.07,287.15,190.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="74,90.23,72.23,327.37,218.45"><head></head><label></label><figDesc></figDesc><graphic coords="74,90.23,72.23,327.37,218.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="74,90.23,371.80,321.32,237.20"><head></head><label></label><figDesc></figDesc><graphic coords="74,90.23,371.80,321.32,237.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="76,144.18,419.76,324.06,212.30"><head></head><label></label><figDesc></figDesc><graphic coords="76,144.18,419.76,324.06,212.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="81,90.23,72.23,322.65,238.18"><head></head><label></label><figDesc></figDesc><graphic coords="81,90.23,72.23,322.65,238.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="82,90.23,407.93,348.30,167.00"><head></head><label></label><figDesc></figDesc><graphic coords="82,90.23,407.93,348.30,167.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="83,90.23,72.23,330.44,220.50"><head></head><label></label><figDesc></figDesc><graphic coords="83,90.23,72.23,330.44,220.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="83,90.23,373.30,325.85,240.54"><head></head><label></label><figDesc></figDesc><graphic coords="83,90.23,373.30,325.85,240.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="87,90.23,72.23,319.64,210.80"><head></head><label></label><figDesc></figDesc><graphic coords="87,90.23,72.23,319.64,210.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="87,90.23,391.16,317.93,212.15"><head></head><label></label><figDesc></figDesc><graphic coords="87,90.23,391.16,317.93,212.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="88,90.23,72.23,313.94,231.75"><head></head><label></label><figDesc></figDesc><graphic coords="88,90.23,72.23,313.94,231.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="90,90.23,125.41,292.67,190.35"><head></head><label></label><figDesc></figDesc><graphic coords="90,90.23,125.41,292.67,190.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="90,90.23,409.91,345.50,230.55"><head></head><label></label><figDesc></figDesc><graphic coords="90,90.23,409.91,345.50,230.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="92,90.23,72.23,328.12,207.45"><head></head><label></label><figDesc></figDesc><graphic coords="92,90.23,72.23,328.12,207.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="92,90.23,387.11,346.70,231.35"><head></head><label></label><figDesc></figDesc><graphic coords="92,90.23,387.11,346.70,231.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="93,90.23,72.23,346.65,255.90"><head></head><label></label><figDesc></figDesc><graphic coords="93,90.23,72.23,346.65,255.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="94,90.23,407.93,291.83,208.80"><head></head><label></label><figDesc></figDesc><graphic coords="94,90.23,407.93,291.83,208.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="95,90.23,85.66,321.80,214.73"><head></head><label></label><figDesc></figDesc><graphic coords="95,90.23,85.66,321.80,214.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="95,90.23,380.98,317.94,234.70"><head></head><label></label><figDesc></figDesc><graphic coords="95,90.23,380.98,317.94,234.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="97,90.23,179.66,290.80,187.85"><head></head><label></label><figDesc></figDesc><graphic coords="97,90.23,179.66,290.80,187.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="97,90.23,474.93,324.30,216.40"><head></head><label></label><figDesc></figDesc><graphic coords="97,90.23,474.93,324.30,216.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="98,90.23,112.52,310.50,229.29"><head></head><label></label><figDesc></figDesc><graphic coords="98,90.23,112.52,310.50,229.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="100,138.23,97.68,335.39,223.80"><head></head><label></label><figDesc></figDesc><graphic coords="100,138.23,97.68,335.39,223.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="100,138.23,399.82,335.40,247.59"><head></head><label></label><figDesc></figDesc><graphic coords="100,138.23,399.82,335.40,247.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="102,90.23,85.66,293.72,188.85"><head></head><label></label><figDesc></figDesc><graphic coords="102,90.23,85.66,293.72,188.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="102,90.23,381.93,340.48,227.20"><head></head><label></label><figDesc></figDesc><graphic coords="102,90.23,381.93,340.48,227.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="103,90.23,72.23,340.00,250.99"><head></head><label></label><figDesc></figDesc><graphic coords="103,90.23,72.23,340.00,250.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="105,90.23,72.23,301.80,185.17"><head></head><label></label><figDesc></figDesc><graphic coords="105,90.23,72.23,301.80,185.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="105,90.23,360.60,302.49,201.85"><head></head><label></label><figDesc></figDesc><graphic coords="105,90.23,360.60,302.49,201.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="106,90.23,72.23,296.40,218.88"><head></head><label></label><figDesc></figDesc><graphic coords="106,90.23,72.23,296.40,218.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="107,90.23,488.49,308.60,187.98"><head></head><label></label><figDesc></figDesc><graphic coords="107,90.23,488.49,308.60,187.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="108,90.23,152.80,322.95,215.50"><head></head><label></label><figDesc></figDesc><graphic coords="108,90.23,152.80,322.95,215.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="108,90.23,422.51,285.05,210.50"><head></head><label></label><figDesc></figDesc><graphic coords="108,90.23,422.51,285.05,210.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="110,90.23,72.23,335.83,191.15"><head></head><label></label><figDesc></figDesc><graphic coords="110,90.23,72.23,335.83,191.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="110,90.23,370.81,335.00,223.54"><head></head><label></label><figDesc></figDesc><graphic coords="110,90.23,370.81,335.00,223.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="111,90.23,72.23,332.09,245.15"><head></head><label></label><figDesc></figDesc><graphic coords="111,90.23,72.23,332.09,245.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="112,90.23,327.36,369.55,183.50"><head></head><label></label><figDesc></figDesc><graphic coords="112,90.23,327.36,369.55,183.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="113,90.23,72.23,329.24,219.70"><head></head><label></label><figDesc></figDesc><graphic coords="113,90.23,72.23,329.24,219.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="113,90.23,372.50,331.25,244.53"><head></head><label></label><figDesc></figDesc><graphic coords="113,90.23,372.50,331.25,244.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="115,90.23,110.92,412.65,182.35"><head></head><label></label><figDesc></figDesc><graphic coords="115,90.23,110.92,412.65,182.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="115,90.23,414.12,431.98,251.45"><head></head><label></label><figDesc></figDesc><graphic coords="115,90.23,414.12,431.98,251.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="116,90.23,99.09,431.93,276.60"><head></head><label></label><figDesc></figDesc><graphic coords="116,90.23,99.09,431.93,276.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,91.76,237.29,429.25,347.55"><head>28,949 30,974 32,138 32,590 32,673 32,916 Effort (Docs reviewed) by RECALL SCORES</head><label></label><figDesc></figDesc><table coords="11,91.76,247.99,429.25,336.85"><row><cell>Topic</cell><cell>Need</cell><cell>Total Documents</cell><cell>Total Relevant</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell cols="2">95% 97.5% 100%</cell></row><row><cell>Topic 100</cell><cell>School and Preschool Funding</cell><cell>290,099</cell><cell>4,542</cell><cell>651</cell><cell>651</cell><cell>651</cell><cell>651</cell><cell>651</cell></row><row><cell>Topic 101</cell><cell>Judicial Selection</cell><cell>290,099</cell><cell cols="6">5,834 6,841 6,895 6,895 6,895 6,895 6,896</cell></row><row><cell>Topic 102</cell><cell>Capital Punishment</cell><cell>290,099</cell><cell cols="6">1,624 1,493 1,493 1,493 1,493 1,493 1,493</cell></row><row><cell>Topic 103</cell><cell>Manatee Protection</cell><cell>290,099</cell><cell cols="6">5,725 7,203 7,203 7,203 7,203 7,203 7,203</cell></row><row><cell>Topic 104</cell><cell>New Medical Schools</cell><cell>290,099</cell><cell cols="6">227 1,091 1,091 1,091 1,091 1,091 1,091</cell></row><row><cell>Topic 105</cell><cell>Affirmative Action</cell><cell>290,099</cell><cell>3,635</cell><cell>582</cell><cell>582</cell><cell>582</cell><cell>674</cell><cell>674</cell></row><row><cell>Topic 106</cell><cell>Terri Schiavo</cell><cell>290,099</cell><cell>17,135</cell><cell cols="5">831 1,987 1,995 2,005 2,025 2,226</cell></row><row><cell>Topic 107</cell><cell>Tort Reform</cell><cell>290,099</cell><cell>2,369</cell><cell cols="5">877 1,142 1,164 1,164 1,164 1,164</cell></row><row><cell>Topic 108</cell><cell>Manatee County</cell><cell>290,099</cell><cell>2,375</cell><cell>696</cell><cell>696</cell><cell>696</cell><cell>696</cell><cell>696</cell></row><row><cell>Topic 109</cell><cell>Scarlet Letter Law</cell><cell>290,099</cell><cell>506</cell><cell>491</cell><cell>496</cell><cell>639</cell><cell>753</cell><cell>753</cell></row><row><cell>Topic 2052</cell><cell>Paying for Amazon Book Reviews</cell><cell>465,147</cell><cell cols="6">265 1,842 1,960 2,213 2,325 2,325 2,325</cell></row><row><cell>Topic 2108</cell><cell>CAPTCHA Services</cell><cell>465,147</cell><cell cols="6">656 2,101 2,101 2,101 2,101 2,101 2,101</cell></row><row><cell>Topic 2129</cell><cell>Facebook Accounts</cell><cell>465,147</cell><cell>589</cell><cell>94</cell><cell>94</cell><cell>94</cell><cell>94</cell><cell>94</cell></row><row><cell>Topic 2130</cell><cell>Surely Bitcoins can be Used</cell><cell>465,147</cell><cell>2,299</cell><cell>283</cell><cell>283</cell><cell>285</cell><cell>285</cell><cell>285</cell></row><row><cell>Topic 2134</cell><cell>Paypal Accounts</cell><cell>465,147</cell><cell>252</cell><cell>19</cell><cell>19</cell><cell>19</cell><cell>19</cell><cell>19</cell></row><row><cell>Topic 2158</cell><cell>Using TOR for Anonymous Internet Browsing</cell><cell>465,147</cell><cell cols="6">1,261 1,332 1,332 1,332 1,332 1,332 1,335</cell></row><row><cell>Topic 2225</cell><cell>Rootkits</cell><cell>465,147</cell><cell>182</cell><cell>183</cell><cell>186</cell><cell>205</cell><cell>214</cell><cell>219</cell></row><row><cell>Topic 2322</cell><cell>Web Scraping</cell><cell>465,147</cell><cell>10,145</cell><cell>194</cell><cell>195</cell><cell>195</cell><cell>195</cell><cell>195</cell></row><row><cell>Topic 2333</cell><cell>Article Spinner Spinning</cell><cell>465,147</cell><cell>4,805</cell><cell>190</cell><cell>228</cell><cell>228</cell><cell>228</cell><cell>228</cell></row><row><cell>Topic 2461</cell><cell>Offshore Host Sites</cell><cell>465,147</cell><cell>179</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell>Topic 3089</cell><cell>Pickton Murders</cell><cell>902,434</cell><cell>255</cell><cell>472</cell><cell cols="3">516 834 191 779 306 306</cell><cell>310</cell></row><row><cell>Topic 3357</cell><cell>Occupy Vancouver</cell><cell>902,434</cell><cell>629</cell><cell>751</cell><cell>751</cell><cell>920</cell><cell>920</cell><cell>920</cell></row><row><cell>Topic 3378</cell><cell>Rob McKenna Gubernatorial Candidate</cell><cell>902,434</cell><cell>66</cell><cell>79</cell><cell>161</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell>Topic 3423</cell><cell>Rob Ford Cut the Waist</cell><cell>902,434</cell><cell>76</cell><cell>92</cell><cell>92</cell><cell>92</cell><cell>92</cell><cell>92</cell></row><row><cell>Topic 3431</cell><cell>Kingston Mills Lock Murders</cell><cell>902,434</cell><cell>1,111</cell><cell>272</cell><cell>272</cell><cell>272</cell><cell>272</cell><cell>272</cell></row><row><cell>Topic 3481</cell><cell>Fracking</cell><cell>902,434</cell><cell>1,966</cell><cell>31</cell><cell>236</cell><cell>367</cell><cell>367</cell><cell>367</cell></row><row><cell>Topic 3484</cell><cell>Paul and Cathy Lee Martin</cell><cell>902,434</cell><cell>23</cell><cell>22</cell><cell>22</cell><cell>22</cell><cell>22</cell><cell>73</cell></row><row><cell cols="2">Figure 1 TOTALS</cell><cell>16,576,800</cell><cell>70,964</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="19,55.32,382.28,489.17,86.24"><head>F1 Topic 100 Rank Topic 101 Rank Topic 102 Rank Topic 103 Rank Topic 104 Rank Topic 105 Rank Topic 106 Rank Topic 107 Rank Topic 108 Rank Topic 109 Rank eDiscoveryTeam</head><label></label><figDesc></figDesc><table coords="19,55.32,389.89,486.44,78.62"><row><cell></cell><cell>68.96%</cell><cell>2</cell><cell>82.45%</cell><cell>4</cell><cell>69.88%</cell><cell>1</cell><cell>90.69%</cell><cell>1</cell><cell>73.53%</cell><cell>1</cell><cell>95.07%</cell><cell>1</cell><cell>97.38%</cell><cell>1</cell><cell>84.40%</cell><cell>1</cell><cell>47.03%</cell><cell>5</cell><cell>95.58%</cell></row><row><cell>NINJA</cell><cell>22.74%</cell><cell>8</cell><cell>79.17%</cell><cell>5</cell><cell>56.38%</cell><cell>5</cell><cell>83.79%</cell><cell>3</cell><cell>57.40%</cell><cell>4</cell><cell>77.24%</cell><cell>2</cell><cell>88.90%</cell><cell>5</cell><cell>50.89%</cell><cell>8</cell><cell cols="2">13.43% 11</cell><cell>48.79%</cell></row><row><cell>UvA.ILPS-baseline</cell><cell>73.55%</cell><cell>1</cell><cell>86.36%</cell><cell>1</cell><cell>56.38%</cell><cell>4</cell><cell>89.94%</cell><cell>2</cell><cell cols="2">10.27% 10</cell><cell>64.13%</cell><cell>5</cell><cell>95.87%</cell><cell>2</cell><cell>77.26%</cell><cell>4</cell><cell>64.47%</cell><cell>1</cell><cell>28.88%</cell></row><row><cell>UvA.ILPS-baseline2</cell><cell>45.56%</cell><cell>5</cell><cell>71.04%</cell><cell>7</cell><cell>42.42%</cell><cell>8</cell><cell>77.24%</cell><cell>6</cell><cell cols="2">2.42% 11</cell><cell>43.27%</cell><cell>7</cell><cell>84.67%</cell><cell>6</cell><cell>47.81%</cell><cell>9</cell><cell>35.13%</cell><cell>8</cell><cell>26.90%</cell></row><row><cell>WaterlooClarke-UWPAH1</cell><cell>11.95%</cell><cell>9</cell><cell cols="2">9.98% 11</cell><cell cols="2">32.16% 11</cell><cell cols="2">10.46% 11</cell><cell>68.51%</cell><cell>2</cell><cell cols="2">15.99% 10</cell><cell cols="2">3.61% 10</cell><cell cols="2">22.96% 11</cell><cell>21.61%</cell><cell>9</cell><cell>0.73%</cell></row><row><cell>WaterlooClarke-UWPAH2</cell><cell cols="2">10.37% 10</cell><cell cols="2">9.98% 10</cell><cell cols="2">32.16% 10</cell><cell cols="2">10.46% 10</cell><cell>65.93%</cell><cell>3</cell><cell cols="2">15.99% 11</cell><cell cols="2">3.54% 11</cell><cell cols="2">23.11% 10</cell><cell cols="2">21.54% 10</cell><cell>0.73%</cell></row><row><cell>WaterlooCormack-Knee100</cell><cell>45.02%</cell><cell>6</cell><cell>67.65%</cell><cell>9</cell><cell>42.32%</cell><cell>9</cell><cell>71.10%</cell><cell>9</cell><cell>28.49%</cell><cell>7</cell><cell>34.08%</cell><cell>8</cell><cell>77.03%</cell><cell>9</cell><cell>53.92%</cell><cell>7</cell><cell>42.65%</cell><cell>7</cell><cell>0.94%</cell></row><row><cell>WaterlooCormack-Knee1000</cell><cell>41.82%</cell><cell>7</cell><cell>67.67%</cell><cell>8</cell><cell>45.21%</cell><cell>7</cell><cell>71.11%</cell><cell>8</cell><cell>31.06%</cell><cell>5</cell><cell>33.90%</cell><cell>9</cell><cell>77.03%</cell><cell>8</cell><cell>57.79%</cell><cell>5</cell><cell>42.65%</cell><cell>6</cell><cell>27.17%</cell></row><row><cell>WaterlooCormack-stop2399</cell><cell>68.21%</cell><cell>3</cell><cell>72.02%</cell><cell>6</cell><cell>51.74%</cell><cell>6</cell><cell>75.55%</cell><cell>7</cell><cell>14.34%</cell><cell>9</cell><cell>58.92%</cell><cell>6</cell><cell>81.60%</cell><cell>7</cell><cell>57.77%</cell><cell>6</cell><cell>58.96%</cell><cell>2</cell><cell>27.17%</cell></row><row><cell>Webis-baseline</cell><cell>66.96%</cell><cell>4</cell><cell>83.87%</cell><cell>3</cell><cell>68.36%</cell><cell>2</cell><cell>82.42%</cell><cell>5</cell><cell>27.95%</cell><cell>8</cell><cell>64.91%</cell><cell>4</cell><cell>94.89%</cell><cell>4</cell><cell>79.24%</cell><cell>3</cell><cell>58.76%</cell><cell>3</cell><cell>0.00% 11</cell></row><row><cell>Webis-keyphrase</cell><cell cols="2">0.14% 11</cell><cell>85.21%</cell><cell>2</cell><cell>67.71%</cell><cell>3</cell><cell>83.15%</cell><cell>4</cell><cell>31.04%</cell><cell>6</cell><cell>65.13%</cell><cell>3</cell><cell>94.90%</cell><cell>3</cell><cell>79.24%</cell><cell>2</cell><cell>58.34%</cell><cell>4</cell><cell>0.33%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="19,55.32,462.44,489.17,109.45"><head>10 F1 Topic 2052 Rank Topic 2108 Rank Topic 2129 Rank Topic 2130 Rank Topic 2134 Rank Topic 2158 Rank Topic 2225 Rank Topic 2322 Rank Topic 2333 Rank Topic 2461 Rank eDiscoveryTeam 45</head><label></label><figDesc></figDesc><table coords="19,55.32,485.29,486.44,86.61"><row><cell></cell><cell>.21%</cell><cell>1</cell><cell>53.99%</cell><cell>1</cell><cell>26.10%</cell><cell>6</cell><cell>64.31%</cell><cell>1</cell><cell>12.23%</cell><cell>6</cell><cell>95.61%</cell><cell>1</cell><cell>84.90%</cell><cell>1</cell><cell>72.60%</cell><cell>3</cell><cell>73.23%</cell><cell>1</cell><cell>16.68%</cell></row><row><cell>NINJA</cell><cell>58.13%</cell><cell>2</cell><cell>53.66%</cell><cell>2</cell><cell>49.22%</cell><cell>2</cell><cell>52.18%</cell><cell>2</cell><cell>39.70%</cell><cell>2</cell><cell>76.26%</cell><cell>2</cell><cell>39.43%</cell><cell>4</cell><cell>24.83%</cell><cell>9</cell><cell>62.65%</cell><cell>6</cell><cell>24.48%</cell></row><row><cell>UvA.ILPS-baseline</cell><cell>10.74%</cell><cell>3</cell><cell>22.74%</cell><cell>9</cell><cell>21.88%</cell><cell>7</cell><cell>41.12%</cell><cell>4</cell><cell>8.08%</cell><cell>7</cell><cell>42.02%</cell><cell>7</cell><cell>7.20%</cell><cell>9</cell><cell>73.20%</cell><cell>2</cell><cell>69.80%</cell><cell>2</cell><cell>7.33%</cell></row><row><cell>UvA.ILPS-baseline2</cell><cell>10.37%</cell><cell>4</cell><cell cols="2">22.45% 10</cell><cell>19.23%</cell><cell>8</cell><cell>30.88%</cell><cell>5</cell><cell>6.96%</cell><cell>8</cell><cell>22.47%</cell><cell>9</cell><cell cols="2">6.45% 10</cell><cell>48.11%</cell><cell>6</cell><cell>46.02%</cell><cell>9</cell><cell>6.53% 10</cell></row><row><cell>WaterlooClarke-UWPAH1</cell><cell>78.54%</cell><cell>5</cell><cell>52.20%</cell><cell>3</cell><cell>56.89%</cell><cell>1</cell><cell>13.42%</cell><cell>8</cell><cell>63.18%</cell><cell>1</cell><cell>40.08%</cell><cell>8</cell><cell>61.45%</cell><cell>2</cell><cell cols="2">5.85% 10</cell><cell cols="2">12.22% 10</cell><cell>49.90%</cell></row><row><cell>WaterlooCormack-Knee100</cell><cell>41.43%</cell><cell>6</cell><cell>33.89%</cell><cell>5</cell><cell>28.52%</cell><cell>5</cell><cell>19.49%</cell><cell>6</cell><cell>18.45%</cell><cell>3</cell><cell cols="2">16.15% 10</cell><cell>41.33%</cell><cell>3</cell><cell>47.39%</cell><cell>7</cell><cell>47.33%</cell><cell>7</cell><cell>43.87%</cell></row><row><cell>WaterlooCormack-Knee1000</cell><cell>38.10%</cell><cell>7</cell><cell>34.00%</cell><cell>4</cell><cell>30.91%</cell><cell>4</cell><cell>19.45%</cell><cell>7</cell><cell>18.45%</cell><cell>4</cell><cell>60.57%</cell><cell>5</cell><cell>27.02%</cell><cell>5</cell><cell>44.11%</cell><cell>8</cell><cell>47.30%</cell><cell>8</cell><cell>21.65%</cell></row><row><cell>WaterlooCormack-stop2399</cell><cell>16.94%</cell><cell>8</cell><cell>31.35%</cell><cell>7</cell><cell>31.01%</cell><cell>3</cell><cell>46.56%</cell><cell>3</cell><cell>15.51%</cell><cell>5</cell><cell>45.06%</cell><cell>6</cell><cell>11.84%</cell><cell>8</cell><cell>75.86%</cell><cell>1</cell><cell>68.87%</cell><cell>3</cell><cell>11.72%</cell></row><row><cell>Webis-baseline</cell><cell>13.24%</cell><cell>9</cell><cell>32.65%</cell><cell>6</cell><cell cols="2">7.73% 10</cell><cell cols="2">0.00% 10</cell><cell cols="2">2.21% 10</cell><cell>61.11%</cell><cell>4</cell><cell>18.36%</cell><cell>6</cell><cell>67.40%</cell><cell>5</cell><cell>68.07%</cell><cell>4</cell><cell>43.56%</cell></row><row><cell>Webis-keyphrase</cell><cell cols="2">10.53% 10</cell><cell>30.56%</cell><cell>8</cell><cell>8.29%</cell><cell>9</cell><cell>0.00%</cell><cell>9</cell><cell>2.21%</cell><cell>9</cell><cell>62.14%</cell><cell>3</cell><cell>12.97%</cell><cell>7</cell><cell>67.72%</cell><cell>4</cell><cell>68.04%</cell><cell>5</cell><cell>31.95%</cell></row><row><cell>F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="19,55.32,565.82,489.17,78.98"><head>Topic 3089 Rank Topic 3133 Rank Topic 3226 Rank Topic 3290 Rank Topic 3357 Rank Topic 3378 Rank Topic 3423 Rank Topic 3431 Rank Topic 3481 Rank Topic 3484 Rank eDiscoveryTeam</head><label></label><figDesc></figDesc><table coords="19,55.32,573.44,486.44,71.36"><row><cell></cell><cell>93.28%</cell><cell>1</cell><cell>82.46%</cell><cell>1</cell><cell>55.39%</cell><cell>4</cell><cell>37.70%</cell><cell>2</cell><cell>86.70%</cell><cell>2</cell><cell>68.21%</cell><cell>1</cell><cell>58.12%</cell><cell>1</cell><cell>99.24%</cell><cell>1</cell><cell>95.48%</cell><cell>1</cell><cell>100.00%</cell></row><row><cell>NINJA</cell><cell>86.84%</cell><cell>2</cell><cell>67.97%</cell><cell>2</cell><cell>22.75%</cell><cell>9</cell><cell>38.98%</cell><cell>1</cell><cell>89.95%</cell><cell>1</cell><cell>67.88%</cell><cell>2</cell><cell>57.85%</cell><cell>2</cell><cell>74.67%</cell><cell>4</cell><cell>71.59%</cell><cell>2</cell><cell>100.00%</cell></row><row><cell>UvA.ILPS-baseline</cell><cell>5.47%</cell><cell>9</cell><cell>2.47%</cell><cell>9</cell><cell>37.25%</cell><cell>5</cell><cell>0.57%</cell><cell>9</cell><cell>12.75%</cell><cell>9</cell><cell>1.39%</cell><cell>9</cell><cell>1.26%</cell><cell>9</cell><cell>21.90%</cell><cell>7</cell><cell>35.00%</cell><cell>7</cell><cell>0.51%</cell></row><row><cell>UvA.ILPS-baseline2</cell><cell cols="2">5.35% 10</cell><cell cols="2">2.39% 10</cell><cell>34.75%</cell><cell>6</cell><cell cols="2">0.39% 10</cell><cell cols="2">11.82% 10</cell><cell cols="2">1.38% 10</cell><cell cols="2">0.74% 10</cell><cell>21.74%</cell><cell>8</cell><cell>29.19%</cell><cell>9</cell><cell>0.51% 10</cell></row><row><cell>WaterlooClarke-UWPAH1</cell><cell>76.14%</cell><cell>3</cell><cell>50.45%</cell><cell>3</cell><cell>24.73%</cell><cell>7</cell><cell>11.90%</cell><cell>5</cell><cell>62.65%</cell><cell>3</cell><cell>32.58%</cell><cell>4</cell><cell>18.65%</cell><cell>5</cell><cell>44.29%</cell><cell>6</cell><cell cols="2">26.87% 10</cell><cell>12.99%</cell></row><row><cell>WaterlooCormack-Knee100</cell><cell>57.66%</cell><cell>4</cell><cell>49.02%</cell><cell>4</cell><cell>64.61%</cell><cell>2</cell><cell>26.09%</cell><cell>3</cell><cell>55.57%</cell><cell>4</cell><cell>57.87%</cell><cell>3</cell><cell>30.70%</cell><cell>3</cell><cell>93.34%</cell><cell>3</cell><cell>53.62%</cell><cell>5</cell><cell>34.07%</cell></row><row><cell>WaterlooCormack-Knee1000</cell><cell>37.35%</cell><cell>5</cell><cell>18.38%</cell><cell>6</cell><cell>68.61%</cell><cell>1</cell><cell>4.59%</cell><cell>7</cell><cell>48.23%</cell><cell>5</cell><cell>11.26%</cell><cell>7</cell><cell>6.77%</cell><cell>7</cell><cell>93.77%</cell><cell>2</cell><cell>61.55%</cell><cell>4</cell><cell>4.07%</cell></row><row><cell>WaterlooCormack-stop2399</cell><cell>16.41%</cell><cell>7</cell><cell>8.43%</cell><cell>7</cell><cell>56.65%</cell><cell>3</cell><cell>2.01%</cell><cell>8</cell><cell>32.80%</cell><cell>6</cell><cell>5.01%</cell><cell>8</cell><cell>3.56%</cell><cell>8</cell><cell>44.78%</cell><cell>5</cell><cell>53.56%</cell><cell>6</cell><cell>1.78%</cell></row><row><cell>Webis-baseline</cell><cell>14.77%</cell><cell>8</cell><cell>47.06%</cell><cell>5</cell><cell>24.51%</cell><cell>8</cell><cell>19.31%</cell><cell>4</cell><cell>18.84%</cell><cell>7</cell><cell>27.37%</cell><cell>5</cell><cell>28.16%</cell><cell>4</cell><cell>19.71%</cell><cell>9</cell><cell>65.54%</cell><cell>3</cell><cell>34.59%</cell></row><row><cell>Webis-keyphrase</cell><cell>19.10%</cell><cell>6</cell><cell>6.40%</cell><cell>8</cell><cell cols="2">18.29% 10</cell><cell>10.22%</cell><cell>6</cell><cell>17.98%</cell><cell>8</cell><cell>18.23%</cell><cell>6</cell><cell>16.04%</cell><cell>6</cell><cell cols="2">19.19% 10</cell><cell>32.89%</cell><cell>8</cell><cell>30.08%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="24,126.23,183.12,390.07,157.46"><head></head><label></label><figDesc>N.C. at Chapel Hill, who explained in Information Seeking in Electronic Environments (Cambridge 1995) that information seeking expertise is a critical skill for successful search. Professor Marchionini argues, and we agree, that: "One goal of human-computer interaction research is to apply computing power to amplify and augment these human abilities." We also follow the teachings of UCLA Professor Marcia J. Bates who has advocated for a multimodal approach to search since 1989. Bates, Marcia J.</figDesc><table /><note coords="24,336.81,263.52,129.07,9.86;24,126.23,276.96,371.38,9.86;24,126.23,290.40,281.53,9.86;24,144.23,303.84,372.07,9.86;24,144.23,321.19,326.93,5.95;24,144.23,334.63,314.27,5.95"><p>, The Design of Browsing and Berrypicking Techniques for the Online Search Interface, Online Review 13 (October 1989): 407-424. As Professor Bates explained in 2011 in Quora: "An important thing we learned early on is that successful searching requires what I called "berrypicking."  Berrypicking involves 1) searching many different places/sources, 2) using different search techniques in different places,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="25,90.23,208.83,434.70,494.39"><head></head><label></label><figDesc>[6]  Predictive Coding is defined by The Grossman-Cormack Glossary of Technology-Assisted Review, 2013 Fed. Cts. L. Rev. 7 (January 2013) (Grossman-Cormack Glossary) as: "An industry-specific term generally used to describe a Technology Assisted Review process involving the use of a Machine Learning Algorithm to distinguish Relevant from Non-Relevant Documents, based on Subject Matter Expert(s) Coding of a Training Set of Documents. Henry VI, Pt II, Act 4, Scene 2, 71-78 ("The first thing we do, let's kill all the lawyers."). This famous anti-lawyer line was spoken by "Dick the butcher," a traitor hoping to start a revolution and prop up his friend as an autocratic ruler. Predictive Coding 1.0 and the First Patents, discussing common prejudice against lawyers by academics and IT that drove the ill-advised imposition of secret control sets in the first versions of predictive coding software. The new drive by Cormack and Grossman to fully automate legal search and eliminate SMEs and attorney search expertise from legal search seems based, at least in part, on the same false premises. Also see Losey, R., Mancia v. Mayflower Begins a Pilgrimage to the New World of Cooperation, 10 Sedona Conf. J. 377 (2009 Supp.); Losey, R., Lawyers Behaving Badly, 60 Mercer L. Rev. 983 (Spring 2009). Of course, these different TREC events had varying experiments and test conditions and so direct comparisons between TREC studies are never valid, but general comparisons are instructive and frequently made in the cited literature. [19] See the report on the Electronic Discovery Institute (EDI) Oracle legal search experiments involving the largest number of legal search participants to date where a member of the e-Discovery Team attained high scores. Bay, M., EDI-Oracle Study: Humans Are Still Essential in E-Discovery: Phase I of the study shows that older lawyers still have e-discovery chops and you don't want to turn EDD over to robots (11/20/13, LTN). Monica Bay, the Editor of Law Technology News, summarizes the conclusion of EDI from the study that: "Conclusion: Software is only as good as its operators. Human contribution is the most significant element." Patrick Oot, co-founder of the Electronic Discovery Institute presented the findings of Phase II of the Oracle Predictive Coding Survey at ILTACON Day 3, as reported in The Relativity Blog, 9/2/15: "[W]hen it comes to what some vendors call Continuous Active Learning, Oot indicated the debate was</figDesc><table coords="25,90.23,397.88,434.70,291.90"><row><cell></cell><cell>Continuous Active Learning for Technology-Assisted Review, SIGIR'15, August 09-13,</cell></row><row><cell></cell><cell>2015, Santiago, Chile. (2015).</cell></row><row><cell>[12]</cell><cell>Losey, R., Predictive Coding 3.0, part two (e-Discovery Team, 10/18/15).</cell></row><row><cell cols="2">[13] Shakespeare, W., [14] Losey, R., Predictive Coding 3.0, part one) (2015 e-Discovery Team), see the subsection</cell></row><row><cell>[7] [8]</cell><cell>Da Silva Moore v. Publicis Groupe 868 F. Supp. 2d 137 (SDNY 2012) and numerous cases later citing to and following this landmark decision by Judge Andrew Peck, including Judge Peck's own more recent Rio Tinto v. Vale, 2015 WL 872294 (March 2, 2015, SDNY). Grossman &amp; Cormack, Evaluation of Machine-Learning Protocols for Technology-Assisted Review in Electronic Discovery, SIGIR'14, July 6-11, 2014; Grossman &amp; Cormack, Comments on "The Implications of Rule 26(g) on the Use of Technology-Assisted Review", 7 Federal Courts Law Review 286 (2014); Herbert Roitblat, series of five OrcaTec blog posts (1, 2, 3, 4, 5), May-August 2014; Herbert Roitblat, Daubert, Rule 26(g) and the eDiscovery Turkey OrcaTec blog, August 11th, 2014; Hickman &amp; therein, 347-386.</cell></row><row><cell>[18]</cell><cell>Schieneman, The Implications of Rule 26(g) on the Use of Technology-Assisted Review, Autonomy and Reliability, supra at pgs. 2-3 ("This paper offers a historical review of</cell></row><row><cell></cell><cell>7 FED. CTS. L. REV. 239 (2013); Losey, R. Predictive Coding 3.0, part one (e-Discovery research efforts to achieve high recall ..." The paper also estimates the Blair Maron</cell></row><row><cell></cell><cell>Team 10/11/15). precision score of 20% and lists the top scores (without attribution) in most TREC years);</cell></row><row><cell>[9]</cell><cell>Id.; Webber, Random vs active selection of training examples in e-discovery (Evaluating Hedin, Tomlinson, Baron, and Oard, Overview of the TREC 2009 Legal Track (TREC 2009);</cell></row><row><cell></cell><cell>e-Discovery blog, 7/14/14). Cormack, Grossman, Hedin, and Oard; Overview of the TREC 2010 Legal Track (TREC</cell></row><row><cell>[10]</cell><cell>See Endnote [2]. This disagreement is within a general framework of agreement on the 2010); Grossman, Cormack, Hedin, and Oard, Overview of the TREC 2011 Legal Track</cell></row><row><cell></cell><cell>superiority of computer assisted methods over traditional linear review, joint criticism (TREC 2011); Evaluation of Information Retrieval for E-Discovery, supra at pgs. 24-27.</cell></row><row><cell></cell><cell>of random selection methods and control sets in legal review, and agreement on the use The top TREC results cited for the six years of Legal track are in the 60% to 70% F1 range</cell></row><row><cell></cell><cell>of continuous active learning, as opposed to one and done, identified by Losey as with a couple of results in the low 80% F1 range. The Recommind participation in the</cell></row><row><cell></cell><cell>Predictive Coding Version 1.0. Predictive Coding 3.0, part one (e-Discovery Team last TREC Legal Track 2011, and their subsequent prohibited marketing advertisements</cell></row><row><cell></cell><cell>10/11/15).</cell></row><row><cell>[11]</cell><cell></cell></row></table><note coords="25,182.51,276.92,304.36,9.90;25,126.23,290.36,392.69,9.90;25,126.23,303.80,387.63,9.90;25,126.23,317.24,383.05,9.90;25,126.23,330.68,380.51,9.90;25,126.23,343.23,392.22,10.79;25,126.23,357.56,378.57,9.90;25,126.23,371.00,379.43,9.90;25,126.23,384.44,112.84,9.90;25,126.23,678.99,364.33,10.79;25,126.23,692.43,368.33,10.79;26,90.23,276.71,17.84,10.11;26,126.23,276.71,355.58,10.11;26,126.23,290.15,372.57,10.11;26,126.23,303.59,371.72,10.11;26,126.23,317.03,221.65,10.11;26,90.23,330.47,17.84,10.11;26,126.23,330.47,384.13,10.11;26,126.23,343.91,392.81,10.11;26,126.23,357.35,378.34,10.11;26,126.23,370.79,372.50,10.11;26,126.23,384.23,389.50,10.11;26,126.23,397.67,394.22,10.11;26,126.23,411.11,358.11,10.11;26,126.23,424.55,394.80,10.11;26,126.23,437.99,371.90,10.11;26,126.23,451.43,157.66,10.11;26,90.23,464.87,17.84,10.11;26,126.23,464.87,376.66,10.11;26,126.23,478.07,394.37,10.11;26,126.23,491.51,356.04,10.11;26,126.23,504.95,380.01,10.11;26,126.23,666.23,378.92,10.11;26,126.23,679.67,373.53,10.11;26,126.23,693.11,379.47,10.11;26,126.23,706.55,365.83,10.11;27,126.23,264.66,393.33,8.72;27,126.23,278.10,370.26,8.72;27,126.23,291.54,394.48,8.72;27,126.23,303.95,385.23,9.75;27,126.23,317.39,393.43,9.75;27,126.23,330.83,319.09,9.75;27,90.23,344.27,17.84,9.75"><p><p><p><p><p><p><p>" A Technology Assisted Review process is defined as: "A process for Prioritizing or Coding a Collection of electronic Documents using a computerized system that harnesses human judgments of one or more Subject Matter Expert(s) on a smaller set of Documents and then extrapolates those judgments to the remaining Document Collection.  TAR processes generally incorporate Statistical Models and/or Sampling techniques to guide the process and to measure overall system effectiveness." Also see: Technology-Assisted Review in E-Discovery Can Be More Effective and More Efficient Than Exhaustive Manual Review, Richmond Journal of Law and Technology, Vol. XVII, Issue 3, Article 11 (2011). Grossman &amp; Cormack, Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review, CoRR abs/1504.06868 (2015); Multi-Faceted Recall of [15]</p>See Zero Error Numerics for a partial list of quality control and quality assurance methods endorsed by the e-Discovery Team, found at ZeroErrorNumerics.com</p>(ZEN  Document Review)</p>. Also see: Concept Drift and Consistency: Two Keys to Document Review Quality, e-Discovery Team (Jan. 20, 2016).</p>[16]</p>The cost of traditional linear document review is often far higher than $1.00 per file in practice. In 2007 the U.S. Department of Justice spent $9.09 per document for review in the Fannie Mae case, even though it used contract lawyers for the review work. In re Fannie Mae Securities Litig., 552 F.3d 814, 817 (D.C. Cir. 2009) ($6,000,000/660,000 emails). At about the same time Verizon paid $6.09 per document for a massive second review project that enjoyed large economies of scale and, again, utilized contract review lawyers. Roitblat, Kershaw, and Oot, Document categorization in legal electronic discovery: computer classification vs. manual review. Journal of the American Society for Information Science and Technology, 61(1):70-80, 2010 ($14,000,000 to review 2.3 million documents in four months). [17] E. M. Voorhees, Variations in relevance judgments and the measurement of retrieval Effectiveness, Information Processing &amp; Management, 36(5):697{716, 2000 (on pooling); Oard, Baron, Hedlin, lewis, Tomlinson, Evaluation of Information Retrieval for E-Discovery, Journal Artificial Intelligence and Law, Vol. 18 Issue 4, December 2010 Pgs. claiming to "win," which led to their lifetime ban from TREC, only attained a Recall of 62.3% in one topic (403). Overview of the TREC 2011 Legal Track (TREC 2011) supra. Contrast all of the prior TREC results with the e-Discovery Team results in 18 topics in the 80% to 100% F1 range, with numerous topics in the mid to high 90% F1 range. somewhat of a red herring, adding, "Continuous Active Learning is just a buzzword." Oot summed up his thoughts by stressing the human component of technology-assisted review. Noting that the best performing technology in the Oracle study was the one used by a senior attorney, Oot said, "A good artist with a good brush is best." Unfortunately the final results of the EDI Oracle study have not yet been published and, as participants in that study, we are currently constrained from any detailed reporting.</p>[20]  </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="28,90.23,487.71,395.35,232.63"><head></head><label></label><figDesc>Discovery Team's TREC Total Recall project commenced on July 14, 2015 with work on Topic 103 Manatee Protection. This topic was run by Losey. He did not complete work until July 22, 2015. Although it may seem fast to see a review of 290,099 documents completed by one attorney in only eight days (with no breaks), there was more time spent on this topic than any of the others. But a significant amount of this time was spent on general set-up, procedures, contract reviewer training, project orientation, and communication protocols. Completion of this Topic was also delayed due to the availability of the contract review attorney, Anne Bottolene, who assisted Losey for the first part of the work on Topic 103, and due to some initial software configuration setup issues.</figDesc><table coords="28,122.63,487.71,213.98,177.59"><row><cell>The e-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>@Reas.</cell><cell>@97.5%</cell></row><row><cell></cell><cell>Call</cell><cell>Recall</cell></row><row><cell>True Positives</cell><cell>4,780</cell><cell>5,582</cell></row><row><cell>True Negatives</cell><cell>284,348</cell><cell>283,793</cell></row><row><cell>False Positives</cell><cell>26</cell><cell>581</cell></row><row><cell>False Negatives</cell><cell>945</cell><cell>143</cell></row><row><cell>Recall</cell><cell>83.49%</cell><cell>97.50%</cell></row><row><cell>Precision</cell><cell>99.46%</cell><cell>90.57%</cell></row><row><cell>F1 Measure</cell><cell>90.78%</cell><cell>93.91%</cell></row><row><cell>Accuracy</cell><cell>99.67%</cell><cell>99.75%</cell></row><row><cell>Error</cell><cell>0.33%</cell><cell>0.25%</cell></row><row><cell>Elusion</cell><cell>0.33%</cell><cell>0.05%</cell></row><row><cell>Fallout</cell><cell>0.01%</cell><cell>0.20%</cell></row></table><note coords="28,90.23,693.38,4.02,5.70;28,97.00,692.32,182.47,13.38;28,282.18,697.06,84.90,8.64;28,369.78,692.32,83.41,13.38;28,455.90,697.06,29.68,8.64;28,90.23,711.70,75.11,8.64;28,168.05,706.96,92.38,13.38"><p>1 Grossman &amp; Cormack Glossary, supra FN 1 at pg. 6. The Confusion Matrix is also referred to as a Contingency Table.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="30,90.23,236.73,427.09,291.77"><head></head><label></label><figDesc>In Step Four the software, Mr. EDR, analyzes the documents designated for training in Step Two in the seed set, and in Step Five thereafter. Mr. EDR then ranks the whole dataset according to probable relevance and irrelevance.</figDesc><table coords="30,90.23,303.93,173.45,184.25"><row><cell>In Step Five the attorneys search for</cell></row><row><cell>more documents to use to train Mr.</cell></row><row><cell>EDR. It is essentially the same as Step</cell></row><row><cell>Two, except now the attorneys can</cell></row><row><cell>add probability and rank based</cell></row><row><cell>searches to their multimodal toolkit.</cell></row><row><cell>That is the Team's full search pyramid,</cell></row><row><cell>shown right. The methods are used</cell></row><row><cell>ad hoc according to what the</cell></row><row><cell>attorney reviewer considers a</cell></row><row><cell>promising method to find additional</cell></row><row><cell>relevant documents based in part on</cell></row><row><cell>the latest EDR rankings and TREC</cell></row><row><cell>submission returns. Once new</cell></row></table><note coords="30,90.23,491.85,405.42,9.77;30,90.23,505.29,425.35,9.77;30,90.23,518.73,422.80,9.77"><p>documents are found that are likely to be relevant, they are then designated in Step Six for Training. Not all documents are so designated. Again this is at the discretion of the attorneys as to what documents they think would best serve to train in the ongoing active learning process.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="33,90.23,435.34,283.23,273.16"><head>Topic 2108 CAPTCHA Services Confusion Matrix-Topic 2108</head><label></label><figDesc>Topic 2108 was run by Losey without any assistance of a review lawyer. The work to search the 465,149 BlackHat World Forum posts started on July 16, 2015, but did not conclude until August 1, 2015.</figDesc><table coords="33,90.23,476.82,259.88,231.68"><row><cell>Total Documents: 465,147</cell><cell></cell><cell></cell></row><row><cell>Total Relevant: 656</cell><cell></cell><cell></cell></row><row><cell>Total Prevalence: 0.14%</cell><cell></cell><cell></cell></row><row><cell></cell><cell>@Reas.</cell><cell>@97.5% Recall</cell></row><row><cell></cell><cell>Call</cell><cell></cell></row><row><cell>True Positives</cell><cell>580</cell><cell>640</cell></row><row><cell>True Negatives</cell><cell>463,566</cell><cell>458,906</cell></row><row><cell>False Positives</cell><cell>925</cell><cell>5,585</cell></row><row><cell>False Negatives</cell><cell>76</cell><cell>16</cell></row><row><cell>Recall</cell><cell>88.41%</cell><cell>97.56%</cell></row><row><cell>Precision</cell><cell>38.54%</cell><cell>10.28%</cell></row><row><cell>F1 Measure</cell><cell>53.68%</cell><cell>18.60%</cell></row><row><cell>Accuracy</cell><cell>99.78%</cell><cell>98.80%</cell></row><row><cell>Error</cell><cell>0.22%</cell><cell>1.20%</cell></row><row><cell>Elusion</cell><cell>0.02%</cell><cell>0.00%</cell></row><row><cell>Fallout</cell><cell>0.20%</cell><cell>1.20%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="36,90.23,425.26,278.01,273.16"><head></head><label></label><figDesc>Topic 101 was run by Losey with the assistance of a review attorney, David Jensen. The work to search the 290,099 Bush Emails started on July 16, 2015 and concluded on July 26, 2015. The project commenced with Losey beginning Step Two, Multimodal Search Reviews, and Jensen assigned Step Three, Random Baseline. Jensen finished the random sample review the next day and began assisting Losey in Step Two, and after submissions began, the echo Step Five, multimodal. Losey handled all of the AI related searches in Step Five, including the probability and ranking related searches. Jensen focused on keyword searches and also made suggestions of documents to submit. Final decisions on submissions were always made by Losey on all Topics.</figDesc><table coords="36,90.23,425.26,278.01,273.16"><row><cell></cell><cell cols="2">Topic 101 Judicial Selection</cell></row><row><cell>Confusion Matrix-Topic 101</cell><cell></cell><cell></cell></row><row><cell>Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell>Total Relevant: 5,834</cell><cell></cell><cell></cell></row><row><cell>Total Prevalence: 2.01%</cell><cell></cell><cell></cell></row><row><cell></cell><cell>@Reas.</cell><cell>@97.5%</cell></row><row><cell></cell><cell>Call</cell><cell>Recall</cell></row><row><cell>True Positives</cell><cell>5,026</cell><cell>5,688</cell></row><row><cell>True Negatives</cell><cell>283,608</cell><cell>281,901</cell></row><row><cell>False Positives</cell><cell>657</cell><cell>2,364</cell></row><row><cell>False Negatives</cell><cell>808</cell><cell>146</cell></row><row><cell>Recall</cell><cell>86.15%</cell><cell>97.50%</cell></row><row><cell>Precision</cell><cell>88.44%</cell><cell>70.64%</cell></row><row><cell>F1 Measure</cell><cell>87.28%</cell><cell>81.93%</cell></row><row><cell>Accuracy</cell><cell>99.49%</cell><cell>99.13%</cell></row><row><cell>Error</cell><cell>0.51%</cell><cell>0.87%</cell></row><row><cell>Elusion</cell><cell>0.28%</cell><cell>0.05%</cell></row><row><cell>Fallout</cell><cell>0.23%</cell><cell>0.83%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="49,90.23,86.38,430.98,549.16"><head></head><label></label><figDesc>run by Losey who started the search of 290,099 Black Hat Forum posts on July 21, 2015 and concluded on August 18, 2015. Losey put aside work on this Topic several times while he gave priority to the Jeb Bush Email Topics. The project commenced as usual with Losey beginning Step Two, Multimodal Search Reviews.Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal search began, including predictive coding features, with iterated training.On August, 2015, after making 12 submissions to TREC, and training after almost every submission, Losey had provided a total 201 documents to TREC and confirmed a total of 163 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 205 documents. After the 12 th TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 89.56% had been attained with a Precision of 81%. There were 23 additional submissions to TREC after the Reasonable call point. A 90% Recall was attained after submitting only 212 documents. A 95% Recall was attained after submitting 891 documents, and 97.5% Recall attained after 3,188 documents. Total Recall was attained after submitting 12,109 documents out of the corpus total of 465,147.A graph mapping how the review was conducted appears below, with the light green line signifying the anticipated 70% Recall call, and the dark green line the Reasonable Recall call.</figDesc><table coords="49,90.23,86.38,260.84,307.48"><row><cell></cell><cell>Topic 2225 Rootkits</cell></row><row><cell>Confusion Matrix-Topic 2225</cell><cell></cell></row><row><cell>Total Documents: 465,147</cell><cell></cell></row><row><cell>Total Relevant: 182</cell><cell></cell></row><row><cell>Total Prevalence: 0.04%</cell><cell></cell></row><row><cell>Call 163 464,927 38 19 89.56% 81.09% 85.11% 99.99% 0.01% 0.00% Topic 2225 was @Reas. True Positives True Negatives False Positives False Negatives Recall Precision F1 Measure Accuracy Error Elusion Fallout 0.01%</cell><cell>@97.5% Recall 178 461,955 3,010 4 97.80% 5.58% 10.56% 99.35% 0.65% 0.00% 0.65%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="51,90.23,416.86,283.63,273.16"><head>Topic 102 Capital Punishment Confusion Matrix-Topic 102 Capital Punishment</head><label></label><figDesc>Topic 102 was run by Losey with the assistance of a review attorney, Jensen. The work to search the 290,099 Bush Emails started on July 26, 2015 and concluded on July 29, 2015. The project commenced with Losey and his assistant beginning Step Two, Multimodal Search Reviews.</figDesc><table coords="51,90.23,457.18,222.70,232.84"><row><cell>Total Documents: 290,099</cell><cell></cell><cell></cell></row><row><cell>Total Relevant: 1,624</cell><cell></cell><cell></cell></row><row><cell>Total Prevalence: 0.56%</cell><cell></cell><cell></cell></row><row><cell></cell><cell>@Reas.</cell><cell>@97.5%</cell></row><row><cell></cell><cell>Call</cell><cell>Recall</cell></row><row><cell>True Positives</cell><cell>941</cell><cell>1,583</cell></row><row><cell>True Negatives</cell><cell>288,345</cell><cell>17,048</cell></row><row><cell>False Positives</cell><cell>130</cell><cell>271,427</cell></row><row><cell>False Negatives</cell><cell>683</cell><cell>41</cell></row><row><cell>Recall</cell><cell>57.94%</cell><cell>97.50%</cell></row><row><cell>Precision</cell><cell>87.86%</cell><cell>0.58%</cell></row><row><cell>F1 Measure</cell><cell>69.83%</cell><cell>1.15%</cell></row><row><cell>Accuracy</cell><cell>99.72%</cell><cell>6.42%</cell></row><row><cell>Error</cell><cell>0.28%</cell><cell>93.58%</cell></row><row><cell>Elusion</cell><cell>0.24%</cell><cell>0.24%</cell></row><row><cell>Fallout</cell><cell>0.05%</cell><cell>94.09%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="57,90.23,75.61,428.94,36.73"><head></head><label></label><figDesc>Topic 105 was run by Losey with the assistance of a review attorney, Jensen. The work to search the 290,099 Bush Emails started on July 29, 2015 and concluded on July 31, 2015. The project commenced with Losey and his assistant beginning Step Two, Multimodal Search Reviews.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" coords="78,90.23,169.40,430.92,547.26"><head></head><label></label><figDesc>These included an AP photographer, the President of Gambia, and protesters in Yemen with first</figDesc><table coords="78,122.63,169.40,191.57,177.18"><row><cell></cell><cell>@Reas.</cell><cell>@97.5%</cell></row><row><cell></cell><cell>Call</cell><cell>Recall</cell></row><row><cell>True Positives</cell><cell>1,107</cell><cell>1,084</cell></row><row><cell>True Negatives</cell><cell>901,309</cell><cell>901,311</cell></row><row><cell>False Positives</cell><cell>14</cell><cell>12</cell></row><row><cell>False Negatives</cell><cell>4</cell><cell>27</cell></row><row><cell>Recall</cell><cell>99.64%</cell><cell>97.57%</cell></row><row><cell>Precision</cell><cell>98.75%</cell><cell>98.91%</cell></row><row><cell>F1 Measure</cell><cell>99.19%</cell><cell>98.23%</cell></row><row><cell>Accuracy</cell><cell>100.00%</cell><cell>100.00%</cell></row><row><cell>Error</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>Elusion</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>Fallout</cell><cell>0.00%</cell><cell>0.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25" coords="84,90.23,74.56,430.44,642.10"><head></head><label></label><figDesc>Topic 3089 was run by Joe White. Work on this topic commenced on August 5, 2015 and concluded on August 28, 2015. Approximately 24 hours were spent on this topic, including a few hours up front researching the subject matter. This served as a proxy for the e-Discovery Team Hybrid Multimodal Model, Step 1, ESI Discovery Communications. Completion of this Topic was drawn out due to time conflicts including vacation.The collection of 902,434 News Articles were generally easier to search than the Bush Emails or Black Hat World Forum posts, though the news articles contained many links, footers and subject matters that were shared with other news stories, creating the appearance of similarity. As would be expected with news articles, misspelled words and names seemed nonexistent, which was helpful. White did, however, find a few gold-standard inconsistencies in this topic.White beganStep Two, multimodal search, by creating several keyword lists based on his judgment and notes from the initial topic research. This research included events, names, locations, and other information related to the case. The keyword list goals were to: (a) to create a seed set to begin finding the potentially relevant documents and to begin training Mr. EDR; (b) to guesstimate how large the relevant document set would be a kind of rough substitute for Step Three Sample; and (c) to highlight relevant terms in the software to facilitate more effective review and training. (Note -all reviewers so highlighted certain keywords as a matter of course to speed up and improve review.)When the initial keywords brought back only just over 220-some documents, while still cognizant of the limitations of keyword search, White believed this meant a relatively small potential data set existed. This afforded him the ability to perform a linear review of all of the keyword hits, but also meant that precision would be easily harmed by false positives. For that reason White knew that care would be needed in ascertaining true relevance. A normal Step 3, Random Baseline sample, was omitted given the likely low prevalence and general time constraints for the work.Based on the initial judgmental sample reviews in Step Two, White submitted initial sets of documents to TREC to establish relevance boundaries and begin whittling down on the set of relevant candidate documents. A minor loss of precision was anticipated on certain documents in exchange for knowledge that would guide subsequent submissions. Each time documents were determined to be relevant, White updated the training and predictive ranking, to facilitate priority-driven review that augmented the judgmental sampling work (see steps Four, Five and Six: AI PredictiveRanking, Multimodal Search Review &amp; Hybrid Active Training). He also utilized conceptual search (predominantly Find Similar, via LSI) to branch off particularly interesting or novel documents to learn more. Although White, like all of the reviewers, did use concept search, and similarity search, he found that the predictive coding rankings (using a more robust technology) proved to be more effective overall. All reviewers had the same experience.</figDesc><table coords="84,90.23,74.56,279.71,272.02"><row><cell>initial</cell><cell cols="2">Topic 3089 Pickton Murders</cell></row><row><cell cols="2">Confusion Matrix-Topic 3089</cell><cell></cell></row><row><cell>Total Documents: 902,434</cell><cell></cell><cell></cell></row><row><cell>Total Relevant: 255</cell><cell></cell><cell></cell></row><row><cell>Total Prevalence: 0.03%</cell><cell></cell><cell></cell></row><row><cell></cell><cell>@Reas.</cell><cell>@97.5%</cell></row><row><cell></cell><cell>Call</cell><cell>Recall</cell></row><row><cell>True Positives</cell><cell>236</cell><cell>249</cell></row><row><cell>True Negatives</cell><cell>902,164</cell><cell>901,971</cell></row><row><cell>False Positives</cell><cell>15</cell><cell>208</cell></row><row><cell>False Negatives</cell><cell>19</cell><cell>6</cell></row><row><cell>Recall</cell><cell>92.55%</cell><cell>97.65%</cell></row><row><cell>Precision</cell><cell>94.02%</cell><cell>54.49%</cell></row><row><cell>F1 Measure</cell><cell>93.28%</cell><cell>69.94%</cell></row><row><cell>Accuracy</cell><cell>100.00%</cell><cell>99.98%</cell></row><row><cell>Error</cell><cell>0.00%</cell><cell>0.02%</cell></row><row><cell>Elusion</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>Fallout</cell><cell>0.00%</cell><cell>0.02%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26" coords="86,90.23,491.79,422.09,117.35"><head></head><label></label><figDesc>On August 28, 2015, after making 19 submissions to TREC providing a total 251 documents, White had found a total of 236 relevant documents. The effort, or number of documents reviewed and coded by White to attain this result, was 834 documents. After the 18 th TREC submission, White decided to call Reasonable. It was later determined that a Recall of 92.55% had been attained, with Precision of 94.02%. There were 37 additional submissions to TREC after the Reasonable call point. After submitting a total of 462 documents, which is only 0.</figDesc><table coords="86,90.23,558.99,422.09,50.15"><row><cell>05%</cell></row><row><cell>of the total 902,434 documents, and reviewing only 834 documents, a 99.61% Recall level was</cell></row><row><cell>attained with 54.98% Precision. 100% Recall with 12.75% Precision was attained after</cell></row><row><cell>submission of 2,000 documents, which is 0.22% of the total.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27" coords="94,90.23,75.61,429.55,278.41"><head></head><label></label><figDesc>Topic 2333 was run by Losey who also started on August 19, 2015. He finished his review of 465,149 forum posts in BlackHat World on August 23, 2015. The project commenced as usual with Losey beginning Step Two, Multimodal Search Reviews. Step Three, Random Baseline, was omitted. After submissions began, the echo Step Five, multimodal search began, including predictive coding features, with iterated training.On August 21, 2015, after making 23 submissions to TREC, and training after almost every submission, Losey had provided a total of 6,666 documents to TREC and confirmed a total of 4201 relevant documents. The effort, or number of documents reviewed and coded by Losey to attain this result, was 228 documents. After the 23 rd TREC submission, Losey decided to call Reasonable. It was later determined that a Recall of 87.43% was attained by submission of only 6,666 documents, which is .043% of the total 465,147 documents. This was accomplished by personal review of only 228 documents, 0.05% of the total collection.</figDesc><table /><note coords="94,90.23,261.34,429.55,12.04;94,90.23,276.97,428.08,9.85;94,90.23,290.41,428.33,9.85;94,90.23,301.66,413.79,12.04;94,90.23,317.29,409.48,9.85;94,90.23,330.73,399.01,9.85;94,90.23,344.17,349.96,9.85"><p>There were 32 additional submissions to TREC after the Reasonable call point. Recall of 90% was attained after submitting after submitting 7,091 documents, and 95% Recall after 10,931. Recall of 98% Recall was reached after submitting 14,698 documents, which was only 3.22% of total of 456,147 collection of BlackHat World Forum posts. Again, this was accomplished by personal review of only 228 documents, 0.05% of the total collection. In all topics we always stopped individual document review after the Reasonable call and relied on Mr. Robots automatic processes wherein the documents were submitted in order of highest ranking.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28" coords="96,90.23,73.42,397.03,334.36"><head>Topic 2129 Facebook Accounts Confusion Matrix-Topic 2129</head><label></label><figDesc></figDesc><table coords="96,90.23,131.52,397.03,276.26"><row><cell>Total Documents: 465,147</cell></row><row><cell>Total Relevant: 589</cell></row><row><cell>Total Prevalence: 0.13%</cell></row><row><cell>Topic 2129 was run by Sullivan who started on August 21, 2015. He finished his review of</cell></row><row><cell>465,149 forum posts in BlackHat World on August 22, 2015.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31" coords="114,90.23,73.42,417.37,320.92"><head>Topic 3226 Traffic Enforcement Cameras Confusion Matrix-Topic 3226</head><label></label><figDesc>Total Documents: 902,434 Total Relevant: 2,094 Total Prevalence: 0.23% Topic 3226 was run by Sullivan who also started on August 27, 2015. He finished his review of 902,434 News Articles on August 28, 2015.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sullivan had no background or knowledge of fracking prior to this exercise. While expert knowledge was not necessary, there were a few instances where some additional knowledge of the topic would have been helpful.</p><p>Sullivan had previously tackled topics in the forums data set, but this was his first topic in the News data set. He found the lack of spelling issues and overall consistency in the documents provided a much easier set of data to review. Much less manual review was necessary with the news topics.</p><p>On the first day, Sullivan used concept searching to identify similar topics, per his standard process. He created a list of most likely relevant keywords and used the list for searching and keyword highlighting. Both search and keyword highlighting lists were modified through the course of the review as new information was obtained.</p><p>Sullivan decided to go with a different approach to this topic. Rather than performing a manual review of documents to begin, he decided to submit as relevant any document that contained over 5 instances of the term "fracking" without review. 286 documents met this standard, and all were returned as relevant when submitted to TREC.</p><p>While the data used for this exercise did not contain any metadata, Sullivan determined any text that appeared in the first 2 lines of the document could be considered the document's title. He found 61 documents that contained "fracking" in the title and an additional instance of fracking elsewhere in the document. The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Paypal Accounts topic, by the time 97.5% Recall had been attained only 4.73% of the corpus, 22,005 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 95.27% or 443,142 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% recall using the multi-modal hybrid model of training EDR. The last chart below represents the amount of effort in terms of documents reviewed to attain 100% recall using the multi-modal hybrid model of training EDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>______________________________________</head><p>______________________________________ signifying the anticipated 70% recall call, and the dark green line the reasonable recall call.</p><p>The following chart shows Precision (left and blue line), F1 (red) and percent of documents submitted (green) as tracked across varying recall thresholds. On the Traffic Enforcement Cameras topic, by the time 97.5% Recall had been attained only 0.29% of the corpus, 2,575 documents, had been submitted for adjudication. The last portion of the graph thus represents the submission of the remaining 99.71% or 899,859 documents.</p><p>The last chart below represents the amount of effort in terms of documents reviewed to attain 100% recall using the multi-modal hybrid model of training EDR.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="95,109.33,328.54,404.34,12.04;95,90.23,341.98,366.18,12.04;95,228.78,657.12,154.86,11.51" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="95,109.33,328.54,404.34,12.04;95,90.23,341.98,337.49,12.04">last chart below represents the amount of effort in terms of documents reviewed to attain 100% Recall using the multimodal hybrid model of search and training of Mr</title>
		<idno>EDR. ______________________________________</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
