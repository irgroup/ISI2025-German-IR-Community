<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,94.90,112.05,405.48,15.12">WaterlooClarke: TREC 2015 Contextual Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-11">November 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,172.54,144.53,80.31,10.48"><forename type="first">Hella</forename><surname>Hoffmann</surname></persName>
							<email>hrhoffmann@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.28,144.53,82.75,10.48"><forename type="first">Pragnya</forename><surname>Addala</surname></persName>
							<email>paddala@uwaterloo.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.69,192.23,105.90,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<email>claclark@uwaterloo.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,94.90,112.05,405.48,15.12">WaterlooClarke: TREC 2015 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-11">November 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">175851768FE7BBAC2DDD5BD4288D72A5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this work we present a first attempt at developing a live system to solve the problem presented in the TREC 2015 contextual suggestion task <ref type="foot" coords="1,243.35,316.81,3.97,6.12" target="#foot_0">1</ref> . The goal of this task is to tailor point-of-interest suggestions to users according to their preferences <ref type="bibr" coords="1,243.23,330.33,9.96,8.74" target="#b2">[3]</ref>. We present how we gathered data for the candidate pointsof-interest, filtered some of the candidates and built a live system to return suggestions that would most likely interest the specific user.</p><p>As part of TREC 2015, the contextual suggestion track is running for the fourth time <ref type="bibr" coords="1,487.59,378.16,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,502.94,378.16,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,515.52,378.16,7.75,8.74" target="#b1">2]</ref> and this is the first time that the participants were required to build a live suggestion system. The general idea is to be able to make real-time suggestions for a particular person (based upon their profile) with a particular context. Unlike the previous years where the participants were asked to build their own candidate list of suggestions, this year the organizers themselves released a fixed set of candidate suggestions for 272 contexts, each context representing a city in the United States. Participants were required to set up a server that could listen to requests from users and respond with relevant suggestions. For each request, which is a profile-context pairing, a ranked list of up to 50 ranked suggestions was to be returned.</p><p>• Input: Context including a location id i, latitude and longitude of the location associated with i, a user profile with a list of user preferences P(u) for a user u and additional information such as trip length and time of travel.</p><p>• Wanted: A ranked list of up to 50 suggested attractions to visit during the stay.</p><p>User profiles that are part of the input resquest correspond to the stated preferences of real individuals, primarily recruited through crowdsourcing sites, who also judge the returned suggestions. These users are asked to give a rating to the suggested points of interests. A rating could be any of the following: strongly interested (4), interested (3), neither interested nor uninterested (2), uninterested (1), strongly uninterested (0) or no rating (-1). P@5 is the main metric used for evaluation of the systems <ref type="bibr" coords="1,481.40,602.67,9.96,8.74" target="#b0">[1]</ref>. Mean reciprocal rank and modified version of time-based gain are other metrics used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setting Up the Server</head><p>To receive requests from users and to respond to these requests, a simple server was set up in python. To ensure that we could make changes to the server at any point during the live experiment without losing any requests we had two servers running with two different run IDs -WaterlooRunA and WaterlooRunB. However, the two runs had exactly similar scripts and the second one was created only for backup and for updating servers at any point if necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gathering Data for Candidate Suggestions</head><p>The track organizers had released a total collection of approximately 1.2 million candidate points of interests for all the 272 context cities put together. Each candidate point of interest had an attraction ID, a context city ID, a title, and a URL. Collection of information for each of these candidate points of interest and techniques used for filtering out some of these candidates are discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Crawling Provided URLs</head><p>The home page of an attraction is expected to contain content that is highly relevant to the attraction. Further more, a large number of URLs from the provided collection were from standard tourist APIs such as Yelp<ref type="foot" coords="2,127.30,201.46,3.97,6.12" target="#foot_1">2</ref> , Foursquare<ref type="foot" coords="2,186.75,201.46,3.97,6.12" target="#foot_2">3</ref> and Tripadvisor <ref type="foot" coords="2,265.06,201.46,3.97,6.12" target="#foot_3">4</ref> . In such cases in particular, all the relevant information about the attraction is found at the given URL and any information on linked pages may not be relevant at all. Thus, we only gather content directly from the webpage pointed to by the provided URLs without downloading any content from linked pages. This downloaded content was used for extracting some key information (see Section 3.2) and also for filtering out some of the candidate suggestions (see Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Crawling Yelp, Foursquare and Tripadvisor</head><p>The majority of the URLs in the candidate suggestions belonged to either of these sources -Yelp, Foursquare or Tripadvisor. Web pages retrieved from these sources contain valuable information about each attraction such as a list of category, average user rating, tips, reviews etc. For each of the candidate attractions, we parsed the downloaded content to extract a set of category tags C a and an average user rating r a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Additional Crawls</head><p>For those points of interests whose URLs were not from sources such as Yelp, Foursquare or Tripadvisor, we extracted relevant Foursquare URLs by searching against Bing<ref type="foot" coords="2,370.04,413.57,3.97,6.12" target="#foot_4">5</ref> . The key phrase for this search included the title of the attraction followed by the keyword "Foursquare". Once the relevant Foursquare pages were retrieved, category tags C a and average user rating r a for each of the attractions were extracted (similar to Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Candidate Suggestions Filtering</head><p>Prior to the start of the live session, participants of the track were allowed to register their system for a test period to ensure that their system was working as required and to get some reliable feedback from the users to improve the design of the suggestion system if desired. From the responses that we received during the testing period, we had observed that a small number of the suggestions made by our system were rated -1. These suggestions mainly involved points of interests whose associated URLs were in Spanish, French and other non-English languages. These URLs and consequently the related points of interest were filtered out by using the python language detection package -langdetect<ref type="foot" coords="2,458.22,567.44,3.97,6.12" target="#foot_5">6</ref> . Downloaded content from the URLs was used for this language detection, helping retain only those with English content.</p><p>The collection also included several duplicates. These were filtered out by searching for duplicate attraction titles and also duplicate URLs within the same context city.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Contextual Suggestions</head><p>Based on the information collected for each of the possible location IDs, the task requires us to construct a ranked list of attractions. For this, we designed a scoring function to quantify the likelihood that a specific user would rate a specific attraction highly and then ranked the candidates accordingly. We define our ranking in Section 4.1 and describe its offline and online computation components in Sections 4.2 and 4.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ranking Definition</head><p>To compute a suitable ranking of the attractions for the suggestion, we use the following three pieces of information:</p><p>• the user's preferences P(u) given in the request, as well as</p><p>• the average online rating r a and</p><p>• the set of category tags C a associated with each attraction.</p><p>The ranking process consists of following two steps which we explain in more detail in the following subsections.</p><p>1. Construct a user profile for u from the list his/her preferences P(u).</p><p>2. Use the profile to compute a scoring function σ(u, a, t) hat quantifies the likelihood of user u would rate a specific attraction highly and use it to construct a candidate ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">User Profile</head><p>Recall that every request contains user preferences in form of ratings (-1,0,1,2,3,4) for a subset of A.</p><p>For each incoming request, we divide the set of all rated attractions into those like (rating 3 or 4) and disliked (rating less than 2) by the user and store their ids as A + (u) and A -(u), respectively. We define C + (u) (C -(u)) as the set of all categories included in at least one of the attractions liked (disliked) by u. In other words:</p><formula xml:id="formula_0" coords="3,251.36,434.81,76.19,22.68">C + (u) := a∈A + (u)</formula><p>C a .</p><p>Liked categories C + (u) and disliked C -(u) together build the user profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Scoring Function and Ranking</head><p>We define σ(u, a, a) s the number of category tags that are both considered to be descriptive of attraction a and liked by user u, i.e.:</p><formula xml:id="formula_1" coords="3,248.18,530.43,98.92,11.72">σ(u, a) = |C + (u) ∩ C a |</formula><p>The output ranking consists of the top 50 elements in the list resulting from sorting all candidate attractions A i for location id i lexicographically by (σ(u, a), r a ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Offline Computation: Data Preparation</head><p>The contextual suggestion task required us to respond to a request within 60 seconds. Due to this time constraint, we chose to implement a very simple ranking procedure and pre-processed the collected data sets as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Fast Information Access</head><p>In order to have quick access to the information required for the ranking, we created the following two dictionaries and stored them in json format:</p><p>1. A dictionary containing all category tags C a for each attraction a in A indexed by its id, and 2. a dictionary containing the average online user rating r a for each attraction a in A indexed by its id.</p><p>• One could also use additional context information such as the season in which the request is made to pre-filter the candidate lists. For example, a ski resort should only be suggested for a trip that takes place in the winter. Since this filtering requires the availability of additional information about when each attraction is available, we decided to leave this step as future work. Note however, that a subset of the data we crawled already includes this kind of information. We suggest incorporating these facts into the offline candidate selection step where possible.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,735.13,426.41,6.64"><p>https://sites.google.com/site/treccontext/trec-2015/trec-2015-contextual-suggestion-track-guidelines</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,87.24,648.29,50.81,6.64"><p>www.yelp.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,87.24,657.80,76.21,6.64"><p>www.foursquare.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,87.24,667.30,80.45,6.64"><p>www.tripadvisor.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,87.24,676.81,88.92,6.64"><p>https://www.bing.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="2,87.24,686.31,165.13,6.64"><p>https://pypi.python.org/pypi/langdetect</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Candidate Selection</head><p>The list of all possible attraction ids to choose from for the suggestions contains over 1.2 Million elements. As a result, online computation of the above defined score and ranking of all possible candidates was intractable. Instead, for every possible location id i, we pre-compiled the list A i of all attractions associated with location i and restricted our attraction suggestions to this list. In addition, we presorted the attractions a in A i by their average rating r a and stored the top 100 attractions in a sorted list called Ranked Cand(i).</p><p>Note that Ranked Cand(i) can be used as a default ranking for the suggestion. In fact, if the list of user prefences P(u) is empty, the desired ranking contains exactly the top 50 elements of Ranked Cand(i) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Online Computation: Personalized Ranking</head><p>The part of the ranking that cannot be computed in a pre-processing is the personalization component. When receiving a request including location id i and user id u, we computed the output ranking as follows.</p><p>1. We used the category dictionaries to Compute C + (u) for user u.</p><p>2. We computed σ(u, a) as defined above for all candidates in A i .</p><p>3. We sorted the pre-ranked set of candidates Ranked Cand(i) by σ(u, a), which gave use the desired lexicographical ordering.</p><p>4. Output the top 50 elements in the resulting ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ideas for the Future</head><p>In the previous sections we described the components implemented by our contextual suggestion system. We believe that the proposed ranking strategy suffices to produce adequate personalized recommendations in an online setting. However, there are many ways to improve the system. In the following list we propose possible improvements, which could be explored and incorporated in future work.</p><p>• In addition to the specific user profiles built for an online request, one could compute average user profiles based on data available from previously completed TREC runs. These profiles can be used to estimate the likelihood that an average person likes an attraction associated with a specific category. We recommend taking this likelihood into account when constructing contextual suggestions. As an example, consider the observation that most Americans like going to American Restaurants. Even if we do not know much about a specific user for which we want to make a suggestions (i.e. the user's preference list is sparse), then suggesting an American Restaurant is still likely going to be a good choice. On the other hand, if the categories associated with a certain attraction a are not very popular among an average user, then we should include a in the list of suggestions for user u only if the user's preferences reflect a liking for these categories. For instance, recommending a tatoo parlour to an average person might not be a good idea even if the tatoo parlour received exceptionally high online ratings.</p><p>• Another way to improve the ranking results could be to refine the primary scoring function σ(u, a). So far we only compute the size of the overlap (#categories) between the categories liked by the user u and the categories associated with attraction a. Instead, one could explore the effectiveness of the following ranking method:</p><p>1. For each category c, let (c, u) be the number of times that user u liked an attraction associated with c in their preferences, i.e. (c, u) := |{a ∈ P + (u) : c ∈ C a }|. Note that there are many similar variants of this method that also take the frequency of category overlap into account, which might be worth exploring.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="5,87.50,189.70,391.41,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,332.26,189.70,142.31,8.74">Evaluating contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.50,209.63,435.77,8.74;5,87.50,221.58,435.78,8.74;5,87.50,233.54,435.78,8.74;5,87.50,245.49,435.78,8.74;5,87.50,257.45,22.69,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,421.77,209.63,101.50,8.74;5,87.50,221.58,140.07,8.74">Overview of the TREC 2012 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,442.10,221.58,81.17,8.74;5,87.50,233.54,172.39,8.74;5,148.77,245.49,119.25,8.74">Proceedings of The Twenty-First Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>The Twenty-First Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">November 6-9, 2012. 2012</date>
			<biblScope unit="volume">2012</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
	<note>Special Publication 500-298</note>
</biblStruct>

<biblStruct coords="5,87.50,277.37,435.77,8.74;5,87.50,289.33,435.78,8.74;5,87.50,301.28,435.78,8.74;5,87.50,313.24,432.82,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,421.77,277.37,101.50,8.74;5,87.50,289.33,138.75,8.74">Overview of the TREC 2014 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,404.40,289.33,118.88,8.74;5,87.50,301.28,138.55,8.74">Proceedings of The Twenty-Third Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>The Twenty-Third Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11-19">2014. November 19-21, 2014. 2014</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.50,333.16,435.78,8.74;5,87.50,345.12,435.78,8.74;5,87.50,357.07,435.77,8.74;5,87.50,369.03,435.78,8.74;5,87.50,380.98,22.69,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,471.17,333.16,52.10,8.74;5,87.50,345.12,189.61,8.74">Overview of the TREC 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,402.53,345.12,120.74,8.74;5,87.50,357.07,150.08,8.74">Proceedings of The Twenty-Second Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<meeting>The Twenty-Second Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-11-19">2013. November 19-22, 2013. 2013</date>
			<biblScope unit="volume">Special Publication</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
