<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.34,184.44,360.93,4.34;1,205.93,206.36,199.81,4.34">E cient semantic indexing via neural networks with dynamic supervised feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,272.13,236.68,67.45,3.02"><forename type="first">Vivek</forename><surname>Dhand</surname></persName>
							<email>vivek.dhand@ccri.com</email>
							<affiliation key="aff0">
								<orgName type="department">Commwealth Computer Research</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.34,184.44,360.93,4.34;1,205.93,206.36,199.81,4.34">E cient semantic indexing via neural networks with dynamic supervised feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1B605340079049F9B420D1CFC66F714</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a portable system for e cient semantic indexing of documents via neural networks with dynamic supervised feedback. We initially represent each document as a modified TF-IDF sparse vector and then apply a learned mapping to a compact embedding space. This mapping is produced by a shallow neural network which learns a latent representation for the textual graph linking words to nearby contexts. The resulting document embeddings provide significantly better semantic representation, partly because they incorporate information about synonyms. Query topics are uniformly represented in the same manner as documents.</p><p>For each query, we dynamically train an additional hidden layer which modifies the embedding space in response to relevance judgements. The system was tested using the documents and topics provided in the Total Recall track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We present a dynamic neural-network based system for portable semantic indexing of text documents to aid in technology-assisted review. Our starting point is the TF-IDF statistic, which is widely used in information retrieval to score the words in a document in terms of relevance and distinctiveness. These scores are then used to represent each document as a sparse vector. By interpreting TF-IDF in terms of graph theory, we are led to incorporate a global statistic for ranking the importance of words, and we modify the sparse document vectors accordingly. We then apply a neural network learning algorithm to represent words as dense vectors in a relatively low-dimensional semantic embedding space. As a result, any block of text can be represented as a sparse vector and then passed through the projection mapping to embedding space. Note that we do not make use of any external language resources or domain-specific knowledge during this process.</p><p>Given an query topic, we use semantic search within embedding space to construct a seed set of documents for review. Supervised feedback in the form of relevance judgements is used to train an additional lightweight neural network. Any subsequent searches are performed inside the expanded embedding space corresponding to the hidden layer of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph theoretic statistics: TF-IDF and LF-IDF</head><p>Let G = (V, E) be a bipartite graph with vertex set V = X tY and edge set E ⇢ X ⇥Y . Let f : E ! R &gt;0 be a function assigning positive real weights to the edges of G. We define several statistics associated to the pair (G, f ). Note that these functions are asymmetric in X and Y , so we only present one-sided definitions for simplicity. Also, for ease of notation we write x ⇠ y when (x, y) 2 E. Given x 2 X, the inverse document frequency of x is defined as:</p><formula xml:id="formula_0" coords="2,248.73,362.61,105.02,33.08">IDF(x) = log ✓ |Y | deg(x)</formula><p>◆ and the global frequency of x is defined as:</p><formula xml:id="formula_1" coords="2,257.14,411.87,97.44,32.39">GF(x) = X x⇠y f (x, y).</formula><p>Given y 2 Y , the maximum weight of y is defined to be:</p><formula xml:id="formula_2" coords="2,259.50,469.57,92.70,16.72">M(y) = max x⇠y f (x, y)</formula><p>Given an edge (x, y) 2 E, we define the term frequency of x relative to y to be:</p><formula xml:id="formula_3" coords="2,247.68,512.21,116.35,10.91">TF(x, y) = f (x, y)/M(y)</formula><p>The well-known term frequency -inverse document frequency statistic is defined as:</p><formula xml:id="formula_4" coords="2,226.07,550.04,159.57,18.65">TFIDF(x, y) = TF(x, y) • IDF(x).</formula><p>The TFIDF statistic can be thought of as providing new weights on the graph G which better express the importance of various edges. Since TFIDF depends on the local statistic TF, it is natural to define a global version which involves summing over the weights of edges incident on a vertex x. In this way, we obtain a global version of TFIDF, which we call log frequency -inverse document frequency:</p><formula xml:id="formula_5" coords="2,216.17,642.55,179.38,18.17">LFIDF(x) = log(1 + GF(x)) • IDF(x).</formula><p>In this paper, we use the product of TFIDF and LFIDF to rank edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Example: words and documents.</head><p>Let Y be a document corpus and let X denote the set of words contained in the corpus. If a word x 2 X is contained in a document y 2 Y , we add the edge (x, y) and set f (x, y) to be the frequency of x in y. In this case, the TFIDF function corresponds to the standard TF-IDF statistic, which yields a surprisingly good baseline for semantic indexing of text documents. However, it is possible for a globally rare word to have an artificially high TF-IDF value in a given document relative to the theme of the document. Working on the assumption that the the thematically important words in a document will be shared across documents, we augment the TFIDF function by multiplying it by LFIDF. This modification partially remedies this problem by giving a boost to words that appear in a relatively small number of documents but with relatively high global frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Example: bigrams.</head><p>Let X = Y be the set of words contained in a document corpus. For any bigram (x, y) which appears in the corpus text m times, we add an edge (x, y) with weight m. In this case, the value T F IDF (x, y) • LF IDF (x) ranks the words y by their a nity for appearing immediately after x in the text. We can also reverse the roles of X and Y , and thereby rank the words x by their a nity for appearing immediately before y in the text. By multiplying these values together, we obtain a symmetric function which expresses the internal a nity for each bigram (x, y). The resulting scores can be used to automatically annotate multi-word idiomatic phrases or proper names. Model improvements resulting from these annotations will be assessed in future work. gives a rough measure of the semantic similarity between the documents. However, this representation is clearly lacking: two semantically related documents could simply use di↵erent words or phrases and their sparse vectors would have low cosine similarity. A standard approach is to apply dimensionality reduction algorithms to map from R |X| to a more manageable embedding space R d and hope that the compression captures latent semantic information. Rather than working with the set of document vectors directly, we propose to learn the projection by associating a semantic vector v(x) to each word x 2 X. To this end, we make use of the skip-gram word embedding model contained in word2vec <ref type="bibr" coords="4,209.22,214.34,10.90,10.43" target="#b1">[2]</ref>. This algorithm e ciently produces clusters of synonyms in R d and organizes them by type to some extent. The embedding vector of a document y is then defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semantic representation of words and documents</head><formula xml:id="formula_6" coords="4,205.48,251.53,200.75,32.39">v(y) = X x2y T F IDF (x, y) LF IDF (x) v(x).</formula><p>In our experiments, we apply a minimal amount of data cleaning to the input text. We lowercase the text, remove all non-alphanumeric characters, and then replace each digit with the # symbol. With enough training, the embedding vectors v(y) outperform the sparse vectors s(y) in terms of precision and recall. For example, below we plot the interpolated precision and recall curves comparing the two algorithms as measured on a sample from the oldreut Reuters corpus (Fig. <ref type="figure" coords="4,329.16,358.85,4.24,10.43" target="#fig_1">1</ref>). Given a query topic, we pass the text of the query through the above process to produce a topic embedding vector t. We then sort the document embeddings v(y) by cosine similarity to t, and return the k-nearest neighbors. Given a relevance judgement for each of these documents relative to the topic, we train a lightweight neural network with 2d neurons in the hidden layer and one output neuron which predicts the probability that a given embedding vector is relevant to the topic. Note that the number of parameters in this neural network is O(d 2 ). In practice, good semantic representation can be achieved for relatively small values of d, so these classifiers are quite e cient in terms of space and training time. To generate the next document recommendations, we find the k-nearest documents to the image of t in the expanded embedding space R 2d and remove any documents that have already been viewed. Any further relevance judgements provide more training data for the neural network, which refines the hidden layer semantic search.</p><p>We ran the above system on three corpora, athome1, athome2, and athome3, with 10 query topics each. The number of documents in each corpus were approximately 290k, 460k, and 900k, respectively. The experiments were performed on a single node with 8 CPU cores and 16GB of RAM. For each corpus, 50-dimensional word embeddings were trained for 10 epochs, where an epoch is defined as one read through the files. For each topic, a classifier neural network with a 100-dimensional hidden layer was trained for 10 epochs, each time sampling up to 5,000 random training points. The size of the batches submitted for assessment was set to the nearest power of 10 less than or equal to the number of documents reviewed, up to a maximum batch size of 2,000 documents.</p><p>The recall as a function of review e↵ort is plotted below, organized by corpus (Fig. <ref type="figure" coords="5,502.89,417.57,4.24,10.43" target="#fig_2">2</ref>, Fig. <ref type="figure" coords="5,123.83,431.12,4.24,10.43">3</ref>, Fig. <ref type="figure" coords="5,159.44,431.12,4.24,10.43">4</ref>), along with the text corresponding to each topic code. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>While our seed model achieves superior semantic representation relative to TF-IDF, the dynamic component has some issues that inhibit performance, as compared to the Baseline Model Implementation (BMI) for continuous active learning (CAL) described in <ref type="bibr" coords="7,112.53,592.61,10.90,10.43" target="#b0">[1]</ref>. If the text of the query topic does not contain su ciently distinctive words, then the topic embedding will not adequately capture the composite meaning of the topic text, to the detriment of the seed recommendations. Annotation of named entities and idiomatic phrases, e.g. as described in Example 2.2, would partly alleviate this problem.</p><p>Additional problems arise when only a miniscule number of documents in the corpus are relevant to a topic, since training a classifier requires positive examples. In this case, it becomes necessary to validate the seed recommendations by incorporating complementary methods, e.g. keyword search, before submitting them for assessment. It would also be helpful to adjust the classifier algorithm so that the training data is not overwhelmed by negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>The author would like to thank T. Emerick and K. Sadeghi for sharing their insights during many helpful discussions. Thanks are also due to the CCRi leadership for supporting this research e↵ort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,100.35,539.45,411.02,10.91;3,100.35,552.99,56.28,10.91;3,161.09,553.67,7.27,17.98;3,172.44,552.99,194.11,10.91;3,370.81,561.15,19.16,2.75;3,394.06,553.67,7.27,17.98;3,405.41,551.70,105.96,14.06;3,100.35,566.54,160.63,10.91;3,264.13,567.22,3.02,17.98;3,270.31,566.54,241.05,10.91;3,100.35,580.09,36.94,10.91;3,137.29,584.60,4.23,7.30;3,142.02,580.09,10.20,10.91;3,152.22,584.60,4.23,7.30;3,159.98,580.77,7.27,17.98;3,170.28,580.09,123.76,10.91;3,297.68,584.60,18.77,7.30;3,316.95,580.57,25.45,10.43;3,346.04,584.60,18.77,7.30;3,365.31,580.57,7.26,10.43;3,242.86,610.82,26.01,10.43;3,268.87,614.85,4.23,7.30;3,273.60,610.34,10.20,10.91;3,283.80,614.85,4.23,7.30;3,288.53,610.82,15.75,10.43;3,310.64,607.47,18.77,7.30;3,329.91,603.44,4.23,10.43;3,336.58,603.63,3.02,17.98;3,342.03,607.47,18.77,7.30;3,361.30,603.44,4.23,10.43;3,308.52,618.50,17.57,17.98;3,326.09,622.33,4.23,7.30;3,330.82,618.30,24.84,10.43;3,355.66,622.33,4.23,7.30;3,360.39,618.30,7.26,10.43"><head>Let</head><label></label><figDesc>X and Y denote the vertices in the word-document graph from Example 2.1. Each document y 2 Y can be represented as a sparse vector s(y) 2 R |X| whose value at a word x is equal to T F IDF (x, y) • LF IDF (x), if x occurs in y, and zero otherwise. Given y 1 , y 2 2 Y , the cosine similarity of s(y 1 ) and s(y 2 ): sim(y 1 , y 2 ) = s(y 1 ) • s(y 2 ) |s(y 1 )||s(y 1 )|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,203.26,584.15,205.21,2.75;4,181.33,377.05,249.06,186.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: precision vs. recall, oldreut</figDesc><graphic coords="4,181.33,377.05,249.06,186.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,209.27,657.79,193.18,2.75;5,156.42,449.32,298.88,188.17"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: recall vs. e↵ort, athome1</figDesc><graphic coords="5,156.42,449.32,298.88,188.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,156.42,304.53,298.88,171.60"><head></head><label></label><figDesc></figDesc><graphic coords="6,156.42,304.53,298.88,171.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,156.42,129.78,298.88,172.10"><head></head><label></label><figDesc></figDesc><graphic coords="7,156.42,129.78,298.88,172.10" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,122.77,372.43,388.60,10.43;8,122.77,385.98,272.16,10.43;8,399.43,393.66,111.93,2.75;8,122.77,407.21,388.60,2.75;8,122.77,420.76,42.05,2.75;8,168.46,413.08,126.42,10.43" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,312.86,372.43,198.51,10.43;8,122.77,385.98,253.08,10.43">Evaluation of machine-learning protocols for technology-assisted review in electronic discovery</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,399.43,393.66,111.93,2.75;8,122.77,407.21,388.60,2.75;8,122.77,420.76,37.85,2.75">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">153162</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,122.77,433.40,388.59,10.43;8,122.77,446.95,134.47,10.43;8,260.89,454.63,81.01,2.75;8,345.55,446.95,24.84,10.43" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,356.06,433.40,155.30,10.43;8,122.77,446.95,129.71,10.43">E cient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,260.89,454.63,75.43,2.75">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
