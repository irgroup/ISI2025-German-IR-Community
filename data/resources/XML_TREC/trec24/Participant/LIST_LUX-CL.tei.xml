<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,77.85,160.61,455.53,17.23;1,88.77,182.53,433.71,17.23">LIST at TREC 2015 Clinical Decision Support Track: Question Analysis and Unsupervised Result Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.84,216.78,109.12,11.97"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Luxembourg Institute of Science and Technology (LIST)</orgName>
								<address>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.42,216.78,95.99,11.97"><forename type="first">Saoussen</forename><surname>Khelifi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Luxembourg Institute of Science and Technology (LIST)</orgName>
								<address>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,77.85,160.61,455.53,17.23;1,88.77,182.53,433.71,17.23">LIST at TREC 2015 Clinical Decision Support Track: Question Analysis and Unsupervised Result Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C9C8244D34BB8CDF08FE08E6B58E398</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our information retrieval approaches to the TREC 2015 Clinical Decision Support Track. We explore di↵erent question analysis methods in order to retrieve articles relevant to the given clinical questions. We particularly study the use of two knowledge sources: MeSH and DBpedia for question expansion and the simplification of questions by removing information about the patient and negation. We also compare single IR models with the fusion of results based on both ranks and scores. Our experiments conclude that (i) query expansion using Mesh and DBpedia improves the results and that (ii) the combination of IR results using the rank outperforms the fusion based on scores. For TREC 2015 CDS task A, our best results were obtained by using DBpedia for query expansion and by combining the 2 IR models Hiemstra LM and LGD using a rank-based method. Our best run achieved an infNDCG score of 0.2894 and was ranked second over 92 runs for task A.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The 2015 Clinical Decision Support Track 1 focuses on the retrieval of biomedical articles relevant for answering clinical questions about medical records. The document collection is the Open Access Subset 2 of PubMed Central (PMC). The target collection contains 733,138 full-text articles 3 , the same collection used for the 2014 track.</p><p>Participants are tasked with retrieving fulltext biomedical articles useful for answering questions related to three types of generic clinical questions. The considered question types are: Diagnosis, Test and Treatment (10 topics are provided for each type). • diagnosis = Iron-Deficiency Anemia</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In our experiments, we explored several semantic analysis methods such as question simplification and query expansion. We also explored unsupervised methods for search result fusion. Figure <ref type="figure" coords="2,295.19,417.04,5.45,10.43" target="#fig_1">1</ref> presents an overview of our retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semantic Question Analysis</head><p>Semantic analysis of natural language questions is an important step towards the construction of relevant queries for information retrieval <ref type="bibr" coords="2,286.10,516.21,10.90,10.43" target="#b0">[1]</ref>. We explored two di↵erent methods for question analysis: Question simplification and question annotation using external knowledge sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MeSH based Annotation vs. DBpedia based Annotation for Query Expansion</head><p>We explored the use of medical Subject Headings (MeSH) terms. We studied several configurations:</p><p>• Adding MeSH terms to the query.</p><p>• Giving a higher weight to MeSH terms in the query.</p><p>• Adding all the synonyms of MeSH terms to the query.</p><p>• Giving a higher weight to MeSH terms and their synonyms in the query.</p><p>We experimentally selected the following IR models as they provided the best results on the TREC 2014 corpus: Hiemstra LM (Hiemstra's language model), TF-IDF and LGD.</p><p>We used the Terrier IR platform <ref type="foot" coords="2,474.68,312.26,4.23,7.30" target="#foot_0">4</ref> for indexing and retrieving documents in the collection. The KODA system was used to annotate questions with DBpedia <ref type="bibr" coords="2,381.90,354.47,10.90,10.43" target="#b3">[4]</ref>. Table <ref type="table" coords="2,433.27,354.47,5.45,10.43" target="#tab_0">1</ref> presents a summary of our experiments on query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Simplification</head><p>We removed two types of information from the question: (i) Age and gender information about the patient and (ii) Negation.</p><p>This question simplification approach improved the results when tested on the CDS 2014 test set. The TF-IDF and Hiemstra LM models obtained the following results using question simplification and query expansion with higher weights on DBpedia terms:</p><p>• Hiemstra LM: P@10 = 0.3167 &amp; R-prec = 0.2378</p><p>• TF-IDF: P@10 = 0.3700 &amp; R-prec = 0.2426  Combining di↵erent IR models can lead to a better overall performance <ref type="bibr" coords="3,207.86,633.13,11.51,10.43" target="#b1">[2,</ref><ref type="bibr" coords="3,222.01,633.13,8.48,10.43" target="#b2">3,</ref><ref type="bibr" coords="3,233.12,633.13,7.67,10.43" target="#b4">5]</ref>. We focus on exploring unsupervised methods for search result fusion and in particular the comparison of two unsupervised approaches: rank-based vs. score-based fusion. Rank fusion aims at combining di↵erent ranked document lists into a single list based on the rank. Several methods can be used such as: CombMAX (max of similarity values), CombMED (median of similarity values) or CombSUM (sum of similarity values) <ref type="bibr" coords="4,253.12,160.14,10.90,10.43" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unsupervised Approaches to Combine Search Results</head><p>Score-based fusion aims at combining di↵erent document lists into a single one based on the score. Di↵erent methods can be used such as Reciprocal Rank Fusion (RRF) <ref type="bibr" coords="4,213.24,215.17,10.90,10.43" target="#b1">[2]</ref>. Given a set D of documents to be ranked and a set of rankings R, each a permutation on 1..|D|,</p><formula xml:id="formula_0" coords="4,76.66,263.71,223.97,44.61">RRF score(d 2 D) = X r2R 1 k + r(D) (k = 60) (1)</formula><p>We tested the di↵erent methods for rankbased fusion. In our experiments on the CDS 2014 test set, CombSUM outperformed the other rank-based methods (such as CombMax) and the RRF score-based method. Table <ref type="table" coords="4,235.43,366.46,5.45,10.43" target="#tab_1">2</ref> summarizes these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Runs and Results</head><p>In CDS 2015, participants may submit a maximum of three automatic or manual runs. Each run consists of a ranked list of up to one thousand PMCID. Retrieved articles are judged relevant if they provide information of the specified type that is pertinent to the given case. The evaluation of submissions follow standard TREC evaluation procedures.</p><p>For task A, we submitted 3 automatic runs using the topics summaries:</p><p>• Run1DBpSimp: IR model TF-IDF. Query expansion using 30 expanded terms within top 20 documents. Semantic annotation of queries using DBpedia. Query simplification by deleting information about patients.</p><p>• Run4HLM: IR model Hiemstra LM. Query expansion using 30 expanded terms within top 20 documents.</p><p>• Run2DBpComb: Combination of 2 IR models Hiemstra LM and LGD. Query expansion using 30 expanded terms within top 20 documents. Semantic annotation of queries using DBpedia.</p><p>Table <ref type="table" coords="4,351.07,257.84,5.45,10.43" target="#tab_2">3</ref> presents our o cial results for task A. The run Run2DBpComb combining the IR models Hiemstra LM and LGD and using DBpedia for query expansion obtained the best results. This run was ranked second over 92 submitted runs for task A.</p><p>For task B, we tested abstract-only indexation. The run Run5DBpAbs consisted in indexing only the titles and the abstracts with the TF-IDF model. Query expansion was also applied using 30 expanded terms within top 20 documents and semantic annotation of queries using DBpedia and query simplification by deleting information about patients. We obtained the following results for run Run5DBpAbs:</p><p>• R-prec: 0.2114</p><p>• P10: 0.4000</p><p>• infAP: 0.0801</p><p>• infNDCG: 0.2855</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Retrieving biomedical articles relevant for answering clinical questions is becoming more and more critical with the exponential growth of biomedical literature. In this paper we described our experiments in the scope of the clinical decision support track at TREC 2015. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,321.51,521.53,217.73,10.43;1,310.61,535.08,228.64,10.43;1,310.61,548.63,228.64,10.43;1,310.61,562.18,228.63,10.43;1,310.61,575.73,47.33,10.43;2,82.91,140.72,177.10,2.75;2,82.91,154.69,5.45,17.98;2,93.82,154.49,125.94,10.43;2,82.91,176.78,5.45,17.98;2,93.82,176.58,206.82,10.43;2,93.82,190.13,206.82,10.43;2,93.82,203.68,206.82,10.43;2,93.82,217.23,19.69,10.43;2,82.91,246.36,176.54,2.75;2,82.91,260.33,5.45,17.98;2,93.82,260.14,129.78,10.43;2,82.91,282.42,5.45,17.98;2,93.82,282.23,206.82,10.43;2,93.82,295.78,161.02,10.43"><head>Example 2 (</head><label>2</label><figDesc>The 2015 CDS Track includes two tasks. Task A is identical to the 2014 track. In Task B, a diagnosis field is provided for the treatment and test topics. Examples 1 and 2 show topics from each task. Example 1 (Task A, CDS 2015): • Question type = diagnosis • Summary = A 65-year-old male presents with dyspnea, tachypnea, chest pain on inspiration, and swelling and pain in the right calf. Task B, CDS 2015): • Question type = treatment • Summary = A 15 yo girl with fatigue, pale skin, low hemoglobin and ferritin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,204.05,139.62,203.15,10.43;3,72.00,150.44,467.25,233.63"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our retrieval system</figDesc><graphic coords="3,72.00,150.44,467.25,233.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,405.86,467.24,176.85"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="3,92.02,444.36,427.21,138.35"><row><cell>IR Models</cell><cell>Query Expansion</cell><cell cols="2">P@10 R-prec infAP infNDCG</cell></row><row><cell>BM25</cell><cell>-</cell><cell>0.2800 0.2246 0.0180</cell><cell>0.1851</cell></row><row><cell>Hiemstra LM</cell><cell>-</cell><cell>0.3233 0.2193 0.0154</cell><cell>0.1671</cell></row><row><cell>Hiemstra LM</cell><cell>MeSH terms (+ higher weight)</cell><cell>0.3400 0.2374 0.0153</cell><cell>0.1641</cell></row><row><cell cols="3">Hiemstra LM DBPedia terms (+ higher weight) 0.3433 0.2389 0.0152</cell><cell>0.1653</cell></row><row><cell>TF-IDF</cell><cell>-</cell><cell>0.3033 0.2234 0.0163</cell><cell>0.1737</cell></row><row><cell>TF-IDF</cell><cell>MeSH terms (+ higher weight)</cell><cell>0.3267 0.2400 0.0168</cell><cell>0.1796</cell></row><row><cell>TF-IDF</cell><cell cols="2">DBPedia terms (+ higher weight) 0.3367 0.2365 0.0165</cell><cell>0.1770</cell></row><row><cell>LGD</cell><cell>-</cell><cell>0.3067 0.2195 0.0164</cell><cell>0.1735</cell></row><row><cell>LGD</cell><cell cols="2">DBPedia terms (+ higher weight) 0.3633 0.2465 0.0166</cell><cell>0.1763</cell></row></table><note coords="3,116.64,405.86,422.60,10.43;3,72.00,419.41,77.35,10.43"><p>Summary of our experiments on CDS 2014 test set: MeSH vs. DBpedia annotation for query expansion</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.00,139.62,467.24,146.44"><head>Table 2 :</head><label>2</label><figDesc>Summary of our experiments on CDS 2014 test set: Combining IR models using a scorebased (RRF) vs. a rank-based method (CombSUM).</figDesc><table coords="5,78.38,189.55,457.01,96.51"><row><cell>IR Model Fusion + Query Expansion (QE)</cell><cell cols="2">P@10 R-prec infAP infNDCG</cell></row><row><cell>RRF (In expB2, Hiemstra LM) + MeSH for QE</cell><cell>0.3200 0.2100 0.0155</cell><cell>0.1677</cell></row><row><cell>RRF (TF IDF, Hiemstra LM) + MeSH for QE</cell><cell>0.3233 0.2103 0.0154</cell><cell>0.1671</cell></row><row><cell>CombSUM (In expB2, Hiemstra LM) + MeSH for QE</cell><cell>0.3267 0.2480 0.0168</cell><cell>0.1789</cell></row><row><cell>CombSUM (TF IDF, Hiemstra LM) + MeSH for QE</cell><cell>0.3300 0.2268 0.0164</cell><cell>0.1743</cell></row><row><cell cols="2">CombSUM (TF IDF, Hiemstra LM) + DBpedia for QE 0.3567 0.2534 0.0162</cell><cell>0.1755</cell></row><row><cell>CombSUM (Hiemstra LM, LGD) + DBpedia for QE</cell><cell>0.3733 0.2654 0.0170</cell><cell>0.1807</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.00,322.15,465.06,233.93"><head>Table 3 :</head><label>3</label><figDesc>CDS 2015: Our O cial Results for Task A vs. Median (over 92 automatic task A runs) Our results show that (i) query expansion using open-domain knowledge bases such as DBpedia and (ii) search result fusion using a rank-based method such as CombSUM led to a noticeable improvement. In future work we plan to enhance further our IR approach by annotating semantically the abstracts of all documents in the test collection.</figDesc><table coords="5,133.56,347.71,344.12,66.22"><row><cell>-</cell><cell cols="4">MEDIAN Run2DBpComb Run4HLM Run1DBpSimp</cell></row><row><cell>R-prec</cell><cell>0.1615</cell><cell>0.2299</cell><cell>0.2152</cell><cell>0.2033</cell></row><row><cell>P10</cell><cell>0.3433</cell><cell>0.4533</cell><cell>0.4500</cell><cell>0.4100</cell></row><row><cell>infAP</cell><cell>0.0414</cell><cell>0.0787</cell><cell>0.0746</cell><cell>0.0715</cell></row><row><cell>infNDCG</cell><cell>0.2109</cell><cell>0.2894</cell><cell>0.2768</cell><cell>0.2571</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,327.19,669.14,51.78,2.11"><p>terrier.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,88.97,628.49,211.66,2.75;5,88.97,642.04,11.57,2.75;5,110.76,634.36,189.88,10.43;5,88.97,647.91,211.66,10.43;5,88.97,660.98,211.66,10.91;5,327.58,450.32,211.66,10.91;5,327.58,463.87,59.31,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,110.76,634.36,189.88,10.43;5,88.97,647.91,207.14,10.43">Medical question answering: Translating medical questions into sparql queries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,102.15,660.98,198.48,10.91;5,327.58,450.32,142.52,10.91">ACM SIGHIT International Health Infor-matics Symposium, IHI 2012</title>
		<meeting><address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,327.58,502.84,211.65,2.75;5,327.58,516.39,97.07,2.75;5,430.99,508.71,108.25,10.43;5,327.58,522.26,211.66,10.43;5,327.58,535.33,211.67,10.91;5,327.58,548.88,211.66,10.91;5,327.58,562.43,211.66,10.91;5,327.58,575.98,211.67,10.91;5,327.58,589.52,122.94,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,430.99,508.71,108.25,10.43;5,327.58,522.26,211.66,10.43;5,327.58,535.81,79.17,10.43">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,428.95,535.33,110.30,10.91;5,327.58,548.88,211.66,10.91;5,327.58,562.43,211.66,10.91;5,327.58,575.98,141.45,10.91">Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
	<note>SIGIR 2009</note>
</biblStruct>

<biblStruct coords="5,327.58,628.49,177.23,2.75;5,515.92,620.81,23.33,10.43;5,327.58,634.36,211.68,10.43;5,327.58,647.91,211.66,10.43;5,327.58,660.98,211.67,10.91;6,88.97,132.56,211.67,10.91;6,88.97,146.11,139.07,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,515.92,620.81,23.33,10.43;5,327.58,634.36,211.68,10.43;5,327.58,647.91,193.26,10.43">CRP henri tudor at TREC 2014: Combining search results for clinical decision support</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,327.58,660.98,211.67,10.91;6,88.97,132.56,86.68,10.91">Proceedings of The Twenty-Third Text RE-trieval Conference</title>
		<meeting>The Twenty-Third Text RE-trieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.97,176.79,211.67,2.75;6,88.97,190.33,190.15,2.75;6,284.58,182.65,16.06,10.43;6,88.97,195.72,211.66,10.91;6,88.97,209.27,211.67,10.91;6,88.97,222.82,211.67,10.91;6,88.97,236.37,211.66,10.91;6,88.97,250.40,70.90,10.43" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,284.58,182.65,16.06,10.43;6,88.97,196.20,167.08,10.43">Towards knowledge-driven annotation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mrabet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Foulonneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Simperl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,279.72,195.72,20.91,10.91;6,88.97,209.27,211.67,10.91;6,88.97,222.82,160.80,10.91">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">January 25-30, 2015. 2015</date>
			<biblScope unit="page" from="2425" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,88.97,280.60,171.74,2.75;6,274.58,272.92,26.06,10.43;6,88.97,285.99,211.67,10.91;6,88.97,299.53,211.67,10.91;6,88.97,313.08,211.67,10.91;6,88.97,327.11,96.96,10.43" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,274.58,272.92,26.06,10.43;6,88.97,286.47,141.24,10.43">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,261.32,285.99,39.32,10.91;6,88.97,299.53,206.58,10.91">Proceedings of The Third Text REtrieval Conference</title>
		<meeting>The Third Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
