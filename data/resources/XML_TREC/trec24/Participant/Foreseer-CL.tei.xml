<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,79.87,73.49,449.98,15.73;1,69.48,93.42,470.76,15.73">Learning from Medical Summaries: The University of Michigan at TREC 2015 Clinical Decision Support Track</title>
				<funder>
					<orgName type="full">Tsinghua University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,117.31,138.62,64.74,10.47"><forename type="first">Fengmin</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,192.85,138.62,77.41,10.47"><forename type="first">Danny</forename><forename type="middle">T Y</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,281.06,138.62,67.12,10.47"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
							<email>qmei@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.89,138.62,117.45,10.47"><forename type="first">V</forename><forename type="middle">G Vinod</forename><surname>Vydiswaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Learning Health Sciences</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,79.87,73.49,449.98,15.73;1,69.48,93.42,470.76,15.73">Learning from Medical Summaries: The University of Michigan at TREC 2015 Clinical Decision Support Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E4FDBFFC81A9D01086C0120F4D3A05E6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of TREC 2015 Clinical Decision Support Track was to retrieve biomedical articles relevant for answering three kinds of generic clinical questions, namely diagnosis, test, and treatment. In order to achieve this purpose, we investigated three approaches to improve the retrieval of relevant articles: modifying queries, improving indexes, and ranking with ensembles. Our final submissions were a combination of several different configurations of these approaches. Our system mainly focused on the summary fields of medical reports. We built two different kinds of indexes -an inverted index on the free text and a second kind of indexes on the Unified Medical Language System (UMLS) concepts within the entire articles that were recognized by MetaMap. We studied the variations of including UMLS concepts at paragraph and sentence level and experimented with different thresholds of MetaMap matching scores to filter UMLS concepts. The query modification process in our system involved automatic query construction, pseudo relevance feedback, and manual inputs from domain experts. Furthermore, we trained a re-ranking sub-system based on the results of TREC 2014 Clinical Decision Support track using Indri's Learning to Rank package, RankLib. Our experiments showed that the ensemble approach could improve the overall results by boosting the ranking of articles that are near the top of several single ranked lists.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recent years have seen significant advances in the field of healthcare, with smarter computerized clinical decision support platforms, better personalized healthcare plans, and availability of large data sets for research and quality improvement studies. According to Accenture's report of healthcare IT Vision of 2015 <ref type="bibr" coords="1,141.79,559.49,9.71,8.21" target="#b9">[9]</ref> and the survey of doctor's attitudes towards to healthcare IT functions <ref type="bibr" coords="1,195.22,569.96,13.51,8.21" target="#b10">[10]</ref>, the digital channels are dramatically influencing the healthcare industry and are expected to do so in near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>While making clinical decisions, physicians often seek out</head><p>Text REtrieval Conference TREC 2015 Clinical Decision Support Track additional information to provide the best care for their patients. There is a growing interest among medical informatics researchers on clinical decision support (CDS) systems that are designed to assist physicians and other health professionals with clinical decision-making tasks. The TREC Clinical Decision Support Track aims to help develop techniques to address these tasks and link medical cases to information relevant to patient care. The track, in its second year, continued to focus on retrieving biomedical articles relevant for answering generic clinical questions about medical records. There were two tasks in TREC 2015 Clinical Decision Support track. Task A was identical to the TREC 2014 CDS track, while in Task B, participants were additionally provided with the diagnosis information for the test and treatment related queries.</p><p>The target document collection for this track was a snapshot of the Open Access Subset of PubMed Central. Participants were also provided a set of case reports, such as those published in biomedical articles, as idealized representations of actual medical records. There were two formats of the case narratives -a long paragraph "description" narrative of the complete account of a patient's visit, including details such as their vital statistics, drug dosage, etc.; and a simplified "summary" narrative that contained less irrelevant information. Participants were challenged with retrieving relevant biomedical articles for those specific case reports to answer questions belonging to one of three generic clinical categories; namely "What is the patient's diagnosis?", "What tests should the patient receive?", and "How should the patient be treated?".</p><p>We reviewed the main approaches following by numerous research teams that participated in the first TREC CDS track in 2014. After reviewing the papers and results from the previous year, we found that there was no clear choice between two versions of case narratives. "Summary" narratives were considered as the first choice by most groups since they contained less irrelevant information and were easier for query modification than "description" narrative. However, several groups got slightly better performance by using the "description" narrative. In <ref type="bibr" coords="1,392.99,639.70,13.51,8.21" target="#b14">[14]</ref>, Xu et al. concluded that using summaries as queries and applying the standard bag-of-words retrieval function, BM25, with pseudo-relevance feedback, could provide a satisfactory baseline performance. Teams also investigated various query modification and reformulation techniques. Some groups applied query reduction approaches by filtering terms with the help of external medi-Figure <ref type="figure" coords="2,272.66,283.45,3.58,8.21">1</ref>: System Architecture cal knowledge resources or domain experts <ref type="bibr" coords="2,231.43,314.49,14.33,8.21" target="#b11">[11,</ref><ref type="bibr" coords="2,249.44,314.49,10.75,8.21" target="#b12">12]</ref>. Other participating teams employed query expansion approaches, such as pseudo-relevance feedback <ref type="bibr" coords="2,198.71,335.41,9.72,8.21" target="#b3">[3,</ref><ref type="bibr" coords="2,213.00,335.41,7.16,8.21" target="#b8">8,</ref><ref type="bibr" coords="2,224.75,335.41,11.77,8.21" target="#b11">11,</ref><ref type="bibr" coords="2,241.09,335.41,11.77,8.21" target="#b14">14]</ref> and synonyms expansion <ref type="bibr" coords="2,127.22,345.88,13.51,8.21" target="#b12">[12]</ref>. Those techniques mostly appeared effective and they improved the overall retrieval performance by boosting articles which were more similar to the top articles. However, they were also prone to query drifting.</p><p>Besides traditional information retrieval methods, re-ranking methods were also widely studied in <ref type="bibr" coords="2,205.58,408.63,9.72,8.21" target="#b3">[3,</ref><ref type="bibr" coords="2,219.00,408.63,7.17,8.21" target="#b4">4,</ref><ref type="bibr" coords="2,229.87,408.63,7.17,8.21" target="#b8">8,</ref><ref type="bibr" coords="2,240.73,408.63,10.75,8.21" target="#b11">11]</ref>. Soldaini et al. <ref type="bibr" coords="2,76.38,419.10,14.32,8.21" target="#b11">[11]</ref> proposed several re-rankers including biographical re-ranker, MetaMap similarity re-ranker, etc. that respectively took features such as the biographic characteristics and the number of concepts in Unified Medical Language System (UMLS) recognized by MetaMap into consideration to generate a final article score. Fusion-based re-ranking approach also proved to be an effective approach and improved the performance significantly <ref type="bibr" coords="2,174.83,492.32,9.72,8.21" target="#b3">[3,</ref><ref type="bibr" coords="2,188.13,492.32,7.16,8.21" target="#b8">8,</ref><ref type="bibr" coords="2,198.88,492.32,10.74,8.21" target="#b11">11]</ref>. Soldaini et al. <ref type="bibr" coords="2,278.62,492.32,14.31,8.21" target="#b11">[11]</ref> also developed a voting-based fusion algorithm to compute the final ranking as an average of four other rankers. André Mourão et al. <ref type="bibr" coords="2,110.48,523.71,9.72,8.21" target="#b8">[8]</ref> applied Reciprocal Rank Fusion (RRF) algorithm to combine different retrieval functions. In <ref type="bibr" coords="2,259.15,534.16,9.20,8.21" target="#b3">[3]</ref>, Choi and Choi trained two kinds of task-specific classifiers and combined the relevance ranking with task-specific ranking using the Borda-fuse algorithm <ref type="bibr" coords="2,181.88,565.55,9.21,8.21" target="#b1">[1]</ref>.</p><p>Though many traditional and state-of-the-art information retrieval approaches have been investigated last year, the mean retrieval scores were relatively poor and lower than expected in general. Most participants only focused on the free text of target document collection and employed the basic pseudo-relevance feedback model as well as straightforward re-ranking strategies. In our system, we focused on two kinds of information: free text information and UMLS concept information recognized by MetaMap. To achieve this, we built two indexes -an inverted index on the free text and a second index on the UMLS concepts recognized by MetaMap in the entire article. Queries were mostly based on the "summary" narrative, and were modified either automatically or with the help of a domain expert. Two versions of queries were presented, a free-text version for the first inverted index and a UMLS Concept Unique Identifier (CUI) version for the second UMLS concept index. We also considered multiple variations of including UMLS concept information at paragraph or sentence level and experimented with different thresholds to filter UMLS concepts based on their MetaMap scores. Finally, we designed and trained a reranking sub-system using the Random Forest algorithm <ref type="bibr" coords="2,546.20,408.63,9.72,8.21" target="#b2">[2]</ref> to combine the ensemble of single ranked lists into a final ranked list.</p><p>In the following sections, we will describe in more detail how we deployed the three major approaches we studied, viz. improving indexes, modifying queries, and re-ranking with ensemble, and how these approaches impacted the retrieval performance of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM ARCHITECTURE</head><p>First, we provide a high-level overview of our system architecture and describe the key components briefly. The system architecture is shown in Figure <ref type="figure" coords="2,473.31,554.17,3.58,8.21">1</ref>. In the figure, blue and yellow colors represent the components that work with free-text information and UMLS concepts, respectively. The primary components of our approach are as follows:</p><p>1. Building Indexes: We built two kinds of indexes. First, we built an inverted index over the Target Document Collection provided to all TREC-CDS participants. Next, we processed the title and abstracts of all documents through MetaMap. This let us identify all medical concepts mentioned in the title and abstract sections. The MetaMap matching scores were used to retain only those concepts that were scored higher than a preset threshold. The second set of indexes were built over the unique identifiers assigned by MetaMap to the filtered concepts.</p><p>2. Query Modification: We processed the "summary" narratives through MetaMap to identify medical concepts mentioned in the narratives. Two kinds of queries were formulated: one consisting of the concept words themselves, and the other consisting of the unique identifiers for the concepts (CUIs). The final queries were constructed by either expanding the queries automatically or using inputs from a domain expert. 3. Retrieval: The free-text queries and the UMLS CUI queries were submitted to appropriate indexes to get an ensemble of single ranked list results. 4. Re-ranking with ensembles: We designed and developed a re-ranking sub-system to combine the ranked results from individual queries. This sub-system considered the rank orders (scores) for articles from individual ranked lists as features and trained a re-ranking model. We used the Random Forest algorithm <ref type="bibr" coords="3,271.41,225.16,9.72,8.21" target="#b2">[2]</ref> to learn the weights of individual rankers and combine them to get the final ranked list.</p><p>Since the setup for the Task A of TREC-CDS 2015 was identical to the task in the previous year's TREC-CDS task, we could train over models on the queries and results from the previous year. On the other hand, the Task B of TREC-CDS 2015 was a new task. So, we could not train new models for Task B. Instead, we adopted the model learned in Task A by adding additional information about the diagnosis that was available for Task B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMPLEMENTATION</head><p>We now describe the detailed implementation of our system for TREC-CDS 2015 track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>Our original goal was to parse the entire collection through MetaMap to identify all medical concepts in the documents. However, since MetaMap is quite time-consuming to run over the full-text articles in the target document collection, we decided to focus only on the titles and abstracts of the articles. We first checked the MetaMapped Medline Baseline results 1 , provided by the National Library of Medicine (NLM). The Medline articles processed by NLM contain unique PubMed identifiers (PMIDs) that were also found in the TREC-CDS target document collection, thereby facilitating merging of the two datasets and avoiding duplication of efforts.</p><p>On parsing the TREC-CDS target document collection, we found that 46, 648 articles (6.36% of the total documents in the collection) did not mention PMIDs, and hence could not be linked to the MetaMapped Medline Baseline corpus. We further analyzed these articles. Most of these articles belong to one of three categories, viz. "abstracts" (42.09%), "bookreviews" (23.45%), or "corrections" (10.53%). Only 3, 566 (7.64%) articles were labeled as "research articles". Based on the analysis of the distribution of article types in the target collection and qrel files conducted by Gobeill et al. <ref type="bibr" coords="3,266.39,652.71,9.21,8.21" target="#b5">[5]</ref>, we found that articles that belonged to the types "abstracts", "book-reviews", and "corrections" consist of less than 4.8% of files found in the qrel file, while research articles accounted for 52.2% of files. This finding also agrees with our intuition 1 https://skr.nlm.nih.gov/resource/MetaMappedBaselineInfo.shtml that the articles of the former three types are generally short and less likely to be found relevant for answering clinical questions. Hence, we decided to only process the missing articles that were classified as "research articles" and ignore the remaining types.</p><p>In addition to articles that did not have PMIDs, we also found 30, 282 articles with PMIDs in the target document collection that were not found in our snapshot of MetaMapped Medline Baseline corpus. Those, when added to the 3, 564 "research abstracts" identified above, gave us a set of 33, 848 articles with missing MetaMap information. We extracted the titles and abstracts from these articles and ran MetaMap on this set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parsing</head><p>Once we collected the MetaMapped results for all documents in the target collection, the next task was to parse the MetaMap output. For each article, the MetaMap Machine Output<ref type="foot" coords="3,346.50,262.23,3.65,3.71" target="#foot_0">2</ref> contains an 'utterance' field, which is a sequence of tokens into which the input text was chunked. Each utterance is followed by one or more sequences of the following three sub-components: 1. phrase: A sub-sequence of the utterance's tokens, along with its syntax. A phrase always appears with corresponding candidates and mappings objects. 2. candidates: A possibly empty list of UMLS candidate concepts identified in the phrase. 3. mappings: A possibly empty list of MetaMap's final mappings and a subset of the candidate set. The mapping list is empty only if the candidates list is empty.</p><p>For each candidate object, we collected the following three fields:</p><p>1. NegScore: The negative mapping score 2. CUI: The concept unique identifier 3. String: The string representation of the concept as it appears in the utterance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Indexing Text and Concepts</head><p>We built two kinds of indexes using the Indri search engine <ref type="foot" coords="3,333.19,502.42,3.65,3.71" target="#foot_1">3</ref> , viz. a free-text inverted index and several UMLS CUI indexes. For the free-text index, we used Indri's built-in XML parser directly to parse and index the article text. For the UMLS CUI indexes, however, some additional processing was required.</p><p>MetaMap assigns score to each concept identified in the text.</p><p>Perfect matches get a score of 1000, and other matches with ambiguities get lower scores. Our previous experiments with MetaMap scores show that scores above 700 are potentially relevant while those above 800 are significant. We parse the title and abstracts of all articles and construct three versions of concept documents -one with all concepts identified in the input text, and two other versions where only concepts with scores over 700 and over 800 are retained. We will refer to these three versions as UMLS-CUI-all, UMLS-CUI-700, and UMLS-CUI-800, respectively. As the threshold value is increased, the concept documents have progressively lesser number of concepts in them.</p><p>We also experimented with the granularity of the documents themselves. Instead of treating the title and abstract as one document, we split the abstract into multiple sentences and treated each sentence as its own document. This approach increases the number of documents in the index, but each document (a sentence) was short and had fewer relevant concepts. We constructed a fourth set of concept documents this way, and retained all concepts identified within each sentence (i.e. we did not filter based on the concept scores).</p><p>We will refer to this version as UMLS-CUI-sen.</p><p>Once the four versions of the concept documents are obtained, we build the four corresponding UMLS-CUI indexes using Indri.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Query Modification</head><p>Next, in the query modification stage, we constructed effective queries based on the "summaries" narrative and the query type. Since we built two kinds of indexes, we constructed two kinds of queries, correspondingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Queries for free-text index</head><p>We investigated and evaluated several approaches and configurations to construct keyword queries for the free-text index. First, using the Indri query language construct #syn to indicate synonyms, we expanded the question types thus:</p><formula xml:id="formula_0" coords="4,67.12,369.12,199.77,29.14">• Diagnosis ⇒ #syn(Diagnosis, Dx) • Test ⇒ #syn(Test, Examination) • Treatment ⇒ #syn(Treatment, Drug, Therapy)</formula><p>Next, we looked at the "summary" narratives in the given case reports. We constructed one query based on just the summary and another query after removing common stopwords. Finally, the queries were shown to a domain expert, who provided us useful recommendations and highlighted important terms in each case report. This input guided us to construct a set of manual queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Queries for UMLS-CUI indexes</head><p>We created three versions of queries for UMLS-CUI indexes as follows:</p><p>1. A query with summary words and the type information. 2. Based on the inputs from the domain expert who highlighted important terms in the summaries, we constructed queries using the CUIs of the highlighted terms as well as the MetaMapped type information. 3. We used an in-house repository of empirical synonyms for medical concepts. This synonym set was collected based on frequent query rewrites in EMERSE, an Electronic Medical Record Search Engine. <ref type="bibr" coords="4,234.06,629.68,13.51,8.21" target="#b13">[13]</ref>. We identified key terms in summaries and expand their CUIs automatically using the empirical synonym set.</p><p>We ran the generated queries generated against different Indri indexes to get an ensemble of single ranked list results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Retrieval</head><p>In the retrieval phase, we explored different configurations of pseudo-relevance feedback and parameter settings for whether to include the query type information and whether to remove stopwords. We tested and evaluated the default setting of pseudo-relevance feedback and tuned three parameters, viz. the number of feedback documents (fbDocs), the number of feedback terms added to the query (fbTerms), and the weight assigned to the original query in contrast to the feedback terms (fbOrigWeight). This way, we created twelve individual ranking configurations that are listed in Table <ref type="table" coords="4,342.15,162.40,3.58,8.21" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Learning to Rank Model</head><p>Once the individual ranked lists were available, we set forth to combine them using a Learning to Rank framework. Learning to Rank refers to a set of machine learning techniques using to train a model to combine an ensemble of ranked lists to improve the overall ranking performance <ref type="bibr" coords="4,517.11,239.04,9.21,8.21" target="#b6">[6]</ref>. Since the TREC 2015 CDS track was similar to the previous year's track, we could utilize the annotated list of relevant documents (in the qrel file) to design and learn a supervised learning to rank model and determine the weights of different retrieval configurations. We used the RankLib library<ref type="foot" coords="4,551.79,289.92,3.65,3.71" target="#foot_2">4</ref> to train a learning to rank model. RankLib is part of the Lemur project and supports eight popular machine learning algorithms, implements many retrieval metrics, and provides multiple ways to carry out the evaluation. We first split the case reports of each type from TREC 2014 CDS track randomly into training and test sets, and compared several models using different learning algorithms. Some of the techniques we tried included Multiple Additive Regression Trees (MART), also known as Gradient boosted regression trees, AdaRank, LambdaMART, and Random Forest. The performance of Random Forest was found to be slightly better and more robust when evaluated using 5-fold cross-validation experiments. We chose Random Forests to implement our final learning to rank model.</p><p>The goal of the second task (Task B) was to investigate if availability of the diagnosis information can help retrieve more relevant articles in the test and treatment related queries. Unlike the first task, the second task was new and, hence, did not have any training data to train learning to rank models. However, we observed that most diagnosis information only contained one or two words. So, we assumed that adding diagnosis terms would not significantly affect the weights for the learning to rank models trained for the first task. Hence, although we constructed new queries using the diagnosis information, we used the same set of individual ranker configurations for both tasks and did not update the learning to rank models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND DISCUSSION</head><p>We now describe and discuss the performance of our six submitted runs over the two tasks, along with additional analysis of the best performing retrieval configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Description of submission runs</head><p>Each participating team was allowed to submit at most three runs to each of the two tasks. All our submissions except the last submission for Task B were based on the same Random Forest re-ranking model; and the submissions differed in the subset of ranking lists used in the learning to rank model ensembles.</p><p>The detailed descriptions of our official submissions are listed below. The numbers refer to the row numbers in Table <ref type="table" coords="5,278.81,412.46,3.58,8.21" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task A</head><p>1. FusionAuto: This submission run ensembles all individual ranking lists that are automatically produced; specifically #1-4 (Text 0/1/2/3 ) and #7-9 (MetaMap 0/1/sen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FusionManual : This submission run ensembles all</head><p>individual ranking lists that involved some manual input, and two automatically produced ranked list; specifically #3-4 (Text 2/3 ), #5-6 (Manual 0/1 ), and #10-12 (MetaMap Manual 0/1/sen). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>The performance evaluations of our runs and the median results among submitted TREC runs in Tasks A and B are shown in Table <ref type="table" coords="5,380.14,669.50,4.60,8.21" target="#tab_2">2</ref> and Table <ref type="table" coords="5,430.36,669.50,3.58,8.21" target="#tab_3">3</ref>, respectively. The best values for each of the four metrics (viz. infAP, infNDCG, R-prec, and Precision@10) are highlighted in boldface. In addition to the submitted runs, we also evaluated the performance of the individual rankers used in the two tasks. The re- From the performance evaluation of our submissions, we found that our submissions are better than the median in all four metrics, suggesting that our approaches of index improvement, query modification, and re-ranking with ensembles are potentially useful in ranking articles for the clinical decision support task. The comparison between our submissions also verified some of our intuitions that the manual inputs from domain expert would improve the results significantly, and that including the diagnosis results could improve the results of test and treatment related queries.</p><p>By comparing individual rankings, we found that the best performing configurations all included the type information and removed stopwords. We also noticed that the results of best ranking configuration based on only the text information, BIR-Text, outperformed the results of FusionAuto and FusionAutoB, while the best ranking configuration using MetaMap, BIR-MetaMap, performed relatively poorly. This suggests that further investigation is required on the fusion algorithm to incorporate the MetaMap information more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Future work</head><p>We would like to develop a clinical decision support system that could retrieve relevant information in a fully automated fashion. In the submitted runs, we used inputs from a domain expert to alter the queries and it helped improve our results significantly. Our next step is to generate a set of rules to select keywords automatically, based on analyzing the nature of input we received from the domain expert. We want to explore the ReQ-ReC double loop system developed by Li et al. <ref type="bibr" coords="6,137.99,700.88,9.72,8.21">[7]</ref> as a potential framework to do so. Further, on comparing the results of FusionManB and Fu- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we described the participation of the University of Michigan in the TREC 2015 Clinical Decision Support track. We built an information retrieval system to retrieve biomedical articles for clinical decision support queries and investigated three major approaches to improve the retrieval performance, namely improving indexes, modifying queries, and re-ranking ensembles of ranked list results. The performance evaluations indicated that our submissions performed significantly better than the reported median performance.</p><p>In future, we plan to continue our investigation on query modifications and re-ranking ensembles to improve clinical decision support and enable improved patient care.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,53.80,54.10,501.83,218.04"><head></head><label></label><figDesc></figDesc><graphic coords="2,53.80,54.10,501.83,218.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,53.80,55.15,497.21,283.08"><head>Table 1 :</head><label>1</label><figDesc>Individual Ranking Configurations</figDesc><table coords="5,59.18,78.30,486.52,214.12"><row><cell>#</cell><cell>RunID</cell><cell>Source 1</cell><cell>TypeInfo</cell><cell cols="4">Pseudo Relevance Feedback 2 Stopword</cell><cell>Index</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">#Docs #Terms</cell><cell>Weight</cell><cell>Removal</cell></row><row><cell>1 Text 0</cell><cell></cell><cell>summary</cell><cell>Yes</cell><cell>20</cell><cell>10</cell><cell>0.5</cell><cell>No</cell><cell>Text</cell></row><row><cell>2 Text 1</cell><cell></cell><cell>summary</cell><cell>No</cell><cell>20</cell><cell>10</cell><cell>0.5</cell><cell>No</cell><cell>Text</cell></row><row><cell>3 Text 2</cell><cell></cell><cell>summary</cell><cell>No</cell><cell>20</cell><cell>10</cell><cell>0.5</cell><cell>Yes</cell><cell>Text</cell></row><row><cell>4 Text 3</cell><cell></cell><cell>summary</cell><cell>Yes</cell><cell>20</cell><cell>10</cell><cell>0.5</cell><cell>Yes</cell><cell>Text</cell></row><row><cell cols="2">5 Manual 0</cell><cell>highlighted terms</cell><cell>Yes</cell><cell>10</cell><cell>5</cell><cell>0.5</cell><cell>Yes</cell><cell>Text</cell></row><row><cell cols="2">6 Manual 1</cell><cell>highlighted terms</cell><cell>Yes</cell><cell>10</cell><cell>5</cell><cell>0.67</cell><cell>Yes</cell><cell>Text</cell></row><row><cell cols="2">7 MetaMap 0</cell><cell>EMERSE</cell><cell>Yes</cell><cell>10</cell><cell>5</cell><cell>0.5</cell><cell>Yes</cell><cell>UMLS-CUI-700</cell></row><row><cell cols="2">8 MetaMap 1</cell><cell>EMERSE</cell><cell>Yes</cell><cell>10</cell><cell>5</cell><cell>0.5</cell><cell>Yes</cell><cell>UMLS-CUI-all</cell></row><row><cell cols="2">9 MetaMap sent</cell><cell>EMERSE</cell><cell>Yes</cell><cell>10</cell><cell>5</cell><cell>0.5</cell><cell>Yes</cell><cell>UMLS-CUI-sen</cell></row><row><cell cols="2">10 MetaMap Manual 0</cell><cell>highlighted CUIs</cell><cell>Yes</cell><cell>10</cell><cell>5</cell><cell>0.5</cell><cell>Yes</cell><cell>UMLS-CUI-700</cell></row><row><cell cols="2">11 MetaMap Manual 1</cell><cell>highlighted CUIs</cell><cell>Yes</cell><cell>10</cell><cell>5</cell><cell>0.5</cell><cell>Yes</cell><cell>UMLS-CUI-all</cell></row><row><cell cols="3">12 MetaMap Manual sen highlighted CUIs</cell><cell>Yes</cell><cell>10</cell><cell>5</cell><cell>0.5</cell><cell>Yes</cell><cell>UMLS-CUI-sen</cell></row></table><note coords="5,56.14,302.94,3.65,3.71;5,63.36,304.03,487.66,7.30;5,53.80,313.00,422.67,7.30;5,56.14,320.87,3.65,3.71;5,63.99,321.97,487.01,7.30;5,53.80,330.93,273.49,7.30"><p><p>1 </p>summary: summary field of a topic; highlighted terms: terms highlighted by a domain expert; EMERSE: CUIs suggested by an Electronic Medical Record Search Engine; highlighted CUIs: CUIs of terms highlighted by a domain expert. 2 #Docs: the number of feedback documents (fbDocs); #Terms: the number of feedback terms added to the query (fbTerms); Weight: the weight assigned to the original query terms (fbOrigWeight).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,53.80,360.16,502.16,359.39"><head></head><label></label><figDesc>3. FusionMAll : This submission ensembles all individual ranking lists we created (#1-12).</figDesc><table coords="5,53.80,360.16,502.16,359.39"><row><cell></cell><cell>model to create the final ranking list. The submis-</cell></row><row><cell></cell><cell>sion run ensembles all individual ranking lists that in-</cell></row><row><cell></cell><cell>volved manual input, and two automatically produced</cell></row><row><cell></cell><cell>ranked lists; specifically #3-4 (Text 2/3 ), #5-6 (Man-</cell></row><row><cell></cell><cell>ual 0/1 ), and #10-12 (MetaMap Manual 0/1/sen).</cell></row><row><cell></cell><cell>3. FusionAdv : This submission run is generated based</cell></row><row><cell></cell><cell>on the intuition that adding diagnosis results into test</cell></row><row><cell></cell><cell>and treatment queries may lead to the results being</cell></row><row><cell></cell><cell>more relevant to answering diagnosis clinical question.</cell></row><row><cell></cell><cell>So, we constructed three different ranking lists: first,</cell></row><row><cell></cell><cell>the FusionManB run; second, the Notype run, an en-</cell></row><row><cell></cell><cell>semble of all individual ranking lists that did not in-</cell></row><row><cell></cell><cell>clude the type information; and third, the Diagnosis</cell></row><row><cell></cell><cell>run, generated by changing all type information to Di-</cell></row><row><cell></cell><cell>agnosis and then merging the ranked lists.</cell></row><row><cell></cell><cell>Every article in the three ranking lists was assigned</cell></row><row><cell></cell><cell>a ranking score, formulated as score = 1001 -rank,</cell></row><row><cell></cell><cell>based on the article's rank in that ranking list. In other</cell></row><row><cell></cell><cell>words, the top ranked article got a score of 1000, the</cell></row><row><cell></cell><cell>next ranked article a score of 999, and so on. Articles</cell></row><row><cell></cell><cell>that did not show up in a ranking list received a score</cell></row><row><cell>Task B</cell><cell>of 0 for that list. The final score was computed as:</cell></row><row><cell>1. FusionAutoB : This submission run is generated based</cell><cell>score = 0.8 × scoreF usionM anB + 0.4 × scoreNotype</cell></row><row><cell>on the FusionAuto run of Task A. We add the text and UMLS-CUIs of diagnosis results into test and treat-</cell><cell>-0.2 × scoreDiagnosis</cell></row><row><cell>ment queries, and apply the same learning to rank</cell><cell></cell></row><row><cell>model to create the final ranking list. The submission</cell><cell></cell></row><row><cell>run ensembles all individual ranking lists that are auto-</cell><cell></cell></row><row><cell>matically produced; specifically #1-4 (Text 0/1/2/3 )</cell><cell></cell></row><row><cell>and #7-9 (MetaMap 0/1/sen).</cell><cell></cell></row><row><cell>2. FusionManB : This submission run is generated based</cell><cell></cell></row><row><cell>on the FusionManual run of Task A. We add the text</cell><cell></cell></row><row><cell>and UMLS-CUIs of diagnosis results into test and treat-</cell><cell></cell></row><row><cell>ment queries, and apply the same learning to rank</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,53.80,55.15,239.18,302.18"><head>Table 2 :</head><label>2</label><figDesc>Performance evaluation of Task A</figDesc><table coords="6,53.80,78.82,239.18,278.51"><row><cell></cell><cell>infAP</cell><cell cols="2">infNDCG R-prec P@10</cell></row><row><cell>FusionAuto</cell><cell>0.0641</cell><cell>0.2445</cell><cell>0.1937 0.3733</cell></row><row><cell>FusionManual</cell><cell>0.0738</cell><cell>0.2815</cell><cell>0.2256 0.46</cell></row><row><cell>FusionMAll</cell><cell>0.0816</cell><cell>0.2954</cell><cell>0.2246 0.47</cell></row><row><cell>Median(Auto)</cell><cell>0.0414</cell><cell>0.2038</cell><cell>0.1615 0.3433</cell></row><row><cell cols="2">Median(Manual) 0.0499</cell><cell>0.2402</cell><cell>0.1735 0.3833</cell></row><row><cell>BIR-Text</cell><cell>0.0726</cell><cell>0.2614</cell><cell>0.2021 0.4</cell></row><row><cell>BIR-Manual</cell><cell>0.0787</cell><cell>0.2867</cell><cell>0.2205 0.4367</cell></row><row><cell>BIR-MetaMap</cell><cell>0.0244</cell><cell>0.1197</cell><cell>0.0996 0.1833</cell></row><row><cell cols="4">sult tables also show the best performance obtained by the</cell></row><row><cell cols="4">individual rankers of the following three types:</cell></row><row><cell cols="4">• BIR-Text: The best individual ranking configuration</cell></row><row><cell cols="4">among the automatically generated ones using only the</cell></row><row><cell cols="4">text information; specifically #1-4 (Text 0/1/2/3 )</cell></row><row><cell cols="4">• BIR-Manual: The best individual ranking configura-</cell></row><row><cell cols="4">tion among those that use only the text information,</cell></row><row><cell cols="4">but involve some manual input from the domain ex-</cell></row><row><cell cols="4">pert; specifically #5-6 (Manual 0/1 )</cell></row><row><cell cols="4">• BIR-MetaMap: The best individual ranking config-</cell></row><row><cell cols="4">uration among those that are automatically generated</cell></row><row><cell cols="4">using only MetaMap information; specifically #7-9</cell></row><row><cell cols="2">(MetaMap 0/1/sen)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,316.81,55.15,239.15,204.16"><head>Table 3 :</head><label>3</label><figDesc>Performance evaluation of Task B we found the results are comparable after adding the ranked lists for Notype and Diagnosis, even with manually set combination weights in FusionAdv. This suggests that additional research is needed to understand the typespecific ranking and learn the optimal combination weights.</figDesc><table coords="6,316.81,78.82,234.62,138.65"><row><cell></cell><cell>infAP</cell><cell cols="2">infNDCG R-prec P@10</cell></row><row><cell>FusionAutoB</cell><cell>0.0841</cell><cell>0.3298</cell><cell>0.2538 0.4633</cell></row><row><cell>FusionManB</cell><cell>0.0955</cell><cell>0.3535</cell><cell>0.2767 0.5233</cell></row><row><cell>FusionAdv</cell><cell>0.0951</cell><cell>0.3473</cell><cell>0.2731 0.53</cell></row><row><cell>Median(Auto)</cell><cell>0.0633</cell><cell>0.2794</cell><cell>0.2123 0.45</cell></row><row><cell cols="2">Median(Manual) 0.0666</cell><cell>0.2899</cell><cell>0.2035 0.4733</cell></row><row><cell>BIR-Text</cell><cell>0.0945</cell><cell>0.3453</cell><cell>0.2589 0.5</cell></row><row><cell>BIR-Manual</cell><cell>0.0985</cell><cell>0.3516</cell><cell>0.2618 0.51</cell></row><row><cell>BIR-MetaMap</cell><cell>0.0327</cell><cell>0.1505</cell><cell>0.1213 0.2333</cell></row><row><cell>sionAdv,</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,321.42,701.01,210.35,8.21"><p>https://metamap.nlm.nih.gov/Docs/2012 MMO.pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,321.42,711.34,159.60,8.21"><p>http://www.lemurproject.org/indri.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,321.42,711.34,169.04,8.21"><p>http://www.lemurproject.org/ranklib.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">ACKNOWLEDGMENTS</head><p>The authors acknowledge the timely contributions and assistance of the domain expert, <rs type="person">Hanqi Tang</rs>, from <rs type="funder">Tsinghua University</rs>, during our participation in the TREC 2015 Clinical Decision Support track.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.29,486.30,96.82,9.82" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,498.14,195.10,8.21;6,335.60,508.59,209.52,8.21;6,335.60,519.06,214.55,8.21;6,335.60,529.52,184.39,8.21" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,487.93,498.14,42.77,8.21;6,335.60,508.59,43.10,8.21">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,397.56,508.59,147.56,8.21;6,335.60,519.06,214.55,8.21;6,335.60,529.52,93.85,8.21">Proceedings of the 24th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 24th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.61,540.98,200.82,8.21;6,335.60,551.43,68.52,8.21" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,392.91,540.98,61.91,8.21">Random forests</title>
		<author>
			<persName coords=""><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,461.43,540.98,70.82,8.21">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,562.89,199.02,8.21;6,335.60,573.36,212.56,8.21;6,335.60,583.82,188.75,8.21;6,335.60,594.27,157.00,8.21" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,471.72,562.89,62.91,8.21;6,335.60,573.36,212.56,8.21;6,335.60,583.82,15.39,8.21">SNUMedinfo at TREC CDS track 2014: Medical case-based retrieval task</title>
		<author>
			<persName coords=""><forename type="first">Sungbin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinwook</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,369.39,583.82,154.97,8.21;6,335.60,594.27,127.80,8.21">Proceedings of the 23rd Text REtrieval Conference Proceedings (TREC)</title>
		<meeting>the 23rd Text REtrieval Conference Proceedings (TREC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,605.73,209.84,8.21;6,335.60,616.20,209.72,8.21;6,335.60,626.66,209.21,8.21;6,335.60,637.11,182.73,8.21;6,335.60,647.58,198.49,8.21" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,356.89,616.20,188.44,8.21;6,335.60,626.66,209.21,8.21;6,335.60,637.11,50.69,8.21">UCLA at TREC 2014 clinical decision support track: Exploring language models, query expansion, and boosting</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Garcia-Gathright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,404.89,637.11,113.44,8.21;6,335.60,647.58,169.29,8.21">Proceedings of the 23rd Text REtrieval Conference Proceedings (TREC)</title>
		<meeting>the 23rd Text REtrieval Conference Proceedings (TREC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,659.04,198.21,8.21;6,335.60,669.50,187.47,8.21;6,335.60,679.95,185.55,8.21;6,335.60,690.42,203.04,8.21;6,335.60,700.88,192.56,8.21;6,335.60,711.34,157.00,8.21" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,335.60,669.50,187.47,8.21;6,335.60,679.95,185.55,8.21;6,335.60,690.42,203.04,8.21;6,335.60,700.88,19.21,8.21">Full-texts representation with Medical Subject Headings, and co-citations network re-ranking strategies for TREC 2014 clinical decision support track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gaudinat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pasche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,373.21,700.88,154.96,8.21;6,335.60,711.34,127.80,8.21">Proceedings of the 23rd Text REtrieval Conference Proceedings (TREC)</title>
		<meeting>the 23rd Text REtrieval Conference Proceedings (TREC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,57.79,202.97,8.21;7,72.59,68.25,197.12,8.21;7,72.59,78.71,96.11,8.21" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,110.81,57.79,160.61,8.21">A short introduction to learning to rank</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Hang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,72.59,68.25,192.81,8.21">IEEE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1854" to="1862" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,90.17,219.04,8.21;7,72.59,100.62,220.33,8.21;7,72.59,111.09,205.50,8.21;7,72.59,121.55,217.22,8.21;7,72.59,132.01,212.78,8.21;7,72.59,142.47,20.99,8.21" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,72.59,100.62,220.33,8.21;7,72.59,111.09,95.07,8.21">ReQ-ReC: High recall retrieval with query pooling and interactive classification</title>
		<author>
			<persName coords=""><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,185.70,111.09,92.40,8.21;7,72.59,121.55,217.22,8.21;7,72.59,132.01,146.26,8.21">Proceedings of the 37th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,153.93,214.17,8.21;7,72.59,164.39,208.80,8.21;7,72.59,174.85,192.55,8.21;7,72.59,185.31,156.99,8.21" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,72.59,164.39,208.80,8.21;7,72.59,174.85,19.21,8.21">NovaSearch at TREC 2014 clinical decision support track</title>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Mourao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Flávio</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joao</forename><surname>Magalhaes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,110.19,174.85,154.96,8.21;7,72.59,185.31,127.79,8.21">Proceedings of the 23rd Text REtrieval Conference Proceedings (TREC)</title>
		<meeting>the 23rd Text REtrieval Conference Proceedings (TREC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,196.77,215.16,8.21;7,72.59,207.23,186.74,8.21;7,72.59,217.69,217.04,8.21;7,72.59,228.15,218.60,8.21;7,72.59,238.61,20.99,8.21" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Kaveh</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rick</forename><surname>Ratliff</surname></persName>
		</author>
		<ptr target="https://www.accenture.com/us-en/insight-healthcare-technology-vision-2015.aspx" />
		<title level="m" coord="7,198.39,196.77,89.37,8.21;7,72.59,207.23,158.63,8.21">Healthcare IT Trends. Accenture Healthcare Technology Vision</title>
		<imprint>
			<date type="published" when="2015-01-31">2015. January 31, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,250.07,218.17,8.21;7,72.59,260.53,220.35,8.21;7,72.59,270.99,213.72,8.21;7,72.59,281.45,210.73,8.21;7,72.59,291.91,128.28,8.21" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,247.48,250.07,43.28,8.21;7,72.59,260.53,84.21,8.21">Healthcare IT Pain and Progress</title>
		<author>
			<persName coords=""><forename type="first">Kaveh</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rick</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kip</forename><surname>Webb</surname></persName>
		</author>
		<ptr target="https://www.accenture.com/us-en/insight-accenture-doctors-survey-2015-healthcare-it-pain-progress.aspx" />
	</analytic>
	<monogr>
		<title level="s" coord="7,163.67,260.53,101.20,8.21">Accenture Doctors Survey</title>
		<imprint>
			<date type="published" when="2015-01-31">2015. January 31, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.60,303.37,206.04,8.21;7,72.59,313.82,217.71,8.21;7,72.59,324.29,213.99,8.21;7,72.59,334.75,215.96,8.21;7,72.59,345.21,20.99,8.21" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,194.06,313.82,96.25,8.21;7,72.59,324.29,123.57,8.21">Query reformulation for clinical decision support search</title>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,214.64,324.29,71.95,8.21;7,72.59,334.75,210.78,8.21">Proceedings of the 23rd Text REtrieval Conference Proceedings (TREC)</title>
		<meeting>the 23rd Text REtrieval Conference Proceedings (TREC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,356.67,183.87,8.21;7,72.59,367.13,220.32,8.21;7,72.59,377.59,217.47,8.21;7,72.59,388.05,191.73,8.21;7,72.59,398.52,60.24,8.21" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,144.80,367.13,148.11,8.21;7,72.59,377.59,142.50,8.21">Query modification through external sources to support clinical decisions</title>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jannifer</forename><surname>Hiu-Kwan Man</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ting-Fung</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,233.40,377.59,56.67,8.21;7,72.59,388.05,191.73,8.21;7,72.59,398.52,31.04,8.21">Proceedings of the 23rd Text REtrieval Conference Proceedings (TREC)</title>
		<meeting>the 23rd Text REtrieval Conference Proceedings (TREC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,409.97,205.74,8.21;7,72.59,420.43,199.25,8.21;7,72.59,430.89,215.98,8.21;7,72.59,441.35,206.74,8.21;7,72.59,451.81,197.23,8.21;7,72.59,462.27,53.09,8.21" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,177.41,420.43,94.44,8.21;7,72.59,430.89,215.98,8.21;7,72.59,441.35,91.98,8.21">Towards intelligent and socially oriented query recommendation for electronic health records retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Danny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Hanauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiaozhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,182.67,441.35,96.66,8.21;7,72.59,451.81,197.23,8.21;7,72.59,462.27,24.20,8.21">Proceedings of the ACM SIGIR workshop on Health Search and Discovery (HSD)</title>
		<meeting>the ACM SIGIR workshop on Health Search and Discovery (HSD)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,473.73,191.85,8.21;7,72.59,484.19,196.65,8.21;7,72.59,494.65,196.37,8.21;7,72.59,505.11,198.49,8.21" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,72.59,484.19,196.65,8.21;7,72.59,494.65,64.21,8.21">HLTCOE at TREC 2014: Microblog and clinical decision support</title>
		<author>
			<persName coords=""><forename type="first">Tan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,155.52,494.65,113.44,8.21;7,72.59,505.11,169.28,8.21">Proceedings of the 23rd Text REtrieval Conference Proceedings (TREC)</title>
		<meeting>the 23rd Text REtrieval Conference Proceedings (TREC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
