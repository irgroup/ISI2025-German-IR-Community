<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,129.13,89.78,366.91,5.05;1,123.90,107.95,377.37,5.05">University of Lugano at TREC 2015: Contextual Suggestion and Temporal Summarization Tracks</title>
				<funder>
					<orgName type="full">RelMobIR and OpiTrack projects of the Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,147.83,139.85,114.52,7.82"><roleName>Seyed</roleName><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.14,139.85,52.67,7.82"><forename type="first">Ali</forename><surname>Bahrainian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.50,139.85,75.76,7.82"><forename type="first">Anastasia</forename><surname>Giachanou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,423.51,139.85,53.82,7.82"><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,129.13,89.78,366.91,5.05;1,123.90,107.95,377.37,5.05">University of Lugano at TREC 2015: Contextual Suggestion and Temporal Summarization Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">088E728238104A5C1BA99F5115514E8F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>contextual suggestion</term>
					<term>user modeling</term>
					<term>SVM</term>
					<term>temporal summarization</term>
					<term>TREC</term>
					<term>DFR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This technical report presents the work of the University of Lugano at TREC 2015 Contextual Suggestion and Temporal Summarization tracks. The first track that we report on, is the Contextual Suggestion. The goal of the Contextual Suggestion track is to develop systems that could generate user-specific suggestions that a user might potentially like. Our proposed method attempts to model the users' behavior and interest using a classifier, and enrich the basic model using additional data sources. Our results illustrate that our proposed method performed very well in terms of all used evaluation metrics. The second track that we report on, is the Temporal Summarization that aims to develop systems that can detect useful, new, and timely updates about a certain event. Our proposed method selects sentences that are relevant and novel to a specific event with the aim to create a summary for this event. The results showed that the proposed method is very e↵ective in terms of Latency Comprehensiveness (LC). However, the approach did not manage to obtain a good performance in terms of Expected Latency Gain (ELG).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This paper describes the participation of the University of Lugano (USI) at TREC 2015 Contextual Suggestion 1 <ref type="bibr" coords="1,282.14,472.76,8.86,7.82" target="#b3">[3]</ref> as well as the Temporal Summarization 2 track. This year's Contextual Suggestion track consisted of two tasks, namely, Task 1: Live Experiment and Task 2: Batch Experiment. We participated in the later task. In this task, for each user contexts and profiles are defined. Each profile consists of 60 places and the user's opinion regarding them. Additionally, We were provided with a list of 30 candidate sug-1 https://sites.google.com/site/treccontext/ 2 http://www.trec-ts.org/ gestions and our task was to rank them. To address this task, we crawled all the main sources of information (i.e. Yelp, Foursquare, and TripAdvisor) to possibly build the most comprehensive dataset of attractions. We then built user model using machine learning classification. Furthermore, we enriched the basic model by combining simple yet e↵ective additional measures with the base user model.</p><p>For this year's Temporal Summarization track, two tasks were defined: Task 1: Filtering and Summarization and Task 2: Summarization Only. We participated in the first task. For this task we were provided with high-volume streams of news articles and blog posts crawled from the Web regarding a set of specific events. The aim of the task was to process the streams in time order, filter out irrelevant content and then select the appropriate sentences that could summarize each event over time. The sentences were to be selected on the basis of being relevant, novel and important regarding a given event <ref type="bibr" coords="1,416.38,381.20,8.39,7.82" target="#b2">[2]</ref>. To address the task, we applied the Divergence From Randomness (DFR) Framework and particular InL2 model proposed by Amati and Van Rijsbergen <ref type="bibr" coords="1,356.81,409.83,8.86,7.82" target="#b1">[1]</ref> to calculate the relevance of a sentence given a topic. We also used query expansion to address the problem of word mismatch. To calculate the novelty of a candidate sentence, we leveraged the number of unique terms between the sentence and the summary already produced. We also considered di↵erent approaches to determine the number of sentences selected each time. We explored three di↵erent approaches: increasing the number of sentences as time passes, decreasing the number of sentences as time passes and selecting a stable number of sentences as time passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CONTEXTUAL SUGGESTION</head><p>In this task we modeled users with making use of Yelp data and we further enriched these models using data from other data sources. We used classifiers to model users' behavior and we enriched user models by combining other measures. Our system generally consists of four modules:</p><p>• Information gathering</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• User modeling</head><p>• User model enrichment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Suggestion ranking</head><p>Our system execution cycle starts with the data collection module. This module collects data such that data from the most important data sources, namely, Yelp, Foursquare, and TripAdvisor are gathered. Using the gathered data, the user modeling module, creates a basic user model per user. Furthermore, the model enrichment module aims at improving the user models by fusing additional measures such as user category profile and Foursquare taste tag models with the basic models. Subsequently, by using the enriched models the suggestion ranking module ranks all candidate places. In the following subsections we elaborate on each component in greater detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Information Gathering</head><p>Considering that we participated in the batch experiments, we were provided with users' history and the attractions to be ranked. Thus, the number of links presenting attractions that we had to process was reduced to virtually 9000. Since, the task of suggesting places per each given user was limited to 30 attractions, it was crucial not to miss any information relevant to the target attractions. Additionally, although almost half of the URLs was from known sources such as Yelp and Foursquare, another half of the URLs we were provided, was from less known websites (e.g. the places' o cial web pages). Consequently, e↵ort was made to find the corresponding profiles of these places on Yelp, Foursquare, and TripAdvisor. To collect data we performed the following steps:</p><p>1. We discard the attractions that users assigned them a score of ' 1' or '2'. This is due to the fact that these places either weren't assigned any ratings or their rating was neutral, thus insignificant.</p><p>2. We detect and discard broken links.</p><p>3. We download the links from the known sources, namely, Yelp, Foursquare, and TripAdvisor.</p><p>4. For each attraction on each of the above-mentioned sources we find the corresponding profiles on the other two sources. (e.g. for a given Yelp profile, we find its corresponding profiles on Foursquare and TripAdvisor).</p><p>5. For the other attractions with unknown links, we download the web pages, analyze their contents to find their corresponding profiles on the three above-mentioned websites.</p><p>Following the above steps, we crawled homogeneous data for virtually all given attractions from the three websites. The final layout of our dataset is as follows:</p><p>• Yelp </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">User Modeling</head><p>We model each user by training a classifier using example suggestions. Our intuition is that a user's opinion regarding an attraction could be learned based on the opinions of other users who gave the same rating as the target user to the same attraction. To train a classifier per user we extract negative and positive samples as explained in the following:</p><p>• Positive samples: We elicit the positive reviews of positive example suggestions.</p><p>• Negative samples: Likewise, we elicit the negative reviews of negative example suggestions.</p><p>We define positive example suggestions as the attractions which a user rated 3 or 4. Positive reviews are the reviews whose corresponding ratings are 3 or 4. Analogously, negative example suggestions and reviews are defined, taking ratings of 0 and 1 as negative.</p><p>We use Support Vector Machine (SVM) and Naive Bayes (NB) as the classifiers of our choice. We train these classifiers using TF-IDF measures as feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">User Model Enrichment</head><p>This module takes as input the created user model and further enriches them using the following algorithm:</p><p>1. For each user and for each place we create an index of all the categories (e.g. Italian Restaurant), based on Yelp data, he/she has referred to as positive or negative.</p><p>2. Then we compute the normalized count per category and add it to positive or negative user models.</p><p>3. By having all normalized counts per category for both the positive and negative models for all places, we compute two negative and positive vectors which represent the user's interests in various categories.</p><p>4. Finally, for a given place in which we would like to predict the level of user's interest, we sum up all the normalized scores for all the existing categories for the place (UI). The result would be a score between ' 1' and '+1'</p><p>We follow the above procedure to compute a similar interest score based on the taste tags regarding each place on Foursquare (UF ). Likewise, a third score is computed based on the categories of TripAdvisor (UT ).</p><p>Foursquare taste tags are special terms extracted from users' tips 3 and are very informative. For example, 'Central Park' in 'New York' is described by these taste terms: picnics, biking, trails, park, scenic views, etc. These terms are very informative and often express characteristics of an attraction. Therefore, they can be considered as categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Suggestion Ranking</head><p>We estimate the similarity between each user and a candidate suggestion using the equation below:</p><formula xml:id="formula_0" coords="3,110.57,325.12,191.10,20.09">Similarity(u, p) = !1UC(u, p) + !2UI(u, p)+ !3UF (u, p) + !4UT (u, p)<label>(1)</label></formula><p>Where !1...4 are the weights assigned to these scores, u is a given user, p is a given attraction, and UC is the confidence score of the classifier which models a user. To find the optimum setting for the weights associated with each score, we conducted a 5-fold cross validation, for which the best setting is:</p><formula xml:id="formula_1" coords="3,122.50,425.77,140.23,7.82">!1 = 1 !2 = 1 !3 = 0.3 !4 = 0.3</formula><p>We rank the candidate suggestions according to the similarity measure computed by this module. The higher the similarity score, the higher the rank would be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Experimental Results</head><p>By applying our method to our gathered dataset, we submitted two runs: '11 ' and '22 '. The two runs di↵er from one another merely in that, run '11 ' utilizes an SVM classifier while '22 ' is based on a NB classifier. The best sets of configuration parameters for the classifiers and equation ( <ref type="formula" coords="3,294.53,541.01,3.57,7.82" target="#formula_0">1</ref>) were obtained based on a 5-fold cross validation.</p><p>In this year's task, we were given 211 profile/context pairs. For each pair a user's history was 60 places and the number of candidate suggestions to be ranked were 30. How interesting a website is, has a scale from '0' up to '4', with '0' being the least interesting, '2' neutral, and '4' the most interesting. Track organizers evaluated all submitted runs using two evaluation metrics, namely, P@5 (precision at 5) and MRR (mean reciprocal rank). In this evaluation, a suggestion is relevant if it is rated 3 or 4 by user.</p><p>3 Tips on Foursquare are short reviews written by users.  <ref type="table" coords="3,347.59,139.01,4.20,7.82" target="#tab_0">1</ref> demonstrates the overall average performances of our runs. It could be seen that both of our runs outperform the median performance of all submitted runs by other contestants, which confirms the e↵ectiveness of the user modeling and enrichment method we proposed in this work. However, our run '11' performs better than the other, which proves that in this task, an SVM classifier is a more powerful tool for building the basic user model.</p><p>Furthermore, the optimal parameter set, !1...4 indicates that in our system the basic user model (UC) and category model (UI) are more important, since their corresponding weights are higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TEMPORAL SUMMARIZATION</head><p>In recent years, temporal summarizing received a lot of attention in the research community. To address this problem, a method should address three sub-problems: (a) relevance of sentences by selecting those that are the most relevant given an event, (b) novelty by selecting the sentences that contain novel content and (b) importance by selecting the sentences that a person would put into a summary. In terms of relevance towards a given event, any IR relevance model can be used. Previous studies have applied di↵erent techniques including BM25 and vector space model <ref type="bibr" coords="3,500.65,373.30,8.39,7.82" target="#b8">[8]</ref>. To calculate the novelty of a sentence, researchers usually measure the text similarity between the candidate sentence and the summary already produced and remove those that are too similar <ref type="bibr" coords="3,350.59,411.47,8.39,7.82" target="#b4">[4]</ref>. Calculating importance is more challenging than calculating relevance and novelty. In terms of importance, Xu et al. <ref type="bibr" coords="3,357.28,430.56,8.85,7.82" target="#b7">[7]</ref> proposed a language model from named entities to measure the sentence's topical salience.</p><p>Another important challenge in the temporal summarization is the number of sentences to select from each time interval. Most of the approaches in the literature, select a stable number of sentences as time passes. Instead, McCreadie et al. <ref type="bibr" coords="3,532.75,487.82,8.86,7.82">[5]</ref> considered to adjust the number of the selected sentences as time passes. This idea is based to the fact that as time passes, the need for new sentences is not the same. The results showed that this approach performs well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methodology</head><p>When a user submits a query to a temporal summarization system, it aims to identify the most relevant, novel and important sentences. In this section, we present our methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Relevance</head><p>In order to rank the sentences by relevance, we applied the Divergence From Randomness (DFR) Framework proposed by Amati and Van Rijsbergen <ref type="bibr" coords="3,436.23,641.49,8.39,7.82" target="#b1">[1]</ref>. In particular, we applied InL2 model since this model is e↵ective for tasks that require early precision. DFR models are based on the idea that the term-weight is inversely related to the probability of termfrequency within the document d. In our case, we consider each sentence s as a document. Therefore:</p><formula xml:id="formula_2" coords="4,105.01,107.74,196.67,13.61">weight(t|s) =/ logP robInL2(t 2 s|Collection) (2)</formula><p>Based on the equation 2 we can then calculate the relevance of the whole sentence s as follows:</p><formula xml:id="formula_3" coords="4,135.33,159.36,166.34,24.19">Relevance(s) = X t2s weight(t|s)<label>( 3 )</label></formula><p>When a term that is rare in the collection appears a lot in a sentence, then this term has high probability to be informative for the topic discussed in the sentence. In other words, if a term frequency is high, then the risk of not being informative is low. In this case, equation 2 gives a high value but a minimal risk provides a small information gain. Equation 2 is smoothed to consider the portion of weight which represents the information gained with the term:</p><formula xml:id="formula_4" coords="4,90.69,291.32,210.99,13.61">gain(t|s) = Prisk ⇤ ( logP robInL2(t 2 s|Collection)) (4)</formula><p>In the equation 4 P robInL2 represents the model of randomness and is calculated as:</p><formula xml:id="formula_5" coords="4,109.15,352.78,141.70,13.61">logP robInL2(t 2 s|Collection) = log2</formula><p>N + 1 nt + 0.5 where nt is the term frequency of the term t in the collection and N is the number of documents in the collection.</p><p>To compute the information gain with a term within a sentence, we use the Laplace model:</p><formula xml:id="formula_6" coords="4,165.44,432.81,53.27,18.66">Prisk = 1 tf + 1</formula><p>where tf is the term frequency of the term t in the sentence s.</p><p>Before using the equation 4, the sentence length dl is normalised to a standard length sl. The term frequencies are calculated with respect to the standard sentence length, as:</p><formula xml:id="formula_7" coords="4,146.76,541.14,91.71,13.61">tfn = tf ⇤ log(1 + c ⇤ sl dl )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Query Expansion</head><p>One frequent problem that hurts the performance of a retrieval system when returning relevant documents given a query is word mismatch. This problem occurs because users do not use the same words to form the query as those used in relevant documents. To address this problem, we used query expansion through which the query is expanded with new terms and this possibly helps to retrieve more relevant documents. For our experiments, we expanded the query using the 5 most appeared words in the summary having been already produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Novelty</head><p>Selecting the most relevant sentences is not enough for a temporal summarization system. In addition to the relevance, the system should also select the most novel sentences. In order to measure the novelty of a sentence, we computed the similarities between a candidate sentence and the summary that was already produced (until the specific timestamp). We applied a simple method that was based on the number of unique terms of the candidate sentence when compared to the generated summary. To present the method more formally, we introduce some notation. Let Si represent the summary produced until the timestamp ti and si+1 a candidate sentence with a timestamp ti+1.</p><formula xml:id="formula_8" coords="4,351.28,202.28,161.44,24.46">Novelty(s) = Novelty(si+1) = NT (Si, si+1) |si+1|</formula><p>where NT (Si, si+1) is the number of unique terms in the sentence si+1 when it is compared with the summary Si already generated until the timestamp ti and |si+1| is the length of the sentence si+1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Combine Relevance and Novelty</head><p>As a last step, we need to combine the relevance and the novelty of the sentences to produce the final ranking. We factorize relevance and novelty as follows:</p><formula xml:id="formula_9" coords="4,334.46,332.04,196.17,13.61">Score(d) = ( ) ⇤ Relevance(s) + (1 ) ⇤ Novelty(s)</formula><p>where Relevance(s) denotes the relevance of the sentence, Novelty(si+1) denotes the amount of novelty in the sentence si+1 compared to the summary Si and 2 [0, 1].</p><p>Because of the lack of training data, it is not possible to find the best parameter for . Therefore, is set to 0.5 so relevance and novelty contribute equally to the final ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>In this section, we present more details for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Dataset</head><p>This year's temporal summarization track was based on the TREC KBA 2014 Stream Corpus. The corpus contains a set of timestamped documents crawled from a variety of news and social media sources. The documents in the corpus span from October 2011 until April 2013. Each document in the collection comprises of a set of sentences. Each sentence has a unique identifier. Due to the size of the KBA 2014 corpus, two smaller datasets were released: the TREC-TS-2015F and the TREC-TS-2015F-RelOnly. The TREC-TS-2015F is a pre-filtered dataset and contains for each event the top documents from a high precision retrieval process. This dataset is high-volume and contains a lot of irrelevant documents. The TREC-TS-2015F-RelOnly collection contains manually selected documents for each event and therefore does not need any filtering to remove the irrelevant content.</p><p>For our experiments we used the TREC-TS-2015F dataset and participated in the filtering and summarization task. This year's task included 21 events, each of which represented by a set of features including title, description, start time, end time, query and type. In this year's task, the type of the events could be any of bombing, accident, earthquake, protest, storm, conflict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Preprocessing</head><p>The TREC-TS-2015F corpus is about 38GB and needs preprocessing. The overall process is as follows:</p><p>• Decrypt the file: After having downloaded the files, we need to decrypt them. To decrypt the files, we use the authorized key provided by the organizers. This step converts the GPG file format to SC.</p><p>• Parsing: The SC files are then converted to TXT files. We use the streamcorpus toolbox <ref type="foot" coords="5,226.41,206.81,3.33,4.99" target="#foot_0">4</ref> to parse the files.</p><p>• Index: The last step is to build the index. We index the collection with the Terrier IR system<ref type="foot" coords="5,249.07,233.81,3.33,4.99" target="#foot_1">5</ref> . Our preprocessing also involves stop-word removal and stemming using the Porter stemmer <ref type="bibr" coords="5,202.85,254.23,8.39,7.82" target="#b6">[6]</ref>. We consider each sentence as a document and create separate indexes for each timestamp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Submitted Runs</head><p>We submitted seven runs for the filtering and summarization task. For all the runs, we used the same approach to calculate relevance and novelty in order to generate the final ranking. For the relevance, we have used DF RInL2. The novelty score is based on the number of novel terms that each sentence has compared to the terms of the summary that was already produced until the specific timestamp. The runs di↵er in terms of how the number of sentences was decided at each timestamp and the way that the first sentence of the summary was selected. After having selected the first sentence of the summary, we use query expansion for the rest of the blocks. For the query expansion, we use the top 5 most frequent terms of the summary already produced.</p><p>Here we present details of our pooled runs:</p><p>• InL2DecrQE1ID1: For this run the number of sentences selected each hour decreases as time passes.</p><p>The first sentence of a summary should contain all the query terms.</p><p>• InL2DecrQE2ID2: For this run the number of sentences selected each hour decreases as time passes. The first sentence of a summary may contain any of the query terms.</p><p>• InL2StabQE2ID3: For this run the number of sentences selected each hour is the same for every block and depends on the number of time blocks per event.</p><p>The first sentence of a summary may contain any of the query terms.</p><p>• InL2IncrQE2ID4: For this run the number of sentences selected each hour increases as time passes. The first sentence of a summary may contain any of the query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Evaluation Metrics</head><p>To evaluate the e↵ectiveness of the systems, track organizers define two metrics: the Expected Latency Gain (ELG) and the Latency Comprehensiveness (LC) which are similar to the traditional IR metrics Precision and Recall respectively. ELG measures the sum of latency-discounted relevance of he nuggets for which that update is the earliest issued. LC measures the nugget recall over all updates issued, where the score for a nugget is not considered if it is reported late. Systems are ranked based on the harmonic mean between ELG and LC, denoted as H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Results</head><p>Table <ref type="table" coords="5,346.78,194.64,4.20,7.82" target="#tab_1">2</ref> reports the performance of our pooled runs for the Filtering and Summarization task in terms of ELG, LC and H for the task filtering and summarization. The results show that all our submitted runs submitted very well in terms of LC. That means that we manage to detect a good number of sentences that should be included in the summary. However, we do not perform well in terms of ELG. This suggests that our produced summaries include a lot sentences that should not be part of the summary. We believe that one reason for that is that we did not consider importance at all. Trying better techniques to filter out the irrelevant documents and measuring the importance of the sentences are two directions we want to explore in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this technical report, we presented the methodology followed for our participation in the 2015 Contextual Suggestion and Temporal Summarization tracks. In the Contextual Suggestion track, we showed that our method in recommending places to users based on their profiles is very e↵ective. The approach we followed for the Temporal Summarization track performed very well in respect to latency comprehensiveness. However, it did not perform well in terms of the expected latency gain. The results suggest that we should modify our approach in a way that the expected latency gain can be improved. One possible way is to also consider the importance of the sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,323.49,70.13,192.54,76.70"><head>Table 1 :</head><label>1</label><figDesc>Overall Average Performances</figDesc><table coords="3,323.49,80.12,171.42,66.71"><row><cell>Runs</cell><cell>P@5</cell><cell>MRR</cell></row><row><cell>11</cell><cell cols="2">0.5858 0.7404</cell></row><row><cell>22</cell><cell>0.5450</cell><cell>0.6991</cell></row><row><cell cols="2">TREC Median 0.5090</cell><cell>0.6716</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,345.25,241.42,174.60,64.39"><head>Table 2 :</head><label>2</label><figDesc>Results of our runs</figDesc><table coords="5,345.25,249.82,174.60,55.99"><row><cell></cell><cell>ELG</cell><cell>LC</cell><cell>H</cell></row><row><cell>InL2DecrQE1ID1</cell><cell cols="3">0.0185 0.3203 0.0184</cell></row><row><cell>InL2DecrQE2ID2</cell><cell cols="3">0.0089 0.3284 0.0172</cell></row><row><cell>InL2StabQE2ID3</cell><cell cols="3">0.0082 0.3225 0.0159</cell></row><row><cell>InL2IncrQE2ID4</cell><cell cols="3">0.0053 0.1848 0.0101</cell></row><row><cell cols="4">ALL submitted runs 0.0483 0.2381 0.0612</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="5,87.76,651.16,93.20,7.82"><p>http://streamcorpus.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="5,87.76,660.58,46.64,7.82;5,140.93,666.34,81.44,1.92"><p>Available at: http://terrier.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research was partially funded by the <rs type="funder">RelMobIR and OpiTrack projects of the Swiss National Science Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,327.58,617.71,88.32,14.19" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,336.44,631.95,182.22,7.82;5,336.44,641.49,203.13,7.82;5,336.44,650.68,190.51,8.18;5,336.44,660.22,186.39,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,472.30,631.95,46.36,7.82;5,336.44,641.49,203.13,7.82;5,336.44,651.04,102.17,7.82">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,445.53,650.68,81.42,8.18;5,336.44,660.22,104.36,8.18">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,96.51,64.39,201.06,7.82;6,96.51,73.93,170.87,7.82;6,96.51,83.12,197.48,8.18;6,96.51,92.66,194.18,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,185.94,73.93,81.43,7.82;6,96.51,83.48,114.50,7.82">TREC 2014 Temporal Summarization Track Overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ekstrand-Abueg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,228.37,83.12,65.62,8.18;6,96.51,92.66,116.13,8.18">Proceedings of the 23rd Text REtrieval Conference</title>
		<meeting>the 23rd Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>TREC 2014</note>
</biblStruct>

<biblStruct coords="6,96.51,103.47,202.58,7.82;6,96.51,113.01,184.74,7.82;6,96.51,122.56,193.98,7.82;6,96.51,132.10,19.11,7.82" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,145.44,113.01,135.80,7.82;6,96.51,122.56,58.18,7.82">Overview of the trec 2014 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="6,96.51,142.55,205.17,7.82;6,96.51,152.09,198.19,7.82;6,96.51,161.63,185.63,7.82;6,96.51,171.18,169.13,7.82;6,96.51,180.72,201.95,7.82;6,96.51,189.90,195.24,8.18;6,96.51,199.45,75.05,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,164.40,161.63,117.74,7.82;6,96.51,171.18,169.13,7.82;6,96.51,180.72,198.18,7.82">University of Glasgow at TREC 2014: Experiments with Terrier in Contextual Suggestion, Temporal Summarisation and Web Tracks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-D</forename><surname>Albakour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mackie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Thonet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Dincer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,106.99,189.90,184.75,8.18;6,96.51,199.45,49.33,8.18">Proceedings of the 23rd Text REtrieval Conference (TREC 2014)</title>
		<meeting>the 23rd Text REtrieval Conference (TREC 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,96.51,210.26,205.17,7.82;6,96.51,219.80,189.30,7.82;6,96.51,228.98,199.10,8.18;6,96.51,238.53,200.33,8.18;6,96.51,248.07,196.27,8.18;6,96.51,257.97,183.14,7.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,258.27,210.26,43.41,7.82;6,96.51,219.80,189.30,7.82;6,96.51,229.34,116.69,7.82">Incremental update summarization: Adaptive sentence selection based on prevalence and novelty</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,229.99,228.98,65.62,8.18;6,96.51,238.53,200.33,8.18;6,96.51,248.07,193.01,8.18">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM &apos;14</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,96.51,268.42,173.94,7.82;6,96.51,277.60,110.92,8.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,148.33,268.42,118.61,7.82">An algorithm for su x stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,96.51,277.60,28.97,8.18">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,96.51,288.42,189.91,7.82;6,96.51,297.60,200.46,8.18;6,96.51,307.14,197.69,8.18;6,96.51,317.04,19.11,7.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,240.91,288.42,45.50,7.82;6,96.51,297.96,141.22,7.82">HLTCOE at TREC 2013: Temporal Summarization</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,255.14,297.60,41.83,8.18;6,96.51,307.14,193.87,8.18">Proceedings of the 22nd Text REtrieval Conference (TREC 2013)</title>
		<meeting>the 22nd Text REtrieval Conference (TREC 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,96.51,327.50,184.34,7.82;6,96.51,337.04,176.16,7.82;6,96.51,346.22,184.75,8.18;6,96.51,355.76,75.05,8.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,241.90,327.50,38.95,7.82;6,96.51,337.04,161.82,7.82">TBJUT at TREC 2014 Temporal Summarization Track</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,96.51,346.22,184.75,8.18;6,96.51,355.76,49.33,8.18">Proceedings of the 23rd Text REtrieval Conference (TREC 2014)</title>
		<meeting>the 23rd Text REtrieval Conference (TREC 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
