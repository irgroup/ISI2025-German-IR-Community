<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,114.68,80.93,382.64,14.85">Exploiting Neural Embeddings for Social Media Data Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,156.83,131.03,76.46,12.37"><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
							<email>sadid.hasan@philips.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,242.20,131.03,51.32,12.37"><forename type="first">Yuan</forename><surname>Ling</surname></persName>
							<email>yuan.ling@philips.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.89,131.03,41.75,12.37"><forename type="first">Joey</forename><surname>Liu</surname></persName>
							<email>joey.liu@philips.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.12,131.03,81.06,12.37"><forename type="first">Oladimeji</forename><surname>Farri</surname></persName>
							<email>dimeji.farri@philips.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,114.68,80.93,382.64,14.85">Exploiting Neural Embeddings for Social Media Data Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">962A3F8FD1862C84831F3AD8616D39EF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our microblog realtime filtering system developed and submitted for the Text Retrieval Conference (TREC 2015) microblog track. We submitted six runs for two tasks related to real-time filtering by using various Information Retrieval (IR), and Machine Learning (ML) techniques to analyze the Twitter sample live stream and match relevant tweets corresponding to specific user interest profiles. Evaluation results demonstrate the effectiveness of our approach as we achieved 3 of the top 7 best scores among automatic submissions across all participants and obtained the best (or close to best) scores in more than 25% of the evaluated topics for the real-time mobile push notification task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main goal of the real-time filtering task in the TREC 2015 microblog track 1 was to monitor a stream of social media posts (Twitter) in order to match relevant content (tweet) corresponding to specific user interest profiles and push notifications to the users considering two different tasks. For Task A, a maximum of 10 interesting tweets per day were requested to be sent to the user in real-time, whereas in Task B, a batch of up to 100 top ranked interesting tweets per interest profile were required to be sent at the end of each day. We submitted six runs for the two tasks using various Natural Language Processing (NLP), specifically IR techniques to analyze the Twitter sample live stream and match relevant tweets corresponding to specific user interest profiles.</p><p>1 https://github.com/lintool/twitter-tools/wiki/TREC-2015-Track-Guidelines NLP and ML have emerged in the recent decades as prominent methodologies to fully leverage the rapid explosion of unstructured information in the digital universe. Deep Learning (DL) techniques typically aim at automatically learning representations of data without requiring any prior domain knowledge or expert annotations. DL alleviates the need for tedious feature engineering by devising efficient unsupervised or semi-supervised algorithms for hierarchical feature learning and extraction. Deep learning for NLP tasks mainly rely on learning high-dimensional vector representations of words, phrases, sentences, or documents and their relationships (called embeddings) using neural network architectures. The trained language model transforms semantically similar textual units into similar vector representations <ref type="bibr" coords="1,444.82,454.36,95.18,10.45" target="#b6">(Mikolov et al., 2013;</ref><ref type="bibr" coords="1,313.20,467.91,97.37,10.45" target="#b2">Le and Mikolov, 2014)</ref>. The main advantage of such architecture over the traditional bag-of-words model is its ability to capture the embedded ordering and semantics by learning fixed-length vector representations for variable-length text structures. We exploited the strength of neural word and phrase embeddings in extending the context of the underlying user interest profiles for our microblog real-time filtering system. In the subsequent sections, we describe the overall architecture of our system, and present the evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>Figure <ref type="figure" coords="1,344.85,658.42,5.45,10.45" target="#fig_0">1</ref> shows the generic architecture of our realtime content filtering system. Our overall approach comprises three main processes: (i) Analysis of Interest Profiles: leveraging a topic signature modeling algorithm and neural word/phrase embeddings Initially, we analyzed all user interest profiles provided by the track organizers using a topic signature algorithm <ref type="bibr" coords="2,164.55,617.77,101.86,10.45" target="#b4">(Lin and Hovy, 2000)</ref> to extract the most important topical keywords to capture the overall context of the information need. These keywords along with their n-gram combinations were utilized to expand the topical vocabulary by extracting related synonym sets from the WordNet database <ref type="bibr" coords="2,155.89,699.07,76.46,10.45" target="#b0">(Fellbaum, 1998)</ref> and exploiting deep neural word/phrase embeddings. The neural word/phrase embeddings were trained on over 60 million tweets (crawled from the Twitter's live sample stream for over two weeks before the start of the evaluation period) by using a deep learning-based word/phrase to vector representation modeling algorithm <ref type="bibr" coords="2,340.08,472.37,95.50,10.45" target="#b6">(Mikolov et al., 2013)</ref>. The expanded topical keyword list per interest profile was indexed in Elasticsearch<ref type="foot" coords="2,351.97,497.50,3.99,7.64" target="#foot_0">2</ref> for further analyses during real-time tweet content filtering.</p><p>In the second step, each incoming tweet was processed to remove noise using various rule-based algorithms in association with curated databases of known noisy elements that are widely used in tweet messages. We then applied tokenization and partof-speech (POS) tagging <ref type="bibr" coords="2,429.49,595.02,97.40,10.45" target="#b1">(Gimpel et al., 2011)</ref> to extract the most important words that preserve the contextual meaning of the tweet.</p><p>In the last step, the tweet content words were used as query words for which an appropriate interest profile was retrieved and matched using algorithms within Elasticsearch. Elasticsearch transformed the query words as various combinations of possible n-grams/phrases to find an overall content match across all the interest profiles. The final relevance of a tweet with respect to a user interest profile was measured using a weighted combination of two scores (Eq 1): (i) TF-IDF based content matching score returned by Elasticsearch, and (ii) semantic similarity score based on an algorithm built on semantic networks of related words and corpus-based statistics <ref type="bibr" coords="3,112.31,183.97,66.20,10.45" target="#b3">(Li et al., 2006)</ref>.</p><formula xml:id="formula_0" coords="3,89.21,201.85,209.59,14.78">W ⇤ semanticscore + (1 W ) ⇤ elasticscore (1)</formula><p>We also calculated the semantic similarity of a new tweet with the tweets that were already sent to the users to minimize redundancy. Finally, an average relevance score over a set of empirical threshold values triggered a tweet to be sent to the matching user for Task A (within a few seconds after the tweet was originally created). For Task B, the incoming tweets were indexed through the end of each day, processed using the same algorithms as Task A, and then the interest profile keywords were used as queries to find a ranked list of up to 100 matching tweets to be sent as an e-mail digest to the corresponding user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Test Data</head><p>The test dataset comprises 225 user interest profiles with three fields: "title" contained a few keywords, the "description" contained a one-sentence statement of the information need, and the "narrative" was a paragraph-length description of the information need. We used all fields for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Corpus</head><p>Twitter's live tweet sample stream was used as the corpus for the track. We built an architecture to continuously monitor the tweet stream by following the guidelines provided by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Run Description</head><p>For Task A, we submitted three runs as follows: 1) prna1-A: considers topical keyword expansion using WordNet synonyms, 2) prna2-A: considers topical keyword expansion using neural word/phrase embeddings, and 3) prna3-A: considers topical keyword expansion using both WordNet synonyms and neural word/phrase embeddings. The runs were also varied by the threshold values for the tweet relevance scores as we set the highest threshold for run 3 and the lowest for run 1. Our three runs for Task B were designed in the same fashion except we use the Elasticsearch score as the only measure for tweet relevance due to time complexity and chose up to 100 top ranked tweets per profile to be sent to the user after the end of each day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation and Analysis</head><p>The evaluation of the 2015 microblog track was conducted on a subset of 51 topics (selected out of the total 225 test topics) by following a similar procedure as the tweet timeline generation (TTG) task from the TREC 2014 microblog track <ref type="bibr" coords="3,483.51,279.54,56.49,10.45;3,313.20,293.09,24.85,10.45" target="#b7">(Wang et al., 2015;</ref><ref type="bibr" coords="3,343.96,293.09,74.93,10.45" target="#b5">Lin et al., 2014)</ref>. The tweets returned by the participant systems were collected into a single judgment pool for both tasks and each tweet was judged by the assessors independently corresponding to the user interest profiles using a three-point scale where spam/junk/not interesting, somewhat interesting, and very interesting tweets received the gain (score) of 0, 0.5, and 1, respectively. After this standard pooling assessment procedure, a clustering protocol was applied to group all tweets into a set of semantically similar clusters. The participant systems were only credited for returning one tweet from each meaningful cluster.</p><p>Figure <ref type="figure" coords="3,355.91,469.51,5.45,10.45" target="#fig_1">2</ref> and Figure <ref type="figure" coords="3,415.55,469.51,5.45,10.45">3</ref> show the overall scores of our runs for Task A across the selected 51 topics as compared to the max scores among all participants' submitted runs for two evaluation measures: expected latency-discounted gain (ELG<ref type="foot" coords="3,488.32,521.74,3.99,7.64" target="#foot_1">3</ref> ), and normalized cumulative gain (nCG<ref type="foot" coords="3,445.31,535.29,3.99,7.64" target="#foot_2">4</ref> ). These results show that our system achieves the best scores across all runs for 14% of the evaluated topics while having close to best scores for 12% of the evaluated topics. In-depth analyses of the results also reveal that WordNet synonyms and neural word/phrase embeddings often have a positive impact on the tweet relevance scores. Figure <ref type="figure" coords="3,417.96,632.10,5.45,10.45" target="#fig_2">4</ref> shows the distribution of per-topic ELG scores for the best runs by mean ELG scores (adopted from the TREC 2015 overview  talk). Analyses reveal that our run (prnaTaskA2, arrow marked in Figure <ref type="figure" coords="6,169.96,89.12,3.94,10.45" target="#fig_2">4</ref>), which exploits neural embeddings for better understanding of the user interest profiles, has achieved one of the top 5 scores among all automatic runs submitted by the participants. Further analyses denote that all three of our submitted runs for Task A are placed in the best 7 ranks across all automatic submissions.</p><p>Figure <ref type="figure" coords="6,114.64,184.47,5.45,10.45">5</ref> shows the overall scores of our runs for Task B across the selected 51 topics as compared to the max scores among all participants' submitted runs for the evaluation measure: normalized Discounted Cumulative Gain (nDCG), which computes the quality of ranking for each system based on the ranked list of 100 tweets graded with relevance judgment and normalized by the maximum possible gain. These results demonstrate that our system can achieve close to the best scores for a few number of topics simply because we could not implement the semantic similarity measure to compute the tweet relevance due to time complexity limitation. Contextual expansion methodologies (i.e. use of Word-Net synonyms, and neural embeddings) show a similar positive effect on the overall results as shown in Task A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we described our participation in the TREC 2015 microblog track. Evaluation results showed the effectiveness of our approach as we achieved additional gains with the implementation of neural word and phrase embeddings in extending relevant contexts for the user interest profiles. In future, we plan to improve upon our tweet relevance scoring algorithms, especially for the email digest task (B), by leveraging powerful computational resources to solve the respective use cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,209.82,372.23,192.36,9.54;2,72.00,72.00,468.00,283.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Real-time filtering system architecture</figDesc><graphic coords="2,72.00,72.00,468.00,283.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,215.63,377.67,180.74,9.54;4,72.00,77.44,467.99,283.47"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ELG scores for each topic (Task A)</figDesc><graphic coords="4,72.00,77.44,467.99,283.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,80.23,377.68,451.54,9.54;5,72.00,77.43,468.01,283.48"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of per-topic ELG scores for best run by mean ELG (Task A); the arrow denotes our system</figDesc><graphic coords="5,72.00,77.43,468.01,283.48" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,329.34,713.93,104.84,8.59"><p>http://www.elasticsearch.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,329.34,678.62,210.66,10.80;3,313.20,691.80,151.65,8.59"><p>ELG is computed using the summation of tweet gains normalized by the number of tweets returned.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,325.85,701.19,2.99,5.73;3,329.34,700.76,210.66,10.80;3,313.20,713.93,203.82,8.59"><p>4 nCG is calculated by the total tweet gain divided by the maximum possible gain given the 10 tweet per day limit.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,72.00,627.82,226.80,9.54;6,82.91,639.78,157.15,9.54" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,161.81,627.82,136.99,9.54;6,82.91,639.78,34.67,9.54">WordNet -An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,653.48,226.80,9.54;6,82.91,665.43,215.89,9.54;6,82.91,677.39,215.89,9.54;6,82.91,689.35,215.89,9.54;6,82.91,698.84,215.89,11.90;6,82.91,710.79,215.89,11.90;6,324.11,73.75,215.89,12.00;6,324.11,88.17,95.34,9.54" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,191.86,677.39,106.94,9.54;6,82.91,689.35,193.17,9.54">Part-of-speech tagging for twitter: Annotation, features, and experiments</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,82.91,698.84,215.89,11.90;6,82.91,710.79,215.89,11.90;6,324.11,73.75,141.72,11.90">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-guage Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-guage Technologies: Short Papers</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,100.87,226.80,9.54;6,324.11,110.37,215.89,12.01;6,324.11,122.32,215.89,11.90;6,324.11,134.28,215.89,11.90;6,324.11,148.69,72.50,9.54" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,448.08,100.87,91.91,9.54;6,324.11,112.83,133.92,9.54">Distributed representations of sentences and documents</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,479.75,110.37,60.25,11.90;6,324.11,122.32,215.89,11.90;6,324.11,134.28,66.05,11.90">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">2014. June 2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,161.39,226.80,9.54;6,324.11,173.35,215.89,9.54;6,324.11,182.84,215.89,12.01;6,324.11,194.80,215.89,12.01;6,324.11,209.22,22.42,9.54" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,408.44,173.35,131.56,9.54;6,324.11,185.31,130.47,9.54">Sentence similarity based on semantic nets and corpus statistics</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">A</forename><surname>Bandar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>O'shea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Crockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,463.86,182.84,76.13,11.90;6,324.11,194.80,156.77,11.90">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1138" to="1150" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,221.92,226.80,9.54;6,324.11,233.87,215.89,9.54;6,324.11,243.37,215.89,11.90;6,324.11,255.32,108.48,12.01" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,440.01,221.92,99.99,9.54;6,324.11,233.87,196.58,9.54">The Automated Acquisition of Topic Signatures for Text Summarization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,324.11,243.37,215.89,11.90;6,324.11,255.32,39.84,11.90">Proceedings of the 18th conference on Computational linguistics</title>
		<meeting>the 18th conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="495" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,270.49,226.80,9.54;6,324.11,282.44,215.89,9.54;6,324.11,291.94,215.89,11.90;6,324.11,303.89,215.89,11.90;6,324.11,315.85,93.98,11.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,324.11,282.44,192.42,9.54">Overview of the TREC-2014 microblog track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sherman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,324.11,291.94,215.89,11.90;6,324.11,303.89,27.42,11.90">Proceedings of The Twenty-Third Text REtrieval Conference</title>
		<meeting>The Twenty-Third Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11-19">2014. 2014. November 19-21, 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,331.01,226.80,9.54;6,324.11,342.96,215.89,9.54;6,324.11,352.46,215.89,12.01;6,324.11,364.41,215.89,11.90;6,324.11,376.37,215.89,12.01;6,324.11,390.78,22.42,9.54" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,392.45,342.96,147.55,9.54;6,324.11,354.92,156.41,9.54">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,503.94,352.46,36.06,11.90;6,324.11,364.41,215.89,11.90;6,324.11,376.37,156.36,11.90">Proceedings of the 27th Annual Conference on Neural Information Processing Systems NIPS 2013</title>
		<meeting>the 27th Annual Conference on Neural Information Processing Systems NIPS 2013</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,403.49,226.80,9.54;6,324.11,415.44,215.89,9.54;6,324.11,424.94,215.89,12.01;6,324.11,436.89,215.89,11.90;6,324.11,448.85,215.89,12.01;6,324.11,463.26,37.36,9.54" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,525.61,403.49,14.39,9.54;6,324.11,415.44,215.89,9.54;6,324.11,427.40,59.00,9.54">Assessor differences and user preferences in tweet timeline generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,406.08,424.94,133.92,11.90;6,324.11,436.89,215.89,11.90;6,324.11,448.85,186.52,12.01">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="615" to="624" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
