<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.64,129.27,285.99,10.34">TUW AT THE FIRST TOTAL RECALL TRACK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,258.72,160.05,57.83,8.97"><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
						</author>
						<title level="a" type="main" coord="1,144.64,129.27,285.99,10.34">TUW AT THE FIRST TOTAL RECALL TRACK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D559CDA7D151A06DDD7732270EB64BFF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the first participation in the TREC Total Recall track, we set out to try some basic changes to the baseline provided by the organisers. Namely, the weighting scheme, the use of stopwords, and the number of learners that contribute to the decision of which documents to ask the virtual assessor to review. We observed that the baseline was extremely strong and none of the runs significantly and consistently outperformed it.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the organizers point out, the focus of the Total Recall Track is to evaluate methods to achieve very high recall, including methods that include a human assessor in the loop <ref type="bibr" coords="1,485.66,321.41,10.90,10.43" target="#b5">[6]</ref>.</p><p>We submitted six automated runs for the small At home task and provided scripts for the sandbox evaluation. The athome1 data contains 290099 files grouped in 115 folders, with 333 files in the smallest, 16850 in the largest, a mean of 2522.6 files per folder and a median of 2228. Table <ref type="table" coords="1,183.74,373.22,5.45,10.43" target="#tab_0">1</ref> shows the 10 topics used for the athome1 part.</p><p>Two other collections were tested in the sandbox environment: the MIMIC II clinical dataset 1 (C) and the Kaine Email Collection 2 (Kaine) indexed each 31174 and, respectively, 401953 documents.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>For this year's participation, we have only modified the provided Baseline Model Implementation (bmi) <ref type="bibr" coords="2,192.66,129.68,11.51,10.43" target="#b1">[2]</ref> to test some very simple changes to the method. Namely, we looked at the use of stopwords (runs indicated by the presence of an "S" in their name), the use of a di↵erent term weighting scheme-a recently introduced adaptation of BM25 <ref type="bibr" coords="2,503.06,155.58,11.51,10.43" target="#b2">[3]</ref> that does not need optimizing for the b parameter, and a modified voting of 6 learners instead of using only 1. We discuss the weighting in the following subsection. The learner voting mechanism appears in Section 2.2.</p><p>The common pre-processing are fundamentally the same as those of the bmi. We only counted additional data necessary in the adapted BM25.</p><p>tokenisation: tokens are identified by first splitting on non-alphanumeric characters and then removing all strings thus obtained that contain at least one digit. All tokens of length 1 were ignored. casing: all tokens were lowercased stopwords: only for runs marked by "S", tokens appeared in the list in the Appendix A were ignored.</p><p>2.1. Term weighting. The bmi used the basic tf.idf weighting scheme, as given by:</p><p>(1) weight T (t, d) = (1 + log(tf t,d )) ⇤ log(N/df t ); where t is a term, d a document, tf t,d the term frequency, df t the document frequency, and N is the number of documents in the collection.</p><p>For our weighting, we used an observation recently by Lipani et al. <ref type="bibr" coords="2,451.50,383.80,10.90,10.43" target="#b2">[3]</ref>, that using the average term frequency in a document and the mean average term frequency over the collection, we can define for BM25 a b parameter that is collection specific. This would be particularly useful here, since it would save us some training e↵ort. The used weight, marked by "B" in the run names, is given by:</p><formula xml:id="formula_0" coords="2,110.16,451.75,393.15,32.18">(2) weight B (t, d) = tf t,d ⇣ 1 mavgtf avgtf d mavgtf + (1 1 mavgtf ) L d avgdl ⌘ k 1 + tf t,d N df t + 0.5 df t + 0.5</formula><p>where, apart from the variables already presented for Eq. 1, k 1 is the usual BM25 parameter controlling tf normalization, avgtf d and L d are the average term frequency in document d and, respectively, its length. Finally, mavgtf and avgdl are the mean average term frequency and the average document length, calculated over all documents. Throughout the experiments, k 1 was maintained at its "standard" value of 1.2. We note that there exists previous work that removes the need for optimizing on k 1 <ref type="bibr" coords="2,385.81,556.13,10.90,10.43" target="#b3">[4]</ref>, which could be applied here.</p><p>At this time, it remains for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learner voting.</head><p>The bmi uses the Sofia ML suite for incremental machine learning algorithms <ref type="bibr" coords="2,163.60,602.88,10.90,10.43" target="#b6">[7]</ref>. In particular it uses logreg-pegasos, i.e. Logistic Regression with Pegasos updates, optimizing over ROC area, with 200k iterations, dimensionality 1.1mil, and = 10 4 . These were all parameters established by the track organisers, and while we fiddled at times with them, we found no compelling reason to change them.</p><p>The only change we made was at a higher level. The Sofia ML library provides 5 more ML algorithms. The following list is quoted directly from the manual, we refer the reader to the website 3 and D. Sculley's publications <ref type="bibr" coords="3,290.43,123.20,11.51,10.43" target="#b6">[7]</ref> for further details.</p><p>pegasos: Use the Pegasos SVM learning algorithm. --lambda sets the regularization parameter, with values closer to zero giving less regularization. Note that Pegasos enforces a hard constraint that the model weight vector must lie within an L2 ball of radius at most 1/sqrt(lambda). Also relies on --eta_type sgd-svm: Use the SGD-SVM learning algorithm. -lambda sets the regularization parameter, with values closer to zero giving less regularization. Also relies on --eta_type passive-aggressive Use the Passive Aggressive Perceptron learning algorithm.</p><p>--passive-aggressive-c sets the largest step size to be taken on any update step; this operates as a capacity term with values closer to zero encouraging simpler models. --passive-aggressive-lambda will force the model weight vector to lie within an L2 ball of radius 1/sqrt(passive-aggressive-lambda) margin-perceptron: Use the Perceptron with Margins algorithm. Sets the update margin with --perceptron-margin-size. When set to 0, this is exactly equivalent to the classical Perceptron by Rosenblatt. When set to 1, this is equivalent to optimizing SVM hinge-loss without regularization. Increasing values may give additional tolerance to noise. Also relies on --eta_type. romma: Use the ROMMA algorithm. No parameters to set. logreg-pegasos: Use Logistic Regression with Pegasos updates; we optimize logistic loss and enforce Pegasos-style regularization and constraints, with --lambda being the regularization parameter. Also relies on --eta_type. Note that the classification values provided by this method regression are logodds, and can be converted to probabilities using: exp(p) / (1 + exp(p)). The runs using all six learners (denoted by "6" in their name), during each iteration of thebmi take first all the documents on which all learners agree, then those on which 5 agree, then those on which 4 agree. After that, they complete the set with the documents proposed by logreg-pegasos but are not yet in the set to be sent for evaluation. This was especially necessary in the first few iterations where extremely little agreement was found between learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>For each recall value, we performed an ANOVA to test the omnibus hypothesis that all the runs are equal by Precision. In most cases, and particularly for high recall values, this hypothesis could not be rejected and therefore we cannot say that any of the runs are actually di↵erent from the baseline. Where the hypothesis was rejected, we performed pairwise tests and we report those. All tests were performed using Carterette's R implementation <ref type="bibr" coords="3,73.44,596.58,10.90,10.43" target="#b0">[1]</ref>.</p><p>The sandbox C collection (Mimic 2 Clinical Decision Dataset) presents a strange behaviour in the sense that it never reaches recall 1. From our own data, we see that all runs and all topics reach a maximum of exactly 31174, from which our assumption that the dataset contains these many documents. However, the User Guide of the dataset<ref type="foot" coords="4,503.37,257.67,3.65,5.48" target="#foot_1">4</ref> states that the April 2011 release (version 2.6) contains around 33k patients. Apparently, the di↵erence consists of documents without any text. Multiplying the reported recall with the known number of relevant documents per topic, we observe that, while exploring the maximum set of 31174 indexed documents, we are missing, on average, 22.79 documents per topic. For most topics this is above 0.995 recall and therefore would be 1.00 when rounding up to the nearest cent (10 2 ). For topic C11 however, the total number of relevant documents is only 180, and one missing document results in a recall of 0.9945, which rounds up to 0.99. For plotting, we forced this to 1.00 as well, to maintain visibility.</p><p>Another data alteration we do for consistency is to assign recall 1.05 to the e↵ort and precision results reported when using the entire dataset. Therefore, when we talk about recall 1.00, we refer to the first time this recall value was obtained. Otherwise, we will talk about recall on the dataset (which is generally 1.00 except for topic C11 mentioned above).</p><p>Figures <ref type="figure" coords="4,161.69,439.49,5.45,10.43" target="#fig_2">1</ref> and<ref type="figure" coords="4,193.68,439.49,5.45,10.43">2</ref> show the precision recall curves for the three test collections and for each topic, respectively. For athome1, the curves are statistically indistinguishable, except for points at recall 30% and 50%. For C, the curves are completely indistinguishable, and as for Kaine, the Webis is significantly lower than the other runs.</p><p>By Precision-Recall curve, probably the most interesting run is athome109 (Scarlet Letter Law), as its precision increases with recall almost up to recall 1. The topic had 506 relevant documents in the collection. The information need is, presumably, any information about laws whose main or side-e↵ect is a public shaming of individuals, but it may be also referring only to a specific law passed and then repealed in Florida. A quick grep on the collection shows that there are 22 documents actually containing the two words "scarlet" and "letter" separated by 3 characters or less. All of these documents contain also the term "Jeb" (case sensitive, representing the first name of former Florida governor, Jeb Bush). Individually, "scarlet" (ignoring case and surrounded by non-alphanumeric characters) appears in 70 documents, 66 of which also contain the term "Jeb". The 4 other documents refer to people named Scarlet (a more common spelling of this name is with a double ending consonant). From these 70 to the total of 506 the system has to figure it out using only "letter" and "law", two relatively common terms. Figures <ref type="figure" coords="5,124.11,500.75,5.45,10.43" target="#fig_3">3</ref> and<ref type="figure" coords="5,154.37,500.75,5.45,10.43" target="#fig_4">4</ref> show the recall versus e↵ort, as calculated by the organizers. Figures 3 shows the average over all topics, by collection, run, and coe cients a and b. Figure <ref type="figure" coords="5,496.38,513.70,5.45,10.43" target="#fig_4">4</ref> shows details for each topic, for a fixed b = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Our submissions this year did not improve upon the provided baseline. This appears to have been the general observation of this year's track: "Several manual and automatic participant e↵orts achieve higher recall with less e↵ort than the baseline on some topics, but none consistently improves on the baseline" <ref type="bibr" coords="5,312.90,615.83,10.90,10.43" target="#b4">[5]</ref>. In our case, the use of stopwords appears to be counter productive, even though we used very few of them. The modified weighting scheme showed insignificant improvements on athome1. The use of multiple  Appendix A. Stop words list a, about, above, after, again, against, all, am, an, and, any, are, aren't, as, at, be, because, been, before, being, below, between, both, but, by, can't, cannot, could, couldn't, did, didn't, do, does, doesn't, doing, don't, down, during, each, few, for, from, further, had, hadn't, has, hasn't, have, haven't, having, he, he'd, he'll, he's, her, here, here's, hers, herself, him, himself, his, how, how's, i, i'd, i'll, i'm, i've, if, in, into, is, isn't, it, it's, its, itself, let's, me, more, most, mustn't, my, myself, no, nor, not, of, off, on, once, only, or, other, ought, our, ours, ourselves, out, over, own, same, shan't, she, she'd, she'll, she's, should, shouldn't, so, some, such, than, that, that's, the, their, theirs, them, themselves, then, there, there's, these,  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,85.40,439.95,3.65,5.48;1,89.05,447.39,277.73,2.11;1,85.40,450.91,3.65,5.48;1,89.05,458.35,225.95,2.11"><head>1</head><label></label><figDesc>https://physionet.org/mimic2/mimic2_clinical_overview.shtml 2 http://www.virginiamemory.com/collections/kaine/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="1,285.52,655.78,4.23,7.61"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,109.31,437.09,51.75,2.75;5,166.51,429.41,299.46,10.43;5,109.31,442.36,356.66,10.43"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Average Precision-Recall curve for each collection. Points beyond 100% recall represent precision after the entire dataset was evaluated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,109.00,441.47,51.05,2.75;7,165.50,433.79,96.57,10.43;7,265.72,441.47,4.68,2.75;7,274.03,433.79,192.24,10.43"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Average E↵ort. The b coe cient is in the title of each sub-plot</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,227.81,725.45,51.05,2.75;8,284.31,717.77,136.61,10.43"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. E↵ort for each topic for b=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,184.89,483.40,205.51,164.88"><head>Table 1 .</head><label>1</label><figDesc>athome1 topics</figDesc><table coords="1,184.89,508.03,205.51,140.26"><row><cell>Topic</cell><cell>Information Need</cell></row><row><cell cols="2">athome100 School and Preschool Funding</cell></row><row><cell cols="2">athome101 Judicial Selection</cell></row><row><cell cols="2">athome102 Capital Punishment</cell></row><row><cell cols="2">athome103 Manatee Protection</cell></row><row><cell cols="2">athome104 New medical schools</cell></row><row><cell cols="2">athome105 A rmative Action</cell></row><row><cell cols="2">athome106 Terri Schiavo</cell></row><row><cell cols="2">athome107 Tort Reform</cell></row><row><cell cols="2">athome108 Manatee County</cell></row><row><cell cols="2">athome109 Scarlet Letter Law</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,73.44,104.98,446.73,54.37"><head></head><label></label><figDesc>they, they'd, they'll, they're, they've, this, those, through, to, too, under, until, up, very, was, wasn't, we, we'd, we'll, we're, we've, were, weren't, what, what's, when, when's, where, where's, which, while, who, who's, whom, why, why's, with, won't, would, wouldn't, you, you'd, you'll, you're, you've, your, yours, yourself, yourselves</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="3,89.63,651.17,179.37,2.11"><p>https://github.com/huitseeker/sofia-ml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,125.77,649.42,235.37,2.11"><p>https://physionet.org/mimic2/UserGuide/node15.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.78,189.54,414.06,8.97;9,87.78,200.50,53.19,8.97;9,144.04,207.20,93.30,2.26;9,240.42,200.50,47.58,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,187.17,189.54,314.67,8.97;9,87.78,200.50,48.76,8.97">Multiple Testing in Statistical Analysis of Systems-based Information Retrieval Experiments</title>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><forename type="middle">A</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,144.04,207.20,89.66,2.26">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.78,211.45,414.05,8.97;9,87.78,222.41,173.43,8.97;9,264.28,229.12,60.90,2.26;9,328.26,222.41,20.96,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,272.98,211.45,228.85,8.97;9,87.78,222.41,157.77,8.97">Evaluation of Machine-learning Protocols for Technologyassisted Review in Electronic Discovery</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,264.28,229.12,56.12,2.26">Proc of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.78,233.37,414.06,8.97;9,87.78,244.33,102.53,8.97;9,193.39,251.04,64.60,2.26;9,261.07,244.33,20.97,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,337.32,233.37,164.51,8.97;9,87.78,244.33,86.76,8.97">Verboseness Fission for BM25 Document Length Normalization</title>
		<author>
			<persName coords=""><forename type="first">Aldo</forename><surname>Lipani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,193.39,251.04,59.68,2.26">Proc. of ICTIR</title>
		<meeting>of ICTIR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.78,255.29,414.06,8.97;9,87.78,266.25,35.77,8.97;9,126.30,272.96,335.35,2.26;9,464.47,266.25,37.36,8.97;9,87.78,277.21,230.94,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,239.98,255.29,261.86,8.97;9,87.78,266.25,19.65,8.97">A log-logistic model-based interpretation of tf normalization of bm25</title>
		<author>
			<persName coords=""><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,126.30,272.96,335.35,2.26;9,464.47,266.25,32.69,8.97">Proceedings of the 34th European Conference on Advances in Information Retrieval, ECIR&apos;12</title>
		<meeting>the 34th European Conference on Advances in Information Retrieval, ECIR&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="244" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.78,288.17,414.05,8.97;9,87.78,299.13,159.06,8.97;9,249.91,305.83,63.74,2.26;9,316.73,299.13,20.96,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,439.68,288.17,62.15,8.97;9,87.78,299.13,143.38,8.97">Notebook draft trec 2015 total recall track overview</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,249.91,305.83,58.01,2.26">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.78,310.08,414.06,8.97;9,87.78,327.75,63.74,2.26;9,154.60,321.04,20.97,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,381.59,310.08,104.86,8.97">Total recall track overview</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegist</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,87.78,327.75,58.01,2.26">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.78,332.00,201.20,8.97;9,292.06,338.71,74.43,2.26;9,369.57,332.00,20.97,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,133.72,332.00,139.21,8.97">Combined Regression and Ranking</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,292.06,338.71,68.81,2.26">Proc. of SIGKDD</title>
		<meeting>of SIGKDD</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
