<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.68,44.81,396.25,14.17">ISCASIR at TREC 2015 Temporal Summarization Track</title>
				<funder>
					<orgName type="full">NIST</orgName>
				</funder>
				<funder ref="#_tYgA57X">
					<orgName type="full">National High Technology Research and Development 863 Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,241.68,87.45,53.52,9.86"><forename type="first">Peixia</forename><surname>Wang</surname></persName>
							<email>peixia@nfs.iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">The National Engineering Research Center of Fundamental Software The Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences (ISCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.75,87.45,44.71,9.86"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The National Engineering Research Center of Fundamental Software The Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences (ISCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.68,44.81,396.25,14.17">ISCASIR at TREC 2015 Temporal Summarization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5F44464981F993EA753BD0682649461A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of Temporal Summarization task is to develop systems which can detect useful, new, and timely sentence-length updates about a developing event. This paper describes our participation in Temporal Summarization track of TREC2015.</p><p>Based on the word embedding technique, we submitted two runs for the summarization task. The query expanding technique is used for the first run and relevant sentences are retrieved by computing the distance between the expanded query and the sentence. The processing of second run is the same with the first run except for the query expanding stage. Using the KBA Stream Corpus 2014, the experimental results show the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the Temporal Summarization 2015 Guidelines <ref type="bibr" coords="1,348.00,391.16,9.38,7.18" target="#b0">[1]</ref> describes, the goal of the TS track is to develop systems which can detect useful, new, and timely sentence-length updates about a developing event. There are three sub-tasks in TREC 2015, however, we only participate in the third sub-task because of the time limit, i.e. Task 3: Summarization Only. During the task, participants will be provided low-volume streams of on-topic documents for a set of topic events and it requires each participant to process those streams in time order, that is to say, the participant needs pick up relative sentences from the documents contained within each stream as updates over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>The way to select relative sentences from the data stream is inspired by WMD distance <ref type="bibr" coords="1,129.24,590.36,9.43,7.18" target="#b1">[2]</ref> which use a new metric for the distance between text documents. Similarly, we measure the document distance by the cumulative amount of distance that the embedded query words of the topic event match the embedded words of the candidate sentence. The difference between our proposed approach and the WMD lies in the specific function of distance computation between sentences, details are described in the following part .</p><p>Our approach leverages recent results by Mikolov <ref type="bibr" coords="1,344.04,683.96,9.51,7.18" target="#b2">[3]</ref> ,i.e. word2vec model which we use to generate high-quality vector representations of words considering that it can capture precise syntactic and semantic word relationships. A particular implementation of neural network based algorithm for training the word vectors is available at code.google.com/p/word2vec. After the training converges, words with semantic relevance are mapped into a similar space in the vector space and therefore we use the distributed representation of words to compute the distance between the query and the sentence.</p><p>In addition, it is necessary to preprocess the data stream to the format we would like to use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>The corpus we use is the KBA Stream Corpus 2014, i.e. the second filtered set TREC-TS-2015F-RelOnly <ref type="bibr" coords="2,217.80,220.76,9.51,7.18" target="#b3">[4]</ref> that consists of a manually selected set of relevant documents for each event because we only participate Task 3. As the data inside each corpus file is encrypted and serialized with thrift format. So it is necessary to preprocess the corpus into the data format that is easy to deal with before we use it.</p><p>Firstly, decrypting the files uses the authorized key and converts the .GPG file format to .SC file format;</p><p>Secondly, deserialize the data into the sentence lists on demand by ways of interacting with stream corpus chunks using the tools provided by the streamcorpus project in github <ref type="bibr" coords="2,171.48,345.56,9.43,7.18" target="#b4">[6]</ref> . The preprocessing stage produces the processed corpus which is to be used in the next stage. The output format of the processed data is in the following tab-separated format:</p><p>Table <ref type="table" coords="2,223.76,395.20,4.67,10.72">1</ref>: the format of the processed corpus where the columns are defined as, The first column: document identifier The second column: decision timestamp The third column: sentence identifier The fourth column: sentence content</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Algorithm</head><p>We submitted two runs for the Task 3. The difference of the two runs lies in the ways of processing the query items. The first run is runvec1, parts of the query items are expanded and the second is runvec2, same with runvec1 except for the query expanding part.</p><p>The key point of expanding phase is to obtain a query item list by adding top k words to the query items according to the semantic distance computed from the word vectors. Next, remove the stop words from the list with high frequency. In the end, add the event type to the newly query items due to its discriminate feature. Except for the expanding stage, the processing progress of runvec2 is exactly the same with runvec1. The common processing parts of the two runs can be described as follows:</p><p>First, compute the cumulative similarity distance between the newly query items and the sentence items for every topic event.</p><p>Second, check whether the value of the distance is greater than the specified threshhold, if so, then check to see if the result sets contains the sentence, if not, add the sentence to the result sets.</p><p>The following is the pseudocode of algorithm:</p><p>Algorithm 1: Input: stream of processed corpus Input: topic queries Output: list of sentence identifiers 1: Initialize: RESULT={} 2:</p><p>for each query q do 3:</p><p>expand the query q as q' 4:</p><p>for each sentence s in processed corpus do 5:</p><p>compute the dist(q',s) 6:</p><p>if dist(q',s)&gt;threshhold and s is not contained in RESULT 7:</p><p>add s to RESULT 8: end if 9: end for 10: end for 14: return RESULT Where dist(q',s) is defined as:</p><formula xml:id="formula_0" coords="3,167.16,433.45,338.12,24.56">∑ ∑ , √ ,<label>(1)</label></formula><p>m is length of the query, n is length of sentence and sim , is the similarity metric of the two words separately in query and sentence. The similarity can be obtained by computing the dot product of the corresponding two word vectors. Formerly, the distance between two words is defined as :</p><formula xml:id="formula_1" coords="3,161.88,522.82,343.52,13.52">sim , . ,<label>(2)</label></formula><p>Where is one word vector from query items and is one word vector from the sentence items.</p><p>Intuitively, the similarity between the query and the sentence can be represented by the matching degree of the corresponding sentences. The matching is measured by the cumulative distance that all word items in query match the words in the sentence. Furthermore, word matching is defined as the dot distance of the two word vectors.</p><p>As to sentence de-duplication, to avoid redundancy in updates and to improve the quality, duplicate sentences are forbidden to go into the result sets. We check whether the sentence exists in the result sets first, if so, delete the sentence and process next one.</p><p>According the TREC authority, there are several metrics, such as (normalized) Expected Gain, Comprehensiveness and HM metric <ref type="bibr" coords="4,339.24,73.88,9.43,7.18">[5]</ref> and etc.. Expected Gain metric. It is the way to evaluate the relevance or precision of the summarization with respect to the event topic, something like the precision in traditional information retrieval.</p><p>Comprehensiveness metric. It is the way to measure the coverage of the summarization with respect to all the essential information contained in the corpus, similar to tradition concept of recall in information retrieval evaluation.</p><p>HM metric. A combined way to incorporate Expected Gain and Comprehensiveness with Latency included.</p><p>We submitted totally two runs for Task 3: runvec1 and runvec2. The results based on these metrics are as follows.</p><p>Table <ref type="table" coords="4,183.32,248.32,4.50,10.72">2</ref>. The comparison of runvec1 and runvec2 for Task 3 The table 2 above shows the detail on three metrics with latency of each topic and the table 3 shows the extended comparison of our two submitted runs: runvec1 and runvec2. From the table, we can conclude that the results show the effectiveness of our method in terms of recall and that we manage to retrieve most of the relevant updates covering the important nuggets, but the precision is lower than average. Furthermore, the expanding technique on the query items does not improve the precision and recall. Besides, the values of metrics fluctuate violently between the minimum and maximum for different event topics on which should be improved in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper reports a word embedding-based framework and technical scheme for Task 3 in TREC 2015 Temporal Summarization Track. The soul of method is to get the distributed representation of words first and use it later to get the relative sentences with respect to the topic event. In addition, filtering out duplicate sentences is important too. This year, we do research on the existed word embedding only. In the future, we will take consider in more information on the embedding ways of sentences and Knowledge Base.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,90.00,413.38,170.74,4.93;2,279.01,413.38,96.81,4.93;2,90.00,428.98,170.74,4.93;2,279.01,428.98,226.12,4.93;2,90.00,444.58,109.30,4.93;2,216.03,444.58,44.71,4.93;2,279.01,444.58,217.74,4.93"><head></head><label></label><figDesc>the aircraft hit a crane on St George Wharf Tower , in Vauxhall , amid heavy fog . of the sky , smashed into two cars as it hit the ground and exploded into flames .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.56,41.56,409.00,186.92"><head>Table 3 .</head><label>3</label><figDesc>The extended comparison of runvec1 and runvec2 for Task 3</figDesc><table coords="5,88.56,58.83,409.00,169.65"><row><cell>Run ID</cell><cell></cell><cell>nE[Gain]</cell><cell>nE[Latency</cell><cell>Comp.</cell><cell>Latency Comp.</cell><cell>HM</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gain]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>runvec1</cell><cell>STD</cell><cell>0.0100</cell><cell>0.0066</cell><cell>0.1962</cell><cell>0.2457</cell><cell>0.0125</cell></row><row><cell></cell><cell>MIN</cell><cell>0.0033</cell><cell>0.0001</cell><cell>0.1163</cell><cell>0.0046</cell><cell>0.0003</cell></row><row><cell></cell><cell>MAX</cell><cell>0.0421</cell><cell>0.0278</cell><cell>0.9844</cell><cell>0.8969</cell><cell>0.0528</cell></row><row><cell></cell><cell>AVG</cell><cell>0.0174</cell><cell>0.0111</cell><cell>0.7852</cell><cell>0.5409</cell><cell>0.0215</cell></row><row><cell>runvec2</cell><cell>STD</cell><cell>0.0112</cell><cell>0.0067</cell><cell>0.1649</cell><cell>0.2139</cell><cell>0.0128</cell></row><row><cell></cell><cell>MIN</cell><cell>0.0076</cell><cell>0.0041</cell><cell>0.3767</cell><cell>0.1004</cell><cell>0.0080</cell></row><row><cell></cell><cell>MAX</cell><cell>0.0520</cell><cell>0.0299</cell><cell>0.9793</cell><cell>0.8969</cell><cell>0.0563</cell></row><row><cell></cell><cell>AVG</cell><cell>0.0190</cell><cell>0.0129</cell><cell>0.7881</cell><cell>0.5813</cell><cell>0.0250</cell></row><row><cell>ALL</cell><cell>AVG</cell><cell>0.0595</cell><cell>0.0319</cell><cell>0.5627</cell><cell>0.3603</cell><cell>0.0472</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>We would like to thank all organizers and assessors of <rs type="institution">TREC</rs> and <rs type="funder">NIST</rs>. This work was supported by the <rs type="funder">National High Technology Research and Development 863 Program of China</rs> under Grants no. <rs type="grantNumber">2013AA01A603</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tYgA57X">
					<idno type="grant-number">2013AA01A603</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,107.44,629.44,400.96,10.72;5,90.00,645.04,49.96,10.72;5,90.00,660.64,414.76,10.72;5,90.00,676.24,163.80,10.72" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,385.10,629.44,117.84,10.72">Temporal Summarization</title>
		<author>
			<persName coords=""><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ekstrand-Abueg</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sakai</surname></persName>
		</author>
		<ptr target="https://38309103-a-62cb3a1a-s-sites.googlegroups.com/site/temporalsummarization/trec2015-ts-guidelines-updated.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,107.16,691.84,386.93,10.72;5,90.00,707.44,361.53,10.72;6,90.00,25.96,394.34,10.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,440.38,691.84,53.70,10.72;5,90.00,707.44,172.95,10.72">From Word Embeddings to Document Distances</title>
		<author>
			<persName coords=""><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,270.91,707.44,180.62,10.72;6,90.00,25.96,159.38,10.72">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.92,41.56,366.04,10.72;6,90.00,57.16,405.26,10.72;6,90.00,72.76,63.00,10.72" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,417.91,41.56,54.05,10.72;6,90.00,57.16,304.15,10.72">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,415.03,57.16,23.29,10.72">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.16,88.36,219.99,10.72;6,90.00,103.96,304.21,10.72" xml:id="b3">
	<monogr>
		<ptr target="http://dcs.gla.ac.uk/~richardm/TREC-TS-2015RelOnly.aws.list" />
		<title level="m" coord="6,107.16,88.36,162.47,10.72">TREC-TS-2015F-RelOnly dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.16,135.16,256.07,10.72" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ekstrand-Abueg</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Metrics</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
