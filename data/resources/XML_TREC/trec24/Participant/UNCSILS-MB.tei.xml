<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.85,89.78,413.47,5.05">Query-expansion Approaches for Microblog Retrieval</title>
				<funder ref="#_5e5VZrC">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,172.76,117.06,73.53,13.50"><forename type="first">Sandeep</forename><surname>Avula</surname></persName>
							<email>asandeep@live.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information &amp; Library Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel</orgName>
								<address>
									<addrLine>Hill Chapel Hill</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.69,117.06,71.90,13.50"><forename type="first">Jaime</forename><surname>Arguello</surname></persName>
							<email>jarguello@unc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information &amp; Library Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel</orgName>
								<address>
									<addrLine>Hill Chapel Hill</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.85,89.78,413.47,5.05">Query-expansion Approaches for Microblog Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">719692F63E09FF0CFD6A1625658879AE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The School of Information and Library Science at the University of North Carolina at Chapel Hill submitted three runs to the "Scenario B" task of the TREC 2015 Microblog Track. The task simulates a scenario where a user specifies a topic of interest in the form of a keyword query and the system produces daily updates with at most 100 tweets about the topic of interest. Systems were responsible for monitoring a stream of tweets and making daily predictions for a set of 250 interest profiles. Each interest profile was in the form of a short keyword query. Systems were asked to produce a ranking of at most 100 tweets per interest profile at the end of each day (shortly after midnight). The evaluation period extended a 10-day period from July 20 to July 29, 2015. All tweets published between 00:00:00 to 23:59:59 UTC were valid candidates for each day of the evaluation period.</p><p>Our team submitted three runs for "Scenario B". All runs were automatic runs and used the interest profile title field as the input query. We explored three di↵erent ways of enriching the query representation through query expansion. In two of our runs (UNCSILS WRM and UNCSILS TRM), we scored tweets proportional to the negative KL-divergence between a relevance model generated from an external collection and the tweet language model. These two runs mainly di↵er by the external collection used to generate a relevance model for interest profile query. In our UNCSILS WRM run, we used an external Wikipedia corpus, and in our UNC-SILS TRM run, we used a corpus of tweets collected during a three-week period prior to the evaluation period. In our third run (UNCSILS HRM), we aimed to expand the query with related hashtags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM OVERVIEW</head><p>We developed a system to run continuously throughout the 10-day evaluation period. We used the infrastructure provided by the Track organizers to sample the public Twitter stream 1 , and we used Lucene 2 to implement the indexing and retrieval components.</p><p>The system operated on a daily cycle. At the start of each day, a new index was created and tweets sampled during the day were written to this index. Then at 23:59:59 UTC, the system closed the index and sequentially executed our three runs for all 250 profiles. Finally, after completing all three runs, the system created a new daily index and began 1 https://github.com/lintool/twitter-tools 2 https://lucene.apache.org/ writing tweets to this new index. We did not store any of the tweets that were sampled while the system executed our three runs. However, our runs only took about 10 minutes to complete, so it is unlikely that we missed indexing many relevant tweets due to this gap. Queries and documents were stemmed using the Porter stemmer <ref type="bibr" coords="1,451.66,251.15,8.85,7.82" target="#b2">[2]</ref> and stopped using the SMART stopword list. 3  During indexing, we used the following heuristics to filter tweets not likely to be relevant to any interest profile:</p><p>• non-English tweets based on the Twitter API's language field; • tweets with one or more swear words;</p><p>• tweets with more than three hashtags;</p><p>• tweets with more than one URL;</p><p>• tweets with more than one user mention;</p><p>• tweets without at a hashtag or URL; • tweets that were more than 70% stopwords; and • tweets with fewer than four tokens that were not URLs, hashtags, and user mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ALGORITHMS</head><p>Our three runs considered di↵erent external corpora for query expansion. We first describe the base scoring function in Section 3.1 and our three query-expansion runs in Sections 3.2-3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Base Retrieval</head><p>For our "base" retrieval, we scored documents proportional the negative KL-divergence between the query and document language models:</p><formula xml:id="formula_0" coords="1,359.86,502.66,129.01,24.33">score(Q, D) = Y w2V P (w| ✓D) P (w|✓ Q ) .</formula><p>Query language models were estimated using maximum likelihood:</p><formula xml:id="formula_1" coords="1,385.77,559.13,77.19,24.82">P (w|✓Q) = #(w, Q) |Q| .</formula><p>Document language models were estimated using Dirichlet smoothing:</p><formula xml:id="formula_2" coords="1,364.38,611.86,136.34,24.82">P (w| ✓D) = #(w, D) + µP (w|✓C ) |D| + µ ,</formula><p>where P (w|✓C ) = #(w,C) |C| .</p><p>3 ftp://ftp.cs.cornell.edu/pub/smart/english.stop</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Wikipedia Relevance Model</head><p>For this run (UNCSILS WRM), we scored tweets proportional to the negative KL-divergence between a relevance model generated from Wikipedia and the tweet language model:</p><formula xml:id="formula_3" coords="2,116.68,112.39,135.53,24.33">score(Q, D) = Y w2V P (w| ✓D) P (w| ✓wiki Q ) .</formula><p>(</p><p>Tweet language models were Dirichlet-smoothed as described in Section 3.1 and the Wikipedia relevance model was estimated using Lavrenko's Relevance Model RM3 <ref type="bibr" coords="2,258.97,162.26,8.39,7.82" target="#b1">[1]</ref>.</p><p>Our implementation of Lavrenko's Relevance Model RM3 proceeded in two steps. In the first step, we generated a relevance model from the top-t Wikipedia results produced using our base retrieval function from Section 3.1. We estimated the probability of word w in this relevance model according to:</p><formula xml:id="formula_5" coords="2,101.40,238.54,200.28,26.76">P (w|✓ wiki Q ) = 1 Z X D2Rt score(Q, D) ⇥ P (w| ✓D),<label>(2)</label></formula><p>where Rt denotes the top-t Wikipedia results and Z is a normalizing constant:</p><formula xml:id="formula_6" coords="2,143.33,288.40,82.21,24.33">Z = X D2Rt score(Q, D).</formula><p>In the second step, we interpolated this relevance model (✓ wiki Q ) with the original query language model (✓Q):</p><formula xml:id="formula_7" coords="2,103.11,344.07,198.57,10.73">P (w| ✓wiki Q ) = P (w|✓Q) + (1 )P (w|✓ wiki Q ).<label>(3)</label></formula><p>We used the following parameter values. All document language models were smoothed using Dirichlet smoothing with µ = 1000. When generating the original relevance model from Wikipedia (Equation <ref type="formula" coords="2,209.69,390.02,3.27,7.82" target="#formula_5">2</ref>), we set t = 10. Moreover, we used only the top-10 terms with the highest probability and re-normalized their probabilities. Finally, we interpolated the Wikipedia relevance model with the original query model (Equation <ref type="formula" coords="2,171.39,428.19,3.73,7.82" target="#formula_7">3</ref>) using = 0.50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Twitter Relevance Model</head><p>For this run (UNCSILS TRM), we build a relevance model from a corpus of tweets gathered during a three-week period before the evaluation period (from June 18 to July 12, 2015). Similar to the Wikipedia Relevance Model approach, we scored tweets proportional to the negative KL-divergence between a relevance model generated from our Twitter corpus and the tweet language model:</p><formula xml:id="formula_8" coords="2,115.05,524.82,138.78,24.33">score(Q, D) = Y w2V P (w| ✓D) P (w| ✓tweet Q ) .</formula><p>(4)</p><p>Our implementation of Lavrenko's Relevance Model RM3 proceeded as described in Section 3.2. That is, we first estimated a relevance model from our corpus of tweets (similar to Equation <ref type="formula" coords="2,131.10,584.24,3.73,7.82" target="#formula_5">2</ref>) and then interpolated this relevance model with the original query model (similar to Equation <ref type="formula" coords="2,273.15,593.78,3.27,7.82" target="#formula_7">3</ref>).</p><p>There was only one main di↵erence. Our corpus of tweets had a large number of duplicates, which resulted in relevance models with only a few terms. In order to discover a greater number of terms related to the original query, we removed duplicates from Rt by going down the ranking and filtering tweets with a Jaccard coe cient greater than or equal to 0.70 compared to a higher ranked tweet. We used the same parameter values used in the previous run. All document language models were Dirichletsmoothed with µ = 1000. Furthermore, when generating the relevance model from our corpus of tweets, we only used the top-10 unfiltered tweets (t = 10) and the top-10 terms (after re-normalizing their probabilities). We interpolated the relevance model with the original query model using = 0.50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hashtag Expansion</head><p>In this approach (UNCSILS HRM), we aimed to expand the original query with relevant hashtags. We used the same corpus of tweets used in our Twitter Relevance Model approach.</p><p>The approach proceeded as follows. Our first goal was to score hashtags based on their relevance to the original query. To this end, we constructed an index of hashtag pseudodocuments, where the text associated with each hashtag pseudo-document was gathered from all tweets containing the hashtag. Then, we built a relevance model of hashtags based on the normalized query-likelihood score from each hashtag's language model. The probability of hashtag h in this relevance model is given by:</p><formula xml:id="formula_9" coords="2,347.93,291.39,169.24,26.76">P (h|✓ hash Q ) = 1 Z Y q2Q #(q, H) + µP (q|✓C H ) |H| + µ .</formula><p>Here, H denotes the pseudo-document associated with h, CH denotes the collection of hashtag pseudo-documents, and Z is a normalizing constant. In practice, we only considered the top-10 hashtags and assigned to other hashtags a zero probability. Similar to the previous approaches, we then interpolated our hashtag relevance model with the original query model: P (w| ✓hash Q ) = P (w|✓Q) + ( <ref type="formula" coords="2,442.83,400.03,3.73,7.82" target="#formula_4">1</ref>)P (w|✓ hash</p><formula xml:id="formula_10" coords="2,487.33,400.03,54.27,8.58">Q ),<label>(5)</label></formula><p>and scored tweets proportional to the negative KL divergence between this interpolated model and the tweet language model:</p><formula xml:id="formula_11" coords="2,356.04,449.02,185.57,24.33">score(Q, D) = Y w2V P (w| ✓D) P (w| ✓hash Q ) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Redundancy Filtering</head><p>The Track guidelines specified that the evaluation would punish redundant tweets. To address this, for each runs' daily output, we proceeded down the ranking and filtered tweets with a Jaccard coe cient greater than or equal to 0.70 compared to a higher-ranked tweet. For each run, the system returned the top-100 unfiltered tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>The results for our three "Scenario B" runs are presented in Table <ref type="table" coords="2,354.59,593.78,4.20,7.82" target="#tab_0">1</ref> in terms of NDCG. The results are averaged across the 51 topics that were included in the evaluation. We tested statistical significance using the approximation of Fisher's randomization test described in Smucker et al. <ref type="bibr" coords="2,507.72,622.41,8.39,7.82" target="#b3">[3]</ref>. Our Wikipedia Relevance Model approach (UNCSILS WRM) outperformed our other two runs by a statistically significant margin. The other two runs (UNCSILS TRM and UNC-SILS HRM) were statistically equal.  1" 2" 3" 4" 5" 6" 7" 8" 9" 10"11"12"13"14"15"16"17"18"19"20"21"22"23"24"25"26"27"28"29"30"31"32"33"34"35"36"37"38"39"40"41"42"43"44"45"46"47"48"49"50"51"52" 1" 2" 3" 4" 5" 6" 7" 8" 9" 10"11"12"13"14"15"16"17"18"19"20"21"22"23"24"25"26"27"28"29"30"31"32"33"34"35"36"37"38"39"40"41"42"43"44"45"46"47"48"49"50"51" 1" 2" 3" 4" 5" 6" 7" 8" 9" 10"11"12"13"14"15"16"17"18"19"20"21"22"23"24"25"26"27"28"29"30"31"32"33"34"35"36"37"38"39"40"41"42"43"44"45"46"47"48"49"50"51" UNCSILS_HRM BEST (c) UNCSILS HRM Figure <ref type="figure" coords="3,355.22,558.87,3.76,8.18">1</ref>: NDCG performance per interest profile compared to the best-performing run for that profile. For each run, profiles are sorted in descending order of NDCG performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,83.56,67.91,218.12,182.40"><head>Table 1 :</head><label>1</label><figDesc>Results in terms of NDCG. Symbols N and M denote a statistically significant improvement over UNCSILS TRM and UNCSILS HRM, respectively (p &lt; .05)Results also show that all three runs were far from the best-performing run for most interest profiles. Figures1.a-1.c show NDCG performance for each run compared to the best-performing run per interest profile. For each run, interest profiles are sorted in descending order of NDCG performance. As the Figures clearly show, for most interest profiles, all three runs underperformed the best run by a wide margin.</figDesc><table coords="3,142.69,128.30,99.40,26.91"><row><cell>UNCSILS TRM 0.189</cell></row><row><cell>UNCSILS HRM 0.190 UNCSILS WRM 0.205 NM</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">ACKNOWLEDGMENTS</head><p>This work was supported in part by <rs type="funder">NSF</rs> grant <rs type="grantNumber">IIS-1451668</rs>. Any opinions, findings, conclusions, and recommendations expressed in this paper are the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5e5VZrC">
					<idno type="grant-number">IIS-1451668</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,87.65,324.76,88.32,11.29" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,96.51,338.55,173.29,7.82;3,96.51,348.10,73.26,7.82;3,172.57,353.85,113.66,2.06;3,96.51,363.40,203.99,2.06;3,96.51,372.94,141.93,2.06;3,241.24,367.18,39.34,7.82;3,96.51,376.72,101.48,7.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,210.14,338.55,59.65,7.82;3,96.51,348.10,58.80,7.82">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,172.57,353.85,113.66,2.06;3,96.51,363.40,203.99,2.06;3,96.51,372.94,141.93,2.06;3,241.24,367.18,36.08,7.82">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,96.51,387.17,205.17,7.82;3,96.51,396.72,184.32,7.82" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="3,148.08,387.17,153.60,7.82;3,96.51,396.72,123.28,7.82">Readings in information retrieval. chapter An Algorithm for Su x Stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,96.51,416.71,171.39,7.82;3,96.51,426.25,166.35,7.82;3,96.51,435.80,128.79,7.82;3,228.10,441.56,65.62,2.06;3,96.51,451.10,166.44,2.06;3,96.51,460.64,154.29,2.06;3,253.60,454.88,39.19,7.82;3,96.51,464.43,101.48,7.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,261.60,416.71,6.30,7.82;3,96.51,426.25,166.35,7.82;3,96.51,435.80,114.68,7.82">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,228.10,441.56,65.62,2.06;3,96.51,451.10,166.44,2.06;3,96.51,460.64,154.29,2.06;3,253.60,454.88,35.92,7.82">Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM &apos;07</title>
		<meeting>the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM &apos;07</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
