<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,114.38,81.96,396.62,15.22;1,163.22,103.08,284.38,15.22">Laval University and Lakehead University Experiments at TREC 2015 Contextual Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.02,137.22,39.12,10.72"><forename type="first">Jian</forename><surname>Mo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Laval University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.41,137.22,81.49,10.72"><forename type="first">Luc</forename><surname>Lamontagne</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Laval University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.67,137.22,77.74,10.72"><forename type="first">Richard</forename><surname>Khoury</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Software Engineering</orgName>
								<orgName type="institution">Lakehead University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,114.38,81.96,396.62,15.22;1,163.22,103.08,284.38,15.22">Laval University and Lakehead University Experiments at TREC 2015 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E25CA6A39DCDF19A04E0EE30814E0716</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ElasticSearch</term>
					<term>Boosting Query</term>
					<term>Recommendation System</term>
					<term>Point-wise re-ranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our effort on TREC Contextual Suggestion Track. We present a recommendation system that built upon Elas-ticSearch along with a machine learning re-ranking model. We utilize real world users' opinion as well as other information to build user profiles. With profile information, we then construct customized Elas-ticSearch queries to search on various fields. After that, a learning to rank regressor is implemented to give better ranking results. Track results of our submitted runs show the effectiveness of the system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In the Contextual Suggestion Track, participants were asked to develop a system that is able to make suggestions for a particular person with a particular context. The recommendations are contextual as they are based on user's location, profile, and preferences. <ref type="bibr" coords="1,151.94,419.09,11.69,8.89" target="#b0">[1]</ref> Two separate tasks were available in this year competition: live experiment and batch experiment. For the live experiment, each group is required to set up a live server and to respond to live requests in a short time. For batch experiments, several candidate suggestions are provided for each group of participants to re-rank the results and a final precision at 5 is calculated. Unlike last year, teams are no longer asked for a description generation. Thus the overall goal of the competition is to generate good suggestions from over 10,000 candidate attractions per city.</p><p>In this paper we present our research group's first attempt at developing a recommendation system to solve the challenge presented in the contextual suggestion task.</p><p>We participated in both tasks of this track. The fundamental approach we adopt consists in selecting similar documents to those the user likes that are highly rated by other users, combined with a reranking algorithm trained to predict the user's preferences. For the live experiment, we combined an ElasticSearch engine together with a SVM regressor on a virtual server in order to respond quickly to recommendation requests. For the batch experiment, we optimized our recommendation model to filter categories to mimic the user's set of preferences. We use Precision at 5 to present the results obtained for the evaluation of our system configurations. And our final P@5 results indicate that a precision of 46% can be reached with both configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In TREC 2014 <ref type="bibr" coords="1,399.31,286.11,10.69,8.89" target="#b2">[3]</ref>, the most common approach were using the language modeling framework. Most teams used positive, neutral and negative profiles in personalization of the suggestion candidates.</p><p>We also adopted this approach and developed a new profile modeling method inspired by Yang and Fang <ref type="bibr" coords="1,346.39,355.11,11.72,8.89" target="#b1">[2]</ref> in which they utilized the comments and ratings of third party users to sum up our users' opinion. Their intuition is that users with similar ratings of a place share something in common of why they like or dislike the place and our user also shares ideas with the third party users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Framework</head><p>To perform our experiments, we relied on the following components:</p><p>1.</p><p>Information gathering: a) Crawlers 2.</p><p>Online Ranking Model a) User profile modeling b) ElasticSearch c) Customized Queries d) Re-ranking regressor 3.</p><p>Offline re-ranking methods a) Category filter The entire framework is illustrated in Figure <ref type="figure" coords="1,521.86,590.60,4.98,8.89" target="#fig_0">1</ref> and each component is described in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. System Overview</head><p>Figure <ref type="figure" coords="1,352.71,645.56,4.98,8.89" target="#fig_0">1</ref> showed the overview of our system. The data are downloaded synchronously into two databases MySQL and ElasticSearch. We query Elas-ticSearch for candidate suggestions while get training data from MySQL for MySQL has a better API for data retrieving though ElasticSearch also has all the data we need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>`</head><p>Then a trained regressor will score each candidate suggestion to get a new rank. We select the top 5 suggestions as final results. In the batch run, the category filter will select final results according to their categories to maintain diversity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INFORMATION GATHERING</head><p>Over 10,000 candidate suggestions were available per context city and 273 cities in total there are as part of the challenge. The first responsibility of our system was to gather information about each of these candidate suggestions. We analysed several open-web service providers, namely Yelp, Google Places, and TripAdvisor. The last two sites provide an API that can be used to access the data source. Yelp's API is more limited, and we had to implement a custom crawler to retrieve the data from that site.</p><p>The crawler extracts useful information such as rating, open hours, price, review count, category for each candidate from the webpages.</p><p>Business information such as has Wi-Fi, has parking lot, smoking allowed are also extracted and stored into the database.</p><p>User reviews were often too numerous to download entirely. Hence we just get the most popular reviews of each attraction, i.e. the first page reviews. Positive and negative reviews are downloaded separately and stored in two different database tables.</p><p>Since our model has two distinct databases, we utilized the pipeline module of Scrapy to auto-sync the downloaded data between MySQL and Elas-ticSearch as the crawler push data to the two simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>` V. ONLINE RANKING MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modeling of User Profile</head><p>While Yang and Fang <ref type="bibr" coords="3,191.93,112.50,11.69,8.72" target="#b1">[2]</ref> only used reviews as their profile data, we exploit all the data available in the database including reviews, categories, business information, tags and other info as our query field. We create the user's positive profile by merging the positive information from all the examples our user likes, and likewise build the negative profile by merging the negative information from the examples our user dislikes. The intuition is that the preferences of one user are reflected by the attractions he/she likes and dislikes. Consequently, we can compare the user profiles with every candidate suggestion in the database and rank them by similarity. For instance, if one candidate suggestion has many elements in common with the positive user profile, this candidate obtains a higher ranking score. In contrast, if it is very similar to the negative user profile, its ranking score is penalized and very low.</p><p>According to the task defined for this track, one user might provide 6 different rating:</p><p>4: Strongly interested 3: Interested 2: Neutral 1: Disinterested 0: Strongly disinterested -1: Website didn't load or no rating given We selected rating 4 and 3 as positive and 1 and 0 as negative. Ratings 2 and -1 were taken as neutral and simply ignored.</p><p>Formally, the user profile can be expressed as:</p><formula xml:id="formula_0" coords="3,122.18,484.70,127.16,41.74">𝑃𝑟𝑜𝑓𝑖𝑙𝑒 = 𝑅𝐸𝑃(𝑃𝑒𝑠 ) 𝑃𝑟𝑜𝑓𝑖𝑙𝑒 = 𝑅𝐸𝑃(𝑁𝑒𝑠 )</formula><p>Where 𝑃𝑒𝑠 is positive example suggestion i, and Pesik is element k (categories, tags, positive reviews, business info) of this positive suggestion. 𝑅𝐸𝑃(𝑃𝑒𝑠 ) defines a special representation or form of the element. For example, the review texts of third party users (which can be quite numerous and quite lengthy) are represented by the 50 most frequently used keywords in the merged set of reviews. After being properly processed, all example suggestions from one user are merged together to form the user profile. At the end of the modeling process, we get four different positive profile elements: positive categories, positive business info, positive tags, and positive reviews. The same steps are performed to build the negative profile. For instance, one possible positive category is a long string like "parks, restaurant, outdoor sports, parks, playground…" which sums up all the categories user has rated positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Query Formulation</head><p>Once the user profile is built, we formulate a customized query to search our ElasticSearch database. For example, suppose the user likes Mexican food, dislikes Japanese food, appreciates Wi-Fi and parking lot and hates smoking. Part of our query could be formulated as the json structure in Figure <ref type="figure" coords="3,323.35,193.02,3.76,8.72">2</ref>. As can be seen, we use a bool boosting query to wrap up the elements of the user profile into one query, which can then be sent to ElasticSearch to retrieve relevant new attractions and similarity scores. These similarity scores will be used by our ranking function.</p><p>"bool": { "should": [ { "boosting": { "positive": { "match": { "category": "Mexican food" } }, "negative": { "match": { "category": "Japanese food" } }, "negative_boost": 0.3 } }, { "boosting": { "positive": { "match": { "business_info": "has wifi, good for kids" } }, "negative": { "match": { "business_info": "smoking" } }, "negative_boost": 0.1 } } Figure <ref type="figure" coords="3,399.05,669.57,4.07,10.62">2</ref>: Json query example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ranking Methods</head><p>The similarity score provided by elastic search in the previous step is sufficient to give satisfactory ` ranking results. However, to improve the performance of our system, we combine this similarity score with other features such as category, popularity, and rating and utilize a learning algorithm to compute a final ranking score for each candidate. We compute a personal regressor for every user according to his/her own preference instead of building a global regressor that predicts common interests shared by all people.</p><p>During our experiments, we tried several learning methods including Linear Regression, SVM and LambdaMart <ref type="bibr" coords="4,155.42,206.88,11.72,8.89" target="#b4">[4]</ref> with different combination of features. We used a 5-fold cross validation training algorithm on last year's dataset to evaluate each configuration. We found that Linear Regression and Lambda Mart perform poorly in this case, because the size of the training data per user is small (less than 50 samples). On the other hand, we observed that SVM applied on all the features we gathered during profile modeling achieves the best ranking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RE-RANKING METHODS IN BATCH MODE</head><p>Our first batch run used the same model as the live run using the SVM-based ranking scheme we described in Section V. For the second batch run, we added a category filter that we developed in order to bias the set of recommendations returned to mimic the diversity of preferences of the user. Our intuition for this filter is that there must be a diminishing return to suggestions of one same type, reflecting the user's decreasing interest in visiting several similar attractions. Consequently, we decided to add diversity to our recommendations to avoid having too many recommendations in one category and to insure we have recommendations in the major categories the user has shown interest in.</p><p>This category filter first calculates the distribution of categories in the user positive profile by summing up the counts of every positive category and then divide the total counts to get the proportion of each category. When generating a set of recommendations for a new city, attractions are selected or rejected so that the set will have a similar proportion to the user's profile. For example, if the user profile has a distribution composed of 40% restaurants and 60% museums and the system is designed to return 50 recommendations, then it will return the top 20 restaurants and top 30 museums ranked similarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RESULTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metric</head><p>An attraction is considered relevant for P@5 if it has a geographical relevance of 1 or 2 and if the user reported that both the description and document were found to be interesting <ref type="bibr" coords="4,466.70,91.92,11.69,8.89" target="#b2">(3)</ref> or strongly interesting (4). A P@5 score for a particular topic (a profile-context pair) is determined by how many of the top 5 ranked attractions are relevant, divided by 5. <ref type="bibr" coords="4,345.79,137.88,11.69,8.89" target="#b0">[1]</ref> B. Submitted Runs Three runs were submitted to the competition:</p><p>LavalIVA-run1, our live run, LavalIVA_1 is batch run 1, using the same model as the live run, LavalIVA_2 is batch run 2, which applies the category filter  <ref type="table" coords="4,363.91,353.55,4.98,8.89" target="#tab_0">1</ref> shows our final result <ref type="bibr" coords="4,464.50,353.55,10.69,8.89" target="#b0">[1]</ref>. As showed from the table, the precision of the first batch run (LavalIVA_1) is much higher than those obtained for the live run (LavalIVA-batch). The difference is not the ranking model or data, which were identical in both versions of the system, but rather that, unfortunately, our live server failed to respond to multiple requests made in the first two days of the evaluation. Furthermore, a few blank responses were provided during the two week period, which is due to a lack of robustness in our online service.</p><p>The track medium P@5 of all batch runs is 0.4946 while the highest is 0.5858. After analyzing our system on qrels, we found two major problems of our system in batch run test. The first problem lies in our data richness. Since we dropped some of the candidates' data source like Wikipedia and some other websites, we don't have all the data of candidates in the batch run as the candidates were provided by other participates system generated in the live run. Thus our system had some blank user profiles and returned false results. The second problem is our system has difficulty in dealing with user who has few positive ratings since our regressors are trained individually on single user so that when user has few positive data our regressors could not be trained properly. This problem is also found in the live run phase. Our system often obtains good score when the user provides sufficient positive feedbacks.</p><p>We also observe that the application of a diversity filter slightly lowers the quality of the recom-` mendations on P@5 standard. This seems to indicate that the best recommendations may often fall into the same category for some users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we have presented our contextual suggestion system. We have described its major components: building new user profile models, retrieving suggestions using ElasticSearch, and reranking the results using a regressor and a category filter. While our batch runs demonstrated a solid performance, the system used for the live run requires improvements to improve its robustness. For future improvements, we plan to replace the private regressors with a global regressor to deal with the problem of data sparseness in the user profiles and also enrich our data source to obtain better result in batch run.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,183.50,547.38,258.02,10.62;2,98.40,112.56,506.52,433.68"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall framework for contextual recommendation</figDesc><graphic coords="2,98.40,112.56,506.52,433.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,337.75,253.09,150.41,109.36"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="4,337.75,253.09,150.41,109.36"><row><cell cols="2">TREC CS 2015 results</cell></row><row><cell>Run</cell><cell>P@5</cell></row><row><cell>LavalIVA-batch</cell><cell>0.2611</cell></row><row><cell>LavalIVA_1</cell><cell>0.4645</cell></row><row><cell>LavalIVA_2</cell><cell>0.4616</cell></row><row><cell>Table</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,365.59,96.12,152.37,8.89;5,323.35,107.52,199.01,8.89;5,323.35,119.04,190.57,8.89;5,323.35,130.56,96.31,8.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,429.41,107.52,92.95,8.89;5,323.35,119.04,137.26,8.89">Overview of the TREC 2015 Contextual Suggestion Track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,478.26,119.04,35.67,8.89;5,323.35,130.56,66.10,8.89">Proceedings of TREC&apos;15</title>
		<meeting>TREC&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,365.59,142.32,161.27,8.89;5,323.35,153.72,200.80,8.89;5,323.35,165.24,77.25,8.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,454.40,142.32,72.46,8.89;5,323.35,153.72,134.56,8.89">An opinion-aware approach to contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,475.33,153.72,48.82,8.89;5,323.35,165.24,46.94,8.89">Proceedings of TREC&apos;13</title>
		<meeting>TREC&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,365.59,177.00,152.35,8.89" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,323.35,188.40,195.48,8.89;5,323.35,199.92,198.57,8.89;5,323.35,211.32,125.90,8.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,479.50,188.40,39.33,8.89;5,323.35,199.92,183.98,8.89">Overview of the TREC 2014 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,323.35,211.32,98.25,8.89">Proceedings of TREC&apos;14</title>
		<meeting>TREC&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,365.59,223.08,135.40,8.89;5,323.35,234.48,187.72,8.89;5,323.35,246.03,176.88,8.89" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,431.02,223.08,69.97,8.89;5,323.35,234.48,183.31,8.89">From RankNet to LambdaRank to LambdaMART: An overview</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
