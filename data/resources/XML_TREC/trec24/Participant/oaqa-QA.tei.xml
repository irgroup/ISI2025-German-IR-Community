<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.43,106.38,273.14,22.40;1,154.83,126.31,302.34,22.40">CMU OAQA at TREC 2015 LiveQA: Discovering the Right Answer with Clues</title>
				<funder>
					<orgName type="full">InMind Intelligent Agents</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,251.58,189.04,37.27,12.96"><forename type="first">Di</forename><surname>Wang</surname></persName>
							<email>diwang@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.22,189.04,52.19,12.96"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.43,106.38,273.14,22.40;1,154.83,126.31,302.34,22.40">CMU OAQA at TREC 2015 LiveQA: Discovering the Right Answer with Clues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6079A399630F06C7399A221F5D19F72C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present CMU's automatic, web-based, real-time question answering (QA) system that was evaluated in the TREC 2015 LiveQA Challenge. This system answers real-user questions freshly submitted to the Yahoo! Answers website that have not been previously answered by humans. Given the title and body of the question, we generated multiple sets of keyword queries and retrieved a collection of web pages based on those queries. Then we extracted answer candidates from web pages in the form of answer passages and their associated clue. Finally, we combined both IR-and NLP-based relevance models to rank and select answer candidates. In the TREC 2015 LiveQA evaluations, human assessors gave our system an average score of 1.081 on a three-point scale, the highest average score achieved by a system in the competition (the second-best score was .677, and the average score was .465 for the 21 systems evaluated).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>LiveQA is an emerging TREC challenge for automatic, real-time, user-oriented question answering (QA) systems. Real user questions are selected as inputs from a stream of newly submitted questions on the Yahoo! Answers site 1 , which have not yet received a human answer. As per the requirements for this track, participants must deploy their systems as web services which provide answers to questions in real time (within one minute). An additional requirement is that the answer text should be no more than 1000 characters in length. System responses are judged by TREC assessors on a 4-level Likert scale.</p><p>Attempting to answer user-generated questions in real time is an exciting but difficult challenge. The questions generated by real users are often ambiguous, ungrammatical and vary greatly in length, topic(s) and language style(s). Unlike previous TREC QA tracks, there is no pre-defined question type (factoid, list, or definition) associated with the question/answer pairs in the LiveQA challenge. There is also no constraint on what the user may ask about, which implies that there is no pre-defined answer type; much like the Jeopardy! challenge, we must deal with "lexical answer types" <ref type="bibr" coords="1,470.52,619.63,10.58,9.54" target="#b0">[1]</ref>. CMU's Open Advancement of Question Answering (OAQA) group developed a real-time webbased QA system for the LiveQA challenge in 2015. Since there is no official training corpus associated with the challenge, our approach leveraged the vast amount of text data that is available online, especially previously-answered questions from Yahoo! Answers. We also designed and implemented a new data model and novel relevance ranking methods for LiveQA. During the official run, our QA web service received one question per minute for 24 hours and provided answers within one minute for 97.9% of the input questions. On a normalized three-point average score metric, the CMUOAQA received a score of 1.081, which was the top score in the 2015 LiveQA evaluation (the second-best average score was .677, and the average over all system runs was .467). In this paper, we describe the OAQA LiveQA system in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>As illustrated in Figure <ref type="figure" coords="2,204.32,280.26,3.74,9.54" target="#fig_0">1</ref>, the architecture of our system decomposes the solution into three major processing phases:</p><p>1. Clue Retrieval. Given a question title and its full text description, we formulate search engine queries and issue them to different search engines (Bing Web Search, Yahoo! Answers) in order to retrieve web pages related to the question. 2. Answer Ranking. Answer candidates (title/body/answer tuples that represent either conceptional questions or answer texts) are extracted from web pages, and ranked based on a relevance estimator. The most effective relevance estimator we found was a heuristicallyweighted combination of: a) optimized BM25 similarity scoring over the title and body texts, and b) a recurrent neural network approach that estimates the relevance of a candidate answer text given a question text. 3. Answer Passage Tiling. Finally, a simple greedy algorithm is used to select a subset of highest-ranked answer candidates; these are simply concatenated without further processing in order to produce the final answer.</p><p>In the remainder of this section, we describe each phase and sub-component in greater detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input</head><p>Each question q transmitted to a LiveQA web service consists of four parts: title q title, body q body, category, and a unique question ID. Our system does not make use of the question category, and only uses the question ID to filter out retrieved Yahoo! Answers pages with the same ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clue Retrieval</head><p>Given a question, firstly we formulate multiple queries, then send them to the search engines, thirdly collect best matching web pages for each, and finally harvest a set of candidate answers for further processing.</p><p>The input question q = hq title, q bodyi is converted to sets of keyword queries. In order to achieve higher recall, we generate multiple sets of keywords from q title, q body, and the concatenation of both. However, the inclusion of full q body text may result in very long queries, for which the search engines do not return any result. Therefore we also generate keyword queries with only informative noun phrases, bigrams, and unigrams. The different keyword queries are then sent to search engines for web retrieval. We collect the snippets returned for each hit and also download the full text of the web page for each hit.</p><p>Our system utilizes Bing's web search API<ref type="foot" coords="2,286.00,692.47,3.49,6.68" target="#foot_1">2</ref> to retrieve answer candidates from the Web. Since Shtok et al. <ref type="bibr" coords="2,156.94,705.02,11.62,9.54" target="#b1">[2]</ref> showed that a significant amount of the unresolved Yahoo! Answers questions can be satisfactorily answered by reusing a best answer from the past, our system also uses the search function on Yahoo! Answers to collect additional answer candidates.</p><p>Our goal here is to search the Internet and select passages of text, containing information relevant to the question and indicating the presence of a candidate answer passage on the same page.</p><p>For this purpose, we build a general data structure called the answer clue (a clue), represented as hc title, c bodyi. This formulation can be used to represent (and effectively combine) either the title and body of a similar question on community QA (cQA) sites like Yahoo! Answers, or a document title and search snippet extracted from a web page. Finally, we heuristically select answer passage text a passage based on its relative position to a clue. Our algorithm favors passages using heuristics like: a) the voted best answer on cQA sites, b) the first reply on Internet forums, and c) paragraphs appearing in contexts which highly match search snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Ranking</head><p>Each candidate answer a = ha clue, a passagei is weighted by estimating how well both the a clue and a passage matches the question. The relevance ranking score between input question q and an answer candidate a is then defined as: Question q and answer clue a clue both contain two fields: title and body. Intuitively the title field is more concise and informative than the body field. The answer clue score S c is, then, computed as weighted sum of retrieval scores of all text and title only text:</p><formula xml:id="formula_0" coords="3,191.89,285.57,159.27,16.42">S(q, a) = S c (q, a clue) ⇥ (1 + w p ⇥ S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>c (q, a clue) = S bm25 (q title q body, c title c body) + w t ⇥ S bm25 (q title, c title) where w t (set to 1.0) is the weight to boost the title field matching score, and S bm25 is the Okapi BM25 <ref type="bibr" coords="3,137.03,456.16,11.62,9.54" target="#b2">[3]</ref> formula (with parameters K 1 = 1.0 and B = 0.75). The idf and average document length values that are required to compute BM25 were obtained by indexing the Yahoo! Answers Comprehensive Questions and Answers dataset 3 , which contains around 4.4 million Yahoo! Answers questions and their answers.</p><p>To integrate the term proximity information in our relevance estimation, we also use the sequential dependence variant of the Markov random field model <ref type="bibr" coords="3,337.79,516.93,11.62,9.54" target="#b3">[4]</ref> to formulate weighted queries to the BM25 function. The weights for unigram, bigram, and proximity are 0.8, 0.1, and 0.1 respectively. We also expanded unigram queries with synonyms from WordNet <ref type="bibr" coords="3,370.37,538.85,11.62,9.54" target="#b4">[5]</ref> when the query word had only one sense in WordNet.</p><p>To calculate the relevance score S p between question and answer passage, we employ a recurrent neural network based approach <ref type="bibr" coords="3,235.67,577.70,10.79,9.54" target="#b5">[6,</ref><ref type="bibr" coords="3,249.21,577.70,8.30,9.54" target="#b6">7]</ref> that uses a multilayer stacked bidirectional Long-Short Term Memory (BLSTM) network to sequentially read words from question and answer passages, and then output their relevance scores. Figure <ref type="figure" coords="3,274.82,599.62,4.98,9.54" target="#fig_2">2</ref> illustrates how we use the stacked BLSTM to model the answer passage ranking and selection problem. The words of input sentences were first converted to vector representations learned from the RNN-based language modeling tool word2vec <ref type="bibr" coords="3,475.45,621.54,10.58,9.54" target="#b7">[8]</ref>. In order to differentiate q title and a passage sentences, we inserted a special symbol, &lt;S&gt;, after the question sequence. Then, the question and answer sentence word vectors are sequentially read by BLSTM from both directions. In this way, the contextual information across words in both question and answer sentences is modeled by employing temporal recurrence in BLSTM. We used the same experiment settings described by Wang and Nyberg <ref type="bibr" coords="3,387.30,682.31,11.62,9.54" target="#b5">[6]</ref> to train this model, except that the training dataset was switched to a random subset of 100k questions from the Yahoo! Answers Comprehensive QA dataset. In order to adapt this dataset for model training, we follow the same data  <ref type="bibr" coords="4,310.71,270.61,11.62,9.54" target="#b8">[9]</ref> to generate negative labels by retrieving other answer passages from the collection. However, since the generated labels contain false negatives, the model may potentially learn low weights for instances with false negative training labels and decrease overall performance. To avoid this we set the combination weight w p = 0.1 for this year's evaluation, and plan to re-train the model and tune w p in the future using the questions and labeled responses from this year's LiveQA evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Passage Tiling</head><p>Given a ranked list of answer passages, the system must an optimal text that answers the question. Good answers vary in length from short to long, and may contain a short reference to a specific entity or factoid, or a long narrative explanation; in general, the ideal answer length varies with the user's information need (as expressed by the question). This year, we simply assumed that longer answers are preferred.</p><p>If the top-scoring answer passage was longer than 500 characters, then the first 1000 characters (LiveQA length constraint) are directly returned as the final answer text. If not, we applied an answer tiling algorithm, which assembles longer answer texts out of shorter answer passages. The algorithm proceeds greedily from the top-scoring answer passages to all subsequent candidates whose ranking score is higher than 90% of the highest score (top decile). Additional answer passages are appended to the final answer text, with a prefix to indicate the start of each new answer passage; e.g. "Opinion 2:" for the second passage, and so on. The tiling algorithm terminates when the length of the generated answer passage is greater than 50% of the maximum allowable length (1000 characters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Development Set Analysis</head><p>The LiveQA organizers provided a sample of 1000 Yahoo! Answers question IDs (QIDs), which we used as a development set for pre-evaluation analysis. Specifically, we used each question's title text as an input query, and then collected outputs from the Yahoo! Answers default "Search Answer" function. In particular, a list of returned QIDs was collected via "Relevance" search (instead of "Newest", "Most Answers", or "Fewest Answers" search). The first 100 search results for each query were collected <ref type="foot" coords="4,196.71,618.40,3.49,6.68" target="#foot_3">4</ref> and automatically labeled<ref type="foot" coords="4,312.35,618.40,3.49,6.68" target="#foot_4">5</ref> by comparing their source question IDs with input question IDs.</p><p>In order to validate our system's performance with this dataset, we processed the same query set with our retrieval and ranking modules, and returned a ranked list of question IDs. MRR (Mean Reciprocal Rank) and P@1 (Precision at one) were then used as evaluation metrics (calculated using the official trec eval evaluation scripts). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Official Evaluation Results</head><p>In this year's LiveQA evaluation, 1,087 questions (out of 1,340 submitted questions) were judged and scored using a 4-level Likert scale:</p><p>• 4: Excellent: "a significant amount of useful information, fully answers the question"</p><p>• 3: Good: "partially answers the question"</p><p>• 2: Fair: "marginally useful information"</p><p>• 1: Bad: "contains no useful information for the question"</p><p>• -2: "the answer is unreadable (only 15 answers from all runs)"</p><p>The evaluation measures used are:</p><p>• avg-score (0-3): "average score over all queries (transferring 1-4 level scores to 0-3, hence comparing 1-level score with no-answer score, also considering -2-level score as 0)"</p><p>• succ@i+: "number of questions with i+ score (i=1..4) divided by number of all questions"</p><p>• prec@i+: "number of questions with i+ score (i=2..4) divided by number of answered only questions"</p><p>Table <ref type="table" coords="5,132.89,552.06,4.98,9.54" target="#tab_0">2</ref> summarizes the results of our system run and average scores from all submitted runs. We believe the overall performance of our system to be encouraging, as it suggests that our system can provide a useful answer (fair, good, or excellent) for more than 53% of questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper presented the overall system architecture and individual phases and components for our LiveQA 2015 system 6 . Although this system performed significantly better than average for the 21 systems evaluated, the low absolute evaluation values indicate that there is still much room for improvement. One promising future direction is to utilize the scored answer passages from this year's evaluation to train supervised models for keyword generation, answer passage extraction and answer passage ranking. We can also extend the current pipeline by incorporating an answer confidence estimation to detect and prune low-quality final answers. Another potentially interesting challenge is to develop scalable evaluation methods that approximate the TREC assessor judgments in a more efficient and cost-effective way (through the use of crowd-sourcing), making it possible to develop much larger labeled datasets for supervised learning. We also intend to focus on more sophisticated models for answer tiling which a) combine answer passage relevance, uniqueness, and conciseness to select final answers, and b) consider post-processing of selected answer passages to make final answers more concise and readable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,188.06,175.28,235.87,9.54"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the CMU-OAQA LiveQA system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,351.74,281.80,2.19,8.20;3,351.16,294.87,4.10,1.76;3,355.77,285.79,11.44,9.13;3,368.87,292.41,51.22,2.51;3,108.00,306.56,24.34,9.54;3,135.97,313.29,9.67,3.26;3,149.77,306.56,14.39,9.54;3,167.79,313.29,6.11,2.51;3,174.47,303.19,2.19,8.20;3,173.90,315.76,4.10,1.76;3,182.13,306.56,321.87,9.54;3,108.00,324.25,11.23,3.26;3,122.56,317.52,283.00,9.54;3,408.38,324.25,10.21,3.26;3,421.92,317.52,82.08,9.54;3,108.00,329.60,92.55,9.54;3,201.13,332.72,15.90,1.76;3,200.56,338.79,4.10,1.76;3,220.26,329.60,14.39,9.54;3,237.36,336.33,6.11,2.51;3,244.05,332.72,14.82,1.76;3,243.47,338.79,4.10,1.76;3,259.36,329.60,244.64,9.54;3,108.00,340.56,90.21,9.54;3,264.18,366.37,6.11,2.51;3,270.86,355.77,2.19,8.20;3,270.28,368.84,4.10,1.76;3,277.66,359.76,7.74,9.13;3,295.56,358.67,29.00,3.26;3,325.13,355.06,14.82,1.76;3,324.56,361.14,4.10,1.76;3,289.37,370.33,22.59,5.38;3,295.48,375.67,4.10,1.76;3,324.64,370.33,21.50,5.38;3,330.74,375.67,4.10,1.76"><head>0p</head><label></label><figDesc>(q, a passage)) where S c and S 0 p are answer clue score and normalized answer passage scores respectively, and w p is the weight factor to merge them. The original answer passage score S p is normalized by the max and min values (S max p and S min p ) in the collection of answer passage scores generated for the current input question:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,171.14,236.18,269.73,9.54;4,167.40,81.86,277.20,143.43"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Answer passage ranking model based on stacked BLSTM</figDesc><graphic coords="4,167.40,81.86,277.20,143.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,108.00,86.37,396.00,214.22"><head>Table 2 :</head><label>2</label><figDesc>Official TREC 2015 LiveQA track evaluation results. Table1summarizes our preliminary experimental results on the Yahoo! Answers question retrieval and ranking task. Although good performance on this dataset does not necessarily correlate to good performance on the LiveQA 2015 challenge, it does demonstrate the necessity of developing nontrivial candidate retrieval and answer ranking methods for any LiveQA-related task.</figDesc><table coords="5,130.01,86.37,351.79,124.50"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Set Recall</cell><cell>P@1</cell><cell></cell><cell>MRR</cell></row><row><cell cols="4">Yahoo! Answers default search Clue Retrieval + Answer Ranking</cell><cell>0.8464 0.9100</cell><cell cols="3">0.5873 0.6434 0.8950 0.9008</cell></row><row><cell></cell><cell cols="5">Table 1: Results on development dataset</cell><cell></cell><cell></cell></row><row><cell>Run ID</cell><cell>Avg score (0-3)</cell><cell>1+</cell><cell cols="2">Success@ 2+ 3+</cell><cell>4+</cell><cell>2+</cell><cell>Precision@ 3+</cell><cell>4+</cell></row><row><cell>CMU-OAQA</cell><cell>1.081</cell><cell cols="7">0.979 0.532 0.359 0.190 0.543 0.367 0.195</cell></row><row><cell>Avg of all runs</cell><cell>0.465</cell><cell cols="7">0.925 0.262 0.146 0.060 0.284 0.159 0.065</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,124.14,729.84,139.88,2.14"><p>https://answers.yahoo.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,124.14,729.84,274.37,2.14"><p>https://datamarket.azure.com/dataset/bing/searchweb</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="3,124.14,729.84,301.27,2.14"><p>http://webscope.sandbox.yahoo.com/catalog.php?datatype=l</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,124.14,702.95,92.84,8.59;4,218.87,709.01,285.13,2.14;4,108.00,718.97,173.65,2.14"><p>This dataset is available at https://github.com/yuvalpinter/LiveQAServerDemo/tree/ master/data/1k-ya-search-results</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,124.14,723.79,224.98,8.59"><p>Note that the generated labels will also contain false negatives.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,124.14,729.84,161.39,2.14"><p>https://github.com/oaqa/LiveQA</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by the <rs type="funder">InMind Intelligent Agents</rs> project funded through a generous gift from Yahoo!. We would also like to thank <rs type="person">Dan Pelleg</rs>, <rs type="person">David Carmel</rs>, and <rs type="person">Eugene Agichten</rs> for insightful discussions.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,124.60,249.96,379.41,9.54;6,124.60,260.92,379.41,9.54;6,124.60,278.60,175.05,2.71;6,302.14,271.88,56.73,9.54" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,325.56,260.92,174.62,9.54">Question analysis: How watson reads a clue</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Mccord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Branimir</forename><surname>Boguraev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,124.60,278.60,170.48,2.71">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,124.60,286.82,379.41,9.54;6,124.60,297.78,164.02,9.54;6,291.68,304.51,212.32,2.71;6,124.60,315.47,68.17,2.71;6,195.26,308.74,87.44,9.54" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,373.14,286.82,130.86,9.54;6,124.60,297.78,146.01,9.54">Learning from the past: Answering new questions with past answers</title>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoelle</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,291.68,304.51,212.32,2.71;6,124.60,315.47,63.35,2.71">Proceedings of the 21st International Conference on World Wide Web</title>
		<meeting>the 21st International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,124.60,323.69,379.41,9.54;6,124.60,334.64,29.67,9.54;6,156.66,341.37,347.34,2.71;6,124.60,352.33,154.10,2.71;6,281.19,345.60,77.48,9.54" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,295.09,323.69,208.92,9.54;6,124.60,334.64,14.39,9.54">On relevance weights with little relevance information</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,156.66,341.37,347.34,2.71;6,124.60,352.33,150.21,2.71">Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,124.60,360.55,379.41,9.54;6,124.60,378.23,379.40,2.71;6,124.60,389.19,171.53,2.71;6,298.62,382.46,87.44,9.54" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,275.67,360.55,211.33,9.54">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,124.60,378.23,379.40,2.71;6,124.60,389.19,167.65,2.71">SIGIR 2005: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,124.60,397.41,241.18,9.54;6,371.16,404.13,119.82,2.71;6,494.04,397.41,9.96,9.54;6,124.60,408.37,71.67,9.54" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,201.75,397.41,160.12,9.54">Wordnet: A lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,371.16,404.13,114.02,2.71">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,124.60,423.31,379.41,9.54;6,124.60,434.27,90.85,9.54;6,217.70,441.00,261.31,2.71;6,481.31,434.27,22.68,9.54;6,124.60,445.23,62.27,9.54" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,236.72,423.31,267.29,9.54;6,124.60,434.27,75.12,9.54">A long short-term memory model for answer sentence selection in question answering</title>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,217.70,441.00,257.45,2.71">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="707" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,124.60,460.17,379.41,9.54;6,124.60,471.13,91.58,9.54;6,218.66,477.86,257.26,2.71;6,478.42,471.13,22.42,9.54" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,239.73,460.17,264.28,9.54;6,124.60,471.13,75.35,9.54">A recurrent neural network based answer ranking model for web question answering</title>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,218.66,477.86,253.20,2.71">SIGIR Workshop on Web Question Answering: Beyond Factoids</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,124.60,486.08,379.41,9.54;6,124.60,497.03,248.12,9.54;6,375.28,503.76,128.73,2.71;6,124.60,514.72,92.80,2.71;6,219.89,507.99,97.40,9.54" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,432.60,486.08,71.40,9.54;6,124.60,497.03,231.98,9.54">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,375.28,503.76,128.73,2.71;6,124.60,514.72,77.86,2.71">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,124.60,522.94,379.41,9.54;6,124.60,533.90,175.05,9.54;6,303.24,540.62,108.50,2.71;6,414.23,533.90,86.61,9.54" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,388.80,522.94,115.21,9.54;6,124.60,533.90,171.25,9.54">Learning to rank answers to non-factoid questions from web collections</title>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,303.24,540.62,104.65,2.71">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="383" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
