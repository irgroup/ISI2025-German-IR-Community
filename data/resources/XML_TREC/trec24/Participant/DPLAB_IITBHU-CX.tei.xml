<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.76,115.96,311.85,12.62;1,274.41,133.89,66.54,12.62">Contextual Suggestion using tag-description similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.60,171.56,90.75,8.74"><forename type="first">Manajit</forename><surname>Chakraborty</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology(BHU)</orgName>
								<address>
									<postCode>221005</postCode>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.45,171.56,65.00,8.74"><forename type="first">Hitesh</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology(BHU)</orgName>
								<address>
									<postCode>221005</postCode>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.60,171.56,82.27,8.74"><forename type="first">Himanshu</forename><surname>Shekhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology(BHU)</orgName>
								<address>
									<postCode>221005</postCode>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,253.89,183.51,107.57,8.74"><forename type="first">Ravindranath</forename><surname>Chowdary</surname></persName>
							<email>rchowdary.cse@iitbhu.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology(BHU)</orgName>
								<address>
									<postCode>221005</postCode>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.76,115.96,311.85,12.62;1,274.41,133.89,66.54,12.62">Contextual Suggestion using tag-description similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D492784EA154C9D24D159A56866EF74C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our approach for the Contextual Suggestion track of 2015 Text REtrieval Conference (TREC). The task aims at providing recommendations on points of attraction for different kind of users and a varying context. Our group DPLAB IITBHU tries to address the problem from the perspective of how relevant the attractions are based on user profiles and rank them based on two similarity measures-wup similarity and another similarity measure proposed by us.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Contextual Suggestion track investigates search techniques for complex information needs that are highly dependent on context and user interests. The task is to provide suggestions to users based on their personal interests as well as their contexts. To tackle this problem, we propose to rank candidate suggestions based on their similarity to the personal profile to the contexts (i.e., geographic and temporal information etc.). The ranking function is computed based on the similarity between a suggestion and the places that the user like and the dis-similarity between the suggestion and the places disliked by the user. The similarities are computed based on the description of the suggestions. The paper describes the first participation of Indian Institute of Technology (BHU) in 2015 TREC Contextual Suggestion track. The descriptions for suggestions were gathered from the url s provided in the collection. We got two runs for the final results based on two similarity measures. The details of our recommendation model include data pre-processing, rating and finally ranking the candidate suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In this section we describe the methodology used to generate the suggestion list for users based on their profiles and the given context. It is to be noted that we used a similar method to generate the recommendation list, the difference being the technique used to calculate similarity between the attractions. The work-flow is as follows:</p><p>-Step 1: Extracting description of the attractions provided in the batch experiment candidate suggestions and pre-processing them (30 for each user)</p><p>(i) First of all, we used a script written to extract information from the url of each attraction provided in the candidate suggestion list. (ii) For attractions (and its corresponding url s) for which the above script couldn't pull the required information due to page redirection or other accessibility issues, we used the Foursquare API <ref type="foot" coords="2,385.26,248.56,3.97,6.12" target="#foot_0">1</ref> . Using this API we extracted the category of the attraction in question along with the user reviews mentioned there. Henceforth, we will refer to the information extracted from url s as 'description' unless otherwise mentioned. (iii) Next we pre-processed the descriptions by removing sentences which contained irrelevant and garbled words. (iv) The stop words and special characters (!@#$%Ë†etc.) were removed and only intelligible words were retained. (v) The filtered descriptions were then tagged using a Parts-of-Speech (POS)</p><p>tagger <ref type="bibr" coords="2,198.14,359.09,10.52,8.74" target="#b0">[1]</ref> in order to keep only nouns, verbs, adverbs and adjectives while discarding the rest.</p><p>-Step 2: Finding the tags of each candidate suggestion: Our whole procedure depends on matching user provided tags with attraction description. Hence, this step is of prime importance, as we try to capture the essential elements from the descriptions in terms of tags.</p><p>(i) We generated a list for similarity list which contained tags provided with TREC 2015 Contextual Suggestion track dataset and processed it. (ii) From observation we found out that some tags were very specific while others were pretty generic. To distinguish between them, we marked each tag as either 0 or 1. If the tag is marked with 0, it implies that the tag has to be matched directly with the attraction description. On the other hand, if it is marked with 1, then instead of directly using the tags we expand it using synsets from Wordnet[2] and then used for matching with the descriptions. This step is done so as to encompass related terms such as 'cuisine' with 'food' etc. For bi-gram tags, each word in the tag was expanded using synset before matching. (iii) In cases of suggestions for which tags weren't explicitly provided, we extracted the description from the url and matched them across the list of all tags. The most similar ones were used as tags for that particular attraction. The similarity matching was done using wup similarity [3] with a threshold of 0.95.</p><p>(iv) Finally, all the tags were matched against candidate suggestion descriptions and the matching ones were appended in a list of that particular suggestion.</p><p>-Step 3: Assigning rating to each candidate suggestion: Approach 1 (RUN 1): The complete process discussed below is carried out for each user profile. (a) A dictionary D 1 is prepared from all the rated attractions with each entry having the highest rated attraction tags first, followed by next highest rated attraction tags and so on. The outline of such a dictionary is provided in Table <ref type="table" coords="3,258.66,236.75,3.87,8.74" target="#tab_0">1</ref>. (b) Now each candidate suggestion description was matched against this dictionary D 1 . This was done to rate the particular attraction with a rating having maximum number of matching tags. So, suppose a suggestion description was matched against the dictionary and the following result came out (Table <ref type="table" coords="3,243.08,452.41,3.87,8.74" target="#tab_1">2</ref>): Then the candidate suggestion is assigned a rating of 3 since there are 6 matched tags. In case of a tie, the highest rating among the choices is assigned. (c) Similarly, for the rest of the 29 candidate suggestions a list of (Rating, No. of Matched tags) is prepared. The list was sorted first on the basis of rating and then further within each rank on the basis of number of tags matched. (d) If the matching of a candidate suggestion's description with tags returned 0, then it wasn't included in the list. This is the prime reason why in the evaluation our approach failed to retrieve any suggestion for few profiles.</p><p>Approach 2 (RUN 2): Similar to Approach 1, this process is carried out for each profile as well. (a) A dictionary was prepared containing a list for each rating. Each element in this list represent the count of tags with rating X where X varies from 4 to -1. E.g., for a rated attraction if the rating was 3, then in list corresponding to rating 3, the count of tags was incremented by 1. (b) Once all the rated attractions were processed, the list corresponding to each rating was normalized , i.e., count of each tag is divided by total number of count in that rating. Now, sum of count of all tags in a particular rating equals to 1. (c) Now each candidate suggestion tags were matched to each rating list and the corresponding score of each tag was added to give the total score of that suggestion. E.g if tags a, b, d occurs in a candidate suggestion, for each rating, their corresponding scores were added and the rating with maximum score was assigned to that suggestion. (d) If no matching tags were found for a particular suggestion, it wasn't added to our list. In case of collision the highest rating was assigned. (e) The list was sorted first on the basis of rating and then within each rank on the basis of total score obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head><p>We employed two techniques for matching tags with candidate suggestion descriptions to find the best possible suggestion for a particular context. These suggestions were then ranked according to the user preferences as provided in the profiles. Among the two approaches, Approach 2 performed better than Approach 1. This can be attributed to the fact that the ranking system in Approach was more fine-grained with each tag carrying its own weight unlike in Approach 1 where either the tags were considered or rejected. So, the number of retrieved suggestions were generally greater in Approach 2. Since, we weren't provided with the evaluation results of other teams, we can't perform a comparative assessment. But, speaking in terms of improvement our approaches perform better than the median results as listed in Table <ref type="table" coords="4,319.60,575.50,3.87,8.74" target="#tab_2">3</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,182.66,277.36,250.04,98.80"><head>Table 1 .</head><label>1</label><figDesc>Dictionary of Tags with Ratings</figDesc><table coords="3,182.66,298.16,250.04,32.57"><row><cell>Rating</cell><cell>Tags</cell></row><row><cell>4</cell><cell>tags from attraction xv tags from attraction yv ...</cell></row><row><cell>3</cell><cell>tags from attraction xx tags from attraction yx ...</cell></row></table><note coords="3,193.98,334.23,4.61,7.86;3,217.29,334.23,215.41,7.86;3,193.98,345.58,4.61,7.86;3,217.38,345.58,215.32,7.86;3,193.98,356.94,4.61,7.86;3,217.06,356.94,215.64,7.86;3,192.45,368.30,7.68,7.86;3,216.29,368.30,216.41,7.86"><p>2 tags from attraction xy tags from attraction yy ... 1 tags from attraction xz tags from attraction yz ... 0 tags from attraction xu tags from attraction yu ... -1 tags from attraction xw tags from attraction yw ...</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,156.45,494.96,302.46,98.80"><head>Table 2 .</head><label>2</label><figDesc>Result of matching a candidate suggestion against the dictionary</figDesc><table coords="3,225.91,515.76,163.55,78.00"><row><cell cols="2">Rating Number of matched tags in a list</cell></row><row><cell>4</cell><cell>5</cell></row><row><cell>3</cell><cell>6</cell></row><row><cell>2</cell><cell>6</cell></row><row><cell>1</cell><cell>2</cell></row><row><cell>0</cell><cell>3</cell></row><row><cell>-1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,134.77,115.91,345.83,165.06"><head>Table 3 .</head><label>3</label><figDesc>Evaluation Results natural language processing and computational linguistics-Volume 1, pages 63-70. Association for Computational Linguistics, 2002. 2. George A Miller. WordNet: a lexical database for english. Communications of the ACM, 38(11):39-41, 1995. 3. Zhibiao Wu and Martha Palmer. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133-138. Association for Computational Linguistics, 1994.</figDesc><table coords="5,170.54,134.97,274.28,43.93"><row><cell></cell><cell cols="4">P@5 R-Precision Reciprocal Rank MAP</cell></row><row><cell cols="3">Approach 1 (IITBHU 1) 0.5308 0.4941</cell><cell>0.6760</cell><cell>0.5581</cell></row><row><cell cols="3">Approach 2 (IITBHU 2) 0.5365 0.4969</cell><cell>0.7030</cell><cell>0.5639</cell></row><row><cell>Average Median 2</cell><cell>0.5090</cell><cell>-</cell><cell>0.6716</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.80,138.92,7.86"><p>https://developer.foursquare.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.35,628.10,342.25,7.86;4,146.91,639.06,333.67,7.86;4,137.50,655.03,3.65,5.24;4,144.73,658.16,216.36,6.12" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,287.85,628.10,151.18,7.86">NLTK: The natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,462.94,628.10,17.66,7.86;4,146.91,639.06,333.67,7.86;4,137.50,655.03,3.65,5.24;4,144.73,658.16,216.36,6.12">Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching 2 TREC 2015 Contextual Suggestion Batch Experiment Results</title>
		<meeting>the ACL-02 Workshop on Effective tools and methodologies for teaching 2 TREC 2015 Contextual Suggestion Batch Experiment Results</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
