<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.14,72.35,355.45,16.84">WaterlooClarke: TREC 2015 LiveQA Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,108.26,118.05,100.70,11.06"><forename type="first">Alexandra</forename><surname>Vtyurina</surname></persName>
							<email>avtyurina@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.08,118.05,57.56,11.06"><forename type="first">Ankita</forename><surname>Dey</surname></persName>
							<email>a5dey@uwaterloo.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.35,118.05,113.51,11.06"><forename type="first">Bahareh</forename><surname>Sarrafzadeh</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.32,158.30,107.08,11.06"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<email>claclark@plg.uwaterloo.ca</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.14,72.35,355.45,16.84">WaterlooClarke: TREC 2015 LiveQA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">95C24A643BECAC9B7D4A69D06396AE79</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of the LiveQA track is to automatically provide answers to questions posted by real people. Previous question answering tracks included factoid questions, list questions and complex questions <ref type="bibr" coords="1,144.20,252.41,11.41,7.86" target="#b3">[3]</ref>. Presented in 2015 for the first time the LiveQA track gave the participants an opportunity to answer questions posed by real people, as opposed to manually configured ones in the previous tasks.</p><p>The questions for the task were harvested from Yahoo! Answers 1 -a community question answering website. Each question was broadcasted to all registered systems. The participants had to supposed to provide an answer to every given question within a timeframe of 60 seconds. The answers were judged by human NIST assessors after the evaluation was over.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The task of automatic question answering has appeared a multiple times in TREC. The tracks have moved from answering factoid questions to list questions, and questions with complex information need. Although the questions were modelled to imitate human askers, they were not coming from real people and the answers often had to be extracted from a restricted corpora of newswire and blogposts.</p><p>LiveQA track brought the task to the new level by providing real world questions, unlimited corpora usage and restricting the answer time. The questions for this task were coming from Yahoo! Answers -a community question answering website. Questions there vary greatly between all topics and question types. Yahoo! Answers users are often seeking other people's opinion, an advice about a problem they're having. Some of them just want to share their insights or emotions about newly acquired knowledge or experience. In certain cases people do not have a well defined information need, but they are looking to start a conversation (see The questions were collected from the list of newly posted (and not yet answered by human users) questions on Yahoo! Answers. Each question was sent to every participating system and an answer was expected to arrive within a 60-seconds window. The answer was supposed to contain, among other fields, a text snippet of length less than 1000 characters and the list of resources, from which it was obtained. If the answer was received after 1 minute, it did not count towards the total score of the system. Participants could also choose not to answer any question.</p><p>The evaluation of the answers given by participating systems was done by human NIST assessors on a 5-level Likert scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">EXPERIMENTAL SETUP</head><p>The experiment was running for the duration of 24 hours starting at 12am PST August 31, until 12am PST on September 1. During this period of time the participating systems were supposed to be online. Yahoo! server collected newly posted question from Yahoo! Answers and broadcasted it to all the registered systems at a rate of approximately 1 question per 60 seconds.</p><p>Every question consisted of 4 fields: qid -question identifier, title -a question, formulated by a person, body -optional detailed description of the question, and finally, categorythe category that the person chose for their question (if the user skips the step of picking a category, it is defined automatically by Yahoo! Answers).</p><p>A response to each question was expected upon 60 seconds after sending. It was supposed to contain the following fields: pid -participant id (uwaterlooclarke was used for this sub-Figure <ref type="figure" coords="2,86.91,303.36,4.12,7.89">1</ref>: An example of a long question, containing a lot of detailed information mission), qid -question identifier, answer -a text of length of max 1000 characters, sources -a list of sources where the answer was fetched from, local time -locally measured time in ms it took to produce the answer, explanation -an optional string containing additional information about the answer. Responses that were received after the 60 seconds were not judged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GENERAL APPROACH</head><p>Our approach was based on finding an answer on the internet using a search engine. After receiving a question we picked key terms from its text and constructed a query. We then used the query to retrieve a set of top-ranked web documents from Bing. Afterwards, we used the obtained set of documents to extract passages that were likely to answer the question. Finally, we ranked all the passages and returned the highest-ranked one as an answer to the given question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background model</head><p>In order to use KL-divergence for query term extraction we needed to have a background language model. Given that the language used in online user-generated content differs significantly from formal English <ref type="bibr" coords="2,189.66,588.59,9.20,7.86" target="#b1">[1]</ref>, we needed to have an example of the language that is usually used on Yahoo! Answers.</p><p>We crawled Yahoo! Answers to collect a dataset of questions and answers from all categories in order to see what type of language is used by people there 23 . For each question thread we collected question title, question body, and answers (if any), posted by other people. We removed web links from obtained text and used the rest of the text to build a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer extraction</head><p>For every question received we combined its title and body together and removed the links from the resulting text snippet. We compared the words distributions in the question text and the previously constructed language model and picked the words with the greater divergence value, which means that these words distinct the given question from the common language. For every word in the text a corresponding KL-divergence <ref type="bibr" coords="2,386.95,177.31,12.69,7.86" target="#b2">[2]</ref> value was computed, using the Yahoo! Answers language model constructed earlier. Afterwards, the words were sorted based on their corresponding KLD score. We also used NLTK<ref type="foot" coords="2,425.12,206.92,3.65,5.24" target="#foot_3">4</ref> to extract named entities from the question text. These named entities as well as the 4 words with the highest KLD score were put together in the order of their occurrence in the initial question to form a resulting query.</p><p>The query was submitted to the Bing Search API and the top-10 returned documents were retrieved. We ignored pages from Yahoo! Answers, as well as all non-html pages (for example, pdf). For every web-page we allowed a 5 seconds time limit to load, otherwise it was ignored. The response from Bing came in json format and contained description -a short text snippet extracted from a document, and the document's url. We used this set of web documents as a corpus to extract an answer to the given question from.</p><p>After the web pages are retrieved, they undergo a preprocessing step, during which only useful text was extracted from each of them. First, we removed the contents of a predefined list of tags (that are highly unlikely to contain the useful text that we are after): style, script, table, label, title, etc. From the remaining portion of the page the tags, with contents of less than 10 words are removed. By doing this we excluded ads, "follow us" links, and other irrelevant information.</p><p>After the preprocessing every web page becomes a clean raw text. At this step we insert a pair of special symbols used to denote the beginning and the end of each sentence. This is done in order to produce more readable results in the future. For every document we found a set of m-covers (passages containing keywords), using the terms from the query we previously submitted to Bing. If the length of a passage was greater than the given limit (1000 characters), it was discarded. The remaining passages were ranked according to the number of query terms they contained and their proximity to each other within the passage <ref type="bibr" coords="2,460.56,585.28,11.60,7.86" target="#b2">[2]</ref>. After the passages were scored, the highest-ranked one was considered to be the answer. At this point the borders of the passage were stretched to the closest beginning and end of a sentence. The URL, corresponding to was final passage is passed along as the resource of the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CODE BASE</head><p>The primary module (Java module) for communication with Yahoo! server was supplied by the track organizers 5 . We used a separate module written in Python to process incoming questions and extracting answers. The two modules communicated with each other using Twisted<ref type="foot" coords="3,240.68,76.79,3.65,5.24" target="#foot_5">6</ref> networking library by sending to each other messages in json format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">FUTURE WORK</head><p>We would like to improve the procedure of finding answer to a given question by analysing existing human-generated question-answer pairs. We are hopeful that finding the ways in which an answer is related to the question will help extract more precise answers in the future.</p><p>It is not uncommon for community question answering services to have an exceedingly long question descriptions. People often want to see an advice that is unique for their situation (see figure <ref type="figure" coords="3,121.24,218.03,3.58,7.86">1</ref>). Redundant details often obstruct question focus, making it hard even for a human to understand. We want to reduce such long questions to a length of 2-3 sentences by extracting only the sentences, reflecting the user's information need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>The LiveQA track revives the task of automatic question answering in TREC. It provides an opportunity for the participants to try their QA systems on real-world questions, collected from Yahoo! Answers -community question answering website. The approach we chose is based on picking key terms from a given question, submitting them to a search engine and extracting an answer from the top 10 retrieved documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,53.80,53.79,239.11,234.79"><head></head><label></label><figDesc></figDesc><graphic coords="2,53.80,53.79,239.11,234.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,270.65,207.92,290.65,391.01"><head>Table Title :</head><label>Title</label><figDesc>Is my nose too big? Body: Is my nose bad or horrible. I know it's bigger but just how bad is it Title: Whats the meaning of life? Body: i wanna know YOUR meaning of life! Title: Emma Stone, Mila Kunis or Penelope Cruz? Who's most beautiful in your opinion? Body: I can't decide they are all gorgeousss &lt;3 :)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="1,316.81,303.40,239.10,46.11"><head>Table 1 :</head><label>1</label><figDesc>Various types of questions asked on Yahoo! Answers</figDesc><table coords="1,316.81,341.64,10.73,7.86"><row><cell>1).</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,58.40,609.45,108.84,7.86"><p>https://answers.yahoo.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,58.40,691.89,166.90,7.86;2,53.80,700.86,194.91,7.86"><p>All code used for this task is available at: https://github.com/sashavtyurina/LiveQATrack</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,58.40,711.19,211.35,7.86"><p>https://github.com/yuvalpinter/LiveQAServerDemo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,321.42,700.86,64.95,7.86"><p>http://nltk.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,321.42,711.19,211.35,7.86"><p>https://github.com/yuvalpinter/LiveQAServerDemo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,58.40,711.19,111.86,7.86"><p>https://twistedmatrix.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,58.28,389.76,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,67.99,402.34,210.70,7.86;3,67.99,412.80,198.65,7.86;3,67.99,423.27,191.01,7.86;3,67.99,433.73,207.65,7.86;3,67.99,444.19,86.14,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,115.40,412.80,151.24,7.86;3,67.99,423.27,22.17,7.86">Finding high-quality content in social media</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,109.15,423.27,149.86,7.86;3,67.99,433.73,178.12,7.86">Proceedings of the 2008 International Conference on Web Search and Data Mining</title>
		<meeting>the 2008 International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="183" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,67.99,455.64,188.83,7.86;3,67.99,466.10,207.42,7.86;3,67.99,476.57,128.54,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="3,67.99,466.10,207.42,7.86;3,67.99,476.57,56.00,7.86">Information retrieval: Implementing and evaluating search engines</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>BÃ¼ttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,67.99,488.02,212.14,7.86;3,67.99,498.48,185.55,7.86;3,67.99,508.94,137.13,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,216.16,488.02,63.98,7.86;3,67.99,498.48,138.48,7.86">Overview of the trec 2007 question answering track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,224.87,498.48,22.94,7.86">TREC</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
