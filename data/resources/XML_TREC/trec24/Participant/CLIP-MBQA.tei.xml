<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,123.68,70.18,362.35,20.62">CLIP at TREC 2015: Microblog and LiveQA</title>
				<funder ref="#_6AyhFrF">
					<orgName type="full">Qatar National Research Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Qatar Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.96,116.43,99.84,13.53"><forename type="first">Mossaab</forename><surname>Bagdouri</surname></persName>
							<email>mossaab@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.47,116.43,90.71,13.53"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="department">iSchool and UMIACS</orgName>
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,123.68,70.18,362.35,20.62">CLIP at TREC 2015: Microblog and LiveQA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2E12C6C4ECE4B2EE17AD710EAFB481DE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Computational Linguistics and Information Processing lab at the University of Maryland participated in two TREC tracks this year. The Microblog Real-Time Filtering and the LiveQA tasks both involve information processing in real time. We submitted nine runs in total, achieving relatively good results. This paper describes the architecture and configuration of the systems behind those runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We participated in Microblog Real-Time Filtering and the LiveQA TREC 2015 tracks. Efficiency was taken in consideration, as the topics of both of these tasks are timesensitive. We describe the systems for these tasks in the following two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MICROBLOG REAL TIME FILTERING</head><p>The TREC Microblog track is concerned this year with the task of real-time filtering. 1 A user has an interest in some broad topic, and wants to stay up to date in that topic using a stream of microblog posts. The setup is operationalized in two scenarios. In the push notifications scenario (Scenario A), a system should notify the user with novel and relevant tweets within a short time after they are first posted. However, the user should not be bombarded with too many notifications. A limit of 10 notifications per day is therefore enforced. In the email digest scenario (Scenario B), a system filters the tweets of a particular day, few hours after the end of that day, to retrieve a ranked list of 100 potentially relevant and novel tweets.</p><p>The track ran for 10 days from July 20 to 29, 2016 (UTC), and was based on Twitter's sample stream (i.e., a random 1% sample of all public tweets). We submitted six automatic runs, three for each scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Relevance Model</head><p>A topic is represented as a triple of a title that contains few keywords, a description that summarizes the topic in one sentence, and a narrative that consists of a paragraph that gives more details. In all of our systems, we restricted ourselves to the title and description fields. Hence, we do not refer to the narrative field henceforth. We stem the topic fields with Porter stemmer as implemented in Lucene 5.3,<ref type="foot" coords="1,551.77,346.38,3.65,5.48" target="#foot_0">2</ref> using its default list of stopwords. We use regular expressions to normalize all the tweets by removing emoticons, user mentions, URLs, RT indicators, and punctuation, before stemming them.</p><p>Our relevance model is based on Okapi BM25 term weights and title expansion using word embeddings and probabilistic structured queries. We use 1.6B English tweets from a corpus covering about 4 years of a Twitter sample stream, accessible from the Internet Archive 3 to train a word2vec model <ref type="bibr" coords="1,344.42,452.81,9.72,8.21" target="#b4">[4]</ref> and to estimate the document frequency (DF) of each term. The word2vec model is used to expand the title query stems with additional similar stems using the Euclidian distance over 200-dimensional vectors.</p><p>Let ti be a stemmed query term in the title, ti,j one of the top J stemmed terms similar to, but different from, ti, with a similarity value of Pi,j; and d an incoming tweet. The score of the expanded title query is computed as:</p><formula xml:id="formula_0" coords="1,317.48,552.44,237.76,17.85">Score(d, Q T itle,exp ) = i BM 25(T F (ti,exp, d), DF (ti,exp)),</formula><p>where the expanded term frequency is estimated as:</p><formula xml:id="formula_1" coords="1,344.27,601.93,184.19,17.85">T F (ti,exp, d) = T F (ti, d) + j Pi,jT F (ti,j, d),</formula><p>and the expanded document frequency as:</p><formula xml:id="formula_2" coords="1,355.77,651.58,161.18,17.86">DF (ti,exp) = DF (ti) + j Pi,jDF (ti,j).</formula><p>The score for the description field is simply the BM25 model. That is, Finally, the relevance score of a tweet d for a topic Q is defined with respect to its title and description as:</p><formula xml:id="formula_3" coords="2,57.77,396.86,231.15,14.78">Score(d, Q) = Score(d, Q T itle,exp ) + α × Score(d, QDesc).</formula><p>We tune the parameters using a grid search on the TREC 2014 Microblog track topics. We set k1 = 0.09, b = 0.5 and average document length = 21 for BM25, J = 5 and α = 0.3.</p><p>For Scenario B only, we eliminate tweets that are not ranked among the top 10,000 tweets at the end of this stage. The remaining tweets go to the rescoring stage, for both scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Tweet Rescoring</head><p>To refine the scores of the relevant tweets, we use the SVM rank package <ref type="bibr" coords="2,126.58,518.75,9.72,8.21" target="#b2">[2]</ref> to train a learning-to-rank model based on the TREC 2014 Microblog track topics, using the relevance score of the previous stage (Section 2.1.1), in addition to the following features:</p><p>• Sender feature: log of the ratio of the number of followers to the number of friends.</p><p>• Tweet features: count of stems, count of stems that are not stopwords, ratio of the previous two features, count of characters in the stemmed tweet, count of URLs, count of hashtags, and count of user mentions.</p><p>• Tweet -query similarity features: the two variants of the Jaccard similarity between the tweet and the description of the topic proposed by Magdy et al. <ref type="bibr" coords="2,76.21,679.96,9.21,8.21" target="#b3">[3]</ref>, the BM25 similarity score between the expanded title and the tweet as explained in Section 2.1.1, and the cosine similarity between the doc2vec vector (i.e., the sum of the word2vec vectors) of the stemmed terms of the tweet and the doc2vec vector for the description field of the topic.</p><p>For Scenario A only, tweets that have a (re-scored) score less than a threshold β are eliminated at the end of this stage. The remaining tweets go to the novelty detection phase, for both scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Novelty Detection</head><p>According to the task definition, a tweet is not considered interesting when the information it conveys has already been reported in an antecedent tweet that was present in the 1% public sample. We implement novelty detection with online single-link clustering based on the Jaccard similarity between the stemmed tweets. For each incoming tweet for topic Q (other than those removed by scenario-specific processing following the relevance or rescoring stage), we assign the tweet to the cluster containing the most similar tweet, if the similarity exceeds certain threshold τ . Otherwise, a new cluster is created and the incoming tweet is assigned to it. We maintain the same set of clusters for the entire 10 days of the live experiment since we don't want to return a relevant tweet if a similar one was returned even in a previous day.</p><p>In the case of Scenario A, the tweet is pushed to the user as soon as a new cluster is created. In the case of Scenario B, at the end of every (UTC) day, the tweet with the highest reranking score from each cluster from which no tweet has been reported on a prior day is selected and added to the ranked list for that day. In either scenario, that cluster is then marked so that it won't be used to suggest interesting tweets to the user (although it will keep gathering similar tweets, so that no new cluster with similar content is created).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Configurations of Submitted Runs</head><p>We submitted three runs for each scenario:</p><p>• CLIP-A-5.0-0.5 and CLIP-A-5.0-0.6 are two runs for Scenario A, where the score threshold (after rescor-  • CLIP-A-DYN-0.5 is a run for Scenario A, where the clustering similarity score was set to τ = 0.5. However, the score threshold (after rescoring) was set per topic, and was tuned with a grid search, so that each topic had about 10 interesting tweets on July 18, 2015 (i.e., two days before the evaluation period).</p><p>• CLIP-B-0.4, CLIP-B-0.5 and CLIP-B-0.6 are three runs for Scenario B, where the clustering similarity score threshold was set to τ = 0.4, τ = 0.5 or τ = 0.6 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>The runs in Scenario A were evaluated using two metrics, each is based on gain and latency discount. The gain of a tweet G(t) is a graded relevance score of 0 (not relevant), 0.5 (relevant) or 1 (highly relevant). A latency discount of max(0, (100d)/100) is then applied to every tweet with a delay d in minutes (rounded down). 4 The first metric is the expected latency-discounted gain (ELG), where the sum of the gains is divided by the number of tweets returned. The second metric is the normalized cumulative gain (nCG), where the same sum of gains is divided by the maximum possible gain.</p><p>Table <ref type="table" coords="3,88.72,535.70,4.60,8.21" target="#tab_0">1</ref> shows the results from our systems for this scenario. It appears that setting the relevance score threshold β dynamically per topic was a good approach. Our best system (CLIP-A-DYN-0.5) ranked 21st out of 30 automatic runs based on ELG, but 3rd based on nCG. This suggests that our relevance model, perhaps, performed well. But we were penalized for attempting to return about 10 tweets a day per topic, while many fewer than that were typically actually relevant. It appears that a higher relevance score threshold could have led to a higher ELG score.</p><p>The runs of Scenario B were evaluated using nDCG@10. Table <ref type="table" coords="3,78.65,650.77,4.60,8.21">2</ref> shows the results from our systems for this scenario. It appears that setting a high clustering similarity threshold τ has a positive impact, on average. Our best system (CLIP-B-0.6) ranked 1st out of 36 automatic runs. 4 Every tweet was processed in about one second. Thus, our gains were not impacted by the latency discount.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LIVEQA</head><p>The TREC LiveQA track <ref type="foot" coords="3,429.11,302.61,3.65,5.48" target="#foot_1">5</ref> is a new task that loosely follows earlier TREC QA tracks, but with several substantial design changes. In this new track, the questions come in real time from real users, as posted on Yahoo! Answers. <ref type="foot" coords="3,528.08,333.99,3.65,5.48" target="#foot_2">6</ref> This results in more natural and more diverse topics then was the case with earlier QA tracks in which the questions are developed by assessors or selected from query logs. LiveQA also incorporates an efficiency challenge, as the answers have to be provided in near-real time (specifically, in no more than one minute). A third challenge is that no document collection is provided, so participants must assemble any online or offline resources on which their systems will base their answers. These changes make the task more realistic and challenging.</p><p>Only one answer per question is judged for each participating system, using a relevance scale from 1 to 4.</p><p>This year, the LiveQA track focuses on eight categories, namely Arts &amp; Humanities, Beauty &amp; Style, Computers &amp; Internet, Health, Home &amp; Garden, Pets, Sports, and Travel. Figure <ref type="figure" coords="3,345.96,503.19,4.60,8.21" target="#fig_3">2</ref> shows the example of a question<ref type="foot" coords="3,484.36,501.37,3.65,5.48" target="#foot_3">7</ref> asked under the category Computers &amp; Internet. We submitted three runs: one using old questions and answers from Yahoo! Answers (Section 3.1), and two using tweets (Section 3.2).</p><p>Figure <ref type="figure" coords="3,356.13,545.03,4.60,8.21" target="#fig_4">3</ref> shows the general architecture of our LiveQA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Answering with Old Yahoo! Answers</head><p>A study by Shtok et al. <ref type="bibr" coords="3,431.38,588.84,9.72,8.21">[5]</ref> has shown that about 25% of the titles of the questions posted on Yahoo! Answers in a 3-month period had occurred in a similar form (i.e., with a cosine similarity above 0.9) in a prior 11-month period. This suggests that it may often be possible to find similar questions that have previously been asked. Assuming that similar questions will have similar answers (which is not necessarily true, for instance, for generic or experience-based questions), then a natural baseline system is one that simply does a search in old questions and answers for each incoming question. This section describes our process for crawling old questions and answers from Yahoo! Answers, and how we used the resulting collection to answer new questions (Figure <ref type="figure" coords="4,69.67,588.45,3.58,8.21" target="#fig_3">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Crawling Old Questions and Answers</head><p>To build a system that answers new incoming questions with the knowledge already existing in Yahoo! Answers, we decided to crawl all of the questions and answers that have ever been published on this platform, and that are still accessible (i.e., they did not get deleted). We do so in four steps:</p><p>1. Crawl all of the categories pages of the the main website, in addition to its 22 localized versions<ref type="foot" coords="4,511.78,544.79,3.65,5.48" target="#foot_4">8</ref> to gather a fairly large set of question identifiers, and add them to a set Q. Each webpage shows up to 1,000 of the most recent questions.</p><p>2. Let Q * be the subset of questions in Q that have not been crawled yet. Crawl the webpages of questions in Q * to obtain the questions, their answers, and the user identifiers of those asking and answering the questions. Add the user identifiers to a set U .</p><p>3. Let U * be the subset of users in U that have not been crawled yet. Crawl the webpages of users in U * to acquire the identifiers of the questions they asked or responded to, which we add to the set Q, and to ac-quire the user identifiers for their friends and followers, which we add to the set U .</p><p>4. If either of Q * and U * is not empty, then go back to</p><p>Step 2.</p><p>While this process has allowed us to gather a large set of 226M questions, 1.3B answers and 43M users, it is not guaranteed that we have obtained all of the data available in the website. This is the case for at least two reasons. First, the privacy settings of some users may be configured to hide the identifiers of the questions they asked or answered or the identifiers of their friends and followers. Second, some groups of users (especially the least active ones) might form an isolated clique that is not accessible by following links (based on questions or users) from the seed questions.</p><p>Once crawling is complete, we index the questions and the best answers found on the main Yahoo! Answers website, after stemming with the Porter stemmer. Crawling the other (localized) systems allows us to identify additional users, who may have also posted questions or answers to the main website, but we did not index the questions or answers from those localized websites because we do not expect many useful matches to be found there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Selecting Answers</head><p>With a large corpus of prior questions and answers, we have several fields we can use for retrieval. Here we consider only the following six possibilities. For the incoming question, we use the title, but we also optionally concatenate it with the description. For the old questions, we consider searching in the title only, in the concatenation of the title and the description, or in only the best answer. When we search in old questions, we return its corresponding best answer. When we search in old best answers, we just return the best answer that we find. Because we did not have any ground truth for selecting among these alternatives in the first year of the track, we instantiated a small crowdsourcing task on CrowdFlower, 9 in which we showed the annotators questions from the final dry run, with up to six answers from the six retrieval configurations (when two or more methods returned the same answer, we would show fewer than six options). We allowed them to check-mark any answer they thought does indeed answer the question. Using annotations for 61 questions assessed by at least three trusted annotators for which at least one of them checked at least one of the answers, we trained a classifier to predict which configuration would be best for an incoming question. As features, we used the number of words and characters in the title and description fields in their stemmed and unstemmed versions, the category of the question, and the Jaccard similarity between the stemmed title and the stemmed description. We trained the cost-sensitive multiclass classifier of VowpalWabbit 10 using these features. Formally, for a training document assessed by N annotators, let vi,n be the binary value implicitly indicated by annotator n for one of the I = 6 retrieval configurations i. The cost ci associated with predicting the configuration i is:</p><formula xml:id="formula_4" coords="5,108.81,667.39,129.08,29.08">ci = 1 - N n=1 vi,n I i ′ =1 N n=1 v i ′ ,n .</formula><p>9 http://crowdflower.com 10 https://github.com/JohnLangford/vowpal_wabbit</p><p>In other words, we assign a high cost for errors on questions for which all of the annotators agreed on the same answer, and a low cost for questions that have multiple good answers marked or high disagreement amongst the annotators. When a new question is received, we apply the trained model to choose which one of the six configurations to use to answer that question.</p><p>The guidelines limit answers to 1,000 characters. If the answer exceeds 1,000 characters, we split it into sentences based on periods and retain the first and last sentence, and as many of the sentences with the highest Jaccard similarity to the title of the question as possible until the 1,000character limit would be exceeded by adding an additional sentence. This summarizes our submitted system CLIP-YA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answering with Tweets</head><p>Twitter, like several other popular social media websites, constitutes an enormous resource for information and opinions that are continuously produced by people around the globe. This makes it a potential place to find answers to questions asked on Yahoo! Answers. In this section we describe our approach to collecting tweets, and to using them as a basis for question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">A Large Corpus of Random Tweets</head><p>We consider Twitter as a resource for finding answers. As we do not have access to all of the tweets that have ever been posted, we need to find ways for increasing our chances of gathering tweets that will be useful to the expected questions. For this, we use two groups of collections. The first is a collection of tweets maintained by the Internet Archive Team since 2011 (Section 2.1.1). It consists of most of the JSON objects grabbed from a sample of about 1% of Twitter's public tweets. Because of some technical difficulties, tweets sent on some days are missing from this collection. Hence, we only have tweets from 1,215 days from the period September 27, 2011 to June 30, 2015. We added to this a collection of tweets that we streamed between July 27, 2015 and August 27, 2015. We denote this collection by PublicStream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">A Small Corpus of Selected Tweets</head><p>The Twitter API allows us to track<ref type="foot" coords="5,471.59,507.50,7.31,5.48" target="#foot_5">11</ref> a set of up to 400 keywords.<ref type="foot" coords="5,356.28,517.96,7.31,5.48" target="#foot_6">12</ref> When doing so, the API returns the tweets that contain at least one of these keywords, subject to the 1% limit computed over all tweets. For each of the eight categories, we think that selecting some keywords that represent its core vocabulary and then tracking Twitter content containing at least one of those keywords might give us a set of tweets that is richer (in terms of relevance to the potential questions) than the ones we would get by relying solely on the public stream.</p><p>We construct these eight core vocabularies following Fung et al. <ref type="bibr" coords="5,340.34,624.38,9.21,8.21" target="#b1">[1]</ref>. Formally, let a document in Yahoo! Answers be a question, its description or one of its answers. we denote by DF (wG) the document frequency of a word w in the set of documents G. We then scale DF (wG) to a value between 0 and 1 as: </p><formula xml:id="formula_5" coords="6,89.38,385.79,164.18,36.77">df (wG) = DF (wG) -min w ′ ∈G DF (w ′ G ) max w ′ ∈G DF (w ′ G ) -min w ′ ∈G DF (w ′ G )</formula><p>.</p><p>Finally, for a given category i, we denote by Hi the value</p><formula xml:id="formula_6" coords="6,119.24,442.45,108.22,14.78">Hi(w) = df (wC ) -df (w C ),</formula><p>where C is the set of documents that belong to the category i, and C is the set of documents that do not belong to that category. This value gives higher credit to words that are more frequent within the category i than within all the other categories. In other words Hi defines a ranking of words by their relevance to the questions and answers of the category i.</p><p>We observe that using the top 400 keywords for each category causes the Twitter API to send warnings for hitting the 1% maximum. Thus, we heuristically set the number of keywords, for every category, so that their filter matches about 1% of all the posted tweets. The final set of keywords for these eight categories is described in Table <ref type="table" coords="6,237.26,585.22,3.58,8.21" target="#tab_1">3</ref>. We tracked these eight sets of keywords using eight Twitter accounts for three weeks (from August 7 to 27, 2015.) We denote each of the eight corresponding collections by TrackedWordsi, while i corresponds to the name of the category.</p><p>Finally, when we receive a question of category i, we search in the union of PublicStream and TrackedWordsi, which we henceforth refer to as the search space Corpusi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Preprocessing</head><p>We normalize all the tweets by removing emoticons, user mentions, URLs, RT indicators, and punctuation before stemming them with the Porter stemmer.</p><p>All of the questions have a title and most of them have a description as well. As both of these fields can be long, running the query as-is risks generating a high I/O load, and thus exceeding the limit of one minute per question. To mitigate this limitation we heuristically select the words of the query following these steps after stemming both the title and the description with Porter stemmer:</p><p>1. If the stemmed title has more than seven terms, we remove from them a list of 74 terms that we had manually selected from the most 100 frequent stemmed terms in the corpus of old questions and answers in Yahoo! Answers (Section 3.1).</p><p>2. We issue the preprocessed title as a query to an Indri<ref type="foot" coords="6,548.11,200.70,7.31,5.48" target="#foot_7">13</ref> index corresponding to the category of the question, using the Okapi BM25 retrieval model.</p><p>3. We use the retrieved documents as a backup if the next stage does not complete within the allowed time limit.</p><p>4. We concatenate the processed title to the description field (processed in a similar manner), and issue the combined query to Indri.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Finding Answers</head><p>By matching dry-run questions against the tweets in our corpora, we observed that several of the retrieved tweets are themselves actually questions. Such questions should not be returned as answers to the original question. This led us to make the distinction between two conceptual types of tweets: those that contain a question, and others (because those that do not contain a question might contain an answer). To implement this distinction, we extract two subsets from Corpusi for each category i:</p><p>• Corpusi,q is the subset of tweets that contain a question mark and are also detected to be questions seeking an answer (using a classifier that has an accuracy of 79% trained on normalized and stemmed 1-3grams from tweets released by Zhao and Mei <ref type="bibr" coords="6,522.55,473.55,9.51,8.21" target="#b6">[6]</ref>). For Corpusi,q, we start with the top 20 retrieved tweets.</p><p>We then scrape their Twitter web pages to get all of their replies. We return the reply that is the most similar to the title of the incoming question, using the Jaccard similarity coefficient, and after processing them as described in Section 3.2.3. This is our submitted run CLIP-TW-Q.</p><p>• Corpusi,a is the subset of tweets that do not contain a question mark and that are also not detected (by that same classifier) to be questions seeking an answer. For Corpusi,a, we simply return the tweet with the highest BM25 relevance score. This is our submitted run CLIP-TW-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>According to the guidelines, each system-submitted answer was given a score of -2 by the NIST annotator if the answer is unreadable. Otherwise, the annotator assigned it a score between 1 (bad) and 4 (excellent). The following performance measures are reported: System score (0-3) succ@1+ succ@2+ succ@3+ succ@4+ prec@2+ prec@3+ prec@4+ • score (0-3) is the average score over all questions after transferring 1-4 level scores to 0-3, and giving unreadable answers a score of 0.</p><p>• succ@i+ is the number of questions with a score of at least i, divided by the total number of answered and unanswered questions.</p><p>• prec@i+ is the number of questions with a score of at least i, divided by the number of answered questions.</p><p>Table <ref type="table" coords="7,88.75,265.24,4.60,8.21" target="#tab_2">4</ref> shows the results for each of our three systems. For reference, the "all runs" row shows the mean score over all systems that participated in this task. Clearly, CLIP-YA performs, on average, better than both of the Twitter based systems CLIP-TW-Q and CLIP-TW-A. Comparing the two Twitter-based systems, it appears that the approach that returns a tweet that is not detected to be a question (i.e., CLIP-TW-A), is performing better by most measures than the approach where the returned tweet is a reply to a tweet that is detected to be a question similar to the incoming question (i.e., CLIP-TW-Q).</p><p>In the remainder of this section, we look at the results from different perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Scores per Category</head><p>Table <ref type="table" coords="7,89.95,422.10,4.60,8.21" target="#tab_4">5</ref> shows the scores of our systems for each category. Health accounts for about 1/3 of all questions. This is also the category for which CLIP-YA performs the best. Travel is the category for which CLIP-YA performs the worst. But it contains less than 6% of the questions. Interestingly, this is the only category where the scores of CLIP-YA and CLIP-TW-A are somewhat comparable.</p><p>Due to a corruption in the index of the Computers &amp; Internet category, CLIP-TW-A did not answer any of the questions in that category. for retrieving answers (Section 3.1.2). For CLIP-YA, we delegate this decision to a classifier. This classifier chose to use the body of the question in only 11 out of 1,079 questions (Table <ref type="table" coords="7,346.07,312.73,3.58,8.21" target="#tab_5">6</ref>). The average score over these questions (0.82) is higher than the average score over the questions where only the terms of the title were used (0.62).</p><p>For both CLIP-TW-Q and CLIP-TW-A, the answer retrieval for a substantial number of questions using the body timed out (461 or 395, respectively). In some additional cases (26 or 98, respectively), the retrieval using only the title of the question took over half of the allowed response period. These two systems do not even attempt to use the body of the question when this happens. As we have observed for CLIP-YA, the questions for which the body was used got an average score higher than those for which only the title was used, although for CLIP-TW-Q the difference is quite small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Retrieval Field for Old Yahoo! Answers</head><p>The classifier used by the CLIP-YA system chooses between three configurations for the fields to be searched in the old answers. As shown in Table <ref type="table" coords="7,469.14,501.28,3.58,8.21" target="#tab_6">7</ref>, in most cases (784 of 1,079), the decision was to match the incoming question against the content of the old answers. Questions for which this configuration was selected had an average score higher than those for which the classifier chose to search in the content of the old questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Best CLIP-YA Configuration</head><p>Combining these insights, we might speculate that the best configuration of our CLIP-YA system would be one that uses the title and the body of the incoming question as a query (Section 3.3.2), and the index of old answers for retrieval (Section 3.3.3). As it happens, only three questions were run using both of those conditions together; their average score is 1.67. Although based on too little data for us to draw any firm conclusion, that average is certainly high enough to get our attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Effect of Twitter Retrieval Corpus</head><p>Our retrieval of answers from Twitter uses the union of two disjoint corpora, a large corpus of random tweets and a smaller focused corpus of selected tweets. For every question, we can thus look at the origin of the returned tweet (the small or the large corpus). As Table <ref type="table" coords="8,232.87,288.49,4.60,8.21" target="#tab_7">8</ref> shows, when an answer is found in the smaller focused corpus, the average score is higher. This suggests that a larger (i.e., longer) focused crawl of tweets that are expected to match the expected question categories might be worthwhile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We have presented the general architecture and the implementation details for the six runs we submitted for the Microblog Real-Time Filtering task, and the three systems we built for the LiveQA task. The results suggest that a per-topic rescoring threshold and a high clustering similarity threshold can each improve the performance of our Microblog systems. We are satisfied with the recall of our relevance model, but we want to focus more on precision (i.e., returning tweets only when there is high enough confidence in their relevance).</p><p>The results of the LiveQA track show that a system returning answers based on old questions and answers from Yahoo! Answers performs better on average than two systems that return tweets as answers, but that a tweet-based system may be useful for answering some difficult questions (e.g., those in the Travel category). In the future, we plan to tune our systems to use the body of the new questions and the content of old answers more frequently. We also plan to track the words related to the categories of interest over a longer period in order to gather a richer collection of potentially useful tweets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,175.49,295.50,258.74,8.21;2,53.80,53.63,504.21,230.55"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General architecture of the real-time filtering systems.</figDesc><graphic coords="2,53.80,53.63,504.21,230.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,100.28,78.82,28.96,8.21;3,181.49,78.82,19.25,8.21;3,217.80,78.82,19.01,8.21;3,100.28,93.79,143.07,8.21;3,100.28,104.25,61.61,8.21;3,181.48,104.25,61.87,8.21;3,100.28,114.71,61.61,8.21;3,181.48,114.71,61.87,8.21;3,63.05,140.66,220.60,8.21;3,130.22,164.33,28.95,8.21;3,187.38,164.33,26.03,8.21;3,130.22,179.30,82.72,8.21;3,130.22,189.76,82.72,8.21;3,130.22,200.22,82.72,8.21;3,76.21,235.78,216.69,8.21;3,76.21,246.25,216.69,8.21;3,76.21,256.71,24.07,8.21"><head></head><label></label><figDesc>set for all topics to be β = 5. The clustering similarity score was set as τ = 0.5 or τ = 0.6 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,324.65,61.58,136.22,4.72;3,327.24,73.95,191.13,9.44;3,327.24,86.93,41.23,9.44;3,327.24,102.79,212.88,5.12;3,327.24,109.71,211.27,5.12;3,327.24,116.63,164.20,5.12;3,327.24,123.54,117.74,5.12;3,327.24,130.46,217.74,5.12;3,327.24,137.38,211.70,5.12;3,327.24,144.30,132.93,5.12;3,327.24,158.14,109.53,5.12;3,327.24,165.06,63.81,5.12;3,327.24,171.98,168.38,5.12;3,327.24,185.81,209.83,5.12;3,327.24,192.73,26.95,5.12;3,327.24,199.65,32.95,5.12;3,327.24,213.49,221.08,5.12;3,327.24,220.41,23.14,5.12"><head></head><label></label><figDesc>Computers &amp; Internet Hardware Laptops &amp; Notebooks Which Computer (Laptop) is the best for gaming? I am getting a new computer and wondering which computer I should get. I found 3 laptops I liked but don't know which one to buy. Please someone give me advice (consider price as well) by the way this is all Australian Dollars. 1. Toshiba L50-A013 15 inch Notebook $ 880 15.6" Intel Core i7-3630M 3.4Ghz CPU, NVIDIA GeForce GT740M 2GB with Optimus + Intel HD4000 Graphics, Windows 8, 4GB RAM, 750GB HDD, 4 cell battery, USB 3.0, HD Webcam, Wi-f-+Bluetooth 4.0, HDM 1, DVD 2. HP Envy TS D-J02ITX Notebook $ 1040 intel 17-4700MQ 2.4GHz NVIDIA 2GB Dedicated graphics, Bluetooth, 8GB RAM, 1TB HDD 3. Alienware (yes I know this is the best but it is pretty expensive if. Would it worth buying it?) Price: $ 1385 3rd Gen i7, 8GB RAM, 1TB HDD, 32GB Solstice drive, Video GT 650, 3GB Dedicated Graphics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,316.81,248.02,239.11,8.21;3,316.81,258.48,45.04,8.21"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Question asked under the category of Computers &amp; Internet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,195.00,505.12,219.71,8.21;4,53.80,53.52,504.56,440.27"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: General architecture of the LiveQA systems.</figDesc><graphic coords="4,53.80,53.52,504.56,440.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,62.86,55.15,220.99,8.21"><head>Table 1 :</head><label>1</label><figDesc>Results for Scenario A of the Microblog Task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,60.65,55.15,225.39,294.64"><head>Table 3 :</head><label>3</label><figDesc>Words in the core vocabulary of each category.</figDesc><table coords="6,61.94,75.99,219.90,273.80"><row><cell>Arts &amp;</cell><cell>Computers</cell><cell>Beauty</cell><cell>Home</cell></row><row><cell>Humanities</cell><cell>&amp; Internet</cell><cell>&amp; Style</cell><cell>&amp; Garden</cell></row><row><cell>book</cell><cell>computer</cell><cell>hair</cell><cell>plant</cell></row><row><cell>books</cell><cell>windows</cell><cell>wear</cell><cell>paint</cell></row><row><cell>poem</cell><cell>laptop</cell><cell>color</cell><cell>wood</cell></row><row><cell>novel</cell><cell>download</cell><cell>skin</cell><cell>walls</cell></row><row><cell>writing</cell><cell>pc</cell><cell>makeup</cell><cell>plants</cell></row><row><cell>author</cell><cell>software</cell><cell>dress</cell><cell>garden</cell></row><row><cell>story</cell><cell>install</cell><cell>style</cell><cell>wall</cell></row><row><cell>twilight</cell><cell>click</cell><cell></cell><cell>furniture</cell></row><row><cell>characters</cell><cell>files</cell><cell></cell><cell>depot</cell></row><row><cell>authors</cell><cell>program</cell><cell></cell><cell>soil</cell></row><row><cell>(13 more)</cell><cell>(11 more)</cell><cell></cell><cell>(65 more)</cell></row><row><cell>Health</cell><cell>Sports</cell><cell>Pets</cell><cell>Travel</cell></row><row><cell>doctor</cell><cell>team</cell><cell>dog</cell><cell>travel</cell></row><row><cell>weight</cell><cell>football</cell><cell>dogs</cell><cell>city</cell></row><row><cell>diet</cell><cell>players</cell><cell>vet</cell><cell>trip</cell></row><row><cell>exercise</cell><cell>win</cell><cell>pet</cell><cell>hotel</cell></row><row><cell>body</cell><cell>teams</cell><cell>cat</cell><cell>airport</cell></row><row><cell>fat</cell><cell>fan</cell><cell>cats</cell><cell>flight</cell></row><row><cell>muscle</cell><cell>player</cell><cell>breed</cell><cell>hotels</cell></row><row><cell>pain</cell><cell>wwe</cell><cell>puppy</cell><cell>cities</cell></row><row><cell>eating</cell><cell></cell><cell>pets</cell><cell>tourist</cell></row><row><cell>calories</cell><cell></cell><cell>animals</cell><cell>places</cell></row><row><cell>(5 more)</cell><cell></cell><cell>(13 more)</cell><cell>(14 more)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,170.23,55.15,269.26,8.21"><head>Table 4 :</head><label>4</label><figDesc>Performance of participating systems in the LiveQA task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,53.80,536.17,239.10,183.38"><head>Table 5 :</head><label>5</label><figDesc>Number of questions answered by each system for every category, with the corresponding score. Using the Body of the Question Our systems have different strategies to decide whether to use the terms that appear in the body of the question</figDesc><table coords="7,56.54,569.91,233.34,126.95"><row><cell></cell><cell cols="2">CLIP-YA</cell><cell cols="4">CLIP-TW-Q CLIP-TW-A</cell></row><row><cell></cell><cell>Score</cell><cell cols="2"># Score</cell><cell cols="2"># Score</cell><cell>#</cell></row><row><cell>Arts &amp; Huma.</cell><cell cols="2">0.64 118</cell><cell>0.06</cell><cell>115</cell><cell>0.16</cell><cell>111</cell></row><row><cell>Health</cell><cell cols="2">0.77 327</cell><cell>0.08</cell><cell>325</cell><cell>0.20</cell><cell>293</cell></row><row><cell>Beauty &amp; Style</cell><cell cols="2">0.50 119</cell><cell>0.13</cell><cell>116</cell><cell>0.21</cell><cell>107</cell></row><row><cell>Sports</cell><cell cols="2">0.51 122</cell><cell>0.13</cell><cell>120</cell><cell>0.23</cell><cell>115</cell></row><row><cell>Home &amp; Garden</cell><cell>0.60</cell><cell>47</cell><cell>0.02</cell><cell>47</cell><cell>0.09</cell><cell>44</cell></row><row><cell>Pets</cell><cell>0.65</cell><cell>94</cell><cell>0.02</cell><cell>94</cell><cell>0.17</cell><cell>82</cell></row><row><cell>Travel</cell><cell>0.29</cell><cell>58</cell><cell>0.15</cell><cell>57</cell><cell>0.25</cell><cell>53</cell></row><row><cell>Comp. &amp; Inter.</cell><cell cols="2">0.60 194</cell><cell>0.15</cell><cell>192</cell><cell>-</cell><cell>0</cell></row><row><cell>3.3.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,316.81,155.84,239.10,99.35"><head>Table 6 :</head><label>6</label><figDesc>Number of questions answered by each system with(out) using the body, with the corresponding score.</figDesc><table coords="7,322.40,189.58,227.92,65.61"><row><cell></cell><cell cols="2">CLIP-YA</cell><cell cols="4">CLIP-TW-Q CLIP-TW-A</cell></row><row><cell></cell><cell>Score</cell><cell cols="2"># Score</cell><cell cols="2"># Score</cell><cell>#</cell></row><row><cell>Body used</cell><cell>0.82</cell><cell>11</cell><cell>0.10</cell><cell>199</cell><cell>0.37</cell><cell>65</cell></row><row><cell>Body not used</cell><cell cols="2">0.62 1068</cell><cell>0.09</cell><cell>867</cell><cell>0.18</cell><cell>740</cell></row><row><cell>-body empty</cell><cell>0.50</cell><cell>387</cell><cell>0.09</cell><cell>380</cell><cell>0.20</cell><cell>267</cell></row><row><cell>-timeout</cell><cell>-</cell><cell>0</cell><cell>0 . 0 9</cell><cell>4 6 1</cell><cell>0 . 1 5</cell><cell>3 9 5</cell></row><row><cell>-risk timeout</cell><cell>0.68</cell><cell>681</cell><cell>0.08</cell><cell>26</cell><cell>0.19</cell><cell>98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,316.81,573.87,239.11,88.68"><head>Table 7 :</head><label>7</label><figDesc>Number of questions answered by CLIP-YA depending on the retrieval field, with the corresponding score.</figDesc><table coords="7,352.71,607.99,164.24,54.56"><row><cell></cell><cell cols="2">CLIP-YA</cell></row><row><cell></cell><cell cols="2">Score Count</cell></row><row><cell>Question title</cell><cell>0.43</cell><cell>109</cell></row><row><cell>Question title and body</cell><cell>0.54</cell><cell>186</cell></row><row><cell>Answer</cell><cell>0.67</cell><cell>784</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,53.80,55.15,239.10,72.45"><head>Table 8 :</head><label>8</label><figDesc>Number of questions answered by each system for each corpus, with the corresponding score.</figDesc><table coords="8,65.48,88.89,212.98,38.71"><row><cell></cell><cell cols="2">CLIP-TW-Q</cell><cell cols="2">CLIP-TW-A</cell></row><row><cell></cell><cell cols="4">Score Count Score Count</cell></row><row><cell>Selected tweets (small)</cell><cell>0.11</cell><cell>158</cell><cell>0.46</cell><cell>24</cell></row><row><cell>Random tweets (large)</cell><cell>0.09</cell><cell>908</cell><cell>0.19</cell><cell>781</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="1,321.42,701.42,112.76,7.65"><p>http://apache.lucene.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,321.42,682.12,206.73,7.65"><p>https://sites.google.com/site/trecliveqa2015</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="3,321.42,692.45,117.46,7.65"><p>https://answers.yahoo.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="3,321.42,702.78,211.43,7.65;3,316.81,711.75,98.68,7.65"><p>https://answers.yahoo.com/question/index?qid= 20130827015155AAwtYLQ</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="4,321.42,711.34,150.99,8.21"><p>e.g., https://es.answers.yahoo.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5" coords="5,321.42,683.49,202.03,7.65;5,316.81,692.45,113.27,7.65"><p>https://dev.twitter.com/streaming/overview/ request-parameters#track</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6" coords="5,321.42,702.78,230.22,7.65;5,316.81,711.75,70.49,7.65"><p>https://dev.twitter.com/streaming/reference/post/ statuses/filter</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7" coords="6,321.42,711.75,173.84,7.65"><p>http://www.lemurproject.org/indri.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This work was made possible by NPRP grant# <rs type="grantNumber">NPRP 6-1377-1-257</rs> from the <rs type="funder">Qatar National Research Fund</rs> (a member of <rs type="funder">Qatar Foundation</rs>). The statements made herein are solely the responsibility of the authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6AyhFrF">
					<idno type="grant-number">NPRP 6-1377-1-257</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,321.29,251.82,96.81,14.27" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.00,269.20,205.76,8.21;8,331.01,279.66,216.80,8.21;8,331.01,290.12,204.73,8.21;8,331.01,300.58,47.54,8.21" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,518.32,269.20,18.44,8.21;8,331.01,279.66,188.03,8.21">Text classification without negative examples revisit</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">P C</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,525.54,279.66,22.27,8.21;8,331.01,290.12,200.54,8.21">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.00,312.04,212.75,8.21;8,331.01,322.50,125.92,8.21" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,385.64,312.04,142.53,8.21">Training linear SVMs in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,331.01,322.50,35.33,8.21">KDD &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.00,333.96,215.87,8.21;8,331.01,344.42,209.92,8.21;8,331.01,354.88,203.40,8.21" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,522.95,333.96,23.92,8.21;8,331.01,344.42,209.92,8.21;8,331.01,354.88,132.33,8.21">QCRI at TREC 2014: Applying the KISS principle for the TTG task in the Microblog track</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elganainy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,481.74,354.88,22.93,8.21">TREC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.00,366.33,192.13,8.21;8,331.01,376.79,213.62,8.21;8,331.01,387.26,141.98,8.21" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,331.01,376.79,213.62,8.21;8,331.01,387.26,20.07,8.21">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,369.65,387.26,74.30,8.21">Workshop at ICLR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.00,398.71,191.50,8.21;8,331.01,409.17,221.82,8.21;8,331.01,419.63,185.20,8.21" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,331.01,409.17,221.82,8.21;8,331.01,419.63,49.41,8.21">Learning from the past: Answering new questions with past answers</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,399.18,419.63,22.60,8.21">WWW</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,331.00,431.09,210.33,8.21;8,331.01,441.55,217.52,8.21;8,331.01,452.01,126.21,8.21" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,416.64,431.09,124.70,8.21;8,331.01,441.55,201.87,8.21">Questions about questions: An empirical analysis of information needs on Twitter</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,331.01,452.01,22.59,8.21">WWW</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
