<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.42,86.55,329.17,18.66">BJUT at TREC 2015 Temporal Summarization Track</title>
				<funder>
					<orgName type="full">Beijing Excellent Talent Development Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Guangxi Colleges and Universities Key Laboratory of</orgName>
				</funder>
				<funder>
					<orgName type="full">Basic Research Programs of Science and Technology of BJUT</orgName>
				</funder>
				<funder ref="#_bbxKbm6">
					<orgName type="full">Importation and Development of High Caliber Talents Project of Beijing Municipal Institutions</orgName>
				</funder>
				<funder ref="#_Xp6296M">
					<orgName type="full">BJUT Science and Technology Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.76,109.78,63.96,15.55"><forename type="first">Yingzhe</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Key Laboratory of Trusted Computing</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for CTISCP</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,269.60,109.78,55.78,15.55"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
							<email>⇤yangzhen@bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Key Laboratory of Trusted Computing</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for CTISCP</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Guangxi Colleges and Universities Key Laboratory of cloud computing and complex systems</orgName>
								<orgName type="institution">Guilin University of Electronic Technology</orgName>
								<address>
									<postCode>541004</postCode>
									<settlement>Guilin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.08,109.78,58.84,15.55"><forename type="first">Kefeng</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<postCode>100124</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Guangxi Colleges and Universities Key Laboratory of cloud computing and complex systems</orgName>
								<orgName type="institution">Guilin University of Electronic Technology</orgName>
								<address>
									<postCode>541004</postCode>
									<settlement>Guilin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Electronics Standardization Institue</orgName>
								<address>
									<postCode>100007</postCode>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.42,86.55,329.17,18.66">BJUT at TREC 2015 Temporal Summarization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8FD6BF6050AEEB1DAE179CABA4438043</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our efforts for TREC Temporal Summarization Track 2015. Since this is the third time we participate in this track,we adopt a different novel method NMFR to solve the newly emerging temporal summarization problem. Our goal of this year is to evaluate the effectiveness of : (1) Considering the semantic structure of document and the manifold structure of document could be as possible to preserve the essential characteristic of the original high-dimensional space of documents during the process of feature reduction.( <ref type="formula" coords="1,96.11,335.46,3.70,8.59">2</ref>)Using the non-negative matrix factorization with our semantic and manifold regularization for summarization is effective and comparable to Affinity Propagation. Finally, we conduct experiments to verify the proposed framework NMFR on TREC Temporal Summarization Track Corpus, and, as would be expected, the results demonstrate its generality and superior performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The TREC Temporal Summarization Track runs for the third time in this year, and its goal is to develop systems which can detect useful, new, and timely sentence-length updates about a developing event. According to the three different corpus, there are three tasks:</p><p>• Task 1: Filtering and Summarization</p><p>Participants will be provided high-volume streams of news articles and blog posts crawled from the Web (TREC-TS-2015 a.k.a. KBA Stream Corpus 2014).Each participant will need to process those streams in time order, filter out irrelevant content and then select sentences from those documents to return to the user as updates describing each event over time. • Task 2:Pre-Filtered Summarization</p><p>Participants will be provided pre-filtered high-volume streams of news articles and blog posts crawled from the Web for a set of events (TREC-TS-2015F).Each participant will need to process those streams in time order, filter out irrelevant content and then select sentences from those documents to return to the user as updates describing each event over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Task 3:Summarization Only</head><p>Participants will be provided low-volume streams of on-topic documents for a set of events (TREC-TS-2015F-RelOnly).Each participant will need to process those streams in time order selecting sentences from the documents contained within each stream to return the user as updates over time.</p><p>In this year's track, we participate in the Pre-Filtered Summarization task using our proposed framework, first pre-processing and filtering TREC-TS-2015F, then summarization, at last post-processing. In order to verify the ability to summarization without filtering, we also have done the Summarization Only task with TREC-TS-2015F-RelOnly that contains on-topic documents. The corpus consists of a set of time stamped documents from a variety of news and social media sources, each with a unique identifier.</p><p>Our method (NMFR) is a novel document partitioning method based on the non-negative factorization of the termdocument matrix of the given document corpus. We consider the pairwise sample similarity by a predefined similarity matrix K both from text semantic structure and sample neighboring relations as the regularization terms of NMF. The matrix K can be constructed either by using the label information in supervised learning or using certain distance metrics in unsupervised learning. Hence, K essentially encodes the class information or the intrinsic structure of data. Experiment results and TREC TS results show our method is effective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-process and index modules</head><p>The corpus downloaded locally is encrypted file, which cannot be used directly. In this sense, First step is to decrypt the files using the authorized key from authority, converting the GPG file format to SC file format. Then we use stream corpus toolbox to parse these SC files to TXT files. The stream corpus toolbox is given by TREC and provides a common data interchange format for document processing pipelines that apply language processing tools to large streams of text. The last step is to build index by lemur for the information retrieval module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Retrieval module</head><p>In this part, we use Lemur for information indexing and retrieval. Lemur is a toolkit designed to facilitate research in language modeling and information retrieval (IR). It supports the construction of basic text retrieval systems using language modeling methods such as BM25. Our experiment has two steps to build the index. First, create a parameter file to tell the lemur toolkit how to index; Secondly, use In-driBuildIndex.exe application to build index. Accordingly, the realization of retrieval also has two steps. First, create a parameter file to tell the lemur toolkit how to retrieve. Secondly, use IndriRunquery.exe application to retrieve.</p><p>In this way, for a given topic, we get a ranked sentence list by its relevance to its topic query words. In fact, we may use query expansion to increase the number of relevant sentences by using words with similar meaning to those in the query to solve the word mismatch problem, because people often describe the same concepts between the queries and documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information filtering and text vectorization</head><p>After IR module, we get a set of sentences related to a topic. Considering the effectiveness of these sentences, we first judge whether these sentences are effective: they should be not only in the range of each topic's begin time and start time ,but also should be not repeated sentences and contain not less than three words in that too short sentences is impossible to describes any information . If a sentence is not effective, abandon it, and at last the rest ones should be treated as candidate sentences. Considering the large amount of these sentences, in order to simplify the computation, we going on another filtering by retaining the candidate sentences whose relevance score are bigger than a given threshold value.</p><p>As is known to all, the Vector Space Model is to simplify the handling of the text content for vector operations of vector space, intuitive and easy to understand. When the document is represented as the document vector of the space, and then we can calculate vector similarity of space on it to express the semantic similarity. In the processing of text documents, Commonly used method to quantify term weight is TF -IDF. In this method, the value is proportionally to the number of times of which a word appears in the document, but is offset by the frequency of the word in the corpus, which else to control the fact that some words are generally more common than others. Of course, In order to further reduce the dimension and improve the accuracy of text representation, we must first going on stop words filtering and web fixed format filtering, such as web page link, finally calculate the effective term weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering and summarization module</head><p>After getting the vectors, the VSM similarity between two documents can be calculated by using the cosine distance:</p><formula xml:id="formula_0" coords="2,383.85,432.32,174.15,30.61">sim(d i , d j ) = d i • d j ||d i || • ||d j ||<label>(1)</label></formula><p>where d i and d j are two vectors representing two different documents and ||d|| is the length of the vector d. Another method to calculate similarity is based on mutual information preserving mapping <ref type="bibr" coords="2,467.39,497.74,13.36,9.54" target="#b0">[1]</ref>, which is a manifold learning algorithm that computes low-dimensional, neighboring-preserving based on mutual information of high-dimensional inputs. Multi-document summarization (MDS) approaches takes as input a set of documents about a topic to be summarized and produce a summary of these documents. One of the most widely used approaches to score sentences for inclusion into a summary is clustering with respect to the centroid of the sentences within the input documents <ref type="bibr" coords="2,470.72,596.63,13.07,9.54" target="#b1">[3]</ref>,thereby selecting those sentences most central to the topic first. In this experiment, we adopt an improved non-negative matrix factorization with similarity preserving feature regularization (NMFR) as clustering summarization technique. There are two prominent advantages: first, comparing with the LSA, its decomposition results have good interpretability owing to its non-negativity constraint; second, different from traditional clustering method needing feature dimension reduction ahead of time, NMF not only can realize clustering but also complete feature dimension reduction at the same time. Moreover, we can add various regularization constraints to restrain dimension reduction and factorization process, thus preserving most important original characteristics of high dimensional space.</p><p>Of course, to contrast, we also used another classical clustering algorithms: Affinity Propagation (AP). Compared with the existing clustering algorithms, such as K-center clustering, AP is an efficient and fast clustering algorithm for large datasets without specifying beforehand clustering number which clusters data, taking a set of real-valued pairwise data point similarities as input.</p><p>Low-rank matrix factorization method is widely employed in various applications such as document clustering [4,5] and collective filtering <ref type="bibr" coords="3,167.95,212.15,10.79,9.54" target="#b4">[6,</ref><ref type="bibr" coords="3,178.74,212.15,7.19,9.54">7]</ref>. Non-negative matrix factorization is a linear, non-negative approximate data representation. Let's assume that our data sets consists of N samples of m non-negative scalar variables. Denoting the (mdimensional) measurement vectors (t = 1, • • • , N), a linear approximation of the data sample is given by</p><formula xml:id="formula_1" coords="3,127.47,286.51,165.03,30.60">x t ⇡ k X i=1 w i h t i = W h t (2)</formula><p>Where W is an m ⇥ k matrix containing the basis vectors as its columns. Note that Note that each sample vector is written in terms of the same basis vectors. The k basis vectors can be thought of as the building blocks of the data, and the (k-dimensional) coefficient vector describes how strongly each building block is present in the sample vector .Arranging the sample vectors into the columns of a matrix X, we can now write</p><formula xml:id="formula_2" coords="3,152.15,415.21,140.35,17.04">X ⇡ W H<label>(3)</label></formula><p>where each column of H contains the coefficient vector corresponding to the sample vector. Written in this form, it becomes apparent that a linear data representation is simply a factorization of the data matrix. Given a data matrix X, the optimal choice of matrices W and H are defined to be those nonnegative matrices that minimize the reconstruction error between X and W H. Various error functions have been proposed (Paatero and Tapper, 1994; Lee and Seung, 2001), perhaps the most widely used is the squared error (Euclidean distance) function .</p><formula xml:id="formula_3" coords="3,60.35,545.06,232.15,29.80">E(W, H) = ||X W H|| 2 = X i,j (V i,j (W H) i,j ) 2 (4)</formula><p>where (X) i,j represents an element of a matrix X. There is neighboring relationship among text data points in term of distance, accordingly, there is also semantic approximation relationship from the semantic aspect. The former is a consideration by the manifold structure (the geometric distribution) of data points while the latter is text semantic relations distribution of data points. In the process of feature selection, we hope to make the low dimensional space as much as possible to retain the intrinsic character of original high dimension space of VSM. So we use text vector cosine similarity and improved mutual information semantic similarity calculation formula to compute the pairwise similarity matrix K, as is shown in formula <ref type="bibr" coords="3,440.46,68.18,10.58,9.54" target="#b2">(5)</ref>.</p><formula xml:id="formula_4" coords="3,387.00,84.13,171.00,11.56">K = X T X + (1 )Y (5)</formula><p>Where X is data matrix, is a tuning parameters ranging from zero and one, controlling the share of two items in the matrix K, the first item is represented as geometric similarity matrix between data points and Y is our calculated semantic similarity matrix between points based on word co-occurrence model, mainly word frequency and document frequency of word.</p><p>Considering the control of model complexity and the pairwise similarity matrix, adding two regularization, we proposed our method NMFR, based on matrix factorization while exploiting the pairwise similarity among data points. NMFR is to solve the following optimization problem,</p><formula xml:id="formula_5" coords="3,319.50,241.69,236.93,27.48">min W,H F = ||X W H|| 2 F +↵||W HH T W T K|| 2 F + ||W || 2 F (<label>6</label></formula><p>) By removing constants in the objective function, the above equation can be rewritten as,</p><formula xml:id="formula_6" coords="3,329.08,298.57,228.92,43.55">F = T r( 2X T W H + H T W T W H)+ T r(W HH T W T W HH T W T W HH T W T K K T W HH T W T ) + T r(W W T ) (7)</formula><p>The coupling between W and H makes the problem in Eq. ( <ref type="formula" coords="3,339.54,361.95,3.87,9.54">5</ref>) difficult to find optimal solutions for both W and H simultaneously. In this work we use an alternative optimization scheme <ref type="bibr" coords="3,363.55,383.87,13.20,9.54" target="#b2">[5]</ref>. Reference to the paper, it is easy to solve the objective function. To save space, we omit it here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-processing module</head><p>After topic clustering, we select each topic clustering center as the final summary sentence (if this step we use the MMR (Maximal Marginal Relevance method) may improve the accuracy of the results, but the calculation will be a big.)Finally, we sort the sentence according to the correlation and time factor, forming the final clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>There are two parts of the results in our temporal summarization works: Pre-Filtered Summarization result and Summarization Only result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Methods</head><p>According the TREC authority, there are three metrics: • Expected Gain. One way to evaluate an update system is to measure the expected gain for a system update. This is similar to traditional notions of precision in information retrieval evaluation. • Comprehensiveness. Similar to tradition notions of recall in information retrieval evaluation. • F measure. In order to summarize expected gain and comprehensiveness, we use a F measure based on both Expected Gain and Comprehensiveness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>There are two parts of the results in our temporal summarization works: Pre-Filtered Summarization results and Summarization Only results.</p><p>Table <ref type="table" coords="4,89.00,442.07,4.98,9.54" target="#tab_0">1</ref> shows the Pre-Filtered Summarization results of our system. In the first line, nE[Latency Gain] signifies the scores of the expected latency-adjusted gain, Latency Comp. signifies the scores of the latency-adjusted comprehensiveness, and HM(nE[LG], Lat.Comp.) signifies the scores of the harmonic mean of the two latency-adjusted measures.This last measure is the primary measure for the track. In the second row, DMSL1AP1 and DMSL1NMF2 (Omit prefix 'NMF' in table1 for brevity) is the runs we submitted, AVG is the mean score for each topic over all pooled runs submitted to the track. In the first column, the meaning of per-topic is obviously, Mean signifies the average values of the scores over the 21 topics are given for each run. In the second column, ALL signifies the mean score over all topics and all pooled runs submitted to the track.The same is with the Table <ref type="table" coords="4,125.04,606.45,3.74,9.54" target="#tab_1">2</ref>, showing the Results of Summarization Only task.</p><p>Comparing the results of Table <ref type="table" coords="4,188.83,629.50,4.98,9.54" target="#tab_0">1</ref> and Table <ref type="table" coords="4,237.08,629.50,3.74,9.54" target="#tab_1">2</ref>, we can find it that for the Results of Summarization Only task, in the term of the three metrics, our system is mostly better than the average performance of the all the participating systems, while for the Pre-Filtered Summarization task,there is better only in the metric of the latency-adjusted comprehensiveness. This indicates that our system is not good at filtering task, and facing too many unrelated documents of corpus is a nightmare for summarization, at the same time, it demonstrates our system is very suitable for summaring with ontopic corpus.</p><p>For each task,we both use our NMFR method and the baseline method -AP method. From two tables,we can see the results of them are similar. Now let's take Table <ref type="table" coords="4,537.80,457.12,4.98,9.54" target="#tab_1">2</ref> for example.Through Table <ref type="table" coords="4,419.48,468.08,3.74,9.54" target="#tab_1">2</ref>, the performance of DMSL2A1 and DMSL2N2 with respect to the metrics latency-adjusted Expected Gain, the latency-adjusted comprehensiveness and the harmonic mean F measure are mostly better than AVG, which means that our methods are effectively. However, there are several topics whose metric value is smaller than the AVG, which means that our methods are not so well in stability. Through the contrast of the last three lines, we come to the conclusion that our run's performance is better than the average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we presented the implementation details of our runs for Temporal Summarization Track, and our system is fit for summarization the on-topic corpus but not does well in summaring the corpus with lots of off-topic content. Through the experiment results, we find our NMFR method is effective and comparable to AP method. This indicates considering the semantic structure of document and the manifold structure of document could be as possible to preserve the essential characteristic of the original high- And our runs performed well respect to for Summarization Only task, but not so well respect to Pre-Filtered Summarization task. The reason may be our filtering function is not good. On the other hand,for some topics, our NMFR method is not better than AP method and the average performance. The possible reason is that we excessive emphasis on the rate of convergence and operating efficiency, and ignored the locally optimal solution of our method. Therefore, the future work emphasis should be on how to improve the filtering ability and stability of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,319.50,537.50,238.50,9.54;1,319.50,548.46,238.50,9.54;1,319.50,559.42,238.50,9.54;1,319.50,570.38,238.50,9.54;1,319.50,581.33,97.96,9.54"><head>Figure 1</head><label>1</label><figDesc>Figure 1 shows our system framework. It mainly consists of five parts: (1) Preprocess and index module, (2) Information Retrieval module, (3) Information filtering and text vectorization, (4) Clustering and Summarization module, and (5) Post-processing module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,99.54,392.18,147.42,9.54"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Framework of System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,73.34,64.01,465.31,297.70"><head>Table 1 :</head><label>1</label><figDesc>The Results of Pre-Filtered Summarization.</figDesc><table coords="4,73.34,78.20,465.31,283.52"><row><cell></cell><cell></cell><cell cols="2">nE[Latency Gain]</cell><cell>Latency Comp.</cell><cell></cell><cell>HM(nE[LG],Lat. Comp.)</cell></row><row><cell></cell><cell></cell><cell cols="2">L1AP1 L1NMF2 AVG</cell><cell cols="2">L1AP1 L1NMF2 AVG</cell><cell>L1AP1 L1NMF2 AVG</cell></row><row><cell></cell><cell>26</cell><cell>0.0174 0.0176</cell><cell cols="2">0.0444 0.2733 0.2658</cell><cell cols="2">0.2758 0.0328 0.0330</cell><cell>0.0667</cell></row><row><cell></cell><cell>27</cell><cell>0.0155 0.0172</cell><cell cols="2">0.0296 0.2696 0.2883</cell><cell cols="2">0.2540 0.0293 0.0324</cell><cell>0.0426</cell></row><row><cell></cell><cell>28</cell><cell>0.0074 0.0063</cell><cell cols="2">0.0246 0.2003 0.1626</cell><cell cols="2">0.2394 0.0142 0.0121</cell><cell>0.0390</cell></row><row><cell></cell><cell>29</cell><cell>0.0469 0.0511</cell><cell cols="2">0.0981 0.3283 0.3117</cell><cell cols="2">0.1717 0.0821 0.0877</cell><cell>0.0884</cell></row><row><cell></cell><cell>30</cell><cell>0.0282 0.0299</cell><cell cols="2">0.0545 0.2534 0.2551</cell><cell cols="2">0.1930 0.0507 0.0535</cell><cell>0.0694</cell></row><row><cell></cell><cell>31</cell><cell>0.0332 0.0539</cell><cell cols="2">0.0743 0.2386 0.3492</cell><cell cols="2">0.2419 0.0582 0.0933</cell><cell>0.1058</cell></row><row><cell></cell><cell>32</cell><cell>0.0168 0.0085</cell><cell cols="2">0.0531 0.0368 0.0189</cell><cell cols="2">0.0860 0.0231 0.0118</cell><cell>0.0594</cell></row><row><cell></cell><cell>33</cell><cell>0.0228 0.0232</cell><cell cols="2">0.0709 0.3317 0.3328</cell><cell cols="2">0.2209 0.0426 0.0434</cell><cell>0.0776</cell></row><row><cell></cell><cell>34</cell><cell>0.0195 0.0137</cell><cell cols="2">0.0470 0.4221 0.2798</cell><cell cols="2">0.2804 0.0372 0.0261</cell><cell>0.0688</cell></row><row><cell></cell><cell>35</cell><cell>0.0032 0.0040</cell><cell cols="2">0.0276 0.1412 0.1791</cell><cell cols="2">0.2470 0.0063 0.0078</cell><cell>0.0440</cell></row><row><cell>Topic</cell><cell>36</cell><cell>0.0130 0.0123</cell><cell cols="2">0.0173 0.4590 0.4195</cell><cell cols="2">0.2163 0.0252 0.0239</cell><cell>0.0304</cell></row><row><cell></cell><cell>37</cell><cell>0.0219 0.0241</cell><cell cols="2">0.0281 0.2589 0.2665</cell><cell cols="2">0.1637 0.0404 0.0443</cell><cell>0.0429</cell></row><row><cell></cell><cell>38</cell><cell>0.0113 0.0113</cell><cell cols="2">0.0725 0.1722 0.1566</cell><cell cols="2">0.2487 0.0211 0.0211</cell><cell>0.0722</cell></row><row><cell></cell><cell>39</cell><cell>0.0089 0.0090</cell><cell cols="2">0.0701 0.3662 0.3649</cell><cell cols="2">0.3842 0.0175 0.0176</cell><cell>0.0701</cell></row><row><cell></cell><cell>40</cell><cell>0.0091 0.0090</cell><cell cols="2">0.0166 0.4678 0.4137</cell><cell cols="2">0.2757 0.0178 0.0177</cell><cell>0.0296</cell></row><row><cell></cell><cell>41</cell><cell>0.0199 0.0235</cell><cell cols="2">0.0333 0.4683 0.4874</cell><cell cols="2">0.3128 0.0382 0.0449</cell><cell>0.0532</cell></row><row><cell></cell><cell>42</cell><cell>0.0136 0.0149</cell><cell cols="2">0.0290 0.2971 0.2912</cell><cell cols="2">0.3473 0.0260 0.0283</cell><cell>0.0470</cell></row><row><cell></cell><cell>43</cell><cell>0.0120 0.0189</cell><cell cols="2">0.0534 0.3309 0.4792</cell><cell cols="2">0.2825 0.0232 0.0364</cell><cell>0.0755</cell></row><row><cell></cell><cell>44</cell><cell>0.0162 0.0179</cell><cell cols="2">0.0802 0.3227 0.3297</cell><cell cols="2">0.2606 0.0309 0.0340</cell><cell>0.0896</cell></row><row><cell></cell><cell>45</cell><cell>0.0176 0.0126</cell><cell cols="2">0.0667 0.3014 0.1816</cell><cell cols="2">0.2365 0.0333 0.0236</cell><cell>0.0917</cell></row><row><cell></cell><cell>46</cell><cell>0.0073 0.0075</cell><cell cols="2">0.0234 0.1251 0.1251</cell><cell cols="2">0.0607 0.0138 0.0141</cell><cell>0.0214</cell></row><row><cell></cell><cell>ALL</cell><cell>0.0483</cell><cell></cell><cell>0.2381</cell><cell></cell><cell>0.0612</cell></row><row><cell>Mean</cell><cell>L1AP1</cell><cell>0.0172</cell><cell></cell><cell>0.2888</cell><cell></cell><cell>0.0316</cell></row><row><cell></cell><cell>L1NMF2</cell><cell>0.0184</cell><cell></cell><cell>0.2838</cell><cell></cell><cell>0.0337</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,54.00,64.01,460.02,349.23"><head>Table 2 :</head><label>2</label><figDesc>The Results of Summarization Only.</figDesc><table coords="5,54.00,80.22,460.02,333.03"><row><cell></cell><cell></cell><cell cols="3">nE[Latency Gain]</cell><cell cols="3">Latency Comp.</cell><cell cols="2">HM(nE[LG],Lat. Comp.)</cell></row><row><cell></cell><cell></cell><cell>L2A1</cell><cell>L2N2</cell><cell>AVG</cell><cell>L2A1</cell><cell>L2N2</cell><cell>AVG</cell><cell>L2A1</cell><cell>L2N2</cell><cell>AVG</cell></row><row><cell></cell><cell>26</cell><cell cols="8">0.0253 0.0204 0.0213 0.4753 0.3533 0.3810 0.0480 0.0386 0.0358</cell></row><row><cell></cell><cell>27</cell><cell cols="8">0.0224 0.0146 0.0274 0.3860 0.2434 0.3685 0.0424 0.0275 0.0459</cell></row><row><cell></cell><cell>28</cell><cell cols="8">0.0254 0.0151 0.0099 0.3561 0.1592 0.1834 0.0475 0.0276 0.0186</cell></row><row><cell></cell><cell>29</cell><cell cols="8">0.0296 0.0341 0.0205 0.2235 0.2517 0.1837 0.0523 0.0601 0.0359</cell></row><row><cell></cell><cell>30</cell><cell cols="8">0.0441 0.0354 0.0288 0.3417 0.2673 0.2620 0.0781 0.0626 0.0458</cell></row><row><cell></cell><cell>31</cell><cell cols="8">0.0845 0.0861 0.0326 0.5820 0.5706 0.3298 0.1475 0.1496 0.0514</cell></row><row><cell></cell><cell>32</cell><cell cols="8">0.0169 0.0183 0.0240 0.0395 0.0416 0.1170 0.0236 0.0254 0.0358</cell></row><row><cell></cell><cell>33</cell><cell cols="8">0.0285 0.0291 0.0141 0.3992 0.3752 0.2852 0.0531 0.0541 0.0250</cell></row><row><cell></cell><cell>34</cell><cell cols="8">0.0284 0.0278 0.0211 0.3678 0.3678 0.3753 0.0527 0.0517 0.0385</cell></row><row><cell></cell><cell>35</cell><cell cols="8">0.0153 0.0105 0.0134 0.5725 0.3878 0.3927 0.0297 0.0205 0.0251</cell></row><row><cell>Topic</cell><cell>36</cell><cell cols="8">0.0111 0.0096 0.0164 0.4839 0.4237 0.3737 0.0217 0.0189 0.0228</cell></row><row><cell></cell><cell>37</cell><cell cols="8">0.1075 0.1234 0.0444 0.5564 0.6152 0.2583 0.1801 0.2055 0.0735</cell></row><row><cell></cell><cell>38</cell><cell cols="8">0.0279 0.0403 0.0238 0.2083 0.2698 0.3386 0.0491 0.0701 0.0390</cell></row><row><cell></cell><cell>39</cell><cell cols="8">0.0184 0.0268 0.0176 0.3560 0.4771 0.3473 0.0350 0.0507 0.0327</cell></row><row><cell></cell><cell>40</cell><cell cols="8">0.0524 0.0588 0.0465 0.4139 0.3835 0.3579 0.0931 0.1020 0.0538</cell></row><row><cell></cell><cell>41</cell><cell cols="8">0.0253 0.0272 0.0107 0.3124 0.3124 0.2625 0.0467 0.0501 0.0202</cell></row><row><cell></cell><cell>42</cell><cell cols="8">0.0295 0.0224 0.0225 0.4476 0.3273 0.3192 0.0553 0.0420 0.0380</cell></row><row><cell></cell><cell>43</cell><cell cols="8">0.0821 0.0947 0.0359 0.4909 0.4909 0.3043 0.1406 0.1587 0.0494</cell></row><row><cell></cell><cell>44</cell><cell cols="8">0.0587 0.0740 0.0642 0.4919 0.4967 0.3170 0.1049 0.1288 0.0689</cell></row><row><cell></cell><cell>45</cell><cell cols="8">0.0250 0.0257 0.0260 0.3823 0.3516 0.3322 0.0470 0.0479 0.0424</cell></row><row><cell></cell><cell>46</cell><cell cols="8">0.0123 0.0132 0.0049 0.1465 0.1531 0.0899 0.0227 0.0242 0.0090</cell></row><row><cell></cell><cell>ALL</cell><cell></cell><cell>0.0251</cell><cell></cell><cell></cell><cell>0.2943</cell><cell></cell><cell></cell><cell>0.0385</cell></row><row><cell>Mean</cell><cell>L2A1</cell><cell></cell><cell>0.0367</cell><cell></cell><cell></cell><cell>0.3826</cell><cell></cell><cell></cell><cell>0.0653</cell></row><row><cell></cell><cell>L2N2</cell><cell></cell><cell>0.0385</cell><cell></cell><cell></cell><cell>0.3485</cell><cell></cell><cell></cell><cell>0.0674</cell></row><row><cell cols="6">dimensional space of documents during the process of fea-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ture reduction.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported by the <rs type="funder">Beijing Excellent Talent Development Foundation</rs>, the <rs type="funder">Importation and Development of High Caliber Talents Project of Beijing Municipal Institutions</rs> (No. <rs type="grantNumber">CIT&amp;TCD201404052</rs>), the <rs type="funder">Guangxi Colleges and Universities Key Laboratory of</rs> cloud computing and complex systems, the <rs type="funder">Basic Research Programs of Science and Technology of BJUT</rs>, the <rs type="funder">BJUT Science and Technology Fund</rs> (ykj-<rs type="grantNumber">2014-11508</rs>) .</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bbxKbm6">
					<idno type="grant-number">CIT&amp;TCD201404052</idno>
				</org>
				<org type="funding" xml:id="_Xp6296M">
					<idno type="grant-number">2014-11508</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,70.60,662.38,221.91,9.54;5,70.60,673.34,221.91,9.54;5,70.60,684.30,221.91,9.54;5,70.60,695.26,165.76,9.54" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,228.95,662.38,63.55,9.54;5,70.60,673.34,152.09,9.54">BJUT at TREC 2014 Temporal Summarization Track</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,243.81,673.34,48.70,9.54;5,70.60,684.30,190.36,9.54">Proceedings of the Twenty-Third Text REtrieval Conference</title>
		<meeting>the Twenty-Third Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="500" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,336.10,407.69,221.91,9.54;5,336.10,418.65,221.91,9.54;5,336.10,429.61,156.06,9.54" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,407.38,407.69,150.62,9.54;5,336.10,418.65,205.93,9.54">From Single to Multi-document Summarization: A Prototype System and its Evaluation</title>
		<author>
			<persName coords=""><forename type="first">C Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,336.10,429.61,89.30,9.54">Proceedings of the Acl</title>
		<meeting>the Acl</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,336.10,459.49,221.91,9.54;5,336.10,470.45,221.91,9.54;5,336.10,481.41,221.91,9.54;5,336.10,492.37,19.93,9.54" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,454.14,459.49,103.86,9.54;5,336.10,470.45,221.91,9.54;5,336.10,481.41,201.15,9.54">Nonnegative Matrix Factorization for Combinatorial Optimization: Spectral Clustering, Graph Matching, and Clique Finding</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,360.73,492.37,197.28,9.54;5,336.10,503.33,188.36,9.54" xml:id="b3">
	<monogr>
		<title level="m" coord="5,360.73,492.37,197.28,9.54;5,336.10,503.33,123.51,9.54">Eighth IEEE International Conference on Data MiningIEEE Computer Society</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,336.10,518.27,221.91,9.54;5,336.10,529.23,221.91,9.54;5,336.10,540.19,221.91,9.54;5,336.10,551.15,103.94,9.54" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,454.78,518.27,103.22,9.54;5,336.10,529.23,96.54,9.54">Exploiting homophily effect for trust prediction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,456.65,529.23,101.36,9.54;5,336.10,540.19,221.91,9.54;5,336.10,551.15,47.72,9.54">Proceedings of the sixth ACM international conference on Web search and data miningACM</title>
		<meeting>the sixth ACM international conference on Web search and data miningACM</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
