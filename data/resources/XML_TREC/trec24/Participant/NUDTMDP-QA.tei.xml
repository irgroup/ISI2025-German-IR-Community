<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.88,117.85,262.56,13.10">NudtMDP at TREC 2015 LiveQA Track</title>
				<funder ref="#_MzRUxYD">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_vujvpFf">
					<orgName type="full">National Key Fundamental Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_xyC5AQP">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_8pzzd7b #_up2zrWZ #_P4Fbvc7">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.17,153.01,52.82,8.69"><forename type="first">Yuanping</forename><surname>Nie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.00,153.01,58.73,8.69"><forename type="first">Jiuming</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.53,153.01,56.91,8.69"><forename type="first">Zongsheng</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,348.44,153.01,24.07,8.69"><forename type="first">Hai</forename><surname>Li</surname></persName>
							<email>haili@nudt.edu.cnjiuming.huang@qq.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.12,153.01,56.67,8.69"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,456.49,153.01,15.65,8.69;1,309.29,163.92,11.74,8.69"><forename type="first">Yan</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.88,117.85,262.56,13.10">NudtMDP at TREC 2015 LiveQA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">299F8DBAFFBCC5EDEBB2B85508280850</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>web based search</term>
					<term>question answering</term>
					<term>CQA</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe a web-based online question answering system which has been evaluated in TREC 2015 Live QA task. Automatic question answering is a classic widely learned technology. TREC have host 8 times QA tracks since 1999. However, the TREC results show that there is still a long way to solve the questions perfectly. LiveQA is kind of questions means asked by 'real users'. Most liveQAs are non-factoid questions and it is much more challenge to answer the liveQAs than factoid QAs. We build a question answering system to find the answers from web data. The system has two channels, one use search engine to obtain the answers and the other focus on community question answering websites. We finally submit 3 runs in the o cial test. Two of our runs can perform much better than the avarge scores.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we describe the online Question Answering system which has been evaluated in TREC 2015 Live QA task. In this year Live QA track, there is only one main task, which has aimed at the task of providing automatic answers for human nature questions. The live QA track, di↵erent from past QA tracks is focusing on 'live' questions that are from real-user. All the testing questions are from Yahoo Answer. YA questions have many question types such as opinion, advice, polls, which make the task much more di cult. The answers also have length limitation, the answer length restriction is 250 chars at first and then increased to 1000 chars. We think the limitation of answers' length is another challenge for this year QA task. We notice that the testing questions are quite di cult. Firstly there is no given source for answers. We need to find answers through Internet and the answers must be extracted as a short query(1000chars). Secondly, all of the questions are from Yahoo Answer questioners. Most questions are asked by spoken language. There exist many oral words and the style of sentences is also complicated. Thirdly the knowledge and information that questioners required are subjective and divergent. In this year Live QA track, lots of questions may have many available answers. And some questions need professional answers. So we decide to crawl and extract the answers by using search engine(Google) and CQAs(Community Question Answering websites). This is our first participation in TREC. Our primary goal this year is to develop a Question Answering system framework to which future enhancements can be applied. We finally submit three runs for the o cial run in Sept.2. One system(nudtmdp 2) just use CQAs which include eight community question answer websites such as Yahoo Answer, AnswerBag and Answers as answers resource. The other two systems(nudtmdp 1 and 3) employ both CQAs and search engine as answers resource. The di↵erence between this two systems is just the di↵erent strategy of choosing answers, which we will introduce later. The results show that the two fusion version QA systems(0.670 and 0.602) performed much better than the CQA version(0.388) and the averge score(0.465).</p><p>In this paper, we introduce the main idea and framework of our QA system. The structure of the QA system is shown at The Question Processing Part is to translate the question to the search query. It has three components, Classification: we classify the questions into di↵erent categories; Filter: the stop words and no useful information can be removed automatically. Extention: We use Word2Vec <ref type="bibr" coords="2,324.95,306.70,9.59,8.69" target="#b1">[2]</ref> and some dictionary to extend the search query in order to acquire more useful results. After question processing, we employ a distributed crawler to search the results. For we have used search engine(Google) and serval CQA sites as the answers resources, we need a stable distributed crawler for the QA systems. In this year, we use Apache Storm[] framework. Finally the Answer Processing Part contained the strategy to choose the best answer and extract the answerto 1000 chars.</p><p>In the following sections, we describe in detail the parts taken to create our runs. We also discuss what we have learned from this exercise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>In this section, we introduce our question answering system framework for the live QA task. Our Question Answering system can automatic answer user's nature language questions based on online search. The input of the system is a nature language question and the outputs are the best answer selected from candidate answers by the ranking scores. The QA system can response one question within 1 min. Our Question answering system includes several phases. And it can be broadly divided into three main parts: Question Processing Part, Distributed Crawler Part and Answer Processing Part. The structure of our QA system is shown at Fig. <ref type="figure" coords="2,211.16,532.60,3.76,8.69" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Processing Part</head><p>The first step of the question answering system is to understand the questioner's nature language questions and translate the questions to search queries. In this year Live QA task, all the testing questions are from Yahoo Answer, which are consisted by two parts: Titles and Bodies. The titles are necessary for every questioners and the bodies are not. We notice that most questions only have titles or the titles already have the whole useful information that meet the questioners' need. The other questions have both titles and bodies. The bodies usually are descriptions for titles and have a much longer length. We found that using the titles as questions are much easier to process. However in this year QA task, we employ a primary strategy to use titles and bodies. The strategy is shown at below: And we use such components to process the nature language questions. 1) Classification component: The given questions from Yahoo Answer do have category features, like Pets, History and Hair. However, we find the categories is to large and di↵erent community question answering systems have their own classification systems. So it is necessary to rebuild the classification system, which is more detailed by ourself. Based on the exist categories, we employ Latent Dirichlet Allocation(LDA) <ref type="bibr" coords="3,240.70,589.42,9.59,8.69" target="#b0">[1]</ref> model, which is a hierarchical nonpara-metric Bayesian approach to discover topic in text corpora training new categories. The classification component can help the system to understand the questioner needs. 2)</p><p>Filter component: We remove the stop words and no useful information such as oral words. We also abandon the non-English words by a language detector called Idig <ref type="bibr" coords="4,201.39,143.06,9.08,8.69" target="#b2">[3]</ref>. This tool kit is a prototype for short message service with 99.1% accuracy for 17 languages. We only keep the questions which are consisted by the vast majority English characters with a threshold value. The filter component increase the e ciency and accuracy of the question search. 3) Extention component: In the QA system we use Word2Vec, and some dictionary to extend the search query in order to obtain more useful results. Some abbreviation such as WW2 should be expanded to world war two and ASAP to as soon as possible. There are also some proper names can be extended, for example the Summer Palace can be extended to Summer Palace, Beijing, China. For the questions are usually quite short, expending the information of questions can search more useful results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributed Crwaler Part</head><p>In this year Live QA task, we employ Google and CQAs as the answers resources. The o cial runs have time limitation: the answer can not be reached over 60 seconds. So it makes great demands on our crawler. We use Apache Storm <ref type="bibr" coords="4,452.49,314.99,9.59,8.69" target="#b4">[5]</ref> as our crawler framework. Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data. We use eight famous community question answering systems are our CQA resources, such as: Yahoo Answer, AnswerBag, Askville, About and so on. One of the most important component is to calculate the similarity between target questions and candidate questions. Similarity calculating component: Calculating the similarity between two questions is a very important component in our QA systems. In search engine and community question answering web sites we can always find candidate questions or answers. How to find the most related information is one of the major task we should solve. In the QA system, we jointly consider the words similarity, words dependency and sentence structure to calculate the similarity of two given questions. To normalized the equation, we get the formula of the similarity calculation is shown at Eqn.1; If there are two questions a and b, we can calculate the similarity of two questions as:</p><formula xml:id="formula_0" coords="4,249.64,483.35,223.25,16.71">Sim = A ⇤ X ⇤ B T + A T ⇤ X ⇤ B (1)</formula><p>where, X is a similarity matrix betweem question a and b. Every element of X means the distence of every two words of sentence a and b, which can be measured using WordNet <ref type="bibr" coords="4,259.64,523.99,9.08,8.69" target="#b3">[4]</ref>. The A and B is a dependency matrix of question a and b. In A every element takes the dependency of every two words in question a. However, we find that it is much more di cult to calculate the similarities between questions and answers. We do some label works which show there is only very weak semantic link between questions and answers. In CQAs there are no such problems, for we should just judge the similarity of two similar questions. But in search engine such as Google, the search results are not questions. In this paper, we use the title or key sentence of the articles as 'questions' so that we can avoid to judge the similarity of questions and answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distributed Crwaler Part</head><p>Answer Process Part is to extract the right answer from the candidate results.</p><p>According to answer processing part, the similarity component can obtain the best question from the candidate questions we search from Internet. As we know, every similar question in CQAs may has more than one answer. Which answer is the best answer of the most similar question is the first component of the Answer Processing Part. And the next part is to extract the answers meeting the length limitations. Answer score: In community question answering websites, we did not use the Nature Language Processing(NLP) method to choose the best answer, because there are many user behaviors can be used. We employed the users behavioral analysis to score the candidate answers. The behavioral information includes: a) Many CQA systems allow questioner choose the best answer(or other similar name) by themselves. There some exist questions have the best answer already. b) The websites like Yahoo Answer allow other users give every answer their attitude such as support(up) and argue(down). c) Answerers have their reputation in CQA systems. We believe that a good answerer can provide good answers. So we employed the answers reputation in our QA system. The reputation is calculated by their question-answer history and the relevancy of the topic.</p><p>Answer Extraction: When the answer length limitation is 250 chars, the answer extraction is very important to the candidate results both from CQAs and Google. We can only get 1-2 sentences for the answers. In this situation, the CQAs' results perform better, because the answers of CQA are much shorter than search engine results. When the answer length limitation is extend to 1000 chars, the situation is di↵erent. There is little influence in CQAs' results, but in the Google results, they are much easier to gain good answers. In this paper, we focus on introducing our Google result's extraction strategy. We first open the website and find the paragraph that contains the matching information. Then we calculate the Importance of the first sentence and the relevancy to next sentence and go through. Then we can get the score of our sentences. We choose as many as possible sentences to consist our answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Evaluation method: We submitted three runs for main task at Live QA in Trec 2015. There were 1087 questions judged and scored using 4-level scale:</p><p>4: Excellent a significant amount of useful information, fully answers the question 3: Good C partially answers the question 2: Fair marginally useful information 1: Bad C contains no useful information for the question -2: the answer is unreadable (only 15 answers from all runs were judged as unreadable)</p><p>The performance measures are:</p><p>avg-score(0-3) : average score over all queries (transfer-ring 1-4 level scores to 0-3, hence comparing 1-level score with no-answer score, also considering -2-level score as 0) succ@i+ : number of questions with i+ score (i=1..4) divided by number of all questions prec@i+ : number of questions with i+ score (i=2..4) divided by number of answered only questions Our three runs results and the average scores of all runs are shown below.</p><p>Run Name Avg Score succ@1+ succ@2+ succ@3+ succ4+ prec2+ prec3+ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>This is our first time participation in LiveQA task. Our primary goal this year is to build an automatic question answering system that can be applied in next years tasks. In the beginning, we firstly try to use knowledge based question answering system to solve the questions. We have built a question answering system using Wiki, Freebase and DBpedia as knowledge base. We crawl some history Yahoo Answer QA pairs as testing database. However, the results of our system are not satisfied. We think the reason may be: the knowledge based answering system can solve objective questions which are contained in the database with high performance. But if the questions knowledge doesnt contain in the database, the system can not give good answers. In this year liveQA task, all the questions are from real person and most of them are subjective. So the performance of our knowledge based QA system is not good. Then we employ search engine based QA system. This QA system version has one important advantage, it is that there are always results when search the search engine. There are also some challenges in search engine based QA system, the results we obtained are very long. And the limitation of answers is no larger than 250 chars at first. It is pretty di cult to extract the accurate information from candidate results. In recent years, community question answering(CQA) systems become very popular. CQAs have many exist question-answer pairs. We find use CQAs based question answering system can solve many questions. However, there still have some questions can not be good answered. So we finally decide to use CQAs and Google as our question answering systems resources. However, we still have many parts which have not been good solved. Some exist components are just using primary strategy. In next year LiveQA task, we will focus on those components and improve our approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,332.04,252.17,140.86,8.69;2,157.42,263.07,315.46,8.69"><head>Fig 1 .</head><label>1</label><figDesc>It contains three parts: Question Processing Part, Distributed Crawler Part and Answer Processing Part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,241.74,263.83,25.43,2.06;3,269.98,258.08,118.60,7.82;3,313.99,268.07,2.33,7.82;3,188.78,118.57,252.77,126.19"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The structure of our QA system ;</figDesc><graphic coords="3,188.78,118.57,252.77,126.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,158.69,224.83,309.84,58.12"><head>Table 1 .</head><label>1</label><figDesc>The results of our 3 runs and the average result</figDesc><table coords="6,442.36,224.83,26.17,7.82"><row><cell>prec4+</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported in part by the <rs type="funder">National Key Fundamental Research and Development Program of China</rs> (<rs type="grantNumber">2013CB329601</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61372191</rs>, <rs type="grantNumber">61202362</rs>, <rs type="grantNumber">61472433</rs>), Project funded by <rs type="funder">China Postdoctoral Science Foundation</rs>(<rs type="grantNumber">2013M5452560</rs>,<rs type="grantNumber">2015T81129</rs>). We are very thankful to <rs type="person">Chao An</rs>, <rs type="person">Hongmei Liu</rs> and <rs type="person">Yue Qi</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vujvpFf">
					<idno type="grant-number">2013CB329601</idno>
				</org>
				<org type="funding" xml:id="_8pzzd7b">
					<idno type="grant-number">61372191</idno>
				</org>
				<org type="funding" xml:id="_up2zrWZ">
					<idno type="grant-number">61202362</idno>
				</org>
				<org type="funding" xml:id="_P4Fbvc7">
					<idno type="grant-number">61472433</idno>
				</org>
				<org type="funding" xml:id="_MzRUxYD">
					<idno type="grant-number">2013M5452560</idno>
				</org>
				<org type="funding" xml:id="_xyC5AQP">
					<idno type="grant-number">2015T81129</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,160.69,236.95,312.21,8.18;7,168.50,246.95,208.18,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,361.29,237.31,93.95,7.82">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,461.95,236.95,10.95,8.18;7,168.50,246.95,137.45,8.18">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,160.69,257.31,312.20,7.82;7,168.50,266.94,304.38,8.18" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,300.25,257.31,172.64,7.82;7,168.50,267.30,157.38,7.82">word2vec explained: deriving mikolov et al.&apos;s negative-sampling word-embedding method</title>
		<author>
			<persName coords=""><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,160.69,277.30,312.20,7.82;7,168.50,287.30,304.39,7.82;7,168.50,297.30,261.25,7.82" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,417.07,277.30,55.82,7.82;7,168.50,287.30,304.39,7.82;7,168.50,297.30,102.67,7.82">Pkuicst at trec 2014 microblog track: Feature extraction for e↵ective microblog search and adaptive clustering algorithms for ttg</title>
		<author>
			<persName coords=""><forename type="first">Chao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feifan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Runwei</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="7,160.69,306.93,312.20,8.18;7,168.50,316.93,95.64,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,235.54,307.29,143.49,7.82">Wordnet: a lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,387.13,306.93,85.75,8.18;7,168.50,316.93,16.42,8.18">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,160.69,327.29,312.20,7.82;7,168.50,337.28,304.39,7.82;7,168.50,346.92,304.39,8.18;7,168.50,356.92,234.94,8.18" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ankit</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siddarth</forename><surname>Taneja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jignesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjeev</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maosong</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jake</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Donham</surname></persName>
		</author>
		<title level="m" coord="7,271.78,346.92,201.11,8.18;7,168.50,356.92,126.97,8.18">Proceedings of the 2014 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2014 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
