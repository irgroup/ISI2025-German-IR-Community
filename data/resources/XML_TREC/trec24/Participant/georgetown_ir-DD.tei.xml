<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,84.91,77.22,442.07,13.32;1,253.56,98.10,104.77,13.32">Re-ranking via User Feedback: Georgetown University at TREC 2015 DD Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,246.67,127.24,48.33,10.72"><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street NW</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,318.33,127.24,46.99,10.72"><forename type="first">Hui</forename><surname>Yang</surname></persName>
							<email>huiyang@cs.georgetown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street NW</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,84.91,77.22,442.07,13.32;1,253.56,98.10,104.77,13.32">Re-ranking via User Feedback: Georgetown University at TREC 2015 DD Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A2E34635AB2F3C9FBD3C801113312C95</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are two principal components involved in a search process, the user and the search engine. In TREC DD, the user is modeled by a simulator, called "jig". The jig and the search engine exchange many messages among themselves, including the relevant passages returned by the jig, user cost spent on examining the documents, etc. In this work, we don't apply any dynamic search algorithms to model these interactions. Instead, we produce a basic re-ranking baseline.</p><p>Our algorithm starts at taking in an initial query from the simulator. During the search, we collect the relevance feedback from the simulator and use them to re-rank the initial retrieval results. Our algorithm terminates itself automatically when it senses that the user has gained enough information about the search topic or that no further relevant documents can be retrieved for the user.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>TREC 2015 Dynamic Domain 1 is an interactive search task. There are two components in it, a search engine and a simulating user. The search engine is implemented by the participants, while the simulating user is provided by TREC 2015 DD Track organizer, and is called "jig". There are 118 search topics in total. For each search topic, a short query, which is the name of the search topic, is given to the participant's system at the beginning. Based on the initial query, the participant's system is required to return five documents to the jig. Then the jig provides feedbacks to the retrieval system regarding these five documents. The retrieval system can decide how to utilize these user feedbacks. If the system decides to continue the search procedure, it returns five more documents to the jig, and the jig judges them and returns feedbacks again. This iteration can loop forever and it is the participant system's responsibility to decide when to end the loop. The feedbacks from the jig contain information about topic ID, jig's confidence degree of its judgment, the subtopics that the returned document covers, the relevant passage content, etc. The detail of the feedback is listed in Table <ref type="table" coords="1,344.24,556.36,4.50,10.72">1</ref>. The documents returned at each loop are combined together and judged by TREC DD evaluation metrics. For example, if a search system interacts with the jig for t times and at j th time, it returns a document set D j , where j belongs to [1, t], then TREC DD combines D 1 to D t chronologically to form a ranked list and evaluates it. A good retrieval system for TREC DD should return high relevant materials at an early position, should cover as many subtopics as possible, and should minimize users' efforts to examine retrieval results.</p><p>Our approaches are built upon the Lemur<ref type="foot" coords="2,286.32,72.30,3.96,6.93" target="#foot_0">2</ref> search engine. We use Lemur's default retrieval algorithm to conduct retrieval and then re-rank the retrieved result based on user feedbacks. Our approaches are able to terminate the search process when they sense that the user has gained enough information about the search topic or that no further relevant documents can be retrieved for the user.  The remaining parts of this paper are organized as follow. We discuss each technical approach in detail from Section 2 to Section 4. In Section 5, we present our submissions and the evaluation results. In Section 6, we conclude our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">LM Retrieval Model</head><p>For the initial query of each search topic, we don't have any user feedback received yet and the only information we have is the initial query, so we directly use Language Modeling with Dirichlet smoothing [1], which is the default search algorithm in Lemur. For one search term t, the document d's relevance score is calculated as:</p><formula xml:id="formula_0" coords="2,234.73,562.27,142.53,29.83">! ! ! = !" !, ! + !"(!|!) !"#$%ℎ ! + !</formula><p>where length(d) is the length of document d. !" !, ! !is the term frequency of term t in document d. P(t|C) is the probability that the corpus C generates term t in the query. The Dirichlet smoothing parameter µ is set to 5000 empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Feedback Re-ranking Model</head><formula xml:id="formula_1" coords="2,72.00,694.06,146.44,12.04">!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!</formula><p>After we retrieve for the initial query using LM, we get a ranked list of documents. Every time when we deliver five documents to the jig, we get some feedbacks from it. Here we use the texts of the relevant passages in the feedbacks to re-rank documents in the original list. The new relevance score of a document d in the original list becomes:</p><formula xml:id="formula_2" coords="3,159.64,136.27,292.72,14.39">! ! ! = 1 -! ! !"# ! ! + ! * !"#!$%&amp;'(!, !""#$%&amp;')</formula><p>Here ! !"# ! ! is the original relevance score calculated by LM. Feedback is the relevant passage texts collected from the jig's feedbacks. SimScore is a similarity score between the content of document d and relevant texts, Feedback. We use the cosine similarity to calculate SimScore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Stopping Criteria</head><p>Our approaches are able to terminate themselves based on user feedbacks. For a search topic, we call the procedure of search engine sending five documents to the jig and the jig returning judgment for the five documents as one iteration. Our approaches terminate themselves if in n adjacent iterations no relevant documents are found. We set n=5, if not a single relevant document has been found for this topic. Otherwise we set n=3. It means that the search engine assumes that in order to finish a search task, the user is willing to spend more effort to examine retrieval results if they find no relevant information yet. Another stopping condition is that for each topic, at most we loop for 20 iterations. We assume that after 20 iterations, the user is tired of current search topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments 5.1 Domains</head><p>We run our experience on three different domains. We index them using Indri. All stopwords are removed and terms are stemmed using Krovetz Stemmer <ref type="bibr" coords="3,347.92,431.08,12.74,10.72" target="#b1">[2]</ref>.</p><p>Ebola. This dataset talks about the 2014-2015 Ebola outbreak in Africa. It contains 497,362 web pages, 19,834 PDFs and 164,961 tweets in total.</p><p>Illicit Goods. Documents in this dataset are threads crawled from underground hacking forums, such as BlackHatWorld.com and HackForums.com, which include posts and metadata. It talks about how illicit and fake goods such as fake Viagra are made, advertised, and sold on the Internet. All documents are in the HTML format.</p><p>Local Politics. This dataset is related to the regional politics in the Pacific Northwest and the small-town politicians. It is a clean HTML news dataset. This corpus is a subset of the original TREC 2014 KBA Stream Corpus. Table <ref type="table" coords="3,268.98,585.16,6.00,10.72" target="#tab_1">2</ref> shows the statistics of these three datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Submission</head><p>GU_RUN3_SIMI. This is a re-ranking approach. In this submission, we use the initial query and the LM retrieve model to obtain an original ranked list. Then we use the Feedback Re-ranking model to re-rank documents in the original list. We tune the parameter ! = 0.6 .</p><p>GU_RUN4_SIMI. This is also a re-ranking approach. It is similar to GU_RUN3_SIMI. The difference is that before we apply the Feedback Re-ranking model, we filter the terms in the relevant feedback texts using IDF. Terms with an IDF value less than 0.1 are thrown away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Table <ref type="table" coords="4,185.31,217.87,6.00,10.32">3</ref>  The official evaluation metrics for TREC 15 DD Track are CubeTest <ref type="bibr" coords="4,415.72,378.49,13.99,10.50">[3]</ref> and ERR <ref type="bibr" coords="4,481.96,378.49,12.74,10.50" target="#b8">[4]</ref>. Table <ref type="table" coords="4,534.00,378.49,6.00,10.50">3</ref> reports the scores of ACT@10, CT@10, ERRA and other metrics for our submissions. It also reports the best and median scores for all submissions in TREC 15 DD Track. The "Best"/ "Median" row doesn't correspond to a real submission. It is constituted by the best/median scores generated by each evaluation metric.</p><p>Our submissions are around the median positions for all evaluation metrics. A median Recall value indicates that the original retrieval list used in our re-ranking approaches doesn't contain enough relevant documents. We believe that this is the fact that limits our submissions' performance. Nonetheless in our experiments, we indeed observe CT score improvement in our submitted runs when we compare them with a naive basic approach, where we only apply LM retrieval model for the initial query and ignore all user feedbacks. This indicates that user feedbacks in a search session are informative and hence they shouldn't be discarded.</p><p>The GU_RUN3_SIMI and GU_RUN4_SIMI runs have similar scores for all metrics. It suggests that filtering low quality terms in the feedback texts isn't important for re-ranking approaches.</p><p>Another observation is that we find out that if we terminate our interaction iterations with the jig earlier, our CT scores will be improved. This indicates that we need a more sensitive stopping strategy than the current one which we describe in Section 4.</p><p>After TREC submission deadline, we implement another system where the user feedbacks are not utilized to re-rank an original list, but are used to generate a new query for retrieving at each iteration. The result shows that the new approach doesn't significantly impact the CT score, however it does improve the Recall score. It indicates that the TREC 15 DD Track is a precision sensitive retrieval task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,97.52,166.64,3.60,9.45;2,112.08,183.20,5.18,9.45;2,143.52,200.00,95.81,9.45;2,143.52,216.56,87.74,9.45;2,143.52,233.12,57.19,9.45;2,163.09,249.92,5.18,9.45;2,179.04,266.48,122.10,9.45;2,179.04,283.04,333.17,9.45;2,179.04,299.84,48.65,9.45;2,163.09,316.40,45.70,9.45;2,143.52,333.20,6.36,9.45;2,143.52,349.76,58.76,9.45;2,112.08,366.32,7.98,9.45;2,112.08,383.12,18.72,9.45;2,97.52,399.68,3.60,9.45"><head></head><label></label><figDesc>: "DD15-1.1", "passage_text": "Federal judge Redden taking himself off the salmon case",</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,205.02,144.21,201.89,9.08"><head>Table 1 An Example of the Jig's Feedbacks</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,86.40,609.33,440.04,92.14"><head>Table 2 Statistics Table for TREC DD 2015 Datasets</head><label>2</label><figDesc></figDesc><table coords="3,306.86,633.30,219.58,10.09"><row><cell>Ebola</cell><cell>Local Politics</cell><cell>Illicit Goods</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,79.00,709.45,141.31,10.50"><p>http://www.lemurproject.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,76.50,75.54,70.85,10.09;5,72.00,96.76,468.00,10.72;5,72.00,110.44,467.99,10.72;5,72.00,124.36,467.99,10.72;5,72.00,138.04,308.86,10.72" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,90.00,75.54,57.35,10.09;5,72.00,96.76,468.00,10.72;5,72.00,110.44,467.99,10.72;5,72.00,124.36,467.99,10.72;5,72.00,138.04,227.12,10.72">Our performance is mainly limited by the low quality of the initial retrieval list used for re-ranking. Based on the evaluation results, we learn that 1) multiple retrievals are needed for TREC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Conclusion We submit two re-ranking approaches to the TREC 2015 DD Track. Track</note>
</biblStruct>

<biblStruct coords="5,396.91,138.04,143.09,10.72;5,72.00,151.96,318.37,10.72" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,396.91,138.04,143.09,10.72;5,72.00,151.96,313.37,10.72">the task is precision sensitive, hence a sensitive and sophisticated stopping criteria is necessary</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,407.82,151.96,132.17,10.72;5,72.00,165.64,365.20,10.72" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,407.82,151.96,132.17,10.72;5,72.00,165.64,360.13,10.72">the feedbacks in the search session are informative, and are useful to help improving retrieval accuracy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,76.50,188.10,108.16,10.09;5,72.00,209.32,468.00,10.72;5,72.00,223.00,467.99,10.72;5,72.00,236.92,314.92,10.72" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,90.00,188.10,94.66,10.09;5,72.00,209.32,468.00,10.72;5,72.00,223.00,467.99,10.72;5,72.00,236.92,309.88,10.72">Acknowledgement The research is supported by DARPA FA8750-14-2-0226, NSF IIS-1453721, and NSF CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,76.10,259.41,65.15,9.08" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,91.24,279.25,448.73,9.64;5,72.00,291.97,264.21,9.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,201.51,279.25,338.46,9.64;5,72.00,291.97,35.42,9.64">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,114.14,291.97,96.99,9.64">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,91.04,308.53,448.86,9.64;5,72.00,321.25,85.50,9.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,199.85,308.53,199.58,9.64">A comparative study of stemming algorithms</title>
		<author>
			<persName coords=""><forename type="first">Anjali</forename><surname>Jivani</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ganesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,411.26,308.53,112.19,9.64">Int. J. Comp. Tech. Appl</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1930" to="1938" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,88.11,337.81,451.84,9.64;5,72.00,350.53,288.65,9.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,268.61,337.81,271.35,9.64;5,72.00,350.53,147.03,9.64">The water filling model and the cube test: multi-dimensional evaluation for professional search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,226.26,350.53,51.23,9.64">CIKM 2013</title>
		<meeting><address><addrLine>Burlingame, CA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,89.20,367.09,450.75,9.64;5,72.00,379.81,145.09,9.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,326.08,367.09,209.39,9.64">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,72.00,379.81,28.54,9.64">CIKM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Hong Kong, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
