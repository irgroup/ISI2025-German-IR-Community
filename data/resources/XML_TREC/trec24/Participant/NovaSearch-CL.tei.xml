<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.76,115.96,313.83,12.62;1,256.58,133.89,102.19,12.62">NovaSearch at TREC 2015 Clinical Decision Support Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.53,171.56,61.29,8.74"><forename type="first">André</forename><surname>Mourão</surname></persName>
							<email>a.mourao@campus.fct.unl.pt</email>
						</author>
						<author>
							<persName coords="1,264.32,171.56,64.71,8.74"><forename type="first">Flávio</forename><surname>Martins</surname></persName>
							<email>flaviomartins@acm.org</email>
						</author>
						<author>
							<persName coords="1,351.72,171.56,69.11,8.74"><forename type="first">João</forename><surname>Magalhães</surname></persName>
							<email>jm.magalhaes@fct.unl.pt</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática Faculdade de Ciências e Tecnologia</orgName>
								<orgName type="institution">NOVA LINCS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Universidade NOVA de Lisboa</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.76,115.96,313.83,12.62;1,256.58,133.89,102.19,12.62">NovaSearch at TREC 2015 Clinical Decision Support Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E0DC646A6E5BB85B0CD81694FF755AAE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the NovaSearch group at TREC Clinical Decision Support 2015. For this year's task, we extended our rank fusion experiments from last year's edition using a supervised Learning to Fuse technique. Learning to Fuse is a technique that incrementally combines multiple runs that use different retrieval algorithms, relevance feedback schemes and query expansion data sources to create a better final rank. We also experimented with query expansion using MeSH, SNOMed CT and Shingles thesaurus and tested a Journal based filtering technique to remove results from irrelevant journals. For Task B runs, we added the diagnosis information to the queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC Clinical Decision Support Track goal is the "retrieval of biomedical articles relevant for answering generic clinical questions about medical records." <ref type="foot" coords="1,467.85,462.23,3.97,6.12" target="#foot_0">1</ref>This is the second edition of the track of the track and shares the same dataset and tasks with the 2014 edition, with the addition of a Task B that includes the diagnosis of the patient for the "treatment" and "test" queries. Our participation on this track follows our work on last year's track <ref type="bibr" coords="1,414.10,511.62,9.96,8.74" target="#b2">[3]</ref>.</p><p>Section 2 details our usage of general and domain specific IR techniques. Section 3 describes our Learning to Fuse framework. Section 4 describes our journal filtering algorithm. Section 5 contains the results and discussion.</p><p>2 Medical text indexing and retrieval Our indexing and retrieval system is based on Lucene, with support for additional retrieval functions, query expansion and pseudo-relevance feedback. <ref type="bibr" coords="1,431.54,613.95,10.52,8.74" target="#b1">[2]</ref> contains a detailed explanation of the full system; on this paper, we'll describe our new experiments for TREC CDS 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query expansion: MeSH, SNOMed and Shingles</head><p>Our baseline query expansion method is based on a SKOS formatted version of MeSH using Lucene-SKOS <ref type="bibr" coords="2,254.23,151.60,9.96,8.74" target="#b0">[1]</ref>. This year, we tested two additional methods:</p><p>-SNOMed CT 2015: we parsed the terms and relations of the Web Ontology Language (OWL) version of SNOMed CT International Release RF2 from January 2015, to make it work with Lucene-SKOS; -Shingles: we created a n-gram (n=8) word based index that enabled expansion of query terms with neighbor terms from documents in the collection.</p><p>When expanding with MeSH and SNOMed, we appended all synonyms, alternative and preferential labels for all query terms with a weight of 0.7 (original query terms are weighed 1). When expanding with Shingles, we added the terms that appeared on more than one of the expansions of the individual query terms. Figure <ref type="figure" coords="2,166.20,292.53,4.98,8.74" target="#fig_0">1</ref> shows some sample expansions for MeSH, SNOMed CT and Shingles.</p><p>58-year-old woman ["women"] with hypertension [ "blood pressure high" | "high blood pressure disorder" | "hypertensive vascular degeneration" | "high blood pressure disorder"] and obesity presents with exercise-related ["aerobic exercise" | "exercise physical" | "isometric exercise" | "lesion"] episodic chest pain ["pain observations" | painful | "part hurts" ] radiating to the back ["back structure excluding neck" | "entire back surface region"]. Custom top-precision reranking When analyzing last year results, we discovered that query expansion lead to a slight decrease in the relevance of the top 2 documents, when compared to the original non-expanded run. Based on this observation, and for all expanded runs, we kept the top 2 results from the non-expanded run and followed by the results from the expanded run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rank fusion: Learning to Fuse</head><p>For last year's track, we selected a set of runs that applied different retrieval functions and combined them using a fusion algorithm. This year, we extended our unsupervised rank fusion method by adding a supervised step that selects what runs to combine, based on their performance on a relevant dataset. These runs can be based on different retrieval functions, expansion methods, etc. It Algorithm 1 Learning to fuse (L2F) algorithm let a rank be a list of documents (q1,id1,1,1),...,(q1,id1,n, n),...,(qm,idm,n,nm) where id is a document id, m is the number of queries and n the number of results per query, Input: R: a sorted list of ranks returned by multiple retrieval systems. The list is sorted in descending order according to a evaluation metric met, Input: met: evaluation metric that takes a rank and a set of relevance judgments and returns the value of that evaluation metric for that rank, Input: comb: fusion algorithm that takes two ranks R1 and R2 and combines them into R f , sorted according to rank and frequency across ranks (e.g. RRF, ISR), Input: tries: natural number representing the number of iterations to run while the results are not improving, Output: R best : final combined rank.</p><p>1: Rcurrent ← R1 2: R best ← R1 3: i ← 2 4: currentT ries ← tries 5: while currentT ries ≥ 0 and i &lt; len(R) do 6:</p><p>Rcurrent ← comb(Rcurrent, Ri) 7:</p><p>if met(Rcurrent) &gt; met(R best ) then 8:</p><p>R best ← Rcurrent 9:</p><p>currentT ries ← tries 10: else 11:</p><p>currentT ries ← currentT ries -1 12: i ← i + 1 13: return R best works by sorting runs by performance on an evaluation metric and fuse additional runs if they keep improving the final result. Our approach is fully detailed in Algorithm 1.</p><p>The tries parameter was introduced to avoid local minima, by allowing the result to get slightly worse at one iteration, if it leads to a better result after adding additional ranks. It was set to 3. The selected evaluation metric for ranking was infNDCG and the fusion algorithm was RRF. The models were trained with last year queries and relevance judgments.</p><p>The set of possible ranks was a combination of all of the following:</p><p>-Retrieval function: TFIDF, BM25+, BM25L or Language Model (LM) -Search Fields: full text (f ) or full text + title + abstract (fat) -Query expansion: no expansion (NoEXP), MeSH (MSH), SNOMed CT (SNO), Shingles (SHI) -Pseudo-Relevance Feedback: true (PRF) or false (NoPRF)</p><p>The runs selected by the algorithm are available in Table <ref type="table" coords="3,413.65,632.21,3.87,8.74" target="#tab_0">1</ref>. Best 2014 is the set of runs that led to our best result on TREC CDS 14. L22F A and B are the runs selected by the Learning to Fuse algorithm for Task A and B, respectively. BM25+, BM25L and TFIDF retrieval functions, MeSH expansion and PRF seem to be present in the bulk of the selected runs, although both selections also improved when Language Model based runs and runs without Query Expansion were added to the query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Journal-based filtering</head><p>The PubMed Central collection contains an huge amount of articles and journals that are not relevant for case-based clinical support systems. These articles and journals may be related to non-human subjects, gene sequencing, amongst others.</p><p>We created a filter that removes from the ranks articles from certain journals deemed irrelevant, for each query type. The filters are based on the Vowpal Wabbit classifier and trained using journal articles Bag-of-Words (BoW), after stemming and stop word removal. For each query type, we selected a sample of relevant and non-relevant documents (evaluated on last year's tracks), aggregated document BoW into journal BoW and trained a model using the relevance judgments as the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and discussion</head><p>Table <ref type="table" coords="4,161.79,607.92,4.98,8.74" target="#tab_1">2</ref> contains a summary of the techniques used in each run. All our runs are based on the case summaries.</p><p>Table <ref type="table" coords="4,177.36,632.21,4.98,8.74" target="#tab_2">3</ref> contains the results of the techniques used in each run. The Learning to Fuse approach on Task A lead to a slight increase in performance when compared to the 2014 selection and to a single, non-fused run. For Task B, the binary relevance on journal filtering algorithm was too aggressive and biased towards last years data. The results may improve when testing the algorithm with this year's relevance judgments. This document will be updated with additional experiments and discussion when the relevance judgments are released. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,434.70,345.82,7.89;2,134.77,445.69,345.83,7.86;2,134.77,456.65,305.25,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Query expansion example. Bold terms represent query terms that were expanded; Blue and italic terms represent MeSH expansions; Green and underlined terms represent Shingle expansions; Red terms represent SNOMed CT expansions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,187.61,345.83,168.86"><head>Table 1 .</head><label>1</label><figDesc>Runs selected by the Learning to Fuse algorithm (L2F A, L2F B) and best last years run (Best 2014)</figDesc><table coords="4,145.60,223.05,324.17,133.41"><row><cell>Best 2014</cell><cell>L2F A</cell><cell>L2F B</cell></row><row><cell cols="3">BM25+, fat, MSH, PRF BM25+, f, NoEXP, PRF BM25+, f, NoEXP, NoPRF</cell></row><row><cell cols="3">BM25L, fat, MSH, PRF BM25+, fat, MSH, PRF BM25+, f, SNO, PRF</cell></row><row><cell cols="3">TFIDF, fat, MSH, PRF BM25+, fat, NoEXP, PRF BM25+, fat, SNO, NoPRF</cell></row><row><cell>LM, fat, MSH, PRF</cell><cell>BM25L, f, MSH, PRF</cell><cell>BM25L, fat, MSH, PRF</cell></row><row><cell></cell><cell>BM25L, fat, MSH, PRF</cell><cell>TFIDF, f, MSH, PRF</cell></row><row><cell></cell><cell>TFIDF, f, MSH, PRF</cell><cell>TFIDF, f, NoEXP, PRF</cell></row><row><cell></cell><cell>TFIDF, f, NoEXP, PRF</cell><cell>TFIDF, f, SHI, PRF</cell></row><row><cell></cell><cell>TFIDF, fat, MSH, PRF</cell><cell>TFIDF, f, SNO, PRF</cell></row><row><cell></cell><cell>TFIDF, fat, SNO, PRF</cell><cell>TFIDF, fat, MSH, PRF</cell></row><row><cell></cell><cell>LM , f, SNO, PRF</cell><cell>TFIDF, fat, SNO, PRF</cell></row><row><cell></cell><cell></cell><cell>LM, f, SNO, PRF</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,115.91,345.83,151.44"><head>Table 2 .</head><label>2</label><figDesc>NovaSearch run summary. Fusion: selected fusion algorithm; QE: Query Expansion; PRF: Pseudo Relevance Feedback; JF: Journal Filtering; Diag: Diagnosis appended to the query. × marks that all the runs applied the technique. Retrieval functions in italics are a set of runs. * marks that one or more of the runs that were combined applied the technique; more information on Table1.</figDesc><table coords="5,151.93,183.73,308.42,83.62"><row><cell></cell><cell cols="6">Run id Retrieval func. Fusion QE PRF JF Diag Notes</cell></row><row><cell></cell><cell>1</cell><cell>Best 2014</cell><cell cols="3">RRF × ×</cell><cell>Fusion Baseline</cell></row><row><cell>Task A</cell><cell>2</cell><cell>BM25L</cell><cell>-</cell><cell cols="2">× ×</cell><cell>Custom Re-Ranking</cell></row><row><cell></cell><cell>3</cell><cell>L2F A</cell><cell cols="2">RRF  *</cell><cell>*</cell><cell>Learning to fuse</cell></row><row><cell></cell><cell>4</cell><cell>BM25L</cell><cell>-</cell><cell cols="2">× ×</cell><cell>× Custom Re-Ranking</cell></row><row><cell>Task B</cell><cell>5</cell><cell>L2F B</cell><cell cols="2">RRF  *</cell><cell>*</cell><cell>× Learning to fuse</cell></row><row><cell></cell><cell>6</cell><cell>L2F B</cell><cell cols="2">RRF  *</cell><cell cols="2">*  × × L2F + filtering</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,134.77,373.82,345.83,116.82"><head>Table 3 .</head><label>3</label><figDesc>TREC CDS 2015 results for NovaSearch runs. Bold values represent our best results.</figDesc><table coords="5,207.42,407.02,197.45,83.62"><row><cell></cell><cell cols="3">Run id infAP infNDCG R-prec P@10</cell></row><row><cell></cell><cell>1</cell><cell>0.0490</cell><cell>0.2242 0.1927 0.3567</cell></row><row><cell>Task A</cell><cell>2</cell><cell>0.0475</cell><cell>0.2208 0.1801 0.3467</cell></row><row><cell></cell><cell>3</cell><cell cols="2">0.0509 0.2295 0.1964 0.3567</cell></row><row><cell></cell><cell>4</cell><cell>0.0665</cell><cell>0.2964 0.2282 0.4567</cell></row><row><cell>Task B</cell><cell>5</cell><cell cols="2">0.0783 0.3207 0.2637 0.4933</cell></row><row><cell></cell><cell>6</cell><cell>0.0675</cell><cell>0.2992 0.2179 0.4900</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,113.47,7.47"><p>http://www.trec-cds.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,559.08,342.25,7.86;5,146.91,570.04,333.67,7.86;5,146.91,581.00,333.68,7.86;5,146.91,591.96,73.22,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,313.02,559.08,167.58,7.86;5,146.91,570.04,23.54,7.86">Using skos vocabularies for improving web search</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Haslhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,192.79,570.04,287.80,7.86;5,146.91,581.00,41.47,7.86">Proceedings of the 22nd international conference on World Wide Web companion</title>
		<meeting>the 22nd international conference on World Wide Web companion</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1253" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,602.46,342.24,7.86;5,146.91,613.42,333.68,7.86;5,146.91,624.38,333.68,8.12;5,146.91,635.34,281.45,8.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,327.57,602.46,153.02,7.86;5,146.91,613.42,149.58,7.86">Multimodal medical information retrieval with unsupervised rank fusion</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mourão</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0895611114000664" />
	</analytic>
	<monogr>
		<title level="j" coord="5,303.54,613.42,177.05,7.86;5,146.91,624.38,10.29,7.86">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>medical visual information analysis and retrieval</note>
</biblStruct>

<biblStruct coords="5,138.35,645.84,342.24,7.86;5,146.91,656.80,333.68,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,314.85,645.84,165.74,7.86;5,146.91,656.80,52.60,7.86">Novasearch at trec 2014 clinical decision support track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mourão</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,219.04,656.80,198.06,7.86">Proceedings of the 2014 Text Retrieval Conference</title>
		<meeting>the 2014 Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
