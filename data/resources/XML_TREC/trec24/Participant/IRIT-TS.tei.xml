<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.27,39.08,327.77,11.81">IRIT at TREC Temporal Summarization 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,110.49,77.48,53.59,9.13"><forename type="first">Rafik</forename><surname>Abbes</surname></persName>
							<email>abbes@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>Paul Sabatier University 118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,174.65,77.48,58.81,9.13"><forename type="first">Bilel</forename><surname>Moulahi</surname></persName>
							<email>moulahi@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>Paul Sabatier University 118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Science of Tunisia</orgName>
								<orgName type="institution">LIPAH</orgName>
								<address>
									<postCode>2092</postCode>
									<settlement>Tunis, Tunisa</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.36,77.48,86.88,9.13"><forename type="first">Abdelhamid</forename><surname>Chellal</surname></persName>
							<email>chellal@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>Paul Sabatier University 118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.81,77.48,100.60,9.13"><forename type="first">Karen</forename><surname>Pinel-Sauvagnat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>Paul Sabatier University 118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,458.99,77.48,36.79,9.13;1,138.04,89.44,46.25,9.13"><forename type="first">Nathalie</forename><surname>Hernandez</surname></persName>
							<email>hernandez@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>Paul Sabatier University 118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.84,89.44,90.35,9.13"><forename type="first">Mohand</forename><surname>Boughanem</surname></persName>
							<email>boughanem@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>Paul Sabatier University 118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.76,89.44,63.22,9.13"><forename type="first">Lynda</forename><surname>Tamine</surname></persName>
							<email>tamine@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>Paul Sabatier University 118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.93,89.44,74.85,9.13"><forename type="first">Sadok</forename><forename type="middle">Ben</forename><surname>Yahia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Science of Tunisia</orgName>
								<orgName type="institution">LIPAH</orgName>
								<address>
									<postCode>2092</postCode>
									<settlement>Tunis, Tunisa</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.27,39.08,327.77,11.81">IRIT at TREC Temporal Summarization 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5ED9A5CB6BB8CCCCF1069CDA0B7B3472</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the IRIT lab participation to the TREC 2015 Temporal Summarization track. The goal of the Temporal Summarization track is to develop systems that allow users to efficiently monitor information about events over time. To tackle this task, we proposed three different methods. Obtained results are presented and discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Task description</head><p>The aim of the Temporal Summarization (TS) track is to develop systems that allow users to efficiently monitor information about events. This year, the track runs three sub-tasks that require systems to iterate over a stream corpus in a chronological order and filter relevant and novel sentences to a developing event. We used the TREC-TS-2015F dataset provided by the track organizers <ref type="foot" coords="1,424.74,356.88,3.97,6.38" target="#foot_0">3</ref> . Each document is identified by a stream id that consists of two dash-separated parts: timestamp and doc id. This year, 21 topics were evaluated. Each topic represents an event characterized by a query, a period, and a type (accident, storm, bombing, earthquake, protest, conflict). For each event, a system should emit a set of timestamped sentences called updates to generate the event temporal summary. The ground truth, represented by a set of nuggets, corresponds to a set of sentences extracted from Wikipedia by the track annotators. Matching updates to nuggets was done by track assessors. Each nugget and update are matched if they refer to the same information. To evaluate systems effectiveness, track organizers define the following metrics: the Expected (Latency) Gain and the (Latency) Comprehensiveness which are similar to the traditional IR notions of Precision and Recall (respectively).</p><p>To tackle this challenge, we propose three different approaches:</p><p>-A named entity recognition based method; -A rank fusion based method; -A real time summarization system relying on novelty and redundancy based approach. This paper is organized as follows. Section 2 introduces the first method based on named entities. Section 3 describes our second method that rely on the rank aggregation approach. Section 4 presents the third method that is based on the two measures novelty and redundancy. We discuss the experimental results in Section 5. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method A : Named entities recognition based method</head><p>The method presented in this section aims at retrieving from a documents stream, sentences that are relevant to a given long-running event. This method works iteratively. For each hour h, we distinguish 3 main steps : (1) selection of relevant documents using the BM25 model, (2) selection of candidate relevant sentences, and (3) verification of the novelty of candidate sentences. Novel sentences are then added to the temporal summary denoted by T S h .</p><p>In what follows, we describe our proximity function that aims to select candidate relevant sentences as well as the novelty function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relevant sentence selection based on the proximity of the query terms</head><p>In this step, we analyze the sentences of the selected documents. For each sentence, we have to decide whether it is relevant or not to the target event. We rely on the intuition that a relevant sentence should be close to query Q of the event.</p><p>The proximity of a sentence with respect to Q can reflect its relevance. A sentence mentioning the event is more likely to be related to it. We express the proximity of sentence s j with regard to the query Q using the following equation:</p><formula xml:id="formula_0" coords="2,173.41,293.86,330.98,33.50">proximityScr(s j , Q) = 1 |Q| t∈Q dmax d=0 e -d * match(t,s j+d ,s j-d )<label>(1)</label></formula><p>|Q| is the number of terms in Q, match(t, s x , s y ) is equal to 1 if t is contained in one of the sentences s x and s y , 0 otherwise, and dmax is the maximal distance to be considered (in number of sentences). We consider only the sentences in proximity to Q by favoring those close to all of the query terms, i.e, having a proximityScr &gt; τ p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Novelty detection based on text divergence and named entity recognition</head><p>Sentences that are selected in the previous step could contain redundant relevant information (i.e. relevant information that has already been identified). To remove redundancy, we compare each candidate relevant sentence to all relevant sentences that are already in the temporal summary. Detecting novelty is not an easy task. Two sentences may have many terms in common, but report two different information, and inversely they may be divergent but contain the same information.</p><p>In our approach, we consider that a candidate relevant sentence s j is novel with regard to already issued sentences (T S h ) if its text is divergent (DIV) and/or contains New Related Entities (NER), not detected in the preceding sentences T S h . Formally s j is novel if it fulfills the following conditions:</p><formula xml:id="formula_1" coords="2,155.68,561.60,348.72,79.18">is novel(sj , T S h ) = DIV(sj , T S h ) • NER(sj , T S h ) ( 2 ) DIV (sj , T S h ) = false if ∃ s k ∈ T S h , cos(sj , s k ) &gt; τn(T S h ) right otherwise (3) NER(sj , T S h ) = right if ∃ x ∈ RE(sj , E), ∀s k ∈ T S h x / ∈ RE(s k , E) false otherwise<label>(4)</label></formula><p>-RE (s j , E) is the set of related entities recognized in the sentence s j<ref type="foot" coords="3,417.64,39.74,3.97,6.38" target="#foot_1">4</ref> . -τ n (T S h ) is a threshold for textual novelty. As the set of relevant sentences T S h grows, the redundancy risk is higher. We thus decrease τ n according to a Gaussian function :</p><formula xml:id="formula_2" coords="3,255.04,85.03,249.35,23.90">τ n (T S h ) = 1 σ √ 2π e -|T S h | 2 δ 2<label>(5)</label></formula><p>Where the σ parameter has an impact on similarity tolerance, and the δ one controls the decay rate of the threshold. |T S h | is the number of sentences in T S h . -The • symbol of equation 2 can either be an AND operator to tune the system as precision-oriented by limiting redundancy or an OR operator to prioritize the exhaustivity. In column 2, we specify the query used to retrieve documents in the first step. In runs * {1, 2, 3}A, we used the the original query as given by the track organizers. We note that we selected documents containing all query terms. In runs * {4, 5, 6}A, we expanded the original query by the month(s) during which the event occurred. For example, for the topic 27: cyclone nilam (oct, 27 2012 to nov, 2 2012), we require that the document mentions the terms october or november in addition to the query terms cyclone and nilam. We also note, that we consider only the top 10 ranked documents in each hour retrieved using the BM25 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Submitted runs</head><p>Column 3 describes the used novelty function : DIV means that we use only the text divergence to detect redundancy/novelty. We denote by DIV-AND-NER (DIV-OR-NER) the use of a combination of our two previously defined factors : the text divergence and the recognition of new related entities using an AND (OR) operator respectively. For the novelty threshold, we fixed δ to 100 and σ to 0.9.</p><p>For the proximity function, we fixed a strict threshold τ p = 0.8 which requires the presence of the majority of the query terms in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method B : Rank fusion based method</head><p>In this section, we present a method that is based on a temporal language modeling framework <ref type="bibr" coords="4,101.89,76.11,10.51,9.13" target="#b0">[1]</ref> and a rank aggregation scheme. This approach is designed to respond to the sub-task 2. Each query (event) term is considered as a query per se. The method includes two main steps. The first step consists in computing the single query-terms relevancy with respect to a topical matching criterion P (w i |d j ) and a temporal relevance model P (t|w i ). This leads to a number of ranked lists associated with each query term. In the second step, we identify the time-span of the top K highly ranked documents of each result list and we merge the ranked lists into one ranking result. We define a set of important periods for all of these documents, that are estimated as the average of the top K document timestamps returned wrt the query terms. The goal of this step is to favour documents that are published in the same time periods as a large number of relevant documents that are returned in response to all of the query-terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generating the query-terms rankings</head><p>In this step, each query-term is individually viewed as a query. The proposed model ranks documents in decreasing order of their probability of relevance based on their temporal (P (t|w i )) and topical (P (q|d)) relevance:</p><formula xml:id="formula_3" coords="4,194.71,273.57,309.69,33.26">P (d t |w i ) = P (d, t|w i ) ∝ P (d|w i )P (t|w i ) ( 6 ) ∝ P (q|w i )P (d)P (t|w i ) ∝ P (w i |d)P (t|w i )<label>( 7 )</label></formula><p>Where P (w i |d) denotes the query-term likelihood on document d, P (d) stands for the prior probability that d is relevant to any query-term. This temporal model is further used as a baseline. P (w i |d) is estimated using the Dirichlet smoothing, yielding:</p><formula xml:id="formula_4" coords="4,233.33,354.32,271.07,34.94">P (w i |d) = tf (w i , d) + µ. tf (wi,d) |D| |d| + µ<label>(8)</label></formula><p>where tf (w i , d) stands for the frequency of w i along d.</p><p>The second factor P (t|w i ) conveys the relative importance of the time point t for the query-term w i . This temporal relevance is estimated using the maximum likelihood model, which is defined as the normalized sum of the relevance scores of documents published at time t for query-term w i :</p><formula xml:id="formula_5" coords="4,257.04,449.61,247.35,31.40">P (t|w i ) = tf (w i , D t ) |D t | (9)</formula><p>where D t is the set of documents published at time t. This weighting function assumes the temporal independency of the query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results merging</head><p>The query-terms generated rankings obtained in the first step, give rise to different lists r w ∈ R wrt both topical and temporal criteria. To merge these lists, we extend an existing RRF rank fusion method <ref type="bibr" coords="4,219.08,564.76,10.50,9.13" target="#b1">[2]</ref> by injecting a temporal proximity distance that exploits the temporal term dependency. To characterize this temporal proximity, we apply the normalized variant of the so-called Gaussian kernel function. The documents scores given by the resulting model are computed as follows:</p><formula xml:id="formula_6" coords="4,202.59,620.00,301.79,27.03">score(d t ∈ D) = r∈R 1 ϵ + r(d t ) * kernel(t, t avg ) ( 1 0 )</formula><p>where r w (d t ) is the position of document d in the rank list r w and t avg is the average time of the top highly ranked documents in R. We assumed that t avg is the most important time period for a given query. This rewards documents, returned by all (or most of ) the query terms, that are published closer to time frame of the K highly ranked documents. That is, if two documents are close enough in terms of importance and time, for all the query terms, they should be highly ranked. It is worth to mention that we assume that each relevant document identified contains at least one relevant sentence. Sentence relevance is computed using the cosine similarity measure. The Gaussian density function is computed as follows:</p><formula xml:id="formula_7" coords="5,213.64,141.72,290.75,25.43">kernel(t 1 , t 2 ) = 1 √ 2πσ * exp[ -(t 1 -t 2 ) 2 2σ 2 ] (<label>1 1 )</label></formula><p>where σ refers to the variance of the density kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Submitted runs</head><p>The method presented in this section gives rise to two runs:</p><p>-FS1B : This run is intended to be a baseline. The computation of document relevance is based on the temporal language modeling formula given in Eq. 6. As previously mentioned, the sentence relevance are computed using the Cosine similarity measure. For each topic, we start by retrieving the first top 10 relevant documents from each hour directory using the temporal language model. We only consider documents for which scores wrt the topical criterion (Cf. Eq. 8) are higher than a threshold th d . The latter is set to 0.5. We also define a threshold th d , set to 0.4 to filter out non relevant sentences wrt the cosine measure.</p><p>Then, for each document, we retrieve the top 5 relevant sentences. The parameter µ of the Dirichlet model is set to 2000. -FS2B : This run is based on the rank fusion model presented in Eq. 10. We apply the same filtering steps used for the run FS1B. For each hour, we merge the first K f use results returned separately by the query terms. K f use is tuned using the TREC 2014 TS track data, and is set to 30. The rank fusion model parameters σ and ϵ are set to 300 and 10, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method C : Temporal summarization based on novelty and redundancy measurement</head><p>The main purpose of this approach is to provide a short summary with maximum coverage, minimum redundancy and low latency. These requirements are fulfilled as follows: (i) The outlined approach is a fully real-time that makes select/ignore decision as soon as the sentence become available, therefore the notification time will correspond to timestamped of the document. (ii) The decision of selecting a given sentence is based on two dimensions, the novelty and redundancy. The former aims to detect new information regarding previous seen one in the stream while the later is used to avoid pushing an information already selected which keeps the summary from being redundant. Given an event described by keywords and a stream S of sentence s i , our approach acts like a filter where only sentences which contain at least two keywords that describe a given event are considered. An incoming sentence s i with timestamps t i will be added to the summary R if and only if:</p><formula xml:id="formula_8" coords="5,213.67,610.38,290.71,45.01">⎧ ⎨ ⎩ NS(s i ) &gt;= δ 1 = AV G ∀ sj ∈S t i , tj &lt;ti [NS(s j )] RS(s i ) &gt;= δ 2 = AV G ∀ sj ∈R t i , tj &lt;ti [RS(s j )]<label>(12)</label></formula><p>Where NS(s i ) and RS(s i ) are the novelty and the redundancy scores of an incoming sentence s i . S ti and R ti are the stream and the summary at t i (publication time of sentence s i ) respectively.</p><p>Combining this two dimensions as a conjunctive condition provides complementarity between them allowing to fulfill the requirements related to novelty, shortness and low redundancy. With linear combination, a sentence with high novelty and low redundancy scores or vice-versa, will likely be added to the summary. Also, notice here that the threshold is a parametric-free value, it is evaluated according to the previous seen values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Novelty score</head><p>Novelty detection is generally based on similarity measures where the new document is compared to all previous seen documents or to summary only. Due to the rapid growth of the number of posted sentences in stream, similarity comparison do not fit well a real time summarization. To overcome this limit, we propose to use HybridTF-IDF <ref type="bibr" coords="6,418.89,207.39,10.52,9.13" target="#b2">[3]</ref> as a measure of novelty. The intuition behind this proposition is that a novel sentence is the one that contains a good mixture of new and important terms in the relevant sentences stream for an event.</p><p>A sentence with only new terms is more likely to be a spam and irrelevant to the event of interest.</p><p>Hence, the Inverse Document Frequency (IDF) at the stream level is used as a measure of term novelty <ref type="bibr" coords="6,159.27,279.12,9.96,9.13" target="#b3">[4]</ref>. To evaluate the importance of the term within stream, we adopt the formula proposed in <ref type="bibr" coords="6,156.34,291.07,10.51,9.13" target="#b2">[3]</ref> in which the entire collection of sentences is considered as one document for computing the term frequency. Notice here that in our approach only sentences that contains keyword describing the event of interest are considered. In addition, to take into account the temporal distribution of terms in the stream, the HybridTF-IDF weight is combined with decay function. It gives a high weight to new words and those did not appear in last time window. Thereby, the novelty score of the sentence s i with the timestamp t i is measured as follows:</p><formula xml:id="formula_9" coords="6,203.85,385.39,300.55,18.60">NS(si) = w j ∈s i T F (wj) × IDF (wj) × Decay(wj) (<label>1 3 )</label></formula><p>T F (wj) = #of wj InAllSenetnces #W ordInAllSentences , IDF(wj) = log2( #AllSentences #Sentences wj Occurs</p><formula xml:id="formula_10" coords="6,458.72,415.13,42.09,8.21">) (<label>1 4</label></formula><p>)</p><formula xml:id="formula_11" coords="6,208.97,436.09,295.42,27.57">decay(wj) = ∆t(w i )-N N 2 if ∆t(wj) &lt;= 2N 1 otherwise (15)</formula><p>Where ∆t(w j ) = t i wit i-1 wi represents the time since the previous occurrence of the word w j in the stream. N represents the size of the time window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Redundancy score</head><p>To assess the redundancy score between the new sentence regarding the summary, we propose to measure the divergence between the language model of incoming sentence and language model of each sentence in the summary. In our approach, we use the Kullback-Leibler (KL) divergence <ref type="bibr" coords="6,150.64,568.46,9.96,9.13" target="#b4">[5]</ref>, in which the divergence between two sentences s i , s j is evaluated as follows:</p><formula xml:id="formula_12" coords="6,214.11,588.88,290.27,26.91">KL(s i , s j ) = w k ∈si∩sj θ si (w k ) log θ si (w k ) θ sj (w k ) (16)</formula><p>where θ si is the uni-gram language model of sentence s i and θ si (w k ) is the probability of occurrence of term w k in sentence s i . The incoming sentence should have a high divergence with the most similar sentence among the summary. The latter is the one that have the minimum Kl divergence with the incoming sentence. Thereby, the redundancy score of an incoming sentence s j is defined by the minimum KL divergence regarding each sentence in the summary R ti at time t i as follows:</p><formula xml:id="formula_13" coords="7,242.09,101.15,262.29,15.45">RS(s i ) = min ∀ sj ∈R t i KL(s i , s j ) (<label>1 7 )</label></formula><p>To avoid the problem of zero probabilities, we use smoothing which combines sentence model and stream model. In our runs the impact of the use of Dirichlet and Jelinek-Mercer (JM) smoothing was investigated. They are defined by the following equations respectively:,</p><formula xml:id="formula_14" coords="7,217.28,166.60,287.11,53.72">θ s (w j ) = tf si (w j ) × µP ti S (w j ) |s| + µ (18) θ si (w j ) = λ × P si (w j ) + (1 -λ)P ti S (w j ) (<label>1 9 )</label></formula><p>Where λ and µ are the smoothing parameter. P si (w j ) and P ti S (w j ) are the probability of occurrence of term w j in sentence s i and in the stream S at time t i respectively. They are evaluated using the maximum likelihood estimation (ML) as follows:</p><formula xml:id="formula_15" coords="7,209.05,262.59,295.33,29.77">P T i (w j ) = tf si (w j ) |T i | , P S ti (w j ) = tf S t i (w j ) |S ti |<label>(20)</label></formula><p>Where tf si (w j ) and tf si (w j ) are the frequency of w j in sentence s i and stream S at time t i . Smoothing parameter λ was set to 0.9 following <ref type="bibr" coords="7,326.81,306.34,10.52,9.13" target="#b5">[6]</ref> recommendation. µ was set to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Submitted runs</head><p>In this approach, we assume that we have as input a relevant stream wherein filtering out irrelevant documents is not handled. Hence our participation using this approach is limited to Summarization Only sub-task. In submitted runs, the impact of the use of the decay function, the smoothing method and the threshold were investigated. Two different thresholds were tested. The first one, is parametric-free in which the threshold is defined as the average of the previous seen values in the last time window of size 300s. In the second one, thresholds were fixed to 0.27 and 3 for the novelty and redundancy score respectively. These values were learned according to experiments carried out on 2014 TREC TS filtered dataset. The parameters of each submitted run are shown in table <ref type="table" coords="7,336.03,447.49,3.87,9.13" target="#tab_2">2</ref>.  Concerning the method A presented in section 2, we can see that using the BM25 model in each hour as well as the proximity function with the query terms seems to be a good criteria as we obtained respectable rates of comprehensiveness (i.e., recall) (∈ [0.38, 0.59]). Expanding the query with the month (FS4A, FS5A, FS6A) degrades results especially in terms of comprehensiveness compared to runs using the Original Query (FS1A, FS2A, FS3A). The expanded query can be useful for ambiguous query like topics 36 and 37 iraq bombing, but seems to be too restrictive for non-ambiguous topics. For the novelty detection, combining the text divergence with the recognition of new related entities using the "AND" operator degrades slightly the results in terms of the measure HM. However, the precision is slightly improved resulting our best run in term of precision (FS4A). Considering the novelty using the "OR" operator can also be useful if the user prefers exhaustive updates (C=0.5899 for FS2A run).</p><p>The rank fusion method, represented by the run FS1B, gives low results comparing to the other runs. However, the latter slightly performs the temporal language modeling framework with a difference 2.67% in terms of the measure HM and 27.48% in terms of nE <ref type="bibr" coords="8,463.64,483.90,14.46,9.13">[G]</ref>. The values of precision and recall measures are low compared to the other runs, this is likely due to the topical matching model performance that fails to retrieve relevant documents. This explains the low values wrt the measure LC for the two runs FS1B and FS2B, that mainly rely on the language modeling framework to rerank the results (Cf., Eq. 8). We conjecture that using a good topical matching model could improve the results, as for the other runs. We also believe that a fine-grained analysis at the query level may reveals interesting insights about the types of queries that each method performs at.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Summarization Only Task</head><p>Table <ref type="table" coords="8,129.09,611.27,4.98,9.13" target="#tab_4">4</ref> reports the results of our participation in the sub-task 3 (Summarization Only). The best performance are obtained by the real time summarization based on the use of Hybrid-TFIDF as novelty measure and redundancy estimation using KL divergence. We observe that:</p><p>(i) the use of the decay function always improves the performance. It gives a high weight for new words in stream which improves the expected gain; (ii) the use of average as threshold outperforms the fixed threshold. In fact, using the average as threshold is more restrictive then a fixed value which leads to reduce the number of sentences pushed in summary and improves the ELG (precision). The use of fixed value as threshold brings much noise which degrades the ELG (precision). It seems that JM smoothing fits better the real time summarization of short text stream. However, it is a preliminary results and extensive experiments need to be carried out to identify which is the best smoothing method that fit well this kind of summarization task.</p><p>We re-run our filtering method presented in section 2 to tackle the Summarization Only Task. We considered only relevant documents that are detected in the top-10 results per hour. We did not use the other relevant documents although they are provided in this task. As a consequence, we missed a lot of relevant sentences. We also failed to answer some topics (32, 34 and 44). Fortunately, our method performs well in terms of precision (nE[G] ) especially when combining the text divergence and the recognition of named entities with and "AND' operator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The experiments conducted within the Pre-Filtered Summarization Task show that the entity recognition based method gives better results than the rank aggregation based approach. We believe that a more fine-grained analysis and parameters tuning may reveal a better understanding of their performance. For the Summarization Only Task, we results show that the novelty and redundancy measurements (method C) are quite promising in generating summaries. Further studies are also needed to determine whether other dimensions could be taken into account to select relevant sentences and improve the expected gain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,125.90,480.16,19.73,7.38;7,188.24,480.16,24.75,7.38;7,253.68,480.16,28.18,7.38;7,317.25,480.03,76.28,7.54;7,429.42,480.16,49.79,7.38;7,124.00,493.51,149.94,8.21;7,320.26,493.51,70.74,8.21;7,447.73,493.51,13.17,8.21;7,124.00,504.86,302.22,8.21;7,447.73,504.86,13.17,8.21;7,124.00,516.22,149.94,8.21;7,320.27,516.22,70.74,8.21;7,436.84,516.22,34.95,8.21;7,124.00,527.58,347.79,8.21;7,124.00,538.94,23.55,8.21;7,165.29,538.94,70.63,8.21;7,262.91,538.94,9.72,8.21;7,320.27,538.94,70.74,8.21;7,436.84,538.94,34.95,8.21;7,124.00,550.30,23.55,8.21;7,163.54,550.30,74.16,8.21;7,262.91,550.30,208.89,8.21;7,124.00,561.65,23.55,8.21;7,165.93,561.65,69.36,8.21;7,262.91,561.65,9.72,8.21;7,320.27,561.65,70.74,8.21;7,447.73,561.65,13.17,8.21;7,124.00,573.01,23.55,8.21;7,164.18,573.01,72.88,8.21;7,262.91,573.01,163.32,8.21;7,447.73,573.01,13.17,8.21"><head></head><label></label><figDesc>δ2 Smoothing OS1C KLTFIDF-L-FIX-decay yes δ1 = 0.27 , δ2 = 3 JM OS2C KLTFIDF-L-AVG-decay yes Average: time window of size 300 s JM OS3C KLTFIDF-D-FIX-decay yes δ1 = 0.27 , δ2 = 3 Dirichlet OS4C KLTFIDF-D-AVG-decay yes Average: time window of size 300 s Dirichlet OS5C KLTFIDF-D-FIX no δ1 = 0.27 , δ2 = 3 Dirichlet OS6C KLTFIDF-D-AVG no Average: time window of size 300 s Dirichlet OS7C KLTFIDF-L-FIX no δ1 = 0.27 , δ2 = 3 JM OS8C KLTFIDF-L-AVG no Average: time window of size 300 s JM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,101.89,212.01,328.53,149.02"><head>Table 1</head><label>1</label><figDesc>presents the different configurations evaluated for this method (A).</figDesc><table coords="3,204.08,246.20,195.07,114.82"><row><cell>Run</cell><cell>Novelty</cell><cell>Query</cell></row><row><cell>FS1A</cell><cell>Original Query</cell><cell>DIV-AND-NER</cell></row><row><cell>FS2A</cell><cell>Original Query</cell><cell>DIV-OR-NER</cell></row><row><cell>FS3A</cell><cell>Original Query</cell><cell>DIV</cell></row><row><cell cols="3">FS4A Original Query + Month DIV-AND-NER</cell></row><row><cell cols="3">FS5A Original Query + Month DIV-OR-NER</cell></row><row><cell cols="2">FS6A Original Query + Month</cell><cell>DIV</cell></row><row><cell>OS1A</cell><cell>Original Query</cell><cell>DIV-AND-NER</cell></row><row><cell>OS2A</cell><cell>Original Query</cell><cell>DIV-OR-NER</cell></row><row><cell>OS3A</cell><cell>Original Query</cell><cell>DIV</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,121.82,369.84,362.68,19.18"><head>Table 1 :</head><label>1</label><figDesc>Configuration of different runs. FS runs are submitted in the Pre-Filtered Summarization Task. OS runs are submitted in the Summarization Only Task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,121.82,590.04,362.65,19.17"><head>Table 2 :</head><label>2</label><figDesc>Configuration of different runs of real time summarization based on novelty and redundancy approach for Only summarization sub-task.5.1 Pre-Filtered Summarization TaskResults of our different configurations in the Pre-Filtered Summarization Task are shown in Table3.</figDesc><table coords="8,126.10,132.87,355.78,107.24"><row><cell>RunID</cell><cell>nE[G]</cell><cell>nE[LG]</cell><cell>C</cell><cell>LC</cell><cell>HM(nE[LG],</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lat. Comp.</cell></row><row><cell>FS3A</cell><cell>0.0852</cell><cell>0.0453</cell><cell>0.5299</cell><cell>0.3192</cell><cell>0.0754</cell></row><row><cell>FS1A</cell><cell>0.0849</cell><cell>0.0414</cell><cell>0.4959</cell><cell>0.2846</cell><cell>0.0676</cell></row><row><cell>FS6A</cell><cell>0.0851</cell><cell>0.0382</cell><cell>0.4335</cell><cell>0.2137</cell><cell>0.0625</cell></row><row><cell>FS4A</cell><cell>0.0875</cell><cell>0.0380</cell><cell>0.3853</cell><cell>0.1909</cell><cell>0.0601</cell></row><row><cell>FS2A</cell><cell>0.0518</cell><cell>0.0251</cell><cell>0.5899</cell><cell>0.3584</cell><cell>0.0449</cell></row><row><cell>FS5A</cell><cell>0.0549</cell><cell>0.0220</cell><cell>0.4774</cell><cell>0.2368</cell><cell>0.0386</cell></row><row><cell>FS1B</cell><cell>0.0422</cell><cell>0.0140</cell><cell>0.2939</cell><cell>0.1261</cell><cell>0.0224</cell></row><row><cell>FS2B</cell><cell>0.0306</cell><cell>0.0124</cell><cell>0.3391</cell><cell>0.1563</cell><cell>0.0218</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,145.86,258.89,314.57,8.21"><head>Table 3 :</head><label>3</label><figDesc>Results of our configurations in the Pre-Filtered Summarization Task</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,126.10,272.53,355.78,161.44"><head>Table 4 :</head><label>4</label><figDesc>Results of our configurations in the Summarization Only Task</figDesc><table coords="9,126.10,272.53,355.78,140.13"><row><cell>RunID</cell><cell>nE[G]</cell><cell>nE[LG]</cell><cell>C</cell><cell>LC</cell><cell>HM(nE[LG],</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lat. Comp.</cell></row><row><cell>OS2C</cell><cell>0.0595</cell><cell>0.0349</cell><cell>0.6632</cell><cell>0.4071</cell><cell>0.0619</cell></row><row><cell>OS1C</cell><cell>0.0524</cell><cell>0.0340</cell><cell>0.6433</cell><cell>0.4362</cell><cell>0.0619</cell></row><row><cell>OS7C</cell><cell>0.0523</cell><cell>0.0335</cell><cell>0.6656</cell><cell>0.4488</cell><cell>0.0614</cell></row><row><cell>OS8C</cell><cell>0.0571</cell><cell>0.0335</cell><cell>0.6642</cell><cell>0.4081</cell><cell>0.0596</cell></row><row><cell>OS4C</cell><cell>0.0536</cell><cell>0.0327</cell><cell>0.6843</cell><cell>0.4390</cell><cell>0.0591</cell></row><row><cell>OS6C</cell><cell>0.0514</cell><cell>0.0315</cell><cell>0.6779</cell><cell>0.4378</cell><cell>0.0571</cell></row><row><cell>OS3C</cell><cell>0.0434</cell><cell>0.0288</cell><cell>0.7120</cell><cell>0.5075</cell><cell>0.0538</cell></row><row><cell>OS5C</cell><cell>0.0429</cell><cell>0.0284</cell><cell>0.7327</cell><cell>0.5202</cell><cell>0.0532</cell></row><row><cell>OS3A</cell><cell>0.0820</cell><cell>0.0298</cell><cell>0.2718</cell><cell>0.0900</cell><cell>0.0422</cell></row><row><cell>OS1A</cell><cell>0.0930</cell><cell>0.0310</cell><cell>0.2570</cell><cell>0.0808</cell><cell>0.0420</cell></row><row><cell>OS2A</cell><cell>0.0720</cell><cell>0.0258</cell><cell>0.3029</cell><cell>0.0976</cell><cell>0.0381</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,111.85,635.85,222.70,8.21"><p>http://dcs.gla.ac.uk/ richardm/TREC-TS-2015F.tar.gz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,111.85,635.85,348.24,8.21"><p>We use the tool developed by the NER Stanford group (http://nlp.stanford.edu/ner/)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.47,64.94,398.92,8.21;10,114.04,75.91,350.63,8.21" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,369.11,64.94,135.29,8.21;10,114.04,75.91,26.71,8.21">Answering general time-sensitive queries</title>
		<author>
			<persName coords=""><forename type="first">Wisam</forename><surname>Dakka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Panagiotis</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,148.66,75.91,226.27,8.21">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="220" to="235" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.47,86.86,398.93,8.21;10,114.04,97.82,390.38,8.21;10,114.04,108.78,390.36,8.21;10,114.04,119.74,175.86,8.21" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,380.61,86.86,123.79,8.21;10,114.04,97.82,217.61,8.21">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,352.08,97.82,152.34,8.21;10,114.04,108.78,362.34,8.21">Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09</title>
		<meeting>the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.47,130.70,398.93,8.21;10,114.04,141.66,161.62,8.21" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,336.18,130.70,145.17,8.21">Summarization of twitter microblogs</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Beaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">I</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jugal</forename><forename type="middle">K</forename><surname>Inouye</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,488.88,130.70,15.52,8.21;10,114.04,141.66,71.93,8.21">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="378" to="402" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.47,152.61,398.94,8.21;10,114.04,163.58,360.59,8.21" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,277.43,152.61,226.98,8.21;10,114.04,163.58,237.90,8.21">Using temporal IDF for efficient novelty detection in text streams</title>
		<author>
			<persName coords=""><forename type="first">Margarita</forename><surname>Karkali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
		<idno>CoRR, abs/1401.1456</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Alexandros Ntoulas, and Michalis Vazirgiannis</note>
</biblStruct>

<biblStruct coords="10,105.47,174.54,398.95,8.21;10,114.04,185.49,127.63,8.21" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,249.81,174.54,125.98,8.21">On information and sufficiency</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,386.78,174.54,117.64,8.21;10,114.04,185.49,35.48,8.21">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1951</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.47,196.45,398.92,8.21;10,114.04,207.41,390.38,8.21;10,114.04,218.37,390.34,8.21;10,114.04,229.33,20.99,8.21" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,412.27,196.45,92.12,8.21;10,114.04,207.41,221.86,8.21">A comparison study for novelty control mechanisms applied to web news stories</title>
		<author>
			<persName coords=""><forename type="first">Arnout</forename><surname>Verheij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allard</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Flavius</forename><surname>Frasincar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frederik</forename><surname>Hogenboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,376.29,207.41,128.13,8.21;10,114.04,218.37,131.45,8.21">IEEE/WIC/ACM International Conferences on Web Intelligence</title>
		<imprint>
			<biblScope unit="page" from="431" to="436" />
			<date type="published" when="2012-12-04">2012. 2012. December 4-7, 2012. 2012</date>
			<pubPlace>WI; Macau, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
