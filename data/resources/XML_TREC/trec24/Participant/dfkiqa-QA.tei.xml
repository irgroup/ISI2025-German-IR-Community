<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,56.64,65.57,456.57,17.13;1,56.64,88.61,384.89,17.13">Question/Answer Matching for Yahoo! Answers Using a Corpus-Based Extracted Ngram-based Mapping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.18,128.92,95.40,13.62"><forename type="first">Stalin</forename><surname>Varanasi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MLT-Lab DFKI</orgName>
								<orgName type="institution">University of Saarland</orgName>
								<address>
									<postCode>D-66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.37,128.92,110.59,13.62"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MLT-Lab DFKI</orgName>
								<orgName type="institution">University of Saarland</orgName>
								<address>
									<postCode>D-66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,56.64,65.57,456.57,17.13;1,56.64,88.61,384.89,17.13">Question/Answer Matching for Yahoo! Answers Using a Corpus-Based Extracted Ngram-based Mapping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F7715040E25604CEC349060F5320CC1F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done by the QA group of the Multilingual Technologies Lab at DFKI for the 2015 edition of the TREC LiveQA track. We describe the system, issues faced and the approaches followed considering the time lines of the track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The basic functionality of a QA system is to automatically answer a given Natural Language question. The TREC LiveQA 2015 track posed a particularly new challenge from previous QA challenges (like TREC and CLEF) as the questions are taken from the Yahoo! Answers (YA) website and they are far from being factoid questions. We describe briefly how we tried to solve this problem.</p><p>A major aspect of the "Live QA" character of the challenge is that one has to deal with real user questions that are extracted from a stream of most recent questions submitted to YA, which have not yet been answered by humans. Furthermore, the allowed processing time of a YA question is limited to 60 seconds, which restricts the development and usage of time consuming complex processing strategies.</p><p>A number of recent published approaches in the area of community Question Answering (cQA) explore an existing Q-A archive (for example the YA archive) for efficient retrieval of known answers for new questions, cf. <ref type="bibr" coords="1,205.51,529.07,148.80,10.32" target="#b5">(Figueroa and Neumann, 2014)</ref>, <ref type="bibr" coords="1,362.59,529.07,86.48,10.32" target="#b9">(Shen et al., 2015)</ref>. Here, the answer sources (i.e., the answers associated with the questions) are given explicitly, and hence, the main focus lies on the mapping of new questions to known questions. We might consider this as the "question paraphrasing" problem of cQA.</p><p>Since for the LiveQA challenge a YA archive cannot directly be used as answer, a major problem is to identify possible answer sources for a question, which might contain the candidate answer. This is in particular difficult, because YA questions can be and usually are about anything from factoid questions like "Who invented the xbox 360?" 1 , to non-factoid questions "Why is Fairy Tail Episode 176 not out yet?" 2 , to very personal questions like "I hate my dad. anyone else?" 3 . Thus a major challenge is: How can we infer useful answer sources for a YA question?</p><p>In our previous work on Question Answering we have successfully explored and developed a Webbased approach (the implemented QA system was named WebQA), in which the whole Internet was exploited as answer source for answering different kinds of questions, e.g., factoid questions, listbased and definition-based questions (cf. <ref type="bibr" coords="2,257.73,92.84,141.31,10.72" target="#b2">Figueroa and Neumann, 2006</ref><ref type="bibr" coords="2,399.05,92.84,30.36,10.72" target="#b3">, 2007</ref><ref type="bibr" coords="2,429.41,92.84,34.59,10.72" target="#b4">, 2009)</ref>. The core idea was to use the N-best snippets returned by the search engine Bing<ref type="foot" coords="2,384.00,103.76,3.96,7.07" target="#foot_0">4</ref> as answer source pool, and to apply clustering and extraction methods based on Latent Semantic Analysis (LSA) combined with pattern matching to identify and extract exact answer strings.</p><p>Thus, it was a natural first step to exploit WebQA also for answering YA questions such that in a first step a question classification is performed in order to select only those questions, which are covered by WebQA and then to call WebQA to find exact answers. This approach failed for several reasons: 1.) Only a small fraction of the YA questions are basically WebQA-suitable simple factoid questions (e.g., only 22 questions out of 910 questions from the 1k-qids.txt corpus provided by the track organizers), 2.) The found answer strings (whether correct or wrong) were actually too small compared to the answers in the YA development corpus, and finally, 3.) The runtime of WebQA for list-based and definition-based questions was actually too high so that we failed the speed limit of 60 seconds. However, WebQA's pipeline that consists of a number of subcomponents (from snippet identification, sentence extraction, sentence re-ranking, phrase extraction, answer validation etc.) has a high degree of modularity so that we decided to use relevant subcomponents of WebQA for building and testing our LiveWebQA system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Overview</head><p>Following our previous work, we assume that the Internet as a whole can serve as reliable answer source for basically any question. However, in contrast to our previous work, we do not assume that the snippets returned by WebQA are sufficient for serving as answer sources but that we have to inspect the whole web documents the snippets point to. The core idea of our approach is now to automatically learn a question-answer mapping from a subset of an available YA-archive (actually we used the ydata-yanswers-all-questions-v1_0 corpus that was made available to the LiveQA participants by the organizers; see also sec. 2.2), and to apply this mapping to YA questions and the whole Web documents identified by WebQA via its snippet identification and ranking step. Thus, there are 3 main components of our system (called LiveWebQA), which dealt with the different aspects of answering a YA question:</p><p>1.) Analyzing Questions 2.) Training 3.) Search and score</p><p>The major workflow is as follows (cf. also Figure <ref type="figure" coords="2,297.95,611.48,4.44,10.72">1</ref>): 0. Train a Q-A map in form of a vector space from a training corpus 1. Send the Yahoo question to the 'Retrieve Answers' module of WebQA 2. Send the title and body to the 'Analyze Question' module 3. Send the 'key' question to the 'Retrieve Answers' module 4. Send the 'key' question to the 'From URLs' module 5. Send the web URLs to the 'Search and Score' module 6. Then, identify the best 3 sentences from the searched content 7. Pass the answer to the 'Retrieve Answers' module Finally, the found answer string is sent to the LiveQA client of the LiveQA track organizers in the required format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Architecture and main information flow of LiveWebQA</head><p>We will now describe the three main components in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Analyzing questions</head><p>To answer a YA question is in general more difficult compared to the answering of a factoid question. For example, in general, the essence of a YA question has to be mined from a text of variable length, since a YA question in general does not exist of single WH-question as, for example, has been the case in the previous TREC and CLEF QA challenges, but a paragraph that contains the main question terms more or less explicitly.</p><p>More precisely, a YA question usually consists of a title and a body. Ideally, the title contains the main question. However, often this is not the case, because the relevant question information is distributed across the title and the body. This means, one has to actually identify and extract these question parts. Automatic text summarization is a field of research on its own right, which could be exploited also here. But since the text of a YA question is usually not too large (and because of time constraints during the development phase), we employed a heuristic to just identify the important question fragments based on the following intuitions: if a sentence in the question paragraph (which consists of the question title and body) contains a pronoun (except for first person pronouns) or a question key-word (like 'who', 'what', 'how', 'where', 'why', '?'), then most likely the previous sentence (if it exists) and the sentence itself together have useful information and should not be discarded. In this way, we do not filter out sentences preceding a trivially important sentence (identified by 'help', '?'). Thus, from a new YA question Q, we construct a question Q' such that Q' consists of all the direct questions and trivially important sentences of Q and the identified important sentences based on the above criteria preceding these direct questions. Then Q' is used to search the Web using the Bing API.</p><p>For example, for the YA question Q with qid=20150713141918AA2OxXt: We compute the following reduced question Q': Here, the underlined sentences are the trivially important sentences and the lines captured preceding those are the identified important sentences. We stop to add more lines if there are no more pronouns in the last sentence that we add.</p><p>Although the YA questions to be answered in the LiveQA track 2015 are restricted to a subset of the possible set of YA categories (i.e., only questions are considered from the topics Arts &amp; Humanities, Beauty &amp; Style, Computers &amp; Internet, Health, Home &amp; Garden, Pets, Sports, Travel), we do not make use of them in the question analysis module nor in other modules of our current LiveWebQA system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>The core idea of the training phase is to learn a mapping from YA question patterns to YA answer patterns using an available YA question/answering corpus as basis. In some sense, the learned mapping would express useful indicators for deciding whether a YA question belongs to a certain question type and whether a sentence found in a Web page could serve as answer for this YA question type. Thus, the goal is to learn to guess whether a sentence embeds useful "YA answer sentence typical information" for specific embedded "YA question typical information".</p><p>Paragraph embedding is a growing field of research. The method proposed by <ref type="bibr" coords="4,435.42,623.96,103.12,10.72" target="#b6">Mikolov et al. (2013)</ref> has proven to get state-of-art results for text similarity tasks. However, the task of QA is usually harder than simple text similarity as it involves the association of a question and answer pair. We had a corpus of questions and answers from YA that was provided to all participants by the organizers of the LiveQA challenge. The corpus (called ydata-yanswers-all-questions-v1_0) contains 4,483,032 questions and their answers from which we used only a small subset of 42K QA pairs (mainly because of development time constraints). In addition of question and answer text, the corpus contains a small amount of meta data, i.e., which answer was selected as the best answer, and the category and sub-category that was assigned to this question. For more information about form and content, cf. <ref type="bibr" coords="5,159.94,65.24,107.47,10.72" target="#b8">(Surdeanu et al., 2008)</ref>. We suspected obtaining question vectors and answer vectors from this corpus during the training phase and associating them may lead to over-fitting using paragraph vectors. Hence, we choose to follow a simpler method to get vectors. Our intuition is that the questions have patterns that can be used to identify the question type and answers associated also have patterns, which are specific to a particular question type. To obtain these generic patterns of questions and answers, we removed the key content words from the questions and answers. We represented a question and an answer in terms of the skipped (by removing rare words) tri-grams that occurred in them. Hence, each question and each answer is a vector whose dimension is equal to the total number of tri-grams that occurred in the corpus. The vectors are then weighted by computing a tfidf-like weight for each cell in the following way:</p><p>Value in each dimension = tri-gram_term_frequency * inverse_document_frequency, where tri-gram_term_frequency is the number of times the tri-gram occurred in the relevant category (question or answer), and inverse_document_frequency is the log of the total number of question answer pairs divided by the number of times tri-gram occurred in the relevant category of question or answer.</p><p>We ignored the tri-grams that contained just the function words by putting a frequency threshold, as the function word tri-grams don't give any information specific to the question. For a questionanswer corpus, we aimed to train two separate vector spaces one for the questions and one for the answers. These two vector spaces are mapped to each other by the co-occurrences of the question and answer pairs in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Search and Score:</head><p>Given a new question, we find the relevant vector in our question-vector space and retrieve the knearest neighbors that we have seen in the training corpus, and from this, we determine the associated answer vectors in the answer-vector space. We then predict the best answer-vector as the average of the sum of answer-vectors associated with these k-nearest neighbors. We took the value of k as 50 for a corpus of the 42k Yahoo question and answer pairs.</p><p>After having selected the answer vector for the current question, we look up the web pages given by the N-best snippets found by WebQA. For each web page, we identify each sentence s and score it according to the following two measures:</p><p>i.) pattern_quality_measure(s): Compute the cosine similarity between the sentence and predicted answer vector. ii.) relevance_measure(s): Since we are looking through the whole web page, we need a second measure to pick only sentences that are relevant to the question. For this reason, we use the word2vec tool (cf. <ref type="bibr" coords="5,224.94,633.32,105.65,10.72" target="#b6">Mikolov et al., 2013)</ref> to compute the cosine similarity between the question and selected sentence. The motivation behind using word embedded vectors here is to weaken the surface-based restriction a direct comparison of the tri-grams between the question and a sentence would have.</p><p>By combining both measures, we compute for each sentence s a score as follows: if the relevance_measure(s) &lt; 0.8 then score(s) = pattern_quality_measure(s) * relevance_measure(s) else score(s) = 0.8 * pattern_quality_measure(s) * relevance_measure (s)</p><p>The motivation behind cutting-off the score for sentences with a relevance measure &gt;= 0.8 is that often these sentences would be questions similar to the asked question itself.</p><p>We then retrieve the three consecutive sentences from the web pages, which have the highest sum of sentence scores. Such a three-sentence long paragraph is then considered as answer for the current YA question. In order to improve speed, we searched in each web page separately by creating a thread per web page, and send the best answer retrieved so far. Unfortunately, not all web pages were processed within the time limit of 60 seconds because of their size, and hence, the overall quality could be negatively affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We tried with different parameters for thresholds to remove the content words in the training process for patterns. We finally set 300 as frequency threshold for the corpus of the 20 million tokens of our 42K training corpus. We tried using a much larger corpus, but the system didn't perform significantly better when we increased the size.</p><p>To get an idea of the potential quality of the results of our approach, we manually verified the answers provided for a test set of the 1000 questions derived from the test corpus 1k-qids.txt during the development of our LiveWebQA system. We found that 20% of the times the system gave a useful answer. We considered a predicted answer as useful if parts of it can serve as "answer" to the asked question.</p><p>We also automatically evaluated the results by comparing the answers predicted by our LiveWebQA system with the answers in the test Q&amp;A corpus using the word2vec (bag of words) cosine similarity. We got 45% of the questions answered with greater than 0.7 cosine similarity measure. Note that this does not automatically mean, that a 0.7 similarity also means that the predicted answer has high accuracy, but only gives an indication of its relatedness on basis of the selected word embedding. The main motivations for using word2vec for our automatic evaluation were twofold: 1) Verifying whether two texts convey the same meaning is a sub-problem to Question-Answering itself. Thus, text similarity between two answers will give us a measure on the quality of the answer even though it doesn't tell us that both have same meaning. 2) We choose word2vec bag-of-models to compare the texts, as it is was easy to integrate also considering the time constraints we had.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>During the official run, overall, 1087 questions were judged by the TREC organizers and scored using 4-level scale: 4: Excellent -a significant amount of useful information, fully answers the question 3: Good -partially answers the question 2: Fair -marginally useful information 1: Bad -contains no useful information for the question -2: the answer is unreadable (only 15 answers from all runs were judged as unreadable)</p><p>Since we had some delay in starting our server, actually only 1058 questions were processed by our system. 893 of them are scored as 1 (bad); 112 as 2 (fair); 42 as 3 (good); 11 as 4 (excellent).</p><p>The score of our system on the official run is given below: avg score (0-3) succ@1+ succ@2+ succ@3+ succ@4+ prec@2+ prec@3+ prec@4+ 0.211 0.973 0.152 0.049 0.010 0.156 0.050 0.010</p><p>If we compare these results with our expectations (about 20% useful answers), we are actually far behind them, which might indicate that our manual analysis was not sufficient or not in line with the official scale.</p><p>Below is the official average of scores of all runs:</p><p>avg score (0-3) succ@1+ succ@2+ succ@3+ succ@4+ prec@2+ prec@3+ prec@4+ 0.465 0.925 0.262 0.146 0.060 0.284 0,159 0.065 Compared to our results, it means that in average the majority of questions are badly answered, but that the average performance of answering questions as good or excellent is higher compared to our result. It is clear that the overall performance in terms of precision of our system is still poor. However, we are now in a position to dive into the problems of our system using the evaluation results from this initial LiveQA track challenge, so that we are convinced to get a much better result next time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,56.64,133.97,481.86,98.20"><head></head><label></label><figDesc>GHz Wireless Nano mouse for about a year and have had no problems up until very recently. The right mouse button is way too sensitive. When I open it up the button on the left still has its click but just resting a finger on the right button makes it rigt click. Is there a way to fix this or should I just buy a new mouse? If I should buy a new one, are there any that you would recommend for gaming?</figDesc><table coords="4,56.64,133.97,206.16,35.08"><row><cell>title: GearHead mouse button too sensitive?</cell></row><row><cell>body: I've been using a GearHead Optical 2.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,63.64,724.28,229.89,10.72"><p>http://datamarket.azure.com/dataset/bing/search</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>https://answers.yahoo.com/question/index?qid=20090211135728AArYykb 2 https://answers.yahoo.com/question/index?qid=20130405112159AAMsPyV 3 https://answers.yahoo.com/question/index?qid=20080602133015AAvvyBf</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="7,56.64,454.52,459.31,10.72;7,56.64,468.44,392.60,10.72" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,339.91,454.52,176.04,10.72;7,56.64,468.44,94.99,10.72">DFKI at QA@CLEF 2008. Working Notes for the CLEF</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Spurk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-09-19">2008 Workshop, 17-19 September. 2008</date>
			<pubPlace>Aarhus, Denmark</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,56.64,496.04,475.53,10.72;7,56.64,509.72,201.62,10.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,309.92,496.04,217.55,10.72">Document embedding with paragraph vectors</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,69.64,509.72,153.63,10.72">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,56.64,537.32,481.19,10.72;7,56.64,551.24,147.66,10.72" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,262.92,537.32,274.91,10.72;7,56.64,551.24,39.34,10.72">Language Independent Answer Prediction from the Web. FinTAL</title>
		<author>
			<persName coords=""><forename type="first">Alejandro</forename><surname>Figueroa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="423" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,56.64,578.84,474.93,10.72;7,56.64,592.52,93.32,10.72" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,262.92,578.84,228.55,10.72">Mining Web Snippets to Answer List Questions</title>
		<author>
			<persName coords=""><forename type="first">Alejandro</forename><surname>Figueroa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,499.57,578.84,32.00,10.72">AIDM</title>
		<imprint>
			<biblScope unit="page" from="61" to="71" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,56.64,620.12,468.25,10.72;7,56.64,634.04,316.93,10.72" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,321.27,620.12,203.62,10.72;7,56.64,634.04,134.00,10.72">Searching for Definitional Answers on the Web Using Surface Patterns</title>
		<author>
			<persName coords=""><forename type="first">Alejandro</forename><surname>Figueroa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Atkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,198.27,634.04,76.98,10.72">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="68" to="76" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,56.64,661.40,430.87,10.72;7,56.64,675.32,451.60,10.72" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,262.92,661.40,224.60,10.72;7,56.64,675.32,226.05,10.72">Category-specific models for ranking effective paraphrases in community Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Alejandro</forename><surname>Figueroa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,291.26,675.32,88.66,10.72">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4730" to="4742" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,56.64,700.52,433.91,10.72;7,56.64,714.44,297.94,10.72" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,354.90,700.52,135.65,10.72;7,56.64,714.44,147.60,10.72">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,56.64,65.24,465.90,10.72" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mikolov</surname></persName>
		</author>
		<title level="m" coord="8,219.61,65.24,264.42,10.72">Distributed representations of sentences and documents</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,56.64,106.52,480.54,10.72;8,56.64,120.44,458.61,10.72;8,56.64,134.12,123.31,10.72" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,362.23,106.52,174.95,10.72;8,56.64,120.44,108.85,10.72">Learning to Rank Answers on Large Online QA Collections</title>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,176.29,120.44,338.96,10.72;8,56.64,134.12,88.32,10.72">Proc. of the 46th Annual Meeting of the Association of Computational Linguistics (ACL)</title>
		<meeting>of the 46th Annual Meeting of the Association of Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,56.64,161.72,441.27,10.72;8,56.64,175.64,468.93,10.72;8,56.64,189.32,52.99,10.72" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,413.92,161.72,83.99,10.72;8,56.64,175.64,375.57,10.72">Question/Answer Matching for CQA System via Combining Lexical and Sequential Information</title>
		<author>
			<persName coords=""><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenge</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuanxin</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,440.24,175.64,30.00,10.72">AAAI</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="275" to="281" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
