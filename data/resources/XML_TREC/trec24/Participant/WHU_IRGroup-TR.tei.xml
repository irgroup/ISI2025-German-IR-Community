<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.03,17.90,317.66,22.52">WHU at TREC Total Recall Track 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.55,64.13,56.12,14.80"><forename type="first">Chuan</forename><surname>Wu</surname></persName>
							<email>wu.chuan@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management Wuhan University Wuhan</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.43,64.13,36.85,14.80"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>weilu@whu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information Management Wuhan University Wuhan</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.47,64.13,71.28,14.80"><forename type="first">Ruixue</forename><surname>Wang</surname></persName>
							<email>ruixue_wang@whu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Information Management Wuhan University Wuhan</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.03,17.90,317.66,22.52">WHU at TREC Total Recall Track 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08020A6A53F561913A4C9665762D1608</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the WHU IRLAB participation to the Total Recall Track in TREC 2015. We implement an end-to-end system to deal with the total recall task. We propose an iterative query expansion method, which construct queries using iteratively selected terms. We choose to participate the "Play-at-home" evaluation. Results are presented and discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Text Retrieval Conference (TREC) this year introduces a new track called Total Recall Track. Given a set of topics as queries, and a collection of documents, participants are required to find as many relevant documents as possible with few effort. High recall is the primary concern. However, since it does not make sense if the cost of information seeking itself is too much, another goal is to limit effort paid into the information seeking behavior.</p><p>In Total Recall Track, the relevance of documents with respect to topics are stored in remote server. All documents identified as relevant are submitted to server for relevance judgments, and the number of times issuing a query against the index is regarded as an effort.</p><p>Two kinds of evaluation are provided, i.e. "Play-at-home" evaluation and "Sandbox" evaluation. For "Play-at-home", participants ran their system on their own with the choice of "automatic" and "manual", indicating whether manual intervention is included. For "Sandbox" evaluation, a virtual machine with a fully automated solution is submitted.</p><p>Based on our limitation, we focus on "Play-at-home" evaluation. We propose an iterative query expansion approach to find relevant documents. In order to achieve this goal, we need to first retrieve relevant documents, and then find subtopics about the given information need. Basically, this is an iterative process as we can always find something new in retrieved relevant documents until all relevant documents are found.</p><p>The rest of this paper is organized as follows. Section 2 gives a brief introduction about the Total Recall task. Section 3 presents the whole framework of our Total Recall System. Section 3 describes how we use query expansion techniques to resolve Total Recall. Section 4 describes details about our experimentation. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>In this section we present the framework used to resolve total recall task. We describe all necessary preprocessing steps, followed by our iterative query expansion method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>As required by Total Recall Track organizers, automatic experiments must use software that, without human intervention, downloads the dataset and conducts the task end to end. Our preprocessing process includes three steps: Downloading Corpus, Corpus Preprocessing, Index Construction.</p><p>First of all, we get a runid from total recall server, and then information of the corpus corresponding to the runid can be obtained, including the url of the corpus. Then the corpus is downloaded and uncompressed. Second, we perform some preprocessing on the corpus if necessary. After analyzing some corpus file, we found that some xml file cannot be directly handled by XML parsing tools. Therefore, we adjust the xml files to make the files well-formed. Finally, we construct the index of the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Expansion based approach</head><p>Given a topic and a collection of documents, our goal is to iteratively find distinctive terms from retrieved documents and expand the original query using these terms. Our approach consists of two iterative steps, i.e. document retrieval, and distinctive term selection. By first retrieving documents using the original query or expanded query and obtaining relevance judgments from server, we find relevant documents. Then we identify distinctive terms from relevant documents to capture various aspects of the query. The whole process is illustrated as follows:</p><p>1) Set query Q00 , the first query in the first iteration as the original query Q.</p><p>2) In the i-th iteration, we have m queries to perform query expansion.</p><p>3) For the j-th query in the i-th iteration, search the given corpus using Qij to get top N retrieval result set.</p><p>4) Obtain relevance judgments from Total Recall Track Server. All relevant documents are grouped into the Relevant Result Set (RRS), which is denoted as RRSij.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) If</head><p>RRSi is empty, then this branch ends, otherwise extract all distinctive terms from RRSij and denote these terms as Distinctive Term Set (DTS), i.e. DT Sij;</p><p>6) If DT Sij is empty, then this branch ends, otherwise construct |DT Sij| queries using DT Sij by query formula Q + wi. Go to step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Document Retrieval</head><p>In document retrieval step, we first search the given corpus using the original query or an expanded query and obtain a ranked list of documents, denoted as Result Set (RS). Instead of submitting all retrieved documents to the server for relevance judgments, we submit top N documents. The reason is that the top results are assumed to be more likely to be relevant. If the number of retrieved document is less than N, then we submit all retrieved documents.</p><p>After obtaining the relevance judgments from the server, we have two sets of documents, i.e. relevant document set (RDS) and irrelevant document set. We pass RDS to the next step to extract distinctive terms for the next round of query expansion if RDS is not empty. If RDS is empty, which means that no relevant documents is found for the given query, move the next query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Distinctive Term Selection</head><p>Since how a document is relevant with a query is encoded in distinctive terms in the document, we propose to identify distinctive terms from relevant documents and perform query expansion using these terms. We implement the Inverse Local Context Analysis method with some modifications.</p><p>Given a query Q and a relevant document set S, identify distinctive terms from S. The steps are as follows:</p><p>1) Iterate over all terms in all documents, for each term ti, 2) Rank terms within RRSi using f (c, QS) (see Formula 1) in ascent order.</p><formula xml:id="formula_0" coords="2,87.07,298.09,184.36,56.67">f = (c, Q) = Y w j 2Q + co_degree(c, wj)) idf (w j ) co_degree(c, wj) = |D 2 RRSi|c 2 D, wj 2 D|</formula><p>3) Select the top k terms in the ranked term list.</p><p>Ranking terms in ascent order means that the lower the relatedness of a term with query terms, the higher the term will be ranked. Then we select top K terms as candidate expansion terms. For each expansion term, we go to step 1 for the next round of query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>The document collection, information need and relevance assessor are all supplied to participants via an on-line server. For the "Play-at-home" evaluation, three tests are provided, i.e. athome1, athome2, and athome3. For each test, a specific corpus and corresponding topics are provided. We apply our method on athome1. The details about the datasets in each test are shown in Table <ref type="table" coords="2,273.99,534.80,3.36,8.59" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation metrics</head><p>As stated by TREC Total Recall Track organizers, both "completeness" and "effort" are reflected in evaluation metrics. Completeness means how nearly all of the relevant documents are found, while effort is a function of the number of documents submitted to the assessment server. In order to evaluate completeness and effort in total recall, Two kinds of evaluation metrics are given, i.e. Rank measures and Set measures. Rank measures reflect completeness for various effort values, while Set measures reflect completeness at a fixed level of effort. Details about the evaluation metrics can be found in Total Recall website. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>On the athome1 dataset, the recall is quite limited compared to the provided baseline. Given that our effort is quite limited, our iterative process ends earlier than expected. The reason could be that we didn't find appropriate combination distinctive terms for query expansion. Simply by adding one distinctive term to perform query expansion is not enough to find all relevant documents. The high precision is reasonable since no much documents are retrieved. It is not very difficult to identify relevant documents in the first rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We participated in the newly introduced TREC 2015 Total Recall Track. One run was submitted for "Play-at-home" evaluation. We proposed an iterative query expansion approach to improve total recall. The results show that the performance of our simple query expansion approach is not as good as the provided baseline. It might be important to find appropriate combination of terms for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGMENTS</head><p>This work is supported by the National Social Science Fund of China (Grant No. 12&amp;ZD1221) and the National Natural Science Foundation of China (Grant No. 71173164).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,334.75,5.66,203.22,106.61"><head>Table 1 :</head><label>1</label><figDesc>Precision, recall of our Method</figDesc><table coords="2,334.75,5.66,203.22,85.30"><row><cell>#Rel.</cell><cell cols="3">Max. recall Effort Precision</cell></row><row><cell>athome101 5836</cell><cell>0.0019</cell><cell>13</cell><cell>0.8462</cell></row><row><cell>athome102 1624</cell><cell>0.0062</cell><cell>11</cell><cell>0.9091</cell></row><row><cell>athome103 5725</cell><cell>0.0690</cell><cell>395</cell><cell>1.0000</cell></row><row><cell>athome105 3635</cell><cell>0.0561</cell><cell>204</cell><cell>1.0000</cell></row><row><cell cols="2">athome106 17135 0.0770</cell><cell>1319</cell><cell>1.0000</cell></row><row><cell>athome107 2375</cell><cell>0.0497</cell><cell>118</cell><cell>1.0000</cell></row><row><cell>athome108 2375</cell><cell>0.1158</cell><cell>276</cell><cell>0.9964</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
