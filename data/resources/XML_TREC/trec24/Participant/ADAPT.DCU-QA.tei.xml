<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.06,68.75,419.16,16.49;1,164.11,90.67,267.06,16.49">ADAPT.DCU at TREC LiveQA: A Sentence Retrieval based Approach to Live Question Answering</title>
				<funder ref="#_dPXJwWj">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
				<funder ref="#_UdsEvwQ">
					<orgName type="full">European Regional Development Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,101.32,121.16,88.77,15.55"><forename type="first">Dasha</forename><surname>Bogdanova</surname></persName>
							<email>dbogdanova@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">ADAPT centre</orgName>
								<address>
									<settlement>Dublin City University Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,199.06,121.16,82.59,15.55"><forename type="first">Debasis</forename><surname>Ganguly</surname></persName>
							<email>dganguly@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">ADAPT centre</orgName>
								<address>
									<settlement>Dublin City University Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.41,121.16,74.56,15.55"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
							<email>jfoster@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">ADAPT centre</orgName>
								<address>
									<settlement>Dublin City University Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.83,121.16,121.13,15.55"><forename type="first">Ali</forename><forename type="middle">Hosseinzadeh</forename><surname>Vahid</surname></persName>
							<email>avahid@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">ADAPT centre</orgName>
								<address>
									<settlement>Dublin City University Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.06,68.75,419.16,16.49;1,164.11,90.67,267.06,16.49">ADAPT.DCU at TREC LiveQA: A Sentence Retrieval based Approach to Live Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">72F03F0EFB0AEE50B0A6FCDB81FA95F8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the work done by ADAPT centre at Dublin City University towards automatically answering questions for the TREC LiveQA track. The system is based on a sentence retrieval approach. In particular, we first use the title of a new question as a query so as to retrieve a ranked list of conceptually similar questions from an index of previously asked on "Yahoo! Answers". We then extract the best matching sentences from the answers of the retrieved questions. In order to construct the final answer, we combine these sentences with the best answer of the top ranked (most similar to the query) question. When no pre-existing questions with sufficient similarity with the new one can be retrieved from the index, we output an answer from a candidate set of pre-generated answers based on the domain of the question.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of automated Question Answering (QA) has been frequently addressed previously, including the TREC competitions of 1999-2004. However, most existing work focused only on factoid questions, that usually require a named or a numerical entity as an answer. The research on answering non-factoid questions, such as manner or reason questions (e.g. a factoid question Who is the prime minister of Ireland? versus a non-factoid How is the prime minister of Ireland elected?), is rather piecemeal.</p><p>Several attempts towards non-factoid question answering were made. For example, <ref type="bibr" coords="1,465.19,475.63,54.48,10.45;1,75.60,489.18,82.53,10.45" target="#b2">Higashinaka and Isozaki (2008)</ref> present a learning-to-rank approach to answer Japanese why questions. The work in <ref type="bibr" coords="1,87.26,502.73,98.28,10.45" target="#b6">Surdeanu et al. (2011)</ref> address the problem of ranking answers to non-factoid how questions from Yahoo! Answers. The authors use a wide variety of features including translational, similarity and web correlation features. Several other studies focus on the task of answer reranking for non-factoid how and why questions, including <ref type="bibr" coords="1,206.79,543.38,88.22,10.45" target="#b3">(Jansen et al., 2014;</ref><ref type="bibr" coords="1,298.16,543.38,80.95,10.45" target="#b5">Sharp et al., 2015;</ref><ref type="bibr" coords="1,382.26,543.38,77.17,10.45" target="#b1">Fried et al., 2015)</ref>. However in neither of the mentioned studies the questions were coming live from real users.</p><p>The TREC 2015 LiveQA track, unlike previous QA tracks, involves answering real questions from Yahoo! Answers in real time. each participant needed to submit a web service application that on receiving a question responds with an answer of no more than 1000 characters. The answer had to be provided within 60 seconds. The questions, being sampled from a stream of real Yahoo! Answers questions, were much more diverse than in past QA tracks. In fact, the questions included not only factoid but also manner, opinion, advice and many other types of questions. All questions submitted to the systems had a title, a body (if any), and a user-reported category from the following list: Arts &amp; Humanities, <ref type="bibr" coords="1,163.55,665.32,356.13,10.45;1,75.60,678.95,30.45,10.24">Beauty &amp; Style, Computers &amp; Internet, Health, Home &amp; Garden, Pets, Sports and Travel.</ref> This paper describes our participation in the TREC 2015 LiveQA track. We undertake a sentence retrieval approach over an indexed collection of 4.48M existing Yahoo! Answers questions 1 crawled in 2007. In particular, we use the title of a new question as the query to retrieve a ranked list of conceptually similar questions previously asked on the forum and then extract the best matching sentences from the answers. When no pre-existing questions with a sufficient degree of overlap with the new one can be retrieved from the index, we output an answer from a candidate set of pre-generated answers based on the domain of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we describe our approach to live question answering in detail. We start by describing the data that is used to construct the archived index of previously asked questions on Yahoo! Answers and then follow it up with a description of how the index is used to retrieve similar questions and extract answer snippets from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Index Construction</head><p>To build our index, we use the L6 dataset<ref type="foot" coords="2,261.26,246.48,3.99,7.64" target="#foot_0">2</ref> of Yahoo! Answers. This data set contains about 4,48M questions along with their answers. We use Lucene,<ref type="foot" coords="2,302.14,260.03,3.99,7.64" target="#foot_1">3</ref> an open-source information retrieval system implemented in Java, to build up the index. We represent each document as a set of individual fields, each field comprising a set of terms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval</head><p>Given a new question, we use the title<ref type="foot" coords="2,246.67,557.06,3.99,7.64" target="#foot_2">4</ref> of the new question as the query to retrieve a ranked list of similar questions from the index. While retrieving from the index, we make use of the Field-based Language Modeling (FLM) with Jelinek Mercer similarity <ref type="bibr" coords="2,344.78,586.12,116.07,10.45" target="#b7">(Zhai and Lafferty, 2001)</ref> as shown in Equation 1. In Equation <ref type="formula" coords="2,182.75,599.67,4.09,10.45" target="#formula_1">1</ref>, i is the weight assigned to the i th field, i = 1, . . . , F , F being the number of fields, P (t|C) is the maximum likelihood estimate of sampling the term t from the collection, and</p><formula xml:id="formula_0" coords="2,75.60,626.89,38.26,10.89">P (t|f i , d</formula><p>) is the probability of sampling the term from the field f i of document d.</p><p>1 L6 dataset suggested as the training set by the task organizers and available on request http://webscope. sandbox.yahoo.com/catalog.php?datatype=l</p><formula xml:id="formula_1" coords="3,188.16,38.89,331.52,34.02">P (d|q) = Y t2q (1 F X i=1 i )P (t|C) F X i=1 i P (t|f i , d)<label>(1)</label></formula><p>The fields that we use in particular for the retrieval are the "MainCategory", "Title" and the "Body" fields, setting equal weights of 0.2<ref type="foot" coords="3,226.80,92.25,3.99,7.64" target="#foot_3">5</ref> for each. For constructing queries, we use the "Title" field of the new question only, because after some initial experiments we noticed that including terms from the "Body" field of the new question is often prone to introducing query drift.</p><p>After obtaining the ranked list of questions from the set of indexed questions, we explore three possible strategies for formulating an answer to the new question by using the information extracted from these similar questions.</p><p>1. Using the best answer of the most similar question as the answer to the new question.</p><p>2. Extracting sentences with highest similarity with the query, i.e. the title of the new question, and then concatenating them together.</p><p>3. A combination of the two approaches, where the final answer contains the first two sentences of the output of the first approach followed by the output of the second approach.<ref type="foot" coords="3,444.97,254.59,3.99,7.64" target="#foot_4">6</ref> </p><p>In order to obtain sentences from the answers that are most similar to the query, we build an inmemory index of the sentences extracted from the "BestAnswer" and the "AllAnswers" fields from the top 10 retrieved set of documents. For sentence splitting, we use the Stanford NLP toolkit. 7 The retrieved ranked sentences are then included in the generated answer in decreasing order of their similarity with the query. Too short sentences, i.e. the ones less than 10 characters, are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-generated Responses</head><p>We observed that many questions were looking for an advice or approval rather than information. For instance, questions such as Am I pretty?, is it okay to wear leggings to work?, etc. are advice seeking in nature lacking definite and precise answers. Thus, when no pre-existing questions with a sufficient similarity with the new one can be retrieved from the index, we output an answer from a candidate set of pre-generated answers based on the domain of the question. These canned answers are not informative but rather comforting. For example, the pre-generated response for the Beauty &amp; Style category was Don't worry about this! You are beautiful! To estimate the similarity between the retrieved question and the query, which we use an approximation for the reliability of the retrieved answer, we calculate the following value: rel(query, top_q) = max(nsimqq(query, top_q), cos(query, top_a))</p><p>(2)</p><p>where top_q and top_a are the top retrieved question and its best answer respectively; cos is the cosine similarity; and nsimqq is the normalized BM25 similarity, i.e.</p><p>nsimqq(query, top_q) = BM 25(query, top_q) BM 25(query, query)</p><p>(3)</p><p>If the certainty value rel(query, top_a) (see Equation <ref type="formula" coords="3,335.23,615.10,4.54,10.45">2</ref>) is lower than a predefined threshold th (which after initial experiments was set to 0.2), we first check if the asked question follows the yes/no pattern. Our approach of checking whether a question is objective type is simple and computationally effective. More specifically, we check if a question starts with do/does/are/am/is etc. In this case we simply reply yes or no. If the question does not follow this pattern, we opt for returning a pre-generated response associated with the top-level category of the question. The list of pre-generated responses was prepared manually, for each category there are 1-3 canned responses. In case there are two or more canned responses for the category, the answer is chosen randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>For the purpose of developing our system, we used the dataset of 1000 questions provided by the task organizers as a semi-official development set. This dataset was crawled from Yahoo! Answers in 2013. It contains questions from the predefined eight categories (the number of questions per category varies from 26 (Home &amp; Garden) to 296 (Health)).</p><p>For internally testing our approach, we select 60 questions from this dataset, trying to make sure that the subset includes questions of different types and different levels of detail. We have manually evaluated the three approaches described in Section 2.2, i.e. (1) extracting the best answer of the most similar question; (2) extracting sentences with highest similarity with the query; and (3) a combination of the two approaches. At the moment of submission, the official evaluation guidelines were not completely clear, so we roughly followed the scheme described in Table <ref type="table" coords="4,392.74,251.83,4.09,10.45" target="#tab_1">2</ref>.</p><p>The first approach relies purely on the user-provided best answers, which unfortunately are not always reliable, even if the retrieved question is semantically equivalent to the query (for example, the best answer to the question I want a phrase 'welcome, sit, goodbye' in all the Indian languages such as Gujarati, Bengali, Assamese, Punjabi...<ref type="foot" coords="4,260.78,304.06,3.99,7.64" target="#foot_6">8</ref> is good luck).</p><p>The obvious drawback of combining answers from different sources and/or combining the outputs of several systems is the possible lack of coherence. However, our observation was that answers generated in such way were more likely to contain useful information. We have selected for the final submission the combined system with the highest score of 2.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>During the competition, the systems stayed live for 24 hours and received 1340 questions. Later, some questions were removed, and the evaluation was done on a subset of 1087 questions. The runs were evaluated by NIST assessors, each answer was assigned a score from 1 (bad or unreadable) to 4 (perfect). The following evaluation metrics were calculated:</p><p>• avgScore(0-3): the average score over all questions transferring the scale to (0 3).</p><p>• succ@i+: the number of questions with score i or above (i 2 2..4) divided by the total number of questions.</p><p>• prec@i+: the number of questions with score i or above (i 2 2..4) divided by number of questions answered by the system.</p><p>The runs were ranked according to the avgScore value. The evaluation metrics for our system are reported in Table <ref type="table" coords="4,154.06,591.53,4.09,10.45" target="#tab_3">3</ref>. The avgScore of our system is slightly below the average score computed over all runs submitted to the track. Our system was ranked 11 out of 21 participating systems. The system answered all the questions, so the succ@1+ has the maximum value. The percentages of fair answers succ@2+ and prec@2+ (which are the same in case of our system since it provided an answer to all questions) are higher than average (0.290 versus 0.262), while the percentages of answers with higher scoressucc@3+, succ@4+ and prec@3+, prec@4+ -are below average. One possible explanation for that is that many answers provided by our system did not get scores better than fair due to the lack of coherence, discussed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score Meaning</head><p>Example 5 Discussion and Future Work</p><p>As reported in Section 4, our automatically generated answer consists of a combination of the best answer of the top retrieved question and most similar sentences from the answer of several similar questions. Combining the output of several systems sometimes results in the final answer not being coherent. To overcome this, the strategy of the CLIP team <ref type="bibr" coords="5,332.25,544.55,118.41,10.45" target="#b0">(Bagdouri and Oard, 2015)</ref> can be applied: the decision on the system choice (title versus body; only the best answer versus all answers) are made for each question using a supervised classifier. <ref type="bibr" coords="5,283.24,571.65,116.15,10.45" target="#b0">Bagdouri and Oard (2015)</ref> obtain the training data for the classifier via crowd-sourcing. The official evaluation assigned the lowest possible value to some of the answers containing arguably relevant information, since the guidelines were looking for usefulness rather than relevance System avgscore succ@1+ succ@2+ succ@3+ succ@4+ prec@2+ prec@3+ prec@4+ ADAPT.DCU 0.  of the answers. Table <ref type="table" coords="6,174.89,248.06,5.45,10.45" target="#tab_4">4</ref> shows some examples of such questions and answers. One drawback of the official evaluation guidelines is that it did not take into account the fact that for some questions it may be difficult to define what kind of answer could be considered as useful. For example, all the answers to the following question: Why does my cat have 2 eyes? received the lowest possible score. Another drawback of our system is that it relies only on the dataset described in Section 2.1. This dataset was created in 2007, and obviously does not contain answers to questions related to topics ahead of its time, e.g. Windows 10. Several other systems also used the Yahoo! Answers as the main resource for answer extraction <ref type="bibr" coords="6,213.47,342.90,119.76,10.45" target="#b0">(Bagdouri and Oard, 2015;</ref><ref type="bibr" coords="6,336.70,342.90,105.07,10.45" target="#b4">Nus and Szpekto, 2015)</ref>. However, these systems made use of larger and more recent datasets, instead of using the L6 one. Using a more recent index will probably increase the performance of the described system.</p><p>One of the main advantages of our system is the speed. Even though it was not one of the evaluation metrics, it is worth noting that our system is able to retrieve an answer within 1.546 seconds on average, while the average time for all systems was about 20 seconds. Our system is thus almost 13 times faster than the average response time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We described the work conducted in the ADAPT research centre in DCU for the purpose of participation in the TREC 2015 LiveQA track. In summary, we used a sentence retrieval approach over an indexed collection of previous Yahoo! Answers questions. We used the title of a new question as the query to retrieve a ranked list of similar questions. We then extracted the best matching sentences from all the remaining answers. The final answer was a combination of these sentences with the best answer of the most similar question. When no pre-existing questions with a sufficient similarity with the new one were retrieved, we opted for an answer from a candidate set of pre-generated answers based on the domain of the question.</p><p>Our submitted system was ranked 11 (out of 21) with an average score very close to the average score computed over all submitted runs. We believe one of the possible ways to improve the performance of our approach is by including more recent questions into the index (only the L6 dataset prepared in 2007 was used). Another possible improvement is in incorporating a classifier that chooses a retrieval strategy for each incoming question, instead of combining the outputs of several systems that results in incoherent answers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,75.60,289.09,444.08,217.85"><head></head><label></label><figDesc>The content of each individual field is extracted from the respective XML tags of each document. The field-based indexing ensures that the contributions from the similarities of each field can be combined to constitute the overall similarity value between a new question and the existing ones. Document collection statistics are shown in Table1.</figDesc><table coords="2,75.60,357.46,444.08,149.48"><row><cell>Field Name</cell><cell>Description</cell><cell>Vocab Size</cell></row><row><cell cols="2">MainCategory Top level category name of the question</cell><cell>179</cell></row><row><cell cols="2">SubCategory Sub category name</cell><cell>1546</cell></row><row><cell>Category</cell><cell>Category description</cell><cell>2919</cell></row><row><cell>Title</cell><cell>Title of a question</cell><cell>945,708</cell></row><row><cell>Body</cell><cell>Body of a question</cell><cell>601,862</cell></row><row><cell cols="2">BestAnswer The text of the best answer for a question</cell><cell>2,039,651</cell></row><row><cell cols="2">AllAnswers Concatenated text for all (but the best) answers for a question</cell><cell>5,123,702</cell></row><row><cell cols="3">Table 1: Summary of the individual fields of the indexed documents comprising the Yahoo! Answers</cell></row><row><cell cols="2">collection. The total number of documents is 4, 483, 032.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,75.60,42.16,444.07,414.12"><head>Table 2 :</head><label>2</label><figDesc>Evaluation guidelines we followed during internal evaluation and example answers to "I need a tooth pulled now but don't have the money for it?"</figDesc><table coords="5,83.80,42.16,427.68,377.87"><row><cell>5</cell><cell>Perfect answer</cell><cell>Contact your local or state dental associ-</cell></row><row><cell></cell><cell></cell><cell>ation and and see if there are any dentists</cell></row><row><cell></cell><cell></cell><cell>who provide free or reduced cost care for</cell></row><row><cell></cell><cell></cell><cell>low-income, disabled or senior patients. In</cell></row><row><cell></cell><cell></cell><cell>some areas, you can reach them now by</cell></row><row><cell></cell><cell></cell><cell>dialing 2-1-1 for "non-emergency informa-</cell></row><row><cell></cell><cell></cell><cell>tion." Go to a dental school, if there is one</cell></row><row><cell></cell><cell></cell><cell>near you, for reduced costs. If you are a se-</cell></row><row><cell></cell><cell></cell><cell>nior citizen, call your local Area Agency on</cell></row><row><cell></cell><cell></cell><cell>Aging or Office on Aging.</cell></row><row><cell>4</cell><cell>Contains the answer but also may contain</cell><cell>Contact your local or state dental associ-</cell></row><row><cell></cell><cell>other non-relevant information</cell><cell>ation and and see if there are any den-</cell></row><row><cell></cell><cell></cell><cell>tists who provide free or reduced cost care</cell></row><row><cell></cell><cell></cell><cell>for low-income, disabled or senior patients.</cell></row><row><cell></cell><cell></cell><cell>Call your local health department and ask</cell></row><row><cell></cell><cell></cell><cell>about health centers that provide dental</cell></row><row><cell></cell><cell></cell><cell>care for free or on a sliding scale. there</cell></row><row><cell></cell><cell></cell><cell>aint that much to pulling a tooth. I needed</cell></row><row><cell></cell><cell></cell><cell>4 teeth pulled!</cell></row><row><cell>3</cell><cell>Relevant and partially useful answer</cell><cell>Not all states have D.D.S. programs. I</cell></row><row><cell></cell><cell></cell><cell>think a different system would be a WON-</cell></row><row><cell></cell><cell></cell><cell>DERFUL idea. In some areas, you can</cell></row><row><cell></cell><cell></cell><cell>reach them now by dialing 2-1-1 for "non-</cell></row><row><cell></cell><cell></cell><cell>emergency information.</cell></row><row><cell>2</cell><cell>Relevant, but not useful answer</cell><cell>Please go to the dentist or wait until you</cell></row><row><cell></cell><cell></cell><cell>can live with the fact that you will need to</cell></row><row><cell></cell><cell></cell><cell>be numbed to have the teeth pulled.</cell></row><row><cell>1</cell><cell>Completely irrelevant and useless answer</cell><cell>you nailed it!</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,75.60,690.40,444.08,24.00"><head>Table 3 :</head><label>3</label><figDesc>Performance of our system in comparison to the average scores computed over all submitted runs.</figDesc><table coords="6,81.58,28.21,432.12,174.12"><row><cell>Title</cell><cell>Body</cell><cell>Our Answer</cell><cell>Score (1-4)</cell></row><row><cell>Why does my cat has 2 eyes?</cell><cell cols="2">So tell me.. It is a defect. In show quality cats,</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>it is considered a fault. 2) Cats</cell><cell></cell></row><row><cell></cell><cell></cell><cell>have TWO eyes, generally, unless</cell><cell></cell></row><row><cell></cell><cell></cell><cell>they lose one.</cell><cell></cell></row><row><cell>Do you like dogs?</cell><cell>-</cell><cell>DOG -that's GOD spelled back-</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>wards say "A dog". There, where</cell><cell></cell></row><row><cell></cell><cell></cell><cell>a dog looks like you, there is the</cell><cell></cell></row><row><cell></cell><cell></cell><cell>best place to get a dog.</cell><cell></cell></row><row><cell>Do you like Cricket? Your choice</cell><cell>-</cell><cell>Brits of course who else? Outside</cell><cell>1</cell></row><row><cell>of any player?</cell><cell></cell><cell>of the Aussies who are by far the</cell><cell></cell></row><row><cell></cell><cell></cell><cell>best in world.Americans they play</cell><cell></cell></row><row><cell></cell><cell></cell><cell>baseball, not cricket. Any player!</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,195.84,214.81,203.60,10.45"><head>Table 4 :</head><label>4</label><figDesc>Some examples from final evaluation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,91.74,676.80,301.27,2.14"><p>http://webscope.sandbox.yahoo.com/catalog.php?datatype=l</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,91.74,687.95,134.50,2.14"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="2,91.74,693.05,410.75,8.59"><p>In our initial experiments, we also used the body of a question for query formulation but it produced worse results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="3,91.74,677.23,178.61,8.59"><p>This value was tuned based on initial experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="3,91.74,688.38,427.94,8.59;3,75.60,699.34,88.42,8.59"><p>We decide to extract first two sentences of the best answer, as according to our observations, in most cases they contain most useful information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5" coords="3,91.74,716.38,150.64,2.14"><p>http://nlp.stanford.edu/nlp/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="4,91.74,713.15,312.03,2.14"><p>https://answers.yahoo.com/question/index?qid=1006010500264</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is supported by the <rs type="funder">Science Foundation Ireland</rs> (Grant <rs type="grantNumber">12/CE/I2267</rs>) as part of ADAPT centre (www.adaptcentre.ie) at <rs type="institution">Dublin City University</rs>. The <rs type="institution">ADAPT Centre for Digital Content Technology</rs> is funded under the <rs type="grantName">SFI Research Centres Programme</rs> (Grant <rs type="grantNumber">13/RC/2106</rs>) and is cofunded under the <rs type="funder">European Regional Development Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dPXJwWj">
					<idno type="grant-number">12/CE/I2267</idno>
					<orgName type="grant-name">SFI Research Centres Programme</orgName>
				</org>
				<org type="funding" xml:id="_UdsEvwQ">
					<idno type="grant-number">13/RC/2106</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,75.60,154.73,444.08,10.45;7,86.45,168.36,433.23,10.24;7,86.45,181.91,55.45,10.24" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,245.35,154.73,170.79,10.45">Clip at trec 2015: Microblog and liveqa</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bagdouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,435.59,154.81,84.09,10.24;7,86.45,168.36,181.14,10.24">Proceedings of The Twenty-Fourth Text REtrieval Conference</title>
		<meeting>The Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11-17">2015. 2015. November 17-20, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.60,204.34,444.08,10.45;7,86.45,217.89,433.23,10.45;7,86.45,231.44,102.74,10.45" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,414.71,204.34,104.97,10.45;7,86.45,217.89,206.81,10.45">Higher-order lexical semantic models for non-factoid answer reranking</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hahn-Powell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,300.81,217.98,218.87,10.24;7,86.45,231.53,46.40,10.24">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.60,253.95,444.08,10.45;7,86.45,267.59,433.23,10.24;7,86.45,281.05,159.29,10.45" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,250.63,253.95,225.73,10.45">Corpus-based question answering for why-questions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,495.83,254.04,23.85,10.24;7,86.45,267.59,279.79,10.24">Third International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-01-07">2008. 2008. January 7-12, 2008</date>
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.60,303.57,444.08,10.45;7,86.45,317.12,433.23,10.45;7,86.45,330.67,433.23,10.45;7,86.45,344.22,134.25,10.45" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,294.22,303.57,225.46,10.45;7,86.45,317.12,107.17,10.45">Discourse complements lexical semantics for nonfactoid answer reranking</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,214.98,317.20,304.70,10.24;7,86.45,330.75,95.50,10.24;7,235.69,330.75,56.93,10.24">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct coords="7,75.60,366.73,444.08,10.45;7,86.45,380.28,433.23,10.45;7,86.45,393.92,296.91,10.24" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,219.82,366.73,299.86,10.45;7,86.45,380.28,141.97,10.45">Answering live questions by previously answered questions -yahoo labs at the liveqa track, trec 2015</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpekto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,249.12,380.37,265.78,10.24">Proceedings of The Twenty-Fourth Text REtrieval Conference</title>
		<meeting>The Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11-17">2015. 2015. November 17-20, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.60,416.35,444.08,10.45;7,86.45,429.89,433.23,10.45;7,86.45,443.53,433.23,10.24;7,86.45,456.99,433.22,10.45;7,86.45,470.54,51.22,10.45" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,336.87,416.35,182.80,10.45;7,86.45,429.89,327.86,10.45">Spinning straw into gold: Using free text to train monolingual alignment models for non-factoid question answering</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,437.22,429.98,82.46,10.24;7,86.45,443.53,433.23,10.24;7,86.45,457.08,136.35,10.24">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="231" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.60,493.06,444.08,10.45;7,86.45,506.61,294.23,10.45" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,337.41,493.06,182.27,10.45;7,86.45,506.61,134.61,10.45">Learning to rank answers to non-factoid questions from web collections</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,229.15,506.69,77.11,10.24">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="383" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.60,529.12,444.08,10.45;7,86.45,542.67,433.22,10.45;7,86.45,556.22,325.81,10.45" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,217.09,529.12,302.59,10.45;7,86.45,542.67,89.57,10.45">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,198.37,542.76,321.31,10.24;7,86.45,556.31,220.76,10.24">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
