<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.18,126.57,322.98,3.63;1,151.41,144.50,312.53,3.63">The University of Amsterdam (ILPS.UvA) at TREC 2015 Temporal Summarization Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.23,172.13,79.33,9.52"><forename type="first">Cristina</forename><surname>Gârbacea</surname></persName>
							<email>g.c.garbacea@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.24,172.13,86.87,9.52"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<email>e.kanoulas@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.18,126.57,322.98,3.63;1,151.41,144.50,312.53,3.63">The University of Amsterdam (ILPS.UvA) at TREC 2015 Temporal Summarization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B8E117716EA78F428DCAFD65B0243FA0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we report on our participation in the TREC 2015 Temporal Summarization track, aimed at encouraging the development of systems able to detect, emit, track, and summarize sentence length updates about a developing event. We address the task by probing the utility of a variety of information retrieval based methods in capturing useful, timely and novel updates during unexpected news events such as natural disasters or mass protests, when high volumes of information rapidly emerge. We investigate the extent to which these updates are retrievable, and explore ways to increase the coverage of the summary by taking into account the structure of documents. We find that our runs achieve high scores in terms of comprehensiveness, successfully capturing the relevant pieces of information that characterize an event. In terms of latency, our runs perform better than average. We present the specifics of our framework and discuss the results we obtained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the continuous growth of online information calls for mechanisms able to find and present the textual content e ciently to the end user. Multi-document summarization techniques aim at producing high quality summaries of text buried inside large collections of related documents by condensing the high volume of information into a short, human comprehensible synopsis. In general, there are two main approaches to the task of summarization -extraction and abstraction. Extractive summarization methods determine which are the most important words, phrases or sentences inside the input documents, and select a subset of these to form a summary. On the other hand, abstractive summarization methods build an internal representation of the original documents and exploit semantics and natural language generation techniques to create a summary close to what a human would output. In this paper we focus on extractive summarization techniques of multiple documents during unexpected news events, such as natural disasters, cataclysms, and mass protests.</p><p>The TREC 2015 Temporal Summarization task runs for the third consecutive year and is focused on the development of systems that can summarize emerging events in a real-time fashion. It consists of three subtasks: i) Filtering and Summarization, ii) Pre-Filtered Summarization, and iii) Summarization Only. All subtasks involve summarization of high volume streams of news articles and blog posts crawled from the web . Before the actual summarization, subtasks i) and ii) require an additional preprocessing step aimed at filtering the relevant documents to be summarized for a specific event. However, our participation in this competition focuses mainly on addressing subtask iii), i.e. we aim to explore ways of identifying potential update sentences by assuming that all documents received as input are relevant to our query event.</p><p>The remainder of this paper is organized as follows: Section 2 describes prior initiatives and methods for temporal summarization of events, Section 3 discusses the experimental design of our study, Section 4 describes the experimental results of the performed experiments, and provides an analysis of these results around the limitations of the methods being tested, and last Section 5 outlines the conclusions of our work as well as future directions informed by these conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous TREC 2013 and TREC 2014 Temporal Summarization campaigns released a series of events that could be used by participant systems to develop algorithms for text summarization and model information reliability in a dynamic setting. There were 10 test events released for the 2013 collection, and 15 test events released for the 2014 collection respectively. Each event is characterized by an event query, the time period the event spans, and the type of the event -can be one of the following: accident, bombing, conflict, earthquake, hostage, protest, riot, storm, and shooting. The corpus, namely the TREC KBA Stream Corpus 1 , consists of a set of timestamped documents from a variety of news and social media sources. The use of external information is allowed as long as this information existed before the event start time, or is time-aligned with the KBA corpus.</p><p>Most participants employed a pipeline where information is first pre-processed (this involves decrypting, decompressing and indexing the corpus), retrieved (using a wide range of methods for document and sentence retrieval), and finally processed (ranking the retrieved sentences by time and similarity to any prior emitted sentences). Almost all participant systems used query expansion techniques as a way to improve recall, given the short length of the query and the typical mismatch between query terms and the terms found inside relevant updates. Both supervised and unsupervised methods have been used to generate sequential update summarizations. Latent Dirichlet Allocation was used to find that latent semantic topics of documents and generate lists of weighted keywords that could help in sentence scoring and ranking. Discriminative methods for extracting keywords (<ref type="foot" coords="2,225.84,588.97,3.97,6.39" target="#foot_0">2</ref> ) have been employed to collect relevant terms describing an event, and later used as features in training an SVM classifier for sentence update detection. Other participants tried to model events by employing a generic event model, leaving from the assumption that event updates share a common vocabulary of terms independent of the event type. Clustering methods were used to group similar sentences, and from each cluster the cluster centroids were picked as the most salient sentences to output. Finally, sentences are tested for novelty and only the ones passing this filter are emitted as updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Design</head><p>In this section we describe the experimental design that we used in our analysis. In retrieving relevant updates we consider di↵erent information retrieval based approaches that have been adopted in text and document summarization. Ideally, an emitted update should be significant, timely, non-verbose, and novel. We aim to incorporate these all these qualifiers in our summarization framework. In what follows we explain how we account for such characteristics of a sentence update, and illustrate the main components of the summarization system we developed.</p><p>1. Corpus preprocessing: TREC KBA Stream Corpus is an encrypted file whose decryption requires an authorized key provided to each participant team by the TREC organizers. Flat files have been serialized into batches of documents called Chunks, and further compressed. In order to retrieve the content of a document, we need to perform these operations in reverse order: after decompressing each file, we use the tools provided by the StreamCorpus 2 toolbox to extract large streams of text from each StreamItem and store its content in a custom format. 2. Document retrieval: Given the large volume of data, we proceed to indexing the extracted documents into multiple ElasticSearch indices. This makes it convenient in terms of scalability, searching for documents in almost realtime, and enhances the repeatability of our experiments. For each event we issue the event query specified in the description of the event and retrieve relevant documents that constitute the input to our sentence extraction and summarization module. We discard all documents which are outside the time range of a given event. 3. Query expansion: The query describing an event is typically very short (2-3 words in legth), and this makes the retrieval of relevant sentences prone to word mismatch problems in cases when the vocabulary of the query di↵ers significantly from the vocabulary of an update. To prevent this, we rely on query expansion techniques to augment a query word with similar terms. We use two methods: i) Wordnet -for each query term we retrieve its Wordnet synonyms <ref type="bibr" coords="3,199.04,553.86,9.96,9.52" target="#b8">[9]</ref>, and augment the original query with these terms, and ii) Word2Vec -we train our model <ref type="bibr" coords="3,288.64,565.82,10.51,9.52" target="#b7">[8]</ref> on sentences from the relevant documents in TREC Temporal Summarization 2013 and 2014 collections, retrieve the most similar terms to a query term, and add them to the expanded query. 4. Sentence extraction and summarization: We employ a variety of sentence selection methods for finding relevant updates inside the relevant documents. In particular, we are interested in finding whether an event update is central in the documents that contain it, and to what extent event updates are retrievable by means of the shared vocabulary between the language of an event query and the language of an event update. To this end, we probe the utility of the following well-established information retrieval methods: (a) Term Frequency: We rank sentences by the number of matching event query terms found inside a sentence. We set a predefined threshold for the least number of times query terms should be present in a sentence based on empirical observations on the TREC Temporal Summarization 2014 collection. This enables our method to perform in a real time streaming scenario. (b) TF.ISF: Similar to the traditional term frequency -inverse document frequency (tf.idf) method used for document retrieval, the vector space model for sentence retrieval uses the term frequency -inverse sentence frequency (tf.isf) method <ref type="bibr" coords="4,284.72,262.88,9.96,9.52" target="#b3">[4]</ref>. Using tf.isf, we rank sentences with the following formula: R(s|q) = X t2q log(tf t,q + 1) log(tf t,s + 1) log</p><formula xml:id="formula_0" coords="4,389.93,289.03,90.65,31.89">✓ n + 1 0.5 + sf t ◆<label>(1)</label></formula><p>where tf t,q is the number of occurrences of term t in query q, tf t,s is the number of occurrences of term t in query s, sf t is the number of sentences that contain term t, n is the number of sentences in the collection.</p><p>To compute the number of sentences that contain a query term t, we treat each document as a collection of sentences. We infer the rest of the counts from documents at the time of emission. We rank sentences in the document according to their corresponding tf.isf values, and keep the ones with a tf.isf score higher than a pre-set threshold. (c) Query Likelihood: The query likelihood model for sentence retrieval ranks sentences by the probability that the query was generated by the same distribution of terms the sentence is from. Since it retrieves sentences that contain exact words as the query, this makes it appropriate for exact similarity match:</p><formula xml:id="formula_1" coords="4,265.95,533.07,210.38,31.66">P (S|Q) / P (S) |Q| Y i=1 P (q i |S) (<label>2</label></formula><formula xml:id="formula_2" coords="4,476.34,544.19,4.24,9.52">)</formula><p>where Q is the query, |Q| is the number of terms in the query, q i is the i th term in the query, and S is a sentence. The e↵ectiveness of the query likelihood model is demonstrated in prior work on sentence retrieval research <ref type="bibr" coords="4,207.28,607.91,9.96,9.52" target="#b1">[2]</ref>, <ref type="bibr" coords="4,223.95,607.91,9.96,9.52" target="#b6">[7]</ref>, <ref type="bibr" coords="4,240.62,607.91,15.49,9.52" target="#b9">[10]</ref> where it outperforms word overlap and tf.isf based measures. In addition to the regular query likelihood model, we are using query likelihood linear interpolation smoothing. (d) Log-Likelihood Ratio (LLR): We aim to extract discriminative terms that can distinguish an update sentence from a non-update sentence.</p><p>We model the characteristics of an event as the set of the most discriminative LLR terms, which we infer for each event type by building two distinct corpora: a foreground corpus consisting of all relevant event updates, and a background corpus to estimate the importance of a word made up of all non-update sentences per event type from the relevant documents. To build these two corpora we use data from past TREC Temporal Summarization tracks. We use a slight variation of the original method <ref type="bibr" coords="5,225.22,202.45,14.61,9.52" target="#b10">[11]</ref>, log-likelihood ratio with cut-o↵ and query sensitivity LLR(CQ), to inform the summarizer to make the output more focused <ref type="bibr" coords="5,168.64,226.36,9.96,9.52" target="#b5">[6]</ref>. We rank terms by their LLR score and consider the top-N retrieved for each event type when selecting which sentences to include in the summary. (e) Latent Dirichlet Allocation (LDA): We use LDA to capture events covered by the relevant documents. These documents typically have a central theme or event, and other sub-events which support or revolve around this central event. The central theme and the sub-events altogether determine the set of topics covered by the relevant documents. LDA <ref type="bibr" coords="5,192.49,322.04,10.51,9.52" target="#b2">[3]</ref> is a generative hierarchical probabilistic model which represents documents as finite mixtures over an underlying set of topics. These topics are modeled in turn as an infinite mixture over an underlying set of topic probabilities. We follow <ref type="bibr" coords="5,298.06,357.90,10.51,9.52" target="#b0">[1]</ref> and we weight sentences using a purely statistical approach of capturing the events documents are based on:</p><formula xml:id="formula_3" coords="5,222.04,384.64,258.54,29.95">P (S|T j ) = X Wi2S P (W i |T j ) ⇤ P (T j |D B ) ⇤ P (D B )<label>(3)</label></formula><p>where -P (S|T j ) is the probability that the sentence S represents topic T j , -P Wi2S P (W i |T j ) is the probability that the words of the sentence S belong to topic T j , -P (T j |D B ) is the probability that topic T j belongs to document D B , -P (D B ) is the probability of document D B (we assume the probability of each relevant document as uniform).</p><p>Additionally, we score and rank sentences by the weight of topic words . (f) Language Modeling: We use TREC Temporal Summarization historical data (2013 and 2014) to build a unigram language model from all relevant event updates. We hypothesize that event updates share a common crisis-related vocabulary, that distinguishes them from other non-update sentences. In our implementation we use SRILM<ref type="foot" coords="5,374.49,576.14,3.97,6.39" target="#foot_1">3</ref> , an extensible language modeling toolkit which supports the creation and evaluation of a variety of language model types based on N-gram statistics <ref type="bibr" coords="5,396.97,601.27,14.61,9.52" target="#b11">[12]</ref>. (g) Cosine Similarity: We rank sentences by the cosine of the angle between the document vector and the query vector. We compute the vector representation of each query and each sentence in turn, using tf.idf term weights. We compute if values on prior documents. After getting the corresponding vectors, the distance between two vectors is simply defined by:</p><formula xml:id="formula_4" coords="6,283.90,165.00,192.45,29.99">cos ✓ = a ⇥ b ||a|| ⇥ ||b|| (<label>4</label></formula><formula xml:id="formula_5" coords="6,476.34,171.56,4.24,9.52">)</formula><p>where ✓ is the angle between the two vector representations a and b. (h) Sentence Centrality: We test across document centrality by running LexRank <ref type="bibr" coords="6,212.01,221.53,9.96,9.52" target="#b4">[5]</ref>, a state-of-the-art graph based summarization algorithm.</p><p>LexRank assesses the centrality of each sentence in a cluster (centrality of a sentence is defined in terms of the centrality of the words contained inside the sentence), and extracts the most salient sentences to include in the summary by building a weighted graph with nodes that represent sentences and edges that represent the cosine similarity between pairs of sentences. 5. Novelty detection: We rank the sentences retrieved by each of the above methods by time, and at each point in time we ensure we are not adding duplicate content to the summary. To this end, we use the cosine similarity metric presented above to check the degree of redundancy between a new sentence we are about to output and all prior sentences already added to the summary. If the similarity measure is higher than the 0.5 threshold, we discard the sentence as the information contained inside the sentence has already been captured by a more timely update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>Dataset. We test our methods on the TREC Temporal Summarization 2015 dataset, which is a subset of the TREC KBA 2014 Stream Corpus (4.5 TB). The corpus spans the time period October 2011 -April 2013, and includes timestamped documents collected from a variety of news articles and social media sources. For the Summarization Only sub-task we use the filtered corpus of on-topic documents (TREC-TS-2015F-RelOnly) with data for a set 21 crisis events. We submit 15 runs to this sub-track based on the methods presented above, or variations thereof. For the Pre-filtered Summarization sub-task, we use the pre-filtered corpus of news articles and blog posts (TREC-TS-2015F), and submit one additional run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>In Table <ref type="table" coords="6,233.75,572.20,4.98,9.52" target="#tab_0">1</ref> we report on the results we obtained using the o cial evaluation metrics for the task. We observe that the performance of our runs is very good in terms of recall, and that we manage to retrieve relevant updates covering the important nuggets using our methods. Except for the query likelihood with smoothing run, the coverage of our summaries in terms of comprehensiveness is above average as illustrated in Figure <ref type="figure" coords="6,388.47,631.98,3.87,9.52" target="#fig_2">3</ref>, and culminates in a maximum of 0.8415 when we identify updates using simple query term frequency. This implies that the summaries we generate identify a great part of the essential information that could have been retrieved for a particular event, and that our methods are e↵ective in terms of recall for the given task.</p><p>In terms of precision, however, our scores are comparable to average or lower, as shown in Figure <ref type="figure" coords="7,221.02,596.11,4.98,9.52" target="#fig_0">1</ref> for the normalized expected gain metric. The query likelihood with smoothing presents the best precision among our runs, ranking ontopic and novel updates better than average. According to this custom precision metric, systems are penalized not only for an incorrect ranking of the retrieved updates, but also for "verbosity" -a characteristic of a system when it retrieves unreasonably long and di cult to read updates. For example, it could be the case that our sentence updates do cover relevant nuggets, but they are too long and therefore get penalized for the additional reading e↵ort they introduce. Compared to the normalized expected gain, the normalized expected latency gain metric adds an extra time dimension to the evaluation of a summary. When this time component is further considered, our scores understandably drop as cascading errors can propagate throughout the system; this e↵ect can be seen in Figure <ref type="figure" coords="8,178.48,190.50,3.87,9.52" target="#fig_1">2</ref>. Interestingly, the query likelihood method with term smoothing is still the top performer. From Figure <ref type="figure" coords="8,302.97,202.45,4.98,9.52" target="#fig_3">4</ref> we can infer statistics for the latency component metric. Contrary to its name, a higher value for latency is better because it means that a system does not delay the emission of sentences to collect more information before issuing updates. There is a lot of variation in the performance of our runs with respect to latency, but overall we observe all runs are doing better than average. Finally, the harmonic mean between the normalized expected latency gain and latency comprehensiveness is used for ranking the systems participating in the track. Figure <ref type="figure" coords="8,339.58,286.14,4.98,9.52" target="#fig_4">5</ref> shows that not all of our runs surpass the average for this combined metric, however our best systems score above the average value. Query likelihood with smoothing and cosine similarity achieve the best results overall, and are our highest ranked submissions to the Summarization Only task.</p><p>We now turn to an event-level comparison of our methods. Out of the total 21 test events released, there are 8 events of type bombing, 7 events of type accident, 2 events of type protest, 2 events of type earthquake, 1 event of type conflict, and 1 event of type storm. For event types "accident" and "bombing", term frequency alone seems to identify many of the relevant updates, which implies that events in discussion share a common vocabulary with the query. This fact is confirmed by the presence of terms like "explosion", "bombing", "arson", "bomber", "bomb", "fire" as query terms for events of type bombing. Log-likelihood ratio, TF.ISF and query likelihood are close performers in retrieving updates that match the gold standard nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have presented a variety of approaches for addressing the task of identifying relevant sentence-level updates that characterize an event for the purpose of extractive document summarization. We observe that traditional information retrieval algorithms present decent performance in detecting these updates, however often times we report an event with considerable time lag after the event has emerged. In future work we would like to focus on improving event detection in real time, and on event summarization at di↵erent granularities, possibly through the use of online hierarchical clustering algorithms and event modeling techniques.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,134.77,141.05,345.82,8.97;10,134.77,152.01,211.85,8.97;10,134.77,161.47,345.83,207.50"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Results for the normalized expected gain metric, i.e. the degree to which the updates within the summary are on-topic and novel.</figDesc><graphic coords="10,134.77,161.47,345.83,207.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,134.77,408.30,345.81,8.97;10,134.77,419.26,284.31,8.97;10,134.77,428.72,345.83,207.50"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Results for the normalized expected latency gain metric, i.e. i.e. the degree to which the updates within the summary are on-topic, novel and timely.</figDesc><graphic coords="10,134.77,428.72,345.83,207.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,134.77,130.71,345.81,8.97;11,134.77,141.67,345.82,8.97;11,134.77,152.63,79.92,8.97;11,134.77,160.35,345.83,207.50"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Results for the comprehensiveness metric, i.e. how many nuggets the system covers. Comprehensiveness is similar to the traditional notion of recall in information retrieval evaluation.</figDesc><graphic coords="11,134.77,160.35,345.83,207.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,134.77,407.18,345.82,8.97;11,134.77,418.13,345.82,8.97;11,134.77,429.09,55.84,8.97;11,134.77,439.05,345.83,207.50"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Results for expected latency metric, i.e. the degree to which the information contained within the updates is outdated (a high value for latency denotes timely performance).</figDesc><graphic coords="11,134.77,439.05,345.83,207.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,134.77,269.44,345.81,8.97;12,134.77,280.40,345.82,8.97;12,134.77,291.36,294.51,8.97;12,134.77,300.82,345.83,207.50"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results for HM (nE[Latency Gain], Latency Component) -the harmonic mean of normalized Expected Latency Gain and Latency Comprehensiveness. This is the o cial target metric for the TREC Temporal Summarization 2015 track.</figDesc><graphic coords="12,134.77,300.82,345.83,207.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,115.84,345.82,401.38"><head>Table 1 .</head><label>1</label><figDesc>TREC Temporal Summarization 2015 results (average for each submitted run across all test events).</figDesc><table coords="7,140.69,150.06,330.90,367.16"><row><cell>Run</cell><cell cols="4">nE[Gain] nE[Latency Gain] Comprehen-Latency HM</cell></row><row><cell></cell><cell></cell><cell></cell><cell>siveness</cell><cell></cell></row><row><cell>Query likelihood</cell><cell>0.0200</cell><cell>0.0145</cell><cell>0.7541</cell><cell>0.5381 0.0277</cell></row><row><cell>no smoothing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Query likelihood</cell><cell>0.0798</cell><cell>0.0453</cell><cell>0.4222</cell><cell>0.2687 0.0618</cell></row><row><cell>with smoothing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Query likelihood</cell><cell>0.0359</cell><cell>0.0204</cell><cell>0.6662</cell><cell>0.4664 0.0375</cell></row><row><cell>with smoothing +</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>higher threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cosine similarity</cell><cell>0.0428</cell><cell>0.0260</cell><cell>0.5708</cell><cell>0.3655 0.0471</cell></row><row><cell>Cosine similarity</cell><cell>0.0281</cell><cell>0.0197</cell><cell>0.7325</cell><cell>0.5118 0.0372</cell></row><row><cell>expanded query</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Word2Vec)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Term frequency</cell><cell>0.0223</cell><cell>0.0160</cell><cell>0.8415</cell><cell>0.6289 0.0310</cell></row><row><cell>Term frequency</cell><cell>0.0200</cell><cell>0.0147</cell><cell>0.8326</cell><cell>0.6209 0.0285</cell></row><row><cell>expanded query</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Wordnet)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Term frequency</cell><cell>0.0264</cell><cell>0.0172</cell><cell>0.7992</cell><cell>0.5865 0.0330</cell></row><row><cell>expanded query</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Word2Vec)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TF.ISF</cell><cell>0.0234</cell><cell>0.0166</cell><cell>0.8196</cell><cell>0.6080 0.0321</cell></row><row><cell>TF.ISF</cell><cell>0.0221</cell><cell>0.0158</cell><cell>0.8260</cell><cell>0.6169 0.0306</cell></row><row><cell>expanded query</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Wordnet)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TF.ISF</cell><cell>0.0212</cell><cell>0.0153</cell><cell>0.8301</cell><cell>0.6107 0.0297</cell></row><row><cell>expanded query</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Word2Vec)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LexRank</cell><cell>0.0224</cell><cell>0.0157</cell><cell>0.7490</cell><cell>0.5111 0.0299</cell></row><row><cell>Language modeling</cell><cell>0.0195</cell><cell>0.0135</cell><cell>0.6871</cell><cell>0.4737 0.0258</cell></row><row><cell>LLR</cell><cell>0.0173</cell><cell>0.0130</cell><cell>0.8348</cell><cell>0.6533 0.0248</cell></row><row><cell>LDA</cell><cell>0.0222</cell><cell>0.0131</cell><cell>0.7036</cell><cell>0.4271 0.0250</cell></row><row><cell>LDAv2</cell><cell>0.0202</cell><cell>0.0126</cell><cell>0.7423</cell><cell>0.4778 0.0241</cell></row><row><cell cols="2">TREC TS 2015 Average 0.0595</cell><cell>0.0319</cell><cell>0.5627</cell><cell>0.3603 0.0472</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,143.52,662.90,188.79,2.11"><p>https://github.com/trec-kba/streamcorpus</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="5,143.52,662.90,193.00,2.11"><p>http://www.speech.sri.com/projects/srilm/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,155.24,142.68,325.35,9.52;9,155.24,154.63,325.34,9.52;9,155.24,166.59,234.00,9.52" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,268.80,142.68,211.79,9.52;9,155.24,154.63,62.26,9.52">Latent dirichlet allocation based multi-document summarization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,242.91,154.63,237.68,9.52;9,155.24,166.59,123.58,9.52">Proceedings of the second workshop on Analytics for noisy unstructured text data</title>
		<meeting>the second workshop on Analytics for noisy unstructured text data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="91" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.24,178.54,325.34,9.52;9,155.24,190.50,325.34,9.52;9,155.24,202.45,325.35,9.52;9,155.24,214.41,112.62,9.52" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,364.66,178.54,115.93,9.52;9,155.24,190.50,82.69,9.52">A comparison of sentence retrieval techniques</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,260.45,190.50,220.14,9.52;9,155.24,202.45,321.50,9.52">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="813" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.24,226.36,325.34,9.52;9,155.24,238.32,212.52,9.52" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,311.63,226.36,111.78,9.52">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,430.72,226.36,49.86,9.52;9,155.24,238.32,127.30,9.52">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.24,247.75,325.34,12.04;9,155.24,262.23,325.34,9.52;9,155.24,274.18,177.65,9.52" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,319.85,250.27,160.73,9.52;9,155.24,262.23,143.02,9.52">A recursive tf-isf based sentence retrieval method with local context</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Štula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stipaničev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,305.75,262.23,174.83,9.52;9,155.24,274.18,84.68,9.52">International Journal of Machine Learning and Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.24,286.14,325.34,9.52;9,155.24,298.09,325.34,9.52;9,155.24,310.05,65.85,9.52" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,261.96,286.14,218.62,9.52;9,155.24,298.09,100.85,9.52">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,267.31,298.09,193.01,9.52">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.24,322.00,325.34,9.52;9,155.24,333.96,325.33,9.52;9,155.24,345.91,325.34,9.52;9,155.24,357.87,325.34,9.52;9,155.24,369.82,27.67,9.52" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,319.53,322.00,161.06,9.52;9,155.24,333.96,238.53,9.52">Measuring importance and query relevance in topic-focused multi-document summarization</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,417.22,333.96,63.36,9.52;9,155.24,345.91,325.34,9.52;9,155.24,357.87,71.48,9.52">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.24,381.78,325.34,9.52;9,155.24,393.73,325.34,9.52;9,155.24,405.69,325.33,9.52;9,155.24,417.64,95.46,9.52" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,437.94,381.78,42.65,9.52;9,155.24,393.73,169.26,9.52">Similarity measures for tracking information flow</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mo↵at</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,348.24,393.73,132.34,9.52;9,155.24,405.69,301.96,9.52">Proceedings of the 14th ACM international conference on Information and knowledge management</title>
		<meeting>the 14th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.24,429.60,325.34,9.52;9,155.24,441.55,325.34,9.52;9,155.24,453.51,308.13,9.52" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,430.83,429.60,49.75,9.52;9,155.24,441.55,284.00,9.52">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,464.26,441.55,16.32,9.52;9,155.24,453.51,207.28,9.52">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.24,465.46,325.35,9.52;9,155.24,477.42,325.33,9.52;9,155.24,489.37,147.52,9.52" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,455.65,465.46,24.94,9.52;9,155.24,477.42,212.23,9.52">Introduction to wordnet: An on-line lexical database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,376.47,477.42,104.11,9.52;9,155.24,489.37,54.55,9.52">International journal of lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.23,501.33,325.35,9.52;9,155.24,513.29,133.68,9.52" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,228.85,501.33,125.59,9.52">Aspects of sentence retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">G</forename><surname>Murdock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="9,155.23,525.24,325.35,9.52;9,155.24,537.20,325.34,9.52;9,155.24,549.15,161.60,9.52" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,265.32,525.24,195.71,9.52">Comparing corpora using frequency profiling</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rayson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garside</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,155.24,537.20,228.46,9.52">Proceedings of the workshop on Comparing Corpora</title>
		<meeting>the workshop on Comparing Corpora</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,155.23,561.11,325.35,9.52;9,155.24,573.06,92.84,9.52" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,239.96,561.11,202.07,9.52">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<editor>IN-TERSPEECH</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
