<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.16,85.26,403.38,5.54">A Constrained Approach to Manual Total Recall</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,166.70,117.63,81.29,12.34"><forename type="first">Jeremy</forename><surname>Pickens</surname></persName>
							<email>jpickens@catalystsecure.com</email>
							<affiliation key="aff0">
								<orgName type="department">Catalyst Repository Systems</orgName>
								<address>
									<addrLine>1860 Blake Street, 7th Floor Denver</addrLine>
									<postCode>80202</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.88,117.63,57.47,12.34"><forename type="first">Tom</forename><surname>Gricks</surname></persName>
							<email>tgricks@catalystsecure.com</email>
							<affiliation key="aff0">
								<orgName type="department">Catalyst Repository Systems</orgName>
								<address>
									<addrLine>1860 Blake Street, 7th Floor Denver</addrLine>
									<postCode>80202</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.94,117.63,56.78,12.34"><forename type="first">Bayu</forename><surname>Hardi</surname></persName>
							<email>bhardi@catalystsecure.com</email>
							<affiliation key="aff0">
								<orgName type="department">Catalyst Repository Systems</orgName>
								<address>
									<addrLine>1860 Blake Street, 7th Floor Denver</addrLine>
									<postCode>80202</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.36,117.63,54.65,12.34"><forename type="first">Mark</forename><surname>Noel</surname></persName>
							<email>mnoel@catalystsecure.com</email>
							<affiliation key="aff0">
								<orgName type="department">Catalyst Repository Systems</orgName>
								<address>
									<addrLine>1860 Blake Street, 7th Floor Denver</addrLine>
									<postCode>80202</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,103.16,85.26,403.38,5.54">A Constrained Approach to Manual Total Recall</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AA303796223C2CF57E966EE87F8ACE06</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Catalyst participation in the manual at home Total Recall Track was both limited and quick, a TREC submission of practicality over research. The group's decision to participate in TREC was made three weeks, and data was not loaded until six days, before the final submission deadline. As a result and to reasonably simulate an expedited document review process, a number of shortcuts were taken in order to accomplish the runs in limited time. Choices about implementation details were due primarily to time constraint and necessity, rather than out of designed scientific hypothesis. We detail these shortcuts, as well as provide a few additional post hoc, non-o cial runs in which we remove some of the shortcuts and constraints. We also explore the e↵ect of di↵erent manual seeding approaches on the recall outcome.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">PRIMARY METHOD</head><p>Given the manner in which Team CATRES approached this project there was no formal hypothesis, as such. Rather, the project was primarily an evaluation of the extent to which a constrained continuous active learning <ref type="bibr" coords="1,246.98,439.00,9.72,8.57" target="#b1">[1]</ref> tool can e↵ectively assimilate and adjust the disparate skills and knowledge of multiple independent, time-pressured reviewers tasked solely with the obligation to expeditiously locate potential seeds to commence ranking. In that sense, the working hypothesis was that a continuous active learning tool, when combined with an initial seeding associated with tight deadlines, limited knowledge and experience, and potentially inconsistent perspectives, will produce a reasonable result.</p><p>The manual seeding e↵ort itself was intentionally limited and necessarily relatively cursory. Three users each worked for no more than one hour apiece to locate potential seed documents based on their personal conjecture as to the potential scope of the terse descriptions for each topic. Within that hour, each had to (individually and separately) carry out all three aspects of the task: (1) familiarize themselves with the topic, (2) issue queries to find relevant information, and (3) read and mark that information for relevance. One of the three users was well-versed on the search tool and its capabilities, query operators, and syntax, but the other two users were essentially brand new to the system. All three users averaged between limited to no knowledge of the topics. Further details are given in Section 1.1.</p><p>After this work was completed, the system was set to an automated, continuous learning (iterative) mode with no additional human intervention other than the o cial judgments. Top ranked, as yet unseen documents were continu-ously (at least until time ran out) fed to the TREC server in batches, truth values from the TREC server were fed back in to the core learning algorithm, and then the remaining unjudged documents in the collection were re-ranked. Normally, iterative batch sizes would be constant, but given time constraints and in order to expedite the process, batch sizes were increased over time. Batch sizes started small to enhance continuous active learning (100 docs per iteration) and then were gradually increased (250, 500, 1000, 2000, and 5000) as the time deadline neared. Final batches were submitted just hours before the deadline.</p><p>Continuing in the theme of constraint, the algorithm underlying the continuous learning protocol was also constrained. These constraints included naive reduction of the feature space, extensive use of pseudo-negatives in training, and lack of explicit diversification. We will explore these and more constraints in greater detail in Section 1.2. We also present results of some non-o cial, post hoc runs in which we relax some of these constraints. Finally, we investigate the e↵ect that di↵erent searchers (and the union thereof) have on the recall-oriented outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">User Involvement</head><p>The initial, manual stage was conducted in the following manner: All three people worked for one hour each, yielding a total of three person-hours per topic. All three team members worked independently of each other and at different times in di↵erent geographic locations and no information was communicated about the topics or topic-related searches between the three team members. No restrictions on the strategies or methods that each adopted were stipulated other than a request to set a filter on one's searches to remove documents that had already been marked by another user. However, due either to time or lack of clarity in communication, this request was only followed by reviewer 1 and not by reviewers 2 and 3. However, as reviewer 1 completed his work before reviewers 2 and 3 began, this has the net e↵ect of zero e↵ort deduplication, i.e complete independence and the possibility of duplication of e↵ort. Thus, each reviewer was free to work as he wished, with activities including (1) researching the topic, (2) searching for relevant information, and (3) reading and marking documents for relevance. Some team members spent more time researching, others spent more time marking, but each person had only a single hour, per topic, to do all activities.</p><p>The reviewers self-reported the following rough breakdown in time spent: Searchers 1 and 2 spent no time researching the topic, other than 15 and 45 minutes (respectively) consulting external resources on topic 109. Otherwise, half of  <ref type="figure" coords="2,236.16,198.85,3.58,8.57">1</ref>: Reviewer Document Judgment Counts their time was spent searching and half of the time reviewing and marking documents, with the searching and reviewing activities done in an interleaved manner. Each of these two reviewers averaged 3-5 primary queries per topic, with 3-5 refinements of the primary queries (addition or deletion of a word or query operator) for a total of approximately 6-10 searches per topic.</p><p>Reviewer 1 explicitly attempted to diversify his e↵orts, never spending too much time pursuing any one topic, going wide and broad with his query selection rather than narrow and deep. This reviewer judged documents solely to determine whether they served the cause of diversity. Reviewer 2 took a more standard approach, where the purpose of each query was to find relevant documents, not diverse documents. Reviewer 3, on the other hand, spent the first 15 minutes researching the topic, the next 30 minutes composing a "synthetic document" with multiple passages of information relevant to the topic (which passages were gleaned from web searches), and the last 15 minutes reviewing and marking results from having used this synthetic document as an initial "pseudo-positive" seed document. No additional queries were issued by Reviewer 3 while in this last stage.</p><p>During the interactive querying and relevance marking (manual) session for each topic, reviewers 1 and 2 chose to not avail themselves of the o cial TREC relevance judgments. Instead, they decided upon a subjective assessment of relevance and use that subjective assessment to guide query formulation and development. Thus, no o cial judgments influenced either of the first two reviewers during their interactivity. Reviewer 3 did look up the o cial relevance judgment as he was examining every document, but because document examination start only at the very end, after research and the single pseudo-positive seed document "query", this also has the e↵ect that no o cial judgments influenced reviewer 3 during any query formulation stage.</p><p>Once all three reviewers were finished working on a topic, every document that any reviewer had examined but that had not yet received an o cial TREC relevance judgment (i.e. had not yet been submitted to the Total Recall server to be recorded as o cial e↵ort for the run) were submitted to the server, and then the continuous learning stage was kicked o↵ with no further human intervention.</p><p>One fact mentioned earlier requires further elaboration. No controls were made for ensuring that the three reviewers didn't duplicate their e↵ort. Thus, during the course of each of their one hour of work, reviewers may have unintention-ally judged the same document twice, completely unaware of the judgment that a previous reviewer had given to that document. In our o cial TREC submission, we submitted the union of all documents that all three reviewers manually found, every relevant as well as every non-relevant document. As per task guidelines, however, it may have been more correct to submit the same documents twice, i.e. to have done the sum of all documents rather than the union. To this end, we o↵er the following statistics in Figure <ref type="figure" coords="2,548.75,313.57,3.58,8.57">1</ref>. This chart shows the number of documents found, relevant or non-relevant, per reviewer per topic. The values are the o cial TREC judgments. It also shows counts for the sum, union, and (sum -union) di↵erence of all three reviewers.</p><p>The fact that the sum equals the union in 6 of the 10 topics means that our o cial run is completely accurate in terms of representing the full amount of manual e↵ort done. Where the sum is greater than the union, it means that reviewer e↵ort was (unintentionally) duplicated. However, as every uniquely reviewed document, both positive and negative, was submitted to the TREC server, this has no e↵ect on the basic shape of the gain curve. Instead, it should just shift that curve a few documents to the right, anywhere from a shift of 0 for most topics, a shift of 13 documents for Topic 102, and a shift of 213 documents for Topic 103.</p><p>Of note is the fact that duplication of relevant e↵ort (different reviewers finding the same relevant documents) was relatively much more common than duplication of non-relevant e↵ort, as well as the slight tendency for this to happen more on sparse topics (e.g. 104 and 109). More could be written along these lines, but we save that for future work.</p><p>We should also briefly note that all three reviewers had high levels of experience running search-based recall-oriented projects in the past, so all felt comfortable in the TREC task. However, reviewer 2 had only used the specific tool a small handful of times, and reviewer 1 had never used the tool before. As such, halfway into running his first topic (106), reviewer 1 felt that he had made some mistakes based on his unfamiliarity with the tool that he felt disqualified his e↵ort from the task. Fifteen minutes into running that topic he stopped and removed all traces of his e↵ort (marked documents). He communicated nothing of what he did (or did not) learn to the other team members. Then reviewers 2 and 3 each worked for 1.5 hours apiece rather than their usual 1 hour, stretching out their individual strategies to proportionally fill the time.</p><p>No team member had more than limited prior knowledge  <ref type="figure" coords="3,116.09,173.35,3.58,8.57">2</ref>: Reviewer Prior Topic Knowledge (watching or reading the news, conversations with friends) about any topic. Reviewer 1 had limited, generic knowledge of a number of the topics, but no specific knowledge, and no knowledge whatsoever on the Scarlet Letter Law, Topic 109. For Topic 109, reviewer 1 spent 5-10 minutes of his hour reading Wikipedia on the Topic. The only topics that reviewer 2 had some knowledge on were topics 106 (Terri Schiavo) and 105 (A rmative Actions). He used Google and Wikipedia to research topic 109 (Scarlet Letter Law) for 45 minutes. He did not have a lot of knowledge on the remaining topics, but felt that he got a general idea from looking at the topic names. Again, however, we note that he viewed no o cial assessments during the course of the hour. Reviewer 3 had some previous knowledge of many of the topics through exposure to news articles and the like. As mentioned previously, for all ten topics he started by assembling a synthetic seed document by using Google for about 30 minutes per topic to find wikipedia and news articles with sections of text that looked relevant.</p><p>The exact breakdown of prior by topic is found in Figure <ref type="figure" coords="3,285.75,405.38,3.58,8.57">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Algorithmic Constraints</head><p>In addition to the time constraints placed on the human reviewers, our o cial run was implemented with number of algorithmic constraints as well. Some of these constraints were intentionally done so as to speed execution time, some constraints unintentionally occurred due to mistakes associated with last minute haste.</p><p>The intentional constraints were two-fold: Features and ranking. On the feature side, only unigram text was extracted. Not extracted were dates, email communication information, named entities, and so on. Moreover, naive feature reduction was done by ignoring unigrams that were either too frequent or not frequent enough. Unigrams with a document frequency under about a dozen, and over about ten thousand were simply ignored. This naive approach sped iterative simulation -important given the time constraints -but might have come at a bit of an e↵ectiveness cost, one that we anecdotally explore in Section 3.2.</p><p>The second intentional constraint that was done to speed limited iteration time was to pre-compute a "pseudo-negative" score across the entire collection and then only train on positive documents. In a tradeo↵ between speed an e↵ectiveness, experiments on other datasets have shown that speed can be increased often without losing too much e↵ectiveness. This bore out for some topics, but turns other topics into relative failures, and will be investigated further in Section 2.</p><p>There were a number of unintentional constraints as well. The first was that the the algorithms were not run fully continuously. This was simply due to running out of time; iteration necessarily stopped with the TREC deadline was hit. As that deadline was approached, topics were actively monitored as time ran out, and batch size was manually increased, subjectively and non-uniformly, based on iterative richness.</p><p>The second unintended constraint was that no algorithmic diversification was performed. Typically we have found various forms of active diversification useful in recall-oriented tasks, but unfortunately a bug in the hastily written code to interface between the algorithm and the TREC server failed to submit any diversification results. Finally, the third unintended algorithmic limitation is that another server interface code bug ended up submitting one hundred documents to Topic 101 that had actually been selected via Topic 100, i.e. the wrong documents had been submitted. This added slightly to the total cost for topic 101, but post hoc resimulation showed that overall e↵ect was negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ADDITIONAL RUNS</head><p>The previous section outlines in large brush strokes the primary methodology used for our o cial manual total recall run. After the o cial run was submitted we did a number of post hoc, non-o cial runs to remove the primary run's constraints and to test the e↵ect of various starting points. The first non-o cial run removed the unintentional constraints of (1) having submitting the wrong documents to the wrong topic, and (2) with more time in which to operate we increased iteration continuousness until greater than 90% of the relevant documents were found for each topic. On the intentional constraint side, we used true negatives rather than pseudo-negatives. The second non-o cial run examines the e↵ect of relaxing some of the feature frequency and feature length constraints. Frequency-wise, a wider range of terms with a document frequency between 3 and 50,000 were allowed. And term length was expanded beyond unigrams to bigrams and trigrams. Section 3.2 discusses one topic for which these expanded features were critical. We call these three runs (one o cial plus two non-o cial) "HC", "MC", and "LC", for highest constraints, medium constraints, and lowest constraints, respectively.</p><p>The third non-o cial run examines reviewer e↵ect. Using the newer, less-constrained algorithm we then compared the e↵ect of various seeding options. Specifically, we compared the e↵ect of the initialization of the runs using the contributions (manual search e↵ort) of each of the three reviewers, separately and individually, against the union thereof. Some reviewers found a higher ratio of relevantto-nonrelevant documents, some found a lower ratio. Some reviewers found more total documents, some found fewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS</head><p>The first thing that we should note is that due to the last minute nature of our e↵ort, and software that was still being written even as the experiments began, we failed to capture the exact order in which documents were judged. It is our understanding that this information will be made available by TREC at some point, but as we did not have access to that information at the time of this writing, we decided to do a re-simulation of the primary run. We knew which documents the reviewers manually examined during their sessions, so using those same documents as starting points, we set the same (fully constrained) algorithmic parameters and ran the simulated review in the same constrained manner (i.e. learning iteration ceased after a certain number of documents, approximately at the same point as was done in the o cial run). Figure <ref type="figure" coords="4,151.85,397.16,4.60,8.57" target="#fig_0">3</ref> shows a Precision-Recall curve of an average of all ten athome1 topics, with the red line as the o cial TREC result and the blue line the re-simulation. The results were close enough that we felt comfortable using the re-simulation's seen document ordering as an honest substitute to the o cial run.</p><p>Figure <ref type="figure" coords="4,91.37,459.92,4.60,8.57">5</ref> shows the results of all ten athome1 topics in the form of gain curves, with percentage of collection judged on the x-axis and recall on the y-axis. The two black lines show the gain curves of an expected linear review and a theoretical perfect run (not a single non-relevant document viewed). The red solid line is the (re-simulated) o cial run with full (highest) constraints ("HC"), the blue dashed line is the secondary run in which the main constraints of pseudonegativity and non-continuousness were removed ("MC"), and the green line the tertiary run in which document frequency constraints were removed and word bigrams and trigrams were added ("LC").</p><p>The results of the manual individual versus the joint seed set are found in the gain curves of Figure <ref type="figure" coords="4,214.42,595.91,3.58,8.57">6</ref>. The joint seeding (i.e. the union of the manual e↵ort of all three reviewers) is in black, and the various individual reviewers are in red lines of various patterns. The x-axes of these curves are not shown; the purpose of the visualization is to compare the relative rise between the various starts. As such, the data is scaled so that the range in which recall rises from 0% to approximately 90% is clearly shown. This range is, of course, di↵erent for topics of di↵erent richness levels and system performance. But again, the purpose of Figure <ref type="figure" coords="4,288.30,690.06,4.60,8.57">6</ref> is to visualize the e↵ect of various starts, not of absolute performance levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Main Experiment</head><p>From Figure <ref type="figure" coords="4,380.34,92.55,4.60,8.57">5</ref> we see that the primary, constrained approach was, with the exception of Topics 105 and 108, fairly reasonable. On Topics 101, 103, 104, and 109 the highest constrained HC approach (red line) is essentially on par with the medium constrained MC secondary approach (dotted blue line), with HC even slightly outperforming MC at mid-range recall levels. And up to about 50% recall there is little di↵erence between the two approaches for almost all of the topics.</p><p>Of course, the goal of this task is Total Recall and it is at higher recall levels that the constraints become a hindrance. A cursory post hoc analysis found that, where the MC diverged from HC was at a point at which continuous active learning was still operating for both approaches. Thus, while non-stop continuousness undoubtedly helped the secondary approach, the more important constraint seems to be the use of pseudo-negative weighting versus true negative weighting.</p><p>The least constrained LC approach (green line) is even better still for a majority of topics. On the "problematic" topics (e.g. 105) there is a massive improvement over not only HC but also over MC. On most of the remainder of the topics, LC continues to outperform the other two approaches. We note, however, that on a few topics such as 104 and 108, the LC approach, while outperforming next best alternative up to about 95% recall, drops o↵ in e↵ectiveness past that point and does not seem to recover. Perhaps the additional higher order features (bigrams and trigrams) increase the chance of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Individual Reviewer Experiment</head><p>The individual versus joint seeding experiments shown in Figure <ref type="figure" coords="4,344.84,418.08,4.60,8.57">6</ref> were all done under the MC constraints. The results here are an interesting first step. On the one hand, the overall magnitude of the di↵erences are quite small. At 50% recall, the average di↵erence across all topics between the lowest and highest performing reviewers is 671 documents (std dev 495), and at 90% recall the average di↵erence is 851 documents (std dev 797). In terms of practical impact, the di↵erence is very low. Furthermore, no matter how the process is seeded (even if there is larger or smaller variation along the way) all starts converge to relatively high recall at approximately the same point. This is not too much of a surprise, as a number of researchers <ref type="bibr" coords="4,475.08,533.15,9.72,8.57" target="#b2">[2,</ref><ref type="bibr" coords="4,488.49,533.15,7.16,8.57" target="#b3">3,</ref><ref type="bibr" coords="4,499.35,533.15,7.16,8.57" target="#b4">4]</ref> have tested the "single seed hypothesis", i.e. the notion that high recall can be achieved with a single relevant seed and continuous iteration. Thus, no matter if one seed, a dozen seeds, or a few hundred seeds are used, high recall can be achieved.</p><p>On the other hand, when there is a di↵erence between starting points, that di↵erence seems to come about by one of the "bolder" approaches. Recall from Section 1.1 that Reviewer 1 and Reviewer 3 each took "extreme" (or as extreme as one can get with a single hour's worth of work) approaches. Whereas Reviewer 2 ran straightforward relevanceseeking queries, Reviewer 1 explicitly tried to make each query as di↵erent as possible from the next query, and marked as many documents as possible, taking "scattered buckshot" approach to the task, quickly marking only those documents that appeared to be diverse from previously marked documents. Evidence of this is found in Figure <ref type="figure" coords="4,498.32,700.52,3.58,8.57">1</ref>, in that Reviewer 1 marked an average of 144.6 documents per topic in his allotted hour. Reviewer 2, on the other hand, only marked 92.5 documents on average in each hour.</p><p>At the other extreme, Reviewer 3 spent the majority of each hour (45 minutes) reading and learning about the topic itself and constructing a pseudo-relevant seed document filled with information gleaned from external sources. It was only in the last 15 minutes of each hour that Reviewer 3 even started engaging with the collection, and then not even to run any additional queries beyond the single, heavily researched initial query that had been created during the first 45 minutes of research. This approach is also reflected in the documents counts: Reviewer 3 marked on average 28.3 documents per topic, over five times fewer than Reviewer 1. Now, against this backdrop, examine Topics 104 and 109. These were the two sparsest topics in the athome1 set. And Reviewer 1's approach of "buckshot" trying to hit as many targets as possible yielded an approach that not only found more rare documents initially, but kept the lead ahead of the other two approaches up past 70% or 80% recall. On the other hand, Topics 101 and 103 are two of the richer topics, and also happen to be ones in which Reviewer 3's single seed, deeper investigatory approach, while slow in its start, caught up and surpassed the other approaches. Topic 102 is the only one for which Reviewer 2's approach maintained a lead. Reviewer 2's approach can be characterized as somewhere between Reviewer 1 and Reviewer 3 in that the quantity and diversity of initial seeds was greater than Reviewer 3, but less than Reviewer 2. As it so happens, Topic 102 also lies between the other topics in terms of its richness. For the other half of the topics (100, 105, 106, 107, 108) there was little di↵erence between the three approaches. This relationship between topic richness versus the reviewer whose seeding approach fared the best can be found in Figure <ref type="figure" coords="5,119.39,402.64,4.60,8.57">4</ref>  We believe this suggests that seeding approaches that are more diversified, scattered, higher diversity may tend to work better on lower richness topics, while seeding approaches that are deeper and more investigatory may tend to work better on higher richness topics. However, these are post hoc explanations, and the number of examples that fit this pattern is so small that it could be due to random chance. Nevertheless, it may be worth further research into when the high diversity, high volume buckshot approach (Reviewer 1) versus the deep investigatory low volume approach (Reviewer 3) versus the middle of the road (medium diversity, medium volume) approach (Reviewer 2) yields the best results.</p><p>Perhaps the more interesting question is why the union of the three reviewer seeds did not fare well. With the ex-ception of Topics 104 and 109, which were sparse enough that the manual phase alone (rather than the continuously iterative algorithmic phase) gave the union a recall boost, the union approach does not surpass, and sometimes even falls behind the best individual reviewer, such as in topics 101 and 103. The di↵erence cannot be due to di↵erence in reviewer relevance assessment, as every coding call was checked against the o cial TREC value before being used in any fashion. We have a few hypothesis, but that none that we're yet willing to commit to writing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Failure Analysis</head><p>For a final bit of discussion we wish to engage in some brief failure analysis. The following observation came from Reviewer 1, who took the buckshot approach by issuing as many queries as possible and not spending too much time engaged with any one particular query. Even when a particular query yielded rich results, he moved on from that query to others, to ensure diversity.</p><p>For topic 105 (A rmative Action), for which there were 3635 total relevant documents, two of the many queries that Reviewer 1 did were the two word phrase queries "a rmative action" and "one florida", the latter being the name of the Florida state government program related to A rmative Action. Those two queries yielded a large number of relevant documents, but in the interest of "buckshotting" the process he moved on to other queries, leaving the remainder to the continuous, algorithmic process. However, had he stuck with those two queries, which would not have been unreasonable given that 150 of the 151 documents that he examined during that hour were relevant, a post hoc analysis shows that he would have achieved the following results with the following boolean queries:</p><p>1. "one florida" = 1678 relevant out of 2739 total hits (61.3% precision at 72.2% recall)</p><p>2. "a rmative action" = 799 relevant out of 971 total hits (82.3% precision at 34.4% recall)</p><p>3. ("one florida" OR "a rmative action") = 2123 relevant out of 3,337 total hits (63.6% precision at 91.4% recall)</p><p>Thus, with absolutely no additional machine learning technology, or even ranked retrieval, the simple boolean query ("one florida" OR "a rmative action") achieves a high 91.4% recall at a not unreasonable 63.6% precision, which was actually higher than our highest constrained HC approach.</p><p>Further analysis showed that the problem likely resided with feature selection. As mentioned in Section 1.2, the HC run did no higher order feature extraction, only unigrams. And in a further naive step, unigrams with a document frequency under about a dozen, and over about ten thousand were simply ignored. This seemed to pose no problem for a number of topics (e.g. 101, 103, 109), but for others such as 105, it was likely the cause of the large loss in fidelity. Case in point: Most of the words in the ("one florida" OR "a rmative action") were ignored by the model. The collection counts (document frequency) for these four terms are:  Thus, all these terms other than "a rmative" were not available to the HC engine; they had been filtered out. We suspected that this is why Topic 105 underperformed, and was part of the motivation for doing additional runs with increasingly relaxed constraints. The additional experiments bore this fact out: The LC run in which the most constraints were removed -and specifically in which bigrams and trigrams were added -far outperformed the more limited approaches. To wit: For Topic 105, 95% recall under the HC run was achieved at rank 122,445, under MC at rank 97,112, and under LC at rank 4,332. Same exact initial manual seeds in all three approaches, just di↵erent levels of constraints in the automated continuous learning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION AND FUTURE WORK</head><p>As mentioned at the beginning, the o cial run for Team CATRES had no formal hypothesis. Rather, it was an attempt to see how well a constrained continuous active learning tool could do at achieving high recall from an ad hoc, not formally organized set of reviewers tasked with manually seeding the process. Results with the highest constrained HC run were decent, though with failures on some topics (100, 105, 108) and successes on other topics (101, 103, 109). As constraints were removed and the continuous learning process re-simulated using the exact same set of manual reviewer seed documents, certain previously failing topics garnered massive improvements.</p><p>We also found that, no matter the starting conditionwhether we started with a lot of documents found by Reviewer 1, over five times fewer documents found by Reviewer 3, or a union of all three sets of documents found by all three reviewers -high recall was achieved after approximately the same total document review cost. There is some slight variation in e↵ectiveness between the various starting conditions at mid-range recall levels, not enough evidence to draw any conclusions but enough to begin to ask more questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,53.80,259.61,239.11,8.57;4,53.80,270.07,239.11,8.57;4,53.80,280.53,173.58,8.57"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision-Recall curves averaged across all ten ath-ome1 topics. O cial run (red line) and re-simulated run ("HC") with similar parameters (blue line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,53.80,733.27,502.12,8.57;6,53.80,743.73,117.21,8.57"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Gain curves for HC run in solid red, the MC run in dashed blue, and the LC run in green. Random and theoretical perfect curves given in black.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,58.28,523.51,96.81,12.37" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,67.99,538.63,208.12,8.57;8,67.99,549.09,201.98,8.57;8,67.99,559.55,131.30,8.57;8,202.36,565.86,71.94,2.26;8,67.99,576.32,214.93,2.26;8,67.99,586.78,41.21,2.26;8,112.26,580.47,114.96,8.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,222.31,538.63,53.81,8.57;8,67.99,549.09,201.98,8.57;8,67.99,559.55,115.94,8.57">Evaluation of machine learning protocols for technology-assisted review in electronic discovery</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,202.36,565.86,71.94,2.26;8,67.99,576.32,96.95,2.26">Proceedings of the ACM SIGIR Conference</title>
		<meeting>the ACM SIGIR Conference<address><addrLine>Gold Coast, Australia; Gold Coast, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07-11">6-11 July 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,67.99,591.93,214.39,8.57;8,67.99,602.39,170.90,8.57;8,67.99,612.85,108.34,8.57;8,179.40,619.16,25.01,2.26;8,207.47,612.85,59.58,8.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,222.31,591.93,60.08,8.57;8,67.99,602.39,170.90,8.57;8,67.99,612.85,104.28,8.57">Autonomy and reliability of continuous active learning for technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,179.40,619.16,20.84,2.26">arXiv</title>
		<imprint>
			<date type="published" when="2015-04-26">April 26, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,67.99,624.30,152.09,8.57;8,67.99,641.08,188.45,2.11;8,67.99,651.54,126.21,2.11;8,197.26,645.23,59.58,8.57" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,110.69,624.30,105.32,8.57">The single seed hypothesis</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dimm</surname></persName>
		</author>
		<ptr target="http://blog.cluster-text.com/2015/04/25/the-single-seed-hypothesis" />
		<imprint>
			<date type="published" when="2015-04-25">April 25, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,67.99,656.68,218.74,8.57;8,67.99,667.14,212.80,8.57;8,67.99,677.60,28.43,8.57;8,99.48,683.92,192.64,2.11;8,67.99,694.38,183.25,2.11;8,67.99,704.84,125.21,2.11;8,196.26,698.53,71.22,8.57;8,67.99,708.99,96.75,8.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,246.17,656.68,40.57,8.57;8,67.99,667.14,212.80,8.57;8,67.99,677.60,24.37,8.57">Predictive coding 2.0: New and better approaches to non-linear review</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tredennick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pickens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eidelman</surname></persName>
		</author>
		<ptr target="http://www.legaltechshow.com/r5/cob_page.asp" />
	</analytic>
	<monogr>
		<title level="m" coord="8,82.09,694.38,169.15,2.11;8,67.99,704.84,78.84,2.11">?category_id=72044&amp;initial_file=cob_ page-ltech_agenda</title>
		<imprint>
			<date type="published" when="2012-01-31">January 31. 2012</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>LegalTech Presentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
