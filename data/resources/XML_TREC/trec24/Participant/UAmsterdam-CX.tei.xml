<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.21,85.26,335.29,5.54;1,190.30,105.19,229.11,5.54">Parsimonious User and Group Profiling in Venue Recommendation</title>
				<funder ref="#_wJzXj5Q">
					<orgName type="full">Netherlands Organization for Scientific Research</orgName>
				</funder>
				<funder ref="#_JHqDV4p">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder ref="#_XMmNC5e">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,132.32,135.09,117.78,14.80"><forename type="first">Seyyed</forename><forename type="middle">Hadi</forename><surname>Hashemi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.66,135.09,96.66,14.80"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.87,135.09,66.21,14.80"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.21,85.26,335.29,5.54;1,190.30,105.19,229.11,5.54">Parsimonious User and Group Profiling in Venue Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FDA52806703E7BD12AC36AF37FA24B21</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the University of Amsterdam's participation in the TREC 2015 Contextual Suggestion Track. Creating e↵ective profiles for both users and contexts is the main key to build an e↵ective contextual suggestion system. To address these issues, we investigate building users' and groups' profiles for e↵ective contextual personalization and customization. Our main aim is to answer the questions: How to build a user-specific profile that penalizes terms having high probability in negative language models? Can parsimonious language models improve user and context profile's e↵ectiveness? How to combine both models and benefit from both a contextual customization using contextual group profiles and a contextual personalization using users profiles? Our main findings are the following: First, although using parsimonious language model leads to a more compact language model as users' profiles, the personalization performance is as good as using standard language models for building users' profiles. Second, we extensively analyze e↵ectiveness of three di↵erent approaches in taking the negative profiles into account, which improves performance of contextual suggestion models that just uses positive profiles. Third, we learn an e↵ective model for contextual customization and analyze the importance of different contexts in contextual suggestion task. Finally, we propose a linear combination of contextual customization and personalization, which improves the performance of contextual suggestion using either contextual customization or personalization based on all the common used IR metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this paper, we present the University of Amsterdam participation in the TREC 2015 Contextual Suggestion Track. The main goal of this track is to investigate search techniques for complex information needs that are highly dependent on context and user interests. In each run, participants have to produce a ranked list of suggestions for each pair of profile and context.</p><p>Each profile corresponds to a user who has judged suggestions given in a specific context. The user profiles contain a five-point scale rating for each pair of profile and example suggestion. The context provided in TREC 2015 is richer than the context being used in previous years <ref type="bibr" coords="1,271.44,658.68,7.15,8.57" target="#b0">[1]</ref><ref type="bibr" coords="1,278.59,658.68,3.58,8.57" target="#b1">[2]</ref><ref type="bibr" coords="1,282.17,658.68,7.15,8.57" target="#b2">[3]</ref>. In TREC 2012, Contextual Suggestion organizers provided contexts having geographical and temporal aspects. However, judging temporal aspects of the context was di cult for the NIST assessors, so it has not been used for the TREC 2013 and TREC 2014.</p><p>In TREC 2015, the contextual suggestion organizers provide a city the user is located in, a trip type, a trip duration, a type of group the person is travelling with, and the season the trip will occur in as contexts of the venue recommendation. Hopefully, almost all of the given contextual suggestion requests have information about all types of the mentioned contexts, which makes it a very interesting data to test contextual suggestion systems.</p><p>TREC 2015 Contextual Suggestion track's setup is different from the previous tracks. In TREC 2013 and 2014, participants used to submit two di↵erent kinds of runs in the Contextual Suggestion Track: 1) open web, or 2) Clue-Web12. However, in TREC 2015, contextual suggestion track organizers distributed a data collection, by running contextual suggestion pre-TREC task, among the participants.</p><p>In addition, in TREC 2015, participants were allowed to participate in the contextual suggestion live or batch experiments. In the live experiment, participants return a list of attraction IDs from the TREC 2015 contextual suggestion collection, but in the batch experiment, participant rank attraction IDs that have been suggested during the live experiment. In this paper, we discuss our participation in the batch experiment that helps us to test our contextual suggestion approach.</p><p>In this paper, our main aim is to study the question: How to build e↵ective users' and contextual groups' profiles for improving contextual suggestion? Specifically, we answer the following research questions:</p><p>1. How to build an e↵ective user profile for suggestion candidates personalization? The rest of this paper is organized as follows. In Section 2 we review some related work on Contextual Suggestion track. In Section 3, we detail our models of Contextual Suggestion, and Section 4 is devoted to the experimental results. Finally, we present the conclusions and future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In the TREC 2012 Contextual Suggestion Track, participants were allowed to use the open web to retrieve suggestion candidates. All of them used the webpages of the aggregator websites such as Yelp, Google Places, Foursquare and Trip Advisor. A considerable fraction of the participants used category of suggestion candidates that is available in the Yelp website. In that track, the given context had geographical and temporal aspects.</p><p>In the TREC 2013 and TREC 2014, the participants could use either the open web or the ClueWeb12 dataset, but there were only seven submitted runs out of 34 in 2013 and 6 out of 31 in 2014 that were ClueWeb12 runs <ref type="bibr" coords="2,237.34,263.46,9.20,8.57" target="#b1">[2]</ref>. The common approach of the open web runs were retrieving a bag of relevant venues to the given context based on the aggregators' API such as Yelp API, and then re-rank the suggestion candidates based on the user profiles and/or the suggestion categories.</p><p>As the most related work, in TREC 2014, University of Amsterdam experimented with the use of anchor text representations in the language modeling framework, and they analyzed e↵ects of using positive, neutral, and negative profiles in personalization of the suggestion candidates <ref type="bibr" coords="2,256.02,368.07,9.20,8.57" target="#b4">[5]</ref>. However, the TREC Contextual Suggestion test collection was not reusable <ref type="bibr" coords="2,107.20,388.99,9.20,8.57" target="#b5">[6]</ref>, and they could not test di↵erent types of ratings of example suggestions. In this paper, we analyze our proposed approach in using both positive and negative profiles in personalization and customization of suggestion candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">APPROACH</head><p>Our contribution in Contextual Suggestion Track has two main parts. One part is personalization of suggestions candidates using users' profiles, which includes just users' preferences in rating example suggestions. The other part is customization of suggestion candidates based on the given contextual information like trip duration and type, and users' profile including their age and gender, but not their preferences in rating example suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parsimonious language models</head><p>In our proposed models in contextual personalization and customization, we have used parsimonious language models. Therefore, in this section, we first detail parsimonious language models.</p><p>Parsimonious language model is introduced by <ref type="bibr" coords="2,255.78,616.83,9.71,8.57" target="#b6">[7]</ref> in the context of information retrieval which aims to make a compact and precise estimation. In parsimonious language model, given a raw probabilistic estimation, i.e. standard language model, the goal is to reestimate the model so that nonessential terms of the raw estimation are eliminated with regards to a background language model. Generally, in parsimonious language model, probability of terms that are common in the background estimation are pushed to zero. In other words, parsimonious language model represent a document with its terms which make it distinguishable from other documents in the collection by penalizing raw inference of terms that are better explained by the background estimation.</p><p>In order to achieve this, an Expectation-Maximization algorithm is employed to reestimate the language model. Consider t determines a possible term and ✓d is a parsimonious language model of document d. The probability of each term t given parsimonious language model ✓d , P (t| ✓d ) is calculated by iterating through the following steps:</p><formula xml:id="formula_0" coords="2,333.13,175.06,222.78,61.92">E step : et = tf t,d • ↵P (t| ✓d ) ↵P (t| ✓d ) + (1 ↵)P (t|✓C ) (1) M step : p(t| ✓d ) = et P t 0 2V e 0 t (2)</formula><p>where, V is the set of all terms with non-zero probability in the initial estimation and ✓C shows the collection language model as background estimation. In the initial E-step, P (t| ✓d ) is estimated using maximum likelihood estimation.</p><p>In the E-step, terms with high probability in the document language model having relatively high probability in the background language model are penalized, and continuing iteration, their probability adjusted toward zero. This way, terms which do not explain the document model in a specific way are discarded. After each M-step, terms with probability bellow a predefined threshold are eliminated. ↵ in Equation 2 controls the level of parsimonization so that the lower values of ↵ result in a more parsimonious language models. The iteration is repeated a fixed number of iterations or until the estimates do not change significantly anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Personalization using PLM</head><p>In this section, we propose how to apply parsimonious language models (PLM) in contextual suggestion as a personalization problem. To this aim, we have used parsimonious language model for building a user's profile in order to estimate relevance of suggestion candidates to the given user. Then, we estimate relevance of each suggestion candidate to the given user by KL-divergence of the user's PLM and standard language model of the suggestion candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLM1.</head><p>We first detail our first submission in TREC 2015 (i.e., PLM1), in which we have used parsimonious language model in order to build more specific and smaller positive profiles for users. To this aim, example suggestions being rated higher than 2 are considered as evidences to build positive profiles as mixture language model of their positive preferences using rates as the mixture weight. At the final step of this model, parsimonious language model of the positive mixture LM is estimated considering the language model using of all the preferences as the background model.</p><p>The same procedure is done on tags assigned to each example suggestion by users, instead of terms in the corresponding web pages. Therefore, we have two users' positive profiles, which are based on either rated web pages' contents or tags given by a user, for estimation of the final relevance of a suggestion candidate to the user. We simply calculate the final score as the linear combination of the relevance of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLM2.</head><p>In PLM2, we tried to build even more specific language models for the positive profiles of users in comparison to the PLM1. To this aim, first we estimate negative language model using low-rated preferences (with rates under 2) and then parsimonize PLM1 considering the negative language model as the background model. Therefore, we penalize terms that are better explained not only in the collection language model, but also in the negative profile language model.</p><p>The same procedure is done on tags assigned to each example suggestion by users, instead of terms in the rated web pages. Same as PLM1, the final score of PLM2 is calculated by the linear combination of the relevance of the suggestion candidate to the profile based on web pages' contents and the profile based on users' given tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Customization using PLM</head><p>In this section, we detail our proposed contextual customization approach using parsimonious language models of di↵erent contexts. TREC 2015 contextual suggestion track provides a reach set of contextual information for almost all of the users' requests, which makes it possible to test our approach in this test collection.</p><p>In order to customize suggestion candidates, parsimonious language models of each class of given contexts are built prior to doing customization for each request. Specifically, PLM of di↵erent group of people (e.g., Family), season (e.g., Spring), trip type (e.g., Holiday), trip duration (e.g., Weekend), person age (i.e., discrimination of age to 6 di↵erent ranges, which is shown in Table <ref type="table" coords="3,183.11,606.37,3.58,8.57" target="#tab_0">1</ref>), and person gender (e.g., Female) are built. Then, in each request, we estimate relevance of the given suggestion candidates to the di↵erent given contexts by calculating the KL-divergence of the standard language models of suggestion candidates and the PLM of di↵erent contexts that was built in advance.</p><p>In order to have the most e↵ective contextual customization using the best linear combination of contextual relevance of di↵erent contexts to the given suggestion candidate, the pairwise SVM rank learning to rank model is used <ref type="bibr" coords="3,53.80,710.98,9.20,8.57" target="#b7">[8]</ref>. According to users' preferences in rating example sug-gestions, it is straightforward to assume rank preferences of example suggestions for the given request.</p><p>In order to do a pairwise learning to rank for contextual customization, we represent each suggestion si for the given contextual request rj by a feature vector vij. Learning in this pairwise approach is finding a vector w such that the maximum number of rank preference constraints are satisfied on the given training set. Specifically, for each satisfied constraint: {(s1, s2)|s1 2 S(rj), s2 2 S(rj), Pr j (s1) &gt; Pr j (s2)} () wv1j &gt; wv2j, in which, S(rj) is a set of example suggestion rated for the given request rj, and Pr j is a set of preference ratings for the given request rj. We have used SVM rank algorithm to estimate the optimal value of w. By having an optimal w, we use wvij as estimation of the relevance of a suggestion candidate si to the given request rj.</p><p>A further study on how to estimate group profiles is done in <ref type="bibr" coords="3,328.42,263.10,9.20,8.57" target="#b3">[4]</ref>, in which in addition to penalizing general terms in group profiles, user specific terms is also penalized to improve group profiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Linear Combination of Suggestion Personalization and Customization</head><p>In order to have a contextual suggestion model, which benefit from both contextual customization and personalization, a linear combination of them is used to fuse the contextual personalization with the contextual customization suggestion ranking. To this aim, the following equation is used to combine the two rankings: p(s|r, ) = p Personalization (s|r) + ( <ref type="formula" coords="3,457.35,398.90,4.09,8.57">1</ref>)pCustomization (s|r), where p(s|r, ) is the relevance probability of a suggestion s to a request r based on the weight and 1 given to Personalization and Customization rankings. In this paper, we have used the reciprocal rank of suggestions for estimating their relevance probability to the given request. The performance of this model and the optimal parameter is discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we mainly answer our research questions. In fact, we answer the research question: How to build effective users' and contextual groups' profiles for improving contextual suggestion?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Personalization</head><p>This section studies our first research question: How to build an e↵ective user profile for suggestion candidates personalization? Building e↵ective profiles for the given users is one of the key steps of doing content-based recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">PLM based Personalization vs. SLM based Personalization</head><p>We first look at the question: Is personalization based on parsimonious user profiling able to improve baseline personalization model in Contextual Suggestion? As it is mentioned in Section 3, we proposed using parsimonious language model to built users' specific language models to have a more compact and as e↵ective users profiles as using the Table <ref type="table" coords="4,89.26,180.89,4.60,8.57" target="#tab_1">2</ref> shows that using parsimonious language model considering the collection language model as a background language model (PLM1) is performing almost same as the baseline contextual suggestion using standard language model of users' positive preferences (i.e., LMP ). However, the model using parsimonious language model of a user, considering the user negative profile as a background language model, performs better than the other two models, and improves the baseline. Specifically, it improves the baseline 2% in terms of P @5 and 5% in terms of MRR. This experiment also shows that negative users' preferences is an important source of information that should be used in contextual suggestion problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Impact of Using Negative Profiles</head><p>This section answers our research question: Does considering negative profiles improve baseline contextual suggestion using positive profiles?</p><p>To this aim, we have used 3 di↵erent approaches to take the negative users preferences in to account in doing contextual suggestion. The first one is the P LM2, in which we have estimated parsimonious language model of P LM1 by considering negative profiles as the background language models. This approach makes very specific and compact language models of users positive preferences.</p><p>The second approach for considering negative preferences in the personalization step is building a parsimonious language model of users' positive preferences by considering negative profiles' language models as background language models. This time, we do not build parsimonious language model by considering both collection language model and negative profiles' language model as background language models, but by just considering negative profiles' language model as background language models. This model is less user specific in comparison to the first model and we have tried to just penalize terms in the negative users' profiles, but not the terms in the neutral users' profiles.</p><p>The third model (LMN ) is based on using standard language models for both positive and negative users' profiles, and simply using the following linear combination for computing the final suggestions' relevance to the given request: score final (s|r) = scorepositive(s|r) scorenegative(s|r), in which, scorepositive(s|r) and scorenegative(s|r) are the relevance score of a suggestion candidate s to the given positive and negative profile of a request r.</p><p>According to Table <ref type="table" coords="4,144.12,669.14,3.58,8.57" target="#tab_2">3</ref>, the last model (i.e., LMN ), which uses larger language models to compute the relevance of suggestion candidates to the given request and does not use parsimonious language models for building users' profiles, is performing better than the other two approaches in terms of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Customization</head><p>This section studies the impact of contextual suggestion customization on the overall performance of contextual suggestion, trying to answer our second research question: What is the e↵ect of using contextual customization based on parsimonious group profiling in overall performance of contextual suggestion?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">PLM based Customization</head><p>We first look at the question: How e↵ective is doing contextual customization for contextual suggestion? To this aim, we have filtered those requests, which do not have all the contextual information defined in the TREC contextual suggestion track. Therefore, experiments of this section is done based on 191 out of 211 requests provided by the track organizers. In order to build precise profiles for di↵erent classes of di↵erent contexts, we have used parsimonious language model. In fact, unlike users' positive profiles, which include just less than or equal to 30 rated web pages, contexts' profiles are made of much more web pages, which makes it more reasonable to use parsimonious language model for making a more compact language models having less terms from other contexts.</p><p>As it is shown in Table <ref type="table" coords="4,426.96,533.27,3.58,8.57" target="#tab_3">4</ref>, although we do not use user specific ratings, the PLM based customization (P LMC ) is performing better than the baseline personalization based on standard language models of positive profiles (i.e., LMP ) in ranking suggestion candidates.</p><p>This experiment indicates that the contextual information is a very important source of information that can be used to customize suggestions for di↵erent group of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Context Importance</head><p>We second look at the question: What is the most important context among the contexts being used in TREC Contextual Suggestion Track? As we mentioned in Section 3, we have learned a w weight vector for an optimal contextual customization of the suggestion candidates. Based on the learned weight vector, this is very interesting to know that the trip duration is the most important context in the contextual customization of the suggestions, and the least important one among the provided contexts is the trip type. Figure <ref type="figure" coords="5,83.60,480.87,4.60,8.57">2</ref> demonstrates the relative importance of di↵erent contexts being examined in this track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Linear Combination of Customization and Personalization</head><p>We now look at the question: What is the e↵ect of using a linear combination of contextual customization and contextual personalization for the contextual suggestion? As the contextual customization is using a di↵erent layer of suggestion candidates personalization based on the example suggestions rated by similar requests, and on the other hand, the proposed personalization approaches just use users' preferences but not other provided contexts of the requests, combination of these two suggestions rank list can improve the overall performance of the contextual suggestion.</p><p>To this aim, as we mentioned in Section 3, we have used a linear combination of the customization and personalization approach, whose optimal parameter is being analyzed in Figure <ref type="figure" coords="5,93.60,669.14,3.58,8.57">1</ref>. In this experiment, we have used our best personalized contextual suggestion ranker (i.e. LMN ) as a personalized ranked list, and fuse it by our proposed contextual customization ranked list (i.e., P LMC ). Figure <ref type="figure" coords="5,243.34,700.52,4.60,8.57">1</ref> shows that a linear combination of the customized ranking and the per-sonalized one is performing better than our best contextual personalization approach in all the common IR metrics being used in this paper. Specifically, considering = 0.7, P LMLC , whose P @5 is 0.5770, is our best contextual suggestion run in term of P @5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we studied Contextual Suggestion problem through building e↵ective language model based profiles for both users and contexts. According to our experiments in doing contextual personalization, although parsimonious language models does not help much in improving baseline contextual suggestion ranking, it provides a more compact profiles, faster personalization approach with an acceptable e↵ectiveness in personalizing search result. Moreover, we discussed three di↵erent contextual suggestion approaches taking negative users' profiles into account, and conclude that the one using standard language models and simply subtracting suggestion candidates' relevance to the negative profiles from the positive profiles, performs better than the ones using parsimonious language models to penalize terms in negative profiles. In addition, we proposed a contextual customization approach using parsimonious language model to build contexts profiles. We learned an optimal weights of suggestion candidates relevance to the contexts by using pairwise SVM learning to rank model. We observed that the trip duration is the most important context in estimating the relevance of suggestion candidates to the given requests. The contextual customization performs almost as effective as the contextual personalization, which shows that the contextual customization is quite promising in solving cold-start problem in the contextual suggestion task. Finally, we have analyzed that the linear combination of the contextual customization and personalization performs better than the others by having P @5 = 0.5770 for the = 0.7. As a future work, we continue to work on building a more effective contextual customization approach by clustering contextual requests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,342.72,540.97,213.97,8.97;1,358.95,551.43,205.43,8.97;1,358.95,561.90,105.63,8.97;1,342.21,574.67,213.71,8.97;1,358.95,585.13,196.14,8.97;1,327.59,598.47,228.32,8.97;1,339.23,608.93,216.69,8.97;1,339.23,619.39,140.99,8.97;1,342.72,632.73,213.20,8.97;1,358.95,643.19,104.94,8.97;1,342.21,655.97,213.71,8.97;1,358.95,666.43,196.96,8.97;1,358.95,676.89,46.05,8.97;1,343.23,689.66,212.69,8.97;1,358.95,700.13,196.96,8.97;1,358.95,710.59,171.44,8.97"><head>( a ) 2 .</head><label>a2</label><figDesc>Is personalization based on parsimonious user profiling able to improve baseline personalization model in Contextual Suggestion? (b) Does considering negative profiles improve baseline contextual suggestion using positive profiles? What is the e↵ect of using contextual customization based on parsimonious group profiling in overall performance of contextual suggestion? (a) How e↵ective is doing contextual customization for contextual suggestion? (b) What is the most important context among the contexts being used in TREC Contextual Suggestion Track? (c) What is the e↵ect of using a linear combination of contextual customization and contextual personalization for the contextual suggestion?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,155.77,200.99,196.60,8.57;5,355.44,207.31,5.88,2.26;5,362.60,200.99,14.34,8.57;5,380.00,207.31,26.50,2.26;5,409.57,200.99,14.82,8.57;5,427.45,207.31,22.67,2.26;5,451.40,200.99,2.56,8.57"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Impact of relative fusion weight ( ) on P @5, MRR, and MAP .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,53.80,54.79,239.10,222.44"><head>Table 1 :</head><label>1</label><figDesc>Discrimination of users' age to 6 age range groups Assuming the fact that the given tags by the users are more likely to be a clear indicator of their preferences compared to the noisy content of webpages, in our experiments, we have heuristically considered ↵ = 1 and = 2.</figDesc><table coords="3,53.80,78.46,239.10,149.79"><row><cell></cell><cell cols="2">group</cell><cell>age range</cell></row><row><cell></cell><cell cols="3">range0 [0,18)</cell></row><row><cell></cell><cell cols="3">range1 [18,26)</cell></row><row><cell></cell><cell cols="3">range2 [26,34)</cell></row><row><cell></cell><cell cols="3">range3 [34,42)</cell></row><row><cell></cell><cell cols="3">range4 [42,50)</cell></row><row><cell></cell><cell cols="3">range5 [50,1)</cell></row><row><cell cols="4">the suggestion candidate to the profile based on the content</cell></row><row><cell cols="2">of web pages (i.e., score</cell><cell cols="2">webpages ) and the profile based on</cell></row><row><cell cols="4">users' given tags (i.e., scoretags):</cell></row><row><cell>score</cell><cell cols="2">final = ↵score</cell><cell>webpages + scoretags.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,54.79,239.11,124.21"><head>Table 2 :</head><label>2</label><figDesc>E↵ects of using parsimonious language model in modeling users' profiles</figDesc><table coords="4,53.80,88.92,239.10,90.09"><row><cell cols="2">Method p@5</cell><cell>MRR</cell><cell>MAP</cell></row><row><cell>LMP</cell><cell>0.5194</cell><cell>0.6839</cell><cell>0.5498</cell></row><row><cell>PLM1</cell><cell>0.5204</cell><cell>0.6765</cell><cell>0.5523</cell></row><row><cell>P LMN</cell><cell cols="3">0.5270 0.7174 0.5619</cell></row><row><cell cols="4">standard language model for giving fast e↵ective suggestions</cell></row><row><cell>for the given request.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,316.81,54.79,239.10,89.05"><head>Table 3 :</head><label>3</label><figDesc>E↵ects of using negative profiles in building users' profiles</figDesc><table coords="4,360.23,88.92,152.27,54.92"><row><cell cols="2">Method p@5</cell><cell>MRR</cell><cell>MAP</cell></row><row><cell>LMP</cell><cell>0.5194</cell><cell>0.6839</cell><cell>0.5498</cell></row><row><cell>PLM2</cell><cell>0.5024</cell><cell>0.6734</cell><cell>0.5483</cell></row><row><cell>P LMN</cell><cell>0.5270</cell><cell cols="2">0.7174 0.5619</cell></row><row><cell>LMN</cell><cell cols="2">0.5526 0.7031</cell><cell>0.5684</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,316.81,158.72,239.10,121.86"><head>Table 4 :</head><label>4</label><figDesc>Contextual customization vs. personalization MAP . However, in terms of MRR, the P LMN , which does not penalize terms in the neutral profiles but penalize terms in the negative profiles, is performing better than the other two approaches.</figDesc><table coords="4,316.81,182.38,189.88,66.81"><row><cell cols="2">Method p@5</cell><cell>MRR</cell><cell>MAP</cell></row><row><cell>LMP</cell><cell cols="3">0.5298 0.6866 0.5575</cell></row><row><cell>P LMC</cell><cell cols="3">0.5277 0.7004 0.5642</cell></row><row><cell>P @5 and</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is funded in part by the <rs type="funder">European Community</rs>'s <rs type="programName">FP7</rs> (project meSch, grant # <rs type="grantNumber">600851</rs>) and the <rs type="funder">Netherlands Organization for Scientific Research</rs> (<rs type="projectName">ExPoSe</rs> project, NWO CI # <rs type="grantNumber">314.99.108</rs>; <rs type="projectName">DiLiPaD</rs> project, NWO Digging into Data # <rs type="grantNumber">600.006.014</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JHqDV4p">
					<idno type="grant-number">600851</idno>
					<orgName type="program" subtype="full">FP7</orgName>
				</org>
				<org type="funded-project" xml:id="_wJzXj5Q">
					<idno type="grant-number">314.99.108</idno>
					<orgName type="project" subtype="full">ExPoSe</orgName>
				</org>
				<org type="funded-project" xml:id="_XMmNC5e">
					<idno type="grant-number">600.006.014</idno>
					<orgName type="project" subtype="full">DiLiPaD</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,67.99,71.88,224.91,8.57;6,67.99,82.34,224.91,8.57;6,67.99,92.40,217.89,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,137.39,82.34,155.51,8.57;6,67.99,92.80,63.77,8.57">Overview of the TREC 2012 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.18,92.40,106.43,8.97">Proceedings of TREC 2012</title>
		<meeting>TREC 2012</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,110.43,224.91,8.57;6,67.99,120.89,224.91,8.57;6,67.99,130.96,224.90,8.97;6,67.99,141.42,74.76,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,196.09,120.89,96.81,8.57;6,67.99,131.35,137.30,8.57">Overview of the TREC 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,234.08,130.96,58.82,8.97;6,67.99,141.42,46.48,8.97">Proceedings of TREC 2013</title>
		<meeting>TREC 2013</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,159.45,224.91,8.57;6,67.99,169.91,224.91,8.57;6,67.99,179.98,217.89,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,137.39,169.91,155.51,8.57;6,67.99,180.37,63.77,8.57">Overview of the TREC 2014 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.18,179.98,106.43,8.97">Proceedings of TREC 2014</title>
		<meeting>TREC 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,198.00,224.91,8.57;6,67.99,208.46,224.91,8.57;6,67.99,218.53,224.91,8.97;6,67.99,229.39,34.76,8.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,67.99,208.46,209.55,8.57">Generalized group profiling for content customization</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marx</surname></persName>
		</author>
		<idno type="DOI">10.1145/2854946.2855003</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,67.99,218.53,102.49,8.97">Proceedings of CHIIR &apos;16</title>
		<meeting>CHIIR &apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,247.02,224.91,8.57;6,67.99,257.09,224.91,8.97;6,67.99,267.55,112.33,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,198.24,247.02,94.67,8.57;6,67.99,257.48,147.82,8.57">Venue recommendation and web search based on anchor text</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,235.15,257.09,57.75,8.97;6,67.99,267.55,107.16,8.97">23rd Text REtrieval Conference (TREC)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,285.58,224.91,8.57;6,67.99,296.04,224.91,8.57;6,67.99,306.10,224.90,8.97;6,67.99,316.56,224.91,8.97;6,67.99,327.02,167.87,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,143.06,296.04,149.84,8.57;6,67.99,306.50,29.13,8.57">On the reusability of open test collections</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiseleva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,118.02,306.10,174.87,8.97;6,67.99,316.56,224.91,8.97;6,67.99,327.02,77.10,8.97">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="827" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,345.05,224.91,8.57;6,67.99,355.12,224.91,8.97;6,67.99,365.58,224.99,8.97;6,67.99,376.04,224.91,8.97;6,67.99,386.50,190.46,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,257.15,345.05,35.75,8.57;6,67.99,355.51,188.57,8.57">Parsimonious language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,275.27,355.12,17.64,8.97;6,67.99,365.58,224.99,8.97;6,67.99,376.04,224.91,8.97;6,67.99,386.50,99.94,8.97">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;04</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,404.14,224.91,8.97;6,67.99,414.60,224.91,8.97;6,67.99,425.06,224.91,8.97;6,67.99,435.91,58.77,8.57" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,122.04,404.53,135.18,8.57">Training linear svms in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,275.27,404.14,17.64,8.97;6,67.99,414.60,224.91,8.97;6,67.99,425.06,195.91,8.97">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
