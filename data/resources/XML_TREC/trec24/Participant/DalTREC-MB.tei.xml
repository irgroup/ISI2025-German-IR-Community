<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.60,110.17,427.41,18.08;1,79.30,135.67,475.88,18.08">Real Time Filtering of Tweets Using Wikipedia Concepts and Google Tri-gram Semantic Relatedness</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.72,190.18,42.43,8.29"><forename type="first">Anh</forename><surname>Dang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<addrLine>6050 University Avenue</addrLine>
									<postCode>B3H 4R2</postCode>
									<settlement>Halifax</settlement>
									<region>NS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,141.14,190.18,62.55,8.29"><forename type="first">Raheleh</forename><surname>Makki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<addrLine>6050 University Avenue</addrLine>
									<postCode>B3H 4R2</postCode>
									<settlement>Halifax</settlement>
									<region>NS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.71,190.18,87.28,8.29"><forename type="first">Abidalrahman</forename><surname>Moh'd</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<addrLine>6050 University Avenue</addrLine>
									<postCode>B3H 4R2</postCode>
									<settlement>Halifax</settlement>
									<region>NS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,311.24,190.18,56.25,8.29"><forename type="first">Aminul</forename><surname>Islam</surname></persName>
							<email>islam@cs.dal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<addrLine>6050 University Avenue</addrLine>
									<postCode>B3H 4R2</postCode>
									<settlement>Halifax</settlement>
									<region>NS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.67,190.18,53.34,8.29"><forename type="first">Vlado</forename><surname>Keselj</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<addrLine>6050 University Avenue</addrLine>
									<postCode>B3H 4R2</postCode>
									<settlement>Halifax</settlement>
									<region>NS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,458.89,190.18,82.99,8.29"><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Milios</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<addrLine>6050 University Avenue</addrLine>
									<postCode>B3H 4R2</postCode>
									<settlement>Halifax</settlement>
									<region>NS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,103.60,110.17,427.41,18.08;1,79.30,135.67,475.88,18.08">Real Time Filtering of Tweets Using Wikipedia Concepts and Google Tri-gram Semantic Relatedness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">305EF38D8CE3A02E353D1EB43A01B372</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the mobile notification and email digest tasks in the TREC 2015 Mircoblog track. The tasks are about monitoring Twitter stream and retrieving relevant tweets to users' interest profiles. Interest profiles contain the description of a topic that the user is interested in receiving relevant posts in real-time. Our proposed approach extracts Wikipedia concepts for profiles and tweets and applies a corpus-based word semantic relatedness method to assign tweets to their relevant profiles. This approach is also used to determine whether two tweets are semantically similar which in turn prevents the retrieval of redundant tweets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The rapid growth of microblogs' popularity greatly promotes the importance of information retrieval systems that suggest relevant content to users with respect to their interests in real-time. The TREC 2015 Microblog track presents two scenarios for the real-time retrieval task: mobile notification (Scenario A) and email digest (Scenario B). In both scenarios, the goal is to retrieve interesting and novel tweets relevant to users' interests profiles which are the description of the topic the user is interested in. In the mobile notification scenario, selected tweets by the system should be pushed to the user's mobile phone as notification relatively short after these tweets are published, while in the email digest scenario, interesting tweets are accumulated in an email and then delivered to the user at the end of the day. This year's Microblog track guidelines 1 specifies that each team can submit up to three runs per scenario and each run should be assigned to one of the three different categories of the amount of human involvement. In the automatic run, no human intervention is allowed, while in the manual preparation and manual intervention run, human supervision is acceptable before the start of the evaluation period and all the time respectively. We participated in both mobile notification and email digest scenarios and submitted the runs under our group name "DALTREC".</p><p>Our proposed approach for this year's filtering task is based on using Wikipedia and Google Trigram for calculating the semantic relatedness between tweets and profiles and also between tweets themselves. Due to the short and noisy nature of Twitter posts and the vocabulary mismatch between the interest profiles and tweets, it is important to consider semantic similarity of tweets to profiles in order to achieve proper recall. We apply the proposed approach to both scenarios considering both automatic and manual preparation runs. In 1 https://github.com/lintool/twitter-tools/wiki/TREC-2015-Track-Guidelines addition, to compare the results of the similarity computation with a standard Ad Hoc information retrieval method, we also applied the Lucene<ref type="foot" coords="1,413.25,262.22,3.36,5.57" target="#foot_0">2</ref> implementation of unigram language model to Scenario B and submitted the results.</p><p>The rest of this paper is organized as follows. The proposed strategies are explained in Section II. Section III describes the evaluation metrics used for ranking participants in this year's track. The results of our participation is discussed in Section IV and conclusions are provided in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>This section describes our method for the real-time filtering task. The first 4 sections describes the core of the system for both automatic and manual runs. Section II-F describes the user involvement for our manual run submissions and Section II-G explains a language model strategy that we applied to Scenario B. Our proposed framework and its sections are shown in Figure <ref type="figure" coords="1,389.52,430.54,3.38,7.53" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preprocessing</head><p>For data processing, we filter out all stop words, non-English tweets, and retweets. After that each tweet and profile are represented as a bag of concepts based on Wikipedia entity linking and we compute semantic similarity between two Wikipedia bags of concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Wikipedia Entity Linking</head><p>As the tweet content is very sparse and noisy, we try to capture the most important topics from tweets using Dexter Wikipedia entity linking <ref type="bibr" coords="1,422.71,571.52,9.67,7.53" target="#b0">[1]</ref>. For Dexter Wikipedia linking, the input is a tweet t i and the output is a list of potential Wikipedia entity tags. Each tweet t i is represented as a vector {c i1 , c i2 ,. . . ,c in } where c i j is a concept from Wikipedia extracted for tweet t i . Furthermore, we filtered all tweets that do not have any Wikipedia entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Similarity Calculation Between Tweets and Users' Profiles</head><p>The goal of the system is to recommend interesting content to users with regard to their interests. We propose the use of semantic similarity for this task. For example, if a user is interested in the topic "airline merger", she may also be interested in "airline integration". Semantic similary between two texts is computed using Google Tri-gram Method (GTM) of Aminul et. al <ref type="bibr" coords="2,272.60,482.75,9.67,7.53" target="#b1">[2]</ref>. GTM is an unsupervised corpus-based approach for computing semantic relatedness between texts. It uses the uni-grams and tri-grams of the Google Web 1T N-grams corpus <ref type="bibr" coords="2,285.98,512.74,10.58,7.53" target="#b2">[3]</ref> to calculate the relatedness between words, and then extends that to longer texts. The Google Web 1T N-grams corpus contains the frequency count of English word n-grams (unigrams to 5-grams) computed over one trillion words from web page texts collected by Google in 2006. To compute the similarity score between a tweet and a profile, we use Wikipedia entity linking method and represent them as bag of concepts and then compute the semantic similarity between two Wikipedia bags of concepts using GTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Filtering Semantically Similar Tweets</head><p>A satisfactory tweet recommendation system should not suggest redundant tweets to users. Therefore, this year's evaluation workflow considers penalties for semantically similar tweets. First, tweets are clustered semantically and within each cluster, only the earliest tweet is considered novel and other tweets in the same cluster are considered redundant.</p><p>We propose the use of both lexical similarity and semantic similarity for identifying semantically similar tweets. As described in Section II-B, we represent each tweet with their Wikipedia concepts. We use the GTM method to calculate the semantic similarity between two tweets t 1 = {c 11 , c 12 , ..., c 1n } and t 2 = {c 21 , c 22 , ..., c 2n }. To compute lexical similarity between two texts, each tweet is represented as a IF-IDF vector, and their cosine value is considered as their similarity score. We consider two tweets to be semantically similar if both their lexical similarity score and semantic similarity score are more than predefined thresholds, i.e. GT M(t 1 ,t 2 ) &gt;= α and Cosine(t 1 ,t 2 ) &gt;= β , where α and β are two thresholds for the semantic similarity and cosine similarity respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Automatic Submissions</head><p>Having all the modules that calculate the most similar profile to a tweet and identify semantically similar tweets, we apply the following strategies for the automatic run submissions of Scenario A and B.</p><p>For Scenario A, we compute similarity score between a tweet content and all 225 interest profiles in the dataset and assign it to the profile that achieves the highest similarity score if it is not semantically duplicated with the previously chosen tweets for that profile. In Scenario B and for each profile, we collect all the tweets that are related to this profile. Since the upper limit of the number of tweets for scenario B is 100 tweets for each profile per day, we cluster these tweets into 100 clusters at the end of the day and select a representative tweet in each cluster to be included in the result (email digest).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Manual Preparation</head><p>In the manual preparation type, the system can incorporate the user input before the evaluation period. Interest profiles contain discriminative keyterms that are good indicators of their topic. However, all keyterms are not of the same importance. For instance, consider a person who is interested in Sudoku puzzles. Keyterm "Sudoku" is a better indicator of the tweets related to this profile rather than keyterm "puzzle" and therefore should have a higher contribution in the calculations. Consequently, we manually constructed a list of discriminative features for each profile. Each feature in this list has a weight that indicates the importance of that feature for the specific profile. We use these weights to bias our similarity and semantically duplicated tweet calculations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Language Model</head><p>As an alternative model and for the sake of comparison, we also submitted the results of applying a unigram language model to Scenario B. We used the Lucene implementation of the language model with the Bayesian smoothing using Dirichlet priors. The value of parameter µ in this smoothing technique is set to 2100 as it is used by Li et.al. for tweets <ref type="bibr" coords="3,296.39,379.54,9.63,7.53" target="#b3">[4]</ref>. In addition, it is shown that although the optimal value of µ varies between 500 and 10000, it is usually around 2000 <ref type="bibr" coords="3,296.33,399.46,9.67,7.53" target="#b4">[5]</ref>. We expect that involving the user and incorporating her knowledge improves the performance of the task. Therefore, we also consider the list of features-weights which is manually created in our language model. Consequently, we submitted our results for Scenario B and under the manual preparation category.</p><p>In this model, we first use Lucene to index the tweets posted during the evaluation period, at the end of each day. Then, for each profile, we create a query from the list of manually extracted discriminative features of that profile, and use query level boosting to set a boost for each feature based on their weights. After that, we use the unigram language model to retrieve a ranked list of relevant tweets based on their relevance score. Having the tweets ranked, we start from the top and include each tweet in the result if they meet both of these two conditions: 1-the tweet should not be a near-duplicate of what has already been included in the result list, 2-its score should be higher than a predefined threshold. If the tweet does not meet either of these conditions, we ignore it and move to the next tweet in the ranked list. This is continued until there are 100 tweets in the results, or there are no more tweets in the ranked list. Having a threshold prevents retrieving nonrelevant tweets containing only one of the manually extracted keyterms that are not discriminative enough to be an indicator of the corresponding topic. For instance, if a tweet contains keyterm "puzzle" it does not necessarily mean that the tweet should be recommended to a user who is interested in Sudoku puzzles.</p><p>Although filtering semantically similar tweets is part of the task, we only applied the lexical similarity calculation to this model and considered two tweets to be near-duplicate if their cosine similarity is higher than 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EVALUATION METRICS</head><p>This section briefly discusses the evaluation metrics considered for each scenario. These metrics are explained in detail in the track guidelines 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scenario A -Mobile Push Notification</head><p>One of the evaluation metrics for Scenario A is Expected Latency-discounted Gain (ELG). This score is computed based on the following relation where T is the set of recommended tweets and gain(t i ) indicates the relevance of the tweet to the corresponding interest profile.</p><formula xml:id="formula_0" coords="3,338.71,240.51,87.84,22.40">ELG = 1 |T | |T | ∑ i=1 {gain(t i )}</formula><p>An explanation of the gain scores is showed in Table <ref type="table" coords="3,550.26,268.88,2.61,7.53">I</ref>. It is also important to consider that only the first tweet from each semantic cluster receives any score <ref type="bibr" coords="3,479.45,288.91,9.67,7.53" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gain Details 0</head><p>Not interesting, spam/junk tweets 0.5 Somewhat interesting tweets 1.0 Very interesting tweets TABLE I: An explanation of gain scores.</p><p>To penalize the late recommendation of tweets, a latency penalty is applied. This penalty is computed as MAX(0, (100 -delay)/100), where the delay is the difference (in minutes) between the time the tweet was published and the time the system sends the notification. In addition to ELG, the normalized Cumulative Gain (nCG) is considered as another metric. This metric is computed based on the following relation, where Z is the maximum potential gain considering the 10 tweet per day limit.</p><formula xml:id="formula_1" coords="3,338.71,479.12,82.69,22.40">nCG = 1 Z |T | ∑ i=1 {gain(t i )}</formula><p>The score of each profile for a day is first computed and the final score of the profiles is an average of their daily scores across all days in the evaluation period. The score of each run will be the average of the scores across profiles <ref type="bibr" coords="3,507.36,537.48,9.67,7.53" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scenario B -Email Digest</head><p>The evaluation metric considered for profile B is the Normalized Discounted Cumulative Gain (NDCG). This metric is calculated over k top retrieved results and is one of the metrics commonly used in evaluation information retrieval systems <ref type="bibr" coords="3,542.73,606.33,9.66,7.53" target="#b6">[7]</ref>. In this year's evaluation, k is set to 10. The following two relations explains the computation of the Discounted Cumulative Gain (DCG) and its normalized version (NDCG), where T is the set of selected tweets and IDCG@k is the maximum possible DCG for the top k tweets.</p><formula xml:id="formula_2" coords="3,338.71,670.34,94.81,41.11">DCG@k = |T | ∑ i=1 2 gain(t i )-1 log 2 (i + 1) NDCG@k = DCG@k IDCG@k</formula><p>For each profile, the value of the NDCG@k is first calculated per day and its final value is the average over 10 days. Therefore, the score for a run is calculated by the average over all profiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scenario A</head><p>Table <ref type="table" coords="4,118.27,197.08,6.09,7.53">II</ref> shows the results all participating teams for Scenario A. In our submitted runs for this scenario, DAL-TRECMA1, DALTRECMA2 and DALTRECAA1, we set the threshold α = 0.6 and β = 0.8. We achieved 0.1753 ELG and 0.2426 nCG for the automatic run which is comparable with other systems. We submitted two manual runs, the first run achieved 0.1620 ELG, 0.1614 nCG and the second run achived 0.1822 ELG, 0.1814 nCG. One of the possible reasons for moderate results may be the threshold for the GTM score which is set to 0.6. We suspect that our system does not capture potentially interesting tweets that their score is below this threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scenario B</head><p>The results of Scenario B are presented in Table <ref type="table" coords="4,281.82,346.81,8.44,7.53">III</ref>. For this scenario, we set the threshold to similar values of Scenario A, i.e. α = 0.6 and β = 0.8. We submitted two automatic runs, DALTRECAB1 and DALTRECMB1, and one manual run, DALTREC B PREP. Automatic systems achieved 0.1339 and 0.1323 for NDCG score and the score for our manual preparation was 0.2210. It seems that our alternative approach, language model, is worth investigating further. In addition, we would like to combine the proposed strategies and examine whether it leads to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper presents the DalTREC team participation in the mobile notification and email digest tasks of the TREC 2015 Mircoblog track. We proposed a novel approach for assigning tweets to profiles and to determine whether two tweets are semantically similar using Wikipedia as an external knowledge source and a corpus-based word semantic relatedness method. Results show that the proposed approach is comparable with other systems in this competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,169.27,430.22,296.36,7.53;2,118.96,107.08,396.82,315.19"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The framework of the proposed methods for the real-time filtering task.</figDesc><graphic coords="2,118.96,107.08,396.82,315.19" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="1,336.08,722.14,88.39,6.03"><p>https://lucene.apache.org/core/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The research was funded in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="4,95.07,635.57,213.92,6.03;4,95.06,643.78,214.16,6.03;4,95.06,651.90,214.11,5.92;4,95.06,660.08,141.68,6.03" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,287.40,635.57,21.59,6.03;4,95.06,643.78,137.66,6.03">Dexter: an open source framework for entity linking</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ceccarelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Trani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,251.47,643.80,57.75,5.92;4,95.06,651.90,214.11,5.92;4,95.06,660.11,60.49,5.92">Proceedings of the sixth international workshop on Exploiting semantic annotations in information retrieval</title>
		<meeting>the sixth international workshop on Exploiting semantic annotations in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,95.07,670.70,214.03,6.03;4,95.06,678.91,214.17,6.03;4,95.06,687.01,213.95,6.03;4,95.06,695.22,213.82,6.03;4,95.06,703.43,144.39,6.03" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,219.28,670.70,89.81,6.03;4,95.06,678.91,25.39,6.03">Text similarity using google tri-grams</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Milios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kešelj</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-30353-129</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-30353-129" />
	</analytic>
	<monogr>
		<title level="m" coord="4,143.12,678.94,166.12,5.92;4,95.06,687.01,181.32,6.03">Proceedings of the 25th Canadian Conference on Advances in Artificial Intelligence, ser. Canadian AI&apos;12</title>
		<meeting>the 25th Canadian Conference on Advances in Artificial Intelligence, ser. Canadian AI&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="312" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,95.07,713.94,214.15,6.03;4,95.06,722.14,70.89,6.03" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,170.28,713.94,133.48,6.03">The google web 1t 5-gram corpus version 1.1</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Franz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="4,341.34,436.36,213.81,6.03;4,341.34,444.46,214.16,6.03;4,341.34,452.70,213.93,5.92;4,341.34,460.88,157.12,6.03" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,468.23,436.36,86.92,6.03;4,341.34,444.46,142.09,6.03">Req-rec: High recall retrieval with query pooling and interactive classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,499.93,444.49,55.57,5.92;4,341.34,452.70,213.93,5.92;4,341.34,460.91,68.70,5.92">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,341.34,471.28,213.93,6.03;4,341.34,479.49,214.16,6.03;4,341.34,487.72,214.22,5.92;4,341.34,495.79,196.30,6.03" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,420.99,471.28,134.28,6.03;4,341.34,479.49,140.55,6.03">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,499.06,479.52,56.44,5.92;4,341.34,487.72,214.22,5.92;4,341.34,495.82,107.88,5.92">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,341.34,506.30,214.14,6.03;4,341.34,514.51,214.15,6.03;4,341.34,522.61,214.16,6.03;4,341.34,530.82,50.84,6.03" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,506.58,506.30,48.90,6.03;4,341.34,514.51,78.08,6.03">Overview of the trec-2015 microblog track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W Y S G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,437.44,514.54,118.05,5.92;4,341.34,522.64,64.09,5.92">Proceedings of the Twenty-Fourth Text REtrieval Conference</title>
		<meeting>the Twenty-Fourth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-11">2015. November, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,341.34,541.33,214.03,6.03;4,341.34,549.43,214.10,6.03;4,341.34,557.64,214.16,6.03;4,341.34,565.85,50.41,6.03" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,398.74,549.43,114.81,6.03">Learning to rank using gradient descent</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,529.27,549.46,26.17,5.92;4,341.34,557.67,186.07,5.92">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
