<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,215.98,85.28,177.75,5.54;1,179.11,105.20,251.49,5.54">Webis at TREC 2015: Tasks and Total Recall Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,105.76,137.59,83.72,12.34"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.09,137.59,69.29,12.34"><forename type="first">Steve</forename><surname>Göring</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.97,137.59,81.92,12.34"><forename type="first">Magdalena</forename><surname>Keil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,396.49,137.59,107.47,12.34"><forename type="first">Olaoluwa</forename><surname>Anifowose</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.12,150.04,70.42,12.34"><forename type="first">Amir</forename><surname>Othman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.47,150.04,65.13,12.34"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<postCode>99421</postCode>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,215.98,85.28,177.75,5.54;1,179.11,105.20,251.49,5.54">Webis at TREC 2015: Tasks and Total Recall Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">91BD5EF2A56A4B8CD849CB2205FDD3D3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we give a brief overview of the Webis group's participation in the TREC 2015 Tasks and Total Recall tracks.</p><p>In the task understanding subtask of the Tasks track, we use di erent data sources (AOL query log, Freebase, etc.) and APIs (Google, Bing, etc.) to retrieve topics related to a given query. All sources are combined in our SQuare system. The task completion subtask is based on combining the results of our ChatNoir 2 for the di erent topics identified in the task understanding subtask. Finally, for the ad-hoc subtask (similar to the previous years' Web tracks), we use an axiomatic re-ranking approach of the search results obtained from ChatNoir 2.</p><p>In the Total Recall track, we employ a simple SVM baseline with variable batch sizes equipped with a keyqueries step to identify potentially relevant documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">TASKS TRACK</head><p>The Tasks track has three subtasks: task understanding, task completion, and the ad-hoc subtask that is similar to previous years' TREC Web tracks. For each of the subtasks, we briefly describe our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Task Understanding</head><p>The goal of the task understanding subtask is to automatically identify related queries for all possible aspects or topics a given user query may consist of. For each of 50 given user queries up to 100 related queries should be generated. Our proposed system is implemented as a web service called SQuare 1 (search for queries that are related). The general approach is divided in two steps: acquisition of related queries and scoring of each query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Acquisition of Related Queries</head><p>In the first step, we generate related queries using di erent APIs and datasets: query suggestion APIs, Graph/RDF-based, search result-based, AOL query log analysis, Netspeak completion and ChatNoir phrase extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suggestion APIs.</head><p>We use the query suggestion APIs from Google and Bing for each given query. Since both APIs typically cover similar 1 http://webis16.medien.uni-weimar.de/square/ related queries, on average we obtained 6-8 related queries from these APIs.</p><p>AOL query log analysis.</p><p>We have divided the AOL query log <ref type="bibr" coords="1,479.73,261.73,9.73,8.95" target="#b8">[8]</ref> into search sessions and missions <ref type="bibr" coords="1,392.88,272.19,9.22,8.95" target="#b5">[5]</ref>. For a given query q, we then identify all search sessions that contain a query with a tf -weighted cosine similarity to q of at least 0.8. All queries of such sessions are then used as potentially related queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph, RDF.</head><p>Since most queries of the Tasks track come with annotated Freebase entities (ID and name), we submit requests for similar entities/topics to the Interestgraph <ref type="bibr" coords="1,512.06,357.38,13.52,8.95" target="#b11">[11]</ref>, Wiki-Data <ref type="bibr" coords="1,339.66,367.84,13.52,8.95" target="#b13">[13]</ref>, and Freebase <ref type="bibr" coords="1,414.96,367.84,9.22,8.95">[3]</ref>. All retrieved topics are used as potentially related queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Results.</head><p>We submit the given user query to the general Google web search <ref type="bibr" coords="1,345.41,421.63,9.73,8.95" target="#b4">[4]</ref> and to the Wikipedia search <ref type="bibr" coords="1,479.26,421.63,14.34,8.95" target="#b14">[14]</ref> to retrieve the top-10 search results. For each search result we get the title and use the terms with at least two characters as potential query phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Netspeak Frequent Phrases.</head><p>For a query q, we use Netspeak <ref type="bibr" coords="1,462.28,485.89,14.34,8.95" target="#b10">[10,</ref><ref type="bibr" coords="1,480.86,485.89,11.78,8.95" target="#b12">12]</ref> to find related queries as follows. Let w1 and wn be the first and last word in q, respectively. We then submit the request ú w1 ú wnú to Netspeak. The query results are the most frequent phrases containing w1 and wn, where the ú-operator matches zero or more words. The top-10 Netspeak results are used as potentially related queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatNoir Keyphrase Extraction.</head><p>Our last data source for related queries is ChatNoir <ref type="bibr" coords="1,543.63,581.54,9.22,8.95" target="#b9">[9]</ref>. We retrieve the top-10 results and extract the top-10 keyphrases from the main content <ref type="bibr" coords="1,463.74,602.46,9.73,8.95" target="#b7">[7]</ref> using a head noun phrase extractor <ref type="bibr" coords="1,385.79,612.92,9.22,8.95" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Query Scoring</head><p>Based on the above di erent techniques of the acquisition step, we obtain a set Q of potentially related queries for a given query q. For the di erent approaches, our pilot studies showed di erent qualities of the found potentially related queries such that we assign di erent weights to queries found by either technique reflecting the quality: A query found by di erent approaches gets the summed weights as its score and the queries are ranked by descending scores. Queries achieving the same score are ordered by the number of ClueWeb12 results found with ChatNoir 2 preferring queries with more results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3">Run webis1</head><p>We only had one run based on the top-100 related queries we found. However, having known before that the pooling depth for evaluation would be 20, we would have submitted two other runs having ranks 21-40 and 41-60 also be judged. On average, we extracted about 250 di erent related queries for a given query. From these, the highest scoring 100 queries are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Task Completion</head><p>Our runs for the task completion and ad-hoc subtask are on the full ClueWeb12 corpus (category A) using the ChatNoir 2 search engine to retrieve an initial baseline result set, that is then potentially re-ranked. ChatNoir 2 is the successor of ChatNoir <ref type="bibr" coords="2,145.19,298.03,9.22,8.95" target="#b9">[9]</ref>; it is built on ElasticSearch using BM25F as the retrieval model.</p><p>As for the task completion subtask, based on a given user query, all documents should be given as output that are relevant to any task a user may be trying to fulfill. In our general approach for this subtask, we use the related queries from the task understanding subtask (cf. Section 1.1) as the task set. For each related query individually, we retrieve documents using ChatNoir 2. Afterwards, we combine the top-10 retrieved documents of the di erent related queries in di erent ways. Note that we do not include the original query here although it might still be the best description of the user need. The reason is that our runs for the ad-hoc task will be based on the original query and we wanted to avoid getting similar judgments for two di erent tasks. The final result list containing results for the original query and the related ones might of course achieve better evaluation results than the combined list of the related queries or the results of the original query individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1">Runs</head><p>For this subtask, we assume a low pooling depth having the third run use documents not in the top-20 of the other two runs. The underlying idea is to obtain judgments for more documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>webisC1.</head><p>In our first run, we use a simple interleaving approach. The first result is the first rank from the highest scoring related query, the second result is the highest rank from the second highest scoring related query, etc. Results that are already contained in the interleaved ranking are not used again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>webisC2.</head><p>Our second run weights the top-10 retrieved documents for each related query based on the number of related queries that have the result in their top-10. The most frequent results form the combined result set. In case of same frequencies, the task understanding score of the query that has the result in the highest result rank determines; if even this is equal, the order is chosen at random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>webisC3.</head><p>In our last run, we again use the interleaving approach of our first run, but skip documents that are contained in the top-20 of our first two runs. The idea is to obtain some more judgments for other documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Ad-hoc Task</head><p>The ad-hoc subtask is similar to the past years' TREC Web tracks. For a given query, a ranked list of documents should be returned. In the last year, we participated in the Web track with an early prototype of an axiom-based reranking framework <ref type="bibr" coords="2,397.49,205.38,9.22,8.95" target="#b6">[6]</ref>. As for this year's ad-hoc subtask, we improved many parts of the framework; especially we added more axioms.</p><p>The basic idea remains the same but we use the new ChatNoir 2 setup and pre-trained static axiom weights obtained in pilot experiments for BM25F or tf •idf as baseline retrieval models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1">Runs</head><p>Again, we have three runs that assume a low pooling depth for judgment and have the lower ranks of our first runs as the top ranks in the third run to obtain judgments for more documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>webisA1.</head><p>We axiomatically re-rank the ChatNoir 2 baseline results using the BM25F axiom weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>webisA2.</head><p>The results of webisA1 from which the top-20 were removed. Again, the idea is to simply get more judgments in case that the pooling depth will be set to 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>webisA3.</head><p>We axiomatically re-rank the results of a tf • idf baseline retrieval model. From the final ranking, the top-20 results of our first two runs are excluded to obtain judgments for more documents. The original tf •idf ranking can be evaluated by re-inserting the excluded documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TOTAL RECALL TRACK</head><p>The objective of the Total Recall track is to retrieve all relevant documents for a given topic with as little e ort as possible. The components involved in our system include Apache Lucene 5.3 (used for indexing and searching employing the BM25 retrieval model), MongoDB 3.0 (database for storage), and LIBSVM (for training an SVM classifier). Our two runs (baseline and keyphrase) are separated into four separate steps: (1) initialization, (2) first iteration, (3) subsequent iterations, (4) finalization. Basically, the only difference of our two runs is in the subsequent iterations step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization.</head><p>As a preprocessing step, the tf • idf scores for the vocabulary in the corpus are derived and stored in a key-value store. The tf • idf weighted document vectors will serve as the feature input of an SVM classifier used in later steps. However, when the number of documents in the corpus increases, there is a possibility that our system substantially slows down due to the SVM classifier. Our pilot experiments show that this e ect occurs around corpus sizes of 300,000 documents. We perform sampling to remedy this situation. If the corpus has more than 300,000 documents, a subset of 300,000 documents would be chosen randomly.</p><p>An index for the corpus is built using Apache Lucene's BM25 retrieval model with default parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First Iteration.</head><p>In each iteration, we send a number of documents to the "user" (i.e., the organizer's API) for relevance judgment. We call this number the batch size. During the first iteration, the batch size is set to 32. The size of 32 is chosen since our pilot experiments showed it to represent a good compromise between sending too many or too few documents for judgment. And since it is a multiple of 2, it is easy to later halve or double the batch size.</p><p>For a given topic, we start by removing the stop words; however, when more than 50% of a topic's terms are stop words, we keep them. The processed topic is then run as an ad-hoc query against the BM25 index. We consider the top-128 results (again a multiple of 2) as relevant and label them as positive examples for the SVM classifier. Another 128 random documents that do not appear in the ad-hoc query's result are labeled as negative examples for the SVM. The SVM is then trained on the labeled documents and used to classify all other corpus documents (remember that it could only be a sample for large corpora). From the resulting ranking, the 16 top-ranking documents are chosen that are not among the top-16 results of the ad-hoc query. These 16 SVM-classified documents are combined with the top-16 results of the ad-hoc query and submitted to the user for judgment. The response is a list of true labels for the 32 documents (relevant or not) that is used in the subsequent iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subsequent Iterations.</head><p>After each iteration, the batch size may change by being doubled or halved (easy due to the initial multiple of 2). There will be a point in a run where the next batch size becomes zero due to rounding. At that point, our approach decides to stop. In case that the batch size is not zero, the following steps will be performed.</p><p>First, the relevance feedback from the previous iteration is used to create a new training set for the SVM classifier. This is done by labeling the user-defined relevant documents as positive and the non-relevant documents as negative. Since the number of relevant and non-relevant documents might di er, we try to balance the sets to reduce bias. If the number of non-relevant documents is less than the number of relevant documents, some random documents from the corpus that have not been judged as relevant are added to the negative examples. If the number of non-relevant documents is larger than the number of relevant documents, non-relevant documents are removed at random until the number of relevant document is reached.</p><p>The trained SVM classifier is then used to classify the corpus documents not judged before. The top-n documents the SVM classifier judges as relevant are selected to send for judgment retrieval. Here, n is the batch size that depends on the previous iteration: It is doubled from the previous batch size if the ratio of relevant documents to non-relevant documents was greater than 0.5 in the previous iteration. It is halved if the ratio of relevant documents to non-relevant documents was less than 0.4. Otherwise, the previous batch size is not changed. To avoid sending too many documents, we set the maximum batch size to 2048 for athome1 and athome2 and to 4096 for all other corpora. If the maximum batch size is reached, the above rules would be ignored and the following would be applied: The batch size is divided by 16 if the ratio of relevant documents to non-relevant documents is less than 0.4 and halved otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finalization.</head><p>Eventually, the batch size will become smaller since the number of relevant documents not shown to the user is monotonically decreasing. When the next batch size becomes zero, a run on a topic is finalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Run with Keyphrase Extraction</head><p>In this run, a keyphrase extraction component is added to enrich the set of potentially positive examples in each iteration instead of shrinking the set of judged non-relevant documents. The only di erence to the baseline is the treatment of subsequent iterations (i.e., iterations following the first iteration). The other parts are identical to the baseline run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subsequent Iterations.</head><p>In case that more than 128 documents were judged as relevant in previous iterations, we choose 128 at random that were not used for keyphrase extraction in previous iterations. For these 128 documents, we compute the pairwise tf • idf -weighted cosine similarities and remove all similarities below 0.2. From the remaining similarities, we select the four documents with the highest pairwise cosine similarities with the assumption that these deal with similar topics and were judged as relevant by the user. If no such documents can be found (e.g., when all similarities are below 0.2 or no relevant documents exist), the keyphrase extraction is skipped. In case that four documents could be identified, we extract the top-10 head noun keyphrases <ref type="bibr" coords="3,501.39,489.31,9.73,8.95" target="#b1">[1]</ref> from their concatenated main contents <ref type="bibr" coords="3,432.39,499.77,9.22,8.95" target="#b7">[7]</ref>. These keyphrases are used to form a keyquery for as many of the documents as possible. A keyquery for a document set is a query that retrieves the documents in its top-k results <ref type="bibr" coords="3,458.46,531.15,9.22,8.95" target="#b2">[2]</ref>. In our case, we employ the Lucene BM25 index as the reference search engine and set k = 32. From a keyquery the top-128 results not previously judged as relevant are used as additional positive examples for training the SVM classifier (in addition to the documents already judged as relevant). In this case, not that many non-relevant documents have to be removed for balancing than was the case in the baseline run. The negative examples again are the documents judged as non-relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All other Steps.</head><p>The other three steps and the batch size adapting strategy of the subsequent iterations step are identical to the baseline run.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,316.81,696.60,239.11,19.41"><head></head><label></label><figDesc>Google suggest 100, Bing suggest 90, AOL query log sessions 80, Inter-estgraph 70, Wikipedia search result titles 60, Google search result titles 50, Freebase 40, Wikidata 30, Netspeak frequent phrases 20, ChatNoir search result keyphrases 10.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,58.28,52.28,96.81,15.55" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,70.58,198.96,8.95;4,72.62,77.27,217.34,12.72;4,72.62,87.73,99.70,12.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,196.45,70.58,75.13,8.95;4,72.62,81.04,152.70,8.95">Using noun phrase heads to extract document keyphrases</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cornacchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,244.08,77.27,45.88,12.72;4,72.62,87.73,41.61,12.72">Proceedings of AI 2000</title>
		<meeting>AI 2000</meeting>
		<imprint>
			<biblScope unit="page" from="40" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,102.96,213.83,8.95;4,72.62,113.42,208.45,8.95;4,72.62,120.11,197.16,12.72" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,265.32,102.96,21.13,8.95;4,72.62,113.42,208.45,8.95;4,72.62,123.88,13.63,8.95">From keywords to keyqueries: Content descriptors for the web</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,105.39,120.11,97.37,12.72">Proceedings of SIGIR 13</title>
		<meeting>SIGIR 13</meeting>
		<imprint>
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,135.34,121.41,8.95;4,72.62,152.11,204.98,2.87;4,72.62,156.26,21.00,8.95" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><surname>Google</surname></persName>
		</author>
		<ptr target="https://developers.google.com/freebase/data" />
		<title level="m" coord="4,106.22,135.34,82.93,8.95">Freebase data dumps</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,167.72,125.70,8.95" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,106.22,167.72,63.28,8.95">Web search API</title>
		<author>
			<persName coords=""><surname>Google</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,179.17,208.74,8.95;4,72.62,189.63,220.28,8.95;4,72.62,196.32,166.17,12.72" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,260.22,179.17,21.13,8.95;4,72.62,189.63,204.97,8.95">From search session detection to search mission detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gomoll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,72.62,196.32,108.17,12.72">Proceedings of OAIR 2013)</title>
		<meeting>OAIR 2013)</meeting>
		<imprint>
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,211.55,195.23,8.95;4,72.62,222.01,204.05,8.95;4,72.62,228.70,220.29,12.72;4,72.62,239.16,21.38,12.72" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,110.89,222.01,165.78,8.95;4,72.62,232.47,116.34,8.95">Webis at TREC 2014: Web, Session, and Contextual Suggestion tracks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Göring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,207.05,228.70,85.85,12.72">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,254.39,189.58,8.95;4,72.62,264.85,208.46,8.95;4,72.62,271.54,177.27,12.72" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,72.62,264.85,193.15,8.95">Boilerplate detection using shallow text features</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kohlschütter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,72.62,271.54,109.96,12.72">Proceedings of WSDM 2010</title>
		<meeting>WSDM 2010</meeting>
		<imprint>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,286.77,220.28,8.95;4,72.62,293.46,197.92,12.72" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,244.37,286.77,48.53,8.95;4,72.62,297.23,23.54,8.95">A picture of search</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Torgeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,114.67,293.46,115.46,12.72">Proceedings of Infoscale 2006</title>
		<meeting>Infoscale 2006</meeting>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,308.69,191.77,8.95;4,72.62,319.15,212.03,8.95;4,72.62,329.61,185.52,8.95;4,72.62,336.30,156.34,12.72" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="4,244.43,319.15,40.21,8.95;4,72.62,329.61,169.89,8.95">ChatNoir: A search engine for the ClueWeb09 corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,72.62,336.30,106.50,12.72">Proceedings of SIGIR 2012</title>
		<meeting>SIGIR 2012</meeting>
		<imprint>
			<biblScope unit="page">1004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,351.53,216.18,8.95;4,72.62,358.21,215.52,12.72;4,72.62,368.67,88.73,12.72" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="4,249.07,351.53,39.73,8.95;4,72.62,361.99,139.92,8.95">Netspeak: Assisting writers in choosing words</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trenkmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,231.44,358.21,56.70,12.72;4,72.62,368.67,43.49,12.72">Proceedings of ECIR 2010</title>
		<meeting>ECIR 2010</meeting>
		<imprint>
			<biblScope unit="page">672</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,383.91,122.85,8.95;4,72.62,400.68,186.65,2.87;4,262.34,394.37,20.98,8.95" xml:id="b11">
	<analytic>
		<title/>
		<ptr target="http://interest-graph.getprismatic.com/" />
	</analytic>
	<monogr>
		<title level="j" coord="4,72.62,383.91,118.08,8.95">Prismatic. InterestGraph API</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,405.82,89.07,8.95;4,164.75,412.14,92.00,2.87;4,259.82,405.82,20.98,8.95" xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Webis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Netspeak</surname></persName>
		</author>
		<ptr target="http://netspeak.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,417.28,129.46,8.95;4,72.62,434.05,120.25,2.87;4,195.94,427.74,20.98,8.95" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="4,115.62,417.28,81.68,8.95">Knowledge base API</title>
		<author>
			<persName coords=""><surname>Wikidata</surname></persName>
		</author>
		<ptr target="https://www.wikidata.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.62,439.20,110.98,8.95" xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j" coord="4,72.62,439.20,82.14,8.95">Wikipedia. Web API</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
