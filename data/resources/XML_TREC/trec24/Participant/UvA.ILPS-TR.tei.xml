<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,122.71,161.76,365.83,15.12;1,241.65,183.68,127.94,15.12">The University of Amsterdam (ILPS) at TREC 2015 Total Recall Track</title>
				<funder ref="#_s5pHXUv">
					<orgName type="full">Netherlands eScience Center</orgName>
				</funder>
				<funder ref="#_wPJmZyW #_k3GhnUJ #_ZCTta9t #_TkNamyg">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">HPC Fund</orgName>
				</funder>
				<funder ref="#_u5D88eK">
					<orgName type="full">Elite Network Shifts</orgName>
				</funder>
				<funder ref="#_gjH2pE4 #_M58bnqD">
					<orgName type="full">Netherlands Organisa-tion for Scientific Research (NWO)</orgName>
				</funder>
				<funder>
					<orgName type="full">Royal Dutch Academy of Sciences (KNAW)</orgName>
				</funder>
				<funder ref="#_DPkrpz6">
					<orgName type="full">ESF</orgName>
				</funder>
				<funder ref="#_QAZD4FS">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder>
					<orgName type="full">Yahoo! Faculty Research and Engagement Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,101.45,217.58,78.04,10.48;1,179.48,215.97,1.88,6.99"><forename type="first">David</forename><surname>Van Dijk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Create-IT</orgName>
								<orgName type="institution">Amsterdam University of Applied Sciences</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.66,217.58,73.97,10.48;1,268.63,215.97,1.88,6.99"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.04,217.58,102.07,10.48;1,382.11,215.97,1.88,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.29,217.58,89.25,10.48;1,505.54,215.97,1.88,6.99"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,122.71,161.76,365.83,15.12;1,241.65,183.68,127.94,15.12">The University of Amsterdam (ILPS) at TREC 2015 Total Recall Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CE3CD36398CFC8CC601511EABE24FEE1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the participation of the University of Amsterdams ILPS group in the Total Recall track at TREC 2015. Based on the provided Baseline Model Implemention ("BMI") we set out to provide two more baselines we can compare to in future work. The two methods are bootstrapped by a synthetic document based on the query, use TF/IDF features, and sample with dynamic batch sizes which depend on the percentage of predicted relevant documents. We sample at least 1 percent of the corpus and stop sampling if a batch contains no relevant documents. The methods differ in the classifier used, i.e. Logistic Regression and Random Forest.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Total Recall track was introduced at TREC this year. In this track, participants implement automatic or semi-automatic methods to identify as many relevant documents as possible, with as little review effort as possible, from document collections containing as many as 1 million documents.</p><p>After downloading the collection and informa-tion need, participants must identify documents from the collection and submit them to the online relevance assessor which return the relevance labels.</p><p>The objective is to submit as many documents containing relevant information as possible, while submitting as few documents as possible, to the automated relevance assessor.</p><p>The track provided "Play-at-Home" and "Sandbox" evaluation.</p><p>For "Play-at-Home" evaluation, participants ran the system on their own hardware, and participant could choose to run "automatic" or "manual", where the latter involved manual intervention. For the "Sandbox" evaluation a virtual machine needed to be submitted, containing a fully automated solution.</p><p>The Information and Language Processing Systems (ILPS) group of the University of Amsterdam participated in the "Play-at-Home" and "Sandbox" evaluation, without the use of manual intervention. In this paper, we explain the runs we submitted and their results.</p><p>Section 2 describes the methods we submitted, Section 3 lists the runs and their results, Section 4 contains our conclusion.</p><p>Before our final submissions we experimented with several methods on the provided test data. We tried different combinations of bootstrapping and sampling methods, features and classifiers. Beating the baseline <ref type="bibr" coords="2,232.13,223.13,11.52,9.57" target="#b0">[1]</ref> turned out challenging. None of the methods provided a significant improvement over the baseline. Therefore we decided to postpone this task, and for now submit two basic methods, that are slight variations on the baseline, in order to see the influence of dynamic sampling, a heuristical stopping criterion and different classifiers.</p><p>Our methods work as follows: For preprocessing some basic filtering and Porter stemming is applied. The two methods are bootstrapped by a synthetic document based on the query. We use TF/IDF features, and sample with dynamic batch sizes which depend on the percentage of predicted relevant documents. We sample at least 1 percent of the corpus and stop sampling if a batch contains no relevant documents. The methods differ in the classifier used, i.e. Logistic Regression and Random Forest. Scikit classifiers were used with default parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs &amp; Results</head><p>We report on the preliminary results for the Athome experiments (datasets Athome1, Athome2 and Athome3, 10 queries each), as well as for one of the Sandbox tests, which was done on the MIMIC II clinical dataset<ref type="foot" coords="2,519.36,386.09,4.23,6.99" target="#foot_0">1</ref> (19 queries). We compare the results of our methods (Baseline1 and Baseline2 ) against the provided baseline (BMI ).</p><p>Athome1 (290K). On the Athome1 dataset, overall the BMI and Baseline1 gave similar results and Baseline2 performed less. Only on athome109 Baseline1 outperformed BMI. Baseline2 was in between the two from the start until around 0.5 recall, then dropped to third place again. Fig. <ref type="figure" coords="2,439.78,537.08,5.45,9.57" target="#fig_0">1</ref> shows the results on topic athome101, reflecting the general performance.</p><p>The picture shows the effort over recall levels (step size 0.05) until the maximum recall achieved by either Baseline1 or Baseline2. Table <ref type="table" coords="2,416.42,604.83,5.45,9.57">1</ref> lists the topics for the dataset, the number of relevant documents per topic, followed by the maximum recall achieved by Baseline1. At the bottom of the table the average maximum recall is provided (0.92730), which gives us some intuition on how well our stop criterion performed. An average maximum recall above 0.9 seems reasonable.</p><p>Athome2 (450K). Though the general picture remains, Baseline1 performs a little less then BMI overall on the Athome2 dataset. Baseline2 is still in third place. See fig. <ref type="figure" coords="3,263.63,255.23,5.45,9.57">2</ref> for an example. Table <ref type="table" coords="3,150.18,268.78,5.45,9.57">2</ref> shows the number of relevant documents per topic are smaller in general then on the Athome1 dataset. The average maximum recall on Baseline1 is a little higher as in the Athome1 experiments; 0.93733.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Athome3 (900K).</head><p>The results from the Athome3 dataset gives similar results as seen so far. On one topic Baseline1 outperforms BMI, see fig. <ref type="figure" coords="3,142.10,390.73,4.24,9.57">3</ref>. Table <ref type="table" coords="3,193.36,390.73,5.45,9.57">3</ref> reveals the relevant documents per topic are, on average, even smaller than on the Athome2 dataset. The Baseline1 's average maximum recall went up to 0.97049, which seems quite good.</p><p>MIMIC II. The size of the MIMIC II dataset is unknown to us at the moment of writing, as it was used in a sandbox test. Again we see similar results as before for several topics. But for some topics we see the performance of Baseline2 getting closer to BMI. Another observation is that Baseline1 stops way too soon for some topics. Fig. <ref type="figure" coords="3,178.71,566.87,5.45,9.57">4</ref> provides an example of both observations. When we look at Table <ref type="table" coords="3,279.13,580.42,5.45,9.57">4</ref> we see that for most topics the stopping criterion is not performing well for Baseline1 on this dataset. Only for 3 topics the maximum recall is above 0.9, the other 13 topics are below 0.73, 5 of those are close to zero. The Baseline1 's average maximum recall is at a 0.48989, not an acceptable level. Table 4 also shows the relevant documents per topic are quite large on average compared to the other datasets.    In the experiments, the baseline's sampling method outperformed our sampling method. The batch size turned out to have a substantial influence on performance.</p><p>Logistic Regression outperformed the Random Forest classifier overall. The influence of the classifier on performance varied among datasets.</p><p>The stopping criterion worked reasonably well on the Athome dataset, but it did not perform well on the MIMIC II dataset. As we do not have access to the latter dataset we have not been able to analyse this result yet. Our stopping criterion depends on our sampling method. As the sampling method did hurt performance, this dependency is unwanted and we aim to experiment with other stopping criteria on the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,73.21,363.48,227.43,8.74;2,91.39,375.43,49.34,8.74;2,73.21,387.39,227.44,8.74;2,91.39,399.34,67.05,8.74;2,73.21,411.30,212.04,8.74;2,73.21,423.25,227.44,8.74;2,91.39,435.21,92.57,8.74;2,73.21,447.16,227.43,8.74;2,91.39,459.12,203.79,8.74;2,73.21,471.07,227.43,8.74;2,91.39,483.03,34.09,8.74;2,73.21,494.98,121.88,8.74;2,73.21,506.94,227.43,8.74;2,91.39,518.89,71.82,8.74;2,73.21,530.85,221.43,8.74;2,68.23,542.80,232.42,8.74;2,91.39,554.76,67.02,8.74;2,68.23,566.72,232.42,8.74;2,68.23,578.67,232.41,8.74;2,91.39,590.63,209.25,8.74;2,91.39,602.58,88.97,8.74;2,68.23,614.54,232.42,8.74;2,91.39,626.49,209.25,8.74;2,91.39,638.45,188.05,8.74;2,68.23,650.40,232.41,8.74;2,91.39,662.36,189.96,8.74"><head>( 1 )</head><label>1</label><figDesc>Create synthetic document from query , code it as relevant. (2) Temporarily code a randomly sampled document as not relevant. (3) Add coded documents to initial training set. (4) Train classifier on initial training set, classify documents in corpus. (5) Review documents in descending order until we find at least one relevant and one not relevant. (6) Initialise the training set with the reviewed documents. (7) Set batch size B to 100. (8) Train classifier on training set, classify documents in corpus. (9) Select B highest scoring documents for review. (10) Review the documents, coding them as relevant or not relevant. (11) Add the reviewed documents to the training set.(12) Set B to the part of relevant docs found in the batch, times 0.1% of the total amount of documents in the corpus. (13) If B is zero, and there has been sampled less then 1% of the corpus, set B to 1% of the corpus minus the amount of documents already sampled. (14) Repeat steps 8 through 13 until the amount sampled is over 1% of the corpus and B is zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,374.08,379.92,101.70,9.57;3,310.61,204.60,230.40,172.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Athome101</figDesc><graphic coords="3,310.61,204.60,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,371.35,629.51,107.15,9.57;3,310.61,454.20,230.40,172.80"><head>Figure</head><label></label><figDesc>Figure 2: Athome2129</figDesc><graphic coords="3,310.61,454.20,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,132.75,305.09,107.15,9.57;4,72.00,129.78,230.40,172.80"><head>Figure</head><label></label><figDesc>Figure 3: Athome3481</figDesc><graphic coords="4,72.00,129.78,230.40,172.80" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,327.19,652.72,211.83,7.47;2,310.61,663.68,65.90,7.47"><p>https://physionet.org/mimic2/mimic2_clinical_ overview.shtml</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was supported by the <rs type="funder">European Community</rs>'s <rs type="programName">Seventh Framework Programme</rs> (<rs type="programName">FP7/2007-2013)</rs> under grant agreement nr 312827 (VOX-Pol), the <rs type="funder">Netherlands Organisa-tion for Scientific Research (NWO)</rs> under project nrs <rs type="grantNumber">727.011.005</rs>, <rs type="grantNumber">612.001.116</rs>, <rs type="grantNumber">HOR-11-10</rs>, <rs type="grantNumber">640.006.013</rs>, <rs type="grantNumber">612.066.930</rs>, <rs type="grantNumber">CI-14-25</rs>, <rs type="grantNumber">SH-322-15</rs>, Amsterdam Data Science, the <rs type="programName">Dutch national program</rs> COMMIT, the <rs type="funder">ESF</rs> <rs type="programName">Research Network Program ELIAS</rs>, the <rs type="funder">Elite Network Shifts</rs> project funded by the <rs type="funder">Royal Dutch Academy of Sciences (KNAW)</rs>, the <rs type="funder">Netherlands eScience Center</rs> under project nr <rs type="grantNumber">027.012.105</rs>, the <rs type="funder">Yahoo! Faculty Research and Engagement Program</rs>, the <rs type="institution">Microsoft Research PhD program</rs>, and the <rs type="funder">HPC Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QAZD4FS">
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_gjH2pE4">
					<idno type="grant-number">727.011.005</idno>
					<orgName type="program" subtype="full">FP7/2007-2013)</orgName>
				</org>
				<org type="funding" xml:id="_M58bnqD">
					<idno type="grant-number">612.001.116</idno>
				</org>
				<org type="funding" xml:id="_wPJmZyW">
					<idno type="grant-number">HOR-11-10</idno>
				</org>
				<org type="funding" xml:id="_k3GhnUJ">
					<idno type="grant-number">640.006.013</idno>
				</org>
				<org type="funding" xml:id="_ZCTta9t">
					<idno type="grant-number">612.066.930</idno>
				</org>
				<org type="funding" xml:id="_TkNamyg">
					<idno type="grant-number">CI-14-25</idno>
				</org>
				<org type="funding" xml:id="_DPkrpz6">
					<idno type="grant-number">SH-322-15</idno>
					<orgName type="program" subtype="full">Dutch national program</orgName>
				</org>
				<org type="funding" xml:id="_u5D88eK">
					<orgName type="program" subtype="full">Research Network Program ELIAS</orgName>
				</org>
				<org type="funding" xml:id="_s5pHXUv">
					<idno type="grant-number">027.012.105</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,87.97,302.28,212.68,8.74;6,87.97,314.23,204.37,8.74;6,87.97,326.19,151.19,8.74;6,87.97,338.14,95.80,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,254.98,302.28,45.66,8.74;6,87.97,314.23,204.37,8.74;6,87.97,326.19,112.87,8.74">Autonomy and reliability of continuous active learning for technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno>CoRR, abs/1504.06868</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
