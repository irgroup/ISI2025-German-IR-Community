<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,108.92,85.26,391.89,5.54">WaterlooClarke: TREC 2015 Total Recall Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,91.02,115.17,75.53,14.80"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
							<email>haotian.zhang@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,176.09,115.17,35.35,14.80"><forename type="first">Wu</forename><surname>Lin</surname></persName>
							<email>wu.lin@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.58,115.17,67.56,14.80"><forename type="first">Yipeng</forename><surname>Wang</surname></persName>
							<email>yipeng.wang@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.28,115.17,103.76,14.80"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<email>claclark@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,427.62,115.17,91.06,14.80"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<email>mark.smucker@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,108.92,85.26,391.89,5.54">WaterlooClarke: TREC 2015 Total Recall Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7950C7F077F820DDB0BB622FFCC6AF3E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The total recall track in TREC 2015 seeks an enhanced model to accelerate the autonomous technology-assisted review process. This paper introduces several noval ideas such as clustering based seed selection method, extended n-grams features and continuous query expansion learned from the relevant documents derived from each iteration. These methods can retrieve more relevant documents from each iteration thereby achieving high recall while requiring less review e↵ort.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The technology-assisted review ("TAR") applies the iterative retrieval and review of documents to find a substantial majority or all of the relevant documents in a collection. Its applications include electronic discovery ("eDiscovery") in legal matters <ref type="bibr" coords="1,125.24,386.70,9.20,8.57" target="#b2">[2]</ref>, systematic review in evidence-based medicine <ref type="bibr" coords="1,92.98,397.16,9.20,8.57" target="#b5">[5]</ref>, and the creation of test collections for information retrieval ("IR") evaluation <ref type="bibr" coords="1,193.72,407.62,13.49,8.57" target="#b10">[10]</ref>. Since the reviewers are typically experts in the subject matter, such as lawyers and medicine specialists, assessing massive documents is extremely expensive. For this reason, it is desirable to maximize the recall and minimize the number of reviewed documemts at the same time.</p><p>According to the literature review performed by the authors, there are a number of search e↵orts aimed at achieving high recall, especially in the field of eDiscovery and IR evaluation. However, most of them require the help from search experts <ref type="bibr" coords="1,87.13,522.69,9.71,8.57" target="#b7">[7]</ref> or topic-or database-specific training <ref type="bibr" coords="1,259.92,522.69,14.31,8.57" target="#b14">[14,</ref><ref type="bibr" coords="1,278.59,522.69,10.73,8.57" target="#b15">15]</ref>. In addition, many search methods are not reliable, and require extensive e↵orts for some topics although the e↵ects on average may be acceptable.</p><p>Our goal is to design a more e cient retrieval system to achieve high recall while requiring less judge e↵ort from reviewers. Also, if possible, the system should work generally well for any topic and any collection. To the best of our knowledge, the two automatic TAR tools widely utilized by legal service providers are Simple Active Learning ("SAL") and Simple Passive Learning ("SPL") <ref type="bibr" coords="1,205.21,637.75,9.20,8.57" target="#b2">[2]</ref>. The SPL protocol constructs the training set based on the operator or random selection, while the SAL protocol uses a machine learning algorithm <ref type="bibr" coords="1,95.41,669.14,14.31,8.57" target="#b11">[11]</ref> to identify the training set and always selects documents lying closest to the decision surface, where the learning algorithm is least certain for review <ref type="bibr" coords="1,235.94,690.06,13.50,8.57" target="#b12">[12]</ref>. Cormack and Mojdeh proposed a new protocol called Continuous Active Learning ("CAL") which is similar to the tradition SAL and applies a keyword search, such as BM25, to identify an initial set of documents <ref type="bibr" coords="1,418.94,220.32,9.20,8.57" target="#b4">[4]</ref>. But it sends the top-scoring (most certain) documents identified by the learning algorithm to the reviewers. Cormack and Grossman claimed that CAL is generally more e↵ective than the other two methods and SAL is as e↵ective as CAL only in a best-case scenario <ref type="bibr" coords="1,352.16,272.62,9.20,8.57" target="#b2">[2]</ref>.</p><p>Cormack and Grossman moved one step forward and came up with an autonomous TAR (Auto TAR) configuration that exhibits "greater autonomy, superior e↵ectiveness, increased generalizability, and fewer, more easily detectable failures" compared to existing TAR methods <ref type="bibr" coords="1,461.49,335.39,9.20,8.57" target="#b3">[3]</ref>. This protocol is implemented as the baseline of TREC 2015 Total Recall Track and its details are discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BASELINE MODEL</head><p>Previous researchers tried three di↵erent ways to construct the initial training set <ref type="bibr" coords="1,407.47,407.62,9.21,8.57" target="#b3">[3]</ref>. The method called "Auto-BM25" was seeded with the top-ranked documents given by BM25, while another one labeled "Auto-Syn" was seeded using a synthetic document created from the query. The last method is called "Auto-Rand" and simply selects a random relevant document at the outset. According to the test results on TREC 2009 Legal Track topics and TREC 2002 Filtering Track topics, we can conclude that the Auto-Syn generates better results than the other two, and reckon that's the reason why the Auto-syn is implemented as the baseline in this year's total recall track.</p><p>As we just discussed, the baseline constructs a synthetic document based on the topic as a first relevant document, and adds it to the training set. Then, 100 documents are randomly selected from the corpus and added to the training set as irrelevant documents. A logistic regression classifier trained by these 101 documents ranks all the documents based on how closely they are relevant to the query, and the highest-scoring documents are chosen for expert review. After that, these reviewed documents with their labels are added to the training set and another 100 random documents are brought in as irrelevant documents. The procedure repeats until all relevant documents are retrieved or some predefined criteria is satisfied. Notably, since the training set is augmented with more judged documents, the classifier becomes more accurate and it's reasonable to send more top-scoring documents for review in a later iteration. Cormack claimed that generally, all Auto TAR runs achieve moderate levels of recall with less review e↵ort than CAL, but for very high levels of recall are indistinguishable from CAL <ref type="bibr" coords="2,76.36,67.89,9.20,8.57" target="#b3">[3]</ref>. The SAL and SPL gain curves are generally inferior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMPROVED RETRIEVAL MODEL 3.1 Clustering-based Seed Selection</head><p>We use an interactive procedure to select potential relevant documents. The first approach is inspired by the multiarmed bandit problem. This approach is called the sampling approach. The approach is listed as below:</p><p>1. Select D documents as candidate documents for seed selection;</p><p>2. Group these documents into K (1 &lt; K &lt; D) clusters, where K can be given or learned from data; (a) relevant, increase weights of edges connected to the document;</p><p>(b) irrelevant, decrease weights of edges connected to the document;</p><p>5. Go to step 3 until all D documents are labeled.</p><p>In this paper, top D retrieved documents ranked by BM25 are used in step 1. Latent semantic indexing (LSI) is applied to learn the conceptual correlations of documents and is based on the entropy weighting term-document matrix <ref type="bibr" coords="2,543.65,88.81,9.20,8.57" target="#b6">[6]</ref>.</p><p>Clustering is operated upon the features generated from entropy weighting LSI and is a great way to group documents based on their conceptual similarity. The graph in step 2 of the graph approach is constructed by:</p><p>1. Documents are considered as nodes in the graph;</p><p>2. We run K-means T times to cluster these documents;</p><p>3. The weight wi,j of a un-directed edge between node i and node j is wi,j = P</p><formula xml:id="formula_0" coords="2,433.41,205.04,42.05,10.57">T t=1 It(i, j),</formula><p>where It(•) is an indicator function and It(i, j) = 1 denotes document i and document j are in a cluster based on the t-th clustering result of K-means.</p><p>The following greedy heuristic is used in step 3 of the graph approach.</p><p>1. Initialize a priority queue for documents 2. If the queue is:</p><p>(a) empty, select a document with the highest BM25 score among all unlabeled documents;</p><p>(b) not empty, select a document with highest weight from the queue;</p><p>3. Label document i based on relevance feedback from experts;</p><p>4. If document i is:</p><p>(a) relevant, let (i) = 1;</p><p>(b) irrelevant, let (i) = 1;</p><p>5. For each unlabeled document j which connects to document i, (a) if document j is not in the queue and document i is relevant, insert document j into the queue with weight wi,j + (i);</p><p>(b) if document j is in the queue, increase the weight of document j in the queue by (i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Seed Selection Strategies Comparison</head><p>Apart form the strategy proposed in the previous section, we also tried several other strategies to select initial seed set. (1) Clustering Jumping: We select the document d n i (ith document in n-th cluster ) with the highest BM25 score for judging. Assuming it is coded as relevant, then a document d n j with second-highest score is picked up from the same cluster cn; otherwise, we select the highest-scored document d m k from other cluster cm. This procedure continues until a certain number of relevant documents are obtained.</p><p>(2) Weighted Clustering: The method is similar to the Clustering Jumping and the di↵erence between them is that an initial weight value of 1.0 is assigned to each cluster cn. If a document d n i is labeled as irrelevant, the corresponding cluster cn weight is multiplied by a heuristic factor which is less than 1. Then the document d m j in the cluster cm with the highest factorized weight is chosen for next iteration.</p><p>Table <ref type="table" coords="3,78.11,99.27,4.60,8.57" target="#tab_1">1</ref> shows the comparison results for the 7 topics(tr0-tr7) on the oldreut and 20ng corpora provided by Total Recall organizers 1 . It should be noticed in Table <ref type="table" coords="3,225.22,120.20,4.60,8.57" target="#tab_1">1</ref> that the BM25 ranking only returns 7 documents for tr2 and 63 documents for tr3. So we just skip the clustering algorithm and send all the 7 documents about tr2 for review. For tr3, we only select half of the returned 63 documents for review. The Graph based method achieves the superior results compared to other methods in the most topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Early Stop</head><p>In step 5 of the graph approach and step 8 of the sampling approach, we stop the iteration when all documents are labeled. Our experiments show that usually seed selection works well to identify relevant documents at the beginning.</p><p>Since usually not all candidate documents in seed selection are relevant, our experiments also show using an early stop strategy can improve overall performance. We use the precision to measure performance in this strategy. The strategy we used is:</p><p>1. Set a width, w, of a window to watch performance of seed selection;</p><p>2. Set a lower bound of acceptable precision, r, (0 &lt; r &lt; 1), to measure performance in a window of w iterations;</p><p>3. Set a width, u, of a window to tolerate unacceptable performance in a window of w iterations; (b)  u, increase t by 1 and go to step 5.</p><p>1 http://quaid.uwaterloo.ca:33333/#/doc</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature Engineering</head><p>The baseline model utilizes two di↵erent features to represent each document. These features are vectorized and set as input vectors to train a logistic regression classifier. One of these features is TF-IDF word-based feature and the other one is binary byte 4-grams feature (combinations of 4 sequential characters). According to the experiment results, a method applying TF-IDF word-based feature performs better than that with binary byte 4-grams feature in most cases. However, we find that binary byte 4-grams feature achieves higher accuracy especially when the query is a complete sentence or composed of multiple words. As for keyword query, TF-IDF is e↵ective enough to train a highly accurate linear classifier.</p><p>An intuitive idea is to combine the results derived from these two features so that the classifier can gain di↵erent information. We apply the RRF (Reciprocal Rank Fusion) fusion on the ranking lists R generated from a set D of documents <ref type="bibr" coords="3,316.81,256.68,9.20,8.57" target="#b1">[1]</ref>.</p><formula xml:id="formula_1" coords="3,370.23,287.68,185.68,26.67">RRFscore(d 2 D) = X r2R 1 k + r(d)<label>(1)</label></formula><p>RRF fusion ensures that the highly ranked documents from both features are more important, while the lower-ranked documents does not vanish, where k = 60 was fixed according to previous experience. The result of this fusion is not as satisfying as we expected. It generates almost the same result as that from TF-IDF, yet consumes more computation costs.</p><p>The second idea is using the entropy gi which describes the relative frequency of term i within the entire collection of documents. We can also use the entropy weighting LSI as features of documents. It can be defined as:</p><formula xml:id="formula_2" coords="3,355.73,457.26,200.18,26.55">gi = 1 + X j pij log pij log n , where pij = tfij gfi<label>(2)</label></formula><p>where n is the number of documents in the corpus, gfi denotes the occurrences of term i in the whole corpus and tfij indicates the term frequency of term i in the document j. LSI performs a Singular Value Decomposition (SVD) on the matrix and reduces the high dimensional sparse termdocument matrix into a given size compact matrix. These two features are generated during seed selection phase and we can directly use them for classification. Although both of the features extract the most key information from each document and perform well on clustering, we find that they are not able to make the classification more precise when comparing with TF-IDF. We assume that the dimension reduction would lead to a information loss to a certain extent. Whereas this kind of loss would not be beneficial to distinguishing one document from another.</p><p>Finally, we are inspired from a query example which is "not bad hotel". If we only consider 1-gram feature for this query, it is hard to learn the positive sentiment from the phrase "not bad". On the contrary, the model will probably learn that "bad" is a negative sentiment word which would make the query totally opposite. How about trying n-grams (combi-nations of n sequential words) to deal with this problem? So we try classify TF-IDF values of simply 2-grams, 3-grams, combination of 1-gram and 2-grams, and combination of 1gram, 2-grams and 3-grams separately. And we find that after combining 1-gram features with 2-grams or 3-grams, the results are much better than that of simply 1-gram feature model. Moreover, the combinations of 1-gram and 2grams and the combinations of 1-gram, 2-grams and 3-grams almost break even for di↵erent topics. Taking computation cost into consideration, we finally only adopt the integration of 1-gram and 2-grams words as final features.</p><p>On top of using n-grams as features for document classification, query terms are also reorganized in order to compose query pairs. For example, "Deutsch Mark" is regarded as two independent terms in the baseline model. The TF-IDF values of "Deutsche" and "Mark" are calculated separately in order to compose synthetic document for initializing train set. In our model, two terms pair are composed directly from query terms regardless of the words' relative positions. For "Deutsch Mark", we now have four candidate word pairs for composing synthetic documents which are "Deutsch", "Mark", "Deutsch Mark" and "Mark Deutsch". If the word pair doesn't appear in the vocabulary list (we only record the word whose document frequency is more than once), this pair would be removed from query pairs. In this case, "Mark Deutsch" is deleted from query pairs due to its sparsity. So the new synthetic document string would be:</p><formula xml:id="formula_3" coords="4,103.36,355.56,139.49,68.72">#Rel : 1{ Deutsch : W eight1 Mark : W eight2 Deutsch M ark : W eight3 }</formula><p>where W eighti corresponds to the document frequency of i th word appearing in the whole corpus. After normalization, the sum of all the W eighti would be 1 in order to make the feature vector consistent with other documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Query Expansion</head><p>We also use the query expansion technique to identify potential relevant documents. Given training data, that is, relevant documents and irrelevant documents labeled by human, informative terms are used to expand query and top ranked documents for a expanded query are considered as potential relevant documents to be judged. We adapt the simple mixture (SM) method <ref type="bibr" coords="4,174.41,552.56,14.31,8.57" target="#b13">[13]</ref> to expand the query. For query expansion, we want to extract informative terms from relevant documents. However, not all terms in relevant documents are informative. In SM, a background model is used to model non-informative terms.</p><p>SM assumes that terms in relevant documents are generated as below:</p><p>1. Given two models ✓0 and ✓1;</p><p>2. Given a mixing coe cient, ! ⇡ = (1 ⇡, ⇡); 3. For the j-th term in the i-th relevant document: where ⇡ is given (eg, 0.9) , d(•) is a family of term distributions, ✓0 is a model for informative terms to be estimated, and ✓1 is a known background model.</p><p>In this paper, the bag-of-word assumption is used and multinomial distributions are used as term distributions, which implies d(•) is the family of multinomial distribution. Given a corpus and irrelevant documents obtained from human judegment, we use maximum likelihood estimation (MLE) to estimate a corpus model, ✓corpus, and an irrelevant model, ✓ irrelevant , respectively. The background model used in this paper is:</p><formula xml:id="formula_4" coords="4,329.07,216.45,226.84,14.92">d(w|✓1) = 0.5 ⇥ d(w|✓corpus) + 0.5 ⇥ d(w|✓ irrelevant ) (3)</formula><p>The inference process for SM <ref type="bibr" coords="4,437.17,232.46,14.30,8.57" target="#b13">[13]</ref> is given as below:</p><p>At k-th iteration for SM,</p><formula xml:id="formula_5" coords="4,356.67,269.95,199.24,20.52">⌘ (k) (w) = (1 ⇡)d (k) (w|✓0) (1 ⇡)d (k) (w|✓0) + ⇡d(w|✓1)<label>(4)</label></formula><formula xml:id="formula_6" coords="4,349.47,297.12,206.44,33.73">d (k+1) (w|✓0) = P i tfi(w)⌘ (k) (w) P w 0 2voc P i tfi(w 0 )⌘ (k) (w 0 ) (5)</formula><p>where "voc" denotes the vocabulary for terms and tfi(w) represents the raw term frequency of w in the i-th relevant document.</p><p>Once ✓0 is estimated, we use top K ranked terms in the model to expand a query. In this paper, given a expanded query, we use the Kullback-Leibler (KL) ranking algorithm and top ranked documents are considered as potential relevant documents to be judged. The KL ranking algorithm uses the KL divergence, which measures the di↵erence between a query q and a document d. The divergence estimates relevance of the document with respect to the query. The KL divergence is defined as:</p><formula xml:id="formula_7" coords="4,324.44,477.14,231.48,26.52">KL(✓q||✓ d ) = X w Pr q (w) ⇥ [log(Pr q (w)) log(Pr d (w))] (6)</formula><p>Note that the divergence is asymmetric. E ciency is the main reason why an asymmetric divergence is used. Usually, a query is shorter than a document. With the help of inverted index, the divergence can be e ciently computed. On the other hand, computing a symmetric divergence is slow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Classifier Selection</head><p>The core idea of active learning process is to make classifier continuously improved and more predictive as iterations increase. On the one hand, train set could contain more judged documents as relevance feedback increases. So it is easier to learn a continuously improved classifier with more certain labeled documents. On the other hand, due to the sparsity of relevant documents in the corpus, it would be more and more di cult to retrieve relevant documents especially during the later train phases. Is it possible to find a superior classifier to replace Logistic Regression with Pegasos updates which has been applied in the baseline model?</p><p>In order to find a superior classifier, we tried the methods shown in Table <ref type="table" coords="5,117.77,67.89,3.58,8.57" target="#tab_2">2</ref>. Although the train set is unbalanced in the beginning since not enough relevant documents can be found initially, the linear classifier still can e ciently dig out the relevant documents. So it would be very di cult to beat LR during the initial stages.</p><p>Derived from the above investigation and thoughts, we propose to use non-linear classifier to replace LR when the accuracy of linear classifier starts to decrease dramatically. Our first choice is the Gaussian kernel support vector machine (RBF kernel SVM), which is able to fit the maximum-margin hyperplane in a transformed high-dimensional feature space. We think that if the hyperplane can be located more precisely, the relevant documents can be found by the classifier more e↵ectively. As for the soft margin parameter C and , the best combination of these two parameters would be selected by grid search with exponentially growing sequences of C and , for example, C 2 {2 5 , 2 3 , ..., 2 13 , 2 15 }; 2 {2 15 , 2 13 , ..., 2 3 , 2 5 }. Multi-fold cross validation is operated in each iteration to pick out the best combination of parameters. After applying this strategy, we find that RBF kernel SVM tends to severely overfit with large imbalanced data. Beyond that, RBF kernel SVM takes more computation and spends around 5 times training time comparing with linear classifier.</p><p>Apart from high dimensional classifier, we also tried some other classifiers, such as Stochastic Gradient Descent (SGD) linear SVM, random forest, XGBoosting and Naive Bayes classifiers. None of them can beat logistic regression. This makes sense for a random forest, which as a highly nonlinear, expressive, high-variance classifier needs a relatively high ration of examples to dimensionality. Linear models are less exacting in this respect, they can even work with d(dimensionality) n(documents). We find only SGD linear SVM can draw with LR, so we tried the RRF fusion of two rank lists generated separately from SGD-SVM and LR. The result of this kind of fusion is still almost the same as that of baseline. In addition, cross validation over one fold of train set is also tried within these two linear classifiers, we would select the classifier with higher cross validation accuracy. However, the results is not able to improve a lot while the train time increases.</p><p>We think that a linear model is well enough for sparse highdimensional data such as bag-of-word. If one document contains some key words or related information about a specific query, it can be regarded as relevant document. So the specific features(words or phases) related to the query terms can determine the relevance of corresponding document. Therefore, we decide to keep using LR instead of using some fancy machine learning methods. We notice that 100 irrelevant documents in train set are selected randomly during each iteration, while this kind of randomly picking may introduce train error. Because all the randomly selected documents are not reviewed, they are labeled as "not relevant" documents presumptively. The combination of multiple LR classifiers with di↵erent randomly selected train set could be a good choice. The results show that the RRF fusion of ranking lists generated from five di↵erent LR classifiers can slightly improve the accuracy of classification. This kind of fusion might aid the classifiers in the coverage diverse aspects of the topics. Moreover it gains a lot obviously especially in the beginning stage when presumptive irrelevant documents make up a high proportion. The disadvantage of this fusion is also the high computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SET-UP</head><p>Our improved TAR process has the following phases:</p><p>1. Generate TF-IDF values for 1-gram and 2-grams features. And build index for each document in the corpus. Apply BM25 ranking to return the top 100 documents with the highest score related to a specific query.</p><p>2. Entropy is generated from TF-IDF. The entropy vector of each document is reduced to 200 dimensions by executing Latent Semantic Indexing(LSI).</p><p>3. Cluster the top 100 documents based on their LSI vectors and compose the clustering weighted graph described in section 3.1. Select documents from the most stable pairs and judge document one by one using at most 50 review e↵orts.</p><p>4. One synthetic document is constructed from query terms.</p><p>5. The initial relevant documents train set consists of one synthetic document and the selected judged documents from step 3. Set initial batch size B as 1.</p><p>6. Randomly select 100 documents from the corpus and temporarily label them "not relevant". Add these presumptive not relevant documents to train set.  Following the baseline <ref type="bibr" coords="6,143.12,182.29,9.20,8.57" target="#b3">[3]</ref>, our implementation used a feature space consisting of words (1-gram and 2-grams) occurring at least twice in the collection, and, following <ref type="bibr" coords="6,250.48,203.21,9.20,8.57">[8]</ref>, Porter stemming, elimination of SMART stopwords, and Cornell ltc term weighting. Indri 5.9 provides BM25 and TF-IDF retrieval model methods to retrieve documents. So we select Indri to build index and rank documents during seed selection and query expansion phases. LSI operation involves dimensionality reduction and singular value decomposition (SVD) which requires numerous calculations. RedSVD is an e↵ective tool to accelerate SVD computation and can shorten the generation time around 3 times. As for clustering, we use K-Means clustering method from Scikit Learn package <ref type="bibr" coords="6,162.73,328.74,9.20,8.57" target="#b9">[9]</ref>. Based on n top ranked documents with the highest BM25 score, we set log n as k clusters. In this case, k is 7 where n = 100.</p><p>For the learning algorithm, we still choose the Sofia-ML implementation of Pegasos SVM, with the following parameters: "-iterations 2000000 -dimensionality 110000". However, the dimensionality needs to be dynamically updated according to the size of features. For large corpus and ngrams features, we may increase the dimensionality.</p><p>The Total Recall organizers provide three di↵erent dataset modes (trivial, test and bigtest) for testing, which respectively contain 7 topics with 2 datasets containing 30 documents each, 7 topics with 2 datasets (Oldreut and 20ng) containing about 20,000 documents each and 2 topics with one dataset (Enron) containing about 750,000 documents. The corpora are mostly made by news articles and emails. In order to verify the e↵ectiveness of our model, we also test our method on 50 selected topics with high prevalence of relevant documents from Newreut and Robust04 corpora.</p><p>There are two tasks to submit the final results. One is Playat-Home in which we can run our own systems locally and access the automated assessors via the Internet. The other one is SandBox in which we should set up our systems in the virtual machine. And we submit the virtual machine so that the coordinators will execute our system within a restricted environment. The methods of two submissions are almost the same. However, there are some slight di↵erences between these two deployments, for example, the configuration for virtual machine environment. We upload our Play-at-Home code to BitBucket 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PRIMARY RESULTS</head><p>2 https://bitbucket.org/HaotianZHANG/uwtotalrecall In order to evaluate our models, we run our systems on di↵erent types of corpus, such as Oldreut, 20ng, Enron, Newreut and Robust04 respectively. We only list the results from Oldreut and 20ng in this paper. Because most of the documents in these two corpora have been judged so the results from them are more reliable and persuasive. Whereas for Newruet and Robust04, both of them only have a small portion of documents judged and the prevalence of relevant documents is also small compared to Oldreut and 20ng. In general, there are two standard ways to evaluate the results: as gain curves and as 75% recall-e↵ort values. A gain curve keeps track of the number of e↵orts to reach a certain number of relevant documents. Seven topics from oldreut and 20ng are listed from Figure <ref type="figure" coords="6,452.44,329.81,4.60,8.57" target="#fig_4">1</ref> to Figure <ref type="figure" coords="6,502.89,329.81,3.58,8.57">7</ref>. As shown in Table <ref type="table" coords="6,353.43,340.27,3.58,8.57" target="#tab_3">3</ref>, the 75% recall-e↵ort values records the amount of e↵orts to achieve recall = 0.75. The gain curve enables us more intuitively to look at the e↵ort spent for any given level of recall. From the figures and table, we can find that our improved models can achieve 75% recall with less e↵ort for topic: tr0 to tr3. As for the topics from tr4 to tr6, there is no clear winner among the improved models and baseline. All the methods are close and the accuracy of baseline are nearly 92% and our models still performs slightly better than the baseline.  achieve high recall with less e↵ort. As for classifier selection, the linear classifier performs well enough for sparse highdimensional data. We also find that the performance of our models varies for di↵erent kinds of topics and datasets. How to wisely choose strategies to deal with di↵erent situations remains a problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,64.58,542.33,192.30,8.57;3,260.72,548.64,6.60,2.26;3,271.40,542.33,21.50,8.57;3,76.21,552.79,69.41,8.57;3,148.69,559.10,3.98,2.26;3,155.74,552.79,61.37,8.57;3,220.18,559.10,3.33,2.26;3,226.07,552.79,7.16,8.57;3,235.80,559.10,6.60,2.26;3,244.69,552.79,16.37,8.57;3,64.58,574.31,21.88,8.57;3,90.64,580.62,15.08,2.26;3,109.91,574.31,6.91,8.57;3,121.27,580.62,22.03,2.26;3,147.49,574.31,122.79,8.57;3,274.47,580.62,4.77,2.26;3,283.44,574.31,9.46,8.57;3,76.21,584.77,216.69,8.57;3,76.21,595.23,39.15,8.57;3,126.63,601.54,6.60,2.26;3,135.52,595.23,16.38,8.57;3,153.44,601.54,8.45,2.26;3,165.97,595.23,6.13,8.57;3,177.20,597.87,3.84,1.51;3,176.37,604.59,5.34,1.51;3,183.06,595.23,2.56,8.57;3,79.70,616.75,11.75,8.57;3,95.94,623.06,16.70,2.26;3,115.70,616.75,31.79,8.57;3,150.56,623.06,3.98,2.26;3,157.61,616.75,19.94,8.57;3,79.19,632.55,12.26,8.57;3,105.67,638.86,6.97,2.26;3,115.70,632.55,11.32,8.57;3,130.08,638.86,3.98,2.26;3,137.13,632.55,30.19,8.57;3,64.58,654.07,50.78,8.57;3,118.42,660.38,6.54,2.26;3,79.70,675.59,11.75,8.57;3,95.94,681.90,17.59,2.26;3,116.59,675.59,129.57,8.57"><head>4 .</head><label>4</label><figDesc>Let the seed selection program run the first w iterations, set counter c to zero, and let t = w + 1; 5. At t-th (t &gt; w) iteration of seed selection, let d be the number of relevant documents found at iteration interval [t w + 1, t]. If d w : (a) &lt; r, increase c by 1; (b) r, set c to zero; 6. If counter c: (a) &gt; u, stop the seed selection program;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,79.70,700.52,213.20,8.57;4,95.94,710.98,30.69,8.57;4,129.70,717.29,10.35,2.26;4,143.11,711.12,7.16,14.78;4,152.84,707.26,56.74,14.78;4,202.17,717.29,5.27,2.26;4,209.58,710.98,6.14,8.57;4,342.21,57.43,165.06,8.57;4,509.51,63.74,12.67,2.26;4,525.24,57.57,7.16,14.78;4,534.97,62.70,31.52,4.56;4,567.49,57.43,6.14,8.57"><head></head><label></label><figDesc>(a) Firstly, independently generate a latent model indicator, zji ⇠ Bernoulli(z| ! ⇡ ); (b) Then, independently generate a term, wji ⇠ d(w|✓z ji );</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,327.59,639.05,228.32,8.57;5,339.23,649.52,107.46,8.57;5,450.55,649.66,4.09,14.78;5,455.85,652.16,9.19,1.51;5,458.76,658.88,3.65,1.51;5,466.52,649.66,4.09,14.78;5,474.49,649.51,81.43,8.57;5,339.23,659.98,216.69,8.57;5,339.23,670.44,121.94,8.57;5,327.59,688.72,186.73,8.57;5,516.53,688.87,4.09,14.78;5,521.82,691.37,9.19,1.51;5,524.73,698.08,3.65,1.51;5,532.49,688.87,4.09,14.78;5,538.80,688.72,17.11,8.57;5,339.23,700.52,170.86,8.57;5,513.69,700.66,4.09,14.78;5,518.98,703.16,5.54,1.51;5,520.06,709.88,3.65,1.51;5,526.00,700.66,4.09,14.78;5,533.69,700.52,22.23,8.57;5,339.23,710.98,118.04,8.57"><head>7. Train 5</head><label>5</label><figDesc>Logistic Regression classifier with di↵erent presumptive train set. Select d 4B 5 e documents with the highest score from fusion list for review and label them as "relevant" or "not relevant". 8. If the prevalence of relevant document in the d 4B 5 e documents is high, continue judging the next b B 5 c documents with the highest score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,64.58,57.43,228.32,8.57;6,76.21,67.89,216.68,8.57;6,76.21,78.35,216.68,8.57;6,76.21,88.81,216.69,8.57;6,76.21,99.27,13.28,8.57;6,92.57,99.42,4.09,14.78;6,97.86,101.92,5.54,1.51;6,98.95,108.64,3.65,1.51;6,104.88,99.42,4.09,14.78;6,112.04,99.27,85.73,8.57;6,59.98,120.84,232.92,8.57;6,76.21,131.31,24.12,8.57;6,103.26,137.62,6.97,2.26;6,113.60,131.31,9.72,8.57;6,126.24,131.45,4.09,14.78;6,132.27,133.95,5.54,1.51;6,131.53,140.67,7.31,1.51;6,140.03,131.45,6.65,14.78;6,150.72,131.31,142.17,8.57;6,76.21,141.77,216.68,8.57;6,76.21,152.23,59.12,8.57"><head>9 .</head><label>9</label><figDesc>Otherwise, obtain expanded terms from judged documents and generate a new ranked list based on Indri TF-IDF retrieval model ranking score. Execute RRF fusion with the list generated from step 7 and select top b B 5 c documents to review. 10. Add all the reviewed documents to the train set. Increase B by d B 10 e. Return to step 6 and start the next iteration until all the documents in the corpus have been reviewed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,316.81,643.92,239.10,2.26;6,316.81,654.38,17.04,2.26;6,320.88,449.78,230.98,173.23"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: tr0-Relevant Documents vs. Review Effort</figDesc><graphic coords="6,320.88,449.78,230.98,173.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,53.80,247.93,239.10,2.26;7,53.80,258.40,17.04,2.26;7,57.86,269.34,230.98,173.23"><head>Figure 2 :Figure 5 :</head><label>25</label><figDesc>Figure 2: tr1-Relevant Documents vs. Review Effort</figDesc><graphic coords="7,57.86,269.34,230.98,173.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.80,220.56,239.11,67.11"><head>Table 1 :</head><label>1</label><figDesc>Seed Selection Strategies Comparison Us-</figDesc><table coords="3,53.80,231.02,217.25,56.65"><row><cell>ing 50 E↵ort</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Methods tr0 tr1 tr2 tr3 tr4 tr5 tr6</cell></row><row><cell cols="2">Jumping 46</cell><cell>1</cell><cell>2</cell><cell>10 47 49 40</cell></row><row><cell cols="2">Weighted 46</cell><cell>0</cell><cell>2</cell><cell>10 47 49 42</cell></row><row><cell cols="2">Sampling 45</cell><cell>1</cell><cell>2</cell><cell>14 48 49 46</cell></row><row><cell>Graph</cell><cell>47</cell><cell>2</cell><cell>2</cell><cell>15 45 50 45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,53.80,101.33,248.05,199.79"><head>Table 2 :</head><label>2</label><figDesc>Classifiers Applied During Experiments</figDesc><table coords="5,53.80,105.58,248.05,195.54"><row><cell>Classifier</cell><cell>Toolbox</cell><cell>Feature</cell></row><row><cell>Logistic Regression</cell><cell>Sofia-ML</cell><cell>Unigram TF-IDF</cell></row><row><cell>Logistic Regression</cell><cell>Sofia-ML</cell><cell>N-gram TF-IDF</cell></row><row><cell>Logistic Regression</cell><cell>Sofia-ML</cell><cell>4-char TF-IDF</cell></row><row><cell>Linear SVM</cell><cell>LIBSVM</cell><cell>Unigram TF-IDF</cell></row><row><cell>Linear SVM &amp; LR fusion</cell><cell>Sofia-ML</cell><cell>4-gram TF-IDF</cell></row><row><cell>RBF SVM</cell><cell>LIBSVM</cell><cell>Entropy</cell></row><row><cell>RBF SVM</cell><cell>LIBSVM</cell><cell>Unigram TF-IDF</cell></row><row><cell>Decision Tree</cell><cell cols="2">Scikit-Learn Unigram TF-IDF</cell></row><row><cell>Naive Bayes</cell><cell cols="2">Scikit-Learn Unigram TF-IDF</cell></row><row><cell>AdaBoost</cell><cell cols="2">Scikit-Learn Unigram TF-IDF</cell></row><row><cell>Gradient Boosting</cell><cell>XGboost</cell><cell>Unigram TF-IDF</cell></row><row><cell cols="3">By recording the number of relevant documents found in</cell></row><row><cell cols="3">each iteration, we find that the logistic regression (LR) clas-</cell></row><row><cell cols="3">sifier performs very well in the initial stages. The percentage</cell></row><row><cell cols="3">of relevant documents among all the highest-scoring rele-</cell></row><row><cell cols="3">vant documents returned by the classifier is around 90%.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,322.19,169.08,239.71,94.60"><head>Table 3 :</head><label>3</label><figDesc>75% Recall E↵ort of Di↵erent Methods</figDesc><table coords="6,322.19,178.40,239.71,85.28"><row><cell cols="5">Topic SD 2gram SD 3gram SD 2gram QE Baseline</cell></row><row><cell>tr0</cell><cell>2119</cell><cell>2150</cell><cell>2107</cell><cell>2145</cell></row><row><cell>tr1</cell><cell>118</cell><cell>183</cell><cell>137</cell><cell>164</cell></row><row><cell>tr2</cell><cell>157</cell><cell>243</cell><cell>188</cell><cell>273</cell></row><row><cell>tr3</cell><cell>215</cell><cell>208</cell><cell>247</cell><cell>248</cell></row><row><cell>tr4</cell><cell>762</cell><cell>763</cell><cell>762</cell><cell>761</cell></row><row><cell>tr5</cell><cell>761</cell><cell>762</cell><cell>762</cell><cell>760</cell></row><row><cell>tr6</cell><cell>764</cell><cell>761</cell><cell>774</cell><cell>765</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Prof. Gordon Cormack</rs> and <rs type="person">Adam Roegiest</rs> for their helpful guidance and feedback.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,58.28,183.29,96.81,15.55" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,198.90,192.93,8.57;8,72.59,209.36,202.03,8.57;8,72.59,219.82,147.84,8.57;8,223.49,226.13,56.65,2.26;8,72.59,236.60,201.73,2.26;8,72.59,247.06,205.96,2.26;8,72.59,251.20,111.25,8.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,72.59,209.36,202.03,8.57;8,72.59,219.82,131.76,8.57">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,223.49,226.13,56.65,2.26;8,72.59,236.60,201.73,2.26;8,72.59,247.06,202.42,2.26">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,262.66,208.12,8.57;8,72.59,273.12,201.98,8.57;8,72.59,283.58,131.30,8.57;8,206.96,289.90,71.93,2.26;8,72.59,300.36,184.31,2.26;8,72.59,310.82,198.45,2.26;8,72.59,314.96,111.25,8.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,226.91,262.66,53.81,8.57;8,72.59,273.12,201.98,8.57;8,72.59,283.58,115.93,8.57">Evaluation of machine-learning protocols for technology-assisted review in electronic discovery</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,206.96,289.90,71.93,2.26;8,72.59,300.36,184.31,2.26;8,72.59,310.82,194.90,2.26">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,326.42,214.39,8.57;8,72.59,336.88,170.89,8.57;8,72.59,347.34,108.34,8.57;8,184.00,353.66,57.70,2.26;8,72.59,364.12,72.92,2.26;8,148.58,357.80,20.94,8.57" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,226.91,326.42,60.07,8.57;8,72.59,336.88,170.89,8.57;8,72.59,347.34,104.28,8.57">Autonomy and reliability of continuous active learning for technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06868</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,72.59,369.26,215.65,8.57;8,72.59,379.72,189.60,8.57;8,72.59,390.18,115.12,8.57;8,190.77,396.50,28.67,2.26;8,222.50,390.18,20.95,8.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,205.11,369.26,83.13,8.57;8,72.59,379.72,189.60,8.57;8,72.59,390.18,99.81,8.57">Machine learning for information retrieval: Trec 2009 web, relevance feedback and legal tracks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mojdeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,190.77,396.50,22.93,2.26">TREC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,401.64,118.01,8.57;8,193.66,407.95,91.43,2.26;8,72.59,418.41,142.62,2.26;8,218.28,412.10,65.47,8.57;8,72.59,422.56,85.56,8.57" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,193.66,407.95,91.43,2.26;8,72.59,418.41,138.70,2.26">Cochrane handbook for systematic reviews of interventions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Wiley Online Library</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,434.02,220.31,8.57;8,72.59,450.79,204.62,2.26;8,72.59,461.25,203.90,2.26;8,72.59,471.71,86.09,2.26;8,161.74,465.40,102.05,8.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,127.03,434.02,150.45,8.57">Probabilistic latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,72.59,450.79,204.62,2.26;8,72.59,461.25,203.90,2.26;8,72.59,471.71,82.54,2.26">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,476.86,209.90,8.57;8,72.59,487.32,220.31,8.57;8,72.59,497.78,214.91,8.57;8,72.59,508.24,125.07,8.57" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Reinhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Brassil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Rugani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jade</surname></persName>
		</author>
		<title level="m" coord="8,161.14,487.32,131.77,8.57;8,72.59,497.78,169.37,8.57">H5 at trec 2008 legal interactive: user modeling, assessment &amp; measurement</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="8,72.59,519.70,219.41,8.57;8,72.59,530.16,196.67,8.57;8,72.59,540.62,35.13,8.57;8,110.80,546.93,176.74,2.26;8,72.59,551.08,65.93,8.57" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,258.13,519.70,33.88,8.57;8,72.59,530.16,196.67,8.57;8,72.59,540.62,31.23,8.57">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,110.80,546.93,172.57,2.26">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,562.54,217.34,8.57;8,72.59,573.00,204.50,8.57;8,72.59,583.46,195.95,8.57;8,72.59,593.92,178.69,8.57;8,72.59,604.38,192.10,8.57;8,72.59,614.84,32.09,8.57;8,107.74,621.15,157.94,2.26;8,72.59,625.30,79.73,8.57" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,132.49,604.38,132.20,8.57;8,72.59,614.84,27.50,8.57">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,107.74,621.15,153.76,2.26">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,636.76,209.05,8.57;8,72.59,647.22,107.97,8.57;8,183.62,653.53,92.38,2.26;8,72.59,663.99,194.19,2.26;8,72.59,674.45,205.96,2.26;8,72.59,678.60,102.04,8.57" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,185.77,636.76,95.87,8.57;8,72.59,647.22,92.44,8.57">Forming test collections with no system pooling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,183.62,653.53,92.38,2.26;8,72.59,663.99,194.19,2.26;8,72.59,674.45,202.42,2.26">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,690.06,203.18,8.57;8,72.59,700.52,58.81,8.57;8,134.47,706.83,136.81,2.26;8,72.59,710.98,68.50,8.57" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,130.79,690.06,144.98,8.57;8,72.59,700.52,54.89,8.57">Machine learning in automated text categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,134.47,706.83,131.76,2.26">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.60,57.43,217.51,8.57;8,335.61,67.89,191.11,8.57;8,529.78,74.20,15.51,2.26;8,335.61,84.67,157.94,2.26;8,496.61,78.35,56.69,8.57" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,430.35,57.43,122.77,8.57;8,335.61,67.89,187.55,8.57">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,529.78,74.20,15.51,2.26;8,335.61,84.67,153.76,2.26">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.60,89.81,212.81,8.57;8,335.61,100.27,214.31,8.57;8,335.61,110.73,8.43,8.57;8,347.10,117.04,208.49,2.26;8,335.61,127.50,165.57,2.26;8,504.25,121.19,22.03,8.57;8,335.61,131.65,86.14,8.57" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,433.56,89.81,114.85,8.57;8,335.61,100.27,210.75,8.57">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>La↵erty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,347.10,117.04,208.49,2.26;8,335.61,127.50,160.74,2.26">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.60,143.11,218.07,8.57;8,335.61,153.57,80.03,8.57;8,418.70,159.88,92.73,2.26;8,335.61,170.34,219.98,2.26;8,335.61,180.80,149.78,2.26;8,488.45,174.49,59.87,8.57;8,335.61,184.95,48.32,8.57" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,436.19,143.11,117.48,8.57;8,335.61,153.57,64.42,8.57">Interactive retrieval based on faceted feedback</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,418.70,159.88,92.73,2.26;8,335.61,170.34,219.98,2.26;8,335.61,180.80,146.24,2.26">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.60,196.41,213.79,8.57;8,335.61,206.87,198.40,8.57;8,335.61,217.33,67.88,8.57" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,519.44,196.41,29.95,8.57;8,335.61,206.87,96.45,8.57">Ucsc at relevance feedback track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>De Arma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
