<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.47,83.99,334.77,16.49;1,117.11,105.90,375.50,16.49">Mining Tasks from the Web Anchor Text Graph: MSR Notebook Paper for the TREC 2015 Tasks Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-01-31">January 31, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.16,134.97,80.58,15.55"><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,357.11,134.97,77.48,15.55"><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
							<email>ryenw@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.47,83.99,334.77,16.49;1,117.11,105.90,375.50,16.49">Mining Tasks from the Web Anchor Text Graph: MSR Notebook Paper for the TREC 2015 Tasks Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-01-31">January 31, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">D44BFAEA4EB2C2E0CF65F8F6E0C990C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Users may have a variety of tasks that give rise to issuing a particular query. The goal of the Tasks Track at TREC 2015 was to identify all aspects or subtasks of a user's task as well as the documents relevant to the entire task. This was broken into two parts: (1) Task Understanding which judged relevance of key phrases or queries to the original query (relative to a likely task that would have given rise to both); (2) Task Completion which performed document retrieval and measured usefulness to any task a user with the query might be peforming through either a completion measure that uses both relevance and usefulness criteria or more simply through an ad hoc retrieval measure of relevance alone. We submitted a run in the Task Understanding track. In particular, since the anchor text graph has proven useful in the general realm of query reformulation <ref type="bibr" coords="1,263.48,434.23,10.58,9.54" target="#b2">[2]</ref>, we sought to quantify the value of extracting key phrases from anchor text in the broader setting of the task understanding track.</p><p>Given a query, our approach considers a simple method for identifying a relevant and diverse set of key phrases related to the possible tasks that might have given rise to the query. In particular, given the effectiveness of sessions for producing query suggestions as well as the fact that sessions tend to be both topically coherent and cohesive with respect to a task, we investigated the effectiveness of mining session co-occurrence data. For a search engine log, session boundaries can be defined in the typical way but to operate over the anchor text graph, we need some notion of a "session". We adopt the suggestion of Dang &amp; Croft <ref type="bibr" coords="1,213.76,602.29,11.62,9.54" target="#b2">[2]</ref> and treat different links pointing to the same document as belonging to the same "session". The basic assumption is that the anchor text of two links pointing to the same document are related via the common reference. Note that this assumption is based on the destination URL of the link being the same.</p><p>Given a query, we then find matching seed candidates (link text from the web graph or queries over search logs) and expand to related candidate key phrases via this session assumption. The final ranking is based on a combination of session count and the similarity of a link to the query. Additionally we perform several types of filtering that prevent over-expanding the set of related queries. We refer to the method as "having coverage" if the method was able to find a matching seed -since this is a necessary step to producing any candidates based on co-occurrence.</p><p>Empirical results demonstrate generally good performance for the method when it finds a matching seed. In particular, of the 34 topics judged for the Query Understanding track, our method had coverage 62% of the time (21 topics). When the method has coverage, the suggested key phrases are above the mean performance (by nearly every measure reported) 2/3 times and the best performer 1/3 times. Given it's simplicity and availability to nearly all participants as well as the fact that coverage can be detected before submission, it is a promsing candidate for future investigation in the track. We now describe the method and results more fully before summarizing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Query Understanding Task</head><p>This section deals with the problem investigated in the Query Understanding task of the track. That is, how can we effectively identify all key phrases or alternate queries that might be involved in any task a user might have which would give rise to the observed query for a topic?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approach Overview</head><p>To provide a brief overview, we remind the reader that our goal is to investigate the effectiveness of basic session cooccurrence for the task understanding task. For search logs, taking the commonly accepted session boundaries of a period of user activity demarcated by 30 minutes of inactivity is straightforward, but to apply the same approach to anchor text we need a notion of session. Like others <ref type="bibr" coords="1,499.61,674.70,11.62,9.54" target="#b2">[2]</ref> for the anchor text graph, we define the text of two links to co-occur in a "session" if the links point to the same destination URL. Given a query, we find matching seed candidates (link text from the web graph or queries over search logs) using a soft matching. These seed candidates are further expanded to all queries/links co-occurring in a session. The candidates are pruned to filter out any globally frequent queries across all sessions, extremely long queries, or candidates that do not provide a minimum similarity to the original query. The final ranking is a simple product of the session count in matched sessions and a similarity of the candidate with the original query. We now describe the method in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">ClueWeb12 Anchor Text Graph</head><p>For most interested participants an extraction of the anchor text graph of ClueWeb12 is easily available at http: //wwwhome.ewi.utwente.nl/ Ëœhiemstra/2013/ anchor-text-for-clueweb12.html <ref type="bibr" coords="2,239.57,233.32,10.58,9.54" target="#b3">[3]</ref>, however we chose <ref type="foot" coords="2,90.60,243.68,3.49,6.68" target="#foot_0">1</ref> to produce an anchor text graph directly from the ClueWeb12 dataset. <ref type="foot" coords="2,133.24,255.64,3.49,6.68" target="#foot_1">2</ref>In particular, we used the publically available HTM-LAgilityPack<ref type="foot" coords="2,107.34,279.55,3.49,6.68" target="#foot_2">3</ref> v. 1.4.9.0. Similar to Hiemstra &amp; Hauff <ref type="bibr" coords="2,281.29,281.14,11.62,9.54" target="#b3">[3]</ref> we only process documents whose html size was less than 50K bytes and we discard "javascript" or "mailto" links. Additionally, we attempt to retain<ref type="foot" coords="2,196.90,315.42,3.49,6.68" target="#foot_3">4</ref> only links where the destination is to an external site. We do this since internal links are often either navigational or assume context. That is, anchor text of "bothell campus" on a "University of Washington" page is likely pointing at the home page for the "University of Washington at Bothell" but the simple descriptor "bothell campus" would not be a high quality keyphrase suggestion absent the context. Additionally, we also only retain links whose destination URL resolves to a document in ClueWeb12 (i.e. we discard links pointing out of ClueWeb12). This was done under the assumption that documents within ClueWeb12 may have better crawl coverage across multiple incoming links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Normalizing Queries to Filter Phrases</head><p>For the text in each of the query fields in the topic xml file, queries were first normalized by removing multiple whitespaces and converting all characters to lower case. After this, we removed stopwords from each of the query text. Rather than stopword lists that are developed based on frequency alone, we used a list of English function words, e.g., "a", "about", "the", "to", "who", etc. While function words are correlated with frequency, anecdotally we found other published stopword lists developed by frequency alone or by taking the most frequent words in our corpora to be too aggressive. Investigating the impact of this choice is an interesting direction for future work.</p><p>As our list of English function words, we took the list of 221 words published by Cook <ref type="bibr" coords="2,435.98,68.97,11.62,9.54" target="#b0">[1]</ref> and available for download at http://homepage.ntlworld.com/vivian.c/ Words/StructureWordsList.htm. For a topic, t, with a particular query q t , we refer to the output after query normalization and stopword removal as a filter phrase, f t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Compute Globally Frequent Candidates</head><p>For any query suggestion method, any co-occurrence approach has to deal with globally frequent items that co-occur independently. To deal with this, we compute the top 1K most frequent texts based on the input. For example, for the ClueWeb12 anchor text graph, the top four most common anchor texts are: "next", "permalink", "prev", and "read more". These top 1K globally frequent candidates will be pruned out later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Matching Filter Phrases to Seed Candidates</head><p>After normalizing queries to filter phrases as described in Section 2.1.2, for each topic a filter phrase f t is considered to match a candidate seed, c t , if the candidate is a superset of f t . That is the candidate seed, c t contains at least all of the words in f t . Because the filter phrase, f t , is never bigger than the original query, q t . This means the filter phrase will match seeds that exactly match the query, partially overlap with the query (as long as all non-function words overlap), and are supersets of the query. The intuition behind adding supersets is since the goal is to identify all possible tasks that might lead to the query, users' queries or anchor text links often contain extra words that are specific to some particular task. An interesting line of future work is to separate the contribution of exact matching seeds (with and without order), overlapping matching seeds, and superset seeds. This set of matching sessions S t , which contain a candidate seed match, is the basis for the remainder of our computation for a topic t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Expanding to Related Candidates</head><p>From the matching sessions containing a matching seed, we expand to related candidates by removing all globally frequent candidates and simply counting the number of sessions in S t each remaining query occurs in. As in the query suggestion literature, future work could consider multiple rounds of expansion or a weighted random walk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.6">Filtering and Similarity Weighting</head><p>The expansion to related candidates based on co-occurrence has several types of common failures. To be conservative and attempt to eliminate these failures, we require a candidate to have overlap with the filter phrase for a topic and meet a length restriction (very long texts will tend to match spuriously). In particular, for every topic t and candidate keyphrase k t and filter phrase f t :</p><p>1. k t is discarded if cos(k t , f t ) ï£¿ 0 (i.e. there must be at least one word overlap).</p><p>2. k t is discarded if its length is longer than a multiple of f t , i.e. if k t &gt; f t L. We choose L = 4 to allow a generous but not extreme upper bound. This condition gets rid of extremely long candidates -usually pastes in search logs or bad tag closures in anchor text.</p><p>To produce the final score, it's intuitive that not only should the frequency of occurrence in a matching session matter, but the candidate's similarity to the query is likely important. To account for this, we weight the count of occurrences in matching sessions, s kt , by the similarity to the filter phrase before normalization. More precisely, we first 1. Scale the matching session occurrence count for the keyphrase by the similarity to the filter phrase:</p><formula xml:id="formula_0" coords="3,61.27,289.30,125.22,29.19">s unnorm kt = cos(k t , f t ) â€¢ s kt . 2.</formula><p>Normalize the final score by the max in the topic:</p><formula xml:id="formula_1" coords="3,73.72,321.01,135.16,11.20">s final kt = s unnorm kt / max kt s unnorm kt</formula><p>. This simply scales the final score to the [0, 1] range and does not alter the final ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>Table <ref type="table" coords="3,78.88,391.84,4.98,9.54">1</ref> reports the mean across all topics differences from the per topic mean by each performance measure (i.e. positive means above mean overall and negative below mean). Table <ref type="table" coords="3,79.45,427.70,4.98,9.54">2</ref> reports similar values but only over the 21 of the 34 judged topics where the method had coverage. In the remaining 13 a matching seed was not found. Table <ref type="table" coords="3,257.48,451.61,4.98,9.54" target="#tab_0">3</ref> reports of the 21 times when the method had coverage, the number of topics where the method the best performer or above average.</p><p>As can be seen, overall the method falls below the mean across all topics, but when taking Table <ref type="table" coords="3,215.90,511.39,4.98,9.54">2</ref> into account, this is because the method sometimes lacks coverage. Since this can be easily detected, the potential of this method for use in combination with other techniques is represented by its performance when it has coverage. In those cases, the mean across topics is quite positive with (as seen in Table <ref type="table" coords="3,269.20,571.16,4.15,9.54" target="#tab_0">3</ref>) the method performing above the mean 2/3 times and obtaining the best performance 1/3 times that it has coverage. Overall, this speaks well to the potential for combining this method with techniques used by other participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusions</head><p>We described a simple approach that can be equally applied to either search logs or the anchor text graph for finding keyphrases for related tasks to a query. The method relies on a simple matching procedure to find starting seeds and uses either common destinations in the anchor text graph or session co-occurrence in a search log to find related candidates. Simple steps of filtering are applied to remove globally frequent candidates as well as candidate keyphrases that have no similarity to the core of the original query.</p><p>Overall, empirical results demonstrate generally good performance for the method when it finds a matching seed. In particular, of the 34 topics judged for the Query Understanding track, our method had coverage 62% of the time (21 topics). When the method has coverage, the suggested key phrases are above the mean performance (by nearly every measure reported) 2/3 times and the best performer 1/3 times. Given its simplicity and availability to nearly all participants as well as the fact that coverage can be detected before submission, it is a promising candidate for future investigation in the track.</p><p>ERR-IA@10 ERR-IA@20 ERR-IA@1000 â†µ-nDCG@10 â†µ-nDCG@20 â†µ-nDCG@1000 -0.0154 -0.0219 -0.0232 -0.0177 -0.0446 -0.0681 Table <ref type="table" coords="4,212.50,169.99,3.88,9.54">1</ref>: Overall Average of Difference from Mean Topic.</p><p>ERR-IA@10 ERR-IA@20 ERR-IA@1000 â†µ-nDCG@10 â†µ-nDCG@20 â†µ-nDCG@1000 0.0959 0.0894 0.0882 0.1126 0.0871 0.0632 Table <ref type="table" coords="4,180.28,389.70,3.88,9.54">2</ref>: Overall Average of Difference from Mean Topic when Coverage.</p><p>ERR-IA@10 ERR-IA@20 ERR-IA@1000 â†µ-nDCG@10 â†µ-nDCG@20 â†µ-nDCG@1000 Max </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,79.34,586.67,396.15,44.63"><head>Table 3 :</head><label>3</label><figDesc>Times best and better than mean when coverage.</figDesc><table coords="4,79.34,586.67,396.15,21.90"><row><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>6</cell><cell>6</cell></row><row><cell>&gt; Mean 15</cell><cell>15</cell><cell>15</cell><cell>15</cell><cell>15</cell><cell>14</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,68.14,654.33,224.76,7.64;2,53.80,663.80,207.00,7.64"><p>Unfortunately the anchor text distribution is via peer-to-peer software the use of which is procedurally complicated at our organization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,68.14,673.53,11.51,7.64;2,81.21,678.91,200.85,1.90;2,283.61,673.53,9.29,7.64;2,53.80,682.99,67.76,7.64"><p>See http://www.lemurproject.org/clueweb12.php/ for more on ClueWeb12.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,68.14,698.10,172.15,1.90"><p>http://htmlagilitypack.codeplex.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,68.14,702.45,224.76,7.64;2,53.80,711.91,238.31,7.64"><p>Many types of relative links might not be detected as well as sites which appear to be different by the URL but are owned by the same organization.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,333.41,292.23,40.71,9.54;3,396.07,292.23,159.84,9.54" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Cook</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.41,301.73,168.15,12.01;3,526.03,310.91,29.89,2.38;3,333.41,322.87,197.26,2.38;3,333.41,334.82,199.75,2.38" xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://homepage.ntlworld.com/vivian.c/Writings/Papers/CalicoPaper88.htm" />
	</analytic>
	<monogr>
		<title level="j" coord="3,333.41,301.73,73.52,11.90">CALICO Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="67" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.41,348.03,222.51,9.54;3,333.41,357.52,185.50,12.01" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,431.91,348.03,124.01,9.54;3,333.41,359.98,34.02,9.54">Query reformulation using anchor text</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,385.27,357.52,49.19,11.90">WSDM 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.41,379.91,222.50,9.54;3,333.41,391.86,222.50,9.54;3,333.41,403.82,222.50,9.54;3,333.41,415.77,222.50,9.54;3,333.41,427.73,95.74,9.54" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="3,451.98,379.91,103.93,9.54;3,333.41,391.86,114.83,9.54">Mirex: Mapreduce information retrieval experiments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Centre for Telematics and Information Technology, University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report CTIT Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
