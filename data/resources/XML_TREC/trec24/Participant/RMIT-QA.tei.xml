<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.86,79.96,238.11,12.54">RMIT at the TREC 2015 LiveQA Track</title>
				<funder ref="#_2vbrtmd">
					<orgName type="full">NPRP</orgName>
				</funder>
				<funder>
					<orgName type="full">Qatar National Research Fund</orgName>
				</funder>
				<funder ref="#_uV2eBXB">
					<orgName type="full">Australian Research Council&apos;s Discovery Projects Scheme</orgName>
				</funder>
				<funder ref="#_mFjZhab">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Qatar Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,131.81,137.59,72.52,8.64"><forename type="first">Ruey-Cheng</forename><surname>Chen</surname></persName>
							<email>ruey-cheng.chen@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.37,137.59,101.23,8.64"><roleName>Tadele</roleName><forename type="first">Shane</forename><surname>Culpepper</surname></persName>
							<email>shane.culpepper@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.08,137.59,66.44,8.64"><forename type="first">Tadela</forename><surname>Damessie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.60,137.59,57.93,8.64"><forename type="first">Timothy</forename><surname>Jones</surname></persName>
							<email>timothy.jones@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,184.35,149.25,61.99,8.64"><forename type="first">Ahmed</forename><surname>Mourad</surname></persName>
							<email>ahmed.mourad@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.68,149.25,42.07,8.64"><forename type="first">Kevin</forename><surname>Ong</surname></persName>
							<email>kevin.ong@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.15,149.25,49.51,8.64"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
							<email>falk.scholer@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.22,149.25,48.88,8.64"><forename type="first">Evi</forename><surname>Yulianti</surname></persName>
							<email>evi.yulianti@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.86,79.96,238.11,12.54">RMIT at the TREC 2015 LiveQA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">90D5DF0FB89CE8ED56272AAE2FE45AC1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TREC LiveQA 2015</term>
					<term>RMIT</term>
					<term>passage retrieval</term>
					<term>summarization</term>
					<term>query trimming</term>
					<term>headword expansion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the four systems RMIT fielded for the TREC 2015 LiveQA task and the associated experiments. The challenge results show that the base run RMIT-0 has achieved an above-average performance, but other attempted improvements have all resulted in decreased retrieval effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. OVERVIEW</head><p>In the TREC LiveQA 2015 challenge, we experimented with four different retrieval-based answer-finding strategies. Instead of pursuing a traditional questionanswering approach that seeks deeper understanding of the questions, we focused on simple enhancements, such as summarization, query trimming, and headword expansion. These are common techniques used in IR, and can easily be integrated into research-purpose retrieval engines or pipelines of a similar scale. In our experiments, we considered the following research questions: RQ 1:. Will shorter or longer summaries result in better answers? RQ 2:. Should all of the terms in a question be used, or should only a subset of "important" terms be used? RQ 3:. Can headword expansion using external resources improve the quality of answers?</p><p>To answer these questions, we configured four different systems in the 2015 challenge. Surprisingly, we found that passage retrieval using the full query with minimal summarization and no query reduction or expansion produced the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATA AND RETRIEVAL SETTINGS</head><p>We now describe the collection and retrieval setting used in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Server architecture</head><p>The servers are built on top of the computing resources we allocated from NecTAR, <ref type="bibr" coords="1,177.52,716.16,3.49,6.05" target="#b0">1</ref> the Australian National Research cloud computing network. Throughout the challenge, we use only one instance to host all of the services.</p><p>1 https://www.nectar.org.au Question responses were dependent on the server ID when the questions were served (see Figure <ref type="figure" coords="1,498.95,249.12,6.28,8.64">II</ref>). In the most basic form, the server converted the questions into a bag-of-words query, and ran these against the indexes. The three most relevant passages retrieved are then submitted to our summarizer component, which outputs a predefined number of sentences ranked by relevance within the summarizer (see Section II-F).</p><p>In our final iteration, we wanted to see if a subset of "important" terms derived from a headword expansion would improve performance. In this iteration, we extracted key terms from the original question, which were then trimmed. Query expansion of the headwords was carried out using word2vec. The reduced query with word2vec headword expansion is passed to the query processor. The retrieved documents were then summarized by the summarizer, and the first sentence is then returned as the response.</p><p>The various services were connected using a resource allocator written in the Go Programming Language. It included graceful handling of timeouts, and guaranteed responses within the 60 second window. For the curious reader, our code is available under a BSD license. <ref type="bibr" coords="1,508.98,498.20,3.49,6.05" target="#b1">2</ref> Please cite this paper if you use the code for anything.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Run descriptions</head><p>RMIT-0<ref type="foot" coords="1,341.22,542.83,3.49,6.05" target="#foot_1">3</ref> (automatic): Indri bag-of-words passage retrieval using all of the terms in the question title, and top three passages summarized by the method described in Section II-F.</p><p>RMIT-1 (automatic): Indri bag-of-words passage retrieval using all of the terms in the question title, and the top three passages summarized by the method described in Section II-F. However, only the first sentence generated by the summary process was returned.</p><p>RMIT-2 (automatic): Indri bag-of-words passage retrieval using terms derived from the question title, term trimming (down to 5 terms), headword expansion (adding up to 5 terms) using word2vec as described in Section II-H, and the top three passages summarized by the method described in Section II-F. Only the first sentence generated by the summary process was returned. RMIT-3 (automatic): Indri bag-of-words passage retrieval using terms derived from the question title, term trimming (down to 5 terms), headword expansion (adding up to 5 terms) using wordnet as described in Section II-H, and top three passages summarized by the method described in Section II-F. Only the first sentence generated by the summary process was returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Collection</head><p>Table <ref type="table" coords="2,91.70,534.86,3.35,8.64" target="#tab_0">I</ref> summarizes the collections we used in the task. We indexed AQUAINT and AQUAINT-2 as TREC Text documents. To prepare the data set for English Wikipedia, we used the open-source tool wp-download, <ref type="bibr" coords="2,53.76,581.01,3.49,6.05" target="#b3">4</ref> to fetch a dump,<ref type="foot" coords="2,135.47,581.01,3.49,6.05" target="#foot_3">5</ref> extracted all of the XML content using wikiextractor, <ref type="bibr" coords="2,132.93,592.97,3.49,6.05" target="#b5">6</ref> and indexed every Wikipedia page as a document. To index the Yahoo! Answers CQA data, we processed the collection as follows: rather than just indexing the best answers, we extracted and indexed only the answers to previous questions and stored them as documents. We did not make use of the subject and content tags (i.e., question title and description) in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Indexing</head><p>We used Indri 5.9 as our retrieval engine with Krovetz stemming and the default InQuery stoplist. <ref type="bibr" coords="2,240.43,711.41,3.49,6.05" target="#b6">7</ref> Once the collection described in the previous section was indexed, we ended up with a single 39 GB index that contained 38.7 million documents and 12.2 million unique terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Passage Retrieval</head><p>According to our prior tests over the live stream, the question title contains 10 terms on average, and the body size is around 30 terms. However, as some of the question bodies can be quite long, we decided to construct queries using only the title. In two of our submitted runs, we tried different ways of expressing the same query intent by doing query reduction and expansion. The details of this approach are described in Section II-B.</p><p>We used the fixed-sized passage operator #combine[passage100:50](...) provided by Indri to retrieve and parse the top three passages from document texts. The result was then sent to the summarizer. For performance reasons, and the length of some of the queries, we used a bag-of-words query, and BM25 ranking. For BM25, our parameter configuration was k 1 = 0.9 and b = 0.4.<ref type="foot" coords="2,396.53,657.28,3.49,6.05" target="#foot_6">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Summarization</head><p>For summarization, we used the model proposed by Takamura and Okumura [7] to generate extractive summaries from the top-ranked passages. In this model, summarization is characterized as a two-way optimization problem, in which coverage over important words is maximized, and redundancies are minimized simultaneously.</p><p>The mathematical formulation is given as follows:</p><p>maximize</p><formula xml:id="formula_0" coords="3,118.76,251.48,112.93,19.91">(1 -λ) j w j z j + λ i j</formula><p>x i w j a ij subject to x i ∈ {0, 1}for all i; z j ∈ {0, 1}for all j;</p><formula xml:id="formula_1" coords="3,126.21,298.75,162.51,59.27">i c i x i ≤ K; i a ij x i ≥ z j for all j<label>(1)</label></formula><p>To produce an extractive summary, one basically makes a choice over the set of sentences and decides what to include. By doing so, one also makes an implicit choice over words. This choice is modeled in the optimization problem as two sets of variables x i and z j , the former indicating the binary decision on keeping sentence i, and the latter on keeping word j in the summary. In other words, for each sentence i, x i is set to 1 if sentence i is to be included in the summary, or 0 otherwise. Analogously for each term j, z j is set to 1 if term j is included.</p><p>In this problem, c i denotes the cost of selecting sentence s i (i.e. number of characters in s i ), and w j denotes the weight of word j. We used a TF-IDF weighting scheme in which the term frequency (tf ) is derived from the question title and body, and the inverse document-frequency (idf ) is learned from a background corpus. The term frequency collected from the question body is further penalized with a factor α &lt; 1 as the information given in the question body can be less precise than in the title.</p><formula xml:id="formula_2" coords="3,89.06,614.62,199.66,12.12">w j = tf title (j) + α tf body (j) * idf (j)<label>(2)</label></formula><p>The correspondence between the sentence i and the word j is coded in the indicator variable a ij , whose value is set to 1 if the word j appears in sentence i, and 0 otherwise. With the first constraint, we limit the size of the summary to K characters at most (K is set to 1,000 throughout). With the second constraint, the word coverage is related to the sentence coverage, thus completing the formulation.</p><p>Empirically, we fine-tuned the parameters λ and α based on prior test runs. In the challenge, we set λ = 0.1 and α = 0.43. We used the IBM CPLEX solver to compute the optimal allocation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Headword Detection</head><p>A headword is the key term in the question that helps to retrieve the most relevant documents. For example, consider the question: What are the sales goals daily and monthly at MAC Cosmetics?. Here, the headword is sales. The process of generating the hypernyms is divided into two stages. To extract the headword, we generate the syntactic parse tree of the question using the Stanford parser. Then, we apply the rule-based model initially defined by Collins [1] and later refined by Huang et al. [2] and Silva et al. [6].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Query Trimming and Headword Expansion</head><p>We also experimented with a feature called query trimming, which is to use only the "important" terms in the question title to retrieve answers. The questions from Yahoo! Answers are quite verbose, and intended to be human readable. However, verbose queries are known to perform poorly in many keyword-based IR systems. So, it seems sensible to use only a subset of terms from the question, especially if the terms selected are likely to contribute the most in the ranking function.</p><p>First, we used the WAND implementation from Petri et al. [3, 4] to extract the MaxScore U b for each term. <ref type="bibr" coords="3,518.58,468.75,3.49,6.05">9</ref> The MaxScore list is then loaded into memory when the server starts. At query time, we used the list to order terms by impact, and trim the initial query down to a predefined size. The size is set to five terms throughout the experiments where trimming is applied.</p><p>We also experimented with two ways of expanding the headword term in each query externally, by drawing information from resources such as word2vec and wordnet:</p><p>• word2vec: In this method, we took a pre-trained word embedding model distributed with word2vec<ref type="foot" coords="3,514.71,591.01,6.97,6.05" target="#foot_8">10</ref> and used gensim<ref type="foot" coords="3,377.88,602.97,6.97,6.05" target="#foot_9">11</ref> to populate a list of query terms that are most similar to a given input. • wordnet: In this method, we implemented the models proposed by Huang et al. [2] and Silva et al. [6].</p><p>We extract the hypernyms of the head word using WordNet, and map the Penn Treebank POS Tags to WordNet tags to decide which part of speech senses should be considered. Then, following the algorithm of Huang et al. [2] for head word-sense disambiguation, we calculate terms overlap between the definition of each sense and the definition of context words (each word in the question excluding the head word) with maximum depth of six. The optimal sense (the one which results in the maximum number of common words) is chosen to populate the list of synonyms. Note that in the latter method, we use the context of the question, excluding the headword, to resolve the semantic ambiguity of different senses and populate a list of synonyms. For example, consider again the question What are the sales goals daily and monthly at MAC Cosmetics?, the headword is sales and the hypernyms are gross sales, income, financial gain, and gain sum.</p><p>For either approach, we added the top five generated expansion terms back into the query without any modification. Note that when query trimming is applied, the query would first get trimmed down to 5 terms, and then expanded using the headword to at most 10 terms totally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head><p>The LiveQA challenge results are given in Table <ref type="table" coords="4,280.74,455.49,6.14,8.64" target="#tab_1">II</ref>, where our submitted runs and the average result across all runs are shown. Our base run RMIT-0 delivered the best performance in our experiment, achieving 0.663 in Avg Score. The base run outperforms the average across all runs submitted to the challenge. On the other hand, all refinements that we tested resulted in decreased performance, below the average over all submitted runs.</p><p>Limiting the output to only the first sentence in the summary (RMIT-1) appears to have a negative effect on precision at all relevance level. This is surprising, since adding more sentences increases the likelihood that nonrelevant information would appear in the summary.</p><p>Our result on query trimming and headword expansion also shows no improvement. Therefore, this combined strategy is not effective when top-sentence precision is of concern. Among the two expansion methods, word2vec (RMIT-2) appears to do more harm than wordnet (RMIT-3). We speculated that headword expansion using word2vec is not practically useful, as word2vec can be too aggressive sometimes, generating terms that are not synonyms to the headword and thus wildly biasing the original intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>We have explored four different system configurations for the TREC LIVEQA Track in 2015. While we are pleasantly surprised with the performance of our baseline system, we believe further improvements can still be realized using query reduction and headword expansion. We hope that further post-run analysis will provide insight into why the approaches were not successful in our current system configurations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,70.53,389.50,454.38,6.91;2,76.63,72.00,438.68,304.76"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. System architecture for each RMIT system. Green shading indicates components that are different when compared to RMIT-0.</figDesc><graphic coords="2,76.63,72.00,438.68,304.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,87.65,72.25,419.67,88.98"><head>Table I SUMMARY</head><label>I</label><figDesc>OF COLLECTIONS INDEXED TO ANSWER QUESTIONS.</figDesc><table coords="3,87.65,103.97,419.67,57.26"><row><cell>Collection</cell><cell cols="3">Number of Documents Number of Words Description</cell></row><row><cell>AQUAINT</cell><cell>1,034K</cell><cell>506M</cell><cell>Newswire, 1999 -2000</cell></row><row><cell>AQUAINT2</cell><cell>907K</cell><cell>410M</cell><cell>Newswire, Oct 2004 -Mar 2006</cell></row><row><cell>Wikipedia-EN</cell><cell>4,847K</cell><cell>1,775M</cell><cell>Online knowledge base</cell></row><row><cell>Yahoo! Answers CQA v1.0</cell><cell>31,972K</cell><cell>1,462M</cell><cell>Question answers converted to documents</cell></row><row><cell></cell><cell></cell><cell></cell><cell>from the Yahoo! Answers website.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,57.86,72.25,479.73,116.17"><head>Table II EFFECTIVENESS</head><label>II</label><figDesc>SUMMARY FOR ALL FOUR RMIT SYSTEMS WHEN COMPARED TO THE AVERAGE ACROSS ALL SYSTEMS PARTICIPATING IN THE 2015 LIVEQA TRACK.</figDesc><table coords="4,137.41,113.23,320.61,75.19"><row><cell>Run ID</cell><cell>Avg. Score (0-3)</cell><cell>@1+</cell><cell cols="2">Success @2+ @3+</cell><cell>@4+</cell><cell>@2+</cell><cell>Precision @3+</cell><cell>@4+</cell></row><row><cell>RMIT0</cell><cell>0.663</cell><cell>0.987</cell><cell>0.364</cell><cell>0.220</cell><cell>0.082</cell><cell>0.369</cell><cell>0.223</cell><cell>0.083</cell></row><row><cell>RMIT1</cell><cell>0.435</cell><cell cols="4">0.992 0.267 0.130 0.039</cell><cell cols="3">0.269 0.131 0.039</cell></row><row><cell>RMIT2</cell><cell>0.378</cell><cell>0.998</cell><cell cols="3">0.232 0.115 0.034</cell><cell cols="3">0.232 0.115 0.034</cell></row><row><cell>RMIT3</cell><cell>0.412</cell><cell cols="4">0.994 0.251 0.126 0.038</cell><cell cols="3">0.252 0.127 0.038</cell></row><row><cell>All Runs</cell><cell>0.465</cell><cell cols="4">0.925 0.262 0.146 0.060</cell><cell cols="3">0.284 0.159 0.065</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="1,318.18,754.63,163.96,6.91"><p>https://github.com/TimothyJones/trec-liveqa-server</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="1,318.18,764.49,223.20,6.91"><p>Originally referred to as Monash-System2 in the LiveQA challenge.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="2,65.46,734.90,129.94,6.91"><p>https://github.com/babilen/wp-download</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="2,65.46,744.77,177.64,6.91"><p>We used an enwiki dump produced on May 15, 2015.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="2,65.46,754.63,125.09,6.91"><p>https://github.com/attardi/wikiextractor</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="2,65.46,764.49,122.21,6.91"><p>http://www.lemurproject.org/indri.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="2,318.18,737.34,223.54,7.71;2,306.72,746.56,234.72,6.91;2,306.72,755.53,235.17,6.91;2,306.72,764.49,146.34,6.91"><p>The values for b and k 1 are different than the defaults reported by Robertson et al.<ref type="bibr" coords="2,363.27,746.56,8.48,6.91" target="#b4">[5]</ref>. These parameter choices were reported for Atire and Lucene in the 2015 IR-Reproducibility Challenge, see github.com/ lintool/IR-Reproducibility for further details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="3,318.18,744.77,202.84,6.91"><p>The code is available at https://www.github.com/jsc/WANDbl.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="3,321.17,754.63,116.24,6.91"><p>https://code.google.com/p/word2vec</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="3,321.17,764.49,106.04,6.91"><p>https://radimrehurek.com/gensim</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment.</head><p>This work was supported in part by the <rs type="funder">Australian Research Council's Discovery Projects Scheme</rs> (<rs type="grantNumber">DP140102655</rs>), and in part by <rs type="funder">NPRP</rs> grant# <rs type="grantNumber">NPRP 6-1377-1-257</rs> from the <rs type="funder">Qatar National Research Fund</rs> (a member of <rs type="funder">Qatar Foundation</rs>). <rs type="person">Shane Culpepper</rs> is the recipient of an <rs type="funder">Australian Research Council</rs> <rs type="grantName">DECRA Research Fellowship</rs> (<rs type="grantNumber">DE140100275</rs>). The statements made herein are solely the responsibility of the authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uV2eBXB">
					<idno type="grant-number">DP140102655</idno>
				</org>
				<org type="funding" xml:id="_2vbrtmd">
					<idno type="grant-number">NPRP 6-1377-1-257</idno>
				</org>
				<org type="funding" xml:id="_mFjZhab">
					<idno type="grant-number">DE140100275</idno>
					<orgName type="grant-name">DECRA Research Fellowship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,323.32,411.44,218.29,8.64;4,323.32,423.22,219.37,8.82;4,323.32,435.35,87.61,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,399.74,411.44,141.87,8.64;4,323.32,423.40,100.54,8.64">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,434.34,423.22,104.69,8.59">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,323.32,447.31,219.87,8.64;4,323.32,459.26,218.29,8.64;4,323.32,471.04,218.12,8.82;4,323.01,483.00,219.68,8.59;4,323.32,495.13,219.78,8.64;4,323.32,507.08,58.84,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,323.32,459.26,218.29,8.64;4,323.32,471.22,42.33,8.64">Question classification using head words and their hypernyms</title>
		<author>
			<persName coords=""><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcus</forename><surname>Thint</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,394.30,471.04,147.14,8.59;4,323.01,483.00,215.42,8.59">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="927" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,323.32,519.04,219.87,8.64;4,323.32,530.82,218.12,8.82;4,322.57,542.77,220.11,8.59;4,323.32,554.91,107.74,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,323.32,530.99,119.24,8.64">Exploring the magic of wand</title>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shane</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,464.29,530.82,77.16,8.59;4,322.57,542.77,215.28,8.59">Proceedings of the 18th Australasian Document Computing Symposium</title>
		<meeting>the 18th Australasian Document Computing Symposium</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,323.32,566.86,219.86,8.64;4,323.32,578.82,218.12,8.64;4,323.32,590.59,218.12,8.82;4,322.71,602.55,218.73,8.59;4,323.32,614.50,219.25,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,323.32,578.82,218.12,8.64;4,323.32,590.77,28.83,8.64">Score-safe term-dependency processing with hybrid indexes</title>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J Shane</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,380.07,590.59,161.37,8.59;4,322.71,602.55,218.73,8.59;4,323.32,614.50,94.43,8.59">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="899" to="902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,323.32,626.64,219.78,8.64;4,323.32,638.41,219.87,8.82;4,322.76,650.37,61.05,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,433.06,638.59,68.33,8.64">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,521.99,638.41,21.20,8.59;4,322.76,650.37,30.13,8.59">Proc. TREC-3</title>
		<meeting>TREC-3</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,323.32,662.50,218.13,8.64;4,322.96,674.46,218.48,8.64;4,323.32,686.41,166.00,8.64;4,505.66,686.23,35.78,8.59;4,323.15,698.19,172.48,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,405.58,674.46,135.86,8.64;4,323.32,686.41,162.33,8.64">From symbolic to sub-symbolic information in question classification</title>
		<author>
			<persName coords=""><forename type="first">Joao</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Wichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,505.66,686.23,35.78,8.59;4,323.15,698.19,76.96,8.59">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,323.32,710.32,184.24,8.64;4,523.84,710.32,17.60,8.64;4,323.32,722.28,218.12,8.64;4,323.32,734.05,218.12,8.82;4,323.07,746.19,63.26,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,523.84,710.32,17.60,8.64;4,323.32,722.28,218.12,8.64;4,323.32,734.23,98.52,8.64">Text summarization model based on maximum coverage problem and its variant</title>
		<author>
			<persName coords=""><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,447.74,734.05,60.59,8.59">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
