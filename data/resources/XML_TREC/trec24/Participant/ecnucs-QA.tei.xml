<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,78.84,28.75,454.31,17.13;1,132.42,46.69,347.14,17.13">Leverage Web-based Answer Retrieval and Hierarchical Answer Selection to Improve the Performance of Live Question Answering</title>
				<funder ref="#_BfXPaEH">
					<orgName type="full">Shanghai Collaborative Innovation Center of Trustworthy Software for Internet of Things</orgName>
				</funder>
				<funder ref="#_g5DrcTf #_jHxPgTH">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,243.14,81.90,63.51,14.27"><forename type="first">Guoshun</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Multidimensional Information Processing Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,316.63,81.90,48.91,14.27"><forename type="first">Man</forename><surname>Lan</surname></persName>
							<email>mlan@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Multidimensional Information Processing Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,78.84,28.75,454.31,17.13;1,132.42,46.69,347.14,17.13">Leverage Web-based Answer Retrieval and Hierarchical Answer Selection to Improve the Performance of Live Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7832A4F50061DE509F8D3652D406833E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the system we submitted to the Live QA Track in TREC 2015 which focused on live question answering for real-user questions. The real user questions extracted from Yahoo! Answers will be sent to the participant systems and the participants are required to provide an answer in real time (less than one minute). To address this task, we constructed a pipeline system which includes four components, i.e., question expansion, answer document retrieval, candidate answers selection and answer re-ranking. The purpose of the last three components aims at identifying accurate answers in a hierarchical manner. Then the answers with less than 1, 000 characters are returned to the server in less than one minute. Our system outperforms the averaged scores of all submitted runs on a 4-level Likert scale and ranks the 2th out of 14 teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open question answering (QA) has been a widely studied research problem in the past few years. The state-of-the-art QA systems are usually implemented in a pipeline architecture, consisting of several key components including question classification, question expansion, answer retrieval, answer reranking, etc. Most previous studies have focused on factoid questions (e.g., Who, When, Where). Recently, with the development of community-based Web sites such as Yahoo! Answer 1 (YA) and WikiAnswers 2 , more researchers have concentrated on the domain of Community-based Question Answering 1 https://answers.yahoo.com/ 2 http://wiki.answers.com/ (CQA). The content of CQA is usually generated by real users and there are more non-factoid questions in CQA than in QA, which make it quite difficult to get the right answer directly. One way to address this problem is to find the similar questions from a large CQA archives for each given question and then regard the answer for the most similar question as the answer for the original question.</p><p>The Live QA Track in TREC 2015 is a CQAbased task, focusing on live question answering for real user questions. These real user questions extracted from the stream of most recent questions submitted on the YA site that have not yet been answered by humans, will be sent to the participant systems. Then the participant systems are required to provide an answer with the support sources where it is extracted and synthesized in real time (less than one minute). The length of returned answer is limited to 1, 000 characters and will be manually judged by TREC editors on a 4-level Likert scale. The questions are restricted to the following eight YA categories<ref type="foot" coords="1,344.54,470.55,3.99,8.78" target="#foot_0">3</ref> , i.e., <ref type="bibr" coords="1,375.92,472.09,164.07,12.02;1,313.20,485.64,226.80,12.02;1,313.20,499.19,136.29,12.02">Arts &amp; Humanities, Beauty &amp; Style, Computers &amp; Internet, Health, Home &amp; Garden, Pets, Sports, and Travel. This</ref> Live QA Track has several challenges. Firstly, the questions are generated by real users, resulting in various qualities of questions. Secondly, the question body usually contains long texts, which requires to a deep linguistic analysis to extract the real purpose of each question. Finally, a time-consuming complicated system is not suitable due to the limitation of runtime.</p><p>Inspired by previous work on QA <ref type="bibr" coords="1,479.59,609.04,60.41,12.02" target="#b8">(Wang, 2006;</ref><ref type="bibr" coords="1,313.20,622.58,134.04,12.02" target="#b3">Kolomiyets and Moens, 2011)</ref> and CQA <ref type="bibr" coords="1,495.77,622.58,44.23,12.02;1,313.20,636.13,92.51,12.02" target="#b0">(Bernhard and Gurevych, 2009;</ref><ref type="bibr" coords="1,409.10,636.13,95.50,12.02" target="#b7">Surdeanu et al., 2011)</ref> work we propose a pipeline system, consisting of four key components, i.e., question expansion (QE) (e.g., expanding questions with relevant information), answer document retrieval (ADR) (e.g., retrieving candidate documents which contain answers to the given question), answer sentence selection (ASS) (e.g., selecting answer sentences from the retrieved documents) and answer re-ranking (ARR) (e.g., extracting deep semantic features and training a supervised ranking model to rank answers). Obviously, the last three components aim at identifying accurate answers in a hierarchical manner. That is, we first retrieve answer documents from external Web resources, and then select out high relevant answer sentences from documents in a shallow analysis and finally re-rank answers by using supervised model based on deep semantic analysis.</p><p>The reminder of this paper is organized as follows. Section 2 overviews the whole architecture of our system. Section 3 presents the experimental results and discussion. Finally, Section 4 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>Figure <ref type="figure" coords="2,344.16,387.40,5.45,12.02">1</ref> shows the architecture of our proposed system, which is composed of four key components, i.e., question expansion (QE), answer document retrieval (ADR), answer sentence selection (ASS), and answer re-ranking (ARR).</p><p>Given each input question, QE is to expand the content of question in three distinct ways, i.e., adding synonyms, or hypernyms of question words or the snippets returned by Search Engine (SE) into question. The expanded question is treated as a search query and submitted into a Web-based site in the ADR component. Then a set of documents which may contain the answers are returned from different external Web resources. To select out the high relevant sentences from large amount of retrieved answer documents, the ASS component is to rank these sentences using a shallow semantic analysis (the product of source weighting score and majority voting score). Finally, in ARR component, the selected top answer sentences are re-ranked by a supervised model trained on a subset of Yahoo! Web-Scope Dataset L6<ref type="foot" coords="3,148.59,22.57,3.99,8.78" target="#foot_1">4</ref> based on deep semantic analysis. We simply concatenated the top answers with a total length less than 1, 000 characters as the best answer and sent it back to the server in the final test stage.</p><p>Text preprocessing has been performed in all components. The Stanford CoreNLP <ref type="bibr" coords="3,244.57,91.85,54.23,12.02;3,72.00,105.40,41.83,12.02" target="#b4">(Manning et al., 2014)</ref> is used for sentence tokenization and part of speech (POS) tagging. The Natural Language Toolkit (NLTK)<ref type="foot" coords="3,139.81,130.96,3.99,8.78" target="#foot_2">5</ref> is utilized for WordNet (WN) based stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Expansion</head><p>Usually natural language questions can be expressed in various ways. In order to enrich the information of question, we present three methods to expand question. The first is to add the synonyms of head words in the question with the aid of WN, where the head words are all nouns and verbs present in the question. For convenience, we only used the synset of the first main sense of each noun and verb. Furthermore, in order to include the semantic abstractions of nouns in the question, the second method is to add the hypernyms of all nouns in the question. Different from the two methods which adopt the WN, the third method expands the question with the aid of Web page snippets returned by external Search Engines (SE). This is based on the observation that the returned snippets from SE are usually closely relevant to the search query. However, in order to alleviate the influence of noise introduced from snippets, we only chose the top N nouns from snippets ranked by Jaccard coefficient. Finally, we regarded the expanded question as a new query and used this new query to search relevant documents in the next component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer Document Retrieval</head><p>Generally, if there is a large QA archive where each question is attached a best answer, we may retrieve the best answer from archive by finding the most similar question with the original question. However, no such large QA archive or corpus is provided by organizers. Neither is it possible for us to collect a large scale of documents and to build our own local corpus, which is labor intensive and time consuming. One possible solution is to make use of ex-isting external resources such as search engines or QA web sites to retrieve answer documents.</p><p>In consideration of open domain of questions, we resort to external Web resources, i.e., online QA Web sites and Search Engines, to retrieve answer documents. In this work, we used two QA Web sites, i.e., Yahoo! Answer and Ask.Web and one search engine, i.e. Bing, as external resources. The YA Web site returns the list of most similar questions with the given question and does not directly provide correct answer. So we choose the top K (K = 9) similar questions and take the best answers as candidate answer documents. Note if there is no best answer provided to the related question, we select out the answer which is thumbed up by the most users as the best answer. Unlike YA only providing question list, the second QA site, i.e., Ask.web<ref type="foot" coords="3,532.79,240.13,3.99,8.78" target="#foot_3">6</ref> , is a question answering-focused Web site, which returns the answer snippets to the query question. The third source is general SE. The expanded questions are submitted into SE as a search query and the top N snippets returned by SE are regarded as candidate answer documents. In this work, we choose Bing to perform Web search. To keep a balance between the coverage and relevance of retrieved results, we choose the top N (N = 10) snippets as candidate documents both for Ask.web and Bing. Finally, these 29 (K + 2N ) candidate answer documents retrieved from three sources are combined together and sent to the next component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answers Sentence Selection</head><p>Typically, these K+2N candidate answer documents extracted from above ADR component still contain a number of non-relevant sentences. Therefore, the ASS component is to further prune out any non-relevant sentences from candidate answer documents in a fast and simple way. To do this, we implement a two-step method, i.e., candidate answers generation, and candidate answers ranking.</p><p>For candidate answer documents retrieved from different sources, we perform different processes to generate candidate answers. Specifically, the documents returned from YA usually contain long texts, so we first split the documents into sentences by using NLTK toolkit and then choose the maximal continuous n sentences as candidate answers, where the length of continuous n sentences are less than 250 characters. As for the documents returned from Ask.web and Bing, we used these documents as candidate answers directly due to their short text length.</p><p>In the second step, we rank the candidate answers using the product of a source weighting score and a majority voting score. The two scores are proposed based on our observations. The source weighting score of candidate answer is to consider the importance of each source where it comes from. For example, the candidate answers from YA have higher quality than those from Ask.web and Bing because the candidate answers returned from YA are the best answers of related questions while the candidate answers from Ask.web and Bing are only snippets of related web pages. Besides, since Ask.web is a QAfocused Web site and Bing is a general SE, the candidate answers from Ask.web are supposed to have higher quality than those from Bing. Therefore, we assign different source weights to the candidate answers from YA, Ask.web, and Bing as of 1/2, 1/3 and 1/6 respectively. The majority voting score of candidate answer is the average of word overlaps (wo) between current candidate answer and candidate answers from different documents. To calculate the word overlaps of candidate answer A1 and candidate answer A2, a series of preprocessing procedures are performed, for example, stop words, repeated words and punctations are removed, all words are converted to their lowercase and stemming are adopted. After that, the word overlap of A1 and A2 is measured as |A1 ∩ A2|/ max (|A1|, |A2|), where |A1| and |A2| denote the number of words of A1 and A2 respectively. Finally, we select the top 10 candidate answers with the ranking scores and sent them to the next component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Re-ranking</head><p>To further identify the accurate answer, the AR-R component is to re-rank the answers by evaluating how well they answer the given question. Unlike the above ASS component which only adopts a shallow analysis on answers, in this component we build a supervised machine learning model to rerank the answers by deeply analyzing the relatedness between answers and given questions. Four type of features are employed, i.e., the word overlap be-tween question and answer (Word Match Features), the probability of question-to-answer transformations using a translation model <ref type="bibr" coords="4,456.37,51.21,83.63,12.02;4,313.20,64.76,38.99,12.02">(Translation Based Features)</ref>, the informativeness of answer (Answer Informativeness Features), lexical semantic similarity of question and answer (Lexical Semantic Similarity Features). We remove the stop words and repeated words from question and answer before extracting these features. The details of these four types of features are described as follows.</p><p>Word Match Feature (WM): This feature records the proportions of co-occurred words between a given QA pair, which is calculated using five measures:</p><formula xml:id="formula_0" coords="4,313.20,201.80,226.80,31.53">|Q ∩ A|, |Q ∪ A|/|Q|, |Q ∩ A|/|A|, |A - Q|/|A|, |Q -A|/|Q|</formula><p>, where |Q| and |A| denote the number of the words of question Q and answer A.</p><p>Translation Based Feature (TB): The above WM feature only considers the overlapped surface words between Q and A and thus it may fail to "bridge the lexical gap" between question and answer. One possible solution is to regard this task as a statistic machine translation problem between question and answer by using the IBM Model 1 <ref type="bibr" coords="4,506.99,322.39,33.02,12.02;4,313.20,335.94,52.05,12.02" target="#b1">(Brown et al., 1993)</ref> to learn the word-to-word probabilities. Following <ref type="bibr" coords="4,360.58,349.49,76.81,12.02" target="#b9">(Xue et al., 2008;</ref><ref type="bibr" coords="4,440.46,349.49,94.84,12.02" target="#b7">Surdeanu et al., 2011)</ref>, we regarded P (Q|A), i.e., the translation probability of question Q when given answer A, as a translation based feature. The probabilities are calculated as: P (Q|A) = w∈Q P (w|A) P (w|A) = (1λ)P tr (w|A) + λP ml (w|C) P ml (w|A) = a∈A P (w|a)P ml <ref type="bibr" coords="4,478.76,476.85,25.45,9.99">(a|A)</ref> where P (w|A) is the probability that the question word w is generated from answer A, λ is smoothing parameter, C is a background collection. P ml (w|C) is computed by maximum likelihood estimator. P (w|a) denotes the translation probability from answer word a ∈ A to question word w. We used GIZA++ Toolkit<ref type="foot" coords="4,408.26,585.78,3.99,8.78" target="#foot_4">7</ref> to compute the probability.</p><p>Answer Informativeness Feature (AI): The AI feature measures the informativeness of a answer text. Intuitively, we assume that the answer which carries more information is more likely to be chosen</p><p>as the best answer by users. We adopt the following answer information measures:</p><p>Answer Length: Number of answer words.</p><p>Answer Head Words: Number of nouns,verbs and adjectives in the answer.</p><p>Lexical Semantic Similarity Feature (LSS): Inspired by <ref type="bibr" coords="5,116.88,106.66,75.28,12.02" target="#b10">(Yih et al., 2013)</ref>, we include the lexical semantic similarity features in our re-ranking model. We use the 300-dimensional version of word2vec <ref type="bibr" coords="5,72.00,147.31,105.23,12.02" target="#b5">(Mikolov et al., 2013)</ref> vectors, which is trained on part of Google News dataset (about 100 billion words). There are two ways to calculate the LSS features. One cosine similarity is calculated by summing up all word vectors in question and answer. Another is to adopt averaged pairwise cosine similarity between each word in question and answer.</p><p>To train a supervised ranking model for answer reraning, several algorithms have been experimented, for example, ranking Perceptron <ref type="bibr" coords="5,223.38,269.67,75.41,12.02;5,72.00,283.22,23.48,12.02" target="#b6">(Shen and Joshi, 2005)</ref>, SVM-rank <ref type="bibr" coords="5,156.37,283.22,75.44,12.02" target="#b2">(Joachims, 2006)</ref>, etc. In consideration of efficiency and timeliness of supervised ranking algorithm, we adopt SVM-rank in our system, which is an instance of structural SVM as a family of Support Vector Machine algorithms that model structured outputs-specifically tailored for ranking problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment 3.1 Datasets</head><p>There are two datasets used in our system training experiments, i.e., Document Retrieval (DR) dataset and Answer Re-ranking (AR) dataset. The former is provided by the organizers and we use it to develop the question expansion and answer document retrieval. The latter dataset is collected by ourselves from a sample of Yahoo! WebScope Dataset L6, which is used to build a supervised answer reranking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Retrieval Dataset (DR):</head><p>The DR dataset is provided by the organizers including 1, 000 questions with given answers and we only choose the questions which have the best answer in YA site. Finally, we get 857 question-answer pairs used for system configuration for question expansion and answer document retrieval.</p><p>Answer Re-ranking Dataset (AR): The AR dataset is constructed by ourselves, which is extract-ed from a sample of Yahoo! WebScope Dataset L6 as a large collection of 4, 483, 032 question-answer pairs. To construct our dataset, we implemented the following filtering steps.</p><p>Step 1: We only selected questions-answers pairs from eight categories mentioned in Section 1.</p><p>Step 2: To reduce the noise of questions and answers, we heuristically kept the questions and answers with at least five words.</p><p>Step 3: In order to meet the answer length requirement, we removed the answers which contain more than 250 characters.</p><p>Finally, we obtained 150, 000 question-answer pairs as our final AR dataset and 60% are used for training, 20% for development, and 20% for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>To evaluate the performance of our system, several metrics are adopted.</p><p>The measure of performance of QE is embeded in ADR component. To evaluate ADR performance, we adopt the percentage of answered questions which have word overlap values larger than a certain value (e.g., 0.3, 0.4 and 0.5), where word overlap is calculated between retrieved answers from Webbased resources and the given best answers for each query question, as formulated in section 2.3. We treat ARR as a answer ranking problem and adopt Mean Reciprocal Rank (MRR) and Re-ranking Precision@1 to evaluate it. The MRR is defined as the average of the reciprocal ranks of the correct answer. The Re-ranking Precision@1 is defined as the average Precision@1 over the questions set, where the Precision@1 of a query is defined as 1 if the correct answer is re-ranked into the first position, 0 otherwise. For the final test of Live QA Track, the results are judged by TREC editors firstly using 4-level scale as follows:</p><p>4:Excellent a significant amount of useful information and fully answers the question.</p><p>3:Good partially answers the question.</p><p>2:Fair marginally useful information. 1:Bad no useful information for the question.</p><p>Then several performance measures are built on above 4-level scale as follows:</p><p>avg-score(0-3) averaged score over all queries (transferring above 1-4 level scores to 0-3, comparing 1-level score with no-answer score).</p><p>succ@i+ number of questions with i+ score (i=1..4) divided by number of all questions.</p><p>prec@i+ number of questions with i+ score (i=2..4) divided by number of answered only questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Preliminary Experiments</head><p>In order to determine the optimum system configuration, we constructed preliminary experiments on the datasets of DR and AR. We did not construct experiments on ASS component individually because of the simpleness of the component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Answer Document Retrieval</head><p>To compare the impacts of question expansion methods and the qualities of sources where answer documents come from, we constructed this experiment on DR dataset. Table <ref type="table" coords="6,195.65,512.11,5.45,12.02" target="#tab_1">1</ref> shows the percentage of answered questions from different sources with different word overlap levels (e.g., 0.3, 0.4 and 0.5) in ADR component, where Raw represents only using the original question words as query, Synonyms, Hypernyms and Bing Snippets represent the methods of using synonyms, hypernyms and bing snippets to expand question. We also perform to search answer documents by combining three sources without question expansion (i.e., Raw (YA+Ask+Bing)).</p><p>From the results, we make the following three observations:  (1) The answers from YA have higher quality than those from Ask.web and Bing on all word overlap levels.</p><p>(2) Although question expansion by synonyms, hypernyms and Bing snippets has been shown a promotion on information retrieval <ref type="bibr" coords="6,454.93,502.25,79.61,12.02;6,313.20,515.80,53.94,12.02">(Carpineto and Romano, 2012)</ref>, it reduces the performance in our experiments. The word ambiguity of synonyms and hypernyms in questions and the noises introduced by snippets may cause this phenomenon.</p><p>(3) The combination of three sources increase the performance, but brings more unrelated answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Answer Re-ranking</head><p>In this experiment, we adopted SVM-rank with c = 300, kernel = linear and the smoothing parameter λ = 0.3 for translation feature, where these parameters are tuned by the experiment on develop-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>#Answered questions avg score (0-3) succ@2+ succ@3+ succ@4+  <ref type="table" coords="7,194.55,192.67,4.09,12.02" target="#tab_2">2</ref>, we clearly found several observations. Firstly, both AI and LSS features significantly increase both P@1 and MRR performance over two baselines. Since AI is a measure of the informativeness of answer text, this indicates that users trend to choose the answer with more information. Unlike the surface word match features which only consider the surface form, the LSS features integrate the context information and therefore the word embeddings complement the surface word match information. Secondly, the TB features do not make as much contribution on performance improvement as we expected. The possible reason may be that the question-answer alignment data contains much noise and reduces the quality of word to word translation probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Time Cost</head><p>Since there is a time limit required to return answer, i.e., less than one minute for each question, we also compared the runtime of four components under our experimental environment (e.g., the computer with Intel i7 CPU, 8GB Memory and on the Science and Technology Network of our university). Table <ref type="table" coords="7,99.15,525.66,5.45,12.02" target="#tab_3">3</ref> shows the runtime (second) per question of four components on DR dataset. From the table, it is interesting to find following observations. Firstly, it takes about average 33.39 seconds per question to return the answer in our system, which is acceptable. Secondly, because of no need of any online source, QE (Synonyms, Hypernyms) component and ASS component take a short time to run. Inversely, for ADR component, it takes the max proportion of time to search related documents from Web sources. Finally, the ARR component does not take much time to rank candidate answers and this owes to the practical efficiencies of pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">System Configuration</head><p>Based on above experimental analysis, we built system configuration as the following list:</p><p>(1) Except for the QE component, we used all the other three components to build our whole system.</p><p>(2) For the ADR component, we used all three Web sources because their combination achieved the best.</p><p>(3) For ARR component, we adopted a SVM-rank algorithm (kernel = linear, c = 300) and used WM, TB, AI and LSS features to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results on the test set of Live QA Track</head><p>In the final test stage, a test dataset with 1, 087 questions is provided. In above system configuration, we perform the live QA procedure on given test dataset.</p><p>Table <ref type="table" coords="7,351.18,417.26,5.45,12.02" target="#tab_4">4</ref> shows the results of top 3 teams (our run is ecnucs) and the averaged results (avg) officially released by Live QA Track organizers in TREC 2015. From Table <ref type="table" coords="7,399.93,457.91,4.09,12.02" target="#tab_4">4</ref>, we find the following observation. Firstly, our pipeline system outperforms the averaged scores of all submitted runs with respect to above four evaluation metrics. Secondly, in our run the number of answered questions is much less than the averaged value and other teams. Although our system answered the given questions in time, 93 answers from our run has not been received by the organizer due to the out of time of network. Thirdly, the leading run, CMUOAQA from Carnegie Mellon University, performed very well compared to our run, according to all measures. However, the score of all results are still by far less than the maximum possible avgScore of 3.0. This is caused by the complexity of the task and it is also a challenge for human beings.</p><p>In this paper, we proposed a pipeline system to address Live QA Track of TREC 2015, i.e., QE, ADR, ASS and ARR. Although we presented three methods including expansion of synonyms, hyperonyms and Bing snippets to expand the question, they did not perform as well in the experiment as in information retrieval task. Thus our final system consists of the last three components. In ADR component, we retrieved answer documents from three different sources (i.e., YA, Ask.web, and Bing). Then in AS-S component we selected candidate answers from the documents. After that we constructed a answer re-ranker which extracts four types of features (i.e., WM, TB, AI, and LSS) and adopts SVM-rank algorithm to train a supervised model. Finally, the top answers returned by ARR component are concatenated as the best answer on the condition that the total length of answers is less than 1, 000 characters.</p><p>For feature work, we focus on the using of question context information to improve the performance of ADR component and ARR component. Also we will explore more features to improve re-ranking performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,504.60,636.13,35.39,12.02"><head>Top 10 snippets Top 10 snippets Best Answers from top 9 related questions</head><label></label><figDesc>, in this</figDesc><table coords="2,97.77,126.34,433.89,163.97"><row><cell></cell><cell>Synonyms Synonyms</cell><cell></cell><cell>Yahoo Answers! Yahoo Answers!</cell><cell>Candidate Candidate</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Answers Answers</cell><cell></cell><cell></cell><cell>Re-ranking Re-ranking</cell></row><row><cell>Question</cell><cell>Hypernyms Hypernyms</cell><cell>Expanded question</cell><cell>General Search General Search</cell><cell>Generation Generation Candidate Answers</cell><cell></cell><cell></cell><cell>Model Model</cell><cell>Best Answer</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Engines Engines</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Snippets Snippets</cell><cell></cell><cell>QA-focused Search QA-focused Search</cell><cell>Candidate Answers Ranking Candidate Answers g Ranking a g</cell><cell cols="2">Top 10 ranked Answers</cell><cell>Answer Re-ranking Answer Re-ranking</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Engines Engines</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Question Expansion</cell><cell></cell><cell>Answer Document Retrieval</cell><cell cols="2">Answer Sentence Selection</cell><cell cols="2">Answer Re-ranking</cell></row></table><note coords="2,461.76,196.95,15.31,5.89;2,204.71,327.36,202.59,10.98"><p>Top N Figure 1: The architecture of our proposed system.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,22.82,468.01,117.48"><head>Table 1 :</head><label>1</label><figDesc>The percentage of answered questions from different sources with different word overlap levels of ADR component on DR dataset, where Raw,Synonyms,Hypernyms and Bing Snippets are different question expansion methods.</figDesc><table coords="6,128.95,22.82,354.10,78.82"><row><cell>Sources</cell><cell></cell><cell>YA</cell><cell></cell><cell></cell><cell>Ask.web</cell><cell></cell><cell></cell><cell>Bing</cell><cell></cell></row><row><cell>Word Overlap</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>Raw</cell><cell>61.2</cell><cell>41.6</cell><cell cols="7">28.06 21.58 11.55 8.01 17.57 9.32 6.96</cell></row><row><cell>Synonyms</cell><cell cols="4">57.54 39.03 26.41 11.44</cell><cell>6.72</cell><cell cols="2">5.19 11.79</cell><cell>6.6</cell><cell>4.48</cell></row><row><cell>Hypernyms</cell><cell>57.42</cell><cell>38.8</cell><cell cols="2">26.06 18.16</cell><cell>9.67</cell><cell cols="4">7.08 12.45 7.07 4.83</cell></row><row><cell>Bing Snippets</cell><cell cols="3">50.47 34.78 23.82</cell><cell>15.8</cell><cell>8.25</cell><cell>5.78</cell><cell>9.08</cell><cell cols="2">4.72 2.83</cell></row><row><cell cols="4">Raw (YA+Ask+Bing) 68.04 45.99 31.13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,318.08,160.29,217.04,216.30"><head>Table 2 :</head><label>2</label><figDesc>Results of ARR component on test set of AR.</figDesc><table coords="6,341.66,160.29,169.88,216.30"><row><cell cols="2">Feature sets</cell><cell cols="2">MRR(%) P@1(%)</cell></row><row><cell cols="2">Random Baseline</cell><cell>36.85</cell><cell>15.08</cell></row><row><cell cols="2">WM Baseline</cell><cell>42.40</cell><cell>20.38</cell></row><row><cell cols="2">WM + TB</cell><cell>42.67</cell><cell>20.48</cell></row><row><cell cols="2">WM + TB + AI</cell><cell>44.41</cell><cell>23.02</cell></row><row><cell cols="2">WM + TB + AI + LSS</cell><cell>45.17</cell><cell>23.57</cell></row><row><cell cols="2">Components</cell><cell cols="2">Runtime</cell></row><row><cell></cell><cell cols="2">Synonyms</cell><cell>0.06</cell></row><row><cell>QE</cell><cell cols="2">Hypernyms</cell><cell>0.05</cell></row><row><cell></cell><cell cols="2">Bing Snippets</cell><cell>3.22</cell></row><row><cell></cell><cell>YA</cell><cell></cell><cell>13.45</cell></row><row><cell>ADR</cell><cell cols="2">Ask.web</cell><cell>6.73</cell></row><row><cell></cell><cell>Bing</cell><cell></cell><cell>4.24</cell></row><row><cell></cell><cell>ASS</cell><cell></cell><cell>1.83</cell></row><row><cell></cell><cell>ARR</cell><cell></cell><cell>3.69</cell></row><row><cell></cell><cell>Total</cell><cell></cell><cell>33.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,313.20,392.31,226.81,22.94"><head>Table 3 :</head><label>3</label><figDesc>The runtime(s) per question of four components on DR dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.00,34.18,409.09,170.52"><head>Table 4 :</head><label>4</label><figDesc>Results on TREC2015 Live QA test set.</figDesc><table coords="7,72.00,34.18,409.09,170.52"><row><cell>1</cell><cell>CMUOAQA</cell><cell>1064</cell><cell>1.081</cell><cell>0.532</cell><cell>0.359</cell><cell>0.190</cell></row><row><cell>2</cell><cell>ecnucs</cell><cell>994</cell><cell>0.677</cell><cell>0.367</cell><cell>0.224</cell><cell>0.086</cell></row><row><cell>3</cell><cell>NUDTMDP1</cell><cell>1041</cell><cell>0.670</cell><cell>0.353</cell><cell>0.210</cell><cell>0.107</cell></row><row><cell></cell><cell>avg</cell><cell>1007</cell><cell>0.467</cell><cell>0.262</cell><cell>0.146</cell><cell>0.060</cell></row><row><cell cols="3">ment set of AR dataset. Table 2 depicts the perfor-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">mance of two baselines and our proposed re-ranking</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">model on test set of AR. The first baseline (Ran-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">dom) sorts the answers by random scores. The sec-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ond baseline (WM) trains the model using just word</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">match features. From Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,329.34,662.75,130.37,9.88"><p>https://answers.yahoo.com/dir/index</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,88.14,651.58,216.73,9.88"><p>http://webscope.sandbox.yahoo.com/catalog.php?datatype=l</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,88.14,662.75,72.98,9.88"><p>http://www.nltk.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="3,329.34,662.75,74.63,9.88"><p>http://www.ask.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="4,329.34,662.75,173.50,9.88"><p>http://www.statmt.org/moses/giza/GIZA++.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported by grants from <rs type="funder">Science and Technology Commission of Shanghai Municipality</rs> under research grant no. (<rs type="grantNumber">14DZ2260800</rs> and <rs type="grantNumber">15ZR1410700</rs>) and <rs type="funder">Shanghai Collaborative Innovation Center of Trustworthy Software for Internet of Things</rs> (<rs type="grantNumber">ZF1213</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_g5DrcTf">
					<idno type="grant-number">14DZ2260800</idno>
				</org>
				<org type="funding" xml:id="_jHxPgTH">
					<idno type="grant-number">15ZR1410700</idno>
				</org>
				<org type="funding" xml:id="_BfXPaEH">
					<idno type="grant-number">ZF1213</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,527.35,226.81,10.98;8,82.91,539.30,215.90,10.98;8,82.91,551.26,215.89,10.98;8,82.91,563.21,215.90,10.87;8,82.91,575.17,215.91,10.87;8,82.91,587.12,215.90,10.87;8,82.91,599.08,215.89,10.98;8,82.91,611.03,172.13,10.98" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,263.38,527.35,35.43,10.98;8,82.91,539.30,215.90,10.98;8,82.91,551.26,178.67,10.98">Combining lexical semantic resources with question &amp; answer archives for translation-based answer finding</title>
		<author>
			<persName coords=""><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,280.99,551.26,17.81,10.87;8,82.91,563.21,215.90,10.87;8,82.91,575.17,215.91,10.87;8,82.91,587.12,215.90,10.87;8,82.91,599.08,27.44,10.87">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08">2009. August</date>
			<biblScope unit="page" from="728" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,626.06,226.81,10.98;8,82.91,638.02,215.90,10.98;8,82.91,649.97,215.90,10.98;8,82.91,661.93,183.20,10.98;8,313.20,24.89,226.82,10.98;8,324.11,36.84,215.90,10.98;8,324.11,48.80,213.38,10.98" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,230.33,638.02,68.48,10.98;8,82.91,649.97,215.90,10.98;8,82.91,661.93,14.39,10.98;8,313.20,24.89,164.39,10.98;8,514.67,24.89,25.35,10.98;8,324.11,36.84,215.90,10.98;8,324.11,48.80,24.02,10.98">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,104.49,661.93,70.41,10.87;8,355.14,48.80,78.51,10.87">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="1993-06">1993. June. 2012. January</date>
		</imprint>
	</monogr>
	<note>ACM Comput. Surv.</note>
</biblStruct>

<biblStruct coords="8,313.20,61.50,226.81,10.98;8,324.11,73.46,215.91,10.98;8,324.11,85.41,215.89,10.98;8,324.11,97.37,102.00,10.98" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,439.75,61.50,100.26,10.98;8,324.11,73.46,44.03,10.98">Training linear svms in linear time</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,522.84,85.41,17.16,10.87;8,324.11,97.37,4.84,10.87">KD-D</title>
		<editor>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dimitrios</forename><surname>Gunopulos</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,110.07,226.81,10.98;8,324.11,122.02,215.90,10.98;8,324.11,133.98,158.74,10.98;8,502.06,133.98,37.94,10.87;8,324.11,145.94,127.01,10.98" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,324.11,122.02,215.90,10.98;8,324.11,133.98,154.78,10.98">A survey on question answering technology from an information retrieval perspective</title>
		<author>
			<persName coords=""><forename type="first">Oleksandr</forename><surname>Kolomiyets</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,502.06,133.98,34.51,10.87">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="5412" to="5434" />
			<date type="published" when="2011-12">2011. December</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,158.64,226.82,10.98;8,324.11,170.59,215.91,10.98;8,324.11,182.55,215.90,10.98;8,324.11,194.50,215.89,10.98;8,324.11,206.46,215.90,10.87;8,324.11,218.41,151.21,10.98" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,353.71,182.55,186.30,10.98;8,324.11,194.50,57.00,10.98">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,400.05,194.50,139.95,10.87;8,324.11,206.46,215.90,10.87;8,324.11,218.41,91.75,10.87">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,231.12,226.81,10.98;8,324.11,243.07,215.90,10.98;8,324.11,255.03,215.90,10.98;8,324.11,266.98,215.90,10.98;8,324.11,278.94,215.89,10.98;8,324.11,290.89,215.89,10.98" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,431.97,243.07,108.04,10.98;8,324.11,255.03,196.22,10.98">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,446.70,278.94,93.30,10.87;8,324.11,290.89,122.06,10.87">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,315.55,226.81,10.98;8,324.11,327.50,215.89,10.98;8,324.11,339.46,38.47,10.98" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,488.31,315.55,51.70,10.98;8,324.11,327.50,103.48,10.98">Ranking and reranking with perceptron</title>
		<author>
			<persName coords=""><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,437.88,327.50,72.69,10.87">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="73" to="96" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,352.16,226.81,10.98;8,324.11,364.12,215.90,10.98;8,324.11,376.07,215.89,10.98;8,324.11,388.03,83.02,10.98" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,401.54,364.12,138.47,10.98;8,324.11,376.07,153.07,10.98">Learning to rank answers to nonfactoid questions from web collections</title>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,484.65,376.07,55.35,10.87;8,324.11,388.03,52.62,10.87">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,400.73,226.82,10.98;8,324.11,412.68,215.90,10.98;8,324.11,424.64,215.91,10.87;8,324.11,436.60,215.90,10.87;8,324.11,448.55,168.71,10.87" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,415.59,400.73,124.43,10.98;8,324.11,412.68,160.92,10.98">A survey of answer extraction techniques in factoid question answering</title>
		<author>
			<persName coords=""><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,503.94,412.68,36.07,10.87;8,324.11,424.64,215.91,10.87;8,324.11,436.60,215.90,10.87;8,324.11,448.55,163.69,10.87">Proceedings of the Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</title>
		<meeting>the Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,461.25,226.81,10.98;8,324.11,473.21,215.90,10.98;8,324.11,485.16,215.90,10.87;8,324.11,497.12,215.90,10.87;8,324.11,509.07,215.90,10.98;8,324.11,521.03,92.14,10.98" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,324.11,473.21,199.75,10.98">Retrieval models for question and answer archives</title>
		<author>
			<persName coords=""><forename type="first">Xiaobing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiwoon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,324.11,485.16,215.90,10.87;8,324.11,497.12,215.90,10.87;8,324.11,509.07,124.63,10.98">Proceedings of the 31st Annual International ACM SI-GIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08</title>
		<meeting>the 31st Annual International ACM SI-GIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,533.73,226.81,10.98;8,324.11,545.69,215.90,10.98;8,324.11,557.64,215.89,10.98;8,324.11,569.60,215.90,10.87;8,324.11,581.55,215.89,10.98;8,324.11,593.51,215.90,10.98;8,324.11,605.46,108.49,10.98" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,434.75,545.69,105.26,10.98;8,324.11,557.64,135.08,10.98">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName coords=""><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrzej</forename><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,479.95,557.64,60.05,10.87;8,324.11,569.60,215.90,10.87;8,324.11,581.55,86.89,10.87;8,459.26,581.55,51.67,10.87">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08">2013. August</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
