<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.43,96.74,285.14,15.11">TREC 2015 Total Recall Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,204.62,129.22,76.32,10.48"><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
						</author>
						<author>
							<persName coords="1,191.85,143.17,100.95,10.48"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
						</author>
						<author>
							<persName coords="1,166.98,157.12,154.31,10.48"><roleName>Wachtell</roleName><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
						</author>
						<author>
							<persName coords="1,330.85,157.12,84.90,10.48"><forename type="first">Rosen</forename><forename type="middle">&amp;</forename><surname>Lipton</surname></persName>
						</author>
						<author>
							<persName coords="1,419.94,157.12,23.63,10.48;1,445.03,155.50,1.41,6.99"><surname>Katz</surname></persName>
						</author>
						<author>
							<persName coords="1,192.75,171.07,100.04,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.43,96.74,285.14,15.11">TREC 2015 Total Recall Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">069B9DF20E7C9C05304A3D7D5B74E88A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The primary purpose of the Total Recall Track is to evaluate, through controlled simulation, methods designed to achieve very high recall -as close as practicable to 100% -with a human assessor in the loop. Motivating applications include, among others, electronic discovery in legal proceedings <ref type="bibr" coords="1,401.78,297.05,9.96,8.74" target="#b3">[2]</ref>, systematic review in evidencebased medicine <ref type="bibr" coords="1,125.41,309.01,14.61,8.74" target="#b12">[11]</ref>, and the creation of fully labeled test collections for information retrieval ("IR") evaluation <ref type="bibr" coords="1,542.05,309.01,9.96,8.74" target="#b9">[8]</ref>. A secondary, but no less important, purpose is to develop a sandboxed virtual test environment within which IR systems may be tested, while preventing the disclosure of sensitive test data to participants. At the same time, the test environment also operates as a "black box," affording participants confidence that their proprietary systems cannot easily be reverse engineered.</p><p>The task to be solved in the Total Recall Track is the following:</p><p>Given a simple topic description -like those used for ad-hoc and Web search -identify the documents in a corpus, one at a time, such that, as nearly as possible, all relevant documents are identified before all non-relevant documents. Immediately after each document is identified, its ground-truth relevance or non-relevance is disclosed.</p><p>Datasets, topics, and automated relevance assessments were all provided by a Web server supplied by the Track.</p><p>Participants were required to implement either a fully automated ("automatic") or semi-automated ("manual") process to download the datasets and topics, and to submit documents for assessment to the Web server, which rendered a relevance assessment for each submitted document in real time. Thus, participants were tasked with identifying documents for review, while the Web server simulated the role of a human-in-the-loop assessor operating in real time. Rank-based and set-based evaluation measures were calculated based on the order in which documents were presented to the Web server for assessment, as well as the set of documents that were presented to the Web server at the time a participant "called their shot," or declared that a "reasonable" result had been achieved. Particular emphasis was placed on achieving high recall while reviewing the minimum possible number of documents.</p><p>The TREC 2015 Total Recall Track used a total of eight test collections: three for Practice runs, three for "At-Home" participation, and two for "Sandbox" participation. Practice and At-Home participation were done using the open Web: Participants ran their own systems and connected to the Web server at a public address. The Practice collections were available for several weeks prior to the At-Home collections; the At-Home collections were available for official runs throughout July and August 2015 (and continue to be available for unofficial runs).</p><p>Sandbox runs were conducted entirely on a Web-isolated platform hosting the data collections, from mid-September through mid-November 2015. To participate in the Sandbox task, participants were required to encapsulate -as a VirtualBox virtual machine -a fully autonomous solution that would contact the Web server and conduct the task without human intervention. The only feedback available to Sandbox participants consisted of summary evaluation measures showing the number of relevant documents identified, as a function of the total number of documents identified to the Web server for review.</p><p>To aid participants in the Practice, At-Home, and Sandbox tasks, as well as to provide a baseline for comparison, a Baseline Model Implementation ("BMI") was made available to participants. 1 BMI was run on all of the * Current affiliation: University of Waterloo. The views expressed herein are solely those of the author and should not be attributed to her former firm or its clients.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>collections, and summary results were supplied to participants for their own runs, as well as for the BMI runs. The system architecture for the Track is detailed in a separate Notebook paper titled Total Recall Track Tools Architecture Overview <ref type="bibr" coords="2,156.97,83.76,14.61,8.74" target="#b17">[16]</ref>.</p><p>The TREC 2015 Total Recall Track attracted 10 participants, including three industrial groups that submitted "manual athome" runs, two academic groups that submitted only "automatic athome" runs, and five academic groups that submitted both "automatic athome" and "sandbox" runs.</p><p>The 2015 At-Home collections consisted of three datasets and 30 topics. The Jeb Bush emails<ref type="foot" coords="2,470.11,130.01,3.97,6.12" target="#foot_0">2</ref> were collected and assessed for 10 topics by the Track coordinators. The "Illicit Goods" and "Local Politics" datasets, along with 10 topics for each, were derived from the Dynamic Domain datasets <ref type="foot" coords="2,344.58,153.92,3.97,6.12" target="#foot_1">3</ref> and assessed by the Total Recall coordinators. These collections continue to be available through the Total Recall Server to 2015 participants, and were made available to 2016 participants for training purposes.</p><p>The Sandbox collections consisted of two datasets and 23 topics. On-site access to former Governor Tim Kaine's email collection at the Library of Virginia<ref type="foot" coords="2,239.16,201.74,3.97,6.12" target="#foot_2">4</ref> was arranged by the Track coordinators, where a "Sandbox appliance" was used to conduct and evaluate participant runs according to topics that corresponded to archival category labels previously applied by the Library's Senior State Records Archivist: "Not a Public Record," "Open Public Record," "Restricted Public Record," and "Virginia Tech Shooting Record." The coordinators also secured approval to use the MIMIC II clinical dataset <ref type="foot" coords="2,182.85,249.56,3.97,6.12" target="#foot_3">5</ref> as the second Sandbox dataset. The textual documents from this dataset -consisting of discharge summaries, nurses' notes, and radiology reports -were used as the corpus; the 19 top-level codes in the ICD-9 hierarchy<ref type="foot" coords="2,144.06,273.47,3.97,6.12" target="#foot_4">6</ref> were used as the "topics."</p><p>The principal tool for comparing runs was a gain curve, which plots recall (i.e., the proportion of all relevant documents submitted to the Web server for review) as a function of effort (i.e., the total number of documents submitted to the Web server for review). A run that achieves higher recall with less effort demonstrates superior effectiveness, particularly at high recall levels. The traditional recall-precision curve conveys similar information, plotting precision (i.e., the proportion of documents submitted to the Web server that are relevant) as a function of recall (i.e., the proportion of all relevant documents submitted to the Web server for review). Both curves convey similar information, but are influenced differently by prevalence or richness (i.e., the proportion of documents in the collection that are relevant), and convey different impressions when averaged over topics with different richness.</p><p>A gain curve or recall-precision curve is blind to the important consideration of when to stop a retrieval effort. In general, the density of relevant documents diminishes as effort increases, and at some point, the benefit of identifying more relevant documents no longer justifies the review effort required to find them. Participants were asked to "call their shot," or to indicate when they thought a "reasonable" result had been achieved; that is, to specify the point at which they would recommend terminating the review process because further effort would be "disproportionate." They were not actually required to stop at this point, they were simply given the option to indicate, contemporaneously, when they would have chosen to stop had they been required to do so. For this point, we report traditional set-based measures such as recall, precision, and F 1 .</p><p>To evaluate the appropriateness of various possible stopping points, the Track coordinators devised a new parametric measure: recall @ aR + b, for various values of a and b. Recall @ aR + b is defined to be the recall achieved when aR + b documents have been submitted to the Web server, where R is the number of relevant documents in the collection. In its simplest form recall @aR + b [a = 1; b = 0] is equivalent to R-precision, which has been used since TREC 1 as an evaluation measure for relevance ranking. R-precision might equally well be called R-recall, as precision and recall are, by definition, equal when R documents have been reviewed. The parameters a and b allow us to explore the recall that might be achieved when a times as many documents, plus an additional b documents are reviewed. The parameter a admits that it may be reasonable to review more than one document for every relevant one that is found; the parameter b admits that it may be reasonable to review a fixed number of additional documents, over and above the number that are relevant. For example, if there are 100 relevant documents in the collection, it may be reasonable to review 200 documents (a = 2), plus an additional 100 documents (b = 100), for a total of 300 documents, in order to achieve high recall. In this Track Overview paper, we report all combinations of a ∈ {1, 2, 4} and b ∈ {0, 100, 1000}.</p><p>At the time of 2015 Total Recall Track, the coordinators had hoped to be able to implement facet-based variants of the recall measures described above (see Cormack &amp; Grossman <ref type="bibr" coords="2,350.95,645.65,10.30,8.74" target="#b4">[3]</ref>), but suitable relevance assessments for the facets were not available in time. We therefore decided to implement such measures in a future Track. The rationale for facet-based measures derives from the fact that, due to a number of factors including assessor disagreement, a goal of recall=1.0 is neither reasonable nor achievable in most circumstances. However, it is difficult to justify an arbitrary lower target of, say, recall=0.8, without characterizing the nature of the 20% relevant documents that are omitted by such an effort. Are these omitted documents simply marginal documents about whose relevance reasonable people might disagree, or do they represent a unique and important (though perhaps rare) class of clearly relevant documents? To explore this question, we wanted to be able to calculate the recall measures for a given run separately for each of several facets representing different classes of documents; a superior high-recall run should be expected to achieve high recall on all facets. This issue remains to be explored.</p><p>In calculating effort and precision, the measures outlined above consider only the number of documents submitted to the Web server for assessment. For manual runs, however, participants were permitted to look at the documents, and hence conduct their own assessments. Participants were asked to track and report the number of documents they reviewed; when supplied by participants, these numbers are reported in this Overview and should be considered when comparing manual runs to one another, or to automatic runs. It is not obvious how one would incorporate this effort formulaically into the gain curves, precision-recall curves, and recall @ aR + b measures; therefore, the coordinators have chosen not to try.</p><p>Results for the TREC 2015 Total Recall Track show that a number of methods achieved results with very high recall and precision, on all collections, according to the standards set by previous TREC tasks. This observation should be interpreted in light of the fact that runs were afforded an unprecedented amount of relevance feedback, allowing them to receive authoritative relevance assessments throughout the process.</p><p>Overall, no run consistently achieved higher recall at lower effort than BMI. A number of runs, including manual runs, automatic runs, and the baseline runs, appeared to achieve similar effectiveness -all near the best on every collection -but with no run consistently bettering the rest on every collection. Thus, The 2015 Total Recall Track had no clear "winner."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Test Collections</head><p>Each test collection consisted of a corpus of English-language documents, a set of topics, and a complete set of relevance assessments for each topic. For Practice runs, we used three public document corpora for which topics and relevance assessments were available:</p><p>• The 20 Newsgroups Dataset,<ref type="foot" coords="3,206.08,407.94,3.97,6.12" target="#foot_5">7</ref> consisting of 18,828 documents from each of 20 newsgroups. We used three of the newsgroup subject categories -"space," "hockey," and "baseball" -as the three practice topics in the test practice collection.</p><p>• The Reuters-21578 Test Collection,<ref type="foot" coords="3,236.86,451.77,3.97,6.12" target="#foot_6">8</ref> consisting of 21,578 newswire documents. We used four of the subject categories -"acquisitions," "Deutsche Mark," "groundnut," and "livestock" -as the four practice topics in the test practice collection.</p><p>• The Enron Dataset used by the TREC 2009 Legal Track <ref type="bibr" coords="3,339.86,497.18,14.61,8.74" target="#b11">[10]</ref>. We used a version of this dataset captured by the University of Waterloo in the course of its participation in TREC 2009, modified to exclude vacuous documents, resulting in a corpus of 723,537 documents. We used two of the topics from the TREC 2009 Legal Track -"Fantasy Football" and "Prepay Transactions" -as the two practice topics for the bigtest practice collection. The relevance assessments were derived from those rendered by the University of Waterloo team and the official TREC assessments, with deference to the official assessments.</p><p>For the At-Home runs, we used three new datasets:</p><p>• The (redacted) Jeb Bush Emails,<ref type="foot" coords="3,224.66,595.24,3.97,6.12" target="#foot_7">9</ref> consisting of 290,099 emails from Jeb Bush's eight-year tenure as Governor of Florida. We used 10 issues associated with his governorship as topics for the athome1 test collection: "school and preschool funding," "judicial selection," "capital punishment," "manatee protection," "new medical schools," "affirmative action," "Terri Schiavo," "tort reform," "Manatee County," and "Scarlet Letter Law." Using the continuous active learning ("CAL") method of Cormack and Mojdeh <ref type="bibr" coords="3,469.75,644.63,9.96,8.74" target="#b6">[5]</ref>, the Track coordinators assessed documents in the corpus to identify as many of the relevant documents for each topic as reasonably possible.</p><p>Participating Team athome1 athome2 • The Illicit Goods dataset collected for the TREC 2015 Dynamic Domain Track <ref type="bibr" coords="4,431.87,257.49,14.60,8.74" target="#b18">[17]</ref>. We used 465,147 documents collected from Blackhat World<ref type="foot" coords="4,241.94,267.87,7.94,6.12" target="#foot_8">10</ref> and Hack Forum. <ref type="foot" coords="4,328.07,267.87,7.94,6.12" target="#foot_9">11</ref> For the athome2 test collection, we used 10 of the many topics that were composed and partially assessed by NIST assessors for use by the Dynamic Domain Track: "paying for Amazon book reviews," "CAPTCHA services," "Facebook accounts," "surely Bitcoins can be used," "Paypal accounts," "using TOR for anonymous browsing," "rootkits," "Web scraping," "article spinner spinning," and "offshore Web sites." The Track coordinators re-assessed the documents using the CAL method of Cormack and Mojdeh <ref type="bibr" coords="4,255.35,329.22,9.96,8.74" target="#b6">[5]</ref>, to identify as many of the relevant documents for each topic as reasonably possible.</p><p>• The Local Politics dataset collected for the TREC 2015 Dynamic Domain Track <ref type="bibr" coords="4,427.77,360.89,14.61,8.74" target="#b18">[17]</ref>. We used 902,434 articles collected from news sources in the northwestern United States and southwestern Canada. For the athome3 test collection, we used 10 of the many topics that were composed and partially assessed by NIST assessors for use by the Dynamic Domain Track: "Pickton murders," "Pacific Gateway," "traffic enforcement cameras," "rooster chicken turkey nuisance," "Occupy Vancouver," "Rob McKenna gubernatorial candidate," "Rob Ford Cut the Waist," "Kingston Mills lock murder," "fracking," and "Paul and Cathy Lee Martin." The Track coordinators re-assessed the documents using the CAL method of Cormack and Mojdeh <ref type="bibr" coords="4,478.75,432.62,9.96,8.74" target="#b6">[5]</ref>, to identify as many of the relevant documents for each topic as reasonably possible.</p><p>For the Sandbox runs, we used two new datasets:</p><p>• The Kaine Email Collection at the Library of Virginia. <ref type="foot" coords="4,324.22,482.01,7.94,6.12" target="#foot_10">12</ref> From the 1.3M email messages from Tim Kaine's eight-year tenure as Governor of Virginia, we used 401,953 that had previously been labeled by the Virginia Senior State Records Archivist according to the following four categories: "public record," "open record," "restricted record," and "Virginia Tech shooting ([subject to a legal] hold)." Each of the four categories was used as a topic in the kaine test collection. The runs themselves were executed on an isolated computer installed at the Library of Virginia and operated by Library of Virginia staff.</p><p>• The MIMIC II Clinical Dataset, <ref type="foot" coords="4,221.66,561.50,7.94,6.12" target="#foot_11">13</ref> consisting of anonymized, time-shifted, records for 31,538 patient visits to an Intensive Care Unit. We used the textual record for each patient -consisting of one or more nurses' notes, radiology reports, and discharge summaries -as a "document" in the corpus, and each of 19 top-level ICD-9 codes supplied with the dataset as a topic for the mimic test collection: "infectious and parasitic diseases," "neoplasms," "endocrine, nutritional and metabolic diseases, and immunity disorders," "diseases of the blood and blood-forming organs," "mental disorders," "diseases of the nervous system and sense organs," "diseases of the circulatory system," "diseases of the respiratory system," "diseases of the digestive system," "diseases of the genitourinary system," "complications of pregnancy, childbirth, and the puerperium," "diseases of the skin and subcutaneous tissue," "diseases of the musculoskeletal system and connective tissue," "congenital anomalies," "certain conditions originating in the perinatal period," "symptoms, signs, and ill-defined conditions," "injury and poisoning," "factors influencing health status and contact with health services," and "external causes of injury and poisoning." The runs were executed on an isolated computer installed at the University of Waterloo and operated by the Track coordinators.</p><p>Table <ref type="table" coords="5,84.09,91.73,4.98,8.74" target="#tab_0">1</ref> shows the number of runs submitted for each test collection by each participating team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participant Submissions</head><p>The following descriptions are paraphrased from responses to a required questionnaire submitted by each participating team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">UvA.ILPS</head><p>The UvA.ILPS team <ref type="bibr" coords="5,151.81,204.73,10.52,8.74" target="#b7">[6]</ref> used automatic methods for the At-Home and Sandbox tests that modified the Baseline Model Implementation in two ways:</p><p>1. adjusted the batch size based on the number of retrieved relevant documents and stopped after a threshold if the batch contained no relevant documents;</p><p>2. one run used logistic regression (as per the Baseline Model Implementation), while another used random forests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">WaterlooClarke</head><p>The WaterlooClarke team <ref type="bibr" coords="5,173.26,326.73,10.52,8.74" target="#b10">[9]</ref> used automatic methods for At-Home and Sandbox tests that:</p><p>1. employed clustering to improve the diversity of feedback to the learning algorithm;</p><p>2. used n-gram features beyond the bag-of-words tf-idf model provided in the Baseline Model Implementation;</p><p>3. employed query expansion;</p><p>4. used the fusion of differently ranking algorithms.</p><p>The WaterlooClarke team consisted of a group of graduate students who had no access to the test collections beyond that afforded to all participating teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">WaterlooCormack</head><p>The WaterlooCormack team <ref type="bibr" coords="5,187.35,484.58,10.52,8.74" target="#b5">[4]</ref> employed the Baseline Model Implementation, without modification, except to "call its shot" to determine when to stop. Therefore, the gain curves, recall-precision curves, and recall @ aR + b statistics labeled "WaterlooCormack" are synonymous with the Baseline Model Implementation. Two different stopping criteria were investigated:</p><p>1. a "knee-detection" algorithm was applied to the gain curve, and the decision that a reasonable result had been achieved was made when the slope of the curve after the knee was a fraction of the slope before the knee; 2. a "reasonable" result was deemed to have been achieved when m relevant and n non-relevant documents had been reviewed, where n &gt; a • m + b , where a and b are predetermined constants. For example, when a = 1 and b = 2399, review would be deemed to be complete when the number of non-relevant documents retrieved was equal to the number of relevant documents retrieved, plus 2,399. In general, the constant a determines how many non-relevant documents are to be reviewed in the course of finding each relevant document, while b represents fixed overhead, independent of the number of relevant documents.</p><p>The WaterlooCormack team consisted of Gordon V. Cormack and Maura R. Grossman, who were both Track coordinators. The Baseline Model Implementation was fixed prior to the development of any of the datasets.</p><p>Cormack and Grossman had knowledge of the At-Home test collections, but not the Sandbox test collections, when the stopping criteria were chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Webis</head><p>The Webis team <ref type="bibr" coords="6,131.47,78.24,15.50,8.74" target="#b16">[15]</ref> employed two methods:</p><p>1. a basic naïve approach in retrieving as many relevant documents as possible;</p><p>2. a keyphrase experiment that built on the BMI system by intelligently obtaining a list of phrases from documents judged by the API as relevant and using them as new topics for ad-hoc search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CCRi</head><p>The CCRi team <ref type="bibr" coords="6,127.16,176.32,10.52,8.74" target="#b8">[7]</ref> represented words from the input corpus as vectors using a neural network model and represented documents as a tf-idf weighted sum of their word vectors. This model was designed to produce a compact versions of tf-idf vectors while incorporating information about synonyms. For each query topic, CCRi attached a neural network classifier to the output of BMI. Each classifier was updated dynamically with respect to the given relevance assessments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">eDiscoveryTeam</head><p>eDiscoveryTeam <ref type="bibr" coords="6,130.64,270.42,15.50,8.74" target="#b14">[13]</ref> employed a manual approach for the athome1, athome2, and athome3 test collections. Eight hours of manual search and review were conducted, on average, per topic. Two of the eight hours were spent composing 25 queries (per topic, on average) and examining their results; six hours were spent reviewing 500 documents (per topic, on average), of which only those deemed relevant were submitted to the automated assessment server. During the search and review process, Web searches were conducted where necessary to inform the searchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">NINJA</head><p>The NINJA team employed a manual approach for the athome1, athome2, and athome3 test collections. One hour of manual search was conducted, in which three queries were composed, on average, per topic. Wikipedia and Google searches were used to inform the searchers. A commercial "predictive coding" tool, trained using the results of the queries, was used to generate the NINJA runs. No documents from the test collection were reviewed prior to being submitted to the automated assessment server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">catres</head><p>The catres team <ref type="bibr" coords="6,134.15,458.62,15.50,8.74" target="#b13">[12]</ref> employed a manual approach for the athome1 test collection. A group of searchers independently spent one hour each investigating each topic, after which a learning tool was used to generate the run. An average of eight manual queries were used per topic, and an average of 262 documents were reviewed. Every document reviewed by the team was also submitted to the automated assessment server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">TUW</head><p>The TUW team <ref type="bibr" coords="6,128.90,540.76,15.50,8.74" target="#b15">[14]</ref> applied six variants on the Baseline Model Implementation to all of the At-Home and Sandbox test collections. The variants included the use and non-use of a BM25 ranking, the use and non-use of stop words, and the use and non-use of tf-idf weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">WHU IRGroup</head><p>The WHU IRGroup team <ref type="bibr" coords="6,172.60,610.95,10.52,8.74" target="#b2">[1]</ref> applied iterative query expansion to the athome1 test collection. The top panel plots effectiveness as a gain curve, while the bottom panel plots effectiveness as a recall-precision curve. The gain curve shows that a number of the systems achieved 90% recall, on average, with a review effort of 10,000 documents. The recall-precision curve, on the other hand, shows that a number of the systems achieved 90%  recall, with higher than 50% precision, on average. It is not obvious which of these results is preferable. For a topic with only 100 relevant documents, it may be considered unreasonable to require the review of 10,000 documents; the average gain curve gives little insight in this regard. Individual per-topic gain curves (Figure <ref type="figure" coords="16,481.65,83.76,4.43,8.74">6</ref>) offer per-topic insight, but are challenging to generalize. The recall-precision curve indicates that it is possible to achieve 90% recall, with 50% precision or better; but if there are 10,000 documents to be found, is 50% good enough? Moreover, is a system that requires the review of 1,000 documents to find 100 (i.e., 10% precision) inferior to a system that requires the review of 5,000 documents to find 1,000 (i.e., 20% precision)? We suggest that a reasonable measure may lie somewhere in between these two extremes: The effectiveness of a system depends both on the absolute effort required (as shown by effort in the gain curve) and the effort required relative to the number of relevant documents found (as shown by precision in the recall-precision curve). Ideally, an effective system would score well on both curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gain Curves and Recall-Precision Curves</head><p>The average gain and recall-precision curves for the remaining Athome test collections -athome2 and athome3 -are shown in Figures <ref type="figure" coords="16,156.64,203.31,4.98,8.74">2</ref> and<ref type="figure" coords="16,183.44,203.31,3.87,8.74">3</ref>; per-topic gain curves are shown in Figures <ref type="figure" coords="16,379.46,203.31,4.98,8.74">7</ref> and<ref type="figure" coords="16,406.25,203.31,3.87,8.74">8</ref>. Average curves for the Sandbox test collections -Kaine and MIMIC II -are shown in Figures <ref type="figure" coords="16,322.87,215.27,4.98,8.74">4</ref> and<ref type="figure" coords="16,349.42,215.27,3.87,8.74">5</ref>. Per-topic gain curves for the Kaine collection are shown in Figure <ref type="figure" coords="16,149.27,227.22,3.87,8.74" target="#fig_1">9</ref>; per-topic curves for MIMIC-II are omitted for brevity. From these curves it can be seen that a number of systems -including the Baseline Model Implementation (denoted as "WaterlooCormack" in the curves) -achieve similar effectiveness, but no single system dominates over all topics, or over all collections. Further study is necessary to determine whether the observed differences among the top-performing systems on particular topics and test collections represent real, reproducible differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recall @ aR+b</head><p>Tables <ref type="table" coords="16,88.64,321.32,52.18,8.74" target="#tab_2">2 through 6</ref> show, for each test collection, the new measure Recall @ aR+b. This measure quantifies the tradeoff between achieving high recall with effort proportionate to the number of relevant documents, and achieving high recall with reasonable overall effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">When to Stop?</head><p>Participants were afforded the opportunity to "call their shot," indicating the point at which they would have recommended terminating the search because the additional effort to identify more relevant documents would have been unreasonable or disproportionate. Some participants did "call their shot," while others simply terminated their retrieval effort. In both cases, we tabulated the recall that had been achieved at that point and the effort (in terms of the number of assessed documents) necessary to achieve it. Clearly, there is a tradeoff between recall and effort; we made no attempt to quantify what was a reasonable compromise, and instead, present the raw, per-topic precision and effort results in Figures 7 through 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In 2015, the inaugural year of the Total Recall Track, it was necessary to develop new, completely labeled datasets, a new evaluation architecture, new evaluation measures, and a Baseline Model Implementation.</p><p>For the Athome task, the original plan was to use datasets labeled by NIST assessors on a five-point relevance scale, according to a two-level hierarchy of topics and subtopics. This labeling effort was not completed within the allotted timeframe and budget, so the Total Recall coordinators decided to conduct binary relevance assessments for 30 topics across three datasets. Candidate documents for assessment were selected using a combination of ad-hoc search and machine learning. With few exceptions (e.g., documents containing "schiavo" or "lethal injection"), every document labeled relevant was assessed by one of the Track coordinators. The selection and assessment of documents continued until the coordinators believed that substantially all relevant documents had been found; documents that were not selected for assessment were summarily labeled "not relevant." The Sandbox task used binary relevance assessments that did not rely in any way on the Track coordinators, or any search or machinelearning software. The Kaine collection was exhaustively labeled by the Senior State Records Archivist; the MIMIC II collection contained ICD-9 diagnostic codes which were used as relevance labels. The similarity between the Athome and Sandbox results suggests that any confounding due to the method of selecting and assessing the documents was minimal.   <ref type="table" coords="24,435.58,700.40,4.37,7.75">9</ref>: Set-based Results (recall over effort) for the athome3 test collection. The top number in each cell indicates the recall achieved when the submission indicated by "calling its shot" that a "reasonable" result had been achieved, or when the submission terminated its run. The second number indicates the number of documents submitted to the automated assessment server at this point. (*) indicates the number of additional documents reviewed by a manual review team, and not necessarily submitted to the automated assessment server. the submission indicated by "calling its shot" that a "reasonable" result had been achieved, or when the submission terminated its run. The second number indicates the number of documents submitted to the automated assessment server at this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The 2015 Total Recall Track successfully deployed a new evaluation architecture, using five new datasets. Three of the datasets will be publicly available, while two datasets must remain private. The results appear to be consistent across all datasets: A fully automated Baseline Model Implementation achieved high recall for all topics, with effort proportionate to the number of relevant documents. Several manual and automatic participant efforts achieved higher recall with less effort than the baseline on some topics, but none consistently improved on the baseline. Some teams appeared to be able to predict when a "reasonable" result had been achieved; however, further work is needed to derive appropriate measures to evaluate what is "reasonable."</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,56.69,686.10,498.61,8.74;6,56.69,698.05,498.61,8.74;6,56.69,710.01,498.62,8.74;6,56.69,721.96,498.61,8.74"><head>Figure 1</head><label>1</label><figDesc>Figure1plots the effectiveness of the best run for each of the participating teams on the athome1 test collection. The top panel plots effectiveness as a gain curve, while the bottom panel plots effectiveness as a recall-precision curve. The gain curve shows that a number of the systems achieved 90% recall, on average, with a review effort of 10,000 documents. The recall-precision curve, on the other hand, shows that a number of the systems achieved 90%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="15,167.76,571.58,276.48,8.74"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Per-Topic Gain Curves for the Kaine Test Collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="25,204.45,369.08,8.74,24.08;25,204.45,350.68,8.74,15.08;25,204.45,342.38,8.74,4.98;25,204.45,313.59,8.74,25.46;25,204.45,266.55,8.74,43.72;25,216.41,395.91,8.74,23.53;25,216.41,332.40,8.74,44.50;25,216.41,286.32,8.74,30.61;25,216.41,257.01,8.74,13.84;25,216.41,233.22,8.74,20.48;25,228.36,388.85,8.74,37.64;25,228.36,338.33,8.74,32.66;25,228.36,282.81,8.74,37.64;25,228.36,235.71,8.74,32.66;25,246.29,438.84,8.74,82.80;25,240.32,393.84,8.74,27.67;25,240.32,340.82,8.74,27.67;25,240.32,287.79,8.74,27.67;25,240.32,238.20,8.74,27.67;25,252.27,392.73,8.74,29.89;25,252.27,342.20,8.74,24.91;25,252.27,286.68,8.74,29.89;25,252.27,239.58,8.74,24.91;25,270.20,438.85,8.74,114.76;25,264.23,393.84,8.74,27.67;25,264.23,340.82,8.74,27.67;25,264.23,287.79,8.74,27.67;25,264.23,238.20,8.74,27.67;25,276.18,400.20,8.74,14.94;25,276.18,347.18,8.74,14.94;25,276.18,294.15,8.74,14.94;25,276.18,244.57,8.74,14.94;25,294.11,438.84,8.74,119.94;25,288.14,393.84,8.74,27.67;25,288.14,340.82,8.74,27.67;25,288.14,287.79,8.74,27.67;25,288.14,238.20,8.74,27.67;25,300.09,392.73,8.74,29.89;25,300.09,342.20,8.74,24.91;25,300.09,286.68,8.74,29.89;25,300.09,239.58,8.74,24.91;25,318.02,438.84,8.74,63.76;25,312.05,393.84,8.74,27.67;25,312.05,340.82,8.74,27.67;25,312.05,287.79,8.74,27.67;25,312.05,238.20,8.74,27.67;25,324.00,392.73,8.74,29.89;25,324.00,342.20,8.74,24.91;25,324.00,286.68,8.74,29.89;25,324.00,239.58,8.74,24.91;25,341.94,438.84,8.74,72.37;25,335.96,393.84,8.74,27.67;25,335.96,340.82,8.74,27.67;25,335.96,287.79,8.74,27.67;25,335.96,238.20,8.74,27.67;25,369.83,711.23,8.74,24.08;25,369.83,695.93,8.74,12.73;25,369.83,648.78,8.74,43.09;25,369.83,614.41,8.74,31.80;25,369.83,581.25,8.74,30.58;25,369.83,557.61,8.74,21.07;25,369.83,525.82,8.74,29.22;25,369.83,508.68,8.74,14.55;25,369.83,492.27,8.74,13.84;25,369.83,464.24,8.74,25.45;25,369.83,443.06,8.74,18.60;25,369.83,393.98,8.74,46.49;25,369.83,372.65,8.74,17.16;25,369.83,355.68,8.74,14.39;25,369.83,320.15,8.74,32.96;25,369.83,309.27,8.74,8.30;25,369.83,287.60,8.74,19.09;25,369.83,270.64,8.74,14.39;25,369.83,229.81,8.74,38.25;25,369.83,213.39,8.74,13.84;25,369.83,187.55,8.74,23.27;25,369.83,148.17,8.74,36.80;25,369.83,122.89,8.74,22.69;25,369.83,106.49,8.74,13.83;25,369.83,56.69,8.74,47.21;25,381.79,695.46,8.74,39.85;25,381.79,681.41,8.74,10.51;25,381.79,644.68,8.74,33.21;25,381.79,630.59,8.74,10.57;25,381.79,603.76,8.74,23.30;25,381.79,581.97,8.74,18.27;25,381.79,573.46,8.74,4.98;25,381.79,514.51,8.74,55.43;25,381.79,486.55,8.74,24.44;25,381.79,466.97,8.74,16.05;25,381.79,443.25,8.74,20.19;25,381.79,400.15,8.74,39.58;25,381.79,387.69,8.74,8.88;25,381.79,361.48,8.74,22.69;25,381.79,344.11,8.74,13.84;25,381.79,293.37,8.74,47.21;25,381.79,242.23,8.74,47.63;25,381.79,228.13,8.74,10.57;25,381.79,206.86,8.74,17.74;25,381.79,184.67,8.74,17.16;25,381.79,152.31,8.74,28.84;25,381.79,115.83,8.74,32.96;25,381.79,74.06,8.74,38.25;25,381.79,56.69,8.74,13.84;25,393.74,702.35,8.74,32.96;25,393.74,691.01,8.74,8.03;25,393.74,641.13,8.74,46.55;25,393.74,594.03,8.74,43.78;25,393.74,581.86,8.74,8.86;25,393.74,564.69,8.74,13.84;25,393.74,514.88,8.74,46.49;25,393.74,464.57,8.74,47.00;25,393.74,435.68,8.74,25.58;25,393.74,423.50,8.74,8.86;25,393.74,404.08,8.74,16.11;25,393.74,375.30,8.74,25.45;26,198.47,395.59,8.74,24.08;26,198.47,377.19,8.74,15.08;26,198.47,368.89,8.74,4.98;26,198.47,332.91,8.74,32.66;26,198.47,322.12,8.74,7.47;26,198.47,275.07,8.74,43.72;26,198.47,249.72,8.74,22.04;26,198.47,240.04,8.74,6.37;26,210.43,507.08,8.74,17.16;26,210.43,467.45,8.74,17.16;26,210.43,425.33,8.74,17.16;26,210.43,383.21,8.74,17.16;26,210.43,343.58,8.74,17.16;26,210.43,303.95,8.74,17.16;26,210.43,261.83,8.74,17.16;26,210.43,217.22,8.74,17.16;26,210.43,175.10,8.74,17.16;26,210.43,135.48,8.74,17.16;26,222.38,501.82,8.74,27.67;26,222.38,462.19,8.74,27.67;26,222.38,417.58,8.74,32.66;26,222.38,377.95,8.74,27.67;26,222.38,338.32,8.74,27.67;26,222.38,298.69,8.74,27.67;26,222.38,254.08,8.74,32.66;26,222.38,209.47,8.74,32.66;26,222.38,169.84,8.74,27.67;26,222.38,130.21,8.74,27.67;26,240.32,541.85,8.74,82.80;26,234.34,501.82,8.74,27.67;26,234.34,462.19,8.74,27.67;26,234.34,420.07,8.74,27.67;26,234.34,377.95,8.74,27.67;26,234.34,338.32,8.74,27.67;26,234.34,298.69,8.74,27.67;26,234.34,256.57,8.74,27.67;26,234.34,211.96,8.74,27.67;26,234.34,169.84,8.74,27.67;26,234.34,130.21,8.74,27.67;26,246.29,505.69,8.74,19.93;26,246.29,466.06,8.74,19.93;26,246.29,426.43,8.74,14.94;26,246.29,381.83,8.74,19.93;26,246.29,342.20,8.74,19.93;26,246.29,302.57,8.74,19.93;26,246.29,257.95,8.74,24.91;26,246.29,218.32,8.74,14.94;26,246.29,176.21,8.74,14.94;26,246.29,134.09,8.74,19.93;26,264.23,541.85,8.74,87.78;26,258.25,501.82,8.74,27.67;26,258.25,462.19,8.74,27.67;26,258.25,420.07,8.74,27.67;26,258.25,377.95,8.74,27.67;26,258.25,338.32,8.74,27.67;26,258.25,298.69,8.74,27.67;26,258.25,256.57,8.74,27.67;26,258.25,211.96,8.74,27.67;26,258.25,169.84,8.74,27.67;26,258.25,130.21,8.74,27.67;26,270.20,505.69,8.74,19.93;26,270.20,466.06,8.74,19.93;26,270.20,421.45,8.74,24.91;26,270.20,381.83,8.74,19.93;26,270.20,342.20,8.74,19.93;26,270.20,302.57,8.74,19.93;26,270.20,257.95,8.74,24.91;26,270.20,213.34,8.74,24.91;26,270.20,173.72,8.74,19.93;26,270.20,134.09,8.74,19.93;26,288.14,541.85,8.74,114.76;26,282.16,501.82,8.74,27.67;26,282.16,462.19,8.74,27.67;26,282.16,420.07,8.74,27.67;26,282.16,377.95,8.74,27.67;26,282.16,338.32,8.74,27.67;26,282.16,298.69,8.74,27.67;26,282.16,256.57,8.74,27.67;26,282.16,211.96,8.74,27.67;26,282.16,169.84,8.74,27.67;26,282.16,130.21,8.74,27.67;26,294.11,508.19,8.74,14.94;26,294.11,468.55,8.74,14.94;26,294.11,426.43,8.74,14.94;26,294.11,384.32,8.74,14.94;26,294.11,344.69,8.74,14.94;26,294.11,305.06,8.74,14.94;26,294.11,262.94,8.74,14.94;26,294.11,218.32,8.74,14.94;26,294.11,176.21,8.74,14.94;26,294.11,136.58,8.74,14.94;26,312.05,541.85,8.74,119.94;26,306.07,501.82,8.74,27.67;26,306.07,462.19,8.74,27.67;26,306.07,420.07,8.74,27.67;26,306.07,377.95,8.74,27.67;26,306.07,338.32,8.74,27.67;26,306.07,298.69,8.74,27.67;26,306.07,256.57,8.74,27.67;26,306.07,211.96,8.74,27.67;26,306.07,169.84,8.74,27.67;26,306.07,130.21,8.74,27.67;26,318.02,505.69,8.74,19.93;26,318.02,466.06,8.74,19.93;26,318.02,421.45,8.74,24.91;26,318.02,381.83,8.74,19.93;26,318.02,342.20,8.74,19.93;26,318.02,302.57,8.74,19.93;26,318.02,257.95,8.74,24.91;26,318.02,213.34,8.74,24.91;26,318.02,171.23,8.74,24.91;26,318.02,131.60,8.74,24.91;26,335.96,541.85,8.74,63.76;26,329.98,501.82,8.74,27.67;26,329.98,462.19,8.74,27.67;26,329.98,420.07,8.74,27.67;26,329.98,377.95,8.74,27.67;26,329.98,338.32,8.74,27.67;26,329.98,298.69,8.74,27.67;26,329.98,256.57,8.74,27.67;26,329.98,211.96,8.74,27.67;26,329.98,169.84,8.74,27.67;26,329.98,130.21,8.74,27.67;26,341.94,505.69,8.74,19.93;26,341.94,466.06,8.74,19.93;26,341.94,421.45,8.74,24.91;26,341.94,381.83,8.74,19.93;26,341.94,342.20,8.74,19.93;26,341.94,302.57,8.74,19.93;26,341.94,257.95,8.74,24.91;26,341.94,213.34,8.74,24.91;26,341.94,173.72,8.74,19.93;26,341.94,131.60,8.74,24.91;26,359.87,541.85,8.74,72.37;26,353.89,501.82,8.74,27.67;26,353.89,462.19,8.74,27.67;26,353.89,420.07,8.74,27.67;26,353.89,377.95,8.74,27.67;26,353.89,338.32,8.74,27.67;26,353.89,298.69,8.74,27.67;26,353.89,256.57,8.74,27.67;26,353.89,211.96,8.74,27.67;26,353.89,169.84,8.74,27.67;26,353.89,130.21,8.74,27.67;26,387.76,711.23,8.74,24.08;26,387.76,695.02,8.74,12.73;26,387.76,647.19,8.74,43.09;26,387.76,611.91,8.74,31.80;26,387.76,577.85,8.74,30.58;26,387.76,553.31,8.74,21.07;26,387.76,520.61,8.74,29.22;26,387.76,505.20,8.74,11.93;26,387.76,487.89,8.74,13.84;26,387.76,451.74,8.74,32.66;26,387.76,440.79,8.74,7.47;26,387.76,418.72,8.74,18.60;26,387.76,368.75,8.74,46.49;26,387.76,345.97,8.74,19.26;26,387.76,336.12,8.74,6.37;26,387.76,315.48,8.74,17.16;26,387.76,297.62,8.74,14.39;26,387.76,261.17,8.74,32.97;26,387.76,249.39,8.74,8.30;26,387.76,226.82,8.74,19.09;26,387.76,208.95,8.74,14.39;26,387.76,167.22,8.74,38.25;26,387.76,149.91,8.74,13.84;26,387.76,123.15,8.74,23.27;26,387.76,82.87,8.74,36.80;26,387.76,56.69,8.74,22.69"><head>TopicTable 10 :Table 11 :</head><label>1011</label><figDesc>Set-Based Results (Recall Over Effort) For the Kaine Test Collection. The top number in each cell indicates the recall achieved when the submission indicated by "calling its shot" that a "reasonable" result had been achieved, or when the submission terminated its run. The second number indicates the number of documents submitted to the automated assessment server at this point. Topic (R) -MIMIC II Collection [Part I] Set-Based Results (Recall Over Effort) for the MIMIC II Test Collection, Part I. The top number in each cell indicates the recall achieved when</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,56.69,58.63,498.62,174.54"><head>Table 1 :</head><label>1</label><figDesc>Participation in the TREC 2015 Total Recall Track.Table entries indicate the number of runs submitted for each test collection by a particular participating team. "M" indicates manual runs, "A" indicates automatic runs.</figDesc><table coords="4,341.34,58.63,147.44,8.77"><row><cell>athome3 Kaine MIMIC II</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="21,387.76,267.41,8.74,257.18"><head>Table 6 :</head><label>6</label><figDesc>Recall @ aR+b for the MIMIC II Test Collection.</figDesc><table coords="22,126.74,120.83,301.64,550.34"><row><cell>Topic (R) -Athome1 Collection</cell><cell>100 101 102 103 104 105 106 107 108 109</cell><cell>(4542) (5836) (1624) (5725) (227) (3635) (17135) (2375) (2375) (506)</cell><cell>0.5374 0.7493 0.5800 0.8351 0.6916 0.9224 0.9847 0.8236 0.3091 0.9605</cell><cell>2536 4771 1071 4817 199 3418 17516 2259 746 510</cell><cell>+651* +6841* +1493* +7203* +1091* +674* +2226* +1164* +696* +753*</cell><cell>0.1497 0.8045 0.6108 0.9310 0.7005 0.7431 0.9048 0.3676 0.0720 0.9170</cell><cell>1437 6025 1895 6998 327 3359 17743 1056 171 1396</cell><cell>0.9036 0.9896 0.9206 0.9857 0.8546 0.7508 0.9879 0.9689 0.9390 0.9723</cell><cell>6618 7538 3679 6823 3551 4876 18180 3582 4543 2901</cell><cell>0.7431 0.9467 0.6552 0.9275 0.1674 0.6526 0.9720 0.7937 0.6830 0.9091</cell><cell>10274 9719 3392 8024 2908 7330 22204 5510 6859 2914</cell><cell>0.0638 0.0526 0.1921 0.0552 0.8194 0.0869 0.0184 0.1301 0.1221 0.0059</cell><cell>307 315 315 315 315 315 315 315 308 308</cell><cell>0.0553 0.0526 0.1921 0.0552 0.7885 0.0869 0.0180 0.1309 0.1217 0.0059</cell><cell>314 315 315 315 315 307 315 308 308 303</cell><cell>0.9813 0.9988 0.9581 0.9974 0.9163 0.8856 0.9977 0.9878 0.9764 0.0059</cell><cell>15256 11396 5729 10336 1232 15257 27254 6325 8498 130</cell><cell>0.9830 0.9991 0.9483 0.9976 0.9119 0.8809 0.9978 0.9861 0.9764 0.9802</cell><cell>16810 11396 5188 10336 1105 15257 27254 5729 8498 3144</cell><cell>0.9194 0.9979 0.9360 0.9963 0.9692 0.7590 0.9973 0.9857 0.9389 0.9802</cell><cell>7702 10336 4251 9373 2841 5729 24749 5729 5188 3144</cell><cell>0.7191 0.9639 0.7285 0.7696 0.2819 0.6856 0.9669 0.8863 0.6097 0.0000</cell><cell>5213 7578 1837 4967 231 4044 17786 2938 2554 110</cell><cell>0.0007 0.9627 0.7174 0.7686 0.3172 0.6875 0.9665 0.8842 0.5912 0.0020</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>eDiscoveryTeam</cell><cell></cell><cell cols="2">NINJA</cell><cell cols="2">UvA.ILPS-baseline</cell><cell cols="2">UvA.ILPS-baseline2</cell><cell cols="2">WaterlooClarke-UWPAH1</cell><cell cols="2">WaterlooClarke-UWPAH2</cell><cell cols="2">WaterlooCormack-Knee100</cell><cell cols="2">WaterlooCormack-Knee1000</cell><cell cols="2">WaterlooCormack-stop2399</cell><cell cols="2">Webis-baseline</cell><cell>Webis-keyphrase</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="22,447.54,56.69,44.60,678.62"><head>Table 7 :</head><label>7</label><figDesc>Set-Based Results (Recall Over Effort) For the athome1 Test Collection. The top number in each cell indicates the recall achieved when the submission indicated by "calling its shot" that a "reasonable" result had been achieved, or when the submission terminated its run. The second number indicates the number of documents submitted to the automated assessment server at this point. (*) indicates the number of additional documents reviewed by a manual review team, and not necessarily submitted to the automated assessment server.</figDesc><table coords="23,138.70,128.30,277.73,535.39"><row><cell>Topic (R) -Athome2 Collection</cell><cell>2052 2108 2129 2130 2134 2158 2225 2322 2333 2461</cell><cell>(265) (661) (589) (2299) (252) (1256) (182) (9517) (4805) (179)</cell><cell>0.9698 0.8850 0.9847 0.8534 0.9563 0.9881 0.8956 0.8512 0.8745 0.9777</cell><cell>871 1505 3854 3802 3689 1339 201 12799 6670 1918</cell><cell>+2325* +2101* +94* +285* +19* +1335* +205* +195* +228* +32*</cell><cell>0.8566 0.6702 0.7793 0.4728 0.6191 0.6393 0.9176 0.1567 0.5290 0.6201</cell><cell>516 990 1276 1867 534 850 665 2497 3310 728</cell><cell>0.9962 0.9803 0.9745 0.7747 0.8651 0.9881 0.9561 0.9032 0.9463 0.9888</cell><cell>4651 5038 4657 6363 5141 4651 4652 13969 8224 4653</cell><cell>0.9660 0.9183 0.8693 0.7234 0.6865 0.6226 0.8681 0.8880 0.7832 0.8883</cell><cell>4672 4746 4736 8472 4722 5704 4715 25613 11549 4692</cell><cell>0.8491 0.3858 0.4414 0.0761 0.7183 0.2508 0.8407 0.0302 0.0651 0.6872</cell><cell>307 315 324 308 315 315 308 315 313 320</cell><cell>0.9811 0.9758 0.9847 0.9622 0.9405 0.0892 0.9231 0.9841 0.9881 0.3799</cell><cell>989 3144 3478 20402 2316 130 630 30010 15257 130</cell><cell>0.9849 0.9788 0.9796 0.9604 0.9405 0.9881 0.9560 0.9862 0.9875 0.9385</cell><cell>1104 3144 3144 20402 2316 2841 1105 33042 15257 1372</cell><cell>0.9925 0.9818 0.9830 0.7086 0.9524 0.9881 0.9835 0.8800 0.9534 0.9888</cell><cell>2840 3478 3144 4697 2841 4251 2841 12562 8498 2841</cell><cell>0.1094 0.9047 0.0459 0.0000 0.0159 0.6600 0.1539 0.7376 0.7378 0.5475</cell><cell>173 3002 110 80 111 1457 123 11313 5611 271</cell><cell>0.0868 0.8684 0.0492 0.0000 0.0159 0.6147 0.1044 0.7354 0.7590 0.4078</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>eDiscoveryTeam</cell><cell></cell><cell cols="2">NINJA</cell><cell cols="2">UvA.ILPS-baseline</cell><cell cols="2">UvA.ILPS-baseline2</cell><cell cols="2">WaterlooClarke-UWPAH1</cell><cell cols="2">WaterlooCormack-Knee100</cell><cell cols="2">WaterlooCormack-Knee1000</cell><cell cols="2">WaterlooCormack-stop2399</cell><cell cols="2">Webis-baseline</cell><cell>Webis-keyphrase</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="23,435.58,56.69,44.60,678.62"><head>Table 8 :</head><label>8</label><figDesc>Set-Based Results (Recall Over Effort) For the athome2 Test Collection. The top number in each cell indicates recall achieved when the submission indicated by "calling its shot" that a "reasonable" result had been achieved, or when the submission terminated. The second number indicates the number of documents submitted to the automated assessment server at this point. (*) indicates the number of additional documents reviewed by a manual review team, and not necessarily submitted to the automated assessment server.</figDesc><table coords="24,138.70,135.78,305.62,599.53"><row><cell>Topic (R) -Athome3 Collection</cell><cell>3089 3133 3226 3290 3357 3378 3423 3431 3481 3484</cell><cell>(255) (113) (2094) (26) (629) (66) (76) (1111) (2036) (23)</cell><cell>0.9255 0.7699 0.9842 0.8846 0.9173 0.8939 0.4474 0.9973 0.9646 1.0000</cell><cell>250 97 5347 95 701 106 40 1121 2077 23</cell><cell>+834* +49* +18* +306* +920* +200* +92* +272* +367* +73*</cell><cell>0.7765 0.7699 0.1395 0.8846 0.9603 0.8485 0.4605 0.5995 0.6027 1.0000</cell><cell>201 143 474 92 714 99 45 673 1392 23</cell><cell>0.9961 1.0000 0.9895 1.0000 0.9857 0.9546 0.8290 0.9991 0.9509 1.0000</cell><cell>9024 9024 9031 9024 9100 9024 9936 9024 9026 9024</cell><cell>0.9765 0.9735 0.9231 0.6923 0.9142 0.9546 0.4474 0.9937 0.8144 1.0000</cell><cell>9057 9097 9032 9106 9103 9091 9116 9046 9325 9084</cell><cell>0.8510 0.9912 0.1423 0.7692 0.4706 0.9848 0.4737 0.2844 0.1552 1.0000</cell><cell>314 330 315 309 315 332 309 315 315 330</cell><cell>0.4353 0.9912 0.9790 0.6923 0.9523 0.8636 0.4605 0.9847 0.9514 1.0000</cell><cell>129 343 4251 111 1526 130 151 1232 5188 111</cell><cell>0.9961 0.9912 0.9733 1.0000 0.9634 1.0000 0.5263 0.9892 0.9504 1.0000</cell><cell>1104 1105 3846 1105 1883 1105 1105 1232 4251 1105</cell><cell>0.9961 1.0000 0.9852 1.0000 0.9841 1.0000 0.6184 0.9991 0.9504 1.0000</cell><cell>2840 2566 5188 2566 3144 2566 2566 3846 5188 2566</cell><cell>0.6980 0.7788 0.1777 0.5385 0.8935 0.7879 0.3816 0.9829 0.6842 1.0000</cell><cell>2156 261 942 119 5338 314 130 9971 2215 110</cell><cell>0.8275 0.0620 0.1380 0.2692 0.8347 0.5000 0.1974 0.9748 0.6680 0.8696</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>eDiscoveryTeam</cell><cell></cell><cell cols="2">NINJA</cell><cell cols="2">UvA.ILPS-baseline</cell><cell cols="2">UvA.ILPS-baseline2</cell><cell cols="2">WaterlooClarke-UWPAH1</cell><cell cols="2">WaterlooCormack-Knee100</cell><cell cols="2">WaterlooCormack-Knee1000</cell><cell cols="2">WaterlooCormack-stop2399</cell><cell cols="2">Webis-baseline</cell><cell>Webis-keyphrase</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,71.94,689.07,292.16,6.64"><p>https://web.archive.org/web/20160221072908/http://jebemails.com/home.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,71.94,698.57,82.80,6.64"><p>http://trec-dd.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="2,71.94,708.08,207.47,6.64"><p>http://www.virginiamemory.com/collections/kaine/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="2,71.94,717.58,127.03,6.64"><p>https://physionet.org/mimic2/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="2,71.94,727.09,208.15,6.64"><p>https://en.wikipedia.org/wiki/List of ICD-9 codes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="3,71.94,687.47,160.34,6.99"><p>http://qwone.com/∼jason/20Newsgroups/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="3,71.94,696.97,258.83,6.99"><p>http://www.daviddlewis.com/resources/testcollections/reuters21578/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="3,71.94,707.05,292.16,6.64"><p>https://web.archive.org/web/20160221072908/http://jebemails.com/home.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="4,71.94,701.35,123.04,6.99"><p>http://www.blackhatworld.com/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="4,71.94,710.85,89.44,6.99"><p>http://hackforums.net/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="4,71.94,720.36,251.06,6.99"><p>http://www.virginiamemory.com/collections/kaine/under-the-hood.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="4,71.94,729.86,117.01,6.99"><p>https://physionet.org/mimic2/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We are grateful to the <rs type="institution">Library of Virginia</rs> for affording us access to the <rs type="institution">Kaine email collection</rs> for the Sandbox evaluation. We give particular thanks to <rs type="person">SusanGray Paige</rs>, <rs type="person">Roger Christman</rs>, <rs type="person">Rebecca Morgan</rs>, and <rs type="person">Kathy Jordan</rs> for their invaluable assistance with and unwavering support for this work.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="27,312.05,519.54,8.74,119.94" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="27,312.05,519.54,8.74,119.94">WaterlooCormack-stop2399</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="27,355.35,194.64,7.28,27.67;27,353.89,152.52,8.74,27.67;27,387.76,711.23,8.74,24.08;27,387.76,695.17,8.74,12.73;27,387.76,647.63,8.74,43.09;27,387.76,612.51,8.74,31.80;27,387.76,578.59,8.74,30.58;27,387.76,554.19,8.74,21.07;27,387.76,521.64,8.74,29.22;27,387.76,506.38,8.74,11.93;27,387.76,489.21,8.74,13.84;27,387.76,453.23,8.74,32.66;27,387.76,442.42,8.74,7.47;27,387.76,420.49,8.74,18.60;27,387.76,370.67,8.74,46.49;27,387.76,348.07,8.74,19.26;27,387.76,334.49,8.74,10.24;27,387.76,314.01,8.74,17.16;27,387.76,296.29,8.74,14.39;27,387.76,259.99,8.74,32.96;27,387.76,248.35,8.74,8.30;27,387.76,225.93,8.74,19.09;27,387.76,208.21,8.74,14.39;27,387.76,166.63,8.74,38.25;27,387.76,149.46,8.74,13.84;27,387.76,122.86,8.74,23.27;27,387.76,82.72,8.74,36.80;27,387.76,56.69,8.74,22.69;27,399.72,721.47,8.74,13.84;27,399.72,671.11,8.74,47.21;27,399.72,628.11,8.74,39.85;27,399.72,614.45,8.74,10.51;27,399.72,578.09,8.74,33.21;27,399.72,564.37,8.74,10.57;27,399.72,537.92,8.74,23.30;27,399.72,516.51,8.74,18.27;27,399.72,508.38,8.74,4.98;27,399.72,449.80,8.74,55.43;27,399.72,422.21,8.74,24.44;27,399.72,403.01,8.74,16.05;27,399.72,379.67,8.74,20.19;27,399.72,336.94,8.74,39.58;27,399.72,324.88,8.74,8.88;27,399.72,299.04,8.74,22.69;27,399.72,282.06,8.74,13.84;27,399.72,231.70,8.74,47.21;27,399.72,180.92,8.74,47.63;27,399.72,167.20,8.74,10.57;27,399.72,146.31,8.74,17.74;27,399.72,124.78,8.74,17.16;27,399.72,92.80,8.74,28.84;27,399.72,56.69,8.74,32.96;27,411.67,697.06,8.74,38.25;27,411.67,679.91,8.74,13.84;27,411.67,643.62,8.74,32.96;27,411.67,632.28,8.74,8.03;27,411.67,582.41,8.74,46.55;27,411.67,535.30,8.74,43.78;27,411.67,523.13,8.74,8.86;27,411.67,505.98,8.74,13.84;27,411.67,456.16,8.74,46.49;27,411.67,405.84,8.74,47.00;27,411.67,376.96,8.74,25.57;27,411.67,364.78,8.74,8.86;27,411.67,345.35,8.74,16.11;27,411.67,316.57,8.74,25.46" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="27,387.76,711.23,8.74,24.08;27,387.76,695.17,8.74,12.73;27,387.76,647.63,8.74,43.09;27,387.76,612.51,8.74,31.80;27,387.76,578.59,8.74,30.58;27,387.76,554.19,8.74,21.07;27,387.76,521.64,8.74,29.22;27,387.76,506.38,8.74,11.93;27,387.76,489.21,8.74,13.84;27,387.76,453.23,8.74,32.66;27,387.76,442.42,8.74,7.47;27,387.76,420.49,8.74,18.60;27,387.76,370.67,8.74,46.49;27,387.76,348.07,8.74,19.26;27,387.76,334.49,5.82,10.24;27,387.76,314.01,8.74,17.16;27,387.76,296.29,8.74,14.39;27,387.76,259.99,8.74,32.96;27,387.76,248.35,8.74,8.30;27,387.76,225.93,8.74,19.09;27,387.76,208.21,8.74,14.39;27,387.76,166.63,8.74,38.25;27,387.76,149.46,8.74,13.84;27,387.76,122.86,8.74,23.27;27,387.76,82.72,8.74,36.80;27,387.76,56.69,8.74,22.69;27,399.72,721.47,8.74,13.84;27,399.72,671.11,8.74,47.21;27,399.72,628.11,8.74,39.85;27,399.72,614.45,8.74,10.51;27,399.72,578.09,8.74,33.21;27,399.72,564.37,8.74,10.57;27,399.72,537.92,8.74,23.30;27,399.72,516.51,8.74,18.27;27,399.72,508.38,8.74,4.98;27,399.72,449.80,8.74,55.43;27,399.72,422.21,8.74,24.44;27,399.72,403.01,8.74,16.05;27,399.72,379.67,8.74,20.19;27,399.72,336.94,8.74,39.58;27,399.72,324.88,8.74,8.88;27,399.72,299.04,8.74,22.69;27,399.72,282.06,8.74,13.84;27,399.72,231.70,8.74,47.21;27,399.72,180.92,8.74,47.63;27,399.72,167.20,8.74,10.57;27,399.72,146.31,6.55,17.74">The top number in each cell indicates the recall achieved when the submission indicated by &quot;calling its shot&quot; that a &quot;reasonable&quot; result had been achieved, or when the submission terminated its run</title>
		<idno>8126 0.5069</idno>
		<imprint/>
	</monogr>
	<note>Table 12: Set-Based Results (Recall Over Effort) for the MIMIC II Test Collection, Part II. The second number indicates the number of documents submitted to the automated assessment server at this point</note>
</biblStruct>

<biblStruct coords="28,77.17,286.58,474.33,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="28,247.91,286.58,174.96,8.74">WHU at TREC Total Recall Track 2015</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruixue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,444.27,286.58,75.72,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,306.19,478.14,8.74;28,77.17,318.14,165.68,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="28,298.42,306.19,162.97,8.74">The Grossman-Cormack Glossary of</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="28,466.30,306.19,89.01,8.74;28,77.17,318.14,111.51,8.74">Technology-Assisted Review. Fed. Cts. L. Rev</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,337.75,478.13,8.74;28,77.17,349.71,478.13,8.74;28,77.17,361.66,233.11,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="28,276.19,337.75,279.11,8.74;28,77.17,349.71,62.24,8.74">Multi-faceted recall of continuous active learning for technologyassisted review</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,158.52,349.71,396.78,8.74;28,77.17,361.66,105.20,8.74">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="763" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,381.27,478.13,8.74;28,77.17,393.23,181.55,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="28,290.51,381.27,264.80,8.74;28,77.17,393.23,52.88,8.74">Waterloo (Cormack) Participation in the TREC 2015 Total Recall Track</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,151.49,393.23,75.72,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,412.84,478.13,8.74;28,77.17,424.80,236.26,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="28,247.11,412.84,308.20,8.74;28,77.17,424.80,108.04,8.74">Machine Learning for Information Retrieval: TREC 2009 web, relevance feedback and legal tracks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mona</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mojdeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,206.21,424.80,75.72,8.74">Proc. TREC-2009</title>
		<meeting>TREC-2009</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,444.41,478.13,8.74;28,77.17,456.36,308.30,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="28,421.82,444.41,133.48,8.74;28,77.17,456.36,179.63,8.74">The University of Amsterdam (ILPS) at TREC 2015 Total Recall Track</title>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,278.24,456.36,75.72,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,475.97,478.14,8.74;28,77.17,487.93,80.99,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="28,144.82,475.97,363.45,8.74">Efficient semantic indexing via neural networks with dynamic supervised feedback</title>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Dhand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,532.64,475.97,22.67,8.74;28,77.17,487.93,49.48,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,507.54,478.14,8.74;28,77.17,519.49,253.74,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="28,356.16,507.54,199.15,8.74;28,77.17,519.49,113.31,8.74">The implications of Rule 26(g) on the use of technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Maura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="28,203.83,519.49,72.92,8.74">Fed. Cts. L. Rev</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,539.11,478.13,8.74;28,77.17,551.06,231.08,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="28,450.45,539.11,104.85,8.74;28,77.17,551.06,102.42,8.74">WaterlooClarke: TREC 2015 Total Recall Track</title>
		<author>
			<persName coords=""><forename type="first">Wang</forename><surname>Yipeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wu</forename><surname>Clarke Haotian Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,201.03,551.06,75.72,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,570.67,478.13,8.74;28,77.17,582.63,152.35,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="28,404.26,570.67,151.04,8.74;28,77.17,582.63,23.54,8.74">Overview of the TREC 2009 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,122.30,582.63,75.72,8.74">Proc. TREC-2009</title>
		<meeting>TREC-2009</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,602.24,478.14,8.74;28,77.17,614.19,121.40,8.74" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="28,249.92,602.24,254.92,8.74">Cochrane handbook for systematic reviews of interventions</title>
		<author>
			<persName coords=""><forename type="first">Julian Pt</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sally</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Wiley Online Library</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,633.80,478.13,8.74;28,77.17,645.76,119.68,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="28,336.47,633.80,214.63,8.74">A Constrained Approach to Manual Total Recall</title>
		<author>
			<persName coords=""><forename type="first">Bayu</forename><surname>Hardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Pickens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Gricks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Noel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,89.63,645.76,75.72,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,665.37,478.13,8.74;28,77.17,677.33,107.23,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="28,310.15,665.37,227.00,8.74">e-Discovery Team at TREC 2015 Total Recall Track</title>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Losey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jim</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Reichenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,77.17,677.33,75.72,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,696.94,344.48,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="28,135.69,696.94,157.30,8.74">TUW at the first Total Recall Track</title>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,314.43,696.94,75.72,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,77.17,716.55,478.14,8.74;28,77.17,728.50,317.87,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="28,517.26,716.55,38.05,8.74;28,77.17,728.50,189.29,8.74">Webis at TREC 2015: Tasks and Total Recall Tracks</title>
		<author>
			<persName coords=""><forename type="first">Magdalena</forename><surname>Keil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olaoluwa Anifowose</forename><surname>Amir Othman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Göring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,287.82,728.50,75.72,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,77.17,59.85,478.13,8.74;29,77.17,71.81,22.69,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="29,253.14,59.85,202.55,8.74">Total Recall Track Tools Architecture Overview</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="29,474.94,59.85,74.86,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,77.17,91.73,478.13,8.74;29,77.17,103.69,22.69,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="29,265.13,91.73,185.16,8.74">Trec 2015 dynamic domain track overview</title>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="29,473.63,91.73,76.18,8.74">Proc. TREC-2015</title>
		<meeting>TREC-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
