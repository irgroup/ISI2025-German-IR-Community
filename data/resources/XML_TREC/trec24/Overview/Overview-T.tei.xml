<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,175.06,162.25,260.13,15.69">Overview of the TREC 2015 Tasks Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-11-04">November 4, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.36,200.35,66.29,2.76"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.49,200.35,93.13,2.76"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.44,200.35,73.32,2.76"><forename type="first">Manisha</forename><surname>Verma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,428.60,200.35,18.22,2.76;1,182.97,213.07,48.07,2.76"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.86,213.07,64.12,2.76"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.60,213.07,85.34,2.76"><forename type="first">Rishabh</forename><surname>Mehrotra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,175.06,162.25,260.13,15.69">Overview of the TREC 2015 Tasks Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-11-04">November 4, 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">8B52E13594C83B73D9137A30826C598B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research in Information Retrieval has traditionally focused on serving the best results for a single query, ignoring the reasons (or the task) that might have motivated the user to submit that query. Often times search engines are used to complete complex tasks (information needs); achieving these tasks with current search engines requires users to issue multiple queries. For example, booking travel to a location such as London could require the user to submit various queries such as flights to London, hotels in London, points of interest around London, etc.</p><p>Standard evaluation mechanisms focus on evaluating the quality of a retrieval system in terms of the relevance of the results retrieved, completely ignoring the fact that user satisfaction mainly depends on the usefullness of the system in helping the user complete the actual task that led the user issue the query. The TREC 2015 Tasks Track is an attempt in devising mechanisms for evaluating quality of retrieval systems in terms of (1) how well they can understand the underlying task that led the user submit a query, and (2) how useful they are for helping users complete their tasks.</p><p>In this overview, we first summarise the three categories of evaluation mechanisms used in the track and briefly describe the corpus, topics, and tasks that comprise the test collections. We then give an overview of the runs submitted to the Tasks Track and present evaluation results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation Goals</head><p>The TREC 2015 Tasks Track consists of three evaluation goals: (1) Task understanding, (2), Task completion, and (3) Adhoc. Participants were provided with a set of 50 queries, together with the Freebase ID for each entity in these queries. The same queries were used for each of these tasks and Clueweb12 was used as the main corpus.</p><p>Details of each evaluation goal, as well as the metrics used for each goal are shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Understanding</head><p>The aim of this evaluation goal is to test whether systems can understand the possible tasks users might be trying to achieve given a query. For this goal, the participants were asked to submit key phrases that represent the possible task the user may be trying to achieve given this query.</p><p>For each query, the participants were asked to submit a ranked list of up to 1000 key phrases that represent the set of all tasks a user who submitted the query may be looking for. For example, for the query '"hotels in London", some relevant key phrases can be: "cheap hotels in London", "reviews of hotels in London", "hotels in London city centre", etc. The goal of this task is to return a ranked list of key phrases that provide a complete coverage of tasks for each query, while avoiding redundancy.</p><p>Evaluating the coverage and relevance of the tasks submitted by the participants requires that a set of "gold standard" tasks that cover the set of all possible tasks are identified in advance. These gold standard tasks were constructed by the organizers, but were not be provided to the participants until the evaluation results are out.</p><p>In order to guarantee the coverage of tasks and be fair to all participants, tasks were developed based on information extracted from the logs of a commercial search engine, as well as by pooling the key phrases submitted by the participants. An example set of tasks for the query "hotels in London" may be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• hotels in London [price]</head><p>• hotels in London [location]</p><p>• hotels [reviews] in London</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• [other accommodation] in London</head><p>• hotels [in locations around] London Given the gold standard tasks, each key phrase submitted by the participants were judged with respect to each of the gold standard tasks by using a three level judging scheme:</p><p>• Highly relevant: The key phrase completely describes the task and could be used as a query submitted to the search engine to complete the task.</p><p>• Relevant: The key phrase somehow describes the task but not fully, it can be used as a query to achieve the task but there are better queries than that.</p><p>• Non Relevant: The key phrase is not relevant to the task and cannot be used to complete it.</p><p>In the aforementioned example, the key phrase "cheap hotels in London city centre" would be judgedl be relevant to both "hotels in London [price]" and "hotels in London <ref type="bibr" coords="2,222.73,642.01,35.57,8.69">[location]</ref>.</p><p>Given these judgments, the quality of each ranked list will then be evaluated using diversity metrics such as ERR-IA and ↵-NDCG <ref type="bibr" coords="2,365.34,663.82,9.08,8.69" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Completion</head><p>The aim of this evaluation goal is to test the usefulness of a retrieval system in helping a user achieve a task. Participants were asked to retrieve a ranked list of documents that could be relevant to any task a user may be trying to achieve given a query.</p><p>For each query, the participants are expected to submit a ranked list of up to 1000 documents that could be relevant to any task a user may be trying to achieve given a query. The ranked lists provided by the participants were evaluated in terms of the diversity and relevance of documents they have submitted with respect to the set of possible tasks the user may be trying to achieve given a query.</p><p>Each document submitted by the participants will then be assessed in terms of its usefulness to complete each possible "gold standard" task by using a three level judging scheme:</p><p>• Key: The document is essential towards the completion of the task. The document is enough on its own to complete the task.</p><p>• Useful: The document is useful towards the completion of the task. However, more documents need to be investigated in order to complete the task.</p><p>• Not Useful: The document is not useful towards the completion of the task. This is the firs time judgments in terms of usefulness have been used in evaluating quality of retrieval systems. Hence, for comparison purposes we also obtain relevance judgments by asking NIST assessors to label each document in terms of its relevance to the query. Fort this purpose, we used a four level judging scheme:</p><p>• Highly Relevant: The page contains significant amount of information about the task.</p><p>• Relevant: The content of this page provides some information on the task, which may be minimal.</p><p>• Non Relevant: The content of this page does not provide useful information about the task.</p><p>• Junk: This page does not appear to be useful for any reasonable purpose; it may be spam or junk.</p><p>Given these judgments, similar to Task Understanding, the quality of each ranked list was then evaluated using diversity metrics such as ERR-IA and ↵-NDCG <ref type="bibr" coords="3,179.01,600.62,9.08,8.69" target="#b0">[1]</ref>. The primary judgments that were used in this evaluation category were based on usefulness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adhoc</head><p>For comparison purposes, we continued to have a traditional Web ad-hoc evaluation mechanism this year as well <ref type="bibr" coords="4,295.69,227.69,9.08,8.69" target="#b0">[1]</ref>. Participants were asked to submit a ranked list of up to 1000 documents.</p><p>For evaluating the quality of the runs submitted, the judgments that were obtained for the Task Completion Task were used, ignoring the usefulness category, and focusing on relevance. For the task completion category, a document is judged for each possible task, given a query. For Adhoc, each document is assigned a single relevance value, which is the maximum relevance label assigned for that document over all possible tasks.</p><p>Once these relevance judgments were obtained, ERR and NDCG were used as the primary metrics for evaluation, similar to previous years' Web Track <ref type="bibr" coords="4,449.76,325.85,9.08,8.69" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participants and Runs</head><p>Table <ref type="table" coords="4,173.78,375.66,4.54,8.69" target="#tab_0">1</ref> summarizes the participation in Tasks track. In total, we recieved 21 runs from five groups, consisting of 11 task understanding runs, 6 task completion runs, and 4 adhoc runs. In terms of the corpus, only one group (WHU) used Category B subset of Clueweb12 for all its runs. Remaining groups submitted runs using Category A corpus of Clueweb12.</p><p>The groups participated in the TREC 2015 Tasks Track, as well as the evaluation categories they participated in can be seen below:</p><p>• Microsoft Research (MSR): 1 task understanding run.</p><p>• Carnegie Mellon University (OAQA): 3 task understanding runs.</p><p>• University of Delaware (UDEL): 3 task understanding, 1 task completion runs.</p><p>• Webis Group (WEBIS): 1 task understanding, 3 task completion, and 3 adhoc runs.</p><p>• WHU group (WHU): 3 task understanding, 2 task completion and 1 adhoc runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Understanding Results</head><p>For the Task Understanding evaluation category, depth-20 pools of the key phrases submitted by the participants were constructed and each key phrase was labelled based on the judging scheme described in Section 2.1. Due to the limited judgment budget available at NIST, only 34 topics were judged for Task Understanding. Hence, the evaluation results reported in this section mainly focus on these 34 topics. Given the lables of key phrases, both ↵-NDCG and ERR-IA metrics were computed at rank 20, focusing on ERR-IA at rank 20 as the primary metric. Table <ref type="table" coords="5,162.74,456.97,4.54,8.69" target="#tab_1">2</ref> shows the evaluation results for this category, sorted in terms of decresing ERR-IA values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task Completion Results</head><p>For Task Completion, depth-10 pools of documents were constructed and each document was labelled in terms of usefulness and relevance to each task, based on the judging schemes described in Section 2.2. Similar to the Task Understanding evaluation category, only 35 topics were labelled for Task Completion due to limited judgment resources available at NIST.</p><p>Given the judgments based on usefulness and relevance, both ↵-NDCG and ERR-IA metrics were computed at rank 10, focusing on ERR-IA at rank 10 computed using judgements based on usefulness as the primary metric. Table <ref type="table" coords="5,457.34,586.43,4.54,8.69">3</ref> shows the evaluation results for this category, sorted in terms of decresing ERR-IA values.</p><p>Table <ref type="table" coords="5,187.59,619.15,4.54,8.69" target="#tab_2">4</ref> shows the evaluation results based on judgments in terms of relevance. The ranking of systems when evaluation metrics are computed based on relevance versus usefulness seem to be identical. Figuremetriccomparison shows how the ranking of systems change when evaluation metrics are computed using judgments in terms of usefulness (x axis in the plots) versus using judgments in  terms of relevance (y axis in the plots). As it can be seen in these plots, even though the values of evaluation metrics change when the di↵erent judgments are used, the ranking of the systems based on the two types of judgment seem identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adhoc Retrieval Results</head><p>In order to evaluate the quality of Adhoc Retrieval runs, the judgments obtained for Task Completion were used in the way described in Section 2.3. ERR and NDCG at rank 10 values were then computing, using ERR at rank 10 as the primary metric. Table <ref type="table" coords="6,239.16,493.28,4.54,8.69" target="#tab_3">5</ref> shows the evaluation results for the adhoc runs, sorted in decreasing relevance in terms of the ERR scores. The metric values for the adhoc runs seem quite low. When the evaluation results for runs submitted by the same groups for Task Completion and Adhoc are compared, the evaluation results seem much higher for Task Completion. When the documents retrieved are compared, the runs submitted for Task Completion do not have much overlap with the runs submitted for Adhoc. Hence, the low evaluation values for Adhoc seems to be due to the nature of the runs submitted for this evaluation category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The TREC 2015 Tasks Track was the first attempt in building test collections for evaluating the usefulness of retrieval systems in terms helping people achieve their search tasks. Since this was the first year of the track, the number of participants was not very high. However, for the task understanding and the task In 2016, the TREC Tasks Track will be running one more time. Building upon the findings of the 2015 track, in 2016 we plan to further continue our attempts in devising evaluation mechanisms for meausing the usefulness of a system in helping users achieve a task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,148.33,344.18,313.54,8.69;6,148.33,361.49,36.32,2.29;6,186.94,355.09,274.93,8.69;6,166.72,236.91,129.29,96.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of evaluation results based on (left) ERR-IA, and (right) ↵-NDCG metrics when judgments based on usefulness versus relevance are used.</figDesc><graphic coords="6,166.72,236.91,129.29,96.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,197.59,127.98,212.01,50.86"><head>Table 1 :</head><label>1</label><figDesc>Tasks Track 2015 participation</figDesc><table coords="4,197.59,147.57,212.01,31.26"><row><cell>Task</cell><cell cols="3">Understanding Completion Adhoc</cell></row><row><cell>Groups</cell><cell>5</cell><cell>3</cell><cell>2</cell></row><row><cell>Runs</cell><cell>11</cell><cell>6</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,163.41,127.98,280.37,263.16"><head>Table 2 :</head><label>2</label><figDesc>Task Understanding results</figDesc><table coords="5,163.41,148.33,280.37,242.80"><row><cell>Group</cell><cell>Run</cell><cell cols="3">Category ERR-IA@20 ↵-NDCG@20</cell></row><row><cell>Whu</cell><cell>TOPIC RUN3</cell><cell>B</cell><cell>0.471</cell><cell>0.573</cell></row><row><cell>Udel</cell><cell>udelRun2</cell><cell>A</cell><cell>0.454</cell><cell>0.565</cell></row><row><cell>Webis</cell><cell>webis1</cell><cell>A</cell><cell>0.350</cell><cell>0.453</cell></row><row><cell>Udel</cell><cell>udelRun1</cell><cell>A</cell><cell>0.347</cell><cell>0.404</cell></row><row><cell>Whu</cell><cell>TOPIC RUN2</cell><cell>B</cell><cell>0.336</cell><cell>0.413</cell></row><row><cell>Msr</cell><cell>MSRTasksQUrun3</cell><cell>A</cell><cell>0.303</cell><cell>0.359</cell></row><row><cell>Whu</cell><cell>NP TU</cell><cell>B</cell><cell>0.299</cell><cell>0.375</cell></row><row><cell>Udel</cell><cell>udelTTTUAOL</cell><cell>A</cell><cell>0.269</cell><cell>0.327</cell></row><row><cell>CMU</cell><cell>rsf</cell><cell>A</cell><cell>0.260</cell><cell>0.335</cell></row><row><cell>CMU</cell><cell>lsf</cell><cell>A</cell><cell>0.249</cell><cell>0.324</cell></row><row><cell>CMU</cell><cell>lsfs</cell><cell>A</cell><cell>0.234</cell><cell>0.313</cell></row><row><cell></cell><cell cols="3">Table 3: Task Completion (Usefulness) results</cell><cell></cell></row><row><cell>Group</cell><cell>Run</cell><cell cols="3">Category ERR-IA@10 ↵-NDCG@10</cell></row><row><cell>Udel</cell><cell>udelRun2CSpam</cell><cell>A</cell><cell>0.442</cell><cell>0.518</cell></row><row><cell>Webis</cell><cell>webisC2</cell><cell>A</cell><cell>0.254</cell><cell>0.300</cell></row><row><cell>Whu</cell><cell>TOPIC RUN3 TC</cell><cell>B</cell><cell>0.177</cell><cell>0.210</cell></row><row><cell>Webis</cell><cell>webisC3</cell><cell>A</cell><cell>0.120</cell><cell>0.134</cell></row><row><cell>Webis</cell><cell>webisC1</cell><cell>A</cell><cell>0.096</cell><cell>0.108</cell></row><row><cell>Whu</cell><cell>TOPIC RUN2 TC</cell><cell>B</cell><cell>0.014</cell><cell>0.021</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,163.41,127.98,280.37,96.66"><head>Table 4 :</head><label>4</label><figDesc>Task Completion (Relevance) results</figDesc><table coords="6,163.41,148.33,280.37,76.31"><row><cell>Group</cell><cell>Run</cell><cell cols="3">Category ERR-IA@10 ↵-NDCG@10</cell></row><row><cell>Udel</cell><cell>udelRun2CSpam</cell><cell>A</cell><cell>0.469</cell><cell>0.554</cell></row><row><cell>Webis</cell><cell>webisC2</cell><cell>A</cell><cell>0.278</cell><cell>0.334</cell></row><row><cell>Whu</cell><cell>TOPIC RUN3 TC</cell><cell>B</cell><cell>0.232</cell><cell>0.293</cell></row><row><cell>Webis</cell><cell>webisC3</cell><cell>A</cell><cell>0.126</cell><cell>0.149</cell></row><row><cell>Webis</cell><cell>webisC1</cell><cell>A</cell><cell>0.108</cell><cell>0.122</cell></row><row><cell>Whu</cell><cell>TOPIC RUN2 TC</cell><cell>B</cell><cell>0.024</cell><cell>0.035</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,148.33,127.98,313.54,115.93"><head>Table 5 :</head><label>5</label><figDesc>Adhoc results</figDesc><table coords="7,148.33,148.33,313.54,95.57"><row><cell>Group</cell><cell>Run</cell><cell cols="3">Category ERR@10 NDCG@10</cell></row><row><cell>Whu</cell><cell>NORM RUN1</cell><cell>B</cell><cell>0.124</cell><cell>0.455</cell></row><row><cell>Webis</cell><cell>webisA2</cell><cell>A</cell><cell>0.011</cell><cell>0.024</cell></row><row><cell>Webis</cell><cell>webisA3</cell><cell>A</cell><cell>0.006</cell><cell>0.019</cell></row><row><cell>Webis</cell><cell>webisA1</cell><cell>A</cell><cell>0.001</cell><cell>0.003</cell></row><row><cell cols="5">completion evaluation categories, the submitted systems seem to have achieved</cell></row><row><cell cols="2">reasonable performance.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,162.47,328.80,299.40,8.69;7,162.47,339.71,123.62,8.69;7,289.72,346.10,172.15,2.29;7,162.47,357.01,299.41,2.29;7,162.47,367.92,39.68,2.29;7,205.18,361.52,20.69,8.69" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,413.89,328.80,47.98,8.69;7,162.47,339.71,105.67,8.69">Overview of the TREC 2012 web track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,289.72,346.10,172.15,2.29;7,162.47,357.01,71.02,2.29">Proceedings of The Twenty-First Text REtrieval Conference</title>
		<meeting>The Twenty-First Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-11-06">2012. November 6-9, 2012, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,162.47,379.70,299.40,8.69;7,162.47,390.60,240.12,8.69;7,405.76,397.00,56.11,2.29;7,162.47,407.90,299.40,2.29;7,162.47,418.81,159.60,2.29;7,325.11,412.41,20.69,8.69" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,260.45,390.60,125.26,8.69">TREC 2013 web track overview</title>
		<author>
			<persName coords=""><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charlie</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,405.76,397.00,56.11,2.29;7,162.47,407.90,185.04,2.29">Proceedings of The Twenty-Second Text REtrieval Conference</title>
		<meeting>The Twenty-Second Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-11-19">2013. November 19-22, 2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
