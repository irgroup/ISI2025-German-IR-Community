<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.02,163.71,307.15,17.20;1,273.37,185.63,64.52,17.20">TREC 2015 Temporal Summarization Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.53,218.45,65.24,11.97"><forename type="first">Javed</forename><surname>Aslam</surname></persName>
						</author>
						<author>
							<persName coords="1,234.25,218.45,74.79,11.97"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
						</author>
						<author>
							<persName coords="1,333.52,218.45,133.20,11.97"><forename type="first">Matthew</forename><surname>Ekstrand-Abueg</surname></persName>
						</author>
						<author>
							<persName coords="1,207.56,232.40,99.01,11.97"><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
						</author>
						<author>
							<persName coords="1,341.75,232.40,61.93,11.97"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
						</author>
						<author>
							<persName coords="1,270.15,246.35,70.94,11.97"><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
						</author>
						<title level="a" type="main" coord="1,152.02,163.71,307.15,17.20;1,273.37,185.63,64.52,17.20">TREC 2015 Temporal Summarization Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">985636705B31452D04964DF3A3B5817A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are many summarization scenarios that require updates to be issued to users over time. For example, during unexpected news events such as natural disasters or mass protests new information rapidly emerges. The TREC Temporal Summarization track aims to investigate how to e↵ectively summarize these types of event in real-time. In particular, the goal is to develop systems which can detect useful, new, and timely sentence-length updates about a developing event to return to the user. In contrast to classical summarization challenges (such as DUC or TAC), the summaries produced by the participant systems are evaluated against a ground truth list of information nuggets representing the space of information that a user might want to know about each event. An optimal summary will cover all of the information nuggets in the minimum number of sentences. Also in contrast to classic summarization and newer timeline generation tasks, the Temporal Summarization track focuses on performing this analysis online as documents are indexed.</p><p>For the third 2015 edition of the Temporal Summarization track, we had four main aims. First, to better address the issues with run incompleteness by producing larger run pools and by using pool expansion based on sentence similarity. Second, to lower the barrier to entry for new groups by providing multiple sub-tasks using corpora of varying sizes, allowing groups to pick the task(s) that their infrastructure can cope with. Third, to refine the metrics to better incorporate latency by considering timeliness against the corpus as well as against updates to the Wikipedia page. Finally, to continue to increase the number of events covered by the evaluation. This is the final year of the Temporal Summarization track. For 2016, the track will merge with the Microblog track to become the new Real-Time Summarization (RTS) Track. This new RTS track will still tackle the same challenges as the Temporal Summarization track, but will incorporate microblog streams and will include a new Living-Lab style evaluation in addition to the classical &lt;event&gt; &lt;id&gt;1&lt;/id&gt; &lt;title&gt;2012 Buenos Aires rail disaster&lt;/title&gt; &lt;description&gt;...&lt;/description&gt; &lt;start&gt;1329910380&lt;/start&gt; &lt;end&gt;1330774380&lt;/end&gt; &lt;query&gt;buenos aires train crash&lt;/query&gt; &lt;type&gt;accident&lt;/type&gt; &lt;/event&gt; dataset-based evaluation.</p><p>The remainder of this overview is structured as follows. Section 2 describes the temporal summarization task in detail. In Section 3, we discuss the corpus of documents from which the summaries are produced, while in Section 4, we discuss how temporal summarization systems are evaluated within the track. Section 5 details the process via which sentence updates were assessed. Finally, in Section 6, we summarize the performance of the participant systems to the 2014 track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The aim of this task is to emit a series of sentence updates over time about a named event, given a high volume stream of input documents. In particular, the temporal summarization task focuses on large events with a high impact, such as protests, accidents or natural disasters. Each event is represented by a topic description, providing a textual query representing that event, along with start and end timestamps defining a period of time within which to track that event. An example topic description is illustrated in Figure <ref type="figure" coords="2,395.95,502.56,3.87,9.52" target="#fig_0">1</ref>.</p><p>More precisely, for an event, participant systems process a stream of Web documents from the tracking period as defined in the topic in temporal order. The aim is to select sentences from those documents to emit as updates describing that event. The set of sentences emitted form a summary of that event over time. An optimal summary is one that covers all of the essential information about the event with no redundancy, where each new piece of information was added to the summary as soon as it became available. In contrast to previous years, there are three sub-tasks running in 2015:</p><p>Task 1: Full Filtering and Summarization • Participants will be provided very high-volume streams of news articles and blog posts crawled from the Web for a set of events. Only a very small portion of the stream will be relevant to the event.</p><p>• Each participant will need to process those streams in time order, filter out irrelevant content and then select sentences from those documents to return to the user as updates describing each event over time.</p><p>Task 2: Partial Filtering and Summarization</p><p>• Participants will be provided high-volume streams of news articles and blog posts crawled from the Web for a set of events.</p><p>• Each participant will need to process those streams in time order, filter out irrelevant content and then select sentences from those documents to return to the user as updates describing each event over time.</p><p>Task 3: Summarization Only</p><p>• Participants will be provided low-volume streams of on-topic documents for a set of events.</p><p>• Each participant will need to process those streams in time order selecting sentences from the documents contained within each stream to return the user as updates over time.</p><p>In summary, the sub-task defines the corpus that the participant uses to find sentences to return to the user. Task 1 uses a generic crawl of the Web from the time period of the event, which will require a large amount of filtering. Task 2 uses an automatically filtered Web crawl, that removed documents that are very unlikely to be relevant, but this crawl will still need significant further filtering. Task 3 uses a low-volume set of manually selected documents. For the 2015 task, participants produced temporal summaries for 20 di↵erent events, spanning accidents, natural disasters, storms, shootings and protests. Table <ref type="table" coords="3,472.50,446.54,4.98,9.52">1</ref> summarizes these 20 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus</head><p>The 2015 Temporal Summarization track used documents from the TREC KBA 2014 Stream Corpus. This corpus consists of a set of timestamped documents from a variety of news and social media sources covering the time period October 2011 through April 2013. Each document contains a set of sentences, each with a unique identifier.</p><p>Each event topic defines a subset of the time period covered by this corpus, representing the period to track that event. Participant systems had three options available when working with the corpus, which defines which sub-task they were involved in: 1. Extract the topic time periods from the TREC KBA 2014 Stream Corpus and process all documents from these time periods. Using this approach results in a Task 1 run. This was the only option available to participants during the 2013 track. 2. Use a pre-filtered version of the TREC KBA 2014 Stream Corpus, denoted TREC-TS-2015F, which only contains documents from the 2015 event topic time periods. TREC-TS-2015F was also subject to pre-filtering such that it focuses on documents that are more likely to contain relevant sentences. Using this approach results in a Task 2 run. This option was also available to participants during the 2014 track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Use a manually pre-filtered version of the TREC KBA 2014 Stream</head><p>Corpus, denoted TREC-TS-2015RelOnly, which only contains only documents that were annotated as containing some relevant content from the 2015 event topic time periods. TREC-TS-2015RelOnly is a subset of TREC-TS-2015F. Using this approach results in a Task 3 run. This option was new for 2015.</p><p>Each document within the TREC KBA 2014 Stream Corpus contains zero or more sentences (the sentence boundaries are pre-defined) and a timestamp representing when that document was crawled. Participants return a list of sentences extracted from the KBA corpus documents for each event. Each sentence is identified by the combination of a document identifier (which document the sentence came from) and a sentence identifier (the position of the sentence within the document). Additionally, when a sentence is emitted, the participant system also records the time with respect to the underlying document stream of that emission. If the participant system is making immediate binary emit/ignore decisions on a per sentence basis, then this timestamp will correspond to crawl-time of the document. However, some participant systems opted to delay the emission of sentences to collect more information before issuing updates -in these cases the timestamps recorded reflect the additional latency of these systems.</p><p>Participants were allowed to include runs that use information external to the KBA corpus. The use of external data had the following requirements:</p><p>• External data must have existed before the event start time, or</p><p>• External data must be time-aligned with the KBA corpus and no information after the simulation decision time can be used.</p><p>Similarly, supporting statistical models or auxiliary programs were subject to the same requirements. For example, participants were not to use a statistical model trained on data that existed after the event end time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate runs according to their relevance, coverage, novelty, and latency of the updates.</p><p>• The relevance or precision of the summary with respect to the event topic, i.e. the degree to which the updates within the summary are on-topic and novel. This is measured by the (normalized) Expected Gain metric (nEG(S)).</p><p>• The coverage of the summary with respect to all of the essential information that could have been retrieved for the event. This is measured by the Comprehensiveness metric (C(S)).</p><p>• The degree to which the information contained within the updates is outdated. This is measured by the Expected Latency metric (E[Latency]).</p><p>We also report the performance of all of the participant runs under a combined measure (that incorporates Expected Gain, Comprehensiveness and Expected Latency), i.e. the Harmonic Mean of normalized Expected Latency Gain (EG ⌧ (S)) and Latency Comprehensiveness (C ⌧ (S)), denoted H. This is the official target metric for the 2015 task. Detailed descriptions of metrics and how they are calculated can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Judging</head><p>The evaluation process occurred in two phases:  The first phase defined the space of relevant information for the queries. In particular, this involves the creation of a set of 'information nuggets' about each event that represent all of of the essential information that a good summary should contain. This phase also associates each information nugget with a timestamp representing approximately when that information became public knowledge. The second phase generates a matching between updates provided by the participants to the information nuggets. It is this matching that forms the basis for evaluating a system's accuracy and coverage. A detailed description of these phases of judging can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We present an overview of the performance of the participant systems (runs) in Tables 2 (Task 1), 3 (Task 2), and 4 (Task 3). The last column of the tables reports the H of each participant run and the TREC average. Due to per-task normalization, metric values across tasks are not comparable.</p><p>Only two teams participated in Task 1 due to the overhead involved in processing the full KBA corpus. Although the ranking of cunlp is consistent with its position in Task 2, we note its expected gain is below average so the performance of the run comes from its strong comprehensiveness and lower latency. The seven teams participating in Task 2 exhibited a range of performance, with even the three above-average teams representing high gain (WaterlooClarke) and high comprehensiveness (cunlp, IRIT). In fact, the comprehensiveness of the high gain runs was below average, emphasizing the impressive magnitude of gains from these runs. The tradeo↵ between gain and comprehensiveness can be visualized in Figure <ref type="figure" coords="6,257.59,581.29,3.87,9.52" target="#fig_2">2</ref>. Regardless of whether they focused on gain or comprehensiveness, top performing runs also consistently had better than average latency. Runs in the Task 3 exhibited a similar tradeo↵ between gain and comprehensiveness although, in this case, the performance in di↵erent regimes was more pronounced.</p><p>Although we normalized metric values per task, when we normalized the values across tasks, we observed a similar ordering of systems. This suggests that, although sampling the corpus removes the ability to match certain nuggets,  on average, the e↵ect was not substantial. We plan on developing additional robustness checks in further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In general, the runs submitted to the 2015 track either had a fairly high precision or novelty with topic coverage, but it appears that it was di cult for systems to do both. From the scale of the results, it appears that attaining high precision is more di cult than achieving recall for this task, and hence it is here that further research is needed. Because of the similarity in experiment design, we recommend participants continue studies in the TREC 2016 Real-Time Summarization Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Metrics</head><p>To evaluate the performance of the summaries produced by participant systems, we define the concept of explicit sub-events or 'nuggets', each with a precise timestamp and text describing the sub-event. An e↵ective summary should cover as many of these nuggets as possible, while minimizing redundancy.</p><p>A sentence update is a timestamped short text string. We generally denote an update as the pair (string, time): u = (u.string, u.t). For example u = ("The hurricane was upgraded to category 4", 1330169580) represents an update describing the hurricane category, now 4, pushed out by system S at UNIX time 1330169580 (i.e. 1330169580 seconds after 0:00 UTC on January 1, 1970). In this year's evaluation, the update string is chosen from the set of segmented sentences in the corpus as defined in the guidelines.</p><p>Two updates are semantically comparable using a text similarity measure or a manual annotation process applied to their string components; if two updates u and u 0 refer to the same information (semantically matching), then we write this as u ⇡ u 0 , irrespective of their timestamps. Because two systems might deliver the same update string at di↵erent times, it is generally not the case that u.string = u 0</p><p>.string implies u.t = u 0 .t. Given an event, our manual annotation process generates a set of gold standard updates called nuggets, extracted from wikipedia event pages and timestamped according to the revision history of the page. Editorial guidelines recommend that nuggets be a very short sentence, including only a single sub-event, fact, location, date, etc, associated with topic relevance. We refer to the canonical set of updates as N . This manual annotation process is retrospective and subject to error in the precision of the timestamp. As a result we might encounter situations where the timestamp of the nugget is later than the earliest matching update.</p><p>In response to an event's news coverage, a system/run broadcasts a set of timestamped updates generated in the manner described in the Guidelines. We refer to a system's set of updates as S. The set of updates received before time ⌧ is,</p><formula xml:id="formula_0" coords="10,256.60,148.96,220.87,17.29">S ⌧ = {u 2 S : u.t &lt; ⌧ } (1)</formula><p>Our goal in this evaluation is to measure the precision, recall, timeliness, and novelty of updates provided by a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Preliminaries</head><p>Our evaluation metrics are based on the following auxiliary functions.</p><p>• Nugget Relevance. Each nugget n 2 N has an associated relevance/importance grade,</p><formula xml:id="formula_1" coords="10,287.10,282.91,190.38,17.29">R : N ! [0, 1] (2)</formula><p>R(n) measures the importance of the content (information) in the nugget.</p><p>Nugget importance was provided on a 0-3 scale by assessors (no importance to high importance). For graded relevance, we normalize on an exponential scale, since high importance nuggets are described as "of key importance to the query", whereas low importance nuggets are "of any importance to the query". When binary relevance is needed, everything of any relevance is relevant (0 is the only non-relevant grade). The actual relevance functions used are presented below; n.i denotes the nugget importance as assigned by the assessor.</p><formula xml:id="formula_2" coords="10,196.12,428.59,56.73,10.71">R graded (n) =</formula><p>e n.i e max n 0 2N n 0 .i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graded relevance</head><p>(3)</p><formula xml:id="formula_3" coords="10,197.19,440.68,280.27,33.91">R binary (n) = ( 1 i↵ n.i &gt; 0 0 otherwise Binary relevance<label>(4)</label></formula><p>Note that for graded relevance, returning exactly the nugget set as the system output updates and nothing more ("perfect system"), would usually not result in an expected gain of 1. However, using binary relevance, the perfect system would score an expected gain of 1.</p><p>The relevance can be discounted in time or in size, hence the following discounting functions.</p><p>• Latency Discount. Given a reference timestamp of a matching nugget, t ⇤ , a latency penalty function L(t ⇤ , t) is a monotonically decreasing function of t t ⇤ . A system may return an update matching Wikipedia information before the Wikipedia information exists; thus we use a function that is smooth and decays on both the positive and negative sides.</p><p>The actual function used is presented below with arguments the nugget Wikipedia time (wiki-edit timestamp) n.t, and the update time u.t as indicated by the system. Current parameters allow the latency discount factor to vary from 0 to 2 (1 means nugget time equal to update time), and flattens at around one day(± 24 hours). Note that as a result, gain and expected gain can be greater than 1.</p><formula xml:id="formula_4" coords="11,176.02,145.11,301.46,23.10">L(n.t, u.t) = 1 2 ⇡ arctan( u.t n.t ↵ ) latency-discount<label>(5)</label></formula><p>• Verbosity Normalization. The task definition assumes that a user receives a stream of updates from the system. Consequently, we want to penalize systems for including unreasonably long updates, since these easily lead to significantly higher reading e↵ort. The verbosity can be defined as a string length penalty function, monotonically increasing in the number of words of the update string. We will refer to this normalization function as V(u).</p><p>For the actual verbosity implementation, we approximate the number of extra nuggets worth of information in a given update. This is done by finding all text which did not match a nugget (as defined by the assessors), and dividing the number of words in the text by the average number of words in a nugget for that query.</p><formula xml:id="formula_5" coords="11,201.29,543.57,276.18,59.15">V(u) = 1 + |all words u | |nuggetmatching words u | AV G n |words n | (7) = 1 + |u| | [ n2M 1 (u,S) M(n, S)| avg n2N |n| (8)</formula><p>where |u|, |n| are the length (in number of words) of the update u, and nugget n.</p><p>Note that if an update has all its words being part of some match to a nugget, the verbosity is V (u)=1; otherwise V (u) 1 is an approximation of the "extra non-matching words" in terms of equivalent number of nuggets.</p><p>• Update-Nugget Matching. We also define a key earliest matching function between a nugget and an update set, M(n, S) = argmin {u2S:n⇡u} u.t</p><p>or ; if there is no matching update for n. M(n, S) should be interpreted as "given n, the earliest matching update in S."</p><p>We also define the set of nuggets for which u is the earliest matching update as,</p><formula xml:id="formula_7" coords="12,237.89,244.60,239.58,17.29">M 1 (u, S) = {n 2 N : M(n, S) = u}<label>(10)</label></formula><p>Note that an update can be the earliest matching update for more than one nugget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Metrics</head><p>Using the previously defined notion of relevance, latency, verbosity, and matching we can define several measures of interest for Temporal Summarization.</p><p>Given an update u and a matching nugget n (i.e. u ⇡ n), we can define the discounted gain as,</p><formula xml:id="formula_8" coords="12,227.80,382.54,249.67,17.29">g(u, n) = R(n) ⇥ discounting factor<label>(11)</label></formula><p>Given the previously defined discounts, we have the following family of discounted gains,</p><formula xml:id="formula_9" coords="12,171.86,438.58,305.61,31.98">g F (u, n) = R(n) discount-free gain (12) g L (u, n) = R(n) ⇥ L(n.t, u.t)</formula><p>latency-discounted gain (13)</p><p>Since an update can be the earliest to match several nuggets (u ⇡ n), we define the gain of an update with respect to a system (or participant run) S as the sum of [latency-discounted] relevance of the nuggets for which it is the earliest matching update:</p><formula xml:id="formula_10" coords="12,243.07,525.83,234.40,30.50">G(u, S) = X n2M 1 (u,S) g(u, n)<label>(14)</label></formula><p>where the gain can be either of the discounted gains described earlier. Note that for an appropriate discounting function, G(u, S) 2 [0, 1], although for the latency-discounted gain, given the imperfect nature of model timestamps,</p><formula xml:id="formula_11" coords="12,133.77,603.35,74.42,17.29">G L (u, S) 2 [0, 2].</formula><p>One way to evaluate a system is to measure the expected gain for a system update. This is similar to traditional notions of precision in information retrieval evaluation. Over a large population of system updates, we can estimate this measure reliably. The computation of the expected update gain for system S by time ⌧ is the average of the gain per update:</p><formula xml:id="formula_12" coords="13,204.83,156.11,272.65,98.21">nEG(S) = 1 Z|S| X u2S G(u, S) (15) = 1 Z|S| X u2S X n2M 1 (u,S) g(u, n) = 1 Z|S| X {n2N :M(n,S)6 =;} g(M(n, S), n) (<label>16</label></formula><formula xml:id="formula_13" coords="13,473.04,228.29,4.43,9.52">)</formula><p>where Z is the maximum obtainable expected gain per topic (similar to DCG normalization. Additionally, we may penalize "verbosity" by normalizing not by the number of system updates, but by the overall verbosity of the system</p><formula xml:id="formula_14" coords="13,180.89,301.80,296.58,35.30">nEG V (S) = 1 P u2S V(u) 1 Z X {n2N :M(n,S)6 =;} g(M(n, S), n)<label>(17)</label></formula><p>Our definition of g is such that it:</p><p>• does not penalize for large a update matching several nuggets, as opposed to a few small updates each matching a nugget, due to verbosity weighting,</p><p>• penalizes for late updates (against matched nugget reference timestamp), and</p><p>• penalizes "verbosity" of updates text not matching any nuggets.</p><p>Furthermore, we have that G(u, S ⌧ ) 2 [0, 1] if all update timestamps are at or after matching model timestamps. Over a set of events, the mean expected gain is defined as,</p><formula xml:id="formula_15" coords="13,251.94,486.76,221.10,32.70">MEG = 1 |E| X ✏2E EG(S ✏ ) (<label>18</label></formula><formula xml:id="formula_16" coords="13,473.04,496.03,4.43,9.52">)</formula><p>where E is the set of evaluation events and S ✏ is the system submission for event</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>✏.</head><p>Because a user interest may be concentrated immediately after an event and because a system's performance (in terms of gain) may be dependent on the time after an event, we will also consider a time-sensitive expected gain for the first ⌧ seconds,</p><formula xml:id="formula_17" coords="13,263.31,607.86,214.15,10.71">EG ⌧ (S) = EG(S ⌧ )<label>(19)</label></formula><p>with MEG ⌧ defined similarly.</p><p>In addition to good expected gain, we are interested in a system providing a comprehensive set of updates. That is, we would like the system to cover as many nuggets as possible. This is similar to traditional notions of recall in information retrieval evaluation. Given a set of system updates, S, we define the comprehensiveness (and latency-comprehensiveness) of the system as:</p><formula xml:id="formula_18" coords="14,195.43,166.45,282.04,94.54">C(S) = 1 P n2N R(n) X {n2N :M(n,S)6 =;} g(M(n, S), n) (20) = 1 P n2N R(n) X u2S X n2M 1 (u,S) g(u, n) = 1 P n2N R(n) X u2S G(u, S)<label>(21)</label></formula><p>We also define a time-sensitive notion of comprehensiveness,</p><formula xml:id="formula_19" coords="14,271.57,292.99,205.90,10.71">C ⌧ (S) = C(S ⌧ )<label>(22)</label></formula><p>with an aggregated measure defined as,</p><formula xml:id="formula_20" coords="14,276.90,330.17,200.57,31.22">Z te ts C ⌧ (S)d⌧<label>(23)</label></formula><p>which measures how quickly a system captures nuggets.</p><p>In order to summarize expected gain and comprehensiveness, we use an F measure as our primary metric based on these values,</p><formula xml:id="formula_21" coords="14,250.21,414.98,227.26,24.53">F(S) = EG V (S) ⇥ C(S) EG V (S) + C(S)<label>(24)</label></formula><p>B Judging</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Gold Nugget Extraction</head><p>In this first phase, assessors were asked to read all edits of the Wikipedia article for each query, manually extracting text perceived as relevant and novel for that edit. Additionally, assessors assigned an importance grade to every text fragment, or nugget. An example portion of the extraction interface can be seen in Figure <ref type="figure" coords="14,176.83,547.71,3.87,9.52" target="#fig_4">3</ref>. In order to simplify later matching, assessors were told to extract nuggets such that they were atomic pieces of information relevant to the query. Unlike in previous years, no dependency extractions were performed, as we found it su cient in previous years to simply allow splitting of nuggets during the matching phase and to remove the notion of dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Update-Nugget Matching</head><p>Once submissions were received, we performed a variant of depth-pooling in order to sample updates for evaluation. We sampled the top approximately 60  updates per query and run as sorted by the provided confidence scores (highest first). Additionally, we performed near-duplicate detection among update text to increase the covered set. This resulted in sampled update counts as per Table <ref type="table" coords="15,161.05,499.75,3.87,9.52">1</ref>. One note here is that not all runs contained 60 updates per query; for the run-query pairs with less than 60 updates, all updates were sampled.</p><p>The sampled updates were presented in an interface similar to the one for extraction. Assessors examined and matched updates to nuggets by selecting portions of updates which matched a given nugget, as nuggets are atomic but updates are not. An assessor was allowed to break a nugget into two or more new nuggets to improve atomicity if desired. Note that a nugget may match multiple updates, and an update may match multiple nuggets. An example view of the matching interface can be seen in Figure <ref type="figure" coords="15,342.68,595.39,3.87,9.52" target="#fig_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Automatic matches for unpooled updates</head><p>The participant updates that did not make it to the pool for manual matching form the set of "unpooled updates". We performed an automatic exact match between these unpooled updates and the known relevant pooled updates (man-ually matched); the updates that matched a known relevant pooled update are also considered relevant and included as matching nuggets for evaluation purposes. All updates, both pooled and unpooled, that do not match any nugget (manual) or other relevant update (automatic), are considered nonrelevant for evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,133.77,250.83,343.70,9.52;2,133.77,262.78,26.64,9.52"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example topic description for the topic '2012 Buenos Aires Rail Disaster'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,140.85,645.92,145.35,9.52;5,141.36,665.71,128.28,9.52"><head></head><label></label><figDesc>(a) Gold Nugget Extraction, and (b) Update-Nugget Matching</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,133.77,644.22,343.71,9.52;8,133.77,656.17,36.67,9.52"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Participant run plot of (normalized) Expected Gain vs. Comprehensiveness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="15,133.77,275.81,343.71,9.52;15,133.77,287.76,71.18,9.52;15,133.77,124.80,343.70,136.12"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Extraction interface used by assessors to extract nuggets from Wikipedia edits.</figDesc><graphic coords="15,133.77,124.80,343.70,136.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="15,135.10,432.06,341.05,9.52;15,133.77,308.68,343.71,108.49"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Matching interface used by assessors to match updates and nuggets.</figDesc><graphic coords="15,133.77,308.68,343.71,108.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,133.77,134.65,343.71,116.87"><head>Table 2 :</head><label>2</label><figDesc>Performance on task 1. Performance for systems summarizing the entire document stream, without using any of the filtered set. Runs sorted by H, the harmonic mean of latency gain and latency comprehensiveness.</figDesc><table coords="6,154.53,169.17,302.19,82.34"><row><cell cols="2">TeamID RunID cunlp 2LtoSnofltr20</cell><cell>nEG(S) 0.1224</cell><cell cols="2">C(S) E[Latency] 0.4691 0.8086</cell><cell>H 0.1531</cell></row><row><cell>CWI</cell><cell>IGnPrecision</cell><cell>0.1894</cell><cell>0.4678</cell><cell>0.6273</cell><cell>0.1396</cell></row><row><cell>Mean</cell><cell></cell><cell cols="2">0.1533 0.4575</cell><cell cols="2">0.6507 0.1279</cell></row><row><cell>CWI</cell><cell>IGn</cell><cell>0.1620</cell><cell>0.5137</cell><cell>0.6538</cell><cell>0.1248</cell></row><row><cell>CWI</cell><cell>docs</cell><cell>0.1242</cell><cell>0.4680</cell><cell>0.6658</cell><cell>0.1222</cell></row><row><cell>CWI</cell><cell>titles</cell><cell>0.1915</cell><cell>0.3107</cell><cell>0.5171</cell><cell>0.1150</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,133.77,243.10,357.76,320.10"><head>Table 3 :</head><label>3</label><figDesc>Performance on task 2. Performance for systems summarizing TREC-TS-2015F. Runs sorted by H, the harmonic mean of latency gain and latency comprehensiveness.</figDesc><table coords="7,139.75,277.62,351.78,285.58"><row><cell cols="2">TeamID WaterlooClarke UWCTSRun1 RunID</cell><cell>nEG(S) 0.2350</cell><cell cols="2">C(S) E[Latency] 0.3520 0.6612</cell><cell>H 0.1762</cell></row><row><cell cols="2">WaterlooClarke UWCTSRun3</cell><cell>0.2252</cell><cell>0.3421</cell><cell>0.6643</cell><cell>0.1718</cell></row><row><cell cols="2">WaterlooClarke UWCTSRun2</cell><cell>0.2872</cell><cell>0.2584</cell><cell>0.6551</cell><cell>0.1710</cell></row><row><cell>cunlp</cell><cell>3LtoSfltr5</cell><cell>0.1371</cell><cell>0.4870</cell><cell>0.6392</cell><cell>0.1282</cell></row><row><cell>cunlp</cell><cell>1LtoSfltr20</cell><cell>0.1203</cell><cell>0.5372</cell><cell>0.6287</cell><cell>0.1100</cell></row><row><cell>IRIT</cell><cell>FS1A</cell><cell>0.0849</cell><cell>0.4959</cell><cell>0.6051</cell><cell>0.0719</cell></row><row><cell>cunlp</cell><cell>4APSAL</cell><cell>0.1011</cell><cell>0.4584</cell><cell>0.5108</cell><cell>0.0674</cell></row><row><cell>Mean</cell><cell></cell><cell cols="2">0.0666 0.4342</cell><cell cols="2">0.4697 0.0499</cell></row><row><cell>IRIT</cell><cell>FS2A</cell><cell>0.0518</cell><cell>0.5899</cell><cell>0.6285</cell><cell>0.0476</cell></row><row><cell>BJUT</cell><cell>DMSL1NMF2</cell><cell>0.0445</cell><cell>0.6123</cell><cell>0.4539</cell><cell>0.0354</cell></row><row><cell>BJUT</cell><cell>DMSL1AP1</cell><cell>0.0413</cell><cell>0.6155</cell><cell>0.4701</cell><cell>0.0338</cell></row><row><cell>l3sattrec15</cell><cell>l3sattrec15run1</cell><cell>0.0408</cell><cell>0.3612</cell><cell>0.3743</cell><cell>0.0268</cell></row><row><cell>l3sattrec15</cell><cell>l3sattrec15run3</cell><cell>0.0400</cell><cell>0.3669</cell><cell>0.3712</cell><cell>0.0262</cell></row><row><cell>IRIT</cell><cell>FS1B</cell><cell>0.0422</cell><cell>0.2939</cell><cell>0.3913</cell><cell>0.0259</cell></row><row><cell>IRIT</cell><cell>FS2B</cell><cell>0.0306</cell><cell>0.3391</cell><cell>0.4491</cell><cell>0.0239</cell></row><row><cell>USI</cell><cell>InL2DecrQE1ID1</cell><cell>0.0182</cell><cell>0.5713</cell><cell>0.5806</cell><cell>0.0196</cell></row><row><cell>USI</cell><cell>InL2DecrQE2ID2</cell><cell>0.0169</cell><cell>0.5758</cell><cell>0.5836</cell><cell>0.0184</cell></row><row><cell>udel fang</cell><cell>WikiOnlyFS2</cell><cell>0.0206</cell><cell>0.5819</cell><cell>0.4600</cell><cell>0.0176</cell></row><row><cell>udel fang</cell><cell>ProfOnlyFS3</cell><cell>0.0258</cell><cell>0.5294</cell><cell>0.4122</cell><cell>0.0174</cell></row><row><cell>USI</cell><cell>InL2StabQE2ID3</cell><cell>0.0171</cell><cell>0.6133</cell><cell>0.5238</cell><cell>0.0170</cell></row><row><cell>udel fang</cell><cell>WikiProfMixFS1</cell><cell>0.0189</cell><cell>0.5965</cell><cell>0.4660</cell><cell>0.0166</cell></row><row><cell>l3sattrec15</cell><cell>l3sattrec15run2</cell><cell>0.0283</cell><cell>0.2276</cell><cell>0.2560</cell><cell>0.0164</cell></row><row><cell>USI</cell><cell>InL2IncrQE2ID4</cell><cell>0.0179</cell><cell>0.5837</cell><cell>0.2888</cell><cell>0.0108</cell></row></table><note coords="8,411.91,455.77,11.35,7.82;8,400.45,464.64,15.45,5.18"><p>f ang uogTr</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
