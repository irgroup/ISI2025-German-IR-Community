<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,63.28,72.10,483.15,17.20">Overview of the TREC 2015 Contextual Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,109.37,117.80,87.22,11.46"><forename type="first">Adriel</forename><surname>Dean-Hall</surname></persName>
						</author>
						<author>
							<persName coords="1,244.53,117.80,107.07,11.46"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
						</author>
						<author>
							<persName coords="1,416.84,117.80,66.22,11.46"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
						</author>
						<author>
							<persName coords="1,209.52,147.36,72.18,11.46"><forename type="first">Julia</forename><surname>Kiseleva</surname></persName>
						</author>
						<author>
							<persName coords="1,378.23,147.36,80.13,11.46"><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,63.28,72.10,483.15,17.20">Overview of the TREC 2015 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B2414C8D64D3033607ED3A5BF286D011</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The TREC Contextual Suggestion Track evaluates pointof-interest (POI) recommendation systems, with the goal of creating open and reusable test collections for this purpose. The track imagines a traveler in a unknown city seeking sites to see and things to do that reflect his or her own personal interests, as inferred from their interests in their home city. Given a user's profile, consisting of a POI list and rating from a home city, participants make recommendations for attractions in a target city (i.e., a new context).</p><p>For example, imagine a group of information retrieval researchers with a November evening to spend in beautiful Gaithersburg, Maryland. A contextual suggestion system might recommend a beer at the Dogfish Head Alehouse 1 , dinner at the Flaming Pit 2 , or even a trip into Washington on the metro to see the National Mall 3 . This is the fourth year that the track has operated (since TREC 2012). If you are familiar with the track from previous years, here are the big changes this year:</p><p>• The track moved from the open web to a fixed set of documents.</p><p>• The track was split into two tasks:</p><p>1. A live task, in which participants set up a server and were sent requests over a period of about three weeks.</p><p>2. A batch task, which was similar to the task run in previous years.</p><p>The live task reflects the track's long term goal of creating a "living lab" service for POI recommendation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TASK OVERVIEW</head><p>For both tasks participants were asked to develop a system that is able to make suggestions for a particular person (based upon their profile) with respect to a particular geographic context (i.e., the target city). As input to the task, participating research groups were given a set of profiles, a set of example suggestions, and a set of contexts.</p><p>Each profile corresponded to a single user, indicating that user's preference with respect to each example suggestion, while each context represented a target city that the user might visit. For each profile/context pairing, participating researchers were required to return a ranked list of 50 proposed suggestions. Each suggestion was expected to be be appropriate to the profile (based on the user's preferences) and the context (according to the target city).</p><p>Profiles correspond to the stated preferences of real individuals, recruited through crowdsourcing. These crowdsourced workers first judged example attractions in seed locations, representing their home cities, later returning to judge suggestions proposed by the participating research groups for various target cities. The live and batch tasks differ primarily through the way in which systems interacted with users, with live participants providing a online server to respond to new profile/contexts on demand.</p><p>Details of what the profiles and contexts contain are given on the track homepage 4 . For example, one suggestion might be to have a beer at the Dogfish Head Alehouse, and the profile might include a negative preference with respect to this suggestion. Each training suggestion includes a title, description, and an associated URL. Each context corresponds to a particular geographical location (a city). For example, the context might be Gaithersburg, Maryland. Participants returned results either by setting up a server and participating in the live experiment, or by submitting during the batch experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">COLLECTION</head><p>Early in 2015, a few volunteers crawled the web for documents that they considered to represent points-of-interest in selected target locations and submitted their set of documents to us. We merged the documents, and after some cleaning and de-duping, these POIs formed the based for the collection used in both tracks. Only documents from this collection could be returned as suggestions from either track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TASK DESCRIPTIONS 4.1 Live Task</head><p>For a few weeks, participants in the live task registered services with us. They were periodically sent requests for suggestions. These requests included the city the request was being made for and details on the person making the requests so that responses could be personalized. Responses from participants were made up of an ordered list of suggestions taken from the collection (and from the correct city).</p><p>The first request made to services contained no personal assessor information and only a location. This first set of suggestions was then shown to assessors. The ratings assessors gave for this first set of suggestions were then sent to services for further requests. Ratings from further requests were again sent to services during multiple rounds of suggestion requests and assessments. During this process suggestions from multiple services were combined into a single list then shown to assessors.</p><p>These assessors were recruited from Mechanical Turk (MT).</p><p>For the first round of assessment, workers were recruited from the general MT pool. For additional rounds of assessment, workers were sent messages through MT asking them to return to complete additional assessment tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Batch Task</head><p>For the batch task requests made during the live tasks were sampled. Any POIs that were rated for that requests were also included in the request with the ratings stripped out as a set of candidate suggestions. For participants in the batch task only these candidates were allowed to be made as suggestions (instead of all points-of-interest in the collection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>Preliminary results for the batch task are shown in Figure <ref type="figure" coords="2,285.75,433.73,3.58,8.97">1</ref>; preliminary results for the live task are shown in Figure <ref type="figure" coords="2,285.75,444.18,3.58,8.97">2</ref>. As in previous years, precision@5 provides the primary evaluation measure, and runs are sorted by the measure. The tables also show mean reciprocal rank (MRR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">FINAL REMARKS</head><p>This report provides an preliminary outline of track activities and results from TREC 2015. Six groups submitted a total of nine runs to the live track, demonstrating the feasibility of our online approach to evaluation. By using a fixed test collection, instead of allowing submissions from the open web, we hope to improve the reusability of the collection when compared to previous years. As we continue to analyze track results, we will examine our success with respect to this goal. The track continues for TREC 2016, where we hope to improve our methods for online evaluation, and continue to build a reusable collection. </p></div>		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
