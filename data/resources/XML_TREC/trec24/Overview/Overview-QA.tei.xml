<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.73,164.85,299.77,15.11">Overview of the TREC 2015 LiveQA Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.11,197.33,89.58,10.48"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
							<email>eugene@math.emory.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Emory University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.56,197.33,71.37,10.48"><forename type="first">David</forename><surname>Carmel</surname></persName>
							<email>dcarmel@yahoo-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Yahoo Labs</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.82,197.33,78.85,10.48"><forename type="first">Donna</forename><surname>Harman</surname></persName>
							<email>donna.harman@nist.gov</email>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Standards and Technology</orgName>
								<address>
									<settlement>Gaithersburg</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.55,197.33,55.62,10.48"><forename type="first">Dan</forename><surname>Pelleg</surname></persName>
							<email>pellegd@acm.org</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Yahoo Labs</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.99,211.28,64.55,10.48"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
							<email>yuvalp@yahoo-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Yahoo Labs</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.73,164.85,299.77,15.11">Overview of the TREC 2015 LiveQA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1442D11B397434C57A29FC6C06B43946</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The automated question answering (QA) track, one of the most popular tracks in TREC for many years, has focused on the task of providing automatic answers for human questions. The track primarily dealt with factual questions, and the answers provided by participants were extracted from a corpus of News articles. While the task evolved to model increasingly realistic information needs, addressing question series, list questions, and even interactive feedback, a major limitation remained: the questions did not directly come from real users, in real time.</p><p>The LiveQA track, conducted for the first time this year, focused on realtime question answering for real-user questions. Real user questions, i.e., fresh questions submitted on the Yahoo Answers (YA) site that have not yet been answered, were sent to the participant systems, which provided an answer in real time. Returned answers were judged by TREC editors on a 4-level Likert scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Yahoo Answers Questions</head><p>In contrast to factoid questions used in previous QA tracks, Yahoo Answers (YA) questions are much more diverse, including opinion, advice, polls, and many other question types, thus making the task far more realistic and challenging.</p><p>The YA questions for submission were sampled from the stream of new questions arriving to YA site, immediately upon arrival. The questions were extracted and submitted to the registered participants during a time period of 24 hours starting August 31, 2015. The questions passed a shallow automatic filtering process (spam, adult, non-English, etc.) before submission to participants. Each submitted question included a YA id (called qid), the question title, the body (if any), and the category of the question, as tagged by the question's asker. The question categories, selected from the YA taxonomy, and announced in advance to participants, were:</p><formula xml:id="formula_0" coords="2,148.71,376.18,106.94,148.23">• Arts &amp; Humanities • Beauty &amp; Style • Computers &amp; Internet • Health • Home &amp; Garden • Pets • Sports • Travel</formula><p>The distribution of categories is presented in Figure <ref type="figure" coords="2,363.66,535.60,3.87,8.74" target="#fig_0">1</ref>.</p><p>Here are a few examples of the questions submitted:</p><p>1. Can lazy eyes fix themselves? My right eye points all the way to the left unless I wear glasses. I wanted to get surgery because this lowers my confidence a great deal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participant Answers</head><p>Each registered participant provided a Web service that gets, as input, a YA question and responds with an answer. The testing system, developed and handled by the organizers, called all registered Web services upon any new arriving question from selected categories and stored the system responses into a pool of answers to be judged.</p><p>The answer length was limited to 1000 characters, and the response time was limited to one minute, thus preventing participants from answering manually, or reusing human answers that are accumulated simultaneously on the YA site. In addition, systems could decide to answer only some of the questions, by returning a null response. Metrics were designed in order to consider both total performance (where non-answered questions are treated the same as bad answers) and system precision (where the decision not to answer a question is rewarded).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>Being the first year for the LiveQA task, there is no previously judged data. However, YA data is publicly available and many exiting datasets could be reused for training. Among them is the a large collection of 4M question and answer pairs provided by Yahoo Labs on the WebScope site (http://webscope. sandbox.yahoo.com/catalog.php?datatype=l(setL6)).</p><p>In addition, as a semi-official training dataset, the organizers provided a set of 1000 YA questions, randomly selected from the predefined categories. This was done by providing the question IDs, as they appear on the public YA site. In addition, a scraping program was provided, to enable extraction of the question and corresponding answerss. On top of this, one of the participants, Di Wang from CMU, shared the search results retrieved by the YA internal search engine for all the questions in the dataset. The search results are resolved questions in the site archive, each associated with many human answers that are similar to the searched question and therefore can be further used for training.</p><p>Finally, a few weeks before the official test day (August 31), namely from June 1, 2015, participants were allowed to experiment and validate their answering service with the testing system that ran on a regular basis, calling all live registered systems in a low rate of one new unresolved YA question per 2-5 minutes. During the training stage, system answers were not stored by the testing system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Testing</head><p>Starting August 31 at 23:59 PDT, and continuing for 24 hours, the testing system submitted fresh questions to all live registered systems at a rate of 1 question per minute. The answers were then stored, conditioned on meeting the one-minute time limit, and 1000-character length limit.</p><p>During the day, 1,340 questions were submitted to 22 systems from 14 institutions. 27,369 valid answers were collected, out of which about 2,200 were "NO ANSWER", "Timeout", or "null". One of the runs returned "NO ANSWER" for all questions and was therefore excluded from the report. The average response time was 21.35 seconds, with 871 answers taking longer than one minute to arrive.</p><p>The questions were further filtered out by the organizers, due to late deletion on the YA site (implying spam or abusive content, reported or discovered at some later time) and due to several other constraints. The final set of 1087 questions, with their pools of valid answers, were submitted to be judged by NIST assessors. The judgment scores are: 0 -unanswered (or unreadable); 1poor; 2 -fair; 3 -good; 4 -excellent.</p><p>We computed 7 measures per run:</p><p>• avgScore(0-3): The average score over all questions (transferring 1-4 level grades to 0-3 score, hence treating a 1-level grade answer the same as an non-answered question). This is the main score used to rank the runs.</p><p>• succ@i+: the number of questions with score i or above (i ∈ {2..4}) divided by the total number of questions. For example, succ@2+ measures the percent of questions with at least fair grade answered by the run.</p><p>• prec@i+: the number of questions with score i or above (i ∈ {2..4}) divided by number of questions answered by the system. This measures the precision of the run, designed not to penalize non-answered questions.</p><p>No </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The following tables present the participating systems with their performance. Table <ref type="table" coords="5,162.01,443.77,4.98,8.74" target="#tab_1">1</ref> presents the list of participants ranked by performance based on the avgScore metric. Table <ref type="table" coords="5,239.53,455.73,4.98,8.74" target="#tab_2">2</ref> shows the average score and succ@i+. Table <ref type="table" coords="5,444.16,455.73,4.98,8.74" target="#tab_4">3</ref> shows the prec@i+ measures. At the time of writing, we are not informed about the different approaches taken by the participating systems for answering live questions. However, we can certainly identify that most runs tried to answer all questions. This is not true for Yahoo-Exp1*, a run from Yahoo Labs which answered much fewer questions than the others. The selective approach taken by this run severely affected its AvgScore, where unanswered questions are treated as poor answers by this measure. However, its precision scores which ignore unanswered questions, prec@i+, are relatively high, probably due to invoking a clever filtering rule which filters out poor answers.</p><p>The leading run, CMUOAQA from CMU, did very well compared to all other runs, according to all measures. Its AvgScore of 1.081 can be interpreted as follows: the automatic answers returned by this run are fair on average (recall that 2-level grade for fair answers is transformed to a score of 1). Its prec@2+ = 0.543 which means that about half of its answers were judged as fair and above. However, this score is still by far less than the maximum possible avgScore of 3.0. This is not surprising due to the complexity of the task and No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>#Answered questions avgScore(0-3) succ@2+ succ@3+ succ@4+ due to the fact that this is the first time of running the LiveQA challenge. It will be interesting to follow how much liveQA systems can improve over this strong baseline in the future. It will also be interesting to measure the quality of human answers for the same set of questions of the LiveQA benchmark. Such measurement can provide an interesting platform for comparing man versus machine on the question answering task.</p><p>Figure <ref type="figure" coords="6,179.80,430.38,4.98,8.74">2</ref> shows the average scores of the systems broken down into the eight question categories contributing questions to the challenge. The categories can be classified according to question difficulty. The most difficult one is the Travel category, for which most runs had difficulty to provide decent answers. On the other hand, the Health and the Computer&amp; Internet categories seem to be easier. This dichotomy calls for further investigation what makes some of the categories more difficult than others. One fact which may be related is that the latter categories are the most frequent of the eight, comprising roughly half of the questions sent to participants (see Figure <ref type="figure" coords="6,334.05,526.02,3.87,8.74" target="#fig_0">1</ref>).</p><p>Figure <ref type="figure" coords="6,179.82,537.97,4.98,8.74" target="#fig_1">3</ref> shows the average scores of the systems for the set of 876 questions which were later answered by Yahoo Answers' users (blue) vs. the complement set of 211 questions with no human answers (red). The observation is clearunanswered questions are much more difficult, for (almost) all systems, than human-answered questions.</p><p>There are several reasons for a question not to be answered on the YA site, including poor clarity, ambiguity, unattractiveness, and many others. Assuming that one of the reasons for ignoring a question is the question's "difficulty", we can hypothesize, based on the superior performance of the systems over answered questions, that there is some correlation between the question difficulty for humans and that for machines. This is surprising, as the common assump-  tion in the QA field is that difficult questions for a machine are not necessarily difficult for a human, and vice versa. The results in this track shows the opposite. The issue of question difficulty for man vs. for machine should be further investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>This is the first year that we ran the LiveQA track, reviving the popular QA track which has been frozen for several years. The track attracted significant attention from the Question Answering research community; 14 teams from around the globe took the challenge of answering complex YA questions with original intent of being answered by humans. The quality of results is still far from human level but, on the other hand, is very promising. Our intention is to run the LiveQA challenge next year, thus, letting participants improve their systems. Our wish is that many other teams will join this joint research effort of answering live questions in real-time. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,153.33,276.62,304.60,8.74;2,185.33,124.80,240.60,136.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of categories in questions sent to participants.</figDesc><graphic coords="2,185.33,124.80,240.60,136.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,133.77,524.52,343.72,8.74;9,133.77,536.47,16.89,8.74;9,133.77,254.28,412.44,255.11"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance distribution over answered and unanswered questions in YA.</figDesc><graphic coords="9,133.77,254.28,412.44,255.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,133.77,259.19,412.46,255.32"><head></head><label></label><figDesc></figDesc><graphic coords="8,133.77,259.19,412.46,255.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,139.75,126.75,363.56,260.16"><head>Table 1 :</head><label>1</label><figDesc>Participating runs</figDesc><table coords="5,139.75,126.75,363.56,238.40"><row><cell></cell><cell>. Run</cell><cell>Organization</cell></row><row><cell>1</cell><cell>CMUOAQA</cell><cell>Carnegie Mellon University</cell></row><row><cell>2</cell><cell>ecnucs</cell><cell>East China Normal University</cell></row><row><cell>3</cell><cell>NUDTMDP1</cell><cell>MDP Lab, National University of Defense Technology</cell></row><row><cell>4</cell><cell>RMIT0</cell><cell>RMIT</cell></row><row><cell>5</cell><cell>Yahoo-Exp1*</cell><cell>Yahoo Labs, Haifa</cell></row><row><cell>6</cell><cell>CLIP1</cell><cell>University of Maryland</cell></row><row><cell>7</cell><cell>emory-Out-of-mEmory</cell><cell>Emory University</cell></row><row><cell>8</cell><cell>NUDTMDP3</cell><cell>MDP Lab, National University of Defense Technology</cell></row><row><cell>9</cell><cell>ECNU-ECNU ICA 2</cell><cell>East China Normal University</cell></row><row><cell>10</cell><cell>HIT SCIR QA Grp</cell><cell>Harbin Institute of Technology</cell></row><row><cell>11</cell><cell>ADAPT.DCU-system7</cell><cell>Dublin City University</cell></row><row><cell>12</cell><cell>RMIT1</cell><cell>RMIT</cell></row><row><cell>13</cell><cell>RMIT3</cell><cell>RMIT</cell></row><row><cell>14</cell><cell>NUDTMDP2</cell><cell>MDP Lab, National University of Defense Technology</cell></row><row><cell>15</cell><cell>RMIT2</cell><cell>RMIT</cell></row><row><cell>16</cell><cell>uwaterlooclarke-system4</cell><cell>University of Waterloo</cell></row><row><cell>17</cell><cell>QU1</cell><cell>Qatar University</cell></row><row><cell>18</cell><cell>dfkiqa</cell><cell>DFKI, Germany</cell></row><row><cell>19</cell><cell>CLIP3</cell><cell>University of Maryland</cell></row><row><cell>20</cell><cell>CLIP2</cell><cell>University of Maryland</cell></row><row><cell>21</cell><cell cols="2">SCU-SantaClaraUniversity Santa Clara University</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,139.75,142.36,341.91,195.14"><head>Table 2 :</head><label>2</label><figDesc>Run Results</figDesc><table coords="6,139.75,142.36,341.91,173.89"><row><cell>1</cell><cell>CMUOAQA</cell><cell>1064</cell><cell>1.081</cell><cell>0.532</cell><cell>0.359</cell><cell>0.190</cell></row><row><cell>2</cell><cell>ecnucs</cell><cell>994</cell><cell>0.677</cell><cell>0.367</cell><cell>0.224</cell><cell>0.086</cell></row><row><cell>3</cell><cell>NUDTMDP1</cell><cell>1041</cell><cell>0.670</cell><cell>0.353</cell><cell>0.210</cell><cell>0.107</cell></row><row><cell>4</cell><cell>RMIT0</cell><cell>1074</cell><cell>0.666</cell><cell>0.364</cell><cell>0.220</cell><cell>0.082</cell></row><row><cell>5</cell><cell>Yahoo-Exp1*</cell><cell>647</cell><cell>0.626</cell><cell>0.320</cell><cell>0.211</cell><cell>0.095</cell></row><row><cell>6</cell><cell>CLIP1</cell><cell>1079</cell><cell>0.615</cell><cell>0.326</cell><cell>0.204</cell><cell>0.086</cell></row><row><cell>7</cell><cell>emory-Out-of-mEmory</cell><cell>884</cell><cell>0.608</cell><cell>0.332</cell><cell>0.190</cell><cell>0.086</cell></row><row><cell>8</cell><cell>NUDTMDP3</cell><cell>1035</cell><cell>0.602</cell><cell>0.319</cell><cell>0.186</cell><cell>0.097</cell></row><row><cell>9</cell><cell>ECNU ICA 2</cell><cell>1057</cell><cell>0.569</cell><cell>0.289</cell><cell>0.191</cell><cell>0.089</cell></row><row><cell>10</cell><cell>HIT SCIR QA Grp</cell><cell>1086</cell><cell>0.522</cell><cell>0.291</cell><cell>0.168</cell><cell>0.063</cell></row><row><cell>11</cell><cell>ADAPT.DCU-system7</cell><cell>1087</cell><cell>0.444</cell><cell>0.290</cell><cell>0.121</cell><cell>0.034</cell></row><row><cell>12</cell><cell>RMIT1</cell><cell>1078</cell><cell>0.435</cell><cell>0.267</cell><cell>0.130</cell><cell>0.039</cell></row><row><cell>13</cell><cell>RMIT3</cell><cell>1082</cell><cell>0.415</cell><cell>0.251</cell><cell>0.126</cell><cell>0.038</cell></row><row><cell>14</cell><cell>NUDTMDP2</cell><cell>1025</cell><cell>0.391</cell><cell>0.228</cell><cell>0.120</cell><cell>0.043</cell></row><row><cell>15</cell><cell>RMIT2</cell><cell>1086</cell><cell>0.381</cell><cell>0.232</cell><cell>0.115</cell><cell>0.034</cell></row><row><cell>16</cell><cell>uwaterlooclarke-system4</cell><cell>1001</cell><cell>0.380</cell><cell>0.241</cell><cell>0.108</cell><cell>0.031</cell></row><row><cell>17</cell><cell>QU1</cell><cell>1082</cell><cell>0.256</cell><cell>0.163</cell><cell>0.070</cell><cell>0.023</cell></row><row><cell>18</cell><cell>DFKI-dfkiqa</cell><cell>1058</cell><cell>0.211</cell><cell>0.152</cell><cell>0.049</cell><cell>0.010</cell></row><row><cell>19</cell><cell>CLIP3</cell><cell>805</cell><cell>0.144</cell><cell>0.102</cell><cell>0.034</cell><cell>0.008</cell></row><row><cell>20</cell><cell>CLIP2</cell><cell>1066</cell><cell>0.092</cell><cell>0.065</cell><cell>0.019</cell><cell>0.007</cell></row><row><cell>21</cell><cell>SCU</cell><cell>809</cell><cell>0.023</cell><cell>0.014</cell><cell>0.006</cell><cell>0.003</cell></row><row><cell>Avg.</cell><cell></cell><cell>1007</cell><cell>0.467</cell><cell>0.262</cell><cell>0.146</cell><cell>0.060</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,248.52,320.79,114.20,8.74"><head>Table 3 :</head><label>3</label><figDesc>Precision Results</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
