<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.78,111.92,332.42,15.22">Overview of the TREC 2010 Legal Track</title>
				<funder>
					<orgName type="full">Topic Authorities</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.05,145.81,99.29,8.81"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<email>gvcormac@plg.uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.52,187.66,99.80,8.81"><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
							<email>mrgrossman@wlrk.com</email>
							<affiliation key="aff1">
								<address>
									<addrLine>Lipton, Rosen &amp; Katz 51 West 52nd Street</addrLine>
									<postCode>10019</postCode>
									<settlement>Wachtell, New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.68,243.45,61.20,8.81"><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
							<email>bhedin@h5.com</email>
							<affiliation key="aff2">
								<orgName type="institution">H5</orgName>
								<address>
									<addrLine>71 Stevenson St</addrLine>
									<postCode>94105</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.16,285.29,85.33,8.81"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff3">
								<orgName type="department">College of Information Studies and Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.78,111.92,332.42,15.22">Overview of the TREC 2010 Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">889BBEC2FA5AEE8F03A7EED67A8353F0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC 2010 was the fifth year of the Legal Track, which focuses on evaluation of search technology for discovery of electronically stored information in litigation and regulatory settings. The TREC 2010 Legal Track consisted of two distinct tasks: the Learning task, in which participants were required to estimate the probability of relevance for each document in a large collection, given a seed set of documents, each coded as responsive or non-responsive; and the Interactive task, in which participants were required to identify all relevant documents using a human-in-the-loop process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We are concerned with the document selection and review component of the e-discovery process, for which the objective is to identify as nearly as practicable all documents from a collection that are responsive to a request for production in civil litigation, while minimizing the number of non-responsive documents that are identified by the method. The Learning and Interactive tasks of the TREC 2010 Legal Track represent two different e-discovery scenarios:</p><p>• The Learning task represents the scenario in which preliminary search and assessment has yielded a set of documents that are coded as relevant or not; this seed set is then used as input to a process involving humans or technology to estimate the probability that each of the remaining documents in the collection is relevant.</p><p>• The Interactive task represents the process of using humans and technology, in consultation with a Topic Authority, to identify as well as possible all relevant documents in the collection, while simultaneously minimizing the number of false positives.</p><p>The Learning task derives from the TREC 2009 Batch task, while the Interactive task reprises the TREC 2009 Interactive task, with three new requests for production, and, in addition, a privilege review for which the objective is to identify documents that may be withheld from production because of attorney-client privilege or work-product protection.</p><p>For the document collection, both tasks used a newly processed variant of the Enron email dataset, containing about 1.3 million email messages captured by the Federal Energy Review Commission (FERC) from Enron, in the course of its investigation of Enron's collapse. The Learning task reused the seven requests for production (topics 201 through 207) from the TREC 2009 Interactive task (which had used a different variant of the Enron email dataset), and also one novel topic (topic 200). The Interactive task used three novel topics (topics 301, 302 and 303), in addition to a privilege review (topic 304).</p><p>Seventeen teams participated in the TREC 2010 Legal Track, as detailed in Table <ref type="table" coords="2,447.09,146.83,3.87,8.80" target="#tab_0">1</ref>. Eight of the teams participated in the Learning task, while twelve participated in the Interactive task; three participated in both tasks. The detailed results given in the following sections identify each teams' results by a "run identifier" whose prefix is given in Table <ref type="table" coords="2,204.39,182.69,3.87,8.80" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run Identifier Prefix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Collection</head><p>The document collection for both tasks was derived from the EDRM Enron Dataset, version 2, prepared by ZL Technologies in consultation with the Legal Track coordinators, and hosted by EDRM. 1 ZL acquired the full collection of 1.3 million Enron email messages from Lockheed Martin (formerly Aspen Systems) who captured and maintain the dataset on behalf of FERC. The EDRM dataset is available in two formats: EDRM XML and PST. The EDRM XML version contains a text rendering of each email message and attachment, as well as the original native format. The PST version contains the same messages, in a Microsoft proprietary format used by many commercial tools. Both versions of the dataset approach 100GB in size, presenting an obstacle to participants. Furthermore, there are a large number of duplicate email messages in the dataset, that were captured more than once by Lockheed Martin. For TREC 2010, a list of 455,449 distinct messages were identified as canonical; all other messages duplicate one of the canonical messages. These messages contain about 230,143 attachment files; together these 455,449 messages plus 230,143 attachments form the 685,592 documents of the TREC 2010 Legal Track collection used for both the Learning and Interactive tasks. Text and native versions of these documents were made available to participants, along with a mapping from the EDRM XML and PST files to their canonical counterparts in the TREC collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relevance Assessments</head><p>In order to measure the efficacy of TREC participant efforts in the two tasks, it is necessary to compare their results to a gold standard indicating whether or not each document in the collection is relevant to a particular discovery request. The Learning task used eight distinct discovery requests, while the Interactive task used four. Ideally, a gold standard would indicate the relevance of each document to each topic, a total of eight million judgments.</p><p>It is impractical to use human assessors to render these eight million assessments. Instead, a sample of documents was identified for each topic, and assessors were asked to code only the documents in the sample as relevant or not. For the Learning task, 78,000 human assessments were used; for the Interactive task, 50,000 human assessments were used.</p><p>The Learning task assessments were rendered by individual volunteers, primarily, but not exclusively, law students. For each document and topic, three binary assessments were rendered, and the majority opinion was taken to be the gold standard. The Interactive task assessments were assessed by professional review companies. Ten percent of the documents for each topic were assigned to more than one reviewer; the agreement among these redundant assessments was used to estimate and correct for assessor error. In both cases, individual assessors were asked to review documents in batches of 500, and reviewed one or more batches.</p><p>The assessors used a new Web-based platform developed by the coordinators to view the documents and to record their relevance judgments. To avoid problems with local rendering software on each assessor's workstation, the assessors made their judgments based on pdf-formatted versions of the documents, as opposed to the original native format documents.</p><p>Assessors were provided with orientation and detailed guidelines created by a Topic Authority. For the Learning task, assessors were given 10 examples each of relevant and a non-relevant documents for their particular topic. The review platform included a "seek assistance" link which assessors were encouraged to use to request that the Topic Authority respond to questions to resolve any uncertainty that may have arisen as to particular documents.</p><p>In reviewing their bins, assessors were instructed to make a relevance judgment of relevant (R), not relevant (N), or broken (B) for every document in their bins. The latter code reflects the fact that a small percentage of documents (1.25%) were malformed and therefore could not be assessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Task</head><p>For each of the eight topics, participants in the Learning Task were given a seed set -a list of documents within the collection, each coded as relevant or not relevant to the topic. For topic 200, which was new to TREC 2010, the seed set was constructed by TREC coordinators, using an interactive search and judging process. For topics 201 through 207, the seed sets were derived indirectly from the relevance assessments used to evaluate the TREC 2009 Interactive Task. It was not possible to used the relevance assessments directly, as TREC 2009 and TREC 2010 used different versions of the Enron dataset. The coordinators employed an approximate match strategy to find analogues to the TREC 2009 documents in the TREC 2009 dataset. Only documents with high similarity were used as seeds; the remainder were disregarded. Table <ref type="table" coords="3,535.02,602.67,4.98,8.80" target="#tab_1">2</ref> shows the number of relevant and non-relevant documents in each seed set.</p><p>The Learning task models the use of automated or semi-automated methods to guide review strategy for a multi-stage document review effort, organized as follows:</p><p>1. Preliminary search and assessment. The producing party analyzes the production request. Using ad hoc methods the team identifies a seed set of potentially responsive documents, and assesses each as responsive or not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learning by example.</head><p>A learning method is used to rank the documents in the collection from most to least likely to be responsive to the production request, and to estimate this likelihood for each document. The input to the learning method consists of the seed set, the assessments for the seed set, and the unranked collection; the output is a ranked list consisting of the document identifier and a probability of responsiveness for each document in the collection.</p><p>The two components of learning by example -ranking and estimation -may be accomplished by the same method or by different methods. Either may be automated or manual. For example, ranking might be done using an information retrieval method or by human review using a five-point scale. Estimation might be done in the course of ranking or, for example, by sampling and reviewing documents at representative ranks.</p><p>3. Review process. A review process may be conducted, with strategy guided by the ranked list. One possible strategy is to review documents in order, thus discovering as many responsive documents as possible for a given amount of effort. Another possible strategy is triage: to review only mid-ranked documents, deeming, without further review, the top-ranked ones to be responsive, and the bottomranked ones to be non-responsive. Review strategy may be guided not only by the order of the ranked list, as outlined above, but also by the estimated effectiveness of various alternatives. Consider the strategy of reviewing the top-ranked documents. Where should a cut be made so that documents above the cut are reviewed and documents below are not? For triage, where should the two necessary cuts be made?</p><p>Using the seed set for each topic, participants were required to submit an estimate of the probability of relevance for every document in the collection. That is, each submitted run contained 5,484,736 probability estimates -8 per document, and 685,592 per topic. Practically every review strategy decision boils down to the question, For some particular set of documents, how many are responsive and how many are not?</p><p>To answer this question it suffices to answer the more detailed question:</p><p>What is the probability of each document in the set being relevant?</p><p>Given an answer to the second question, the answer to the first is simply the sum of the probabilities. For this reason, participants in the Learning task were required to provide an estimate of the probability of relevance for each document in the collection. The optimal relevance ranking follows from the probability estimate. If the probability estimate is accurate, documents with a higher probability are more likely to be relevant. At any given rank k, the expected number of relevant documents up to and including rank k is the sum of their probabilities, and this sum is maximized when the documents with the highest probabilities are given the highest ranks. Furthermore, if the probability estimate is reasonable, the sum is itself an accurate estimate of the number of relevant documents among the top k. This estimate may be used to guide review strategy, as it allows the legal team to evaluate the tradeoff between review effort and the number of documents retrieved.</p><p>The number of relevant documents retrieved, as a fraction of the number of relevant documents in the collection, is known as recall. In document production for civil litigation, recall is typically more important than precision, the fraction of retrieved documents that are relevant. The major question to be answered in e-discovery is: if we examine the top-ranked k documents, what recall will be achieved? The answer to this question is encoded in the probability estimates submitted by TREC participants.</p><p>The TREC Legal Track evaluation process provides a post-hoc answer to that question, against which the participants' efforts may be compared. The accuracy of the estimate is defined to be accuracy = 100% × min(estimate, true value) max(estimate, true value) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relevance Assessment and Evaluation</head><p>For each submitted run, the Learning Task evaluation process uses stratified sampling and redundant assessment to count, for each possible 1 ≤ k ≤ 650000, the number of responsive documents within the k highest-ranked within the run. From these counts are derived point estimates for recall at each possible cutoff value, as well as summary estimates of retrieval effectiveness over all cutoff values. For each topic, each document in a stratified sample of 2,720 documents was assessed by three independent volunteer reviewers. The majority opinion of these three assessors was taken to be ground truth, and used as the gold standard against which the submitted runs were evaluated. Each reviewer had legal training; the majority were third-year law students who received pro bono credits from their academic institution.</p><p>The four strata whose sizes are detailed in Table <ref type="table" coords="5,311.34,457.69,3.87,8.80" target="#tab_2">3</ref>, were defined as follows. The first stratum (100) consisted of any document that was ranked within the top 100 of any of the 20 runs. That is, stratum 100 was constructed by the TREC pooling method, with pool depth 100. The second stratum (1000) was also constructed using the pooling method, with pool dept 1000, and excluding all documents in the first stratum. The third stratum (10000) used a pool depth of 10000, excluding prior strata, while the last stratum (1000000) consisted of the entire corpus, excluding those documents in the first three strata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Eight participating groups submitted 20 runs. For each topic within each run, the number of relevant documents retrieved (and hence recall) was computed for all possible cutoff values k. That is, for each k between 1 and 650,000, the actual and estimated number of relevant documents was determined. Table <ref type="table" coords="5,535.02,587.61,4.98,8.80" target="#tab_4">4</ref> shows the estimated number of relevant documents for each topic, and Tables <ref type="table" coords="5,408.62,599.56,49.96,8.80" target="#tab_8">5 through 8</ref> show the resulting recall values for each run and each topic at four representative values of k : 20,000 (3% of the collection), 50,000 (7.5% of the collection), 100,000 (15% of the collection), and 200,000 (30% of the collection). In addition, the tables show the average recall over all topics, the estimated average recall over all topics, and the accuracy of the estimated average. From Table <ref type="table" coords="5,293.43,647.38,4.98,8.80" target="#tab_5">5</ref> we see that the best-performing system identifies 49.8% of all relevant documents within the top-ranked 3% of documents in the collection. Table <ref type="table" coords="5,467.30,659.34,4.98,8.80" target="#tab_6">6</ref> shows that the same system identifies 63.5% of all relevant documents within the top-ranked 7.5%. In terms of document review strategy, this indicates that a review team would have to examine two-and-a-half times as many  documents (but still only 7.5% of the entire collection) to find 30% more documents. Table <ref type="table" coords="6,483.69,227.52,4.98,8.80" target="#tab_7">7</ref> shows that the system identifies 74.3% of the relevant documents within the top-ranked 15% of the collection, while Table <ref type="table" coords="6,99.36,251.43,4.98,8.80" target="#tab_8">8</ref> shows that the system identifies 84.3% of the relevant documents within the top-ranked 30% of the collection.</p><p>It is apparent that, as expected, recall increases as the cutoff k increases, in a nonlinear fashion. The tradeoff between relevant documents retrieved and non-relevant documents retrieved at rank k may be expressed as a Receiver Operating Characteristic Curve, also known as a recall-fallout curve <ref type="bibr" coords="6,482.46,299.26,9.96,8.80" target="#b3">[4]</ref>. An ROC curve plots the fraction of relevant documents retrieved (recall) as a function of the fraction of non-relevant documents retrieved (fallout). While ROC curves are ubiquitous in signal detection and diagnostic test theory, recall-fallout curves have largely been supplanted by recall-precision curves in much of the work on information retrieval evaluation because of the emphasis of recall-precision curves on precision at early ranks. For e-discovery, ROC curves better illustrate system effectiveness at high recall levels.</p><p>Figures <ref type="figure" coords="6,122.81,370.99,51.95,8.80" target="#fig_1">1 through 4</ref> show the ROC curves for each of the eight topics used in the Learning task. The top-performing run for each participant is plotted in the set of graphs for each topic. Figures <ref type="figure" coords="6,488.33,382.94,51.67,8.80">5 through 8</ref> show the ROC curves for the top-performing run (according to AUC, see below) for each participant. Each graph in those figures shows the ROC curves for each topic. Note that the curves are plotted on a logit scale. <ref type="foot" coords="6,95.30,417.25,3.97,6.16" target="#foot_0">2</ref>A nearly perfect system generates a concave curve that rises steeply, rapidly approaching the top-left corner of the graph, then continues to the top-right corner; a random ranking yields a straight line along the main diagonal. In general, a superior curve represents superior effectiveness. A common summary measure of the height of the curve is the area under the ROC curve (AUC). AUC is a number between 0 and 1, where 1 indicates perfection, and 0.5 indicates a random ranking. Table <ref type="table" coords="6,363.96,478.58,4.98,8.80">9</ref> shows the AUC results for every topic within every run and, in addition, the average AUC over all topics for each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluating F 1</head><p>The end goal of the discovery process is to produce a set of documents that are responsive to the request, not simply a prioritized list. To convert a a prioritized list to a set, it is necessary to choose a particular cutoff value k, and to include in the production set only the top-ranked k documents. The ideal set would, of course, include all the responsive documents and none of the non-responsive ones. In general, this ideal is impossible to realize: for any real ranking and for any k, there will always be some relevant documents that are not in the top-ranked k, or some non-relevant documents in the top k, or both. The challenge for evaluation, then, is to measure how close to ideal any particular set of produced documents is.</p><p>F 1 , used as the principal measure of effectiveness by the Interactive Task, is defined to be the harmonic mean of recall and precision:     While it is debatable whether or not F 1 aptly reflects the quality of a result set in any practical situation, optimizing F 1 is nonetheless a challenging proposition that requires good probability estimates for two purposes: to rank the documents, and to determine the optimal value of k.</p><formula xml:id="formula_0" coords="6,254.05,641.87,63.48,16.39">F 1 = 2</formula><p>Figure <ref type="figure" coords="17,118.83,423.59,9.96,8.80" target="#fig_3">10</ref> shows the actual F 1 scores achieved on each topic by every run. The average over all topics ranges from 3.6% to 37.1%. Figure <ref type="figure" coords="17,239.16,435.54,8.48,8.80">11</ref>, in contrast, shows the hypothetical F 1 scores achieved on each topic by every run. This score represents the F 1 score that would have been achieved, had the probability estimates been accurate and therefore yielded the optimal value of k. That is, actual F 1 measures the quality of the ranking and also the accuracy of the probability estimates, while hypothetical F 1 measures only the quality of the ranking. A large discrepancy between actually and hypothetical F 1 indicates poor probability estimates. We see that the hypothetical F 1 scores range from 11.6% to 42.6%, indicating room for considerable improvement in the accuracy of probability estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>Four broad themes stand out in the Learning track results. First, to the extent that we can use the randomselection main diagonal as a surrogate for manual review, most teams convincingly beat manual review on most topics. From this we can conclude that learning methods have a place in the design of cost-effective document review processes. Second, no team found more than half of the relevant documents that we estimate to exist for more than half of the topics when the result set was limited to 20,000 documents. From this we can conclude that further research on the application of learning techniques to this task is called for. Third, if we interpret the average accuracy of system estimates to be a measure of the degree to which systems can help their users to determine a cost-effective point at which to cease further learning, present systems seem particularly poor at that important task. Indeed, some systems do well at that "stoppingguidance" task at some points in the ranked list but not at others, while others do consistently less well. Xerox Research Centre Europe (xrce) is among the very few that do fairly well at this "stopping-guidance" task consistently, suggesting that their paper would be well worth reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Interactive Task</head><p>The Legal Track's Interactive task more fully models the conditions and objectives of a search for documents that are responsive to a production request served during the discovery phase of a civil lawsuit. The 2010 exercise represented the third year that the Interactive task, in its current design, was featured in the Legal Track. Results from the first two years can be found in the track overviews for 2008 <ref type="bibr" coords="18,438.32,485.65,10.51,8.80" target="#b6">[7]</ref> and 2009 <ref type="bibr" coords="18,494.06,485.65,9.96,8.80" target="#b5">[6]</ref>. In this year's overview, we briefly review the task design (Section 5.1); describe the specific features that defined the 2010 exercise (Section 5.2); summarize the results obtained for each of the 2010 topics (Section 5.3); and provide additional analysis on certain points of interest (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Design</head><p>The most complete discussion of the design of the Interactive task, and of the reasoning behind it, can be found in the 2008 task guidelines <ref type="bibr" coords="18,225.97,579.47,9.96,8.80" target="#b1">[2]</ref>. While the core features of the task have not changed since 2008, we have, in each of the subsequent years, introduced various modifications to the design in an effort to improve the effectiveness and efficiency of the exercise. In this section, we briefly review the task's core features (Section 5.1.1) and summarize the modifications introduced for the 2010 running of the exercise (Section 5.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Core Features</head><p>The real-world circumstance modeled by the Interactive task is that of a search for documents that are responsive to a discovery request served in civil litigation. The task is designed to gauge the effectiveness Table <ref type="table" coords="19,100.89,507.07,8.48,8.80" target="#tab_0">11</ref>: Hypothetical F 1 scores achieved by submitted runs, using optimal cutoff k. These numbers represent the best possible F 1 that could have been achieved with the same relevance ranking and a better choice of k.</p><p>of various approaches to document retrieval (whether they be fully automated, fully manual, or something in between) at meeting the requirements of this sort of search. The core features of the task design are as follows.</p><p>Complaint and topics. Context for the Interactive task is provided by a mock complaint that sets forth the legal and factual basis for the hypothetical lawsuit that motivates the discovery requests at the heart of the exercise. Associated with the complaint are document requests that specify the categories of documents which must be located and produced. For purposes of the Interactive task, each of these document requests serves as a separate topic. The goal of a team participating in a given topic is to retrieve all, and only, documents relevant to that topic (as defined by the "Topic Authority;" see below).</p><p>Most of the topics featured in the Interactive task have been modeled on typical subject-matter requests for production (i.e., they seek documents pertinent to the legal and factual issues that are the focus of the litigation). The 2010 exercise, for the first time, also featured a "privilege" topic, a topic requiring the identification of any and all documents that could be withheld from production on grounds of privilege or work product protection.</p><p>The Topic Authority as the source of the operative standard of relevance. A key role in the Interactive task is played by the "Topic Authority." The Topic Authority plays the role of a senior attorney who is charged with overseeing a client's response to a request for production and who, in that capacity, must certify to the court that the client's response to the request is complete and correct (commensurate with a reasonable and good-faith effort). In keeping with that role, it is this attorney who, weighing considerations of genuine subject-matter relevance as well as pragmatic considerations of legal strategy and tactics, holds ultimate responsibility for deciding what is and is not to be considered responsive for purposes of the document production (or, in the terms of the Interactive task, what is and is not to be considered relevant to a target topic). The Topic Authority's role, then, is to be the source for the authoritative conception of relevance that each participating team, in the role of a hired cohort of manual reviewers or of a vendor of document-retrieval services, will be asked to replicate across the full document collection. Now, needless to say, different lawyers may well take different approaches to discovery, and the interpretation which one lawyer would give to a request will likely not be identical to the interpretation another lawyer would give to the same request. That is nothing more than to state that responsiveness, like relevance, is, to a certain extent, subjective. In the real-world circumstance modeled by the Interactive task, however, there is, from the perspective of a vendor hired to assist in the search for responsive documents, only one lawyer (or litigation team) that matters, the client and its counsel, and one interpretation of responsiveness that matters, that of the client and its counsel, whose retrieval requirements the vendor has been hired to fulfill. Once fully situated in a real-world context, therefore, in which there is a client and its counsel who set the requirements and a vendor who is hired to implement those requirements, the subjectivity of relevance gives way to a single operative standard of relevance, that of the client and its counsel (or, in the terms of the Interactive task, that of the Topic Authority). In the Interactive task, it is this single standard of relevance, set by the Topic Authority, that defines the target set of documents for each topic.</p><p>Each topic has one, and only one, Topic Authority, and each Topic Authority has responsibility for one, and only one, topic.</p><p>Allowance for interaction with the Topic Authority. If it is the Topic Authority who defines the target (i.e., who determines what should and should not be considered relevant to a topic), it is essential that provision be made for teams to be able to interact with the Topic Authority in order to gain a better understanding of the Topic Authority's conception of relevance. In the Interactive task, this provision takes the following form. Each team can ask, for each topic for which it plans to submit results, for up to 10 hours of a Topic Authority's time for purposes of clarifying a topic. A team can call upon a Topic Authority at any point in the exercise, from the kickoff of the task to the deadline for the submission of results. How a team makes use of the Topic Authority's time is largely unrestricted: a team can ask the Topic Authority to pass judgment on exemplar documents; a team can submit questions to the Topic Authority by email; a team can arrange for conference calls to discuss aspects of the topic. One constraint that is placed on communication between the teams and their designated Topic Authorities is introduced in order to minimize the sharing of information developed by one team with another; while the Topic Authorities are instructed to generally be free in sharing the understanding they have of their topic, they are asked to avoid volunteering to one team specific information that was developed only in the course of interaction with another team.</p><p>Participant submissions. Each participant's final deliverable is a binary classification (relevant / not relevant) of the full test collection for relevance to each target topic in which it has chosen to participate.</p><p>Composition of evaluation samples. Once participants have completed their submissions, we are in a position to draw the evaluation samples for each topic. The evaluation samples are composed using a stratified sampling design that allows for disproportionate allocation among strata. More specifically, the samples are composed as follows.</p><p>With regard to stratification, strata are defined on the basis of participant submissions. The sets of documents deemed relevant by the participants in a topic allow for a straightforward submission-based stratification of the collection: one stratum contains the documents all participants submitted as relevant, another stratum contains the documents no participant submitted as relevant, and other strata are defined for each of the other possible submission combinations. If, for example, there are five teams that submitted results for a topic, the collection will be partitioned into 2 5 = 32 strata.</p><p>With regard to allocation among strata in composing the samples, strata are represented largely in keeping with their full-collection proportions. In order to ensure that a sufficient number of documents are drawn from all strata, however, some small strata may be over-represented, and some large strata underrepresented, relative to their full-collection proportions. In particular, the "All-N" stratum, the stratum containing the documents all participants have deemed not relevant, tends to be very large, often making up 90% or more of the collection. While we do sample extensively from this stratum, as we want to include in the analysis an ample number of documents that no participant deemed relevant, we under-represent it in the sample, relative to its full-collection proportion, in order to be able also to represent adequately the strata defined for documents that one or more participants have deemed relevant.</p><p>Sampling within a stratum is simple random selection without replacement. Two-stage assessment protocol. Once evaluation samples are drawn, the documents in each sample are assessed for relevance to the topic for which the sample was drawn. In the Interactive task, a two-stage process is followed in order to arrive at final sample assessments. In the first stage, a team of assessors is assigned to each sample and asked to complete, under the guidance of the Topic Authority, a first-pass assessment of the sample. In the second stage, a subset of those first-pass assessments are escalated to the Topic Authority for final adjudication. These subsets consist primarily of assessments that a participant, after reviewing the results of the first-pass assessment, has chosen to appeal to the Topic Authority (because the participant believes the first-pass assessment to be out of keeping with the Topic Authority's guidance). In the 2010 Interactive task, we also, for the first time, escalated a number of non-appealed assessments to the Topic Authority for final adjudication (additional details in Section 5.1.2 below).</p><p>Effectiveness measures. Once all escalated assessments have been adjudicated by the Topic Authority, we are in a position to obtain final estimates of the level of effectiveness achieved by each submission. Given the binary nature of the submissions, we look to set-based metrics to gauge effectiveness. In the Interactive task, the metrics used are recall, precision, and, as a summary measure of effectiveness, F 1 . For further detail on the estimation procedures followed in the Interactive task, see the appendix to the Overview of the TREC 2008 Legal Track <ref type="bibr" coords="21,181.73,541.35,9.96,8.80" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">New to the 2010 Exercise</head><p>While keeping constant the core features of the design of the Interactive task, we did introduce a small number of new features to the 2010 task with an eye to improving the efficiency of the task and to laying the groundwork for further analysis of task results. Chief among these were modifications to the assessment and adjudication process. The modifications are as follows.</p><p>Professional assessors. In 2008, the first-pass assessment was carried out entirely by individual volunteers: law students and legal professionals who volunteered to assess one or two 500-document batches of documents. In 2009, for the first time, we were able to engage, on a pro bono basis, the services of firms offering professional document-review services to carry out the first-pass assessment for some, but not all, of that year's evaluation samples; the samples for other topics again were assessed by individual volunteers. In 2010, we were again able to engage, on a pro bono basis, the services of firms offering professional documentreview services; for 2010, the document-review professionals were able to carry out the first-pass assessment for all of the evaluation samples.</p><p>Dual assessment. In 2008 and 2009, the first-pass assessment consisted of our gathering, from the team of assessors, a single relevance assessment on each document in the evaluation sample. In 2010, we also gathered, on each document in a subsample of the evaluation sample, a second independent assessment. We did so (i) in order to have data to support further analysis of rates of assessor error and of rates of inter-assessor agreement and (ii) in order to identify a set of documents that, even if not appealed, would be good candidates for escalation to the Topic Authority for adjudication (e.g., cases of conflicting assessment).</p><p>The dual-assessment subsample was selected as follows. The full evaluation sample was selected (in accordance with the stratified design described above) and randomly distributed into 25 assessment batches of approximately equal size. Then, for purposes of obtaining dual assessments, a supplemental set of messages was added to each batch; the supplement consisted of messages drawn, via simple random selection without replacement, from messages already included in the sample but not assigned to the batch for which the supplement was intended (and not already added as a dual-assessment supplement to another batch). In all, for each topic, about 10% of the messages in the evaluation sample were selected for dual assessment in this way.</p><p>Adjudication of non-appealed assessments. As noted above, the Interactive task follows a twostage procedure in arriving at final sample assessments: first-pass assessment followed by adjudication. The purpose of the adjudication stage is to identify and correct any errors made in the first-pass assessment ("errors" being relevance assessments that are not in keeping with the Topic Authority's definition of relevance). In 2008 and 2009, adjudication was an entirely appeals-driven process; that is to say, the set of documents escalated to the Topic Authority for final adjudication consisted of all, and only, the documents the first-pass assessments of which participants, after reviewing the results of the first-pass review, had appealed. We have found that an entirely appeals-driven adjudication process is an effective mechanism for correcting first-pass errors, as long as participants make diligent use of the appeals process; we have also found, however, that, in cases in which participants in a topic elect to make little use of appeals (e.g., Topic 206 from the 2009 exercise), the adjudication process will likely leave many first-pass errors uncorrected <ref type="bibr" coords="22,424.44,397.89,10.51,8.80" target="#b5">[6]</ref>  <ref type="bibr" coords="22,437.91,397.89,9.96,8.80" target="#b8">[9]</ref>. For 2010, therefore, we decided to supplement the adjudication set with a certain number of non-appealed documents. Further details on the selection of non-appealed messages for adjudication are provided in Section 5.3.4 below.</p><p>Adjudication materials. In both 2008 and 2009, we asked participants to prepare, in support of their appeals, documents detailing the specific grounds for each of their appeals. The purpose of these "grounds for appeals" documents was to enable participants to direct the Topic Authority to specific features of a document that they believed, given the Topic Authority's prior relevance guidance, would be decisive as to the document's being deemed relevant or not.</p><p>The approach was taken both for the sake of efficiency (enabling the Topic Authority to skip to the salient parts of often long documents) and for the sake of accuracy (enabling participants to draw the Topic Authority's attention to the often subtle features of a document). The approach did raise some concerns, however, as some participants found the preparation of the "grounds for appeals" documents rather burdensome and others argued that information about what the first-pass assessment (and the proposed alternative) was might, in some way, influence the Topic Authority's final assessment.</p><p>In 2010, therefore, on a trial basis, we decided to implement a blind adjudication protocol. Participants were not asked to document the grounds for their appeals; they were asked simply to submit a list of the documents they wished to have adjudicated. The Topic Authorities, when given their adjudication sets, were given no information as to the first-pass assessment, the alternative assessment being proposed by the appealing team, or whether or not the document was included in the set as a result of an appeal. We analyze the results of this year's protocol below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task Specifics</head><p>The following features defined the specific landscape of the 2010 Interactive task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Test Collection</head><p>The document collection used for the 2010 Interactive task was that derived from the EDRM Enron Dataset, v. 2; see Section 2 above for details on this collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Topics &amp; Topic Authorities</head><p>For 2010, the exercise included an entirely new mock complaint and three associated document requests, each of which served as a separate topic <ref type="bibr" coords="23,251.05,161.34,9.96,8.80" target="#b0">[1]</ref>. In addition to these three subject-matter topics, the task also featured a "privilege" topic, a topic designed to model a search for material that could be withheld from a production on grounds of privilege or work-product protection. For each of the four topics, a separate Topic Authority was designated. The topic statements and Topic Authorities were as follows .</p><p>• Topic 301. All documents or communications that describe, discuss, refer to, report on, or relate to onshore or offshore oil and gas drilling or extraction activities, whether past, present or future, actual, anticipated, possible or potential, including, but not limited to, all business and other plans relating thereto, all anticipated revenues therefrom, and all risk calculations or risk management analyses in connection therewith.</p><p>-Topic Authority: Mira Edelman (Hughes Hubbard &amp; Reed).</p><p>• Topic 302. All documents or communications that describe, discuss, refer to, report on, or relate to actual, anticipated, possible or potential responses to oil and gas spills, blowouts or releases, or pipeline eruptions, whether past, present or future, including, but not limited to, any assessment, evaluation, remediation or repair activities, contingency plans and/or environmental disaster, recovery or clean-up efforts.</p><p>-Topic Authority: John F. Curran (Stroz Friedberg).</p><p>• Topic 303. All documents or communications that describe, discuss, refer to, report on, or relate to activities, plans or efforts (whether past, present or future) aimed, intended or directed at lobbying public or other officials regarding any actual, pending, anticipated, possible or potential legislation, including but not limited to, activities aimed, intended or directed at influencing or affecting any actual, pending, anticipated, possible or potential rule, regulation, standard, policy, law or amendment thereto.</p><p>-Topic Authority: Robert E. Singleton (Squire, Sanders, &amp; Dempsey).</p><p>• Topic 304. Should Defendants choose to withhold from production any documents or communications in the TREC Legal Track Enron Collection on the basis of a claim of privilege, attorney work-product, or any other applicable protection, they should identify all such documents or communications.</p><p>Important procedural note specific to Topic 304. Solely for the purpose of the TREC 2010 Legal Track, participants who choose to submit results for Topic 304 should identify any and all documents or communications in the TREC Legal Track Enron Collection that are subject to a claim of privilege, attorney work-product, or other applicable protection, regardless of whether they are responsive to any of the Requests for Production specified above.</p><p>-Topic Authority: Michael Roman Geske (Aphelion Legal Solutions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Participating Teams</head><p>The Interactive task received submissions from twelve teams, who, collectively, submitted a total of 22 single-topic runs. The twelve teams that submitted results for evaluation are shown in Table <ref type="table" coords="23,479.76,635.43,3.87,8.80" target="#tab_0">1</ref>.</p><p>It should be noted that Douglas Oard, a track coordinator who was on sabbatical at the University of Melbourne and RMIT during a part of this period, participated in that team's research and that Gordon Cormack, of the University of Waterloo and also a track coordinator, offered some advice to the University of Waterloo team.</p><p>A team could ask to participate in as many, or as few, topics as it chose. Given constraints on the number of teams for which a Topic Authority could take responsibility (typically, a maximum of eight teams), we indicated that we might not be able to give all teams all of their choices and asked teams to rank their topic selections in order of preference. Topics were assigned largely on a first-come-first-serve basis. For the 2010 task, it turned out that we were able to give all teams their preferred topics. Table <ref type="table" coords="24,436.91,122.92,9.96,8.80" target="#tab_1">12</ref> shows the number of runs submitted by each team for each topic (in the table, an empty cell represents no submissions for the given team-topic combination).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics</head><p>Total Team 301 302 303 304 Runs</p><formula xml:id="formula_1" coords="24,223.54,212.24,166.09,152.25">CB 2 4 6 CS 1 1 EQ 1 1 IN 1 1 2 IS 1 1 2 IT 1 1 2 LA 1 1 MM 1 1 SF 1 1 UB 1 1 UM 1 1 UW 1 1 1<label>3</label></formula><p>Total Runs 5 6 6 5 22</p><p>Table <ref type="table" coords="24,243.75,395.32,8.48,8.80" target="#tab_1">12</ref>: Runs submitted for each topic.</p><p>As can be seen from the table, in most cases, each team submitted, in accordance with the task guidelines, just one run for each topic in which it participated. In one case, however, a team asked for, and was given, permission to submit multiple runs for a single topic.</p><p>The Cleary-Backstop team (CB) wished, for both Topic 303 and Topic 304, to have two submissions evaluated, one taking into account "family" associations between documents and the other not; in the following, these runs are designated CB1 (with family associations) and CB2 (without family associations). The team also wished, for Topic 304, to have two additional runs evaluated. Both of these runs targeted a broader notion of "potentially privileged;" again one taking into account family associations and the other not. These two "broader" runs are designated CB3 (with family associations) and CB4 (without family associations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Assessors</head><p>As noted above, for the 2010 Interactive task, the first-pass assessment of the evaluation samples for all four topics was carried out by firms that provide professional contract or managed review services, using their typical review processes and procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Unit of Assessment</head><p>In evaluating the effectiveness of approaches to assessing the relevance of email messages, one must decide whether one wants to assess effectiveness at the message level (i.e., treat the parent email together with all of its attachments as the unit of assessment) or to assess effectiveness at the document level (i.e., treat each of the components of an email message (the parent email and each child attachment) as a distinct unit of assessment).</p><p>For the 2010 exercise, in an effort to gather data on both levels, we asked participants to submit their results at the document level (in order to enable document-level analysis) from which we would then derive message-level values (which would serve as the primary basis for evaluation). The specific rules governing the assignment of assessments were as follows.</p><p>• A parent email should be deemed relevant either if, in itself, it has content that meets the definition of relevance or if any of its attachments meet that definition; contextual information contained in all components of the email message should be taken into account in determining relevance.</p><p>• An email attachment should be deemed relevant if it has content that meets the Topic Authority's definition of relevance; in making this determination, contextual information contained in associated documents (parent email or sibling attachments) should be taken into account.</p><p>• A message will count as relevant if at least one of its component documents (parent email or attachments) has been found relevant.</p><p>• For purposes of scoring, the primary level is the message-level; document-level analysis is on between documents reviewed and supplementary. By contrast, the Learning task reports only document-level analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Task Results</head><p>The 2010 Interactive task got underway, with the release of the final task guidelines <ref type="bibr" coords="25,452.22,322.62,10.51,8.80" target="#b4">[5]</ref> and of the mock complaint and associated topics <ref type="bibr" coords="25,211.31,334.57,9.96,8.80" target="#b0">[1]</ref>, on July 6, 2010. In this section, we summarize the results of the exercise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Team-TA Interaction</head><p>As noted above, the Interactive task permits teams to call on up to 10 hours (600 minutes) of a Topic Authority's time for purposes of clarifying the scope and intent of a topic. Of course, teams are not required to use their full allotment and, in previous years, we have seen considerable variation in the amount of Topic Authority time that teams choose to use. Figure <ref type="figure" coords="25,118.44,426.67,4.98,8.80" target="#fig_2">9</ref> summarizes, for the 2010 exercise, the participants' use of the Topic Authorities' time for each topic. In the diagram, each bar represents the total time allowed for team-TA interaction (600 minutes); the gray portion of the bar represents the amount of the permitted time that was actually used by a team (with the number of minutes used indicated just above the gray portion). As can be seen from the diagram, there is, once again, considerable variation in the extent to which teams utilized their allotted time for interacting with the Topic Authority: some teams used less than an hour of their available time, while others used seven hours or more. On the whole, however, teams tended to use considerably less than the maximum amount of time that they were allowed. In 15 of the 18 participant-TA interactions, the participant utilized less than 50% of the time available for interacting with the Topic Authority; in only three instances (301-IS; 303-EQ; 304-IN) did a team utilize more than 50% of the time permitted. We consider below (Section 5.4.1) whether there is any correlation between the amount of time spent interacting with the Topic Authority in the preparation of a run and the effectiveness of the run that results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Submissions</head><p>Participants submitted their results on or before September 16, 2010. Table <ref type="table" coords="26,406.02,215.02,9.96,8.80" target="#tab_2">13</ref> summarizes, at the message level, the submissions received for each topic. The table shows (for the complement of the union of all submissions; for the union of all submissions; for each submission; and for the intersection of all submissions): (i) the number of messages that belong to each designated subset, (ii) the proportion, out of all messages in the full collection, that each subset represents, and (iii) the proportion, out of the union of all submissions, that each subset represents. (Recall that the full collection consists of 455,449 messages.)</p><p>The submission data alone, of course, tell us little about the effectiveness of the runs; that can be gauged only after review of the sampling and assessment data. The submission data do, however, permit a couple of initial observations. First, with regard to the yield of the topics, we see that, for three of the four topics, the union of all submissions represents a relatively sizeable proportion of the collection: for each of Topics 301, 303, and 304, the union of all submissions represents 8% or more of the full collection. For Topic 302, on the other hand, the proportion of the collection submitted as relevant by at least one team is quite small: for this topic, the union of all submissions represents approximately 1% of the collection (and most of that can be attributed to a single submission). The submission data suggest, then, that three of the four topics are moderate to high yielding and that one of the four is low yielding. Of course, actual yields can be determined only after reviewing the sampling and assessment data (see Section 5.3.5).</p><p>Second, with regard to differences among submissions, we see that there is considerable variation in the number of messages that participants found relevant. Looking simply at the ratio of the largest to the smallest submission for each topic, we see that, for Topic 301, nearly 40 (39.7) messages were submitted as relevant by the largest submission (SF) for every one message submitted as relevant by the smallest (IS); for Topic 302, the ratio of largest to smallest submission is 50.9; for Topic 303, the ratio is 14.4; and, for Topic 304, the ratio is 18.6. It is clear that, across the full set of participants in any given topic, there was not a common understanding of (or at least a consistent implementation of) the scope that the Topic Authority intended for the topic; that is not to say, of course, that some subset of participants in a topic did not arrive at something approaching a shared understanding of the topic's intended scope.</p><p>Of course, what matters, in the end, is how closely each of the various submissions overlaps with the subset of messages that actually meet the Topic Authority's definition of relevance. In order to gauge that, we turn to sampling and assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Stratification &amp; Sampling</head><p>Once the submissions were received, the collection was stratified for each topic and evaluation samples were drawn. Stratification followed the submission-based design noted above (Section 5.1.1), whereby one stratum was defined for messages all participants found relevant (the "All-R" stratum), another for messages no participant found relevant (the "All-N" stratum), and others for the various possible cases of conflicting assessment among participants. The operative unit for stratification was the message, and messages were assigned intact (parent email together with all attachments) to strata.</p><p>Samples were composed following the allocation plan sketched above (Section 5.1.1), whereby strata are represented in the sample largely in accordance with their full-collection proportions. An exception to proportionate representation is made in the case of the very large All-N stratum, which is under-represented in the sample relative to its full-collection proportions, thereby allowing each of the R strata to be somewhat over-represented relative to their full-collection sizes. Selection within a stratum was made using simple random selection without replacement. The operative unit for selection into a sample was the message, and any message selected was included intact (parent email together with all attachments) in the sample.</p><p>Tables showing, for each topic, the stratum-by-stratum partitioning of the collection, the samples drawn from each stratum, and the pre-and post-adjudication assessments attached to those samples are provided in an appendix to this document (Appendix A). For purposes of this section, we present, in Table <ref type="table" coords="28,518.03,158.78,8.48,8.80" target="#tab_14">14</ref>, a high-level view of the outcome of the stratification and sample selection process. In the table, we aggregate, for each topic, the totals for each of the individual R strata into a single row (labeled "R Strata," with the number of non-empty individual strata so aggregated noted in parentheses) and present the view of collection and sample composition that results.  The table enables us to make a few observations. First, with regard to the size of samples, we see that the samples are all fairly large. We set out, taking into account the capacity of our review resources, to construct samples that were in the neighborhood of 11,500 documents for each topic. Given that our unit of selection was the message, however, and not the document, the number of documents included in each sample could not be precisely specified in advance. The results of the selection process can be seen in the table. In terms of messages, the samples ranged from 5,779 messages (for Topic 302) to 7,120 messages (for Topic 303), with the average size of a sample being 6,377 messages. In terms of documents, we see that the samples ranged in size from 11,166 documents (Topic 301), to 12,280 documents (Topic 302), with the average size of a sample coming to 11,583 documents.</p><p>Second, comparing the size of the set formed by aggregating the R strata to the size of the All-N stratum, we see, as we saw in the previous section (5.3.2), that, in the full collection, the R strata, collectively, represent a relatively small proportion of the population, representing, depending on topic, between 1% (Topic 302) and 13% (Topic 304) of the messages in the collection. Looking at representation in the sample, on the other hand, we see that, in accordance with our sampling design, the R strata are represented in higher proportions, and the All-N strata in lower proportions, than their full-collection proportions would dictate: the R strata represent between 34% (Topic 302) and 61% (Topic 303) of the messages in the evaluation samples.</p><p>Third, comparing, for each topic, the document-to-message ratio found in the subset formed by the R strata (full collection) to the document-to-message ratio found in the N stratum (also full collection), we see that, for all topics, the document-to-message ratio is higher in the R strata than it is in the N stratum. For Topic 301, the ratio of the ratios is 1.8 (i.e., the document-to message ratio in the aggregated R strata is 1.8 times that in the N stratum); for Topic 302, the ratio of the ratios is 2.4; for Topic 303, 1.1; and, for Topic 304, 1.4. The same trend was observed in the 2009 exercise <ref type="bibr" coords="29,337.85,134.87,9.96,8.80" target="#b5">[6]</ref>. The explanation for the trend could lie in the distribution of relevant documents across messages, in the nature of the retrieval systems evaluated, or in some combination of both. Determining which explanation is correct will require further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Assessment &amp; Adjudication</head><p>As noted above (Section 5.1.1), the Interactive task follows a two-stage assessment protocol, whereby an initial relevance assessment is made of each document in each evaluation sample and then a selection of those first-pass assessments are escalated to the pertinent Topic Authority for final adjudication. In this section, we summarize the results of assessment and adjudication for the 2010 exercise.</p><p>First-Pass Assessment. Once the evaluation samples were drawn, they were made available to review teams for first-pass assessment. The review teams, for the 2010 exercise, were all staffed by commercial providers of document-review services. Four firms volunteered their services, with each firm taking responsibility for the review of the sample for one of the four 2010 topics.</p><p>In order to conduct their reviews, the review teams were provided with detailed assessment guidelines (compiled largely from the relevance guidance that the Topic Authority had provided the participants in the course of the exercise). In addition, at the outset of each review team's work, an orientation call was held with the Topic Authority for the team's topic; on the call, the Topic Authority outlined his or her approach to the topic, and the review team had the opportunity to ask any initial questions it had regarding the relevance criteria to be applied in assessing documents. Finally, once the review got under way, an email channel was opened, whereby the review team could ask the Topic Authority any questions that arose, whether regarding specific documents or regarding the relevance criteria in general, in the course of their assessment of the evaluation sample.</p><p>In assessing their samples, the review teams were instructed to make a relevance judgment (relevant (R) or not relevant (N)) for each document in their samples. A small number of documents in each sample were such as not to permit a relevance judgment by the review team (due, e.g., to errors in the processing of the data or due to non-English content); in these cases, the review teams were instructed to code the document as "broken." Out of the 46,331 documents reviewed across all four topics, 2,608 (5.6% of the total) were found to be "broken" and so counted as non-assessable.</p><p>It should be noted that, although, as described above (Section 5.1.2), each of the samples was organized into 25 batches, with each batch consisting of approximately 500 documents, it is not necessarily the case that the review firms that provided the sample assessments observed the batch organization in assigning documents to individual assessors. Some firms found it to be more in keeping with their usual review procedures simply to take the sample as a whole and then to allocate documents among assessors in the way that they believed would be most efficient. As a result, one should not assume that there exists a one-to-one relation between a given batch and any individual assessor.</p><p>Dual Assessment. As noted above (Section 5.1.2), for the 2010 exercise, we gathered a second independent assessment on a subset of the messages included in each evaluation sample. The dual-assessment subset was chosen by random selection from messages already included in the sample. In order to gather the second assessments, a second instance of each message selected for dual assessment was included in the full set passed to the review team (with the second instance randomly ordered with regard to the first). Both assessments were thus supplied by the same review team; indeed, it is not impossible that, in some cases, the same individual supplied both assessments. What we can say about the two assessments is that they represent distinct assessments of the same message on two different occasions.</p><p>Table <ref type="table" coords="29,114.71,657.36,9.96,8.80" target="#tab_16">15</ref> summarizes the overall rates of agreement achieved on the dual-assessed messages. The table shows, for each topic and for the aggregate of all four topics, (i) the total number of messages included in the dual-assessment subset, (ii) the number of those on which the two assessments were in agreement (both as a count of messages and as a proportion of total), and (iii) the number of those on which the two assessments were in conflict (both as a count of messages and as a proportion of total).  As can be seen from the table, the overall rates of agreement on twice-assessed messages are fairly high, ranging from 87% (Topic 304) to 96% (Topic 302), with an aggregate rate of agreement of 91%. Overall rates of agreement can be misleading, however, in that, in cases in which the vast majority of documents are not relevant (as is typical in e-discovery), a high overall rate of agreement will result simply from the assessors' agreement on the large number of documents that are not remotely relevant. Looking at Topic 302, for example, we see that the assessors for this topic achieved the highest rate of agreement; we also know, however, judging from participant submissions (Table <ref type="table" coords="30,309.24,347.66,8.30,8.80" target="#tab_2">13</ref>), that Topic 302 is likely to be significantly loweryielding than the others (and this hypothesis is borne out by analysis of the post-adjudication assessment data, Section 5.3.5). We cannot say, therefore, whether the higher rate of agreement observed for Topic 302 is simply a function of the lower yield of the topic or genuinely reflects a higher level of consistency, on the part of the Topic 302 assessors, in applying the relevance criteria for that topic.</p><p>What we would like is a metric that is not sensitive to the yield of a topic, and for that we turn to the overlap metric. Overlap is a gauge of interassessor consistency when making positive assessments. More specifically, it tells us the proportion, out of all documents judged relevant by either of the assessors, that are judged relevant by both of the assessors (or, in other words, the proportion that the intersection of the two sets of R assessments represents out of the union of the two sets of R assessments).</p><p>Table <ref type="table" coords="30,114.94,467.21,9.96,8.80" target="#tab_17">16</ref> summarizes the overlap data on the dual-assessed messages. The table shows, for each topic and for the aggregate of all four topics, (i) the total number of dual-assessed messages that were judged relevant on at least one occasion, (ii) the number of dual-assessed messages that were judged relevant on both occasions (both as a count of messages and as a proportion of total), and (iii) the number of dualassessed messages that were judged relevant on one occasion and non-relevant on the other occasion (both as a count of messages and as a proportion of total).</p><p>As can be seen from the table, when we confine our attention to just the dual-assessed messages that received a positive assessment on at least one occasion, we find rates of interassessor consistency that are much lower than the overall rates of agreement: overlap ranges from 31% (Topic 302) to 61% (Topic 303), coming to 50% on the aggregated data. Indeed, the dual-assessment set that showed the highest rate of overall agreement (Topic 302) also showed the lowest rate of overlap, suggesting that the high overall rate was in fact primarily a result of the low yield of the topic. These data are evidence that, while, overall, the assessors almost always agreed on the assessment to assign to a document, once in the neighborhood of potentially relevant documents, the assessors showed a lower rate of agreement as to how the relevance criteria should be applied.</p><p>The conflicts in assessment among the dual-assessed messages are obvious candidates for escalation to the Topic Authority for final adjudication. We next consider the data on the appeal and adjudication of the first-pass assessments.</p><p>Adjudication. As noted above (Section 5.1.2), for the 2010 exercise, the set of first-pass assessments escalated to the Topic Authority for final adjudication derived from two sources: (i) first-pass assessments appealed by one or more of the participants and (ii) non-appealed first-pass assessments.</p><p>With regard to the appeals, once first-pass assessment was complete, participants were provided (i) with the first-pass assessments made on all documents in the evaluation sample, (ii) with the (message-level) probability of selection associated with each document in the sample, and (iii) with preliminary (i.e., preadjudication) estimates of the recall, precision, and F 1 scores achieved in their submitted runs. Participants were then invited to appeal any first-pass assessments that they believed were incorrect (i.e., out of keeping with the Topic Authority's relevance guidance). Participants were asked to submit their appeals at the document level (i.e., on the assessment assigned to a specific parent or attachment). In a departure from previous implementations of the Interactive task, participants were not asked to prepare documentation of the grounds for their appeals; participants simply submitted lists of the IDs of the documents the assessments of which they wished to challenge. There was no limit on the number of assessments that a participant could appeal, and all appealed assessments were included in the set escalated to the Topic Authority for adjudication. Although appeals were made at the document level, messages were included intact in the adjudication set (i.e., if the assessment of any one component of a message was appealed, all components of that message were included in the set sent to the Topic Authority for adjudication).</p><p>With regard to the non-appealed messages included in the adjudication set, these derived from the following sources (all after the exclusion of messages already included in the adjudication set via the appeals process):</p><p>• dual-assessed messages on which the two first-pass assessments were in conflict;</p><p>• dual-assessed messages on which the two first-pass assessments were in agreement;</p><p>• single-assessed messages from the All-N stratum on which the first-pass assessment was Relevant;</p><p>• single-assessed messages from the All-N stratum on which the first-pass assessment was Not Relevant;</p><p>• single-assessed messages from the R-strata.</p><p>In composing the non-appealed subset to be included in the adjudication set, priority was given to cases of (non-appealed) dual-assessment conflict and to cases of R assessment in All-N stratum (i.e., the first and the third of the sources listed above). Selection of messages within each of the subsets was made via simple random selection without replacement, and the unit of selection was the message (not document).</p><p>Table <ref type="table" coords="31,114.33,606.69,9.96,8.80" target="#tab_19">17</ref> summarizes the composition of the adjudication set for each of the 2010 topics. Shown are the number of messages (and documents) included in the set both via the appeals process and via the sampling of non-appealed messages (both as counts and as proportions of the full sample).</p><p>As can be seen from the table, there was a fair amount of topic-to-topic variation in the number of messages appealed. Expressed as a percentage of the messages in the full evaluation sample, appeals ranged from 3.5% of the sample (Topic 304) to 11.3% of the sample (Topic 303), with the average across the four topics coming to 6.6% of the sample. Participants in Topic 303 clearly made the most extensive use of the  As for non-appealed messages included in the adjudication set, the budget for these was largely a matter of the adjudication capacity of the Topic Authorities, and so the numbers are fairly consistent across topics: as a percentage of messages in the full evaluation sample, non-appealed messages included in the adjudication set ranged from 2.6% of the sample (Topic 303) to 4.4% of the sample (Topic 304), with the average across the four topics coming to 3.5% of the sample.</p><p>The full adjudication sets (both appealed and non-appealed) represent fairly sizeable proportions of the evaluation samples. In terms of messages, between 7.9% (Topic 304) and 13.9% (Topic 303) of the evaluation samples were escalated to the Topic Authority for final adjudication.</p><p>Once selected, the adjudication sets were made available to the Topic Authorities for final assessment. In making their assessments, the Topic Authorities had access to the assessment guidelines they had prepared for the first-pass assessors, as well as any other materials they had compiled in the course of their interactions with the participants. The Topic Authorities did not, as noted above, have access to any documentation of the grounds on which a participant was appealing a given first-pass assessment. The Topic Authorities were not, in fact, made aware of the first-pass assessments that initially had been rendered on the documents in their adjudication sets nor were they made aware of which documents had been included in the set via an appeal and which had been otherwise included. Topic Authorities were asked to make their assessments at the document level (taking appropriate account of the context provided by message body and associated attachments, as in the initial assessment stage).</p><p>The results of the adjudication process are summarized in Table <ref type="table" coords="32,378.41,612.41,8.48,8.80" target="#tab_21">18</ref>. For each topic, the table breaks down the results by source (appealed, non-appealed, total), and shows (i) the total number of messages adjudicated from the source, (ii) the total number of messages for which adjudication resulted in no change in the (message-level) assessment (both as a count and as a proportion of all messages adjudicated from the given source), and the total number of messages for which adjudication did result in a change in the assessment (both as a count and as a proportion of all messages adjudicated from the given source).  The table enables a few observations. First, we see that, for appealed messages, the rate at which firstpass assessments were overturned is fairly consistent across topics. Overturn rates range from 36.7% (Topic 304) to 41.8% (Topic 302); on average, about 38% of the messages included in the adjudication set via the appeals process saw a change in assessment.</p><p>Second, we see that some non-appealed messages also saw a change in assessment as a result of adjudication by the Topic Authority. Overturn rates for non-appealed messages range from 11.1% (Topic 302) to 31.0% (Topic 301), with an average across topics of 23.5%.</p><p>Third, while, for all topics, the overturn rate was greater for the appealed messages than it was for the non-appealed messages, the difference in rates was, at least for some of the topics, not as great as we might have expected. Comparing the odds of an overturn for an appealed message to the odds of an overturn for a non-appealed message, we see that, for Topic 301, the odds ratio is 1.31 (the odds of an appealed message having its first-pass assessment overturned are just 1.3 times those of a non-appealed message having its first-pass assessment overturned). For Topic 302, on the other hand, we see a much greater difference in overturn rates: the odds ratio for Topic 302 is 5.76. For Topic 303, the odds ratio is 2.11; for Topic 304, the ratio is 1.39.</p><p>We take a closer look at some of the implications of these adjudication data below (Section 5.4). For now, we turn to participants' final scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Final Results</head><p>Once the Topic Authorities had completed their reviews of their adjudication sets and the sample assessments had been finalized, we were in a position to calculate final estimates of the overall yield for each topic and of the recall, precision, and F 1 achieved in each run submitted by participants. Before turning to those estimates, we add two further notes by way of background to our calculations.</p><p>The first note concerns the derivation of message-level values in cases of partially-assessed messages (i.e., cases in which a message has one or more components that have been assessed as "broken"). In most of these cases, we simply ignore the unjudged components and derive the message-level value on the basis of the judged components: if any judged component is assessed as Relevant, the message counts as Relevant; if all of the judged components are assessed as Not Relevant, the message counts as Not Relevant. There are, however, a small number of cases in which none of the judged components have been assessed as Relevant, but there is at least one unjudged component that has been submitted by a participant as Relevant; in this case, the message counts as Unjudged (rather than Not Relevant), because the component(s) that the participant found Relevant did not receive a definitive assessment.</p><p>The second note concerns the use of the adjudicated assessments. As noted above, some of the (nonappealed) messages included in the adjudication set were chosen via random selection from pertinent subsets of the evaluation sample. For purposes of calculating the estimates reported below, any changes in assessment on such messages that occurred as a result of adjudication affect only the message actually adjudicated: we do not project from the messages selected for adjudication to the larger subsets from which they were drawn. Put another way, the basis for calculating participant scores was simply the set of post-adjudication assessments associated with the full evaluation sample; the procedures used to arrive at those scores are those detailed in the Overview to the 2008 Legal Track <ref type="bibr" coords="34,315.72,230.52,9.96,8.80" target="#b6">[7]</ref>.</p><p>We now turn to the estimates themselves.  As can be seen from the table, our hypothesis, based on the submission data (see Section 5.3.2), that Topic 302 was low yielding has been borne out by the assessment results: we estimate that just 0.1% of the messages in the collection are relevant to this topic (which was on the subject of responses to oil and gas spills). The other three topics, on the other hand, while not extremely high-yielding, do find representation in substantial numbers of emails: 4.2% of the collection is relevant to Topic 301, 2.7% to Topic 303, and 4.4% to Topic 304.</p><p>Table <ref type="table" coords="34,114.76,508.97,9.96,8.80" target="#tab_25">20</ref> reports measures of how effective the participants were at retrieving the messages relevant to each topic. More specifically, the table reports, for each run submitted, estimates of the message-level recall, precision, and F 1 achieved in the run.</p><p>The data presented in the table permit a few observations on the results observed for each topic. With regard to Topic 301, we see that, while some of the submitted runs achieved relatively high levels of precision (with three of the five runs scoring over 50% on the point estimate for this metric), all of the runs found recall a challenge (with no run scoring above 25% on recall). Looking at the relatively high yield of this topic (4.2%), we see that the Topic Authority took a rather broad view of what was relevant to Topic 301; it appears that none of the participants succeeded in capturing what the Topic Authority viewed as the full scope of the topic.</p><p>With regard to Topic 302, the lowest yielding of the topics, we see that participants again scored better on precision than they did on recall: five of the six runs score better than 40% on precision, but none of the six exceed 25% on recall.</p><p>The strongest scores were turned in for Topic 303 (on the subject of lobbying). Five of the six runs for this topic scored above 70% on either precision or recall, and two of the six scored above 50% on both  precision and recall. It appears that participants in this topic were generally more successful in capturing the Topic Authority's understanding of the intent and scope of the topic. For Topic 304, the "privilege" topic, the two runs that scored highest on F 1 did so by achieving relatively high scores on recall (greater than 60%) while scoring lower on precision (less than 35%). Interestingly, these were the two runs that deliberately took a broad view of the topic (see Section 5.2.3); it may be that retrieval efforts that focus on "potentially privileged" (rather than genuinely privileged) are more successful at capturing the genuinely privileged material, even if that comes at the cost of some loss of precision.</p><p>The results for all four topics are summarized in Figure <ref type="figure" coords="35,328.12,533.71,8.48,8.80" target="#fig_3">10</ref>. The figure plots the post-adjudication results for each of the 22 submitted runs on a precision-recall diagram. In the figure, topics are distinguished by shape as per the legend. That the majority of the points lie on the left-hand side of the diagram underlines the fact that the submissions in the 2010 Interactive task generally performed better on the precision metric than they did on recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Further Analysis</head><p>The results of the 2010 Interactive task raise a number of questions that merit further study. In this section, we confine ourselves to a brief look at a few points of interest. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Team-TA Interaction</head><p>Earlier (Section 5.3.1), we saw that there was considerable variation in the amount of time teams chose to spend with the Topic Authorities for the purpose of clarifying the intent and scope of the target topics; times ranged from zero minutes in three instances to 504 minutes in another instance. Such variation prompts the question of whether there is a correlation between the amount of time spent with a Topic Authority and retrieval effectiveness.</p><p>Figure <ref type="figure" coords="36,119.90,456.94,9.96,8.80">11</ref> plots, for each run in the 2010 exercise, the run's retrieval effectiveness (as measured by post-adjudication F 1 scores) against the time spent with the Topic Authority in preparing the run.</p><p>As can be seen from the chart, it is true that the run that scored highest, in terms of F 1 , of any of the 2010 runs (303-EQ), is also the run that utilized the most TA time (504 minutes), suggesting that perhaps there is some correlation between effectiveness and time spent with the Topic Authority. When we look at the other runs, however, it is hard to discern a pattern: some of the runs that scored low on F 1 used a lot of TA time, and some of the runs that scored relatively high on F 1 made limited use of the Topic Authority's time. When we test for the significance of the correlation, moreover, by calculating an estimate and 95% confidence interval for the Pearson product-moment correlation coefficient, we find that our doubts about a correlation are borne out; while the point estimate for the coefficient is positive (0.200), the 95% confidence interval (-0.242, 0.573) includes zero: the data are not evidence of a positive correlation between effectiveness and time spent with the Topic Authority.</p><p>These results, on the question of Team-TA interaction, for the 2010 exercise are similar to those obtained in the 2009 exercise. Evidently, in looking for factors that drive effectiveness, we have to look not merely at quantitative measures of Team-TA interaction but also at qualitative aspects of that interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Resource Utilization</head><p>Apart from the question of how well the amount of time spent with the Topic Authority correlates with retrieval effectiveness, we are also interested in how well other measures of resource utilization correlate With regard to the first measure (overall preparation time), we asked that participants, upon submission of their results, report an estimate of the total person-hours they had spent in preparing their submissions for each topic. In estimating their total hours, participants were asked to include any hours spent either on work specific to a particular topic or on work specific to the loading and analysis of the data set; participants were told not to include any hours spent on the development of tools and methods that, though used for the Interactive task, had general application beyond the exercise. With regard to the second measure (documents reviewed), participants were asked to report the number of documents that their team had manually reviewed in the course of preparing their submission.</p><p>Figure <ref type="figure" coords="37,117.39,410.39,9.96,8.80" target="#fig_4">12</ref> looks at how well each of these measures of resource utilization correlates with effectiveness. The figure shows two charts. In the left-hand chart, we plot, for each run in the 2010 exercise, the run's retrieval effectiveness (as measured by post-adjudication F 1 scores) against the total time spent (in person-hours) in preparing the run. In the right-hand chart, we plot, for each run in the 2010 exercise, the run's retrieval effectiveness (again, as measured by post-adjudication F 1 scores) against the total number of documents manually reviewed in the course of preparing the run,  Simple visual inspection of the two charts finds that, if any of these measures of resource utilization is to correlate well with retrieval effectiveness, that measure is likely to be documents reviewed: the data points on the documents-reviewed chart pattern more tightly in a linear trajectory than do the data points on the preparation-time chart. The hypothesis prompted by visual inspection is borne out by the calculation of estimates and confidence intervals for the Pearson product-moment correlation coefficient. For the correlation between preparation time and F 1 , we estimate the coefficient to be 0.079, with a 95% confidence interval of (-0.354, 0.485): the data are not evidence of a significant correlation between preparation time and effectiveness (as measured by F 1 ). For the correlation between documents reviewed and F 1 , on the other hand, we estimate the coefficient to be 0.721, with a 95% confidence interval of (0.430, 0.876): the data are evidence of a significant correlation between documents reviewed and effectiveness (as measured by F 1 ).</p><p>In this section, together with the previous, we have looked at three variables that might be expected to correlate, positively, with the effectiveness of a submission: time spent interacting with the Topic Authority, overall time spent in preparing a submission, and the number of documents manually reviewed in the course of preparing a submission. Of these three, only one, documents reviewed, has in fact been found to have a significant correlation with the F 1 realized by s submission. This is not to say that the other two factors (interaction with the Topic Authority, preparation time) have no impact on the effectiveness of a retrieval effort; it is rather to say that, if these factors do make a contribution to effectiveness, we have to capture the nature of that contribution in something other than minutes spent with the Topic Authority or hours spent in preparation. We may have to look at the quality, rather than the quantity, of time spent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Adjudication</head><p>The results of the adjudication process always merit further analysis, as such analysis can provide insights into how to make the process more efficient and effective. A full analysis of the results, however, is beyond the scope of this overview; for purposes of this report, we briefly touch on two aspects of the 2010 adjudication results.</p><p>First, with regard to the results for the appealed messages that were included in the adjudication sets, we noted above (Section 5.3.4) that a fair number of these saw a change in assessment as a result of the adjudication process: on average, the assessments on about 38% of the messages included in the adjudication sets via appeal were overturned. We also noted above, that, for the 2010 exercise, we did not provide the Topic Authorities with participant-prepared documentation of the grounds for their appeals. Now, in previous years, we did provide the Topic Authorities with such documentation, and, in previous years, we also observed higher rates of overturn on appealed documents, with overturn rates regularly exceeding 70% (see the 2009 Track Overview <ref type="bibr" coords="38,202.84,478.03,10.29,8.80" target="#b5">[6]</ref>). A question for further study, therefore, is whether the absence of appeals documentation, in the 2010 exercise, resulted, on occasion, in a Topic Authority's missing a salient feature of a document to which such documentation could have directed his or her attention.</p><p>Second, with regard to the results for the non-appealed messages that were included in the adjudication sets, we noted above that the assessments on some of these were also overturned: averaging across the four topics, the assessments on about 23.5% of non-appealed messages included in the adjudication sets were changed as a result of the adjudication process. We now look at how the overturn rates vary by the specific source from which the non-appealed messages were drawn.</p><p>Recall that the non-appealed messages included in the adjudication sets were drawn from five sources: (i) dual-assessment conflicts; (ii) dual-assessment agreements; (iii) single-assessment R's from the All-N stratum; (iv) single-assessment N's from the All-N stratum; and (v) single-assessment R's and N's from the R strata. Table <ref type="table" coords="38,100.06,609.54,9.96,8.80" target="#tab_1">21</ref> breaks down the adjudication results by source. The table shows the overturn rate observed on messages from each source, as well as, in parentheses, the number of overturns over the number of messages adjudicated. Results are shown for each topic and for the aggregate results of all four topics.</p><p>As can be seen from the table, of the non-appealed messages included in the adjudication sets, those with the highest overturn rate are the dual-assessment conflicts: in aggregate, over 50% of the messages included from this source saw a change in assessment as a result of adjudication. This is not surprising, given that a conflicting assessment had already been rendered on each of these. The messages with the second highest</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,130.02,679.24,351.96,8.80"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ROC curves for topics 204 (top) and 205 (bottom), best run per team.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,130.02,679.24,351.96,8.80"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ROC curves for topics 206 (top) and 207 (bottom), best run per team.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="25,224.98,663.37,162.05,8.80"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Team-TA interaction time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="36,195.55,336.99,220.90,8.80"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Interactive runs -recall and precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="37,111.51,667.97,388.98,9.71"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Interactive runs -F 1 vs. Preparation Time and F 1 vs. Documents Reviewed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,94.82,217.31,422.36,240.62"><head>Table 1 :</head><label>1</label><figDesc>Run tags and group names for TREC 2010 Learning and Interactive task participants.</figDesc><table coords="2,94.82,217.31,422.36,218.81"><row><cell cols="3">Learning Task Interactive Task Participating Group</cell></row><row><cell></cell><cell>CS</cell><cell>Clearwell Systems Inc.</cell></row><row><cell></cell><cell>SF</cell><cell>University of South Florida, IS/DS Department</cell></row><row><cell></cell><cell>IS</cell><cell>Indian Statistical Institute, Kolkata</cell></row><row><cell>ITD</cell><cell>IT</cell><cell>IT.com, Inc.</cell></row><row><cell></cell><cell>UW</cell><cell>University of Waterloo (Clarke)</cell></row><row><cell></cell><cell>IN</cell><cell>Integreon Discovery Solutions</cell></row><row><cell>rmit</cell><cell>UM</cell><cell>RMIT University and University of Melbourne</cell></row><row><cell></cell><cell>LA</cell><cell>Los Alamos National Laboratory</cell></row><row><cell></cell><cell>MM</cell><cell>Waterford Technologies (MailMeter)</cell></row><row><cell></cell><cell>EQ</cell><cell>Equivio</cell></row><row><cell></cell><cell>UB</cell><cell>University at Buffalo, State University of New York</cell></row><row><cell>Bck</cell><cell>CB</cell><cell>Backstop LLP and Cleary Gottlieb Steen and Hamilton LLP</cell></row><row><cell>DUTH</cell><cell></cell><cell>Democritus University of Thrace, Greece</cell></row><row><cell>tcd</cell><cell></cell><cell>TCDI</cell></row><row><cell>xrce</cell><cell></cell><cell>XEROX</cell></row><row><cell>ot</cell><cell></cell><cell>Open Text Corporation</cell></row><row><cell>URSK</cell><cell></cell><cell>Ursinus College</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,179.22,73.90,253.57,129.43"><head>Table 2 :</head><label>2</label><figDesc>Seed set sizes for the TREC 2010 Learning Task.</figDesc><table coords="4,216.96,73.90,178.08,107.63"><row><cell cols="4">Topic Relevant Not Relevant Total</cell></row><row><cell>200</cell><cell>230</cell><cell>621</cell><cell>851</cell></row><row><cell>201</cell><cell>168</cell><cell>523</cell><cell>691</cell></row><row><cell>202</cell><cell>1006</cell><cell>403</cell><cell>1409</cell></row><row><cell>203</cell><cell>67</cell><cell>892</cell><cell>959</cell></row><row><cell>204</cell><cell>59</cell><cell>1132</cell><cell>1191</cell></row><row><cell>205</cell><cell>333</cell><cell>1506</cell><cell>1839</cell></row><row><cell>206</cell><cell>19</cell><cell>336</cell><cell>355</cell></row><row><cell>207</cell><cell>80</cell><cell>511</cell><cell>591</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.00,73.90,468.00,80.02"><head>Table 3 :</head><label>3</label><figDesc>Average stratum sizes, sample sizes, and sampling rates, over all topics, for Learning Task evaluation.</figDesc><table coords="5,183.67,73.90,244.67,58.21"><row><cell cols="4">Stratum Stratum Size Sample Size Sampling Rate</cell></row><row><cell>100</cell><cell>1063.0</cell><cell>1063.0</cell><cell>1.0</cell></row><row><cell>1000</cell><cell>7813.3</cell><cell>551.9</cell><cell>0.07</cell></row><row><cell>10000</cell><cell>73182.0</cell><cell>551.9</cell><cell>0.007</cell></row><row><cell cols="2">1000000 603533.6</cell><cell>553.2</cell><cell>0.0009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,123.37,195.15,365.26,8.80"><head>Table 4 :</head><label>4</label><figDesc>Estimated total number of relevant documents (C.I.=Confidence Interval).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,280.60,648.61,77.36,18.91"><head>Table 5 :</head><label>5</label><figDesc>Recall (%) at k=20,000 (3% cut).</figDesc><table coords="6,280.60,648.61,77.36,18.91"><row><cell>1 recall +</cell><cell>precision 1</cell><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,208.60,674.57,194.79,8.80"><head>Table 6 :</head><label>6</label><figDesc>Recall (%) at k=50,000 (7.5% cut).</figDesc><table coords="8,109.28,80.09,393.43,267.82"><row><cell></cell><cell>Topic</cell><cell></cell><cell></cell><cell></cell><cell>Avg</cell></row><row><cell>Run</cell><cell>200 201 202 203 204 205</cell><cell>206</cell><cell cols="3">207 Actual Est Acc</cell></row><row><cell>xrceLogA</cell><cell cols="3">33.7 94.5 91.0 82.5 69.9 51.4 78.5 93.0</cell><cell>74.3</cell><cell>79.0 94.1</cell></row><row><cell>xrceCalA</cell><cell cols="3">33.7 94.5 91.0 82.5 69.9 51.4 78.5 93.0</cell><cell>74.3</cell><cell>76.9 96.7</cell></row><row><cell>otL10rvlT</cell><cell cols="3">29.4 86.7 60.5 76.2 65.5 58.4 100.6 82.7</cell><cell>70.0</cell><cell>90.3 77.6</cell></row><row><cell>rmitindA</cell><cell cols="3">67.3 88.2 68.4 95.9 74.5 58.1 80.4 18.8</cell><cell>69.0</cell><cell>33.5 48.5</cell></row><row><cell>otL10FT</cell><cell cols="3">99.8 74.4 73.5 93.7 37.5 61.8 70.5 18.8</cell><cell>66.2</cell><cell>83.6 79.2</cell></row><row><cell>BckBigA</cell><cell cols="3">42.3 73.4 70.1 60.4 64.8 56.9 61.2 73.1</cell><cell>62.8</cell><cell>28.3 45.1</cell></row><row><cell>BckExtA</cell><cell cols="3">41.0 73.4 70.1 59.9 64.8 56.8 61.2 73.1</cell><cell>62.5</cell><cell>28.5 45.6</cell></row><row><cell cols="4">DUTHsdtA 41.1 85.6 70.0 85.5 65.3 58.0 72.9 16.3</cell><cell>61.8</cell><cell>88.1 70.2</cell></row><row><cell cols="4">DUTHsdeA 41.1 85.6 70.0 85.5 65.3 58.0 72.9 16.3</cell><cell>61.8</cell><cell>68.2 90.7</cell></row><row><cell cols="4">DUTHlrgA 41.1 85.6 70.0 85.5 65.3 58.0 72.9 16.3</cell><cell>61.8</cell><cell>93.1 66.4</cell></row><row><cell>otL10bT</cell><cell cols="3">45.5 81.0 63.7 83.2 36.0 55.5 97.7 24.8</cell><cell>60.9</cell><cell>97.3 62.6</cell></row><row><cell cols="4">xrceNoRA 25.8 64.8 70.0 66.8 31.7 49.4 72.7 85.7</cell><cell>58.4</cell><cell>60.2 96.9</cell></row><row><cell>BckLitA</cell><cell cols="2">40.9 72.5 72.8 88.4 30.6 56.1 60.3</cell><cell>8.9</cell><cell>53.8</cell><cell>29.2 54.3</cell></row><row><cell>rmitmlsT</cell><cell cols="3">15.9 42.4 67.1 57.4 37.0 36.9 59.2 15.3</cell><cell>41.4</cell><cell>47.3 87.6</cell></row><row><cell>tcd1</cell><cell cols="3">20.9 35.1 16.9 40.4 45.9 22.0 74.2 70.5</cell><cell>40.7</cell><cell>34.3 84.2</cell></row><row><cell>rmitmlfT</cell><cell cols="3">23.5 42.3 65.6 28.1 39.9 36.6 58.0 13.3</cell><cell>38.4</cell><cell>44.9 85.5</cell></row><row><cell>URSLSIT</cell><cell cols="3">31.0 19.8 19.4 42.1 43.6 13.3 74.7 20.9</cell><cell>33.1</cell><cell>65.4 50.6</cell></row><row><cell cols="4">URSK70T 31.0 19.0 10.7 42.1 40.8 13.3 74.7 17.1</cell><cell>31.1</cell><cell>78.3 39.7</cell></row><row><cell cols="4">URSK35T 36.2 16.2 13.3 37.0 33.9 13.2 76.5 16.0</cell><cell>30.3</cell><cell>72.8 41.6</cell></row><row><cell>ITD</cell><cell>0.0 34.2 86.7 11.2 18.2 29.2</cell><cell>8.3</cell><cell>53.9</cell><cell>30.2</cell><cell>36.3 83.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,111.77,361.53,388.45,299.42"><head>Table 7 :</head><label>7</label><figDesc>Recall (%) at k=100,000 (15% cut).</figDesc><table coords="8,111.77,393.13,388.45,267.83"><row><cell></cell><cell>Topic</cell><cell></cell><cell>Avg</cell></row><row><cell>Run</cell><cell cols="3">200 201 202 203 204 205 206 207 Actual Est Acc</cell></row><row><cell>xrceLogA</cell><cell>77.9 97.1 97.8 91.3 73.8 66.7 77.8 92.0</cell><cell>84.3</cell><cell>88.9 94.8</cell></row><row><cell>xrceCalA</cell><cell>77.9 97.1 97.8 91.3 73.8 66.7 77.8 92.0</cell><cell>84.3</cell><cell>82.8 98.2</cell></row><row><cell>otL10FT</cell><cell>97.9 89.2 96.6 97.7 68.8 81.1 88.4 21.7</cell><cell>80.2</cell><cell>96.6 83.0</cell></row><row><cell cols="2">DUTHsdtA 90.6 90.9 72.1 97.5 98.0 80.9 88.2 18.7</cell><cell>79.6</cell><cell>90.1 88.3</cell></row><row><cell cols="2">DUTHsdeA 90.6 90.9 72.1 97.5 98.0 80.9 88.2 18.7</cell><cell>79.6</cell><cell>82.5 96.5</cell></row><row><cell cols="2">DUTHlrgA 90.6 90.9 72.1 97.5 98.0 80.9 88.2 18.7</cell><cell>79.6</cell><cell>96.3 82.6</cell></row><row><cell>otL10rvlT</cell><cell>39.8 88.3 64.5 83.4 85.2 82.9 99.6 86.6</cell><cell>78.8</cell><cell>98.9 79.7</cell></row><row><cell>BckExtA</cell><cell>78.9 74.0 75.4 71.4 67.5 75.4 85.0 80.9</cell><cell>76.1</cell><cell>49.7 65.4</cell></row><row><cell>BckBigA</cell><cell>80.7 74.1 75.4 66.4 67.4 75.4 85.0 80.9</cell><cell>75.7</cell><cell>49.6 65.5</cell></row><row><cell>rmitindA</cell><cell>72.9 92.3 72.5 98.0 79.2 85.0 80.9 19.8</cell><cell>75.1</cell><cell>53.4 71.1</cell></row><row><cell>tcd1</cell><cell>67.2 55.3 85.0 76.1 76.2 53.3 98.8 87.4</cell><cell>74.9</cell><cell>55.9 74.6</cell></row><row><cell cols="2">xrceNoRA 83.2 73.5 76.1 79.7 35.2 58.7 78.9 92.0</cell><cell>72.2</cell><cell>73.3 98.5</cell></row><row><cell>otL10bT</cell><cell>52.4 88.4 67.8 84.5 49.5 65.6 98.3 51.1</cell><cell>69.7</cell><cell>99.2 70.3</cell></row><row><cell>BckLitA</cell><cell>44.1 74.9 75.7 85.4 42.5 77.8 63.1 11.7</cell><cell>59.4</cell><cell>49.7 83.6</cell></row><row><cell>rmitmlsT</cell><cell>66.6 58.6 72.2 64.5 45.6 54.2 61.8 16.3</cell><cell>55.0</cell><cell>70.7 77.8</cell></row><row><cell>rmitmlfT</cell><cell>68.7 57.2 70.4 47.6 47.5 52.8 62.3 15.7</cell><cell>52.8</cell><cell>67.1 78.6</cell></row><row><cell>ITD</cell><cell>0.0 44.8 88.3 19.5 41.6 36.1 26.4 74.7</cell><cell>41.4</cell><cell>54.1 76.5</cell></row><row><cell cols="2">URSK70T 51.0 18.9 13.0 44.5 62.2 24.6 88.9 22.6</cell><cell>40.7</cell><cell>91.0 44.7</cell></row><row><cell>URSLSIT</cell><cell>51.0 21.0 21.1 44.5 50.6 24.6 88.9 22.5</cell><cell>40.5</cell><cell>83.5 48.5</cell></row><row><cell cols="2">URSK35T 51.3 25.1 15.2 45.1 40.5 27.7 91.8 18.3</cell><cell>39.4</cell><cell>93.3 42.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,207.59,674.57,196.83,8.80"><head>Table 8 :</head><label>8</label><figDesc>Recall (%) at k=200,000 (30% cut).</figDesc><table coords="9,85.00,80.20,405.81,286.90"><row><cell></cell><cell>99.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell>% Recall</cell><cell>50.00</cell><cell></cell><cell></cell><cell>BckExtA DUTHsdtA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>URSK70T</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>otL10FT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>rmitindA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tcd1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>xrceLogA</cell></row><row><cell></cell><cell>10.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.00</cell><cell>10.00</cell><cell>50.00</cell><cell>90.00</cell><cell>99.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell>% False Positive Rate</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="18,72.00,73.90,468.00,301.59"><head>Table 10</head><label>10</label><figDesc></figDesc><table coords="18,148.35,73.90,315.29,267.83"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Topic</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="9">200 201 202 203 204 205 206 207 Avg</cell></row><row><cell>xrceCalA</cell><cell cols="9">17.7 31.6 64.9 30.4 20.0 38.6 4.7 88.8 37.1</cell></row><row><cell>xrceLogA</cell><cell cols="9">3.8 27.1 50.7 33.4 26.0 43.7 4.8 89.9 34.9</cell></row><row><cell cols="10">xrceNoRA 15.1 27.0 57.6 22.0 13.4 35.2 1.8 21.4 24.2</cell></row><row><cell>otL10FT</cell><cell>8.2</cell><cell cols="8">6.8 33.2 25.8 9.5 40.6 6.2 16.9 18.4</cell></row><row><cell>otL10bT</cell><cell cols="9">20.0 15.1 24.9 16.8 6.4 34.2 4.1 16.1 17.2</cell></row><row><cell cols="2">DUTHsdtA 5.7</cell><cell cols="8">6.0 25.6 16.6 4.4 20.9 4.8 12.3 12.0</cell></row><row><cell>otL10rvlT</cell><cell>0.9</cell><cell cols="3">6.0 14.4 7.3</cell><cell cols="5">6.7 28.5 4.7 20.6 11.1</cell></row><row><cell>DUTHlrgA</cell><cell>4.2</cell><cell cols="8">6.4 18.4 17.6 3.3 24.8 3.5 10.5 11.1</cell></row><row><cell>rmitmlsT</cell><cell>0.4</cell><cell cols="8">3.6 17.1 12.1 7.1 31.0 2.5 10.6 10.6</cell></row><row><cell>rmitmlfT</cell><cell>0.9</cell><cell cols="3">3.8 22.0 6.8</cell><cell cols="3">3.9 32.6 2.2</cell><cell cols="2">7.4 10.0</cell></row><row><cell>rmitindA</cell><cell>1.0</cell><cell cols="3">4.1 12.0 4.6</cell><cell cols="3">3.1 47.5 0.9</cell><cell>2.3</cell><cell>9.4</cell></row><row><cell>BckExtA</cell><cell>1.0</cell><cell>0.8</cell><cell>2.4</cell><cell>1.4</cell><cell cols="3">3.1 45.0 0.3</cell><cell>7.9</cell><cell>7.7</cell></row><row><cell>BckBigA</cell><cell>1.0</cell><cell>0.8</cell><cell>2.4</cell><cell>1.3</cell><cell cols="3">3.1 44.5 0.3</cell><cell>7.9</cell><cell>7.7</cell></row><row><cell>BckLitA</cell><cell>1.0</cell><cell>0.8</cell><cell>2.4</cell><cell>1.5</cell><cell cols="3">2.9 44.4 0.3</cell><cell>2.1</cell><cell>6.9</cell></row><row><cell>tcd1</cell><cell>1.2</cell><cell>0.8</cell><cell>5.5</cell><cell>1.9</cell><cell cols="5">3.0 28.5 1.0 11.0 6.6</cell></row><row><cell cols="2">DUTHsdeA 1.3</cell><cell cols="3">4.8 25.6 0.7</cell><cell>0.6</cell><cell>1.1</cell><cell cols="3">2.7 12.3 6.1</cell></row><row><cell>URSK70T</cell><cell>2.8</cell><cell>5.3</cell><cell>0.6</cell><cell>7.4</cell><cell>8.5</cell><cell>6.9</cell><cell cols="3">4.8 11.4 6.0</cell></row><row><cell>ITD</cell><cell>0.0</cell><cell>1.1</cell><cell>2.3</cell><cell>1.0</cell><cell cols="5">2.4 16.5 0.2 17.5 5.1</cell></row><row><cell>URSLSIT</cell><cell>2.8</cell><cell>3.4</cell><cell>2.8</cell><cell>7.4</cell><cell>2.3</cell><cell>6.9</cell><cell>4.8</cell><cell>3.8</cell><cell>4.3</cell></row><row><cell>URSK35T</cell><cell>2.1</cell><cell>1.4</cell><cell>3.2</cell><cell>4.7</cell><cell>3.2</cell><cell>3.5</cell><cell>2.9</cell><cell>8.1</cell><cell>3.6</cell></row></table><note coords="18,107.95,354.74,432.04,9.71;18,72.00,366.69,36.21,8.80"><p>: F 1 scores achieved by submitted runs, using submitted probability estimates to estimate optimal cutoff k.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="28,186.71,456.25,238.57,8.80"><head>Table 14 :</head><label>14</label><figDesc>Stratification &amp; sampling -high-level view.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="30,176.63,242.61,258.74,8.80"><head>Table 15 :</head><label>15</label><figDesc>Dual Assessment -Overall Rates of Agreement.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="31,165.29,88.82,275.27,126.22"><head>Table 16 :</head><label>16</label><figDesc>Dual Assessment -Overlap.</figDesc><table coords="31,165.29,88.82,275.27,102.79"><row><cell></cell><cell>Union of</cell><cell cols="4">Assessments Agree Assessments Conflict</cell></row><row><cell>Topic</cell><cell>R Asmnts</cell><cell>Msgs</cell><cell>Of Total</cell><cell>Msgs</cell><cell>Of Total</cell></row><row><cell>301</cell><cell>132</cell><cell>72</cell><cell>0.545</cell><cell>60</cell><cell>0.455</cell></row><row><cell>302</cell><cell>35</cell><cell>11</cell><cell>0.314</cell><cell>24</cell><cell>0.686</cell></row><row><cell>303</cell><cell>189</cell><cell>115</cell><cell>0.608</cell><cell>74</cell><cell>0.392</cell></row><row><cell>304</cell><cell>146</cell><cell>55</cell><cell>0.377</cell><cell>91</cell><cell>0.623</cell></row><row><cell>Total</cell><cell>502</cell><cell>253</cell><cell>0.504</cell><cell>249</cell><cell>0.496</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="32,72.00,331.52,468.00,62.54"><head>Table 17 :</head><label>17</label><figDesc>Adjudication Set -sources. appeals mechanism, collectively appealing more than twice the number of messages appealed by participants in any of the other topics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="33,195.77,331.52,220.45,8.80"><head>Table 18 :</head><label>18</label><figDesc>Adjudication Set -summary of results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="34,72.00,242.47,468.00,146.64"><head></head><label></label><figDesc>Table 19 reports the estimated full-collection yield of relevant messages for each of the four Interactive topics; yield is reported both as a count of messages and as a proportion of the full collection. (Recall that the full collection consisted of 455,449 messages.)</figDesc><table coords="34,187.87,304.51,230.12,84.60"><row><cell></cell><cell cols="2">Relevant Messages</cell><cell cols="2">Of Full Collection</cell></row><row><cell>Topic</cell><cell>Est.</cell><cell>95% C.I.</cell><cell>Est.</cell><cell>95% C.I.</cell></row><row><cell>301</cell><cell>18,973</cell><cell>(16,688, 21,258)</cell><cell>0.042</cell><cell>(0.037, 0.047)</cell></row><row><cell>302</cell><cell>575</cell><cell>(174, 976)</cell><cell>0.001</cell><cell>(0.0004, 0.002)</cell></row><row><cell>303</cell><cell>12,124</cell><cell>(11,261, 12,987)</cell><cell>0.027</cell><cell>(0.025, 0.029)</cell></row><row><cell>304</cell><cell>20,176</cell><cell>(18,427, 21,925)</cell><cell>0.044</cell><cell>(0.040, 0.048)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="34,186.34,403.36,239.32,8.80"><head>Table 19 :</head><label>19</label><figDesc>Estimated yields (C.I.=Confidence Interval).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25" coords="35,162.85,408.24,286.30,9.71"><head>Table 20 :</head><label>20</label><figDesc>Post-adjudication estimates of recall, precision, and F 1 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26" coords="37,2.84,69.21,537.16,242.38"><head></head><label></label><figDesc>Figure 11: Interactive runs -F 1 vs. TA-time.with effectiveness. For the 2010 Interactive task, we looked at two such measures: (i) overall time spent in preparing a submission and (ii) the number of documents reviewed in the course of preparing a submission.</figDesc><table coords="37,2.84,69.21,439.77,199.47"><row><cell>m</cell><cell>TA-Time (in minutes)</cell><cell>F1 (final) 0.256</cell><cell></cell><cell></cell><cell></cell><cell cols="3">F1 vs. Time Spent with TA</cell></row><row><cell></cell><cell></cell><cell>0.052</cell><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>18</cell><cell>0.242</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 84 0 0 60</cell><cell>0.214 0.036 0.031 0.160 0.160 0.180 0.277 0.275 0.631 0.559</cell><cell>F1 (Post-Adjudication)</cell><cell>0.20 0.40 0.60 0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.671</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>56</cell><cell>0.254</cell><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>38 69</cell><cell>0.424 0.228</cell><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell><cell>500</cell><cell>600</cell></row><row><cell></cell><cell></cell><cell>0.249</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Minutes Interacting with TA</cell></row><row><cell></cell><cell></cell><cell>0.324</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.408</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.385</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.126</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="6,87.24,675.79,61.53,8.89;6,139.90,682.18,13.58,5.24;6,154.67,677.64,2.35,7.04"><p>logit(x) = log x 1-x .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The TREC Legal Track could not exist without the countless hours of pro bono effort afforded by the <rs type="funder">Topic Authorities</rs>, and professional and law student reviewers. Special thanks to <rs type="person">John M. Horan</rs>, who acted as a surrogate Topic Authority for all eight topics of the Learning task, and who spent countless hours supervising some fifty students in the completion of 135 review batches. Thanks are due to the students of <rs type="person">Loyola Law School</rs>, who took on the majority of those assignments. The coordinators recognize <rs type="person">Denise Tirrell of Loyola Law School</rs> as the reviewer with the highest accuracy on any batch of documents in the Learning task. In a batch of 496 documents, Denise correctly identified 194 of 198 relevant documents, and 297 of 298 non-relevant documents-an overall accuracy of 99%. For their contributions to the interactive task, we owe special thanks to the professional review firms that provided the pro bono review resources needed to assess the evaluation samples (<rs type="affiliation">BIA, Huron, Daegis, and Aphelion Legal Solutions</rs>) and are especially grateful to our dedicated Topic Authorities, who gave generously of their time both in providing guidance to participants and in adjudicating assessments (<rs type="person">Mira Edelman</rs>, <rs type="person">John F. Curran</rs>, <rs type="person">Robert E. Singleton</rs>, and <rs type="person">Michael Roman Geske</rs>).</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Msg</head><p>overturn rate are the single-assessed messages from the R strata: in aggregate, the assessments on about 24% of the messages drawn from this source were changed as a result of adjudication. This too is perhaps not surprising, given that, in most cases, a message in an R stratum will have been found Relevant by at least one participant and Not Relevant by at least one other participant. Indeed, of the 214 non-appealed messages that saw a change in assessment as a result of adjudication, 195 came from one of the two sources just noted. Fewer messages were drawn from the other sources, but they also had lower rates of overturn, with those rates, on the aggregated results, ranging from 20% (for the All-N R's) down to less than 4% (for the All-N N's; note also that, for two of the four topics, the overturn rate on messages from this source was 0%). Understanding the implications of these results will require further study.</p><p>We look forward to continuing the analysis of the adjudication data, and of the 2010 Interactive task more generally, in other papers and venues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This has been the fifth year of the TREC Legal Track, and our second year of building test collections based on Enron email <ref type="bibr" coords="39,143.60,462.44,10.51,8.80" target="#b2">[3,</ref><ref type="bibr" coords="39,157.82,462.44,7.75,8.80" target="#b5">6,</ref><ref type="bibr" coords="39,169.26,462.44,7.75,8.80" target="#b6">7,</ref><ref type="bibr" coords="39,180.71,462.44,7.01,8.80" target="#b7">8]</ref>. Relevance judgments are now available for 11 topical production requests and now also for privilege. The Legal Track will continue in 2011 with an expanded Learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Sampling &amp; Assessment Tables-Interactive Task</head><p>In this appendix, we present tables that summarize, for each of the four Interactive topics, the results of the sampling and assessment process followed in the 2010 exercise. Each table shows: (i) total messages in each stratum (in the full collection); (ii) total messages sampled from each stratum; (iii) total sampled messages observed to be assessable; (iv) total sampled messages observed to be assessable and relevant (preadjudication); and (v) total sampled messages observed to be assessable and relevant (post-adjudication). It is on the basis of the data contained in these tables that we arrived at the estimates of the message-level recall, precision, and F 1 attained in each run.</p><p>Each table is structured as follows. The leftmost columns represent the relevance values (R = Relevant; N = Not Relevant) from the participant submissions that define each stratum. The right-hand columns show the counts of messages in each stratum; more specifically, the columns show the following data: N = total messages in the stratum; n = total messages sampled from the stratum; a = total sampled messages observed to be assessable; r 1 = total sampled messages observed to be assessable and relevant (pre-adjudication); r 2 = total sampled messages observed to be assessable and relevant (post-adjudication).</p><p>The tables for the four Interactive topics follow.       </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="40,86.71,95.62,188.25,7.92;40,86.71,106.58,356.25,7.92" xml:id="b0">
	<monogr>
		<ptr target="http://trec-legal.umiacs.umd.edu/LT10_Complaint_K_final-corrected.pdf" />
		<title level="m" coord="40,86.71,95.62,184.05,7.92">TREC-2010 Legal Track -Complaint K, 2010</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="40,86.71,121.52,453.29,7.92;40,86.71,132.48,411.13,7.92" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="40,388.87,121.52,151.13,7.92;40,86.71,132.48,67.97,7.92">Interactive Task Guidelines -TREC-2008 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://trec-legal.umiacs.umd.edu/2008InteractiveGuidelines.pdf" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,86.71,147.43,453.29,7.92;40,86.71,158.39,295.19,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="40,324.42,147.43,138.68,7.92">TREC-2006 Legal Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,485.10,147.43,54.89,7.92;40,86.71,158.39,213.14,7.92">The Fifteenth Text REtrieval Conference Proceedings (TREC 2006)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="79" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,86.71,173.33,453.29,7.92;40,86.71,184.29,102.37,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="40,343.15,173.33,160.04,7.92">Receiver operating characteristics curves</title>
		<author>
			<persName coords=""><forename type="first">Viv</forename><surname>Bewick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lic</forename><surname>Cheek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="40,260.18,173.33,65.15,7.92;40,510.02,173.33,29.98,7.92;40,86.71,184.29,17.19,7.92">Statistics review</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="508" to="512" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Critical Care</note>
</biblStruct>

<biblStruct coords="40,86.71,199.23,453.29,7.92;40,86.71,210.19,381.55,7.92" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="40,419.90,199.23,120.09,7.92;40,86.71,210.19,97.40,7.92">Interactive Task Guidelines -TREC-2010 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<ptr target="http://trec-legal.umiacs.umd.edu/itg10_final.pdf" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,86.71,225.14,453.29,7.92;40,86.71,236.09,297.28,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="40,397.34,225.14,142.66,7.92;40,86.71,236.09,21.13,7.92">Overview of the TREC 2009 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,127.66,236.09,228.08,7.92">The Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,86.71,251.04,453.29,7.92;40,86.71,262.00,302.32,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="40,397.34,251.04,142.66,7.92;40,86.71,262.00,21.13,7.92">Overview of the TREC 2008 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,127.66,262.00,175.71,7.92">The Seventeenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2008">2008. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,86.71,276.94,453.29,7.92;40,86.71,287.90,289.04,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="40,401.07,276.94,138.93,7.92;40,86.71,287.90,21.13,7.92">Overview of the TREC 2007 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,127.66,287.90,219.84,7.92">The Sixteenth Text Retrieval Conference (TREC 2007)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,86.71,302.84,453.29,7.92;40,86.71,313.80,453.29,7.92;40,86.71,324.76,20.99,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="40,371.06,302.84,150.93,7.92">Assessor error in stratified evaluation</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Falk</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,86.71,313.80,448.22,7.92">Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
