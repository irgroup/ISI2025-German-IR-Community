<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.40,71.96,328.81,16.59">Overview of the TREC-2010 Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,114.60,116.82,55.31,11.06"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>iadh.ounis@glasgow.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.92,116.82,90.05,11.06"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craig.macdonald@glasgow.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.32,116.82,64.88,11.06"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
							<affiliation key="aff1">
								<orgName type="institution">NIST Gaithersburg</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.40,71.96,328.81,16.59">Overview of the TREC-2010 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C0C543740552443C772CA7C5CB716542</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Blog track aims to investigate the information seeking behaviour in the blogosphere. The track was initiated in 2006, and has used an incremental approach in tackling several search tasks by their level of difficulty. In TREC 2010, the track has investigated two main search tasks:</p><p>• Faceted blog distillation: A blog search task where systems aim to retrieve bloggers (i.e. RSS feeds) that have a recurring and central interest in a topic X <ref type="bibr" coords="1,196.32,306.12,9.51,8.07" target="#b6">[6]</ref>, and which also satisfy a number of facets (or attributes), representing the nature or the quality of the sought blogs (e.g. opinionated, factual) <ref type="bibr" coords="1,280.20,327.00,9.51,8.07" target="#b7">[7]</ref>.</p><p>• Top stories identification: A task that addresses news-related issues on the blogosphere, namely investigating whether the blogosphere can be leveraged to identify the top news stories of a given day in a real-time fashion. The task has also a search diversity flavour, where for a given story, a representative set of blog posts discussing the story from various perspectives <ref type="bibr" coords="1,122.30,408.36,10.43,8.07" target="#b7">[7]</ref> is shown to the user.</p><p>Both tasks this year used the Blogs08 corpus <ref type="bibr" coords="1,230.05,428.76,9.68,8.07" target="#b7">[7,</ref><ref type="bibr" coords="1,242.65,428.76,6.45,8.07" target="#b9">9]</ref>, which is a sample of the blogosphere covering a timespan ranging from January 2008 to February 2009. The Blogs08 collection consists of roughly 1.4M blog feeds and 29M blog posts. In addition, for the purposes of the top stories identification task, a new large corpus of news stories covering the same timespan as Blogs08 has been released by Thomson Reuters. The corpus, called Thomson Reuters Research Collection (TRC2), contains both the headlines and content of over 1.8M news stories.</p><p>The topics for the faceted blog distillation task have been developed and assessed by NIST. On the other hand, for the top stories identification task, a number of dates have been sampled from the range of dates covered by Blogs08 and used as query dates.</p><p>To develop topics for the search diversification component of the top stories identification task, the organisers have selected a set of news stories, for which the participating groups were asked to rank diverse blog posts discussing these stories in the blogosphere. In a marked departure from the usually adopted community judgements, in TREC 2010, the Blog track organisers made a first attempt at using crowdsourcing within TREC, where all runs submitted to the top stories task have been assessed through the use of the Amazon Mechanical Turk (AMT) service.</p><p>A total of 16 different groups participated in the 2010 Blog track, spread across four continents. Many groups attempted both tasks, deploying varying approaches ranging from advanced probabilistic retrieval models, to classification and/or machine learning-driven techniques. The remainder of this paper is structured as follows. Section 2 describes the faceted blog distillation task, and discusses the main obtained results by the participating groups. Section 3 describes the top stories identification task and its corresponding results. Concluding remarks are provided in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FACETED BLOG DISTILLATION TASK</head><p>The blog distillation task was first introduced in TREC 2007 <ref type="bibr" coords="1,543.08,267.47,9.63,8.07" target="#b6">[6]</ref>. Blog search users often wish to identify blogs about a given topic X, which they can subscribe to and read on a regular basis in their RSS reader. For a given topic X, a retrieval system aims to find blogs that are principally devoted to X over the timespan of the collection. An overview of the retrieval techniques used in the TREC Blog track to build such systems can be found in <ref type="bibr" coords="1,496.27,330.23,9.68,8.07" target="#b6">[6,</ref><ref type="bibr" coords="1,508.51,330.23,6.68,8.07" target="#b9">9,</ref><ref type="bibr" coords="1,517.87,330.23,10.59,8.07" target="#b14">14]</ref>. However, in its TREC 2007 &amp; 2008 incarnations, the blog distillation task only focused on topical relevance. It did not address the quality aspect or the nature of the retrieved blogs.</p><p>Inspired by a position paper by Hearst et al. <ref type="bibr" coords="1,479.56,372.11,10.43,8.07" target="#b4">[4]</ref> in TREC 2009 <ref type="bibr" coords="1,544.76,372.11,9.63,8.07" target="#b7">[7]</ref>, we proposed a refinement of the blog distillation task that takes into account a number of facets that allow the filtering of blogs according to various attributes, such as the authority of the blog, its opinionated nature, the trustworthiness of its authors, or the genre of the blog and its style of writing.</p><p>As detailed in <ref type="bibr" coords="1,380.52,434.87,9.51,8.07" target="#b7">[7]</ref>, the faceted blog distillation task mimics an exploratory search task. Each facet has one or more inclinations, which allow the user to specify the way in which a facet restriction should be applied. For example, a user might be interested in blogs to read about a topic X, but where the blogger is regarded as trusted -in this case, the facet is trustworthiness, and the active inclination is trustworthy. In other words, a user might not be interested in all blogs having a recurring and principal interest in a given topic X, but only those blogs that satisfy the set facet inclinations. The new faceted blog distillation task can therefore be summarised as "Find me a good blog with a principal, recurring interest in X", where the sought quality and nature of the blogs is characterised through a set of facet inclinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Definition and Topics</head><p>The same three facets proposed for the TREC 2009 blog distillation task <ref type="bibr" coords="1,357.96,602.03,10.43,8.07" target="#b7">[7]</ref> have been used again in TREC 2010, all assumed to have binary inclinations for operational simplicity. In particular, the three facets used for TREC 2010 were: Opinionated: Some bloggers may make opinionated comments on the topics of interest, while others report factual information. A user may be interested in blogs, which show prevalence to opinionatedness. For this facet, the inclinations of interest are 'opinionated' vs 'factual' blogs.</p><p>Personal: Companies are increasingly using blogging as an activity for public relations purposes. However, a user may not  wish to read such mostly marketing or commercial blogs, and may prefer instead to keep up with blogs that appear to be written in personal time without commercial influences. For this facet, the inclinations of interest are 'personal' vs 'official' blogs.</p><p>In-depth: Users might be interested to follow bloggers whose posts express in-depth thoughts and analyses on the reported issues, preferring these over bloggers who simply provide quick bites on these topics, without taking the time to analyse the implications of the provided information. For this facet, the inclinations of interest are 'indepth' vs. 'shallow' blogs (in terms of their treatment of the subject).</p><p>NIST has developed 50 new topics for TREC 2010. During the topic development, one appropriate facet was chosen for each topic. In particular, the Opinionated facet has been associated to 17 topics, the Personal facet has been associated to 16 topics, and the In-depth facet has been associated to 17 topics. An example of a topic associated with the Opinionated facet is included in Figure <ref type="figure" coords="2,286.10,549.47,3.34,8.07" target="#fig_0">1</ref>.</p><p>A fundamental objective for the TREC Blog track 2010 faceted blog distillation task was to identify the most effective and robust ranking approaches with respect to a given facet. As a consequence, inspired by the experimental setup used for the TREC 2008 opinion-finding task <ref type="bibr" coords="2,128.28,601.79,13.70,8.07" target="#b14">[14]</ref>, in 2010, the faceted blog distillation task involved two separate sub-tasks:</p><p>• Baseline Blog Distillation: This sub-task consists in ranking 100 blogs that the deployed system assesses to be relevant to a topic, without any consideration of the facet attached to this topic. This task exactly corresponds to the TREC 2007 &amp; 2008 blog distillation tasks <ref type="bibr" coords="2,189.36,672.95,9.68,8.07" target="#b6">[6,</ref><ref type="bibr" coords="2,202.56,672.95,10.59,8.07" target="#b14">14]</ref>, or the "None" facet rankings from TREC 2009 <ref type="bibr" coords="2,174.20,683.39,9.51,8.07" target="#b7">[7]</ref>.</p><p>• Faceted Blog Distillation: In this sub-task, for each topic, systems should supply two rankings of 100 blogs each: one for the first inclination of the facet enabled, and one with the second inclination of the facet enabled. For example, for the Personal facet, the first ranking would have 100 blogs that the system assesses to be 'personal', and the second ranking would have 100 blogs, which the system assesses to be 'official'.</p><p>To aid cross-comparison and to allow participants to study the performance of their specific faceted search approach across a range of different baseline systems, NIST selected three "standard baselines" from the submitted baseline blog distillation runs, which were redistributed to all participants prior to the faceted blog distillation sub-task submission deadline.</p><p>Finally, to permit the future analysis of the difficulty of the topics across the years, as well as to facilitate the investigation of the effect of various training regimes, the participating groups were asked to submit their runs using the 50 new TREC 2010 topics and the 50 old topics from the TREC 2009 faceted blog distillation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Assessments and Pools</head><p>Participating groups were allowed to submit up to 2 runs to the baseline blog distillation sub-task, including a compulsory automatic query-only run. They were then permitted to submit up to 4 runs, which are based on each of their two previously submitted baseline runs (i.e. 4 runs per own baseline, 8 maximum). One of these submitted runs must be an automatic, query-only run. Moreover, to improve the quality of the pool, we encouraged groups to submit manual runs, and to avoid varying the length of the query (with/without description or narrative) from the baseline to the faceted runs, so as to ensure the clarity of their analysis.</p><p>In addition, groups could submit up to 4 runs for each of the provided 3 standard baselines. Hence, in total, each group could submit a maximum of 20 runs (4* (3 standard baselines + 2 own baseline runs)). To aid the cross-comparison of the deployed faceted ranking approaches and to facilitate the analysis of their performance and robustness, the participating groups were encouraged to apply any given facet ranking approach on each of the three standard baselines. For those runs where the system cannot be clearly broken down into baseline and facet-ranking features, the groups were advised to indicate "N/A" as the baseline run.</p><p>Based on observation from the TREC 2009 faceted blog distillation task, where there was no noticeable reduction in the quality of the test collection when only pooling from the baseline runs, the TREC 2010 pool was drawn only from the submitted baseline runs rather than from all baselines and faceted search runs. All baseline runs were pooled to depth 40. Similar to TREC 2009 <ref type="bibr" coords="2,529.30,522.11,9.51,8.07" target="#b7">[7]</ref>, the following scale has been used for the assessment of the returned blogs:</p><p>-1 Not judged. The content of the blog was not examined due to offensive URLs or headers (such documents do exist in the collection due to spam). Although the content itself was not assessed, it is very likely, given the offensive headers, that the blog is irrelevant.</p><p>0 Not relevant. The blog and its posts were examined, and does not contain any interest in the target topic area, or refers to it only in passing (i.e. the blog is not principally about the target X).</p><p>1 Relevant. The blog has a clear principal, and recurring interest in the target X, but it is not relevant to either facet (or both facets). 3 The blog is relevant and is clearly inclined towards the "second" facet inclination ('factual', 'official', or 'shallow').</p><p>All assessments have been conducted by NIST assessors. The current evaluation results are preliminary as some topics do not yet have complete judgments. Of the 50 new topics, 31 have at least one relevant blog for each inclination of the topics' facet. Thus, for the purposes of analyses, all results reported below are for those 31 topics only.</p><p>For the 31 used topics, Table <ref type="table" coords="3,169.70,355.20,4.48,8.07" target="#tab_1">1</ref> shows the breakdown of the relevance assessments of the pooled blogs per-facet, using the relevance levels described above. About 90% of the pooled blogs were judged as irrelevant (a slight difference from the 96% irrelevant blogs found in the TREC 2009's pool).</p><p>In the following, Section 2.3 summarises the main obtained results by the participating groups on the 31 used new topics in the baseline blog distillation sub-task, while Section 2.4 provides an overview of the main results and findings from the corresponding faceted blog distillation sub-task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Baseline Blog Distillation Results</head><p>As mentioned in Section 2.1, the baseline blog distillation subtask is an adhoc task, where no particular faceted search approach is applied. This is akin to a topic-relevance baseline, where all returned blogs judged 1 or above as per the assessment procedure described in Section 2.2 are deemed relevant. The primary measure for evaluating the retrieval performance of the participating groups is the mean average precision (MAP). Other metrics reported are R-Precision (rPrec), binary Preference (bPref), and Precision at 10 documents (P@10). Table <ref type="table" coords="3,149.92,564.72,4.48,8.07" target="#tab_2">2</ref> reports the per-topic best and medians of the submitted baseline blog distillation runs.</p><p>A total of 24 runs were submitted by 13 groups to the baseline blog distillation sub-task, of which there was 1 manual run. Table <ref type="table" coords="3,67.08,606.60,4.48,8.07" target="#tab_4">3</ref> shows the best submitted automatic query-only baseline blog distillation run from each participating group, ranked by MAP. Table <ref type="table" coords="3,67.08,627.47,4.48,8.07" target="#tab_5">4</ref> shows the best performing baseline run from each participating group, regardless of topic type and run type.</p><p>The top performing baseline run was submitted by the BIT group. They treated a blog as a large document where all postings of the blog are concatenated into a virtual document. They then used a language modelling approach to rank the resulting virtual documents. The PKUTM and HIT LTRC groups also deployed a language modelling approach to aggregate blog post scores into blog scores. The ICTNET group used an approach based on ensemble From the 24 submitted baseline runs, NIST selected three standard baselines of varying performances, and made them available to all participating groups. Table <ref type="table" coords="3,436.84,312.48,4.48,8.07" target="#tab_7">5</ref> lists the three selected standard baseline runs, as well as their performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Faceted Blog Distillation Results</head><p>In this section, we summarise the results of the participating groups in the faceted blog distillation sub-task. Since different topics were assessed with respect to different facets, each run is evaluated by averaging its performance over all used 31 topics, but with its performance on a particular topic calculated with respect to the first and second facet inclinations (relevance labels 2 and 3, respectively) appropriate to the topic. For example, for the topic 1154 (Opinionated), we assess the performance of the run on the 'opinionated' and 'factual' inclinations of the facet. More precisely, similar to TREC 2009 <ref type="bibr" coords="3,385.38,449.64,9.51,8.07" target="#b7">[7]</ref>, given that three facets were used in the topics, each run is assessed on its resulting associated 6 rankings (2 rankings per-facet, corresponding to each inclination of the facet).</p><p>A total of 119 runs were submitted by 11 groups to the faceted blog distillation task. Of these, 70 were based on one of the three standard baselines, and 2 runs were manual. Table <ref type="table" coords="3,508.86,501.96,4.48,8.07" target="#tab_3">6</ref> reports the per-topic best and median results for each facet inclination, across all submitted faceted blog distillation runs. Similar to TREC 2009, the median performances varied from a facet to another, with the In-depth facet ('indepth' and 'shallow' inclinations) seemingly the most difficult.</p><p>Table <ref type="table" coords="3,348.36,564.72,4.48,8.07" target="#tab_8">7</ref> selects the best run for each group, which has the best overall Mean Facet MAP, regardless of topic type, used baseline (own or standard), or run type (automatic or manual). Mean Facet MAP is calculated as the mean of Facet MAP over all facet inclinations. In other words, Table <ref type="table" coords="3,431.24,606.60,4.48,8.07" target="#tab_8">7</ref> shows the best deployed system per-group on average on all facet inclinations.</p><p>Table <ref type="table" coords="3,348.24,627.48,4.48,8.07" target="#tab_9">8</ref> provides a summary of the results obtained by the four groups who achieved the best retrieval performances according to the MAP measure on a given facet inclination, i.e. Facet MAP (facet run), regardless of topic length, baseline, or run type. To assess the extent to which the faceted approach of a given run is effective, we compare its retrieval effectiveness on a given facet inclination (i.e. Facet MAP (facet run)) to the facet performance of the corresponding baseline run, which applies no particular facet inclination approach (denoted Facet MAP(baseline run)). <ref type="bibr" coords="3,509.53,711.24,12.35,8.07">For</ref>  Facet MAP(baseline run) for a given facet inclination (e.g. 'opinionated') is the evaluation of the baseline ranking when only the (e.g. 'opinionated') blogs are treated as relevant. This means that Facet MAP(baseline run) is different for each inclination, and is not the same as the figures reported in Tables <ref type="table" coords="4,215.34,476.04,4.48,8.07" target="#tab_4">3</ref> and<ref type="table" coords="4,236.82,476.04,3.34,8.07" target="#tab_5">4</ref>. Increases are only reported when the facet runs did not report "N/A" as their corresponding baseline run. A relative MAP increase in performance indicates that the used faceted search strategy was successful. A relative MAP decrease in performance indicates that the deployed faceted search technique did not help in retrieval (see column Improvement in Table <ref type="table" coords="4,127.32,538.80,3.23,8.07" target="#tab_9">8</ref>). In general, the results show that the best performing runs for each inclination were able to improve over their corresponding baseline. In particular, promising improvements were achieved by the best performing runs for the 'opinionated', 'personal' and 'indepth' inclinations. For the 'factual', 'official' and 'shallow' inclinations, smaller margins of improvements were observed in the strongest runs. Furthermore, we investigated the performance and robustness of a given faceted search approach across all the three provided standard baselines. The more a faceted search approach consistently improves the corresponding faceted retrieval performance of the three provided baselines, the more likely that it is effective and robust. For a fair comparison of the deployed faceted search approaches, we only considered the groups who attempted their faceted search approaches on all and each three provided standard baselines. Table <ref type="table" coords="4,114.12,695.76,4.48,8.07">9</ref> lists the best faceted search approach from each group. The Mean Facet MAP over all facet inclinations is reported for each standard baseline. Approaches are ranked by the average Mean Facet MAP over all three standard baselines. Increases in Mean Facet MAP per-standard baseline are also shown.</p><p>The results in Table <ref type="table" coords="4,399.86,455.16,4.48,8.07">9</ref> show that only one approach, namely the 'hitFeeds ' approach from the HIT LTRC group, has consistently improved upon the faceted performances of the three provided standard baselines. In particular, the HIT LTRC group used a Maximum Entropy Model toolkit to predict the facet inclination of every blog post in a feed. The ULugano group continued deploying their last year's approach based on scoring facets using cross entropy and various tailored lexicons, while the BIT group used SVM facet classifiers as input to a mixture of topic relevance model and facet relevance model constructed by pseudo-relevance feedback, respectively. The uogTr group used a learned voting approach combining over 900 post-level and blog-level features, including lexicons for each facet inclination. The UICIR group used concept-based retrieval to improve recall, and SVM classifiers to detect facets from these concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TOP NEWS STORIES TASK</head><p>The top stories identification task was first run as a pilot task in TREC 2009 to address the news dimension of the blogosphere as detailed and motivated in <ref type="bibr" coords="4,413.04,659.88,9.51,8.07" target="#b7">[7]</ref>. In particular, it addresses whether the blogosphere can be used to identify the most important news stories for a given day. The task involves two aspects: • Identifying relevant blog posts for a given news story, that cover different/diverse aspects or opinions -the News Blog Post Ranking Task.</p><p>Differently from TREC 2009, the top stories identification task involved using a set of five standardised news categories (World, US, Sport, Science &amp; Technology, Business) and, more importantly, was defined as an online event detection <ref type="bibr" coords="5,225.48,398.40,13.70,8.07" target="#b18">[18]</ref>, i.e. it mimics a real-time search environment. To allow the components of participating systems to be evaluated independently, the task involved two stages: in the first stage, the participating groups aim to identify the top news stories for a given day. Once this task is completed, in the second stage, using a common set of top stories, the participating systems aim to identify and rank a diverse set of blog posts discussing each story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition and Topics</head><p>In addition to the Blogs08 corpus, the participating groups were provided with a large new sample of news stories from throughout the timespan of the Blogs08 corpus. For the TREC Blog track 2010, Thomson Reuters has released the TRC2 newswire corpus, which contains both the headlines and content of over 1.8M news stories, and is distributed by NIST free of charge. The TRC2 corpus replaced the smaller New York Times (NYT) headline corpus used in TREC 2009.</p><p>As mentioned above, a further change from TREC 2009 is the use of categories, where the participating systems were asked to identify the top stories for a given category. In TREC 2010, the following five categories were used from a United States' perspective:</p><p>• World -all international news, including political news outside of USA.</p><p>• U.S. -all general United States news, including politics.</p><p>• Sport -all sport news.</p><p>• Sci.Tech -all technology/IT news as well as science/environment etc. • Business -all finance/economics/business news.</p><p>Importantly, as stressed above and differently from TREC 2009, the top story identification task was treated as an online event detection, thereby enforcing a real-time search scenario. To facilitate this, the organisers provided common timestamp information for each story (i.e. headline + content) in the TRC2 corpus, each blog post in the Blogs08 corpus, and each date query. In particular, the timestamp is an integer representing the number of days elapsed since 14th January 2008 (the 1st day of the Blogs08 corpus).</p><p>For the story ranking task, and in response to a date query, systems should provide a ranking of 100 news stories that they think were important on the specified day (as defined by matching the timestamps between the topic and the news story), for each of the five provided categories of news. When ranking stories, because of the real-time nature of the tackled task, the participating groups were required to only use evidence from blog posts which were published at or before the timestamp of the date query, i.e. blog post evidence from after the date query timestamp cannot be used to identify top news.</p><p>Figure <ref type="figure" coords="5,351.50,648.47,4.48,8.07" target="#fig_1">2</ref> details the format of a TRC2 story, where the DOCNO tag contains the unique identifier of the story that the system should return; the BLOGS08DAY tag contains the integer timestamp described above; the HEADLINE and CONTENT tags contain the headline and content of the story, as provided by Thomson Reuters. Figure <ref type="figure" coords="5,351.86,700.67,4.48,8.07" target="#fig_2">3</ref> provides an example of topic illustrating a date query. Only the TRC2 news stories with the same value in the BLOGS08DAY  tag as the topic has in the blogs08day tag should be ranked in response to a date query. For example, for a topic with &lt;blogs08day&gt; 5 &lt;/blogs08day&gt;, the participating systems should only rank TRC2 news stories with &lt;BLOGS08DAY&gt; 5 &lt;/BLOGS08DAY&gt;, using blog post evidence from Blogs08, which have timestamp ≤ 5. A total of 50 new query dates were randomly sampled from across the timespan of the Blogs08 corpus. The selected dates have a balanced coverage of the months of the Blogs08 collection, as well as the seven days of the week. After the submission of the story ranking task runs, for the purposes of the news blog post ranking task, the organisers selected 68 news stories covering the five categories for which relevant and diverse blog posts should be identified by the participating systems. In particular, for each news story, the participating systems were asked to provide 3 rankings of 50 blog posts, which should be relevant to the news story, and discuss the different aspects of the news stories (e.g. different opinions, type of blog posts, etc). To investigate how blog postings about a story evolve over time, each of the three required rankings is centred at a different period of time:</p><p>1. Before the timestamp of the "query date". i.e. blog posts must have timestamp ≤ query timestamp 2. One day after the "query date". i.e. blog posts must have timestamp ≤ query timestamp + 1 day 3. One week after the timestamp. i.e. blog posts must have timestamp ≤ query timestamp + 7 days</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Assessments and Pools</head><p>Participating groups were allowed to submit up to 3 runs for the story ranking task. Each run consists of a ranking of 100 news stories for each news category on each query date (i.e. 5 rankings for each given query date). A total of 18 runs were received from 5 groups, including one manual run.</p><p>Pools were created using stratified sampling, as defined for the statMAP measure <ref type="bibr" coords="6,384.23,637.92,9.51,8.07" target="#b2">[2]</ref>. In particular, 32 news stories for each category and day were sampled from the headlines ranked in the top 30 by any of the submitted runs. In a marked departure from the usually adopted community judgements within TREC, we made a first attempt at using crowdsourcing for assessing all the generated pools in this task. In particular, we employed over 720 unique workers from the Amazon's Mechanical Turk to judge 7,427 stories spanning 50 query dates and 5 news categories.  <ref type="table" coords="7,77.16,171.72,3.71,8.07">9</ref>: For each group, the best set of runs for each group, applied over all three standard baselines. In an approach, denotes the part of the run name representing the used standard baseline. Some groups did not submit faceted runs using all three baselines.</p><p>was shown the pool of news stories for a given category on a given day, and asked to judge each news story as one of the following:</p><p>Important and correct category: This is a big story, which should be ranked highly for this category.</p><p>Not important but correct category: This story is not very important and should be ranked lower.</p><p>Wrong category: This story could be either important or not, but it doesn't matter because it doesn't fit into this category.</p><p>To assure quality, we used best practises in crowdsourcing <ref type="bibr" coords="7,282.80,326.28,10.08,8.07" target="#b3">[3,</ref><ref type="bibr" coords="7,53.76,336.72,10.59,8.07" target="#b16">16]</ref>, whereby each story was judged by three independent workers, resulting in over 24,000 individual judgments. The majority vote for each story was taken as the final binary relevance label, with Not important but correct category and Wrong category being collapsed into a single Not important label. The labelling resulted in high levels of between-worker agreement, increasing our confidence in the quality of the results. Furthermore, all judgments were subject to a manual validation before being approved. Poor quality and fraudulent judgments were rejected and the work republished for new workers to attempt.</p><p>Participating groups were also allowed to submit up to 3 runs for the news blog post ranking task. A total of 11 runs from 4 groups were received -all runs were automatic.</p><p>The blog post pools for each of the 68 news stories selected as topics were created from the top 20 blog posts ranked in the preferred run submitted by each group. This resulted in a pool of 7975 blog posts. Each of these blog posts was judged as relevant, possibly relevant or not relevant to the news story they where retrieved for. In summary, each of the 7975 blog posts pooled were judged as to their relevancy to a news story, as shown below.</p><p>Relevant: Story is discussed.</p><p>Possibly relevant: Post could be discussing the story.</p><p>Not Relevant: Story is not discussed. Furthermore, to enable the assessment of the diversity of each ranking produced, all blog posts were also labelled using a number of predefined perspectives that describe each blog post. In particular, to evaluate the range of perspectives that each blog post ranking covers, for the 68 stories, each blog post was also assigned zero or more of the following nine perspectives:</p><p>Factual Account: The post just describes the facts as is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Opinionated Positive:</head><p>The post expresses a viewpoint endorsing some aspect of the story.</p><p>Relevance Level # Stories Not Important 5984 Important 1443</p><p>Table <ref type="table" coords="7,340.56,254.16,7.91,8.07" target="#tab_1">10</ref>: Breakdown of relevance levels for the story ranking task judgements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Opinionated Negative:</head><p>The post criticises some aspect of the story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Opinionated Mixed:</head><p>The post expresses both positive and negative opinions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short summary/Quick bites:</head><p>The post contains only a sentence or two about the story.</p><p>Live Blog: The post was continually updated at the time about the story.</p><p>In-depth analysis: The post goes into significant detail about the story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aftermath:</head><p>The post gives a round-up or retrospective account of the story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictions:</head><p>The post was written before the story and discusses what might happen.</p><p>The relevance assessment phase resulted in high levels of agreement with a gold standard generated by the track organisers, increasing our confidence in the quality of the results. Further details regarding the crowdsourcing of relevance assessments for these tasks can be found in <ref type="bibr" coords="7,394.80,523.08,13.70,8.07" target="#b10">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Story Ranking Task Results</head><p>In this section, we provide an overview of the the results of the story ranking task, namely the effectiveness of the participating systems in identifying the top news for a given query date. The TRC2 corpus contains 1,613,707 newswire stories published by Thomson-Reuters, an average of 4236 stories per day. After our evaluation, 19% of the pooled stories for each day and category were judged to be important. Table <ref type="table" coords="7,444.76,617.04,8.92,8.07" target="#tab_1">10</ref> provides the detailed breakdown of the relevance assessment of the pooled stories.</p><p>Due to the use of stratified sampling, we report the statMAP evaluation measure <ref type="bibr" coords="7,375.62,648.48,10.43,8.07" target="#b2">[2]</ref> for the evaluation of story ranking task runs. Moreover, the TRC2 corpus often has many duplicate stories on a given day, which have been updated with more information as breaking news evolves. was allowed. This ensured that systems which did or did not perform duplicate removal were treated fairly. Table <ref type="table" coords="8,84.48,192.00,8.92,8.07" target="#tab_11">11</ref> reports the best and median statMAP measures for each news category. From this, we note that the U.S. and World categories were the easiest for the systems, perhaps due to their superior coverage in the blogosphere.</p><p>Table <ref type="table" coords="8,84.48,233.88,8.92,8.07" target="#tab_14">12</ref> shows the best submitted run for each group, regardless of run type (automatic or manual) and used TRC2 fields (headline and/or content). They are ranked by the mean statMAP over each of the 5 categories, denoted Mean statMAP. The top-performing run was submitted by POSTECH KLE, and used a probabilistic model that considers events, news stories and blog posts. The ikm100 system used a headline-post network structure to identify important stories. ICTNET treated the headline and content of each news story as a query, and accumulated the BM25 scores for relevant blog posts on each day. The UoS group identified the terms whose frequencies in blog posts increased substantially on the day of the query. These terms where then used as a query to rank the stories, using the Terrier platform and its PL2 weighting model. The uogTr group used a learned voting technique to rank news stories for a day of interest. In particular, the ranking is learned using 1076 voting features, extracted using 8 story representations and varying temporal evidence from the 10 days before the day of interest. Finally, ULugano used a clustering method to identify the most important terms on a given day, which are then used to rank news stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">News Blog Post Ranking Task Results</head><p>In this section, we provide an overview of the results of the News Blog Post Ranking Task, specifically the effectiveness of participating systems at retrieving blog posts related to a news story in a real-time manner. As noted earlier, the track organisers selected 68 news stories, each comprised of a headline, some article content and a date, which act as the topics that systems were to rank blog posts for. For all 68 news stories, participating systems were to return three distinct rankings, representing searches at three points in time relative to the time the story was published. In particular, for each news story, systems were to retrieve blog posts from:</p><p>1. Before the story was published (Real-time) 2. One day after the story was published and before (+1 day) 3. Seven days after the story was published and before (+7 days)</p><p>A total of 11 runs from 4 groups were received. A run was evaluated based upon the number, ranking and diversity of relevant blog posts contained. The primary evaluation metric for the news blog post ranking task is α-Normalised Discounted Cumulative Gain at rank 10 (α-nDCG@10). This measure incorporates both support for the three level graded relevance judgments used, and promotes systems that diversify their rankings in terms of the nine perspectives described earlier in Section 3.2. each run and each type of ranking individually (Real-time, +1 day and +7 days. From this, we note that as time progresses, the effectiveness of systems tends to increase. This is intuitive, as over time, new blog posts discussing each story will be posted. Table <ref type="table" coords="8,347.16,216.24,8.92,8.07" target="#tab_15">14</ref> reports the best run submitted by each of the four groups in terms of mean α-nDCG@10 over all three rankings per run in addition to the α-nDCG@10 score for each ranking type individually. Runs are ranked based upon the mean α-nDCG@10 reported. The best performing run was that submitted by uogTr, and leveraged a learning to rank approach over 81 blog post features to rank blog posts for each story. Notably this run did attempt to diversify the blog post rankings. POSTECH KLE also applied an effective diversification strategy, which considers both the relevance and similarity between a news story and blog posts. The ICTNET system employed an ensemble ranking strategy to rank blog posts but did not apply any diversification. The ikm100 system ranked blog posts based upon the normalised cosine similarity between each news story headline and each blog post considered and did not diversify the rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In its fifth year, the TREC Blog track has tackled advanced tasks in the form of faceted blog distillation and top news identification. In both tasks, compared to TREC 2009, sub-tasks have been formulated that allow the effect of components of participants systems to be evaluated independently. It is of note that the relevance assessments of the top stories identification task have been obtained through crowdsourcing, the first successful attempt of its kind within a TREC track.</p><p>TREC 2010 represents the final year of the Blog track in its current form. Over the past five years, we have developed test collections for several user search tasks on the blogosphere, namely opinion-finding, blog distillation (aka feed search) and top news identification. Two blog corpora have been developed, namely Blogs06 and Blogs08, and two news corpora (NYT and TRC2) have been released for the benefit of the information retrieval (IR) community. We believe that these corpora and test collections will be valuable to the IR researchers and practitioners for sometime to come. In TREC 2011, the Blog track will morph into the Microblog track, and will tackle search tasks prevalent on micro-blogosphere such as social and real-time search. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.76,324.11,239.18,8.07;2,53.76,334.67,233.36,8.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Blog track 2010, faceted blog distillation task, topic 1154. The query tag corresponds to the traditional topic title.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,316.80,404.27,239.08,8.07;5,316.80,414.83,136.13,8.07"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Blog track 2010, story ranking task. Format of a news article in the TRC2 collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,53.76,505.44,239.18,8.07;6,53.76,516.00,239.08,8.07;6,53.76,526.44,183.44,8.07"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Blog track 2010, story ranking task, topic 1 where the num tag contains the topic number and the blogs08day tag contains the integer timestamp described above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,316.80,700.67,239.18,18.63"><head>Table 1 : Breakdown of relevance levels for the faceted blog dis- tillation sub-task.</head><label>1</label><figDesc></figDesc><table coords="3,90.48,55.44,165.77,159.39"><row><cell>Relevance Level</cell><cell cols="3"># Queries # Blogs</cell></row><row><cell>Not Relevant</cell><cell></cell><cell>31</cell><cell>7276</cell></row><row><cell>Relevant (can't tell)</cell><cell></cell><cell>31</cell><cell>88</cell></row><row><cell cols="2">Relevant (opinionated)</cell><cell>7</cell><cell>208</cell></row><row><cell>Relevant (factual)</cell><cell></cell><cell>7</cell><cell>68</cell></row><row><cell>Relevant (official)</cell><cell></cell><cell>10</cell><cell>86</cell></row><row><cell>Relevant (personal)</cell><cell></cell><cell>10</cell><cell>119</cell></row><row><cell>Relevant (indepth)</cell><cell></cell><cell>14</cell><cell>103</cell></row><row><cell>Relevant (shallow)</cell><cell></cell><cell>14</cell><cell>181</cell></row><row><cell></cell><cell>MAP</cell><cell>P@10</cell></row><row><cell>Best</cell><cell cols="2">0.4340 0.6000</cell></row><row><cell cols="3">Median 0.1925 0.3097</cell></row></table><note coords="2,316.80,700.67,239.18,8.07;2,339.24,711.23,204.44,8.07"><p><p>2</p>The blog is relevant and is clearly inclined towards the "first" facet inclination ('opinionated', 'personal', or 'indepth').</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.76,228.60,239.26,18.51"><head>Table 2 : Best and Medians for the baseline blog distillation sub- task.</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,316.80,55.44,239.26,233.79"><head>Table 6 : Best and Medians for the various facets of the submit- ted faceted blog distillation runs. ranking</head><label>6</label><figDesc>, while the uogTr group used an advanced version of their voting model-based approach. The PRIS group also used an approach based on the voting model. Finally, the StanfordNLP system used a probabilistic model that leverages individual blog post evidence to improve blog search.</figDesc><table coords="3,360.72,55.44,151.12,135.99"><row><cell></cell><cell>Facet</cell><cell>MAP</cell><cell>P@10</cell></row><row><cell>Best Median</cell><cell>opinionated</cell><cell cols="2">0.4805 0.5429 0.1275 0.2286</cell></row><row><cell>Best Median</cell><cell>factual</cell><cell cols="2">0.5452 0.3429 0.1259 0.1143</cell></row><row><cell>Best Median</cell><cell>official</cell><cell cols="2">0.5181 0.4100 0.1561 0.1300</cell></row><row><cell>Best Median</cell><cell>personal</cell><cell cols="2">0.4024 0.3900 0.0827 0.1200</cell></row><row><cell>Best Median</cell><cell>indepth</cell><cell cols="2">0.5043 0.3071 0.1408 0.1143</cell></row><row><cell>Best Median</cell><cell>shallow</cell><cell cols="2">0.2941 0.3571 0.0712 0.1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,524.07,711.24,31.73,8.07"><head>Table 3 : Baseline blog distillation sub-task: automatic query-only runs, 1 per group. Ranked by MAP, where relevant is blogs judged</head><label>3</label><figDesc></figDesc><table coords="3,524.07,711.24,31.73,8.07"><row><cell>instance,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,53.76,403.44,502.16,19.23"><head>Table 4 : Baseline blog distillation sub-task: 1 per group. Ranked by MAP, where relevant is blogs judged</head><label>4</label><figDesc></figDesc><table /><note coords="4,461.27,403.44,94.65,8.97;4,53.76,414.60,16.28,8.07"><p>≥ 1. * denotes a manual run.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,330.12,700.07,225.66,19.23"><head>•</head><label></label><figDesc>Identifying top news stories for a given unit of time and category -the Story Ranking Task.</figDesc><table coords="5,75.60,55.44,458.29,50.31"><row><cell cols="2">Std. Baseline Baseline Run</cell><cell cols="2">Baseline Mean Facet MAP MAP</cell><cell cols="5">MAP by Facet opinionated factual official personal indepth shallow</cell></row><row><cell>stdbaseline1</cell><cell>ICTNETBDRun2</cell><cell>0.3197</cell><cell>0.2082</cell><cell>0.2598</cell><cell>0.2693 0.2439</cell><cell>0.1377</cell><cell>0.2345</cell><cell>0.1038</cell></row><row><cell>stdbaseline2</cell><cell>uogTrapeMN5k</cell><cell>0.2024</cell><cell>0.1397</cell><cell>0.1054</cell><cell>0.2068 0.1938</cell><cell>0.0755</cell><cell>0.1309</cell><cell>0.1259</cell></row><row><cell>stdbaseline3</cell><cell>FEUPirlab1</cell><cell>0.1597</cell><cell>0.1170</cell><cell>0.0767</cell><cell>0.1660 0.2014</cell><cell>0.0899</cell><cell>0.0756</cell><cell>0.0923</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,61.22,119.40,489.36,152.01"><head>Table 5 : Performances of the standard baseline runs</head><label>5</label><figDesc></figDesc><table coords="5,61.22,141.18,489.36,130.23"><row><cell>Group</cell><cell>Run</cell><cell>Baseline</cell><cell cols="2">Topic Mean Facet Fields MAP</cell><cell cols="5">MAP by Facet opinionated factual official personal indepth shallow</cell></row><row><cell>BIT</cell><cell>BIT10bl1fd3</cell><cell>BITblog10bl1</cell><cell>Q</cell><cell>0.2537</cell><cell>0.2415</cell><cell>0.2948 0.3301</cell><cell>0.1736</cell><cell>0.3211</cell><cell>0.1610</cell></row><row><cell>ICTNET</cell><cell>ICTNETFBD3</cell><cell>N/A</cell><cell>Q</cell><cell>0.2285</cell><cell>0.2554</cell><cell>0.2670 0.3134</cell><cell>0.1321</cell><cell>0.3042</cell><cell>0.0988</cell></row><row><cell>ULugano</cell><cell>LexMIRuns1</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.2180</cell><cell>0.2656</cell><cell>0.2693 0.2415</cell><cell>0.2121</cell><cell>0.2365</cell><cell>0.0832</cell></row><row><cell cols="2">HIT LTRC hitFeeds1</cell><cell>stdbaseline1</cell><cell>?</cell><cell>0.2089</cell><cell>0.2607</cell><cell>0.2695 0.2464</cell><cell>0.1385</cell><cell>0.2349</cell><cell>0.1035</cell></row><row><cell>PKUTM</cell><cell cols="2">PKUTM121onB1 PKUTMB1</cell><cell>Q</cell><cell>0.1857</cell><cell>0.2807</cell><cell>0.1399 0.1930</cell><cell>0.1636</cell><cell>0.2398</cell><cell>0.0973</cell></row><row><cell>uogTr</cell><cell>uogTrfL919s1</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.1837</cell><cell>0.2440</cell><cell>0.1369 0.2456</cell><cell>0.1017</cell><cell>0.2578</cell><cell>0.1162</cell></row><row><cell>UICIR</cell><cell>uicfbdstd1b</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.1588</cell><cell>0.1938</cell><cell>0.1494 0.1948</cell><cell>0.1215</cell><cell>0.1917</cell><cell>0.1017</cell></row><row><cell>UniNE</cell><cell>run3swnpn10</cell><cell>stdbaseline1</cell><cell>QN</cell><cell>0.1434</cell><cell>0.1590</cell><cell>0.0929 0.2414</cell><cell>0.1293</cell><cell>0.1627</cell><cell>0.0752</cell></row><row><cell>PRIS</cell><cell>PrisStdQE1</cell><cell>stdbaseline1</cell><cell>QDN</cell><cell>0.1253</cell><cell>0.2065</cell><cell>0.2464 0.0052</cell><cell>0.0188</cell><cell>0.1990</cell><cell>0.0757</cell></row><row><cell>PCUHK</cell><cell>Std1stPI</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.1006</cell><cell>0.1504</cell><cell>0.0930 0.1535</cell><cell>0.0789</cell><cell>0.0916</cell><cell>0.0362</cell></row><row><cell>RMIT</cell><cell>rmitfaceted</cell><cell>rmitprob</cell><cell>Q</cell><cell>0.0530</cell><cell>0.0682</cell><cell>0.0246 0.0515</cell><cell>0.0668</cell><cell>0.0662</cell><cell>0.0405</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="5,53.76,285.12,502.32,18.51"><head>Table 7 : Faceted blog-distillation sub-task: Best deployed faceted ranking systems on average on all facets, 1 per group. Ranked by Mean Facet MAP. Run hitFeeds1 did not declare its used topic fields.</head><label>7</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="6,53.76,55.44,502.33,437.25"><head>Table 8 : For each facet, the best faceted blog distillation run from the top four groups sorted by Facet MAP. Facet MAP(baseline run) is the Facet MAP of the baseline ranking on the same facet inclination. * denotes a manual run.</head><label>8</label><figDesc></figDesc><table coords="6,53.76,55.44,501.38,437.25"><row><cell>Group</cell><cell>Run</cell><cell>Baseline</cell><cell cols="4">Topic Fields Facet MAP(baseline run) Facet MAP(facet run) Improvement</cell></row><row><cell></cell><cell></cell><cell></cell><cell>opinionated</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PKUTM</cell><cell>PKUTM111onB1</cell><cell>PKUTMB1</cell><cell>Q</cell><cell>0.1761</cell><cell>0.2807</cell><cell>59.40%</cell></row><row><cell>BIT</cell><cell>BIT10bl2fd3</cell><cell>BITblog10bl2</cell><cell>Q</cell><cell>0.2033</cell><cell>0.2806</cell><cell>38.02%</cell></row><row><cell>ULugano</cell><cell>LexMIRuns1</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.2598</cell><cell>0.2656</cell><cell>2.23%</cell></row><row><cell cols="2">HIT LTRC hitFeeds1</cell><cell>stdbaseline1</cell><cell>?</cell><cell>0.2598</cell><cell>0.2607</cell><cell>0.35%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>factual</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIT</cell><cell>BIT10bl1fd4</cell><cell>BITblog10bl1</cell><cell>Q</cell><cell>0.2976</cell><cell>0.2987</cell><cell>0.37%</cell></row><row><cell>PKUTM</cell><cell>PKUTM211STD1</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.2693</cell><cell>0.2761</cell><cell>2.53%</cell></row><row><cell>ICTNET</cell><cell>ICTNETFBD1</cell><cell>ICTNETBDRun1</cell><cell>Q</cell><cell>0.2563</cell><cell>0.2740</cell><cell>6.91%</cell></row><row><cell cols="2">HIT LTRC hitTDNfeedR *</cell><cell>N/A</cell><cell>QDN</cell><cell>N/A</cell><cell>0.2735</cell><cell>N/A</cell></row><row><cell></cell><cell></cell><cell></cell><cell>official</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIT</cell><cell>BIT10bl1fd4</cell><cell>BITblog10bl1</cell><cell>Q</cell><cell>0.3312</cell><cell>0.3333</cell><cell>0.63%</cell></row><row><cell>ICTNET</cell><cell>ICTNETFBD3</cell><cell>N/A</cell><cell>Q</cell><cell>N/A</cell><cell>0.3134</cell><cell>N/A</cell></row><row><cell>PKUTM</cell><cell>PKUTM123STD1</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.2439</cell><cell>0.2937</cell><cell>20.42%</cell></row><row><cell cols="2">HIT LTRC hitFeeds1</cell><cell>stdbaseline1</cell><cell>?</cell><cell>0.2439</cell><cell>0.2464</cell><cell>1.03%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>personal</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ULugano</cell><cell>LexMIRuns1</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.1377</cell><cell>0.2121</cell><cell>54.03%</cell></row><row><cell>BIT</cell><cell>BIT10std1fd4</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.1377</cell><cell>0.1950</cell><cell>41.61%</cell></row><row><cell>PKUTM</cell><cell>PKUTM111onB2</cell><cell>PKUTMB2</cell><cell>QDN</cell><cell>0.1441</cell><cell>0.1901</cell><cell>31.92%</cell></row><row><cell cols="2">HIT LTRC hitTDNfeedR *</cell><cell>N/A</cell><cell>QDN</cell><cell>N/A</cell><cell>0.1549</cell><cell>N/A</cell></row><row><cell></cell><cell></cell><cell></cell><cell>indepth</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICTNET</cell><cell>ICTNETBD4</cell><cell>N/A</cell><cell>Q</cell><cell>N/A</cell><cell>0.3478</cell><cell>N/A</cell></row><row><cell>BIT</cell><cell>BIT10bl1fd1</cell><cell>BITblog10bl1</cell><cell>Q</cell><cell>0.2153</cell><cell>0.3211</cell><cell>49.14%</cell></row><row><cell>uogTr</cell><cell>uogTrfL728s1</cell><cell>stdbaseline1</cell><cell>Q</cell><cell>0.2345</cell><cell>0.2971</cell><cell>26.70%</cell></row><row><cell>PKUTM</cell><cell>PKUTM111onB1</cell><cell>PKUTMB1</cell><cell>Q</cell><cell>0.1644</cell><cell>0.2407</cell><cell>46.41%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>shallow</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIT</cell><cell>BIT10bl1fd4</cell><cell>BITblog10bl1</cell><cell>Q</cell><cell>0.2104</cell><cell>0.2108</cell><cell>0.19%</cell></row><row><cell>uogTr</cell><cell>uogTrfC919</cell><cell>uogTrLv450</cell><cell>Q</cell><cell>0.1521</cell><cell>0.1496</cell><cell>-1.64%</cell></row><row><cell>UICIR</cell><cell>uicfbdstd2b</cell><cell>stdbaseline2</cell><cell>Q</cell><cell>0.1259</cell><cell>0.1370</cell><cell>8.82%</cell></row><row><cell cols="2">HIT LTRC hitTDNfeedbl *</cell><cell>hitTDNbl</cell><cell>QDN</cell><cell>0.1395</cell><cell>0.1331</cell><cell>-4.59%</cell></row><row><cell>&lt;top&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">&lt;num&gt;TS10-01&lt;/num&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">&lt;date&gt;2008-04-24&lt;/date&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">&lt;day&gt;Wednesday&lt;/day&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">&lt;blogs08day&gt;100&lt;/blogs08day&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;/top&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="7,316.80,679.80,239.18,39.51"><head>Table 11 : Best and medians for the story ranking stage of top news stories identification task, broken down by category.</head><label>11</label><figDesc>To account for these during evaluation, we created equivalence classes of news stories based on headline clustering. Only one news story per equivalence class was judged, and when evaluating runs, only one news story per equivalence class</figDesc><table coords="8,120.60,55.44,105.46,60.75"><row><cell></cell><cell>Best</cell><cell>Median</cell></row><row><cell cols="3">Business 0.3155 0.0314</cell></row><row><cell cols="3">Sci.Tech 0.3009 0.0160</cell></row><row><cell>Sport</cell><cell cols="2">0.4661 0.0927</cell></row><row><cell>U.S.</cell><cell cols="2">0.5958 0.1535</cell></row><row><cell>World</cell><cell cols="2">0.5405 0.0949</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="8,53.76,55.44,446.28,674.31"><head>Table 13 :</head><label>13</label><figDesc>Table 13  reports the best and median α-nDCG@10 measures over all runs, both in terms of the mean of the three rankings in Best and medians α-nDCG@10 for</figDesc><table coords="8,381.84,55.44,108.94,50.31"><row><cell></cell><cell>Best</cell><cell>Median</cell></row><row><cell>all</cell><cell cols="2">0.6097 0.4207</cell></row><row><cell cols="3">Real-time 0.6070 0.4137</cell></row><row><cell>+1 Day</cell><cell cols="2">0.6044 0.4185</cell></row><row><cell>+7 Days</cell><cell cols="2">0.6176 0.4298</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="8,316.80,119.40,239.12,29.07"><head>the blog post ranking stage of top news stories identification task, broken down by category.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="9,53.76,55.44,502.14,188.43"><head>Table 12 : Top stories identification task: Ranking of runs for identifying important stories, one run per group. Ranked by Mean statMAP over all categories. * denotes a manual run.</head><label>12</label><figDesc></figDesc><table coords="9,105.00,55.44,399.64,188.43"><row><cell></cell><cell>Run</cell><cell cols="6">TRC2 Fields statMAP Business Sci-Tech Sport Mean statMAP by Category U.S.</cell><cell>World</cell></row><row><cell cols="3">POSTECH KLE KLERUN1</cell><cell>HC</cell><cell>0.2206</cell><cell>0.1851</cell><cell>0.1821</cell><cell>0.1916 0.2458 0.2986</cell></row><row><cell>ikm100</cell><cell cols="2">ikm100jing</cell><cell>HC</cell><cell>0.2151</cell><cell>0.1144</cell><cell>0.2483</cell><cell>0.1725 0.3897 0.1504</cell></row><row><cell>ICTNET</cell><cell cols="2">ICTNETTSRun2</cell><cell>HC</cell><cell>0.2138</cell><cell>0.0969</cell><cell>0.1898</cell><cell>0.2405 0.3025 0.2396</cell></row><row><cell>UoS</cell><cell>strath2*</cell><cell></cell><cell>HC</cell><cell>0.1285</cell><cell>0.0218</cell><cell>0.0029</cell><cell>0.2308 0.1275 0.2595</cell></row><row><cell>uogTr</cell><cell cols="2">uogTrLC151</cell><cell>HC</cell><cell>0.1139</cell><cell>0.0907</cell><cell>0.0058</cell><cell>0.1066 0.1230 0.2434</cell></row><row><cell>ULugano</cell><cell cols="2">CombMNZ</cell><cell>HC</cell><cell>0.1000</cell><cell>0.0428</cell><cell>0.0698</cell><cell>0.0926 0.2801 0.0149</cell></row><row><cell>Group</cell><cell></cell><cell>Run</cell><cell></cell><cell cols="4">Mean α-nDCG@10 Real-time +1 Day +7 Days α-nDCG@10 by Category</cell></row><row><cell>uogTr</cell><cell></cell><cell>uogTrL81</cell><cell></cell><cell>0.4771</cell><cell></cell><cell>0.4688</cell><cell>0.4671</cell><cell>0.4953</cell></row><row><cell cols="3">POSTECH KLE KLE1</cell><cell></cell><cell>0.4651</cell><cell></cell><cell>0.4665</cell><cell>0.4626</cell><cell>0.4663</cell></row><row><cell cols="2">ICTNET</cell><cell cols="2">ICTNETPRRun3</cell><cell>0.4266</cell><cell></cell><cell>0.4255</cell><cell>0.4175</cell><cell>0.4368</cell></row><row><cell>ikm100</cell><cell></cell><cell>run3</cell><cell></cell><cell>0.4075</cell><cell></cell><cell>0.3779</cell><cell>0.4096</cell><cell>0.4350</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="9,53.76,257.04,502.09,19.11"><head>Table 14 : Top stories identification task: Ranking of runs for blog post ranking, one run per group. Ranked by Mean</head><label>14</label><figDesc></figDesc><table /><note coords="9,504.83,257.04,51.02,8.97;9,53.76,268.08,71.12,8.07"><p>α-nDCG@10 over all categories.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The description of system runs are based on paragraphs contributed by the participating groups. We would like to express our thanks and appreciation to <rs type="person">Thomson Reuters</rs> for providing the large sample of headlines and content used in the top stories identification task. They are provided to support research in the TREC blog track. Thanks are also due to <rs type="person">Richard McCreadie</rs> for his help and assistance with the use of crowdsourcing and for contributions to this overview.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,58.25,297.11,96.95,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.20,312.24,182.03,8.07;9,73.20,322.80,218.59,8.07;9,73.20,333.24,85.71,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,73.20,322.80,98.86,8.07">Diversifying Search Results</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,187.68,322.80,100.12,8.07">Proceedings of WSDM 2009</title>
		<meeting>WSDM 2009<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.20,344.64,213.88,8.07;9,73.20,355.08,210.16,8.07;9,73.20,365.64,68.04,8.07" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,178.25,344.64,108.82,8.07;9,73.20,355.08,117.18,8.07">A Practical Sampling Strategy for Efficient Retrieval Evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>North Eastern University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,73.20,377.04,201.52,8.07;9,73.20,387.48,200.45,8.07;9,73.20,397.92,148.30,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,142.86,377.04,131.85,8.07;9,73.20,387.48,186.90,8.07">Fast, cheap, and creative: Evaluating translation quality using Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,73.20,397.92,103.55,8.07">Proceedings of EMNLP 2009</title>
		<meeting>EMNLP 2009<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.20,409.44,198.88,8.07;9,73.20,419.88,196.46,8.07;9,73.20,430.32,69.08,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,205.27,409.44,66.81,8.07;9,73.20,419.88,68.08,8.07">What Should Blog Search Look Like?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,153.84,419.88,90.64,8.07">Proceedings of SSM 2008</title>
		<meeting>SSM 2008<address><addrLine>Napa Valley, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.20,441.84,185.11,8.07;9,73.20,452.28,212.28,8.07;9,73.20,462.72,72.36,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,208.20,441.84,50.10,8.07;9,73.20,452.28,96.01,8.07">Click-through prediction for news queries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,185.03,452.28,96.44,8.07">Proceedings of SIGIR 2009</title>
		<meeting>SIGIR 2009<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.20,474.12,192.03,8.07;9,73.20,484.68,198.05,8.07;9,73.20,495.12,93.24,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,220.70,474.12,44.53,8.07;9,73.20,484.68,82.53,8.07">Overview of TREC-2007 Blog track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.12,484.68,96.13,8.07">Proceedings of TREC 2007</title>
		<meeting>TREC 2007<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.20,506.52,192.03,8.07;9,73.20,516.96,198.05,8.07;9,73.20,527.52,93.24,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,220.70,506.52,44.53,8.07;9,73.20,516.96,82.53,8.07">Overview of TREC-2009 Blog track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.12,516.96,96.13,8.07">Proceedings of TREC 2009</title>
		<meeting>TREC 2009<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.20,538.92,217.35,8.07;9,73.20,549.36,189.16,8.07;9,73.20,559.80,210.10,8.07;9,73.20,570.36,137.26,8.07;9,73.20,581.28,178.06,7.05;9,73.20,591.72,194.38,7.05" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,174.93,538.92,115.62,8.07;9,73.20,549.36,166.35,8.07">The TREC Blogs06 Collection : Creating and Analysing a Blog Test Collection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<ptr target="http://www.dcs.gla.ac.uk/∼craigm/publications/macdonald06creating.pdf" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">DCS Technical Report</note>
</biblStruct>

<biblStruct coords="9,73.20,602.64,216.28,8.07;9,73.20,613.20,210.93,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,272.10,602.64,17.37,8.07;9,73.20,613.20,80.92,8.07">Blog track research at TREC</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,161.38,613.20,47.38,8.07">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="75" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.24,299.16,211.13,8.07;9,336.24,309.60,211.76,8.07;9,336.24,320.04,150.70,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,492.68,299.16,54.69,8.07;9,336.24,309.60,150.40,8.07">Crowdsourcing Blog Track Top News Judgments at TREC</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,503.63,309.60,44.36,8.07;9,336.24,320.04,51.93,8.07">Proceedings of CSDM 2011</title>
		<meeting>CSDM 2011<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.24,331.56,203.48,8.07;9,336.24,342.48,210.58,7.05;9,336.24,352.92,231.44,7.05" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,378.65,331.56,135.12,8.07">State of the Blogosphere, introduction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mclean</surname></persName>
		</author>
		<ptr target="http://technorati.com/blogging/article/state-of-the-blogosphere-2009-introduction" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.24,363.84,199.97,8.07;9,336.24,374.40,170.44,8.07" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,438.63,363.84,84.08,8.07">A Study of Blog Search</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,336.24,374.40,94.04,8.07">Proceedings of ECIR 2006</title>
		<meeting>ECIR 2006<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.24,385.80,188.80,8.07;9,336.24,396.24,186.03,8.07;9,336.24,406.68,195.60,8.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,379.73,396.24,129.47,8.07">Overview of TREC-2006 Blog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,336.24,406.68,96.13,8.07">Proceedings of TREC 2006</title>
		<meeting>TREC 2006<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.24,418.20,202.98,8.07;9,336.24,428.64,198.05,8.07;9,336.24,439.08,93.12,8.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,481.46,418.20,57.76,8.07;9,336.24,428.64,82.53,8.07">Overview of the TREC-2008 Blog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,434.16,428.64,96.13,8.07">Proceedings of TREC 2008</title>
		<meeting>TREC 2008<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.24,450.60,209.81,8.07;9,336.24,461.04,212.12,8.07;9,336.24,471.48,78.08,8.07" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,474.97,450.60,71.08,8.07;9,336.24,461.04,89.67,8.07">Event detection and tracking in social streams</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sayyadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,441.84,461.04,102.53,8.07">Proceedings of ICWSM 2009</title>
		<meeting>ICWSM 2009<address><addrLine>San Jose, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.24,483.00,218.25,8.07;9,336.24,493.44,213.82,8.07;9,336.24,503.88,201.44,8.07;9,336.24,514.32,80.78,8.07" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,516.52,483.00,37.96,8.07;9,336.24,493.44,213.82,8.07;9,336.24,503.88,78.47,8.07">Cheap and fast-but is it good?: Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,430.08,503.88,103.67,8.07">Proceedings of EMNLP 2008</title>
		<meeting>EMNLP 2008<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.24,525.84,196.74,8.07;9,336.24,536.28,205.67,8.07;9,336.24,546.72,219.68,8.07;9,336.24,557.16,104.94,8.07" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,384.89,525.84,148.09,8.07;9,336.24,536.28,108.95,8.07">Bloggers during the London attacks: Top information sources and topics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,460.80,536.28,81.11,8.07;9,336.24,546.72,219.68,8.07;9,336.24,557.16,19.17,8.07">Proceedings of the 3rd International Workshop on the Weblogging Ecosystem (WWE 2006)</title>
		<meeting>the 3rd International Workshop on the Weblogging Ecosystem (WWE 2006)<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.24,568.68,179.16,8.07;9,336.24,579.12,212.48,8.07;9,336.24,589.56,157.26,8.07" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,474.55,568.68,40.84,8.07;9,336.24,579.12,152.35,8.07">A Study on Retrospective and On-line Event Detection</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,504.36,579.12,44.36,8.07;9,336.24,589.56,49.69,8.07">Proceedings of SIGIR 1998</title>
		<meeting>SIGIR 1998<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
