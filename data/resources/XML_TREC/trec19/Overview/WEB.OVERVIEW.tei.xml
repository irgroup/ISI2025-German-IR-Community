<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.40,114.69,277.25,12.31">Overview of the TREC 2010 Web Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,90.96,148.42,105.76,10.97"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.92,148.42,70.08,10.97;1,238.08,162.46,48.10,10.97"><forename type="first">Nick</forename><forename type="middle">Craswell</forename><surname>Microsoft</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ian Soboroff NIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.16,148.42,103.73,10.97"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.40,114.69,277.25,12.31">Overview of the TREC 2010 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FFA34CD0215DCAA18CF5966A3E2AECCA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Web Track explores and evaluates Web retrieval technology over large collections of Web data. In its current incarnation, the Web Track has been active for two years. For TREC 2010, the track includes three tasks: 1) an adhoc retrieval task, 2) a diversity task, and 3) a spam task. As we did for TREC 2009, we based our experiments on the billion-page ClueWeb09<ref type="foot" coords="1,509.88,289.06,4.23,5.47" target="#foot_0">1</ref> data set created by the Language Technologies Institute at Carnegie Mellon University.</p><p>The TREC 2009 Web Track included a traditional adhoc retrieval task, employing topical binary relevance assessments and reporting estimated MAP as its primary effectiveness measure <ref type="bibr" coords="1,503.76,330.82,10.91,10.91" target="#b3">[4]</ref>. For TREC 2010, we modified this traditional assessment process to incorporate multiple relevance levels, which are similar in structure to the levels used in commercial Web search. This new assessment structure includes a spam/junk level, which also assisted in the evaluation of the spam task. The top two levels of the assessment structure are closely related to the homepage finding and topic distillation tasks appearing in older Web Tracks.</p><p>The diversity task was introduced for TREC 2009 and continues in TREC 2010, essentially unchanged <ref type="bibr" coords="1,127.32,425.74,10.91,10.91" target="#b3">[4]</ref>. The goal of this diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. The adhoc and diversity tasks share topics, which were developed by NIST with the assistance of information extracted from the the logs of a commercial Web search engine <ref type="bibr" coords="1,431.88,466.30,10.91,10.91" target="#b8">[9]</ref>. Topic creation and judging attempts to reflect a mix of genuine user requirements for the topic.</p><p>An analysis of last year's results indicates that the presence of spam and other low-quality pages substantially influenced the overall results <ref type="bibr" coords="1,277.08,506.98,10.91,10.91" target="#b6">[7]</ref>. This year we provided a preliminary spam ranking of the pages in the corpus, as an aid to groups who wish to reduce the number of low-quality pages in their results. The associated spam task required groups to provide their own ranking of the corpus according to "spamminess".</p><p>Table <ref type="table" coords="1,119.28,561.22,5.45,10.91" target="#tab_0">1</ref> summarizes participation in the TREC 2010 Web Track. A total of 23 groups participated in the track, a slight decrease from last year, when 26 groups participated. Many of the groups participating the diversity task also participated in the adhoc task, but not vice versa. The spam task attracted only 3 participants, including one group that participated only in this task. Only one group, ICTNET, participated in all three tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topics</head><p>NIST created and assessed 50 new topics for the track, which were used by both the adhoc task and diversity tasks. figure <ref type="figure" coords="2,199.80,346.30,5.45,10.91" target="#fig_0">1</ref> provides two examples. Each topic contains a query field, a description field, and several subtopic fields. The query field is intended to represent the text a user might enter into a Web search engine, if they were seeking the information indicated by the description field or by any of the subtopics. For the adhoc task, relevance is judged on the basis of the description field. For the diversity task, relevance is judged separately with respect to each subtopic. Initially, only the query field was released to track participants. The full topics were not released until the participants had submitted their experimental runs.</p><p>Each topic is assigned one of two types. Topics with ambiguous queries, such as topic 72 in figure <ref type="figure" coords="2,103.80,468.22,4.21,10.91" target="#fig_0">1</ref>, have several unrelated interpretations. One of these interpretations is chosen for the description, while a wider range of interpretations appear in the subtopics. Topics with faceted queries, such as topic 73 in the figure, have one primary interpretation, usually reflected in the description field. For these queries, the subtopics address narrower aspects of the broader topic.</p><p>Each subtopic is assigned one of two types. Navigational subtopics (with type "nav") assume the user is seeking a specific page or site. Navigational subtopics typically have a single relevant page. Informational subtopics (with type "inf") assume the user is seeking information without regard to its source, provided that the source is reliable. Informational subtopics typically have a large number of relevant pages.</p><p>Subtopics were developed with the assistance of query-term clusters extracted from the logs of a commercial Web search engine <ref type="bibr" coords="2,233.88,603.70,10.91,10.91" target="#b8">[9]</ref>. Topic development followed essentially the same procedure as last year <ref type="bibr" coords="2,129.36,617.26,10.91,10.91" target="#b3">[4]</ref>. Subtopics were chosen to be roughly balanced in terms of popularity. Strange and unusual aspects and interpretations were avoided as much as possible.</p><p>All topics are expressed in English. Non-English documents are never considered relevant, even if the assessor understands the language of the document and the document would be relevant in that language.</p><p>&lt;topic number="72" type="ambiguous"&gt; &lt;query&gt;the sun&lt;/query&gt; &lt;description&gt; Find information about the Sun, the star in our Solar System. &lt;/description&gt; &lt;subtopic number="1" type="inf"&gt; Find information about the Sun, the star in our Solar System. &lt;/subtopic&gt; &lt;subtopic number="2" type="nav"&gt; Find the homepage for the U.K. newspaper, The Sun. &lt;/subtopic&gt; &lt;subtopic number="3" type="nav"&gt; Find the homepage for the Baltimore Sun newspaper. &lt;/subtopic&gt; &lt;/topic&gt; &lt;topic number="73" type="faceted"&gt; &lt;query&gt;neil young&lt;/query&gt; &lt;description&gt; Find music, tour dates, and information about the musician Neil Young. &lt;/description&gt; &lt;subtopic number="1" type="nav"&gt; Find albums by Neil Young to buy. &lt;/subtopic&gt; &lt;subtopic number="2" type="inf"&gt; Find biographical information about Neil Young. &lt;/subtopic&gt; &lt;subtopic number="3" type="nav"&gt; Find lyrics or sheet music for Neil Young's songs. &lt;/subtopic&gt; &lt;subtopic number="4" type="nav"&gt; Find a list of Neil Young tour dates. &lt;/subtopic&gt; &lt;/topic&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tasks and Measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adhoc Task</head><p>An adhoc task in TREC investigates the performance of systems that search a static set of documents using previously-unseen topics. The goal of an adhoc task is to return a ranking of the documents in the collection in order of decreasing probability of relevance. The probability of relevance of a document is considered independently of other documents that appear before it in the result list.</p><p>For the adhoc task, documents are judged on the basis of the description field using a six-point scale, defined as follows:</p><p>1. Nav: This page represents a home page of an entity directly named by the query; the user may be searching for this specific page or site. (relevance grade 4)</p><p>2. Key: This page or site is dedicated to the topic; authoritative and comprehensive, it is worthy of being a top result in a web search engine. (grade 3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HRel:</head><p>The content of this page provides substantial information on the topic. (grade 2)</p><p>4. Rel: The content of this page provides some information on the topic, which may be minimal; the relevant information must be on that page, not just promising-looking anchor text pointing to a possibly useful page. (grade 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Non:</head><p>The content of this page does not provide useful information on the topic, but it may provide useful information on other topics, including other interpretations of the same query. (grade 0)</p><p>6. Junk: This page does not appear to be useful for any reasonable purpose; it may be spam or junk. (grade 0)</p><p>After each description, we list the relevance grade assigned to that level for the purpose of calculating graded effectiveness measures. The primary effectiveness measure for the adhoc task is expected reciprocal rank (ERR) as defined by Chapelle et al. <ref type="bibr" coords="4,200.88,490.18,10.91,10.91" target="#b1">[2]</ref>. We also report a variant of nDCG <ref type="bibr" coords="4,393.36,490.18,10.91,10.91" target="#b7">[8]</ref>, as well as standard binary measures, including mean average precision (MAP) and precision at rank k (P@k). We compute ERR at rank k (ERR@k) as follows:</p><formula xml:id="formula_0" coords="4,217.08,539.87,322.83,32.00">ERR@k = k i=1 R(g i ) i i-1 j=1 (1 -R(g i )),<label>(1)</label></formula><p>where R(g) = 2 g -1</p><p>16 and g 1 , g 2 , ..., g k are the relevance grades associated with the top k documents. We compute nDCG@k as DCG@k ideal DCG@k , where</p><formula xml:id="formula_1" coords="4,238.80,626.99,301.11,32.00">DCG@k = k i=1 2 g i -1 log 2 (1 + i) .<label>(2)</label></formula><p>For the binary relevance measures, we treat grades 1-4 as relevant and grade 0 as non-relevant. We apply trec_eval to compute the binary measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Diversity Task</head><p>The diversity task is similar to the adhoc retrieval task, but differs in its judging criteria and evaluation measures. The goal of the diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. For this task, the probability of relevance of a document is conditioned on the documents that appear before it in the result list.</p><p>For the diversity task, documents are judged on the basis of the subtopics. For each subtopic, the assessor makes a binary judgment as to whether or not a document satisfies the information need associated with that subtopic.</p><p>The primary effectiveness measure for the adhoc task is a variant of intent-aware expected reciprocal rank (ERR-IA) as defined by Chapelle et al. <ref type="bibr" coords="5,332.40,217.30,10.91,10.91" target="#b1">[2]</ref>. We also report a number of other intent aware measures appearing in the literature, including α-nDCG@k [6], NRBP <ref type="bibr" coords="5,441.48,230.86,10.91,10.91" target="#b4">[5]</ref>, and MAP-IA <ref type="bibr" coords="5,525.48,230.86,10.82,10.91" target="#b0">[1]</ref>. Clarke et al. <ref type="bibr" coords="5,135.48,244.42,11.54,10.91" target="#b2">[3]</ref> provide a detailed description and analysis of the novelty and diversity measures employed in the TREC Web track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Spam Task</head><p>The goal of the spam task is to score each English-language document in the Category A ClueWeb09 collection according to how likely it is to be spam. For the purposes of this task, we employ a broad definition of spam, which comprises pages that are essentially junk, along with pages that are more obviously deceptive or detrimental. Spam had a major impact on the TREC 2009 adhoc submissions; practically every one was improved dramatically by the application of a spam filter <ref type="bibr" coords="5,525.48,362.02,10.82,10.91" target="#b6">[7]</ref>.</p><p>Participant submissions were evaluated by how well they identified spam in the adhoc and diversity submissions, as measured by area under the receiver operating characteristic curve (AUC) <ref type="bibr" coords="5,525.48,389.02,10.82,10.91" target="#b6">[7]</ref>. For the purposes of the spam task evaluation, pages judged to be "Junk" under the six-point relevance scale described in Section 4.1 were considered to be spam, while all other judged pages were considered to be non-spam (i.e., "ham"). Unjudged pages were not considered in the computation of AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Pooling and Judging</head><p>For each topic, participants in the adhoc and diversity tasks submitted a ranking of the top 10,000 documents for that topic. All submitted runs were included in the pool for judging. This year, a common pool was used for both tasks, and all runs were judged to depth 20 using both the adhoc and diversity judging criteria. In this paper, we report results only for runs explicitly submitted to one task or the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Cat ERR@20 nDCG@20 P@20 MAP 6 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Adhoc and Diversity Tasks</head><p>Table <ref type="table" coords="6,102.36,544.18,5.45,10.91" target="#tab_1">2</ref> presents the top adhoc task results ordered by ERR@20. Table <ref type="table" coords="6,421.80,544.18,5.45,10.91" target="#tab_2">3</ref> presents the top diversity task results ordered by ERR@20. The figures mix results for both Category A and B runs.</p><p>Experience with the ClueWeb09 collection suggests that the Category B subset generally contains higher quality documents than the rest of the collection. The results support this view, with several Category B runs achieving good performance. The baseline run was created at the University of Waterloo. While the run represents an official submission to the track, it should be considered as a special case in any discussion and analysis of the track results. To create this run, the queries were submitted to a commercial search engine, and the results were filtered against the ClueWeb09 collection. Thus, the performance of this run might be considered as a very crude lower bound on the performance of the commercial search engine.</p><p>All runs submitted to the adhoc and diversity tasks were judged using the judging criteria of both tasks, even runs that were not submitted to both tasks. This additional judging allows us to make direct conparisons between runs optimized for the two tasks, supporting efforts to determine if the different judging criteria and evaluation measures identify genuine differences. For example, figure <ref type="figure" coords="7,102.48,115.42,5.45,10.91" target="#fig_2">2</ref> provides a scatter plot comparing the performance of the runs under ERR@20 and ERR-IA@20, the primary effectiveness measures for the adoc and diversity tasks respectively. While the values are correlated, there are clear differences in the relative performance of runs under the two measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Spam Task</head><p>Figure <ref type="figure" coords="7,105.84,205.90,5.45,10.91">3</ref> presents receiver operating characteristic curves for the spam task submissions, along with the official baseline ("Baseline") and an additional baseline ("Britney"). The official baseline is the preliminary spam ranking supplied to the track participants. No submitted run substantially outperformed this baseline at any point on the curve, as well as by overall AUC. The additional baseline represents an essentially unsupervised spam ranking of the corpus, generated without labeled training data. Apart from the documents themselves, the only data used to generate this run was a collection of queries frequently submitted to commercial search engines (e.g., "britney spears") <ref type="bibr" coords="7,114.84,300.70,10.91,10.91" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Plans</head><p>The adhoc and diversity tasks will continue in more-or-less their current format for at least one more year, with perhaps some changes in the type of topics selected. Given the small number of participants, the spam task will not be continued for TREC 2011. However, the spam labelings created by the current task will be available for the use of future participants.</p><p>For TREC 2009 and 2010, diversity/adhoc topics were chosen to be of medium-to-high frequency and ambiguous. A new direction that we are considering for TREC 2011 is to work with more obscure topics, which may still be underspecified (i.e., faceted) but may be less ambiguous. Search engines have difficulty with queries of this type, since they can rely less on click/anchor information, and popularity signals like PageRank. With these new tough topics we have a chance to work in an area of Web retrieval that has received relatively little attention. Given the smaller number of pages that may be relevant for these topics, we may potentially be able to create a more reusable collection, with sufficiently exhaustive judgments for the topics.   Figure <ref type="figure" coords="8,107.64,660.70,4.21,10.91">3</ref>: Web spam results. One run, Budapest2, is not plotted, since its curve is essentially identical to that of Budapest1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,176.64,592.78,253.70,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of TREC 2010 Web track topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,160.61,304.51,3.89,6.48;8,150.88,272.91,13.62,6.48;8,154.77,241.31,9.73,6.47;8,150.88,209.71,13.62,6.47;8,154.77,178.16,9.73,6.47;8,150.88,146.56,13.62,6.47;8,154.77,114.96,9.73,6.47;8,150.88,83.36,13.62,6.47;8,167.73,311.51,3.89,6.48;8,196.56,311.51,13.62,6.48;8,230.26,311.51,13.62,6.48;8,263.96,311.51,13.62,6.48;8,297.66,311.51,13.62,6.48;8,333.26,311.51,9.73,6.48;8,365.01,311.51,13.62,6.48;8,398.71,311.51,13.62,6.48;8,432.41,311.51,13.62,6.48;8,466.11,311.51,13.62,6.48;8,133.07,220.75,6.47,38.61;8,133.07,193.14,6.47,25.66;8,133.07,165.52,6.48,25.67;8,133.07,134.02,6.47,29.56;8,265.41,322.01,109.80,6.48"><head></head><label></label><figDesc>@20 (primary diversity measure) ERR@20 (primary adhoc measure)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,82.68,351.94,441.74,10.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of runs under the primary adhoc and diversity effectiveness measures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.00,73.87,468.05,211.10"><head>Table 1 :</head><label>1</label><figDesc>Participation in the TREC 2010 Web track</figDesc><table coords="2,72.00,73.87,468.05,211.10"><row><cell cols="5">Task Adhoc Diversity Spam Total</cell></row><row><cell>Groups</cell><cell>20</cell><cell>12</cell><cell>3</cell><cell>23</cell></row><row><cell>Runs</cell><cell>55</cell><cell>32</cell><cell>5</cell><cell>92</cell></row><row><cell cols="2">2 Category A and B Collections</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">The billion-page ClueWeb09 collection was crawled from the general Web during January and</cell></row><row><cell cols="5">February 2009, and consists of 25TB of uncompressed data (5TB compressed) in multiple languages.</cell></row><row><cell cols="5">Since some participants were not able to work with the full collection, the track accepted runs based</cell></row><row><cell cols="5">on the smaller "Category B" subset of the full "Category A" collection. This Category B data set</cell></row><row><cell cols="5">comprises about 50 million English-language pages, including the entirety of the English-language</cell></row><row><cell cols="5">Wikipedia. Nonetheless, we strongly encouraged participants to use the full Category A data set,</cell></row><row><cell cols="5">if possible. Results reported in this paper are labeled by their collection category.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,87.22,468.05,341.39"><head>Table 2 :</head><label>2</label><figDesc>Top adhoc task results ordered by ERR@20. Only the best run from each group is included in the ranking. The baseline run is discussed in the body of the paper.</figDesc><table coords="6,78.00,87.22,461.16,341.39"><row><cell>msrsv</cell><cell></cell><cell>msrsv3</cell><cell></cell><cell>A</cell><cell>0.166</cell><cell>0.237</cell><cell>0.344</cell><cell>0.082</cell></row><row><cell>baseline</cell><cell></cell><cell>uwgym</cell><cell></cell><cell>A</cell><cell>0.164</cell><cell>0.241</cell><cell>0.374</cell><cell>0.069</cell></row><row><cell>umass</cell><cell></cell><cell>umassSDMW</cell><cell></cell><cell>A</cell><cell>0.138</cell><cell>0.293</cell><cell>0.484</cell><cell>0.148</cell></row><row><cell>isi</cell><cell></cell><cell>IvoryL2Rb</cell><cell></cell><cell>B</cell><cell>0.134</cell><cell>0.225</cell><cell>0.379</cell><cell>0.133</cell></row><row><cell>THUIR</cell><cell></cell><cell>THUIR10QaHt</cell><cell></cell><cell>A</cell><cell>0.128</cell><cell>0.201</cell><cell>0.331</cell><cell>0.112</cell></row><row><cell>uogTr</cell><cell></cell><cell>uogTrA42</cell><cell></cell><cell>A</cell><cell>0.127</cell><cell>0.245</cell><cell>0.411</cell><cell>0.127</cell></row><row><cell>IRRA</cell><cell></cell><cell>irra10b</cell><cell></cell><cell>B</cell><cell>0.126</cell><cell>0.260</cell><cell>0.443</cell><cell>0.133</cell></row><row><cell>unimelb</cell><cell></cell><cell>UMa10IASF</cell><cell></cell><cell>A</cell><cell>0.119</cell><cell>0.181</cell><cell>0.293</cell><cell>0.080</cell></row><row><cell cols="2">CMU LIRA</cell><cell>cmuWiki10</cell><cell></cell><cell>A</cell><cell>0.112</cell><cell>0.212</cell><cell>0.400</cell><cell>0.157</cell></row><row><cell cols="4">UAmsterdam UAMSA10mSF30</cell><cell>B</cell><cell>0.110</cell><cell>0.145</cell><cell>0.237</cell><cell>0.043</cell></row><row><cell>Group</cell><cell>Run</cell><cell cols="6">Cat ERR-IA@20 α-nDCG@20 NRBP MAP-IA</cell></row><row><cell>msrsv</cell><cell cols="2">msrsv3div</cell><cell>A</cell><cell></cell><cell>0.347</cell><cell>0.491</cell><cell>0.303</cell><cell>0.068</cell></row><row><cell>baseline</cell><cell cols="2">uwgym</cell><cell>A</cell><cell></cell><cell>0.346</cell><cell>0.487</cell><cell>0.299</cell><cell>0.050</cell></row><row><cell>THUIR</cell><cell cols="2">THUIR10DvNov</cell><cell>A</cell><cell></cell><cell>0.336</cell><cell>0.474</cell><cell>0.289</cell><cell>0.070</cell></row><row><cell>ICTNET</cell><cell cols="2">ICTNETDV10R2</cell><cell>A</cell><cell></cell><cell>0.322</cell><cell>0.464</cell><cell>0.279</cell><cell>0.038</cell></row><row><cell>uogTr</cell><cell cols="2">uogTrB67xS</cell><cell>B</cell><cell></cell><cell>0.298</cell><cell>0.418</cell><cell>0.262</cell><cell>0.074</cell></row><row><cell>unimelb</cell><cell cols="2">UMd10IASF</cell><cell>A</cell><cell></cell><cell>0.255</cell><cell>0.377</cell><cell>0.208</cell><cell>0.050</cell></row><row><cell>CMU LIRA</cell><cell cols="2">cmuWi10D</cell><cell>A</cell><cell></cell><cell>0.248</cell><cell>0.345</cell><cell>0.215</cell><cell>0.093</cell></row><row><cell cols="3">UAmsterdam UAMSD10aSRfu</cell><cell>B</cell><cell></cell><cell>0.242</cell><cell>0.341</cell><cell>0.210</cell><cell>0.026</cell></row><row><cell>UCDSIFT</cell><cell cols="2">UCDSIFTDiv</cell><cell>B</cell><cell></cell><cell>0.210</cell><cell>0.312</cell><cell>0.170</cell><cell>0.062</cell></row><row><cell>qirdcsuog</cell><cell cols="2">qirdcsuog3</cell><cell>B</cell><cell></cell><cell>0.205</cell><cell>0.304</cell><cell>0.165</cell><cell>0.051</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,452.26,467.98,24.47"><head>Table 3 :</head><label>3</label><figDesc>Top diversity task results ordered by ERR-IA@20. Only the best run from each group is included in the ranking. The baseline run is discussed in the body of the paper.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.56,637.84,171.27,6.22"><p>boston.lti.cs.cmu.edu/Data/clueweb09.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The track could not operate without the ClueWeb09 collection, created by <rs type="person">Jamie Callan</rs>, <rs type="person">Mark Hoy</rs>, and the <rs type="institution">Language Technologies Institute at Carnegie Mellon University. University of Waterloo</rs> graduate student <rs type="person">Hani Khoshdel Nikkhoo</rs> generated the the track baseline runs (uwgym). We thank the participants for their hard work in making the track a success.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,88.92,99.10,450.89,10.91;9,88.92,112.66,450.86,10.91;9,88.92,126.22,111.86,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,449.64,99.10,90.17,10.91;9,88.92,112.66,29.84,10.91">Diversifying search results</title>
		<author>
			<persName coords=""><forename type="first">Sreenivas</forename><surname>Rakesh Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,143.40,113.03,332.08,10.05">2nd ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.92,148.78,451.07,10.91;9,88.92,162.34,450.98,10.91;9,88.92,175.90,99.26,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,419.16,148.78,120.83,10.91;9,88.92,162.34,95.36,10.91">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,209.88,162.71,324.23,10.05">18th ACM Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.92,198.34,451.05,10.91;9,88.92,211.90,451.18,10.91;9,88.92,225.46,208.82,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,411.84,198.34,128.13,10.91;9,88.92,211.90,208.04,10.91">A comparative analysis of cascade measures for novelty and diversity</title>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azin</forename><surname>Ashkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,325.80,212.27,214.30,10.05;9,88.92,225.83,114.52,10.05">th ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.92,248.02,450.86,10.91;9,88.92,261.58,317.66,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,352.20,248.02,183.06,10.91">Overview of the TREC 2009 web track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,102.60,261.95,148.46,10.05">18th Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.92,284.02,451.03,10.91;9,88.92,297.58,451.01,10.91;9,88.92,311.14,198.98,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,402.96,284.02,136.99,10.91;9,88.92,297.58,180.18,10.91">An effectiveness measure for ambiguous and underspecified queries</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,294.36,297.95,245.57,10.05;9,88.92,311.51,91.37,10.05">2nd International Conference on the Theory of Information Retrieval</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="188" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.92,333.70,451.22,10.91;9,88.92,347.26,450.99,10.91;9,88.92,360.70,451.09,10.91;9,88.92,374.26,262.94,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,281.04,347.26,258.87,10.91;9,88.92,360.70,17.39,10.91">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Ashkann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,131.04,361.07,408.97,10.05;9,88.92,374.63,101.69,10.05">31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.92,396.82,451.05,10.91;9,88.92,410.38,451.11,10.91;9,88.92,423.94,193.70,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,409.32,396.82,130.65,10.91;9,88.92,410.38,212.22,10.91">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno>arxiv.org/abs/1004.5168</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,309.48,410.75,100.98,10.05">Information Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>To appear. Preprint available at</note>
</biblStruct>

<biblStruct coords="9,88.92,446.38,450.98,10.91;9,88.92,459.94,312.62,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,291.72,446.38,243.39,10.91">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,88.92,460.31,206.48,10.05">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.92,482.50,450.94,10.91;9,88.92,496.06,451.10,10.91;9,88.92,509.62,24.62,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,342.36,482.50,197.50,10.91;9,88.92,496.06,45.53,10.91">Inferring query intent from reformulations and clicks</title>
		<author>
			<persName coords=""><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,157.20,496.43,227.78,10.05">19th International World Wide Web Conference</title>
		<meeting><address><addrLine>Raleigh, North Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
