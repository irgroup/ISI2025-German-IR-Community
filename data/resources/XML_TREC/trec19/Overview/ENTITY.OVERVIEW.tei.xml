<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,170.05,137.95,289.90,15.12">Overview of the TREC 2010 Entity Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,195.15,170.43,79.88,10.48"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>krisztian.balog@idi.ntnu.no</email>
						</author>
						<author>
							<persName coords="1,370.93,170.43,84.38,10.48"><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NTNU</orgName>
								<address>
									<settlement>Trondheim</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Yandex</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Arjen P. de Vries CWI</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,170.05,137.95,289.90,15.12">Overview of the TREC 2010 Entity Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">535EDB7C1A3C37E26F640662BFFD247A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The overall goal of the track is to perform entity-oriented search tasks on the World Wide Web. Many user information needs concern entities (people, organizations, locations, products, ...); these are better answered by returning specific objects instead of just any type of documents.</p><p>Defining entities on the Web is still an unsolved problem. We settled on representing entities by their homepages, under the assumption that any entity of interest would have at least one homepage. The homepage URL is used as unique identifier. In this scenario, entity ranking corresponds to the task of returning the homepages of entities of a given type, that are relevant to the user's information need (represented as natural language text). We have to also consider that many entity queries could have very large answer sets (e.g., "actors playing in hollywood movies"); extra problematic with corpora the size of ClueWeb. In 2009, we decided therefore that finding associations between entities would be a more challenging one (in terms of modeling) and also a more manageable one (from a test collection building perspective) than finding associations between entities and topics, and defined the Related Entity Finding (REF) task <ref type="bibr" coords="1,464.28,452.79,57.72,8.74;1,108.00,464.74,22.14,8.74" target="#b1">(Balog et al., 2010)</ref>. Related entity finding requests a ranked list of entities (of a specified type) that engage in a given relationship with a given source entity. REF ran as a pilot in 2009 and is the track's main task in this year; the document collection has been enlarged to the English subset of ClueWeb. We intend to repeat the REF task at least one more time in 2011.</p><p>One observation from the 2009 edition of the track is that many of the proposed approaches build heavily on Wikipedia and use it as a "semantic backbone": considering Wikipedia a large repository of entity names and types. Our goal is however not to evaluate entity retrieval over Wikipedia (this task has already been looked at in INEX, and a test collection exists), nor to limit ourselves to the (mostly popular) entities that are present in Wikipedia. As of this year, we are therefore not accepting Wikipedia pages as entity homepages.</p><p>The issue of combining (noisy) textual material (the Web) with semi-structured data (like Wikipedia or slightly more structured data sources like IMDB) is however an interesting line of research. As many data sources, and in particular those being constructed as so-called Linked Open Data (LOD), are naturally organized around entities, it would be reasonable to examine this problem in the context of entity retrieval. To foster research in this direction, we introduced the new Entity List Completion (ELC) pilot task. ELC is motivated by the same user scenario as REF, but with the main difference that entities are represented by their URIs in a Semantic Web crawl (the Billion Triple Collection). In addition, a small number of example entities (defined by their URIs) are made available as part of the topic definition. Our goal is to turn this pilot task to an "official" task in 2011.</p><p>In the remainder of the paper we discuss the REF and ELC tasks in detail, in Sections 2 and 3, respectively. We summarize our findings and outline future plans in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Entity Finding</head><p>Related Entity Finding (REF) ran as the main task of the track. Based on the experience gained from last year's pilot edition of the REF task, we implemented the following changes to the 2009 setup: (i) the document collection is enlarged to ClueWeb English, (ii) Wikipedia pages do not receive special treatment anymore, (iii) supporting documents are not required, (iv) "location" is added to target entity types, and (v) primary homepages receive more credit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task</head><p>The Related Entity Finding (REF) task is formulated as follows:</p><p>Given an input entity, by its name and homepage, the type of the target entity, as well as the nature of their relation, described in free text, find related entities that are of target type, standing in the required relation to the input entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Input</head><p>For each request (query) the following information is provided:</p><p>• Input entity, defined by its name and homepage</p><p>• Type of the target entity (person, organization, product, or location)<ref type="foot" coords="2,434.19,388.28,3.97,6.12" target="#foot_0">1</ref> </p><p>• Narrative (describing the nature of the relation in free text) An example topic is shown below: &lt;query&gt; &lt;num&gt;23&lt;/num&gt; &lt;entity_name&gt;The Kingston Trio&lt;/entity_name&gt; &lt;entity_URL&gt;clueweb09-en0009-81-29533&lt;/entity_URL&gt; &lt;target_entity&gt;organization&lt;/target_entity&gt; &lt;narrative&gt;What recording companies now sell the Kingston Trio's songs?&lt;/narrative&gt; &lt;/query&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Output</head><p>For each query, participants could return up to 100 answers (homepages). For each answer entity a single homepage must be returned; optionally, the name of the entity may also be returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Data collection</head><p>The document collection is the English portion of ClueWeb, comprising of approximately 500 million pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topics and assessments</head><p>Both topic development and relevance assessments were performed by NIST. For the 2010 edition of the track 50 new REF topics have been created. Out of these 47 ended up being assessed <ref type="bibr" coords="3,108.00,143.36,184.55,8.74">(excluded topics are: #35, #46, and #59)</ref>. Participants were also requested to submit results for the 20 queries from last year.</p><p>We differentiate between primary and relevant homepages of a given entity: (i) a primary homepage is devoted to and in control of the entity, and (ii) a relevant homepage is devoted to the entity, but is not in control of the entity. By definition (and, unlike last year), the Wikipedia page of a given entity is non-relevant. Pages that only mention the entity (but are not about the entity) are also considered non-relevant. News articles and blog posts, even if exclusively about the entity, are not accepted as entity homepages. Products are required to have a separate page under the manufacturer's site.</p><p>All runs were pooled down to 20 records. Entity homepages were judged on a three-point relevance scale: (0) non-relevant, (1) relevant, or (2) primary. Names were judged as (0) incorrect, (1) inexact, or (2) correct, for the page returned. If the page is not primary, the correctness of the name is immaterial for the task. Finally, primary homepages are grouped together; primary documents in the same class are equivalent, and correct names for them are all valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Qrels</head><p>In the qrels file, the fields are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>topic doc name rel class rel_name</head><p>Where topic denotes the topic ID (corresponds to the num field of the topic definition), doc is a ClueWeb document ID, name is the normalized name of the entity, rel is the judgment for the document (0, 1, or 2), class is a class number for the document, and rel name is the judgment for the name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Evaluation metrics</head><p>The main evaluation measure we use is NDCG@R; that is, the normalized discounted cumulative gain at rank R (the number of primaries and relevants for that topic) where primary homepages get gain 3 and relevant homepages get gain 1. We also report on R-Precision (precision at rank R), and Mean Average Precision, both computed over primary pages only.</p><p>Note that evaluation results are not computed using the standard trec eval tool, but a script developed specifically for the 2010 edition of the REF task<ref type="foot" coords="3,391.74,521.27,3.97,6.12" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Runs and results</head><p>Each group was allowed to submit up to four runs. Fourteen groups submitted a total of 48 runs; of those, 29 were automatic runs. Eight groups submitted a total of 19 manual runs.</p><p>The best automatic and manual runs from each group are shown in Table <ref type="table" coords="3,457.21,593.03,4.98,8.74" target="#tab_0">1</ref> and<ref type="table" coords="3,486.20,593.03,31.93,8.74" target="#tab_1">Table 2</ref>, respectively, while Table <ref type="table" coords="3,214.96,604.98,4.98,8.74">5</ref> displays all submitted runs. The Kendall tau rank coefficients indicate very strong correlation between the rankings of participating systems using various metrics (0.92 for NDCG@R vs. MAP, 0.94 for NDCG@R vs. rPrec, and 0.94 for MAP vs. rPrec). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Approaches</head><p>The following are descriptions of the approaches taken by the different groups. These paragraphs were contributed by participants and are meant to be a road map to their papers.</p><p>BIT BIT Entity Group employs a logical sitemap constructor to extract hierarchical structures in order to enrich the anchor text model for finding more relevant pages. Those hierarchical structures, such as menus, navigational bars or breadcrumbs, indicate the logical relations between pages in the same site and the concise summary of pages in some sense. Under the assumption that items in similar visual presentations are probable similar in nature and to be classified in a group, they discriminate extracted entities by their locations in DOM tree and prefers to multiple entities in tables and lists. Finally, they find homepages from multiple sources and rank the homepages by their confidences and existences in ClueWeb09a English part for each candidate entity. <ref type="bibr" coords="4,361.19,506.84,81.36,8.74" target="#b15">(Yang et al., 2011)</ref>  CARD UALR To find relevant entities and their homepages, first, we identified the entities and their types using Stanford Named Entity Recognizer. Due to its limitations, we could only identify PERSON, LOCATION and ORGANIZATION type entities. Next, an entity-entity co-occurrence graph was established. If two entities co-occurred in a webpage more than a specified threshold, the two entities were linked. Given the query entity, relevant entities are extracted based on a novel centrality measure (Cumulative Structural Similarity-CSS) using the intuition that an important entity will share many common neighbors with adjacent entities. Additionally, PageRank, HITS and Ensemblebased approaches are submitted. <ref type="bibr" coords="5,278.41,196.70,95.23,8.74" target="#b0">(Agarwal et al., 2011)</ref> FDWIM2010 The FDWIM group proposes a multiple-stage retrieval framework for the task of related entity finding. In the document retrieval stage, search engine is used to improving the retrieval accuracy. In the next stage, they extract entity with NER tools, Wikipedia and text pattern recognition. Then stoplist and other rules is employed to filtering entity. Deep mining of the authority pages is effective in this stage. In entity ranking stage, many factors including keywords from narrative, page rank, combined results of corpus-based association rules and search engine are considered. Finally, an improved feature-based algorithm is proposed for the entity homepage detection. <ref type="bibr" coords="5,384.11,300.31,89.10,8.74">(Wang et al., 2011a)</ref> HPI The approach of the HPI-group studies in particular the exploitation of advanced features of different Web search engines to achieve high quality answers for the related entity finding task. Thus, the system preprocesses a topic using part-of-speech tagging and synonym dictionaries, and generates an enriched keyword query employing advanced features of the particular Web search engine. After retrieving a corpus of documents, the system constructs an extraction rule that consists of the source entity (and synonyms), the target entity type and words that should occur in the context of both (taken from the narrative relation description). After the extraction of potentially related entities, they are subjected to a deduplication mechanism and scored for each document with respect to the distance to the source entity. Finally, these scores are aggregated across the corpus by incorporating the rank position of a document. For homepage retrieval the HPI-system further employed advanced features of the used Web search engines -for instance to retrieve candidate URLs by queries such as "entity in anchor". Homepages are ranked by a weighted aggregation of feature vectors. The weight for each of the 17 used features was determined beforehand using a genetic learning algorithm. The submitted runs compare the performance of the three most popular search engines, that were employed by the system. <ref type="bibr" coords="5,442.22,499.57,79.78,8.74" target="#b7">(Hold et al., 2011)</ref> ICTNET The ICTNET group proposes a bipartite graph reinforcement model for entity ranking. Firstly, the candidate entities are extracted from related text snippets and are ranked based on a probabilistic model. Secondly, the lists which may contain several target entities are also extracted. Thirdly, a bipartite graph is constructed in which candidate entities and lists are considered as the two disjoint sets of graph vertices. Finally, the reinforcement algorithm is applied over the graph to get the final score for each candidate entity.</p><p>For the homepage finding, google is used to search for top-K urls and some heuristic rules are used to identify the real homepage. <ref type="bibr" coords="5,306.53,603.18,76.38,8.74" target="#b4">(Cao et al., 2011)</ref> LIA UAPV The LIA and iSmart group proposes a Question Answering approach to address REF. They were focused on a way to validate candidate named entities at the end of the QA process. For this, they proposed an unsupervised way to determine in what extent a named entity belongs to a given type. They started by extracting a fined grained type from topic's narrative field (e.g. "teammates"), collected web pages about it and computed word distribution on them. They used similar process for each candidate named entity. Then, they computed a degree of similarity between an entity and the type by comparing their word distribution. Finally, they proposed different ways to re-rank candidate named entities. <ref type="bibr" coords="6,202.43,113.02,99.76,8.74" target="#b2">(Bonnefoy et al., 2011)</ref> NiCT In 2010, the NiCT group mainly focused on improving target entity extraction and entity ranking, both of them play vital roles in the REF system. A Named Entity Recognition tool is first used to extract entities that match types of target entities such as organization, person, etc. Secondly, dependency tree-based patterns learnt automatically are employed to filter out the extracted entities that do not match fine-grained types of name entities such as university, airline, author, etc. In ranking part, a dependency tree-based similarity approach is proposed, which is better than language model. <ref type="bibr" coords="6,396.33,204.67,74.17,8.74" target="#b14">(Wu et al., 2011)</ref> PITTSIS Our method is based on a two-layer probability model for integrating document retrieval and entity extraction together. The document retrieval layer finds highly relevant documents, and the entity extraction layer extracts the right entities. Our goal in this year TREC is to set up a frame work for evaluating and exploring each individual layer as well as the overall workflows. This method helps to reduce the overall retrieval complexity while keeping high accuracy in locating target entities. <ref type="bibr" coords="6,374.61,284.37,77.35,8.74" target="#b9">(Li and He, 2011)</ref> PRIS The PRIS group proposes Document-Centered Model (DCM) and Entity-Centered Model (ECM) for the entity finding task. In DCM, documents are seen as a bridge. Both probabilities of a query and entity with respect to a document are estimated. In ECM, snippets extracted from documents are at the bottom to support entities. BM25 method is also introduced into ECM besides indri retrieval model. Another improvement aims to entity extraction. Special web page, NER tool and entity list generated by some rules are all taken into account. <ref type="bibr" coords="6,233.64,376.03,89.66,8.74">(Wang et al., 2011b)</ref> Purdue IR In the related entity find (REF) task, we generally follow our previous work on TREC Entity 2009. The structures of tables and lists are further investigated to extract related target entities from them. Moreover, we infer the type of target entities from the query topic and infer the type of candidate entities from their profiles, and then match the two types. <ref type="bibr" coords="6,197.99,443.78,80.40,8.74" target="#b6">(Fang et al., 2011)</ref> SIEL IIITH We use external resources like Wikipedia and Web, as Clueweb Category A dataset is not available. We extract all entities from Wikipedia using pattern finding techniques and indexed them with their type. We searched query in this index to find target entities. We use web search to find target entities not present in Wikipedia index. We then combine both the results to get final ranking. We then used Clueweb's URL-DocId mapping to find urls of target entities present in Clueweb dataset and present corresponding DocID as final results. This approach give satisfactory results in the absence of Clueweb dataset. <ref type="bibr" coords="6,222.07,547.39,83.30,8.74" target="#b10">(Shaik et al., 2011)</ref> UAms To address REF we look for homepages of entities of the target type that co-occur with the source entity in contexts of a certain size, emphasizing contexts that contain terms from the relation (the narrative provided with a topic). We experimented with context size by varying a window size parameter. To perform filtering based on type and homepage finding we use Freebase, which provides category labels and homepage URLs.</p><p>To remove NER errors we restrict the candidate entities to those in Freebase. In addition to Freebase homepage URLs we submitted entity strings to a web search engine to find homepages. <ref type="bibr" coords="6,186.09,651.00,80.70,8.74" target="#b3">(Bron et al., 2011)</ref> UAmsterdam The University of Amsterdam, group of Jaap Kamps, participates only in the main related entity finding task, and uses Wikipedia as a pivot to search for entities. The is very similar to last year's approach. Wikipedia topic categories are manually assigned to the query topics, which are more specific as the given target categories. These more specific target categories are used to retrieve entities within Wikipedia. To search web entities the external links in Wikipedia are used, and an anchor text index is searched. <ref type="bibr" coords="7,175.89,148.88,89.72,8.74" target="#b8">(Kamps et al., 2011)</ref> UWaterlooEng The University of Waterloo investigated whether related entity finding problem can be addressed by unsupervised approaches that rely primarily on statistical methods and common linguistic tools, such as named-entity taggers and syntactic parsers. An initial candidate list of entities is extracted from top ranked documents retrieved for the query, and then refined using a number of statistical and linguistic methods. One of the key components of their method consists of finding hyponyms of the category name specified in the narrative, representing candidate entities and hyponyms as vectors of grammatical dependency triples, and calculating similarity between them. (Vechtomova, 2011)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Common themes</head><p>In this subsection we discuss some general tendencies that we observed among participating systems this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Manual runs</head><p>The fraction of manual runs, as opposed to automatic ones, was relatively high this year (19 out of 48 runs); two teams (PRIS, SIEL IIITH) actually submitted manual runs only. Here, we briefly review the various types of interventions in the retrieval process that groups resorted to in their manual runs. The FDWIM team constructed queries for retrieval of support documents manually. The PRIS group checked the correctness of extraction for some part of entities and boosted the score of manually recognized entities. The Purdue IR group submitted a manual run in which the types of target entities were chosen manually. On a similar account, the UAmsterdam team assigned more specific entity types to each query by hand. The UAms group did not interfere much with the automatic execution of the retrieval workflow; they merely removed stop words and added the base forms of verbs and singular forms of plural terms to the narrative manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">External resources and Wikipedia</head><p>Another observation we make is that most runs (39 out of 48) used external resources. This is much higher than in last year (15 out of 41). On the other hand, the reliance on Wikipedia has decreased slightly (14 out of 48 runs treated Wikipedia in a special way, in contrast to last year's 16 out of 41). The former, in part, might be necessitated by the move to ClueWeb English; groups that could not handle the collection resorted to Web search engine APIs. The latter is probably due to the fact that Wikipedia pages are no longer accepted as relevant answers.</p><p>The BIT groups uses Google and Realnames search engines for homepage finding. HPI queries Freebase to find synonyms of entities; these, then, are used to construct a query which is sent to Google, Bing, or Yahoo!. Moreover, they make extensive use of search operators when querying Google. LIA UAPV uses the Yahoo! search engine to find the canonical form of a person name and then to find support documents (again, by querying Yahoo!). Finally, they use Yahoo! to find the homepages of retrieved entities. UWaterlooEng, PITTSIS, and NiCT also use Yahoo! to find support documents. NiCT uses YAGO/DBPedia data to learn patterns for "isA" relations. ICTNET uses Wordnet to find synonyms. UAms uses Bing, as well as Freebase/DBPedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Named entity recognition</head><p>Based on participating systems' descriptions it seems that only the UAmsterdam group did not use named entity tagging. Most teams (BIT, CARD UALR, FDWIM2010, ICTNET, LIA UAPV, PRIS, Purdue IR, and UAms) used the Stanford Named Entity Recognizer or some extension of it. HPI employed the SAP Business Objects Thingfinder, NiCT used the UIUC NE toolkit, and UWaterlooEng applied an LBJ-based Named Entity Recognizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Entity List Completion</head><p>The Entity List Completion (ELC) task has been introduced this year and ran as a pilot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task</head><p>ELC addresses essentially the same task as REF does: finding entities that are engaged in a specific relation with an input entity. There are three differences to REF:</p><p>• Entities are not represented by their homepages, but by a unique URI (from a specific collection, a sample of the Linked Open Data cloud),</p><p>• A small number of known relevant entities are made available as part of the topic definition, as examples.</p><p>• The target type is mapped to the most specific class within the DBPedia ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data collection</head><p>We use the Billion Triple Challenge (BTC) collection<ref type="foot" coords="8,336.12,408.66,3.97,6.12" target="#foot_2">3</ref> , a publicly available Semantic Web crawl; we consider this collection as a reasonable sample of Linked Open Data (LOD). Not all nodes in this Semantic Web graph are entities; identifying the nodes which refer to an entity is one of the challenges introduced by the task. Besides, the BTC collection appears to be noisy and incomplete. For instance, it contains far less Wikipedia entities than those which are the part of the ClueWeb B collection. This may be representative of the situation where entity classes are not that well covered by specialized entity repositories (as opposed to the coverage of the most popular entity classes in Wikipedia).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Topics and assessments</head><p>In order to help participants of 2009 use their previous approaches in the new setup, we use a subset of the 20 topics developed in the 2009 pilot run of the track. We had to exclude 6 topics from this set (#8, #9, #10, #13, #14, and #18) which had either too many additional entities as answers, or whose answer set from 2009 was complete, so could not be extended (for instance, all members of a band were found by participants of REF task in 2009). For each of the remaining 14 topics, the answer entities identified in the 2009 Entity track serve as the list of examples. Both the input entity and the examples were then manually mapped to LOD by track organizers with the help of a baseline entity search system. Entities might be identified by one or more URIs, but the set of URIs corresponding to a given entity is not necessarily complete. Additionally, the target type was mapped to the single most specific class within the DBPedia ontology Relevance judgements were also performed by the track organizers. All submitted runs were assessed up to rank 100 using a binary system of judgments for URIs; names were not evaluated.</p><p>One topic had proven too problematic because of the huge set of potentially correct answers (#1). Five more topics had to be excluded because no relevant entities were found for them in the BTC corpus <ref type="bibr" coords="9,194.59,421.86,127.76,8.74">(#2, #3, #6, #16, and #19)</ref>. This left us with 8 topics in total, listed in Table <ref type="table" coords="9,135.40,433.81,3.87,8.74" target="#tab_3">3</ref>. Similarly to REF, relevant entities were assigned to equivalence classes. Where topic denotes the topic ID (corresponds to the num field of the topic definition), doc is a BTC URI, rel is the judgment for the document (0 or 1), and class is a class number for the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Evaluation metrics</head><p>The main evaluation measure we use is Mean Average Precision. We also report on R-Precision (precision at rank R). Relevant entities previously seen in the ranking are considered irrelevant. Note that evaluation results are not computed using the standard trec eval tool, but a script developed specifically for the ELC task<ref type="foot" coords="10,306.37,381.39,3.97,6.12" target="#foot_4">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Runs and Results</head><p>For the ELC pilot task, three groups submitted a total of 5 runs, all of which were automatic. The results are shown in Table <ref type="table" coords="10,246.04,441.20,3.87,8.74" target="#tab_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Approaches</head><p>Below are the summaries of approaches, contributed by the participating teams (edited slightly for better presentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CMU Lira</head><p>The team from CMU (CMU Lira) focused on Entity List Completion using Set Expansion techniques. Set expansion refers to expanding a partial set of "seed" objects into a more complete set. They propose a two stage retrieval process. The first stage takes the given query entity and target entity examples as seeds and does set expansion. In the second stage, candidates generated by first stage are type checked and ranked. The first stage of this approach focuses on recall while the second stage tries to improve precision of the intermediate result list. They have submitted two runs, by doing set expansion on the Web and on the Clueweb corpus. <ref type="bibr" coords="10,297.65,605.03,82.61,8.74" target="#b5">(Dalvi et al., 2011)</ref> Purdue IR In the entity list completion (ELC) task, we leverage IR techniques to store the semantic data about entities and to retrieve the entities by Indri structured query retrieval language. Furthermore, we perform type matching between the target entity type and the candidate entity type. <ref type="bibr" coords="10,231.98,660.83,80.40,8.74" target="#b6">(Fang et al., 2011)</ref> UAms address ELC we look for entities similar to the given example entities. We find items that are linked to by example entities and consider other entities that link to those items to be candidate entities. For each entity we consider its links as well as the items to which it links. The combination of a link and a linked item forms a link-item pair. Each entity has its set of associated link-item pairs. We rank entities by set overlap between their link-item pairs and the example entity link-item pairs. We then re-rank these intermediate results based on word overlap between the topic narrative and entity link-item pairs. <ref type="bibr" coords="11,469.27,172.79,52.72,8.74;11,132.91,184.75,23.80,8.74" target="#b3">(Bron et al., 2011)</ref> 4 Summary</p><p>The second edition of the Entity track featured the Related Entity Finding (REF) as the main task: given an input entity, the type of the target entity (person, organization, product, or location), and the relation, described in free text, systems had to return homepages of related entities, and, optionally, the name of the entity.</p><p>For the second year of the track, 50 topics were created and assessed. In addition, participants were also requested to generate results for the 20 REF topics from 2009. We had slightly more submissions compared to the previous year (14 vs. 13 participants, 48 vs. 41 runs). This serves as a good motivation to run the task again next year. However, it becomes especially interesting if there are other applications within the same domain which have the potential to attract as many researchers as the REF task.</p><p>Entity 2010 also featured a pilot task: Entity List Completion (ELC). ELC is motivated by the same user scenario as REF, but entities are represented by their URIs in a Semantic Web crawl (the Billion Triple Collection), and a small number of example entities are made available as part of the topic definition. Our pilot run of the ELC task was not as popular as REF, probably due to the fact that participation needed a significant additional effort, because of the different nature of the dataset. We plan to run the task again in 2011, so that participants could have enough time to build their systems and process the data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,108.00,107.98,414.00,228.80"><head>Table 1 :</head><label>1</label><figDesc>Best automatic REF runs from each group ordered by NDCG@R. The columns of the table (from left to right) are: runID, group, type of the run (Automatic/Manual), whether the Wikipedia subcollection received a special treatment (Yes/No), whether any external resources were used (Yes/No), NDCG@R, MAP, R-Precision, number of relevant retrieved homepages, and number of primary retrieved homepages. Highest scores for each metric are in boldface.</figDesc><table coords="4,114.86,179.58,402.95,157.20"><row><cell>Run</cell><cell>Group</cell><cell cols="4">Type WP Ext. NDCG@R MAP</cell><cell cols="3">rPrec #rel #pri</cell></row><row><cell>bitDSHPRun</cell><cell>BIT</cell><cell>A</cell><cell>N</cell><cell>N</cell><cell cols="4">0.3694 0.2726 0.3075 150 314</cell></row><row><cell>FduWimET4</cell><cell>FDWIM2010</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="4">0.3420 0.2223 0.2837 140 333</cell></row><row><cell>KMR1PU</cell><cell>Purdue IR</cell><cell>A</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.2485 0.1555 0.2099</cell><cell cols="2">91 246</cell></row><row><cell>SuppHome</cell><cell>NiCT</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1696 0.0953 0.1453</cell><cell cols="2">74 187</cell></row><row><cell>ICTNETRun1</cell><cell>ICTNET</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1611 0.0839 0.1305</cell><cell cols="2">95 173</cell></row><row><cell>UWAT2</cell><cell cols="2">UWaterlooEng A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1393 0.0722 0.1223</cell><cell cols="2">96 154</cell></row><row><cell>LearnDPI</cell><cell>LIA UAPV</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0766 0.0305 0.0591</cell><cell>72</cell><cell>81</cell></row><row><cell>G16</cell><cell>HPI</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0745 0.0357 0.0539</cell><cell>27</cell><cell>71</cell></row><row><cell>UAbaselinkA</cell><cell>UAmsterdam</cell><cell>A</cell><cell>Y</cell><cell>N</cell><cell cols="2">0.0496 0.0185 0.0349</cell><cell>34</cell><cell>81</cell></row><row><cell>ilpsA500</cell><cell>UAms</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0460 0.0178 0.0325</cell><cell>35</cell><cell>88</cell></row><row><cell>YahooEnHP</cell><cell>PITTSIS</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0375 0.0118 0.0229</cell><cell>37</cell><cell>42</cell></row><row><cell cols="2">CARDENSMBLE CARD UALR</cell><cell>A</cell><cell>N</cell><cell>N</cell><cell cols="2">0.0084 0.0000 0.0003</cell><cell>20</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,108.00,544.76,414.00,143.18"><head>Table 2 :</head><label>2</label><figDesc>Best manual REF runs from each group ordered by NDCG@R. Highest scores for each metric are in boldface.</figDesc><table coords="4,128.32,578.56,376.02,109.38"><row><cell>Run</cell><cell>Group</cell><cell cols="4">Type WP Ext. NDCG@R MAP</cell><cell cols="2">rPrec #rel #pri</cell></row><row><cell>bitRFRun</cell><cell>BIT</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="3">0.3897 0.2876 0.3209 153 319</cell></row><row><cell cols="3">FduWimET3 FDWIM2010 M</cell><cell>Y</cell><cell>Y</cell><cell cols="3">0.3376 0.2218 0.2886 116 297</cell></row><row><cell>KMR3PU</cell><cell>Purdue IR</cell><cell>M</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.2917 0.1916 0.2505</cell><cell>93 296</cell></row><row><cell>EntityHP1</cell><cell>PITTSIS</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="3">0.2884 0.1664 0.2258 140 278</cell></row><row><cell>PRIS2</cell><cell>PRIS</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="3">0.2846 0.1607 0.2296 128 312</cell></row><row><cell>SIELRUN1</cell><cell>SIEL IIITH</cell><cell>M</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.1576 0.1019 0.1414</cell><cell>38 198</cell></row><row><cell cols="2">ilpsM50agfil UAms</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0718 0.0331 0.0496</cell><cell>36</cell><cell>99</cell></row><row><cell cols="3">UAcatslinkA UAmsterdam M</cell><cell>Y</cell><cell>N</cell><cell cols="2">0.0708 0.0485 0.0678</cell><cell>29</cell><cell>84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,108.00,464.59,414.00,233.75"><head>Table 3 :</head><label>3</label><figDesc>ELC topics. #ex is the number of example entities provided and #rel is the number of additional relevant entities identified.</figDesc><table coords="9,132.48,498.38,69.51,8.74"><row><cell>ID</cell><cell>Narrative</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,108.00,108.53,414.00,133.16"><head>Table 4 :</head><label>4</label><figDesc>Runs submitted to ELC task, ordered by MAP. The columns of the table (from left to right) are: runID, group, type of the run (Automatic/Manual), whether the ClueWeb09 collection was used (Yes/No), whether any external resources were used (Yes/No), MAP, Rprecision, and number of relevant retrieved results. Highest scores for each metric are in boldface.</figDesc><table coords="10,168.80,168.18,291.73,73.52"><row><cell>Run</cell><cell>Group</cell><cell cols="4">Type CW Ext. MAP rPrec #rel</cell></row><row><cell>KMR5PU</cell><cell>Purdue IR</cell><cell>A</cell><cell>N</cell><cell>N 0.2613 0.3116</cell><cell>33</cell></row><row><cell cols="2">ilpsSetOLnar UAms</cell><cell>A</cell><cell>N</cell><cell>N 0.1152 0.0899</cell><cell>43</cell></row><row><cell>ilpsSetOL</cell><cell>UAms</cell><cell>A</cell><cell>N</cell><cell>N 0.1105 0.0947</cell><cell>40</cell></row><row><cell cols="3">LiraSealClwb CMU LIRA A</cell><cell>Y</cell><cell>N 0.0755 0.0494</cell><cell>15</cell></row><row><cell cols="3">LiraSealgoog CMU LIRA A</cell><cell>N</cell><cell>Y 0.0228 0.0274</cell><cell>15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,112.95,108.70,404.11,578.61"><head>Table</head><label></label><figDesc>All REF runs ordered by NDCG@R. Highest scores for each metric are in boldface.</figDesc><table coords="13,130.07,132.46,372.77,554.85"><row><cell>Run</cell><cell>Group</cell><cell cols="4">Type WP Ext. NDCG@R MAP</cell><cell cols="3">rPrec #rel #pri</cell></row><row><cell>bitRFRun</cell><cell>BIT</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="4">0.3897 0.2876 0.3209 153 319</cell></row><row><cell>bitDSHPRun</cell><cell>BIT</cell><cell>A</cell><cell>N</cell><cell>N</cell><cell cols="4">0.3694 0.2726 0.3075 150 314</cell></row><row><cell>bitDSRRun</cell><cell>BIT</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="4">0.3694 0.2726 0.3075 150 314</cell></row><row><cell>FduWimET4</cell><cell>FDWIM2010</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="4">0.3420 0.2223 0.2837 140 333</cell></row><row><cell>FduWimET2</cell><cell>FDWIM2010</cell><cell>A</cell><cell>Y</cell><cell>Y</cell><cell cols="4">0.3382 0.2272 0.2917 120 303</cell></row><row><cell>FduWimET3</cell><cell>FDWIM2010</cell><cell>M</cell><cell>Y</cell><cell>Y</cell><cell cols="4">0.3376 0.2218 0.2886 116 297</cell></row><row><cell>FduWimET1</cell><cell>FDWIM2010</cell><cell>A</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.3259 0.2235 0.2823</cell><cell cols="2">83 276</cell></row><row><cell>KMR3PU</cell><cell>Purdue IR</cell><cell>M</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.2917 0.1916 0.2505</cell><cell cols="2">93 296</cell></row><row><cell>EntityHP1</cell><cell>PITTSIS</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="4">0.2884 0.1664 0.2258 140 278</cell></row><row><cell>PRIS2</cell><cell>PRIS</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="4">0.2846 0.1607 0.2296 128 312</cell></row><row><cell>EntityHP</cell><cell>PITTSIS</cell><cell>M</cell><cell>Y</cell><cell>Y</cell><cell cols="4">0.2837 0.1556 0.2009 168 312</cell></row><row><cell>KMR1PU</cell><cell>Purdue IR</cell><cell>A</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.2485 0.1555 0.2099</cell><cell cols="2">91 246</cell></row><row><cell>PRIS3</cell><cell>PRIS</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="4">0.2160 0.1141 0.1498 141 301</cell></row><row><cell>PRIS1</cell><cell>PRIS</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="4">0.2158 0.1180 0.1639 130 310</cell></row><row><cell>PRIS4</cell><cell>PRIS</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="4">0.1761 0.0984 0.1361 130 291</cell></row><row><cell>SuppHome</cell><cell>NiCT</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1696 0.0953 0.1453</cell><cell cols="2">74 187</cell></row><row><cell>SuppHomeIsA</cell><cell>NiCT</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1655 0.0971 0.1446</cell><cell cols="2">61 174</cell></row><row><cell>ICTNETRun1</cell><cell>ICTNET</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1611 0.0839 0.1305</cell><cell cols="2">95 173</cell></row><row><cell>SIELRUN1</cell><cell>SIEL IIITH</cell><cell>M</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.1576 0.1019 0.1414</cell><cell cols="2">38 198</cell></row><row><cell>SIELRUN2</cell><cell>SIEL IIITH</cell><cell>M</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.1576 0.1019 0.1414</cell><cell cols="2">38 198</cell></row><row><cell>SIEL10RUN1</cell><cell>SIEL IIITH</cell><cell>M</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.1576 0.1019 0.1414</cell><cell cols="2">38 198</cell></row><row><cell>UWAT2</cell><cell cols="2">UWaterlooEng A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1393 0.0722 0.1223</cell><cell cols="2">96 154</cell></row><row><cell>UWAT1</cell><cell cols="2">UWaterlooEng A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1264 0.0608 0.1033</cell><cell cols="2">95 151</cell></row><row><cell>UWEntTI</cell><cell cols="2">UWaterlooEng A</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.1259 0.0603 0.0974</cell><cell cols="2">95 148</cell></row><row><cell>SuppIsA</cell><cell>NiCT</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1245 0.0703 0.0991</cell><cell cols="2">76 143</cell></row><row><cell>Supp</cell><cell>NiCT</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.1237 0.0647 0.0909</cell><cell cols="2">85 150</cell></row><row><cell>LearnDPI</cell><cell>LIA UAPV</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0766 0.0305 0.0591</cell><cell>72</cell><cell>81</cell></row><row><cell>G16</cell><cell>HPI</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0745 0.0357 0.0539</cell><cell>27</cell><cell>71</cell></row><row><cell>Comp</cell><cell>LIA UAPV</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0737 0.0261 0.0463</cell><cell>74</cell><cell>74</cell></row><row><cell>ValueDoc</cell><cell>PITTSIS</cell><cell>M</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.0723 0.0251 0.0500</cell><cell>50</cell><cell>54</cell></row><row><cell>ilpsM50agfil</cell><cell>UAms</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0718 0.0331 0.0496</cell><cell>36</cell><cell>99</cell></row><row><cell>UAcatslinkA</cell><cell>UAmsterdam</cell><cell>M</cell><cell>Y</cell><cell>N</cell><cell cols="2">0.0708 0.0485 0.0678</cell><cell>29</cell><cell>84</cell></row><row><cell>ilpsM50</cell><cell>UAms</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0692 0.0298 0.0455</cell><cell>35</cell><cell>94</cell></row><row><cell>UAcatscombB</cell><cell>UAmsterdam</cell><cell>M</cell><cell>Y</cell><cell>N</cell><cell cols="2">0.0685 0.0323 0.0452</cell><cell>47</cell><cell>82</cell></row><row><cell>G64</cell><cell>HPI</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0625 0.0252 0.0500</cell><cell>29</cell><cell>76</cell></row><row><cell>RanksDivComp</cell><cell>LIA UAPV</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0610 0.0200 0.0373</cell><cell>76</cell><cell>76</cell></row><row><cell>ilpsM50var</cell><cell>UAms</cell><cell>M</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0571 0.0234 0.0375</cell><cell>40</cell><cell>77</cell></row><row><cell>UAbaselinkA</cell><cell>UAmsterdam</cell><cell>A</cell><cell>Y</cell><cell>N</cell><cell cols="2">0.0496 0.0185 0.0349</cell><cell>34</cell><cell>81</cell></row><row><cell>ilpsA500</cell><cell>UAms</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0460 0.0178 0.0325</cell><cell>35</cell><cell>88</cell></row><row><cell>Div</cell><cell>LIA UAPV</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0428 0.0129 0.0189</cell><cell>76</cell><cell>77</cell></row><row><cell>YahooEnHP</cell><cell>PITTSIS</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0375 0.0118 0.0229</cell><cell>37</cell><cell>42</cell></row><row><cell>UAbaseanchB</cell><cell>UAmsterdam</cell><cell>A</cell><cell>Y</cell><cell>N</cell><cell cols="2">0.0314 0.0063 0.0167</cell><cell>47</cell><cell>42</cell></row><row><cell>Y64</cell><cell>HPI</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0222 0.0055 0.0223</cell><cell>14</cell><cell>39</cell></row><row><cell cols="2">CARDENSMBLE CARD UALR</cell><cell>A</cell><cell>N</cell><cell>N</cell><cell cols="2">0.0084 0.0000 0.0003</cell><cell>20</cell><cell>1</cell></row><row><cell>CARDSGFCS</cell><cell>CARD UALR</cell><cell>A</cell><cell>N</cell><cell>N</cell><cell cols="2">0.0081 0.0001 0.0006</cell><cell>19</cell><cell>2</cell></row><row><cell>CARDFPR</cell><cell>CARD UALR</cell><cell>A</cell><cell>N</cell><cell>N</cell><cell cols="2">0.0077 0.0005 0.0018</cell><cell>18</cell><cell>3</cell></row><row><cell>CARDHITS</cell><cell>CARD UALR</cell><cell>A</cell><cell>N</cell><cell>N</cell><cell cols="2">0.0070 0.0001 0.0003</cell><cell>24</cell><cell>2</cell></row><row><cell>B64</cell><cell>HPI</cell><cell>A</cell><cell>N</cell><cell>Y</cell><cell cols="2">0.0178 0.0044 0.0122</cell><cell>12</cell><cell>30</cell></row><row><cell>Median</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.1252 0.0628 0.0983</cell><cell cols="2">74 149</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,123.24,664.73,276.30,6.99"><p>Note that the input entity does not need to be limited to these four types.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,123.24,648.38,182.36,6.99"><p>http://trec.nist.gov/data/entity/10/eval-entity.pl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,123.24,679.25,101.62,6.64"><p>http://vmlion25.deri.ie/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="8,123.24,688.76,135.50,6.64"><p>http://wiki.dbpedia.org/Ontology</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="10,123.24,680.88,229.64,6.64"><p>http://trec.nist.gov/data/entity/10/eval-entity-elc.pl</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,108.00,485.56,414.00,8.74;11,117.96,497.52,404.04,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,425.57,485.56,96.43,8.74;11,117.96,497.52,46.06,8.74">UALR at 2010 TREC Conference</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bisgin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Swamy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,184.71,497.52,306.99,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,517.44,414.00,8.74;11,117.96,529.40,404.04,8.74;11,117.96,541.35,22.69,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,421.38,517.44,100.63,8.74;11,117.96,529.40,75.79,8.74">Overview of the TREC 2009 Entity Track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,213.68,529.40,303.79,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,561.28,414.00,8.74;11,117.96,573.23,404.04,8.74;11,117.96,585.19,208.10,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,280.59,561.28,241.41,8.74;11,117.96,573.23,249.95,8.74">LIA-iSmart at TREC 2010 : A Web-oriented Language Modeling Approach for Question Related Entity Finding</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Benoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,392.03,573.23,129.97,8.74;11,117.96,585.19,177.55,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,605.12,414.00,8.74;11,117.96,617.07,404.03,8.74;11,117.96,629.03,286.54,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,482.47,605.12,39.53,8.74;11,117.96,617.07,334.46,8.74">The University of Amsterdam at TREC 2010: Session, Entity and Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,472.34,617.07,49.65,8.74;11,117.96,629.03,255.99,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,648.95,414.00,8.74;11,117.96,660.91,379.32,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,379.57,648.95,142.43,8.74;11,117.96,660.91,18.15,8.74">ICTNET at Entity Track TREC 2010</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,157.53,660.91,309.21,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,680.83,414.00,8.74;11,117.96,692.79,352.21,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,270.47,680.83,246.93,8.74">Entity List Completion Using Set Expansion Techniques</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,130.42,692.79,309.21,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,101.06,80.15,8.74;12,265.25,101.06,256.75,8.74;12,117.96,113.02,404.04,8.74;12,117.96,124.97,393.08,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,421.44,101.06,100.56,8.74;12,117.96,113.02,404.04,8.74;12,117.96,124.97,32.40,8.74">Purdue at TREC 2010 Entity Track: a Probabilistic Framework for Matching Types between Candidate and Target Entities</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S</forename><surname>Al-Ansari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,171.28,124.97,309.22,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,144.90,414.00,8.74;12,117.96,156.85,404.03,8.74;12,117.96,168.81,258.28,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,497.09,144.90,24.91,8.74;12,117.96,156.85,298.72,8.74">ECIR -A Lightweight Approach for Entity-Centric Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Leben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Emde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Barczynski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,442.08,156.85,79.92,8.74;12,117.96,168.81,227.73,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,188.73,414.00,8.74;12,117.96,200.69,404.04,8.74;12,117.96,212.64,89.14,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,284.20,188.73,237.79,8.74;12,117.96,200.69,137.78,8.74">Using Anchor Text, Spam Filtering and Wikipedia for Web Search and Entity Ranking</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kaptein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,276.68,200.69,245.32,8.74;12,117.96,212.64,58.60,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,232.57,414.00,8.74;12,117.96,244.52,258.28,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,184.10,232.57,240.40,8.74">Searching for Entities When Retrieval Meets Extraction</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,444.80,232.57,77.20,8.74;12,117.96,244.52,227.73,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,264.45,414.00,8.74;12,117.96,276.40,379.32,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,349.38,264.45,172.62,8.74;12,117.96,276.40,18.15,8.74">IIIT Hyderabad at Entity Track TREC 2010</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gosukonda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,157.53,276.40,309.21,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,296.33,414.00,8.74;12,117.96,308.28,352.21,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,182.34,296.33,335.09,8.74">Related Entity Finding: University of Waterloo at TREC 2010 Entity Track</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,130.42,308.28,309.22,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,328.21,414.00,8.74;12,117.96,340.16,404.04,8.74;12,117.96,352.12,145.68,8.74" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<title level="m" coord="12,285.04,328.21,236.96,8.74;12,117.96,340.16,184.05,8.74;12,324.87,340.16,197.14,8.74;12,117.96,352.12,110.15,8.74">A Multiple-Stage Framework for Related Entity Finding: FDWIM at TREC 2010 Entity Track</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010). a</note>
</biblStruct>

<biblStruct coords="12,108.00,372.04,414.00,8.74;12,117.96,384.00,404.03,8.74;12,117.96,395.96,191.19,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,454.56,372.04,67.44,8.74;12,117.96,384.00,226.40,8.74">PRIS at TREC 2010: Related Entity Finding Task of Entity Track</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,368.75,384.00,153.25,8.74;12,117.96,395.96,155.11,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct coords="12,108.00,415.88,414.00,8.74;12,117.96,427.84,286.54,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,250.26,415.88,199.86,8.74">NiCT at TREC 2010: Related Entity Finding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,472.34,415.88,49.65,8.74;12,117.96,427.84,255.99,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,447.76,414.00,8.74;12,117.96,459.72,404.04,8.74;12,117.96,471.67,22.69,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,290.42,447.76,231.57,8.74;12,117.96,459.72,63.20,8.74">Reconstruct Logical Hierarchical Sitemap for Related Entity Finding</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,204.57,459.72,312.89,8.74">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
