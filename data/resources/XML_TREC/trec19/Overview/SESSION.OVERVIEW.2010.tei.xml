<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.25,112.05,295.50,15.12">Overview of the TREC 2010 Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,102.74,148.66,102.07,10.48;1,204.80,147.04,1.41,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.01,148.66,76.57,10.48;1,303.58,147.04,1.88,6.99"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.79,148.66,63.57,10.48;1,389.35,147.04,1.88,6.99"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.96,148.66,83.30,10.48;1,509.26,147.04,1.88,6.99"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science &amp; Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.25,112.05,295.50,15.12">Overview of the TREC 2010 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F16B7F2B3E83E29E36FEE31277476C19</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research in Information Retrieval has traditionally focused on serving the best results for a single query. In practice however users often enter queries in sessions of reformulations. The Sessions Track at TREC 2010 implements an initial experiment to evaluate the effectiveness of retrieval systems over single query reformulations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research in Information Retrieval has traditionally focused on serving the best results for a query.But users often begin an interaction with a search engine with a sufficiently ill-specified query that they will need to reformulate before they find what they are looking for: early studies on web search query logs showed that half of all Web users reformulated their initial query: 52% of the users in 1997 Excite data set, 45% of the users in the 2001 Excite dataset <ref type="bibr" coords="1,218.54,357.74,14.61,8.74" target="#b12">[13]</ref>. A search engine may be able to better serve a user not by ranking the most relevant results to each query in the sequence, but by ranking results that help "point the way" to what the user is really looking for, or by complementing results from previous queries in the sequence with new results, or in other currently-unanticipated ways.</p><p>The standard evaluation paradigm of controlled laboratory experiments is unable to assess the effectiveness of retrieval systems to an actual user experience of querying with reformulations. On the other hand, interactive evaluation is both noisy due to the high degrees of freedom of user interactions, and expensive due to its low reusability and need for many test subjects. The TREC 2010 Session Track is an attempt to evaluate the simplest form of user interaction with a retrieval engine: a single query reformulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation Tasks</head><p>We call a sequence of reformulations in service of satisfying an information need a "session", and thes goal of this track are: (G1) to test whether systems can improve their performance for a given query by using a previous query, and (G2) to evaluate system performance over an entire query session instead of a single query.</p><p>For this first year, we limited the focus of the track to sessions of two queries, and further limited the focus to particular types of sessions (described in Section 3.2). This is partly for pragmatic reasons regarding the difficulty of obtaining session data, and partly for reasons of experimental design and analysis: allowing longer sessions introduces many more degrees of freedom, requiring more data from which to base conclusions.</p><p>A set of 150 query pairs (initial query, query reformulation) was provided to participants by NIST. For each such pair the participants submitted 3 (three) ranked lists of documents for three experimental conditions, 1. one over the initial query (RL1) 2. one over the query reformulation, ignoring the initial query (RL2) 3. one over the query reformulation taking into consideration the initial query (RL3) By using the ranked lists (RL2) and (RL3) we evaluated the ability of systems to utilize prior history (G1). By using the returned ranked lists (RL1) and (RL3) we evaluate the quality of ranking function over the entire session (G2). Note that this was not be an interactive track. Query reformulations were provided by NIST along with the initial queries. Further note that when retrieving results for (RL3) the only extra information about the user's intent is the initial query. This was a single-phase track, with no feedback provided by the assessors.</p><p>3 Test Collection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpus</head><p>The track used the ClueWeb09 collection. The full collection consists of roughly 1 billion web pages, comprising approximately 25TB of uncompressed data (5TB compressed) in multiple languages. The dataset was crawled from the Web during January and February 2009. Participants were encouraged to use the entire collection, however submissions over the smaller "Category B" collection of 50 million documents were accepted. Note that Category B submissions was evaluated as if they were Category A submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Queries and Reformulations</head><p>There is a large volume of research regarding query reformulations which follows two lines of work: a descriptive line that analyzes query logs and identifies a taxonomy of query reformulations based on certain user actions over the initial query (e.g. <ref type="bibr" coords="2,244.53,413.83,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,263.43,413.83,7.75,8.74" target="#b0">1]</ref>) and a predictive line that trains different models over query logs to predict good query reformulations (e.g. <ref type="bibr" coords="2,280.57,425.78,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,294.75,425.78,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="2,306.18,425.78,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="2,322.57,425.78,7.20,8.74" target="#b8">9]</ref>). Analyses of query logs have shown a number of different types of query reformulations with three of them being consistent across different studies (e.g. <ref type="bibr" coords="2,72.00,449.69,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,85.83,449.69,11.62,8.74" target="#b9">10]</ref>):</p><p>Specifications: the user enters a query, realizes the results are too broad or that they wanted a more detailed level of information, and reformulates a more specific query.</p><p>Drifting/Parallel Reformulation: the user entered a query, then reformulated to another query with the same level of specification but moved to a different aspect or facet of their information need.</p><p>Generalizations: the user enters a query, realizes that the results are too narrow or that they wanted a wider range of information, and reformulated a more general query.</p><p>In the absence of query logs, Dang and Croft <ref type="bibr" coords="2,285.03,550.16,10.52,8.74" target="#b2">[3]</ref> simulated query reformulations by using anchor text, which is readily available. In the Session Track we used a different approach. To construct the query pairs (initial query, query reformulation) we started with the TREC 2009 and 2010 Web Track diversity topics. This collection consists of topics that have a "main theme" and a series of "aspects" or "sub-topics". The Web Track queries were sampled from the query log of a commercial search engine and the sub-topics were constructed by a clustering algorithm <ref type="bibr" coords="2,237.08,609.93,15.50,8.74" target="#b10">[11]</ref> run over these queries aggregating query reformulations occurring in the same session. We used the aspect and main theme of these collection topics in a variety of combinations to provide a simulation of an initial and second query. An example of part of a 2009 Web track query is shown below. &lt;topic number="4" type="faceted"&gt; &lt;query&gt;toilet&lt;/query&gt; &lt;description&gt; Find information on buying, installing, and repairing toilets. &lt;/description&gt; &lt;subtopic number="1" type="inf"&gt; What different kinds of toilets exist, and how do they differ? &lt;/subtopic&gt; &lt;subtopic number="2" type="inf"&gt; I'm looking for companies that manufacture residential toilets. &lt;/subtopic&gt; &lt;subtopic number="3" type="inf"&gt; Where can I buy parts for American Standard toilets? &lt;/subtopic&gt; &lt;subtopic number="4" type="inf"&gt; How do I fix a toilet that isn't working properly? &lt;/subtopic&gt; &lt;subtopic number="5" type="inf"&gt; What companies manufacture bidets? &lt;/subtopic&gt; &lt;subtopic number="6" type="inf"&gt; I'm looking for a Kohler wall-hung toilet. Where can I buy one? &lt;/subtopic&gt; &lt;/topic&gt; To construct specification reformulations (1) used the Web Track ¡query¿ section as the initial query, (2) we selected one of the subtopics and used the ¡subtopic¿ section as the description of the actual information need, and (3) we then extract keywords out of the subtopic description and used these keywords as the query reformulation. For instance, in the example above we used the Web Track query "toilet" as the first query, we selected one of the subtopics as the information need ("I'm looking for a Kohler wall-hung toilet. Where can I buy one?") and we extract the keyword "Kohler" and used it as the second query (query reformulation). Essentially, this example simulates a user that is actually looking for a Kohler wall-hung toilet but he poses a more general query to the search engine ("toilet"). Given that "toilet" is a quite general term the user reformulates his query to "Kohler" to find web pages closer to his information need. &lt;topic number="1" reformtype="specification" source="webtrack2009"&gt; &lt;query&gt;toilet&lt;/query&gt; &lt;reformulation&gt;Kohler&lt;/reformulation&gt; &lt;description&gt;I'm looking for a Kohler wall-hung toilet.</p><p>Where can I buy one?&lt;/description&gt; &lt;/topic&gt; To construct drifting reformulations (1) we selected two of the subtopics and used the ¡subtopic¿ sections as the description of the two information needs, and (2) we then extract keywords out of the subtopic descriptions and used these keywords as the first query and the query reformulation. For instance, in the example above we selected "Where can I buy parts for American Standard toilets?" and "I'm looking for a Kohler wall-hung toilet. Where can I buy one?" as the two information needs. Then we extracted the keywords "American Standard" and "Kohler" and used them as the initial query and the query reformulation. Essentially this reformulation simulates a user that first wants to buy some toilet parts from American Standard but while he is browsing the results of the query or possibly other web pages he decides that he also wants to purchase Kohler wall-hungs and thus there is a slight drifting in his information need. &lt;topic number="2" reformtype="drifting" source="webtrack2009"&gt; &lt;query&gt;American Standard&lt;/query&gt; &lt;description&gt;Where can I buy parts for American Standard toilets?&lt;/description &lt;reformulation&gt;Kohler&lt;/reformulation&gt; &lt;rdescription&gt;I'm looking for a Kohler wall-hung toilet.</p><p>Where can I buy one?&lt;/rdescription&gt; &lt;/topic&gt; Finally, to construct generalization reformulations we followed a slightly more complicated process. The reason is that the Web Track queries include queries that are under-specified and then subtopics that are the exact specification of different information needs but it does not include over-specified or mis-specified queries that could lead the user to generalize his initial query.</p><p>Thus in the first method to construct generalization reformulation (1) we selected one of the subtopics and we extracted as many keywords as possible to construct an over specified query, e.g. (we selected "What different kinds of toilets exist, and how do they differ?" and we extracted the keywords "different kinds of toilets" which seems to be a lexical over-specification), (2) we used a subset of these keywords to generalize the initial query (e.g. "toilet"). Essentially this reformulation simulates a user that first wanted to find what different kinds of toilets exist, and how do they differ but he lexically over-specified his need, the returned results were poor and thus reformulated his initial query to a more general one. &lt;topic number="3" reformtype="generalization" source="webtrack2009"&gt; &lt;query&gt;different kinds of toilets&lt;/query&gt; &lt;reformulation&gt;toilets&lt;/reformulation&gt; &lt;description&gt; What different kinds of toilets exist, and how do they differ?&lt;/description&gt; &lt;/topic&gt;</p><p>The second method used was to essentially consider that a user mis-specifies his need to something very narrow and then he generalizes. In this case (1) we selected one of the subtopics or query description from the Web Track topics as the information need, (2) extracted keywords from a different subtopic that seemed related but essentially it was a mis-specification of something very narrow, and (3) extracted keywords from the subtopic used as information need.</p><p>&lt;topic number="4" reformtype="generalization" source="webtrack2009"&gt; &lt;query&gt;American Standard toilet&lt;/query&gt; &lt;reformulation&gt;toilet&lt;/reformulation&gt; &lt;description&gt;Find information on buying, installing, and repairing toilets.&lt;/description&gt; &lt;/topic&gt; Furthermore, given that not all queries in the Web Track proved appropriate to construct query reformulations we used some of the Million Query 2009 (MQ09) queries and built query reformualations and information needs from scratch. The MQ09 queries was used as the initial queries. Often they were submitted to a web search engine whose results were used as an assistance to create information needs. Queries from the "Related Search" section of Bing were also used as an assistance to construct query reformulations. Often, if the queries from the "Related Search" were not good examples one of them was selected, re-submitted to Bing and the new "Related Search" section was used for assistance. Examples of such queries can be viewed bellow.</p><p>&lt;topic number="47" reformtype="specification" source="mqtrack2009"&gt; &lt;query&gt;cuttle fish bone&lt;/query&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submissions</head><p>Sites were permitted to submit up to three runs. Each submitted run includes three separate ranked result lists for all 150 topics. Files were named "runTag.RLn", where "runTag" is a unique identifier for the site and the particular submission, and "RLn" is RL1, RL2, or RL3, depending on the experimental condition.</p><p>The track received 27 runs from the 10 groups listed in Table <ref type="table" coords="5,341.87,527.45,3.87,8.74" target="#tab_0">1</ref>. Seven sites submitted three runs and three sites submitted two runs. Section B at the end of the document summarizes the methods used by each of the participating sites. For further details on the techniques used refer to the individual groups reports for the Session Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Session Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relevance Judgments</head><p>Judging was done by assessors at NIST. The top 10 documents of all the ranked lists submitted by all participants (i.e. RL1, RL2, and RL3) were pooled together for each one of the topics (depth-10 pooling). For each topic the NIST assessors judged documents with respect to one or two information needs simultaneously. In the case of specification and generalization reformulations, both the initial query and its reformulation were assumed to represent the same information need, and assessors judged all documents against that need. In the case of drifting reformulations the query reformulation represented an information need different from (but related to) the information need of the initial query, and thus judgments were made with respect to both needs. (Note that the information needs were not provided to participants.) Due to limited resources, topics assigned to each NIST assessor were prioritized by the size of the depth-10 pool -the smaller the pool size the higher the priority of the topic -and NIST assessors rotated between the three types of query reformulations preserving this order. The judging process lasted approximately two weeks and judgments were provided for 136 out of the 150 sessions. The IDs of the topics that were not judged are the following: 24, 30, 35, 36, 40, 58, 70, 100, 114, 118, 120, 126, 130, and 136. The judged 136 topics include 47 specification, 47 drifting and 42 generalization reformulation types.</p><p>A total of 33,121 documents were judged, with 11,525 documents being judged with respect to two information needs, resulting in 44,646 relevance judgments. Of the 44,646 judgments, 2,958 (6.6%) were highly relevant, 4,798 (10.7%) were relevant, and 36,890 (82.7%) non-relevant.</p><p>The specification topics had an average of 15.5 highly relevant documents, 23.6 relevant, and 201 non-relevant. The generalization topics had more relevant material, with an average 23.1 highly relevant documents, 40.5 relevant, and 181 non-relevant. The drifting topics had an average of 13.8 highly relevant, 20.1 relevant, and 211.9 non-relevant documents for the initial information need, and 12.8, 22.1, and 210.1 respectively for the reformulated information need.</p><p>The format of the qrels file generated is topicID 0 docID rel1.rel2</p><p>i.e. the typical qrels format except for the rel field that has been replaced by rel.rel. For generalization and specification topics, the value on the left of the decimal is the relevance judgment, and the right value is always -1. For drifting topics, the values are for the two information needs respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Normalized session DCG</head><p>Järvelin et al. <ref type="bibr" coords="6,133.14,458.62,10.52,8.74" target="#b7">[8]</ref> extended the nDCG metric into a session metric that takes into account multiple interactive queries. The metric discounts documents that appear lower in the ranked list for a given query and further discounts documents that require query reformulations to be found. In a sense the new model on which the session-based nDCG (nsDCG) is defined incorporates a cost for reformulating a query.</p><p>The session-based nDCG is defined as follows: for each query in a series of reformulations, one can compute DCG, as defined above, in isolation of all other queries in the series, i.e. for a query q,</p><formula xml:id="formula_0" coords="6,240.06,547.80,130.68,30.32">DCG@k(q) = k i=1 rel(i, q) (1 + log b (i))</formula><p>where b is the base of the logarithm of the discount function and controls the persistence or patience of the user to move down the ranked list of documents for a given query. Each query in this series of reformulations is penalized based on a similar discount as a function of the position q of the query in the series. Based on that the session DCG (sDCG) for a query in position q is defined as,</p><formula xml:id="formula_1" coords="6,213.71,648.67,184.57,23.23">sDCG@k(q) = 1 (1 + log bq (q)) * DCG@k(q)</formula><p>where bq is the logarithm base for the query discount. The session DCG can be written then as,</p><formula xml:id="formula_2" coords="7,199.75,95.86,211.31,65.27">sDCG@k(q) = 1 (1 + log bq (q)) Z i=1 rel(i, q) (1 + log b (i)) = Z i=1 rel(i, q) (1 + log bq (q))(1 + log b (i))</formula><p>As with the standard formulation of DCG, we can compute an "ideal" score based on an optimal ranking of documents in decreasing order of relevance to the query, then normalize DCG by that ideal score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation metrics for the Session Track</head><p>We have evaluated the submitted runs by three metrics, (a) nsDCG@10 as described above, (b) a variant that penalizes duplicates in the second ranking (nsDCG dupes@10), and (c) nDCG10 of each submitted ranking. The first two evaluate the entire session and thus for each one of the two metrics we compute an evaluation score for RL1→RL2 (nsDCG@10.RL12) and an evaluation score for RL1→RL3 (nsDCG@10.RL13). nsDCG@10 for RL1→RL3 is the official metric for (G2). Comparing the nsDCG@10 or nsDCG dupes@10 scores for RL1→RL2 and RL1→RL3, or the nDCG@10 scores for RL2 and RL3 we evaluate how well systems performed with respect to (G1).</p><p>The specific instantiation of nsDCG10 used is :</p><formula xml:id="formula_3" coords="7,151.69,355.64,307.63,30.20">10 r=1 2 rel(r,RL1) -1 (log 2 (r + 1)) * (log 4 (1 + 3)) + 10 r=1 2 rel(r,RL2/RL3) -1 (log 2 (r + 10 + 1)) * (log 4 (2 + 3))</formula><p>where rel(r,RL1) is the relevance of the document in rank r in RL1 and rel(r,RL2/RL3) is the relevance of the document at rank r in RL2 (or RL3). Note that the instantiation of nsDCG used is slightly different than the one proposed by Järvelin et al. <ref type="bibr" coords="7,250.87,421.03,9.96,8.74" target="#b7">[8]</ref>. A detailed discussion on the choice of the discounting and the implications of certain choice is given in section 7.3 nsDCG dupes@10 is similar to the nsDCG@10 except that duplicate documents in RL2@[1..10] and RL3@[1..10] with respect to RL1@[1..10] (i.e. documents that appear in the top 10 ranks of RL2 and RL3 that have previously appeared in the top 10 ranks of RL1) are considered non-relevant. Note that in the case of drifting reformulations there are no duplicate documents since the reformulated query represents a different information need that the one captured by the initial query and thus the two metrics, nsDCG and nsDCG dupes are exactly the same.</p><p>Further note that in the case of specification and generalization the ideal sDCG differs between the two metrics. The ideal sDCG@10 for snDCG@10 is computed by concatenating the top 10 components of the idealDCG vector twice with each repeated result discounted according to the sDCG formula. Essentially, the ideal DCG for each rank is the DCG corresponding to the optimal ranking of documents. Since, duplicate documents are considered relevant by nsDCG, the optimal ranking is the same both for the initial query and its reformulation. On the other hand the idealsDCG for snDCG dupes@10 is computed by concatenating the idealDCG[1..10] with the idealDCG <ref type="bibr" coords="7,223.54,596.98,18.52,8.74">[11.</ref>.20] and discounting appropriately. Since duplicate documents are considered non-relevant in this case the optimal ranking is the one in which the best 10 documents appear in ranks 1 to 10 in RL1 and the second best 10 documents appear in ranks 1 to 10 in RL2/RL3.</p><p>The reason for computing nsDCG dupes was the fact that one way of utilizing the initial query to produce a better ranking for the query reformulation would be to remove from RL3 documents that appeared in RL1. The snDCG metric does not award novelty in RL3. On the contrary, since retrieval systems that remove duplicates need to find more relevant documents to populate RL3[1..10], nsDCG actually penalizes novelty.</p><p>The instantiation of the nDCG@10 used was, 10 r=1 2 rel(r,RL1/RL2/RL3) -1 (log 2 (r + 1))</p><p>for the three ranked lists RL1, RL2 and RL3 in isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Results</head><p>In the sections below we present the results of the evaluation for the two goals of the track. We beginning with the overall session evaluation, i.e. the goal (G2), which measures how well retrieval systems performed over the entire session. Then we present the evaluation of the (G1) goal of the track, i.e. the ability of retrieval systems to utilize past user queries to improve the results over the current query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Session Evaluation: G2</head><p>Figure <ref type="figure" coords="8,104.07,290.57,4.98,8.74">1</ref> shows the ranking of systems by nsDCG.RL13 and nsDCG dupes.RL13, i.e. the nsDCG for the session RL1→RL3, grouped by participating sites. Table <ref type="table" coords="8,330.68,302.53,4.98,8.74" target="#tab_1">2</ref> reports the snDCG@10 scores for the session RL1→RL3 over all sessions and the specification, generalization and drifting sessions in isolation. Runs are sorted by snDCG@10.RL13 for all sessions. Some of the runs whose ranking over particular reformulation types appears to significantly diverge from the over all sessions ranking are marked by boldface. As it can be observed, overall, systems appear to perform better over the generalization and drifting sessions than the specification ones. The Kendall's τ correlations between the rankings for all the sessions, the specification, the generalization and the drifting sessions appears in the table below. It is clear that there is a strong "System -Reformulation Type" interaction effect.</p><p>Kendall's τ Specification Generalization Drifting All sessions 0.8461538 0.8176638 0.8176638 Specification 0.7321937 0.6638177 Generalization 0.6695157 q q q q q 0.00 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>nsDCG.RL13 all sessions</head><p>Figure <ref type="figure" coords="9,103.99,612.27,3.87,8.74">1</ref>: Evaluation scores based on nsDCG@10 for all submitted runs for the ranked lists RL1 → RL3. 95% confidence intervals of the mean nsDCG@10 scores are also depicted in the plot. The snDCG@10 scores for the session RL1→RL3 over all sessions and the specification, generalization and drifting sessions in isolation. Runs are sorted by snDCG@10.RL13 for all sessions. Some of the runs whose ranking over particular reformulation types appears to significantly diverge from the over all sessions ranking are marked by boldface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Session Evaluation: G1</head><p>In this section we present the results towards the goal (G1) of the session track. That is, whether retrieval systems can utilize the history of user requests to increase their performance regarding the current request. The history of user requests consists only of a single query, the initial query, while the current request is the query reformulation. For each topic participants have submitted a ranked list over the query reformulation ignoring the initial query (RL2) and a ranked list over the the query reformulation when the initial query is taken into account. Comparing the performance of the retrieval systems between RL2 and RL3 can show us whether retrieval systems could perform better when they utilized the initial query. Table <ref type="table" coords="11,488.32,169.57,4.98,8.74" target="#tab_2">3</ref> shows the snDCG@10 and snDCG dupes@10 scores for RL1→RL2 and RL1→RL3 sessions along with the nDCG@10 scores for RL1, RL2 and RL3. The ↑ symbol denotes an increase in the performance of the retrieval system when utilizing the initial query compared with the performance of the system when ignoring the initial query, the ⇑ symbol denotes a statistically significant increase (when t-test applied), the ↓ symbol denotes a drop in the performance while the ⇓ symbol denotes a significant drop. Systems are ordered by the nsDCG@10.RL12.</p><p>The same results can be viewed in Figures <ref type="figure" coords="11,263.26,241.30,7.75,8.74" target="#fig_6">6,</ref><ref type="figure" coords="11,274.74,241.30,3.87,8.74" target="#fig_10">8</ref>, and 9. The ordering of the systems in the three figures is by nsDCG@10.RL12, nsDCG dupes@10.RL12 and nDCG@10.RL2, respectively.</p><p>Focusing on the nsDCG@10 RL12→RL13 column 11 out of 27 runs improved their performance when the initial query was considered compared to when it was ignored. Out of these 11 systems, only the RMITBase system improved its performance significantly (with significance measures by a paired t-test). The essex1 and essex3 submissions also demonstrated an important increase in their performance without however being statistically significant. On the other hand, 8 out of the 16 drops in performance were statistically significant.</p><p>Given that some participants chose to remove duplicate documents from RL3 with respect to RL1, we've also computed nsDCG dupes@10, which considers documents that have previously appeared in RL1[1.</p><p>.10] as non-relevant. The change in the performance of the systems wrt. to this metric can be viewed in the second column of Table <ref type="table" coords="11,144.95,369.43,3.87,8.74" target="#tab_2">3</ref>. An interesting thing to notice is that when comparing the absolute scores of nsDCG@10 and nsDCG dupes@10 it appears that considering duplicate documents as non-relevant actually increases the performance of the retrieval systems. This is an artifact which comes from the fact that nsDCG@10 and nsDCG dupes@10 use different normalization (sDCG of the ideal ranked list). In principle the ideal sDCG@10 is always larger than the ideal sDCG dupes@10, as we have described in section 5.2. Looking at the sDCG@10 scores instead one can see that there is a number of duplicate documents retrieved over the lists RL2 and RL3. When comparing the nsDCG@10 and nsDCG dupes@10 columns of the table there are no disagreements regarding the change in the system performance when utilizing the initial query and when not, apart from one, denoted with an asterisk ( * ) at the right-most column of the table. There were at least two submitted runs in which RL3 was simply the results of RL2 excluding the results of RL1 (UM10SibmbB and essex2). As it can be viewed in the table penalizing duplicate documents did not alter the change in the performance of these systems between RL2 and RL3. We hypothesis that this is due to the fact that both submissions removed duplicate documents wrt to RL1 <ref type="bibr" coords="11,322.79,512.89,36.35,8.74">[1..2000</ref>] instead of RL[1.</p><p>.10] making it extremely hard to find so many novel relevant documents in the collection. (For more details one should look at the individual reports of the University of Melbourne and the University of Essex.)</p><p>There are more disagreements between nsDCG@10 RL12→RL13 and nDCG@10 RL2→RL3. Opposite to the disagreements between the nsDCG@10 and the nsDCG dupes@10, these disagreements are due to the choice of the discounting for nsDCG used in our evaluations. (A detailed description is given in section 7.3. As already mentioned in section 5.2, the instantiation of nsDCG used here discounts RL2[1.</p><p>.10] and RL3[1..10] as if they were at rank 11 to 20 of RL1 (with an extra discounting due to the reformulation). The nDCG@10 on the other hand discounts RL2 and RL3 as usual, i.e. rank=1..20. Thus, for instance if in RL2 the system has retrieved slightly less relevant documents than in RL3 but slightly higher in the ranked list then the nDCG may penalize RL3 more than snDCG does and so when looking at nDCG one may see a drop from RL2→RL3, while you see an increase from RL2→RL3 when looking at nsDCG. To demonstrate this phenomenon assume the following two hypothetical RL2 and RL3 rankings. The snDCG@10 and snDCG dupes@10 scores for RL1→RL2 and RL1→RL3 sessions along with the nDCG@10 scores for RL1, RL2 and RL3. The ↑ symbol denotes an increase in the performance of the retrieval system when utilizing the initial query compared with the performance of the system when ignoring the initial query, the U parrowsymbol denotes a statistically significant increase (when t-test applied), the ↓ symbol denotes a drop in the performacen while the ⇓ symbol denotes a significant drop. The † symbol at the left column denotes a disagreement between the nsDCG@10.RL12-to-nsDCG@10.RL13 and the nDCG@10.RL2-to-nDCG@10.RL3 change of performance, a ‡ denotes a disagreement in the significance of the change between the same two metrics while an * symbol denotes a disagreement between the nsDCG@10.RL12-to-nsDCG@10.RL13 and the nsDCG dupes@10.RL12-to-nsDCG dupes@10.RL13.</p><formula xml:id="formula_4" coords="13,238.01,74.28,135.99,20.25">RL2 : R R N N N N N N N N RL3 : N N R R R N N N N N</formula><p>The DCG@10.RL2 is 1.4048 while the DCG@10.RL3 is 1.1349, pointing out that RL2 is a better ranking than RL3. The sDCG@10.RL2 (a component of the snDCG@10.RL12) is 0.473, while the sDCG@10.RL3 is 0.662, pointing out that RL3 is actually a better ranking than RL2.</p><p>A dagger ( †) at the right-most column of the table denotes a disagreement between the nsDCG@10.RL12-to-nsDCG@10.RL13 and the nDCG@10.RL2-to-nDCG@10.RL3 change of performance, while a double dagger ( ‡) denotes a disagreement in the significance of the change between the same two metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">System performance and type of reformulation</head><p>In this section we do some further analysis of the performance of the retrieval systems with respect to the different types of reformulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average number of relevant documents</head><p>Table <ref type="table" coords="13,99.19,328.14,4.98,8.74">4</ref> shows the number of relevant and highly relevant retrieved on average in RL1[1..10], RL2[1.</p><p>.10] and RL3[1.</p><p>.10], along with the number of relevant and highly relevant novel documents retrieved on average in RL2[1.</p><p>.10] and RL3[1.</p><p>.10] over all sessions, specification sessions, generalization sessions and drifting sessions.</p><p>Focusing on the RL1 column one can observe the difference on the average number of relevant documents retrieved by each participating difference between the specification and the generalization/drifting reformulations. Submitted runs retrieved much fewer relevant documents for the initial query of the specialization sessions than the generalization or the drifting ones.</p><p>Focusing at each row of the table across RL1, RL2 and RL3 one can observe a clear increase in the number of relevant and highly relevant documents when moving from RL1 to RL2/RL3 (except for the case of drifting sessions). The larger increase appears in the case of specification sessions which implies that the initial query in the case of specification sessions was quite hard to answer, while the reformulated one was handled by retrieval systems better. This can be considered as a proof of validity for the constructed specification topics.</p><p>On the other hand the initial query of the generalization sessions seem to be handled already good enough by the retrieval systems, which might indicate that the initial query in these topics was well-specified instead of over-specified or mis-specified as indented.</p><p>Finally comparing the RL2/RL3 columns with the RL2/RL3 Novel ones one can observe that even though participating runs retrieved a number of duplicate documents, especially in the case RL3, nevertheless most of the retrieved documents in RL2 and RL3 seem to be "novel" (or at least not exact duplicates). along with the number of relevant and highly relevant novel documents retrieved on average in RL2[1..10] and RL3[1..10] over all sessions, specification sessions, genralization sessions and drifting sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RL1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Session evaluation per reformulation type</head><p>Table <ref type="table" coords="14,100.19,97.28,4.98,8.74">5</ref> reports the snDCG@10 scores for RL1→RL2 and RL1→RL3 sessions. The ↑ symbol denotes an increase in the performance of the retrieval system when utilizing the initial query compared with the performance of the system when ignoring the initial query, the ⇑symbol denotes a statistically significant increase (when t-test applied), the ↓ symbol denotes a drop in the performance while the ⇓ symbol denotes a significant drop. Table <ref type="table" coords="14,98.80,504.73,3.87,8.74">5</ref>: The snDCG@10 scores for RL1→RL2 and RL1→RL3 sessions. The ↑ symbol denotes an increase in the performance of the retrieval system when utilizing the initial query compared with the performance of the system when ignoring the initial query, the U parrowsymbol denotes a statistically significant increase (when t-test applied), the ↓ symbol denotes a drop in the performacen while the ⇓ symbol denotes a significant drop. The † symbol denotes runs that improved over the specification and generalization sessions but didn't over the drifting, the + symbol denores runs that improved over the generalization and drifting but not over the specification, the • symbol denotes runs that improved over generalizations only, the * symbol denotes runs that improved over the specification only, while the √ symbol denotes runs that improved over all types.</p><p>One can observe a number of patterns across all the runs by examining the improvements and drops of the performance over the three reformulation types. Some of those patterns have been noted by the symbols at the right-most column of the table. The dagger ( †) denotes runs that improved over the specification and generalization sessions but didn't over the drifting, the plus (+) denotes runs that improved over the generalization and drifting but not over the specification, the bullet (•) denotes runs that improved over generalizations only, the asterisk ( * ) denotes runs that improved over the specification only, while the tick ( √ ) denotes runs that improved over all types. These patterns alone indicate a strong "system-reformulation type" effect, with some systems well utilizing the initial query to improve their performance on the reformulated one for certain reformulation types but on the same time dropping their performance on others.</p><p>Figures 6-9 at the end of this document also illustrate the change of the system performance as measures by nsDCG@10, nsDCG dupes@10 and nDCG@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Analysis over queries</head><p>Figure <ref type="figure" coords="15,104.51,182.12,4.98,8.74" target="#fig_2">2</ref> illustrate the hardness of the initial query as measured by the average nDCG@10.RL1 over all queries. Figure <ref type="figure" coords="15,140.78,194.08,4.98,8.74" target="#fig_3">3</ref> illustrates the query hardness both of the initial query (left-hand side plot) and the query reformulation (right-hand side plot) as measured by the average nDCG@10.RL1 and nDCG@10.RL2 over the three reformulation types. As it can be observed by the two figures the initial queries were in general hard queries. The initial queries of the specialization sessions appear to be the hardest ones, and thus they offer a larger margin of improvement of performance over the reformulated query. On the contrary the initial query of generalization sessions seems to be well specified, even though an improvement over the reformulated query can be observed on average.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">A detailed discussion on nsDCG discounting and logarithm bases</head><p>In the framework used to define typical nDCG metric, relevance scores are mapped to relevance grades, e.g. a score of 3 is given to highly relevant documents, a score of 2 to fairly relevant documents and so on. Relevance scores are viewed as the gain returned to a user when examining the document. Thus, the relative value of relevance scores dictates how more valuable for instance a highly relevant document is to the user than a marginally relevant. Even though, relevance scores were used directly as gains when nDCG was originally defined, alternative gain functions that map gain values to relevance scores have been used.</p><p>To account for late arrival of relevant documents, gains are then discounted by a function of the rank. The discount function is viewed as an indication of the patience of a user to step down the ranked list of documents. In other words, it expresses the probability of a user seeing a document at a certain rank. The discounted gains are then summed progressively from rank 1 to k and this discounted cumulative gain is normalized to the range of 0 to 1, resulting in the normalized discounted cumulative gain (nDCG).</p><p>The original instantiation of nDCG, as it appeared in Järvelin and Kekäläinen <ref type="bibr" coords="16,418.19,448.75,9.96,8.74" target="#b6">[7]</ref>, can be computed as,</p><formula xml:id="formula_5" coords="16,141.23,470.08,329.54,30.55">nDCG@k = DCG@k optDCG@k where DCG@k = b i=1 rel(1) + k i=b+1 rel(i)/ log b (i)</formula><p>where rel(i) is the relevance score of the document at rank i, k is an arbitrary cut-off rank and optDCG denotes the DCG value for an ideal ranked list. The logarithm base b controls the severity of the discount function; the smaller the base the less patient the user modeled is. The base 2 logarithm is the one that has mostly appeared in the literature.</p><p>Note that the afore-described instantiation of nDCG does not discount the first b + 1 documents in the ranked list. To cope with this a number of different discount functions have been used. Similar to discount functions a number of different gain functions have also appeared in the literature. For instance, in Burges et al. <ref type="bibr" coords="16,97.46,600.71,10.52,8.74" target="#b1">[2]</ref> DCG was computed as,</p><formula xml:id="formula_6" coords="16,250.07,625.83,110.66,30.32">DCG@k = k i=1 2 rel(i)-1 log 2 (i + 1)</formula><p>while in Järvelin et al. <ref type="bibr" coords="16,172.90,667.73,10.52,8.74" target="#b7">[8]</ref> DCG was computed as, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>nsDCG.RL12 to nsDCG.RL13 all sessions</head><p>Figure <ref type="figure" coords="17,105.10,508.32,3.87,8.74">4</ref>: Average nsDCG@10.RL12→nsDCG@10.RL13 for the queries where the average difference in performance across all systems is statistically significant.</p><formula xml:id="formula_7" coords="17,246.20,560.97,118.41,30.32">DCG@k = k i=1 rel(i) (1 + log 2 (i))</formula><p>The session-based nDCG has been proposed to incorporate multiple queries in an interactive retrieval scenario, in which a user moves down a ranked list of documents and at some cut-off rank, she/he reformulates the query. In that case DCG is computed at the reformulation cut-off for each query and the stopping cut-off for the last reformulation. In the case of Sessions Track, the reformulation cut-off is not known since it is not an interactive track. Given that a fixed reformulation cut-off has been selected at rank 10. That is, a user is assumed to reformulate his query after observing the document at rank 10.</p><p>A question that needs to be answered is how should one select the logarithm bases. As mentioned earlier, in the case of nDCG the logarithm base captures different search scenarios (e.g. precision vs. recall-orient retrieval, etc.) and users (patient vs. impatient users). In the case of nsDCG the logarithm bases play a similar role, nevertheless there is a constraint of how to set the bases. Under a realistic scenario, the documents in the ranked list for query q should not receive discounting larger the documents in the ranked list for query q + m, for m ≥ 1.</p><p>Given that the reformulation cut-off in Sessions Track is set at rank 10, the maximum penalty received by a document in query q is the penalty of the last document before the reformulation, i.e. the document at rank 10. The penalty that this document receives is,</p><formula xml:id="formula_8" coords="18,250.30,484.39,111.39,9.65">(1 + log b 10) * (1 + log bq q)</formula><p>The minimum penalty that a document receives for a query q + m is the penalty that the top document of the query q + 1 receives, thus the penalty of that document is, (1 + log b 1) * (1 + log bq (q + 1))</p><p>Since we only have two queries in the session track, q = 1 and thus q + 1 = 2. By replacing the q's in the above formulas we get that (1 + log b 10) * (1 + log bq 1) should be less than (1 + log b 1) * (1 + log bq 2) Solving the inequality for b and setting bq = 2 results in b &gt; 10. By repeating the above calculations for the discount function in Burges et al. <ref type="bibr" coords="18,224.20,643.16,10.52,8.74" target="#b1">[2]</ref> we find that for bq = 2, b should be at least 6.</p><p>As one can notice the afore-derived logarithm bases are much larger than the typical base of 2 used in most implementations of nDCG. An alternative instantiation of a session-based nDCG could be defined as follows: since we assume that a user will never look beyond rank 10 in any query we can concatenate the top 10 documents of the query q + 1 at the bottom of the top 10 documents of the query q and essentially simply compute nDCG@(10*q). In the case of the session track this is nDCG@20. Naturally, documents after the user's reformulation will be penalized more than the ones before the reformulation and we can select the base of the discounting logarithm without any constraints. Moreover we can further penalize the reformulated ranked list by the position of the query in a series of reformulations as in <ref type="bibr" coords="19,394.32,122.98,9.96,8.74" target="#b7">[8]</ref>.</p><p>Nevertheless, in this instantiation of the session-based nDCG the relative discount between two consecutive documents is different for each query. This can be view in Figure <ref type="figure" coords="19,367.31,151.18,3.87,8.74" target="#fig_5">5</ref>. The discounts of the RL1[1..10] and RL2/RL3 <ref type="bibr" coords="19,106.04,163.13,34.04,8.74">[1..10]</ref> are shown in isolation to make the comparison of the relative drop of the weight for each document in the two ranked lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>This paper describes results and analysis of the first year of the TREC Session Track. The main conclusion is that task G2-improving results for a second query given only the first query-is difficult; few groups were able to show any improvement, and no group had a statistically significant improvement. Further academic study of this problem will almost certainly require some kind of interaction data.</p><p>We observed a strong interaction between system and reformulation type. This may partly be a result of lack of training data: groups could know very little about how their systems would perform on the different types. If we use the same types next year we may see stronger improvements.</p><p>There are many ways the user model, data, and evaluation methodology could be improved, and we intend to tackle these problems in the second year. As a pilot study for the problem, we believe the data shows the task and track were interesting and worthy of further study.</p><p>A Plots of the nsDCG, nsDCG dupes, and nDCG scores for all sessions and the three reformulation types q q q q q q q q q q 0 5 10 q q q q q q q q q q 0 5 nsDCG.RL12 to nsDCG.RL13 specialization sessions q q q q q q q q q q 0 5 10  nsDCG_dupes.RL12 to nsDCG_dupes.RL13 all sessions q q q q q q q q q q 0 5 10 15 20 25 30 35 0.05 0.10 0.15 0.20 submitted runs nsDCG_dupes.RL12 --to--nsDCG_dupes.RL13 q q Groups UALR_Srini GALE RMIT unimelb ULugano budapest_acad EssexUni udel Uams Webis nsDCG_dupes.RL12 to nsDCG_dupes.RL13 specialization sessions q q q q q q q q q q 0 5 10  The ones for the drifting-reformulation sessions are exactly the same as the nsDCG@10. q q q q q q q q q q 0 5 nDCG.RL2 to nDCG.RL3 all sessions q q q q q q q q q q 0 5 10 nDCG.RL2 to nDCG.RL3 generalization sessions q q q q q q q q q q 0 5  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Descriptions of Submitted Runs</head><p>Each of the methods used by each one of the groups that participated in the track is summarized below. For further details on the techniques used refer to the individual groups reports for the Session Track.</p><p>Bauhaus University Weimar: The Webis group from the Bauhaus University Weimar submitted two runs, webis2010 and webis2010w. The participants took a two steps approach. In a preprocessing phase they used query segmentation, comparing possible query segments against the Google n-gram collection. In the second phase queries were submitted to the Carnegie Mellon ClueWeb search engine and run against the Category B ClueWeb collection. The webis2010 run applied a maximum keyword framework in which for each query or query session all query terms are considered and the longest query that has a reasonable number of hits (between 1 and 1000) is selected to better represent the user's information need. In the case that all queries returned more than 1000 results then the complete query containing all terms along a session was used. The webis2010w run used different term weights in the Indri query language to cope with case where query terms were added or deleted during the query reformulation. Query terms that only appeared in the first query were given a weight of 0.5, those that appeared only in the second query were given a weight of 2 and those that appeared in both queries were given a weight of 1. It was noticed that due to short sessions the maximum query algorithm would often times select all query terms as the maximum query.</p><p>Gale, Cengage Learning: The Cengage Learning group submitted three runs, CengageS10R1, CengageS10R2, and CengageS10R3. The participants indexed the Category B subset of the ClueWeb09 collection using Lucene. A number of different techniques were then used for their submissions:</p><p>Query Term Weighting : Query term weights we applied depending upon which query the occurred in. Query terms were divided into three different categories: terms that appear only in the first query, terms that appear only in the second query, and terms that appear in both queries. Through experimentation, it was found that the best weights for these three groups were dependent upon the type of reformulation. This approach necessitated the ability to automatically categorize query pairs. A number of different techniques were used for this purpose.</p><p>Category Re-ranking : Query and documents were first classified over the Open Directory Project (ODP) categories. Every category in the ODP was indexed against its title and the descriptions of all the pages categorized under it. To categorize a query or document, the text of that query or document was submitted to the ODP index and a list of search results was returned with a retrieval score for each result. The top ten results were selected as the best category matches for the query and were given a weight proportional to the retrieval score returned by Lucene.</p><p>Query Expansion : (a) Usage Log Query Expansion : For each query a list of related search terms was produced by mining the usage logs from Cengage Learning products. The term collocation formula was similar in concept to the tf-idf weighting scheme, in that it rewards frequently co-occurring terms, but minimizes the impact of the most common search terms. Expansion terms were sought for all possible sub-queries, with expansion terms for longer phrases receiving a bonus based on that length. (b) Corpus Collocation Query Expansion : Corpus-based collocation expansion was done very similarly to usage logbased collocation expansion. The major difference is in how the expansion terms were collected. Cengage Learning maintains a collocation database. This database was compiled against large portions of Cengage Learning's digital material and could return a list of the fifty most common words and phrases that appear near a given term.(c) WordNet Expansion : The words in the query needed to be resolved as to which sense of the word they referred by determining which sense of the word is most similar to the other words in the query. We created a measure which incorporated WordNet's tag count, a measure of how often each sense was encountered in the tagging of corpora.</p><p>The CengageS10R1 submission used Query Term Weighting and Corpus terms collocation expansion, the CengageS10R2 used Term weighting, Usage-log expansion, Corpus collocation expansion, Pseudo-relevance expansion and the CengageS10R3 used WordNet expansion and Category re-ranking.</p><p>Hungarian Academy of Science: The Hungarian Academy of Science group submitted two runs, bpacad10s1 and bpacad10s2. The entire ClueWeb corpus was used. For the RL1 and RL2 the AND Boolean operator was used to construct a new query out of the original query and its reformulation and documents were ranked by Okapi BM25. For the RL3, documents in the RL2 list were re-ranked based on the reformulation type and whether they occurred in RL1 or not -bpacad10s1. The reformulation type was determined from the surface form of the question. The bpacad10s2 submission was produced by the weighted union of the RL1 and RL2 result lists.</p><p>RMIT University: The RMIT group submitted two runs, RMITBase and RMITExp. The experiments were run on the ClueWeb Category B collection. Indexing and searching was done with the Lemur toolkit (v 4.12). Dirichlet-smoothed language model was used for ranking. In the RMITBase submission for the RL1 and RL2 the first query and its reformulation was used respectively. For the RL3 the union of the query terms from both queries were used. In the RMITExp the queries (first query, reformulation and union of their terms) were first submitted to Google to obtain "related search" suggestions. The union of the query terms of the Google query suggestions and the three aforementioned queries were then run against the ClueWeb dataset.</p><p>The University of Melbourne: The University of Melbourne group submitted three runs, UM10SimpA, UM10SibmA, and UM10SibmB, using the entire ClueWeb09 corpus. Regarding the retrieval methods for RL1 and RL2, the UM10SimpA was a content-only run, and original impact model was employed for similarity computation. For the other two runs, the impact-based version of BM25 was employed. Moreover, the similarity score was a combination of content (50%), incoming anchor (25%) and PageRank (25%) scores. For all three runs, the documents with Waterloo spam scores of 30% or less were discarded from the results. The RL3 result were just the merging of RL1 and RL2, with two merging methods -A and B. In the merging method A, which was applied to UM10SimpA and UM10SibmA, a similarity degree s between the two respective queries was calculated, and the score S3 is S2 -s * S1, where S1, S2, S3 are scores for RL1, RL2 and RL3, respectively. In the merging method B, which applied to UM10SibmB, S3 was simply S2 -S1.</p><p>University of Amsterdam: The University of Amsterdam group submitted three runs, uvaExt1, uvaExt2, and uvaExt3 using the entire ClueWeb09 corpus. The experiments by UAms focused on the use of blind relevance feedback to bias a follow-up query towards or against the topics covered in documents that were returned to the user in response to the original query. Blind relevance feedback takes the most discriminative terms from a set of documents retrieved for a query, and uses these to build a query model that incorporates information about the topic underlying the documents. These experiments followed the intuition that for original queries, when no context for disambiguation is available, diverse result lists should be presented, that have a high chance of answering any aspect of the information need underlying a query. Once more context is available, this is used to bias results towards the relevant aspects of the query using blind relevance feedback. Three methods for biasing results for the follow-up query were explored. First, it was assumed that results returned for the original query were helpful and can be used to focus or disambiguate results for the follow-up query. Thus, feedback terms extracted from the top-ranked documents of the original query were used to expand the follow-up query -uvaExt1. Second, the assumption that results for the original query were not helpful was covered. The set of expansion terms generated from the top-ranked documents returned for the follow-up query was taken as a base set, and the feedback terms that were generated using the original query were removed -uvaExt2. Finally, UAms considered that the underlying topic may best be represented by both queries, and used the feedback terms generated by both queries to expand the follow-up query -uvaExt3. The system was implemented based on the Indri retrieval engine. Retrieval runs for the original queries (RL1) were generated by interpolating language modeling and phrase-based retrieval scores. Based on those runs, diversification was performed using the maximum marginal relevance method (MRR) with clusters obtained by latent Dirichlet allocation (LDA). For the follow-up queries where no additional context is taken into account (RL2), three different methods are explored: (1) language modeling + phrase-based retrieval -uvaExt1, (2) diversification using pseudo-relevance feedback -uvaExt2, and (3) diversification as for the original query (using MRR and LDA) -uvaExt3. From these individual runs, runs that combine information using the original and the follow-up query (RL3) are generated using the pseudo-relevance methods described above.</p><p>University of Arkansas at Little Rock: The University of Arkansas group submitted three runs using the full ClueWeb09 corpus, CARDBNG, CARDWIKI, and CARDWNET. No query processing was performed for RL1 and RL2. Regarding RL3, the CARDBNG submission used the Bing search engine to categorize the reformulation type (generic/specific/drifting etc.) and used query expansion according to categorization above. The CARDWIKI used the Wikipedia search engine to categorize the reformulation type query expansion according to the categorization. The CARDWNET used the Wordnet dictionary for query expansion of the intersection and the union of the original query and its reformulation query terms.</p><p>University of Delaware: The University of Delaware submitted three runs using the same baseline retrieval method on three different subsets of the ClueWeb09 collection: the full Category A set, the Category A set filtered for spam pages using the Waterloo spam scores, and the Category B set. The RL3 submission estimated a probability that a document in RL2 would be viewed by a user twice (once for the first query, once for the second), and scaled the document's score down accordingly.</p><p>University of Essex : The University of Essex submitted three runs, essex1, essex2, and essex3. In all their runs they used the publicly available Carnegie Mellon ClueWeb search engine. For the ranked lists RL1, RL2 queries were submitted as they are to the search engine which in return used the Query Likelihood model to retrieve a ranked list of documents. The Waterloo Spam Rankings for the ClueWeb09 Dataset was used to filter the spam documents from the ranked lists. The essex1 run is the first baseline method to retrieve ranked list RL3, in which a new query consisting of both queries in the session was submitted to the Indri search engine. The essex2 run is the second baseline method to retrieve ranked list RL3. It reflects on the assumption that the user is not satisfied with the first set of results and that is why she reformulated her original query. In this baseline the participants use a naive way to utilize the original query by filtering the retrieved documents for the reformulated query in the session. The filtering works simply by eliminating the documents which appear in the first ranked list. In essex3 a method for extracting useful terms and phrases to expand the reformulated query in the session was used. This method stems from previous work in using query logs to extract related queries and the group's past work in the AutoAdapt project to learn domain models from query logs. Due the lack of availability of query logs an anchor log constructed from the same dataset (the ClueWeb09 category B dataset) was used to simulate the query log. The anchor log was extracted and made publicly available by the University of Twente. Using the anchor log they extract the top common associated queries of both queries in the session using Fonseca's association rules <ref type="bibr" coords="26,499.66,416.83,9.96,8.74" target="#b3">[4]</ref>. Then they expanded the reformulated query with the extracted phrases or terms and the original query giving higher weights to the reformulated query.</p><p>University of Lugano : The University of Lugano group submitted three runs, USIML052010, USIML092010, and USIRR2010. The Category B subset of the ClueWeb was indexed with the Terrier information retrieval system and used for the retrieval. The ranked lists RL1 and RL2 for all the three submitted runs were generated by using the original query and its reformulation, respectively and scoring documents by the BM25 implementation of Terrier. With respect to RL3, two approaches were used. In the first approach the third ranking (RL3) was generated by scoring documents according to the weighted summation of the reciprocal ranks of documents in RL1 and RL2, where the weight given to documents from RL1 was negative and RL2 was positive. Thus the score for a document d was computed as follows: score(d, RL3) = α * (1/rank(d, RL1)) + (1 + α) * (1/rank(d, RL2)). If a document was not present in one of the ranked lists, its reciprocal rank was set to 0. The parameter α was empirically set to 0.2 -USIRR2010. In the second approach the relevance model for the first and second query was build using the top N ranked documents from RL1 and RL2 as the pseudo-relevant documents (denoted PR1 and PR2). The relevance models R1 and R2 were estimated by averaging the relative frequencies for terms across the pseudo-relevant documents. The R1 estimates were smoothed with a background language model based on collection term frequency estimates. The goal of the Lugano group was to highlight words from the term distribution of R2 that are rare in the term distribution of R1 in order to reduce the number of documents from RL1 that are present in RL3. To this end,term probabilities in R2 were weighted by their relative information in R2 and R1 to calculate a new query model R3: p(w|R3) ∝ p(w|R2) * log(p(w|R2)/p(w|R1)). The normalizing constant in this case is simply the Kullback-Leibler divergence between R2 and R1. After obtaining the new term distribution, the top K terms were selected from R3 and submitted as a weighted query to Terrier again using the BM25 retrieval function. In this way two runs were generated by varying the effect of the background smoothing, USIML052010 and USIML092010.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="15,205.75,462.04,143.32,61.92;15,348.69,452.84,10.99,10.77;15,357.40,446.77,7.46,8.23;15,363.05,437.69,6.12,9.89;15,368.88,427.96,3.72,6.18;15,370.32,423.65,3.72,6.18;15,371.76,417.08,3.72,6.18;15,374.63,408.73,3.72,6.18;15,377.41,397.90,6.77,10.12;15,382.37,383.05,10.43,15.44;15,392.42,361.39,2.60,6.18;15,393.20,356.87,3.90,6.18;15,394.73,345.48,4.59,9.83;15,398.16,330.35,2.60,6.18;15,203.76,536.04,3.72,6.18;15,273.69,536.04,7.43,6.18;15,343.63,536.04,11.15,6.18;15,415.42,536.04,11.15,6.18;15,184.07,514.08,6.18,9.29;15,184.07,484.60,6.18,9.29;15,184.07,455.11,6.18,9.29;15,184.07,425.62,6.18,9.29;15,184.07,396.14,6.18,9.29;15,184.07,366.65,6.18,9.29;15,184.07,337.16,6.18,9.29;15,255.75,300.16,116.54,7.22;15,304.58,552.09,18.89,6.18;15,168.02,451.95,6.18,24.40;15,168.02,431.51,6.18,18.57;15,168.02,416.92,6.18,12.74;15,168.02,393.04,6.18,22.02;15,168.02,383.58,6.18,7.60;15,168.02,369.46,6.18,12.26;15,209.78,435.24,3.72,6.18;15,210.33,445.36,2.60,6.18;15,209.68,451.23,3.90,6.18;15,217.65,435.90,41.25,6.18;15,217.65,443.92,43.04,6.18;15,217.65,451.94,21.27,6.18"><head></head><label></label><figDesc>* o * o + + o + + o * + * o + * o + o ** o + + + * + +o + * o ** +o o o * + * o * + +* + o * o o * + + + + o * o + ** ** * + o o * o + + + o + o + * o + o + * + o* **</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="15,134.69,582.43,342.62,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Query hardness measured by the average nDCG@10 over all session.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,72.00,316.44,467.99,8.74;16,72.00,328.40,203.57,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Query hardness measured by the average nDCG@10 over initial query (RL1) and query reformulation (RL2) for the three reformulation types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="17,123.00,413.36,9.43,13.94;17,123.00,364.36,9.43,13.94;17,123.00,315.36,9.43,13.94;17,123.00,266.36,9.43,13.94;17,123.00,217.37,9.43,13.94;17,123.00,168.37,9.43,13.94;17,123.00,119.37,9.43,13.94;17,248.36,470.26,139.37,9.43;17,98.93,290.23,9.43,59.64;17,98.93,265.72,9.43,21.72;17,98.93,203.30,9.43,59.64;17,167.71,134.67,56.86,9.43;17,167.71,146.70,64.67,9.43;17,167.71,158.74,31.76,9.43"><head></head><label></label><figDesc>Topics that change significantly nsDCG.RL12 --to--nsDCG.RL13 Specification Generalization Drifting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="18,72.00,324.50,468.00,8.74;18,72.00,336.46,381.12,8.74"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Different discounts for top 10 documents in the first query and the query reformulation. The discounts are shown for the two queries in isolation, starting from rank 1 in both cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="20,72.00,467.22,468.00,8.74;20,72.00,479.18,468.00,8.74;20,72.00,491.13,468.00,8.74;20,72.00,503.09,107.76,8.74"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Evaluation scores based on nsDCG@10 for all submitted runs for the ranked lists RL1 → RL2 and RL1 → RL3. Arrows indicate the change in the evaluations score between the two ranked lists for each one of the submitted runs. Thick solid arrows indicate statistically significant changes according to paired two-sided Student t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="21,72.00,608.09,468.00,8.74;21,72.00,620.05,348.83,8.74"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Evaluation scores based on nsDCG@10 for all submitted runs for the ranked lists RL1 → RL2 and RL1 → RL3 for specification, generalization and drifting-reformulation sessions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="22,72.00,565.45,468.00,8.74;22,72.00,577.41,468.00,8.74;22,72.00,589.36,468.00,8.74;22,72.00,601.32,468.00,8.74;22,72.00,613.27,468.00,8.74;22,72.00,625.23,211.78,8.74"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Evaluation scores based on nsDCG dupes@10 for all submitted runs for the ranked lists RL1 → RL2 and RL1 → RL3. Duplicate documents wrt. RL1 are considered non-relevant. Arrows indicate the change in the evaluations score between the two ranked lists for each one of the submitted runs. Thick solid arrows indicate statistically significant changes according to paired two-sided Student t-test. Scores are also shown for specification and generalization-reformulation sessions. The ones for the drifting-reformulation sessions are exactly the same as the nsDCG@10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="23,72.00,577.41,468.00,8.74;23,72.00,589.36,468.00,8.74;23,72.00,601.32,468.00,8.74;23,72.00,613.27,389.18,8.74"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Evaluation scores based on nDCG@10 for all submitted runs for the ranked lists RL2 and RL3. Arrows indicate the change in the evaluations score between the two ranked lists for each one of the submitted runs. Thick solid arrows indicate statistically significant changes according to paired two-sided Student t-test. Scores are also shown for specification, generalization and drifting-reformulation sessions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,73.96,468.00,360.02"><head>Table 1 :</head><label>1</label><figDesc>Groups participating in the 2010 Sessions Track.</figDesc><table coords="5,72.00,73.96,468.00,360.02"><row><cell>Bauhaus University Weimar</cell><cell>Gale, Cengage Learning</cell></row><row><cell>Hungarian Academy of Science</cell><cell>RMIT University</cell></row><row><cell>The University of Melbourne</cell><cell>University of Amsterdam</cell></row><row><cell cols="2">University of Arkansas at Little Rock University of Delaware</cell></row><row><cell>University of Essex</cell><cell>University of Lugano</cell></row><row><cell cols="2">&lt;reformulation&gt;cuttlebone casting&lt;/reformulation&gt;</cell></row><row><cell cols="2">&lt;description&gt;Find information about the cuttlebone metal casting</cell></row><row><cell cols="2">technique and how can it be used to create jewelry?&lt;/description&gt;</cell></row><row><cell>&lt;/topic&gt;</cell><cell></cell></row><row><cell cols="2">&lt;topic number="38" reformtype="drifting" source="mqtrack2009"&gt;</cell></row><row><cell>&lt;query&gt;sun spot activity&lt;/query&gt;</cell><cell></cell></row><row><cell cols="2">&lt;description&gt;What is sunspot activity and what causes it?&lt;/description&gt;</cell></row><row><cell cols="2">&lt;reformulation&gt;sun spot earthquake&lt;/reformulation&gt;</cell></row><row><cell cols="2">&lt;rdescription&gt;Can sunspot activity be used to predict earthquakes?</cell></row><row><cell cols="2">Are there any studies?&lt;/rdescription&gt;</cell></row><row><cell>&lt;/topic&gt;</cell><cell></cell></row><row><cell cols="2">&lt;topic number="41" reformtype="generalization" source="mqtrack2009"&gt;</cell></row><row><cell>&lt;query&gt;history crossword&lt;/query&gt;</cell><cell></cell></row><row><cell>&lt;reformulation&gt;history games&lt;/reformulation&gt;</cell><cell></cell></row><row><cell cols="2">&lt;description&gt;Find printable history games to be used in a</cell></row><row><cell>history class.&lt;/description&gt;</cell><cell></cell></row><row><cell>&lt;/topic&gt;</cell><cell></cell></row><row><cell cols="2">A set of 150 (initial query, query reformulation) pairs 1 were provided to participants, 52 of which were</cell></row><row><cell cols="2">specification reformulations, 50 were drifting reformulations and 48 were generalization reformulations.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,72.00,181.23,390.09,365.35"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="10,149.91,181.23,312.18,343.88"><row><cell>Run</cell><cell></cell><cell cols="2">snDCG@10.RL13</cell><cell></cell></row><row><cell></cell><cell cols="4">all sessions Specification Generalization Drifting</cell></row><row><cell>CengageS10R1</cell><cell>0.2377</cell><cell>0.1827</cell><cell>0.2606</cell><cell>0.2723</cell></row><row><cell>CengageS10R2</cell><cell>0.2348</cell><cell>0.1870</cell><cell>0.2528</cell><cell>0.2665</cell></row><row><cell>CengageS10R3</cell><cell>0.2295</cell><cell>0.1631</cell><cell>0.2577</cell><cell>0.2708</cell></row><row><cell>essex3</cell><cell>0.2249</cell><cell>0.1481</cell><cell>0.2531</cell><cell>0.2763</cell></row><row><cell>UM10SibmA</cell><cell>0.2243</cell><cell>0.1445</cell><cell>0.2495</cell><cell>0.2816</cell></row><row><cell>essex1</cell><cell>0.2233</cell><cell>0.1456</cell><cell>0.2538</cell><cell>0.2738</cell></row><row><cell>UM10SibmbB</cell><cell>0.2200</cell><cell>0.1424</cell><cell>0.2434</cell><cell>0.2767</cell></row><row><cell>udelIndriASF</cell><cell>0.2104</cell><cell>0.1605</cell><cell>0.2362</cell><cell>0.2373</cell></row><row><cell>udelIndriB</cell><cell>0.2076</cell><cell>0.1570</cell><cell>0.2190</cell><cell>0.2480</cell></row><row><cell>USIRR2010</cell><cell>0.2044</cell><cell>0.1532</cell><cell>0.2648</cell><cell>0.2015</cell></row><row><cell>essex2</cell><cell>0.1993</cell><cell>0.1395</cell><cell>0.2190</cell><cell>0.2416</cell></row><row><cell>UM10SimpA</cell><cell>0.1894</cell><cell>0.1294</cell><cell>0.2221</cell><cell>0.2202</cell></row><row><cell>USIML052010</cell><cell>0.1856</cell><cell>0.1200</cell><cell>0.2499</cell><cell>0.1937</cell></row><row><cell>USIML092010</cell><cell>0.1787</cell><cell>0.1149</cell><cell>0.2419</cell><cell>0.1860</cell></row><row><cell>webis2010w</cell><cell>0.1724</cell><cell>0.1002</cell><cell>0.1890</cell><cell>0.2299</cell></row><row><cell>webis2010</cell><cell>0.1674</cell><cell>0.1114</cell><cell>0.1763</cell><cell>0.2156</cell></row><row><cell>RMITBase</cell><cell>0.1529</cell><cell>0.0939</cell><cell>0.1701</cell><cell>0.1966</cell></row><row><cell>RMITExp</cell><cell>0.1402</cell><cell>0.1199</cell><cell>0.1493</cell><cell>0.1523</cell></row><row><cell>uvaExt1</cell><cell>0.1321</cell><cell>0.1035</cell><cell>0.1694</cell><cell>0.1273</cell></row><row><cell>uvaExt2</cell><cell>0.1299</cell><cell>0.1108</cell><cell>0.1569</cell><cell>0.1247</cell></row><row><cell>uvaExt3</cell><cell>0.1282</cell><cell>0.1029</cell><cell>0.1666</cell><cell>0.1190</cell></row><row><cell>bpacad10s1</cell><cell>0.0827</cell><cell>0.0761</cell><cell>0.0819</cell><cell>0.0900</cell></row><row><cell>udelIndriA</cell><cell>0.0785</cell><cell>0.0612</cell><cell>0.0797</cell><cell>0.0946</cell></row><row><cell>bpacad10s2</cell><cell>0.0774</cell><cell>0.0653</cell><cell>0.0896</cell><cell>0.0785</cell></row><row><cell>CARDBNG</cell><cell>0.0648</cell><cell>0.0599</cell><cell>0.0553</cell><cell>0.0781</cell></row><row><cell>CARDWIKI</cell><cell>0.0593</cell><cell>0.0388</cell><cell>0.0552</cell><cell>0.0834</cell></row><row><cell>CARDWNET</cell><cell>0.0456</cell><cell>0.0208</cell><cell>0.0426</cell><cell>0.0731</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,72.00,151.34,446.22,365.35"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table coords="12,93.78,151.34,424.44,343.88"><row><cell>Submitted</cell><cell>nsDCG@10</cell><cell>nsDCG dupes@10</cell><cell></cell><cell cols="2">nDCG@10</cell><cell></cell></row><row><cell>Run</cell><cell>RL12 → RL13</cell><cell>RL12 → RL13</cell><cell>RL1</cell><cell>RL2</cell><cell>→</cell><cell>RL3</cell></row><row><cell>UM10SibmA</cell><cell cols="6">0.2506 ⇓ 0.2243 0.2605 ⇓ 0.2326 0.2391 0.2631 ⇓ 0.1811</cell></row><row><cell>UM10SibmbB</cell><cell cols="6">0.2506 ⇓ 0.2200 0.2605 ⇓ 0.2283 0.2391 0.2631 ⇓ 0.1698</cell></row><row><cell cols="7">CengageS10R1 0.2355 ↑ 0.2377 0.2451 ↑ 0.2478 0.2177 0.2616 ↓ 0.2604  †</cell></row><row><cell cols="7">CengageS10R2 0.2329 ↑ 0.2348 0.2424 ↑ 0.2448 0.2147 0.2599 ↓ 0.2572  †</cell></row><row><cell cols="7">CengageS10R3 0.2291 ↑ 0.2295 0.2385 ↑ 0.2390 0.2096 0.2576 ↑ 0.2579</cell></row><row><cell>essex3</cell><cell cols="6">0.2154 ↑ 0.2249 0.2230 ↑ 0.2327 0.2077 0.2215 ↑ 0.2461</cell></row><row><cell>essex1</cell><cell cols="6">0.2154 ↑ 0.2234 0.2230 ↑ 0.2309 0.2077 0.2215 ↑ 0.2353</cell></row><row><cell>essex2</cell><cell cols="6">0.2154 ⇓ 0.1993 0.2230 ⇓ 0.2060 0.2077 0.2215 ⇓ 0.1700</cell></row><row><cell>UM10SimpA</cell><cell cols="6">0.2143 ⇓ 0.1894 0.2237 ⇓ 0.1980 0.2019 0.2341 ⇓ 0.1625</cell></row><row><cell>udelIndriASF</cell><cell cols="6">0.2098 ↑ 0.2104 0.2200 ↑ 0.2201 0.1956 0.2288 ↓ 0.2282  †</cell></row><row><cell>udelIndriB</cell><cell cols="6">0.2065 ↑ 0.2076 0.2160 ↑ 0.2170 0.1898 0.2308 ↑ 0.2358</cell></row><row><cell>USIRR2010</cell><cell cols="6">0.2043 ↑ 0.2044 0.2150 ↓ 0.2149 0.1895 0.2145 ↑ 0.2147  *</cell></row><row><cell cols="7">USIML052010 0.2043 ⇓ 0.1856 0.2150 ⇓ 0.1948 0.1895 0.2145 ⇓ 0.1649</cell></row><row><cell cols="7">USIML092010 0.2043 ⇓ 0.1787 0.2150 ⇓ 0.1875 0.1895 0.2145 ⇓ 0.1455</cell></row><row><cell>webis2010w</cell><cell cols="6">0.1796 ↓ 0.1724 0.1872 ↓ 0.1797 0.1638 0.2014 ↓ 0.1776</cell></row><row><cell>webis2010</cell><cell cols="6">0.1796 ⇓ 0.1674 0.1872 ⇓ 0.1747 0.1638 0.2014 ⇓ 0.1621</cell></row><row><cell>RMITExp</cell><cell cols="6">0.1435 ↓ 0.1402 0.1493 ↓ 0.1450 0.1289 0.1706 ↓ 0.1525</cell></row><row><cell>RMITBase</cell><cell cols="6">0.1375 ⇑ 0.1529 0.1427 ⇑ 0.1584 0.1246 0.1534 ↑ 0.1834  ‡</cell></row><row><cell>uvaExt1</cell><cell cols="6">0.1358 ↓ 0.1321 0.1437 ↓ 0.1399 0.1071 0.1890 ↓ 0.1773</cell></row><row><cell>uvaExt3</cell><cell cols="6">0.1262 ↑ 0.1282 0.1335 ↑ 0.1358 0.1071 0.1543 ↑ 0.1642</cell></row><row><cell>uvaExt2</cell><cell cols="6">0.1260 ↑ 0.1299 0.1334 ↑ 0.1374 0.1071 0.1623 ↑ 0.1714</cell></row><row><cell>bpacad10s1</cell><cell cols="6">0.0830 ↓ 0.0827 0.0874 ↓ 0.0870 0.0632 0.1122 ↓ 0.1077</cell></row><row><cell>bpacad10s2</cell><cell cols="6">0.0830 ↓ 0.0774 0.0874 ↓ 0.0812 0.0632 0.1122 ↓ 0.0997</cell></row><row><cell>udelIndriA</cell><cell cols="6">0.0761 ↑ 0.0785 0.0789 ↑ 0.0813 0.0626 0.1034 ↑ 0.1133</cell></row><row><cell>CARDBNG</cell><cell cols="6">0.0664 ↓ 0.0648 0.0690 ↓ 0.0679 0.0554 0.0900 ↓ 0.0825</cell></row><row><cell>CARDWIKI</cell><cell cols="6">0.0664 ↓ 0.0593 0.0690 ↓ 0.0619 0.0554 0.0900 ↓ 0.0632</cell></row><row><cell>CARDWNET</cell><cell cols="6">0.0664 ⇓ 0.0456 0.0690 ⇓ 0.0470 0.0554 0.0900 ⇓ 0.0263</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,87.24,684.67,255.50,6.99"><p>Pairs, topics and sessions are used interchangeably in this document.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="27,92.48,150.15,447.52,8.74;27,92.48,162.11,170.12,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="27,198.22,150.15,341.78,8.74;27,92.48,162.11,26.09,8.74">Query reformulation on the internet: Empirical data and the hyperindex search engine</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bruza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dennis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,139.79,162.11,22.93,8.74">RIAO</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="488" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,182.04,447.52,8.74;27,92.48,193.99,447.52,8.74;27,92.48,205.95,115.95,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="27,298.24,182.04,219.17,8.74">Learning to rank with nonsmooth cost functions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,486.52,193.99,21.10,8.74">NIPS</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,225.87,447.52,8.74;27,92.48,237.83,447.52,8.74;27,92.48,249.78,52.31,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="27,213.96,225.87,168.12,8.74">Query reformulation using anchor text</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,404.21,225.87,135.79,8.74;27,92.48,237.83,290.83,8.74">WSDM &apos;10: Proceedings of the third ACM international conference on Web search and data mining</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,269.71,447.52,8.74;27,92.48,281.66,444.52,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="27,362.75,269.71,177.25,8.74;27,92.48,281.66,97.05,8.74">Using association rules to discover search engines related queries</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">M</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">B</forename><surname>Golgher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">S</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ziviani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,210.53,281.66,237.67,8.74">Proceedings of the First Latin American Web Congress</title>
		<meeting>the First Latin American Web Congress</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,301.59,447.52,8.74;27,92.48,313.54,447.52,8.74;27,92.48,325.50,207.36,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="27,236.06,301.59,303.94,8.74;27,92.48,313.54,15.54,8.74">Analyzing and evaluating query reformulation strategies in web search logs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">N</forename><surname>Efthimiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,126.94,313.54,407.83,8.74">CIKM &apos;09: Proceeding of the 18th ACM conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,345.42,447.52,8.74;27,92.48,357.38,370.37,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="27,270.68,345.42,228.44,8.74">Patterns of query reformulation during web searching</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Spink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="27,506.89,345.42,33.11,8.74;27,92.48,357.38,263.42,8.74">Journal of American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1358" to="1371" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,377.30,447.52,8.74;27,92.48,389.26,447.52,8.74;27,92.48,401.21,342.47,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="27,224.67,377.30,267.91,8.74">IR evaluation methods for retrieving highly relevant documents</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,511.74,377.30,28.26,8.74;27,92.48,389.26,447.52,8.74;27,92.48,401.21,101.66,8.74">SIGIR &apos;00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,421.14,447.52,8.74;27,92.48,433.09,295.97,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="27,391.15,421.14,148.85,8.74;27,92.48,433.09,168.11,8.74">Discounted cumulated gain based evaluation of multiple-query ir sessions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M L</forename><surname>Delcambre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,281.48,433.09,22.21,8.74">ECIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="4" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,453.02,447.52,8.74;27,92.48,464.97,273.57,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="27,303.72,453.02,134.55,8.74">Generating query substitutions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,460.63,453.02,79.38,8.74;27,92.48,464.97,189.75,8.74">15th International World Wide Web Conference (WWW-2006)</title>
		<meeting><address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,484.90,447.52,8.74;27,92.48,496.85,447.52,8.74;27,92.48,508.81,189.13,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="27,197.93,484.90,284.60,8.74">Patterns of search: analyzing and modeling web query refinement</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,503.80,484.90,36.20,8.74;27,92.48,496.85,306.68,8.74">UM &apos;99: Proceedings of the seventh international conference on User modeling</title>
		<meeting><address><addrLine>Secaucus, NJ, USA; New York, Inc</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,528.74,447.52,8.74;27,92.48,540.69,447.52,8.74;27,92.48,552.65,126.77,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="27,292.43,528.74,229.78,8.74">Inferring query intent from reformulations and clicks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,92.48,540.69,343.49,8.74">WWW &apos;10: Proceedings of the 19th international conference on World wide web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1171" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,572.57,447.52,8.74;27,92.48,584.53,447.52,8.74;27,92.48,596.48,217.32,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="27,192.92,572.57,347.08,8.74;27,92.48,584.53,15.94,8.74">Mining term association patterns from search logs for effective query reformulation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,127.34,584.53,407.43,8.74">CIKM &apos;08: Proceeding of the 17th ACM conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,92.48,616.41,447.52,8.74;27,92.48,628.36,144.95,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="27,339.96,616.41,195.12,8.74">Vox populi: The public searching of the web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wolfram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Spink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Saracevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="27,92.48,628.36,32.21,8.74">JASIST</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1073" to="1074" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
