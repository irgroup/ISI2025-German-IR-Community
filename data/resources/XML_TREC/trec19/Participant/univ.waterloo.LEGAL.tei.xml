<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.12,102.01,389.87,15.46">University of Waterloo at TREC 2010: Legal Interactive</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Waterloo</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,79.91,134.36,90.24,10.91"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences</orgName>
								<address>
									<country>University of Waterloo</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,182.03,134.36,105.91,10.91"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.81,134.36,103.82,10.91"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,438.27,134.36,89.13,10.91"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences</orgName>
								<address>
									<country>University of Waterloo</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.12,102.01,389.87,15.46">University of Waterloo at TREC 2010: Legal Interactive</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A51F5143ADE34A0F298BB1B8A79D6E8F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year the University of Waterloo (UW) participated in the TREC Legal Interactive track and used the same process as last year except that this year we used three different human operators as opposed to only one as UW did last year. We participated in three topics: 301, 302, and 303. Relative to other participants, we performed well on one of the three topics. For two of the topics, low recall significantly hurt our F1 scores. Overall, we believe a contributing factor in our lower performance this year was with our interaction with the topic authorities, which resulted in our failing to understand the wide range of what constituted relevant material.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year we had two goals for our participation in the legal interactive track. Our first goal was to test the system and process used by the University of Waterloo (UW) in 2009 <ref type="bibr" coords="1,164.47,485.15,10.52,8.94" target="#b0">[1]</ref> with human operators other than one of the system's creators. The second goal was to bring up to speed other researchers at UW on the legal interactive track process. We attempted to achieve these goals by having three different researchers tackle one of three different topics. The three researchers are the co-authors of this paper minus Gordon Cormack, who is one of the creators of the UW system.</p><p>In 2009, UW participated in four topics and in all four of these topics, UW achieved the highest F1 <ref type="bibr" coords="1,287.77,605.61,9.96,8.94" target="#b1">[2]</ref>. The process used to achieve these results involves a human operator conducting an initial search for relevant documents using a standard search interface that produces 10 results with variable length summaries in response to a query. The operator can judge the summary as relevant, not relevant, or flag it as seen with no judgment. From a summary, the operator can click to view the whole document and parent email and attachments.</p><p>After searching for and judging documents in this fashion, the process then switches to an active learning process. A classifier is trained on the judged documents. Random documents from the collection are used as additional non-relevant documents if the training set is unbalanced. The classifier ranks the documents from most likely to least likely to be relevant. Using this ranking, the operator judges unjudged documents. After another period of judging, the classifier can be trained on the new, larger set of judgments and this process is repeated. At the conclusion of judging, the distribution of relevant documents is estimated to determine a cutoff that aims to optimize F1. The operator's job is to train the classifier to be a model of what the topic authority (TA) would consider relevant and not relevant, and throughout the whole process, the operator engages the TA as appropriate to help maintain a focus on what is and is not relevant.</p><p>While the final ranking is automatically produced, the operator's choices from the initial search to the end of the active learning stage may have a significant impact on performance.</p><p>To perform a crude analysis of the sensitivity of the process to the operator in charge, this year we replaced UW's 2009 sole operator, Gordon Cormack, with three other IR researchers (the three other coauthors of this paper). Of note, the three IR researchers had no previous experience with the legal interactive track. While we did this experiment primarily to see how robust the UW process is to different human operators, we also did this experiment out of necessity -Gordon Cormack was one of the track's organizers this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The methods employed were the same as UW used in 2009 <ref type="bibr" coords="1,333.85,701.25,9.96,8.94" target="#b0">[1]</ref>. We submitted results for 3 topics: documents and trained the classifier for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Results and Discussion</head><p>Table <ref type="table" coords="2,104.10,388.13,4.98,8.94" target="#tab_0">1</ref> shows the results for our submission, watlint10, compared to the other participants for topics 301, 302, and 303. Compared to other participants, we did best on topic 302 and effectively tied for best performance on this topic (F1 of 0.275 vs. 0.277).</p><p>Across the three topics, we tended to have good precision but low recall. In particular, we believe that our interaction with the topic authorities on topics 301 and 303 contributed to our low recall on these topics.</p><p>For topic 301, the operator exchanged a number of email messages with the TA, including discussions about specific documents. Nonetheless, the operator's understanding of what was relevant turned out to be much narrower than the TA's understanding, an error which we believe substantially contributed to the low recall values for that operator. Our recall on topic 302 was low, but the recall was above average compared to other participants. For topic 303, there was a general lack of interaction with the topic authority, which we believe contributed to our low recall on this topic.</p><p>Overall, our results in 2009 were dramatically better than this year. As described in the introduction, in 2009 we had a single operator while this year we used three new operators. Without a measure of how last year's operator would have performed this year, we can't conclusively isolate the cause of this year's lower performance.</p><p>Perhaps the most significant difference between 2009 and 2010 is that this year considerably fewer documents were judged for relevance. In 2009, 50,000 judgments were completed for 4 topics, which is an average of 12500 documents judged per topic. This year, 8,877 judgments were completed for 3 topics, which is an average of 2959 documents per topic. Table <ref type="table" coords="2,328.68,443.97,4.98,8.94" target="#tab_1">2</ref> shows detailed counts of judgments for each topic.</p><p>This difference in the amount of judging may be caused by both the topics this year compared to last year as well as the difference between human operators. For example, between the three operators this year, they individually judged 3513, 1484, and 3880 documents.</p><p>Unfortunately, our experiment was not setup to cleanly compare the effect of varying the human operator. Each operator worked on a different topic. Nevertheless, we did succeed at our second goal, which was for the co-authors to learn more about the legal interactive track. Going forward we now know that the operator of our system has to actively engage the topic authority to clearly understand the scope of a topic's relevance, which can be much broader than in non-legal domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>This year the University of Waterloo (UW) used the same process for the legal interactive track as UW used in 2009 except that instead of one human operator, UW used three different operators. While we found differences between the behavior of the operators in terms of the number of documents judged, the number of documents judged is not a good predictor of final performance. We performed well compared to other participants on one of three topics. On the other two topics, we believe that our interaction with the topic authorities contributed to our low recall on each of these topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,310.98,701.25,229.06,20.89"><head>Table 1 :</head><label>1</label><figDesc>Estimated recall, precision, and F1 for our run, watlint10. Also shown is the rank of the run among submitted runs with rank 1 being the best scoring run. The mean recall, precision, and F1 is shown for all of the participant runs submitted for each of the three topics.</figDesc><table coords="1,501.58,701.25,38.44,8.94"><row><cell>301, 302,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,72.00,282.63,468.04,20.89"><head>Table 2 :</head><label>2</label><figDesc>Number of relevance judgments made per topic using the manual search interface and the active learning interface.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="4">Acknowledgments</head><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>, and in part by the <rs type="funder">University of Waterloo</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,87.51,364.03,213.56,8.94;3,87.50,375.98,213.55,8.94;3,87.50,387.94,213.53,9.34;3,87.50,399.90,26.57,8.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,227.06,364.03,74.00,8.94;3,87.50,375.98,213.55,8.94;3,87.50,387.94,136.39,8.94">Machine learning for information retrieval: TREC 2009 web, relevance feedback and legal tracks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mojdeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,245.82,387.94,50.59,9.34">TREC 2009</title>
		<imprint>
			<publisher>NIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,87.51,419.82,213.57,8.94;3,87.50,431.78,213.54,8.94;3,87.50,443.73,84.87,9.34" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,115.94,431.78,168.29,8.94">Overview of the TREC 2009 legal track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,87.50,443.73,50.36,9.34">TREC 2009</title>
		<imprint>
			<publisher>NIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
