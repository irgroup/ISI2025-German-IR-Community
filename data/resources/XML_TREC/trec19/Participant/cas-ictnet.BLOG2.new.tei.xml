<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.10,111.13,327.02,19.27">ICTNET at Blog Track TREC 2010</title>
				<funder ref="#_cfDCefW #_VeKGS3d">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_ajGKGnN">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.90,133.72,31.52,8.93"><forename type="first">Xueke</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.66,133.72,23.77,8.93"><forename type="first">Yue</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.86,133.72,37.64,8.93"><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName coords="1,252.24,133.72,41.30,8.93"><forename type="first">Xiaoming</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.28,133.72,39.85,8.93"><forename type="first">Zeying</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.00,133.72,41.94,8.93"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,396.12,133.72,33.63,8.93"><forename type="first">Lihao</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,438.60,133.72,48.05,8.93"><forename type="first">Shuaishuai</forename><surname>Nie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.10,111.13,327.02,19.27">ICTNET at Blog Track TREC 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FA88F8280F9D7E4836B86863E84FDE35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in blog track of TREC2010. We submit runs for both two tasks, this paper mainly describe approaches to the two tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Blog Track 2009 introduced two pilot tasks, i.e. faceted blog distillation and top stories identification, and each task has two separate sub-tasks. The Blog Track 2010 refines the two tasks of 2009 in many aspects. For example, one major change is that a two-stage submission strategy is adopted which facilitates separately investigating the performance and robustness of deployed approaches for the second sub-task. In this year ICTNET group participates in blog track and submits runs for both two tasks. For both tasks, data preprocessing, which mainly focuses on post content extraction, plays an important role, and we adopt a link tables removing algorithm <ref type="bibr" coords="1,230.80,358.51,11.40,8.77">[5]</ref> to detect valuable content blocks from post pages and discard noisy blocks. The blog track use a collection called Blogs08 which is one order of magnitude bigger than Blogs06 and amounts to over 2TB of data, making indexing and retrievaling more challenging. We use "FirteX" platform 1 , which is developed by our lab, for indexing and retrievaling preprocessed posts. For blog distillation sub-task, inspired by the idea of "ensemble ranking", we combine various rankings to improve the robustness of our system. These rankings may differ from each other in underlying representation models, pseudo-relevance feedback approaches or resources used for pseudo-relevance feedback. For faceted blog distillation sub-task, a language model is learnt for each facet inclination using Google blog search service and annotated data by Know-center. Based on the learnt language model, a generation model is introduced to combine topic-relevance and facet inclination degree in a probabilistic framework. With this model, baseline rankings without considering any facet feature are improved for faceted sub-task, and are further combined to get the final faceted run. For story ranking sub-task, we use training data crawled from Reuters website to learn a classifier to categorize news stories into 5 categories, and then we measure the importance of each news story by accumulating the BM25 scores of posts published on the query day, treating headline and content of the story as query respectively. For news blog post ranking sub-ask, there are two runs without special consideration of diversity requirement, we adopt a similar "ensemble ranking" strategy with blog distillation task for these two runs. For another run considering diversity, we explicitly extract and model 1 http://www.firtex.org/ aspects of each news story based on k-means clustering technology, and posts with formerly well covered aspects are more penalized in the ranking procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Faceted Blog Distillation Task 2.1 Baseline Sub-Task Candidate feeds selection</head><p>For each query, we first select candidate feeds based on the assumption that a relevant feed should contain at least one relevant post. To this end, for each topic, we produce a list of top N ad-hoc relevant posts based on our "FirteX" platform. The relevance scores of posts are computed according to a variant of BM25 model which takes into account proximity information of query words <ref type="bibr" coords="1,496.16,314.41,10.38,8.77" target="#b2">[2]</ref>. We use this list to identify candidate feeds. The parameter N is set to 2500 according to the training on the 2009 topics. By feeds selection, we prune feeds not deserving to be ranked, and thus remarkably improve the efficiency of our system. Based on this candidate feed set, we can produce various rankings for baseline sub-task. We may appropriately select and combine these rankings to improve the robustness of our system. Basically, these rankings may fall into two kinds according to their underlying feed representation models: Local Representation Model and Global Representation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Representation Model</head><p>In this model, each feed is considered as a collection of its constituent posts. How to accumulate the individual post evidence to infer the feed's overall relevance is a key issue. To this end, we adopt small document model which exploits the relationship between the post and the feed <ref type="bibr" coords="1,528.87,493.45,10.33,8.77" target="#b1">[1]</ref>. In this model the topic relevance of feed F is given by the likelihood of F given Q as follows.</p><formula xml:id="formula_0" coords="1,354.96,523.70,143.16,23.38">( | ) ( ) ( | ) ( | ) SD P F P F Q P F P Q P P P F ∈ = ∑</formula><p>Here, ( ) P F is feed prior, which is log( ) F N in our system, flavoring large feeds, F N is the size of feed F, ( | ) P Q P is query likelihood of post P measuring topic relevance, and ( | ) P P F is post centrality. Model components are estimated in various ways, and correspondingly, we have following rankings: 1.</p><p>( | ) P Q P is given by the retrieval score by the variant of BM25, ( | ) P P F is uniform for each post.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Similar with 1, but ( | )</head><p>P P F is proportional to exponential value of negative KL divergence between respective language mode. Both two models are estimated using Maximum Likelihood Estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Similar with 1，but ( | )</head><p>P Q P is given by the classic BM25. Corresponding query is the expansion words obtained with a pseudo-relevance feedback approach based on Wikipedia Resource, and expansion words are weighted with Bol model <ref type="bibr" coords="1,445.22,717.11,10.39,8.78" target="#b3">[3]</ref>.</p><p>It's notable that we only exploit top N ad-hoc relevant posts (i.e. ( | ) P Q P is set to 0 if post P is not in the list). The reason may be that since most posts are irrelevant or weakly relevant even they contains some query terms, they may play an overwhelming part in a feed, weaken the information delivered by relevant posts and make model biased to noisy information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Representation Model</head><p>This model considers a blog as a virtual document which is concatenation of all its constituent posts. This model may avoid words sparsity issue and reflect the recurring interest of the blog, but ignore any distinction between individual posts within the feed. We use a language model approach to rank. Specifically, both the feed and the query are represented as a language model (i.e. multinomial distribution over words), respectively, and relevance score is negative KL divergence between the two language models. Feed language model can be inferred using Maximum Likelihood Estimation with Dirichlet smoothing, while query language model can be inferred by pseudo-relevance feedback based approaches. Given a query, we use different resources (internal or external) for obtaining pseudo-relevance document collection, and based on the obtained collection we use different word weighting approaches, measuring how informative the word is in pseudo-relevance collection against the whole collection, to infer the probability of a word. Correspondingly, we have following different rankings. 4. Blogs08 Resource, Bol word weighting approach 5. Blogs08 Resource, Divergence Minimization word weighting approach [6] 6. Wikipedia Resource, Bol word weighting approach 7. Google blog search service Resource, Bol word weighting approach We also incorporate temporal evidence into the final ranking score. We adapt the idea of entropy to measure whether a feed has a recurring interest in given Q.</p><formula xml:id="formula_1" coords="2,55.50,505.14,211.37,33.87">1 1 1 ( , ) ( , ) log( ) ( , ) ( , ) m t t M M t i i i i r F Q r F Q - r F Q r F Q Recurring-Degree(F,Q)= log(M) = = = ∑ ∑ ∑</formula><p>Here, M is the number of days throughout the timespan, ( , ) t r F Q is the sum of relevance scores of constituent posts published on day t, log( ) M is used for normalization. To obtain final ranking score, the relevance score will be multiplied by the Recurring-Degree score for all above rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rankings Combination</head><p>Given a list of rankings: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Faceted Blog Distillation Sub-Task</head><p>For faceted blog distillation sub-task, we introduce a generative model which combines topic-relevance and facet inclination degree in a probabilistic framework. In this model, faceted ranking score of feed F for facet inclination V is given by the likelihood of F given Q and Q V , where Q V is topic-specific facet inclination language model:</p><formula xml:id="formula_2" coords="2,334.92,220.84,114.87,15.74">( | , ) ( | , ) ( | ) Q Q w P F Q V P F Q w P w V = ∑</formula><p>We can easily derive like that:</p><formula xml:id="formula_3" coords="2,330.24,255.57,163.53,14.66">( | , ) ( ) ( | ) ( | ) ( | , ) Q Q w P F Q V P F P Q F P w V P w F Q ∑ =</formula><p>Obviously, there are two parts in this model. ( ) ( | ) P F P Q F reflects topic relevance of the feed, and can be estimated using any formerly mentioned approaches to baseline subtask .</p><p>( | ) ( | , )</p><formula xml:id="formula_4" coords="2,309.12,299.48,233.55,35.65">Q w P w V P w F Q ∑ gives facet inclination degree estimation, where ( | ) Q P w V is probability of word w in Q V , (<label>|</label></formula><p>, ) P w F Q is probability of w given F and Q, which is estimated depending on both query Q and feed F. Specifically, we estimate it with Maximum Likelihood Estimation, but only considering topic-relevant part of feed F (i.e. only top 2500 relevant posts of query Q) to highlight words closely related to the topic. Via this model, we combine two factors of topic relevance and facet inclination degree to infer ranking score of each feed in a probabilistic framework with theoretical justification. For each facet inclination V, we assign a weight to each word, which reflect its relatedness with specific facet inclination in general, using annotated data by Know-Center <ref type="bibr" coords="2,333.03,459.12,11.95,8.77" target="#b4">[4]</ref>. Then, given a query, we can learn a topicspecific language model Q V by following steps: First, we submit the original query to Google blog search service, and fetch top 100 topical relevant web pages. Second, we use top weighted words as query to compute a score for each page using BM25 model. Top 30 pages are used as pseudo-facet-inclinationrelevance pages. Finally, we use Bol word weighting approach, measuring how informative the word is in the pseudofacet-inclination-relevance page set against the whole Blogs08 collection, to infer the probability of a word in Q V .</p><p>( ) ( | ) P F P Q F can be estimated using formerly mentioned approaches to baseline sub-task. Correspondingly, we can obtain different faceted rankings by replacing ( ) ( | ) P F P Q F with ranking scores in the baseline rankings, respectively. We also use the Q V LM for ranking feeds by the negative KL divergence, and get one more faceted ranking. Similar with baseline sub-task, we obtain faceted runs by selecting and combining these faceted rankings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,206.46,629.06,20.58,5.83;2,192.54,622.95,20.50,10.08;2,228.12,622.95,34.42,10.08;2,253.32,629.06,3.24,5.83;2,168.48,622.95,13.60,10.08;2,197.58,622.95,25.97,10.08;2,243.72,622.95,9.35,10.08;2,184.86,619.34,6.14,13.71;2,267.24,624.59,18.86,8.78;2,69.36,634.37,30.91,8.78;2,81.24,639.72,4.06,5.06;2,55.56,634.37,230.48,8.78;2,52.50,644.09,29.90,8.78;2,92.70,649.18,4.27,5.33;2,84.18,643.59,153.21,9.35;2,262.62,644.15,18.91,8.78;2,241.50,644.09,44.50,8.84;2,52.50,653.87,159.14,8.78;2,236.88,653.87,18.91,8.78;2,215.76,653.87,70.27,8.78;2,52.50,663.59,51.70,8.78;2,138.06,680.43,27.29,9.06;2,68.82,688.41,17.88,9.06;2,166.21,688.41,3.35,9.06;2,116.34,696.21,2.01,9.06;2,134.46,696.21,2.01,9.06;2,105.18,687.47,15.82,5.24;2,123.72,680.43,37.17,9.06;2,120.30,696.21,12.25,9.06;2,110.10,685.59,4.14,7.12;2,87.78,674.41,16.51,18.48;2,171.96,688.97,2.44,8.78;2,52.50,712.37,233.62,8.78;2,52.50,722.09,233.62,8.78;2,309.12,106.86,233.63,8.78;2,309.12,116.64,233.55,8.78;2,309.12,126.36,106.89,8.78"><head>.</head><label></label><figDesc>be the position of feed F in the ranking m rk ,the combination ranking of rks be ( ) comb rks , then the final ranking score of feed F in ( Note that our combination manner can be hierarchical (i.e. a combination ranking may be further combined with other rankings). Two combination strategies are obtained by training in the 2009 topics, and correspondingly, we have two different baseline runs.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="3,309.12,684.02,220.62,7.88;3,309.12,693.74,197.27,7.88;3,309.12,702.78,102.49,8.77"><p>[6]Zhai, C. and Lafferty, J. 2001. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of CIKM '01.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="4.">ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Thomson-Reuters</rs> for kindly providing TRC2 newswire corpus, and thank <rs type="person">Know-Center</rs> for providing annotated blog data. This work was funded by <rs type="funder">National Natural Science Foundation of China</rs> under grant number <rs type="grantNumber">60903139</rs>, <rs type="grantNumber">60933005</rs>. 973 Program of China <rs type="grantNumber">2007CB311103</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cfDCefW">
					<idno type="grant-number">60903139</idno>
				</org>
				<org type="funding" xml:id="_VeKGS3d">
					<idno type="grant-number">60933005</idno>
				</org>
				<org type="funding" xml:id="_ajGKGnN">
					<idno type="grant-number">2007CB311103</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Top Stories Identification Task 3.1 Story Ranking Sub-Task</head><p>We first use training data crawled from Reuters website to learn a classifier to categorize news stories into 5 categories. Then, based on the observation that important news stories should be those concerning wide-ranging influential events and thus mentioned by bloggers extensively, we measure the importance of a news story by summing up the BM25 relevance scores of posts on given day, treating its headline or content as the query respectively. Specifically, for a given query day d, the importance of a news story N can be measured by following formula: Finally, for each category, we rank news stories belonging to that category according to their importance. Note that for a category which has no enough stories, we add to the ranking list the news stores for which the category is second-likely according to the classifier with corresponding importance scores discounted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">News Blog Post Ranking Sub-Task</head><p>There are two runs without special consideration of diversity criterion, we adopted a similar "ensemble ranking" strategy with blog distillation task for these two runs. For each news story, we first retrieve top 50000 blog posts relevant to the news story using headline as query with classic BM25 model. Among these posts, we only consider the blog posts with timestamp &gt;= query timestamp -2 and &lt;= query timestamp + 9. Since posts concerning the event may be issued around the event day with a burst characteristic, and posts deviating from this day are highly probably irrelevant. Then we estimate news story language model with following different approaches, respectively. 1. Use news content and Bol word weighting approach to infer the probability of word w in news story language model 2. Use top 15 posts in original retrieval results and Bol model to infer the probability of word w in news story language model. 3. Similar with 2, but a post sharing common feed with formerly picked posts will be discounted for its contribution to the language model 4. Similar with 2, but top 15 posts are obtained according to the negative KL divergence between news story content language model by 1 and post language model. Here post language model is estimated using MLE. 5. Similar with 4, but a post sharing common feed with formerly picked posts will be discounted for its contribution to the language model. As the task require, there should be three rankings which are centered at a different period of time respectively. For each period, we choose candidate posts within the required period, and then we compute ranking score for each post with the negative KL divergence between the news story language model and the post language model. Post language model is estimated using MLE. Finally, we have different rankings corresponding to different news story language models. Note that the original retrieval ranking is also considered for combination. Similar with blog distillation task, we obtain the two runs by selecting and combining these rankings. For the run considering diversity, we explicitly extract and model aspects of each news story based on k-means clustering technology, and posts will be penalized for sharing formerly covered aspects in the ranking procedure. Specifically, we partition top 150 posts into 5 disjoint clusters using k-means clustering technology. We assume each cluster represent an aspect of the news story. Then, we adopt a greedy strategy to iteratively pick support posts. First, ranking score of each candidate post is initiated with its combination score. Then, at each iteration step, top scored unpicked post is picked, and the ranking score of each unpicked post is penalized according to aspect distribution similarity between the post and the formerly picked posts.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="3,312.80,478.44,76.19,8.77" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,320.57,488.40,222.11,8.77;3,309.12,498.12,233.51,8.77;3,309.12,507.84,120.88,8.78" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,334.34,498.12,204.39,8.77">Retrieval and feedback models for blog feed search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Elsas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,319.74,507.84,85.76,8.78">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,322.26,521.52,206.15,8.77;3,309.12,531.24,223.61,8.77;3,309.12,541.02,121.00,8.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,368.09,531.24,161.01,8.77">ICTNET at Web Track 2009 Ad-hoc task</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,319.74,541.02,105.47,8.77">Proceedings of TREC-2009</title>
		<meeting>TREC-2009</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,321.73,554.64,208.36,8.77;3,309.12,564.42,229.88,8.77;3,309.12,574.14,109.30,8.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,518.14,554.64,11.95,8.78;3,309.12,564.42,226.40,8.77">An effective statistical approach to blog post opinion retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,319.74,574.14,95.10,8.77">Proceeding of CIKM &apos;08</title>
		<meeting>eeding of CIKM &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,321.56,587.82,221.07,8.77;3,309.12,597.54,233.55,8.77;3,309.12,607.32,233.55,8.77;3,309.12,617.04,125.02,8.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,309.12,597.54,233.55,8.77;3,309.12,607.32,79.35,8.77">Stylometric features for emotion level classification in news related blogs</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Muhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Juffinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,413.64,607.32,129.03,8.77;3,309.12,617.04,96.27,8.77">Proceedings of the 9th RIAO Conference (RIAO 2010)</title>
		<meeting>the 9th RIAO Conference (RIAO 2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,321.85,630.66,220.80,8.77;3,309.12,640.44,233.63,8.77;3,309.12,650.16,233.54,8.77;3,309.12,659.94,233.57,8.77;3,309.12,669.66,46.84,8.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,338.18,640.44,204.57,8.77;3,309.12,650.16,79.43,8.77">ContentEx: a framework for automatic content extraction programs</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,411.96,650.16,130.70,8.77;3,309.12,659.94,233.57,8.77;3,309.12,669.66,42.93,8.78">Proceedings of the 2009 IEEE international Conference on intelligence and Security informatics</title>
		<meeting>the 2009 IEEE international Conference on intelligence and Security informatics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
