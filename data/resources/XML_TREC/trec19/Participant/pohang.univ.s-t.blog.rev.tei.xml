<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.24,116.00,308.87,12.91;1,266.63,133.93,82.10,12.91">POSTECH at TREC 2010 Blog Track: Top Stories Identification</title>
				<funder ref="#_MMqszPw">
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_JnpWwTQ">
					<orgName type="full">Ministry of Education, Science and Technology</orgName>
					<orgName type="abbreviated">MEST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.01,171.76,35.58,8.97"><forename type="first">Yeha</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName coords="1,184.45,171.76,57.69,8.97"><forename type="first">Woosang</forename><surname>Song</surname></persName>
							<email>woosang@postech.ac.kr</email>
						</author>
						<author>
							<persName coords="1,249.23,171.76,64.91,8.97"><forename type="first">Hun-Young</forename><surname>Jung</surname></persName>
						</author>
						<author>
							<persName coords="1,320.90,171.76,62.28,8.97"><forename type="first">Vinh</forename><forename type="middle">Tao</forename><surname>Thanh</surname></persName>
							<email>vinhtt@postech.ac.kr</email>
						</author>
						<author>
							<persName coords="1,407.22,171.76,66.11,8.97"><forename type="first">Jong-Hyeok</forename><surname>Lee</surname></persName>
							<email>jhlee@postech.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Division of Electrical and Computer Engineering Pohang</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<addrLine>San 31, Hyoja-Dong, Nam-Gu</addrLine>
									<postCode>790-784</postCode>
									<settlement>Pohang</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Blog Language Model and News Story Language Model correspond to Query Language Model</orgName>
								<orgName type="institution">News Headline Language Model</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.24,116.00,308.87,12.91;1,266.63,133.93,82.10,12.91">POSTECH at TREC 2010 Blog Track: Top Stories Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">03758A4EC23F00BB7A131BBBAF9CD2DE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the TREC 2010 Blog Track. For the Top Stories Identification Task, we explore the relationship among news events, news stories and blog posts. We first extract important news events from the TRC2 corpus using a probabilistic mixture model. Then, we propose a probabilistic approach to identify top news stories. Furthermore, we use an additional feature that can be useful in identifying top news stories. For the News Blog Post Ranking Task, we apply the Maximal Marginal Relevance method (MMR) to make the aspects of the blog posts more diverse.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Blog Track explores information seeking behavior in the blogosphere. In TREC 2010, the Blog Track has two main tasks: Faceted Blog Distillation Task and Top Stories Identification Task. We only participate in the Top Stories Identification Task.</p><p>The Top Stories Identification Task was first introduced in the TREC 2009 Blog Track, and consists of two subtasks: Story Ranking Task and News Blog Post Ranking Task. The tasks explore the usefulness of blogosphere in identifying top news stories.</p><p>The Story Ranking Task aims to find the most important news stories for a given date query. The TREC 2010 Blog Track task has some properties distinguishing it from the last year. First, the task was treated as online event detection. Therefore, we can use only blog posts which were published at or before a query date. Second, for the task, the TRC2 newswire corpus was provided by Thomson-Reuters. The TRC2 corpus contains news contents as well as news headlines. Finally, we need to provide ranked lists according to each of five categories: world, us, sport, scitech and business.</p><p>The News Blog Post Ranking Task retrieves blog posts relevant to a given news story. The retrieved blog posts should cover diverse aspects of the news story.</p><p>For the Top Stories Identification Task, our system consists of three components: Preprocessing, News Stories Ranking and News Blog Post Ranking. In Preprocessing, we remove HTML tags and non-relevant contents such as site descriptions and menus. For the Stories Ranking Task, we extract news events from a set of news stories and propose a probabilistic approach to identify the important news stories. For the News Blog Post Ranking Task, we use the Maximal Marginal Relevance <ref type="bibr" coords="1,403.06,644.17,11.62,8.97" target="#b0">[1]</ref> to cover diverse aspects related to a given news story.</p><p>The TREC Blogs08 collection contains permalinks, feed files and blog homepages. We only used the permalink pages for the Top Stories Identification Task. The permalinks are encoded by HTML, and there are many different styles of permalinks. Besides the relevant textual parts, the permalinks contain many non-topical or non-relevant contents such as HTML tags, advertisements, site descriptions, and menus.</p><p>The non-relevant contents consist of many different blog templates which may be provided by commercial blog service venders. We used the DiffPost algorithm <ref type="bibr" coords="2,449.59,217.30,10.79,8.97" target="#b1">[2,</ref><ref type="bibr" coords="2,462.03,217.30,8.29,8.97" target="#b2">3]</ref> to deal with the non-relevant contents.</p><p>To preprocess the Blogs08 corpus, we firstly discarded all HTML tags and applied the DiffPost algorithm to remove non-relevant contents. DiffPost segments each document into lines using the carriage return as a separator. DiffPost tries to compare sets of lines and then regards the intersection of sets as non-content information.</p><p>For example, let P i and P j be blog posts within the same blog feed. Let S i and S j be the sets of lines corresponding to P i and P j , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NoisyIn f ormation(P</head><formula xml:id="formula_0" coords="2,318.28,324.01,162.31,10.57">i , P j ) = S i ∩ S j (1)</formula><p>We discarded non-relevant contents through the set difference between a document and noisy-information. Then, we performed additional preprocessing by stemming using the Porter stemmer and eliminating stopwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Story Ranking Task</head><p>The Story Ranking Task aims to find important news stories for a given day (i.e. query). For this task, we explore the relationship among news events, news stories and blog posts. We assumed that news events happen, and then news stories related to them are reported. Subsequently, blog users keep up with the events from the news stories.</p><p>Let B q and N q be a set of blog posts and a set of news stories published on a given day q (i.e. query), and I(n i , q) be the importance of a news story n i on the date. We evaluate the importance of the news story n i as follows:</p><formula xml:id="formula_1" coords="2,260.75,523.78,219.84,13.16">I(n i , q) ∝ P(n i |B q , N q )<label>(2)</label></formula><p>The news story set N q can capture the events that happened at a query date q. Then, by our assumption, we can rewrite the probability as follows:</p><formula xml:id="formula_2" coords="2,240.18,581.79,240.40,13.16">P(n i |B q , N q ) ∝ P(B q |n i )P(n i |N q )<label>(3)</label></formula><p>We call these two probabilities Blog Post Likelihood (BPL) and News Story Likelihood (NSL).</p><p>In the following sections, we address how to estimate each probability: BPL and NSL. In addition, we use an additional feature which can be useful in identifying top news stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Blog Post Likelihood</head><p>Blog Post Likelihood (BPL) is the probability that a news story will generate a set of blog posts B q . If a news story is important or newsworthy, the story will attract a lot of attention from many blog users. Then, the users express their opinions or thoughts on their blogs. Therefore, the popularity of the news story in the blogosphere can be used to evaluate its importance. We used BPL to capture the popularity of the news stories in the blogosphere.</p><p>To estimate BPL, we adopted a method proposed in <ref type="bibr" coords="3,361.49,209.17,10.58,8.97" target="#b4">[5]</ref>. The authors first estimate Blog Language Model (BLM) and News Story Language Model 1 (NSLM), and evaluate BPL based on the language model framework.</p><p>We gather blog posts published on a query date (i.e. B q ), and estimate BLM based on the posts. However, the posts may cover diverse topics including sports, economy and science. If you try to capture these topics using a single language model, the language model cannot correctly capture the diverse topics. To mitigate this problem, we divided the blog posts B q into K B clusters using the K-means algorithm and estimate the K B number of BLMs from the clusters. We set the number of clusters K B = 300.</p><p>For estimating NSLM, because the contents of a news story are available unlikely the last year's task, we used the contents of a news story instead of blog posts relevant to the news story. Let θ n be a language model of a news story n. We estimated θ n using the maximum likelihood estimate of the contents of n and the Dirichlet smoothing <ref type="bibr" coords="3,464.25,352.63,10.58,8.97" target="#b5">[6]</ref>.</p><formula xml:id="formula_3" coords="3,242.93,370.70,233.79,23.54">P(w|θ n ) = c(w; n) + µP(w|θ C ) |n| + µ (<label>4</label></formula><formula xml:id="formula_4" coords="3,476.71,378.12,3.87,8.97">)</formula><p>where |n| is the length of n, µ is a smoothing parameter which was set to 1000, and P(w|θ C ) is a collection language model of Blogs08 corpus. Finally, we estimated the query likelihood using the maximum value among scores, which are evaluated by the KL-divergence language model <ref type="bibr" coords="3,371.40,439.83,10.58,8.97" target="#b6">[7]</ref>, between the BLMs and the NSLM as follows:</p><formula xml:id="formula_5" coords="3,213.71,469.92,266.88,23.61">P(B q |n) ∝ max k ∑ w P(w|θ BLM k ) log P(w|θ n )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">News Story Likelihood</head><p>News Story Likelihood (NSL) is the probability that a set of news stories N q will generate a news story n i . For evaluating NSL, we extracted news events that happened at the query day from the TRC2 corpus and used the events to estimate the importance of the news story.</p><p>Let E = e 1 , e 2 , . . . , e |K E | be a set of events that happened at a query day. We can rewrite NSL as follows:</p><formula xml:id="formula_6" coords="3,203.56,605.80,277.02,28.99">P(n i |N q ) = |K E | ∑ j=1 P(n i , e j |N q ) = |K E | ∑ j=1 P(n i |e j )P(e j |N q )<label>(6)</label></formula><p>where |K E | indicates the number of news events that happened at a query day. We set</p><formula xml:id="formula_7" coords="4,134.77,130.28,47.04,10.57">|K E | = 100.</formula><p>For extracting news events, we used a probabilistic mixture model <ref type="bibr" coords="4,422.84,142.90,11.62,8.97" target="#b3">[4]</ref> which was used for extracting salient themes (topics) from a stream of text. The main idea of this approach is to assume that each word in the document is a sample from a mixture model with multiple language models, each representing a theme. We regarded the events as the salient topics in N q .</p><p>We used two kinds of language models as components of a mixture model. One is the TRC2 corpus background model to capture common words used in news articles. The other is a set of news event models representing each event that occurred at a query day. Then a news story n i can be modeled as a sample of words drawn from the following mixture model:</p><formula xml:id="formula_8" coords="4,184.33,267.34,292.39,27.96">P(w; n i ) = λ T RC2 P(w|θ T RC2 ) + (1 -λ T RC2 ) K E ∑ j=1 {π i, j P(w|e j )} (<label>7</label></formula><formula xml:id="formula_9" coords="4,476.71,276.63,3.87,8.97">)</formula><p>where w is a word in a news story n i , λ T RC2 is the mixing weight for θ T RC2 , θ T RC2 is background model and π i, j is a mixing parameter that control a weight for a news story n i to select a news event e j such that ∑ K E j=1 π i, j = 1. We set the parameter λ T RC2 to 0.8. We estimate the background probability P(w|θ T RC2 ) using the TRC2 corpus as follows:</p><formula xml:id="formula_10" coords="4,230.93,366.77,249.66,26.65">P(w|θ T RC2 ) = ∑ n∈T RC2 c(w; n) ∑ w∈V ∑ n∈T RC2 c(w; n) (8)</formula><p>where c(w; n) is the count of word w in a news story n. We used the EM algorithm to estimate the parameters, π i, j and P(w|e j ), by maximizing the log-likelihood of the news stories N q according to the mixture model.</p><p>The remaining issues are how to evaluate NSL, that is, two probabilities, P(n i |e j ) and P(e j |N q ). First, P(n i |e j ) implies the probability that a news event will generate a news story. We calculated the probability using π i, j which was already estimated from the EM algorithm. As mentioned in the event extraction step, π i, j means a weight that a news story n i will choose a news event e j . We can view π i, j as the conditional probability P(e j |n i ). Thus, we estimated the probability P(n i |e j ) using Bayes' rule as follows:</p><formula xml:id="formula_11" coords="4,264.00,516.50,216.59,30.09">P(n i |e j ) = π i, j ∑ K E j=1 π i, j<label>(9)</label></formula><p>We assume that P(n i ) for all the news stories has an uniform distribution.</p><p>Next, the probability P(e j |N q ) can be viewed as the prior about the importance of the news event. There can be a number of approaches to estimate the prior. We evaluate the prior using blog data, because we want to investigate the usefulness of the blogosphere in identifying top news stories. For this purpose, similar to BPL, we use the probability that a news event will generate blog posts as the prior.</p><p>We first smooth the news event language model, P(w|e j ), estimated in the Event Extraction step using Jelinek-Mercer smoothing <ref type="bibr" coords="4,328.73,638.02,10.58,8.97" target="#b5">[6]</ref>.</p><formula xml:id="formula_12" coords="4,228.18,652.86,248.26,13.16">P(w|e j ) = (1 -λ)P(w|e j ) + λP(w|θ C ) (<label>10</label></formula><formula xml:id="formula_13" coords="4,476.44,656.12,4.15,8.97">)</formula><p>where λ is a smoothing parameter, we set λ to 0.8. Then, we can estimate the probability P(e j |N q ) as follows:</p><formula xml:id="formula_14" coords="5,212.46,153.53,268.12,23.61">P(e j |N q ) ∝ max k ∑ w P(w|θ BLM k ) log P(w|e j )<label>(11)</label></formula><p>Now, using the results of Eq. 9 and 11, we can evaluate NSL from Eq. 6. However, a news story is usually dedicated to only one news event. We reformulate Eq. 6 as follows:</p><formula xml:id="formula_15" coords="5,224.15,219.86,252.29,29.00">P(n i |N q ) = |K E | ∑ j=1 δ(e j , n i )P(n i |e j )P(e j |N q ) (<label>12</label></formula><formula xml:id="formula_16" coords="5,476.44,230.18,4.15,8.97">)</formula><p>where δ(e j , n i ) = 1 if e j is same to a news event of n i 0 otherwise</p><p>We assume δ(e γ , n i ) = 1 when γ = argmax j π i, j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Additional Feature</head><p>Similar to last year's approach, we used an additional feature for this task, Temporal Profile.</p><p>Temporal Profile uses the temporal information of blog posts relevant to a news story n. To achieve this, we first retrieved blog posts using a news story as a query. Similar to event extraction in section 3.1, we assumed that the news story n is generated by a mixture model consisting of a query model (news story model) θ n and a background model θ T RC2 . We use the EM algorithm to estimate the query model, and evaluate the relevance score between the news story n and a blog post d using the KL-divergence language model <ref type="bibr" coords="5,200.62,444.75,10.58,8.97" target="#b6">[7]</ref>.</p><p>We evaluated the temporal profile of n using an approach proposed in <ref type="bibr" coords="5,426.24,456.70,10.58,8.97" target="#b4">[5]</ref>. We select 100 blog posts between -14 and +0 days from a query date in order of relevance score. The smoothing parameter α was set to 0.5 and the cosine kernel parameter σ was set to 50. The period φ was set between -7 and +0 days from the query day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Final Ranking Function</head><p>Let S I (n i , q) and S F (n i , q) be scores estimated using the importance of a news story and the temporal profile, respectively.</p><p>S I (n i , q) can be calculated using Eq. 2 as follows:</p><formula xml:id="formula_17" coords="5,193.42,584.19,287.17,28.66">I(n i , q) ∝ P(B q |n i )P(n i |N q ) = P(B q |n i )P(n i |e γ )P(e γ |N q ) S I (n i , q) = log P(B q |n i )P(n i |e γ )P(e γ |N q )<label>(13)</label></formula><p>To integrate two scores, we first adjust each score from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(n, q)</head><p>= S(n, q)min n,q max n,qmin n,q (</p><p>where min n,q = min n S(n, q) and max n,q = max n S(n, q). S(n, q) indicates one score of S I (n, q) and S F (n, q). Finally, we defined the ranking function as follows:</p><formula xml:id="formula_19" coords="6,223.36,163.66,253.08,13.16">Score(n, q) = (1 -β) S I (n, q) + β S F (n, q) (<label>15</label></formula><formula xml:id="formula_20" coords="6,476.44,166.91,4.15,8.97">)</formula><p>where β is the weighting parameter. We set β to 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">News Stories Classification</head><p>In the TREC2010 Blog Track, we need to provide a ranked list with five categories (world, us, sport, scitech and business), instead of an overall ranking. After ranking all the news stories using the approaches described in the above sections, we classified them into five categories using the SVM classifier 2 .</p><p>To train the classifier, we used the categories of the New York Times 3 : WORLD, U.S., SPORTS, TCHNOLOGY+SCIENCE and BUSINESS. Among the news stories published throughout the whole timespan of the Blogs08 corpus, we randomly selected 2,000 news stories from each category. We trained the classifier using the linear kernel and a binary feature of unique terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">News Blog Post Ranking Task</head><p>For the News Blog Post Ranking Task, we viewed a given news story as a query and retrieved blog posts relevant to the news story. Then, to select blog posts that provide diverse aspects of the news story, we used the Maximal Marginal Relevance <ref type="bibr" coords="6,434.53,421.76,11.62,8.97" target="#b0">[1]</ref> method. d = argmax</p><formula xml:id="formula_21" coords="6,217.10,446.73,259.34,21.97">d i ∈R\S λSim 1 (d i , n) -(1 -λ) max d j ∈S Sim 2 (d i , d j ) (<label>16</label></formula><formula xml:id="formula_22" coords="6,476.44,449.99,4.15,8.97">)</formula><p>where R is the ranked list of blog posts, S is the subset of blog posts in R already selected. R\S is the set difference, i.e, the set of as yet unselected documents in R, Sim 1 is the similarity metric used in document retrieval. Finally, Sim 2 is a similarity metric between d i and d j . We defined Sim 2 using the cosine measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Story Ranking Task</head><p>For the Story Ranking Task, we submitted 3 runs as follows:</p><p>1. KLERUN1: Using only Blog Post Likelihood -P(B q |n i ) 2. KLERUN2: Using the importance of a news story -log P(n i |B q , N q ) 3. KLERUN3: Interpolating the score of KLERUN2 with Temporal Profile -0.2 × S I (h, q) + 0.8 × S F (h, q)  <ref type="table" coords="7,175.39,431.47,4.98,8.97" target="#tab_0">1</ref> shows the performances of our runs according to each category. For all the categories except sport, KLERUN1 yields the best performances, and KLERUN2 results in the worst. These results may mean that NSL approach using news event extraction failed to improve the performance of the Story Ranking Task. We have to find a way to extract news events and take advantage of the events for the Story Ranking Task. We retain this problem for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">News Blog Post Ranking Task</head><p>For the News Blog Post Ranking Task, we submitted 2 runs as follows:</p><p>1. KLE1: λ = 0.2 in Eq. 16 2. KLE2: λ = 0.5 in Eq. 16</p><p>Table <ref type="table" coords="7,174.09,619.72,4.98,8.97" target="#tab_1">2</ref> shows the performances of our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have described our participation in the TREC 2010 Blog Track. Compared with the last year, we proposed an approach using event extraction to identify top news stories. Although the approach failed to improve the performance of the Rank Task, we think there is still room for refinement. We will further research in this direction. For the News Blog Post Rank Task, we used the similarity between blog posts to obtain diverse posts about the news story. We think there are several methods such as opinion mining and considering feed information that may increase the diversity of the blog posts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,184.47,117.78,246.40,264.06"><head>Table 1 .</head><label>1</label><figDesc>Story Ranking Task</figDesc><table coords="7,184.47,117.78,246.40,264.06"><row><cell>Run</cell><cell></cell><cell>Measure</cell><cell>Categories</cell></row><row><cell></cell><cell></cell><cell cols="2">business scitech sport</cell><cell>us</cell><cell>word</cell></row><row><cell></cell><cell></cell><cell cols="3">statMAP 0.1851 0.1821 0.1916 0.2458 0.2986</cell></row><row><cell>KLERUN1</cell><cell cols="4">statMPC 10 0.5314 0.2878 0.3310 0.5630 0.7501</cell></row><row><cell></cell><cell cols="4">statMNDCG 10 0.1318 0.2227 0.0985 0.2146 0.1781</cell></row><row><cell></cell><cell></cell><cell cols="3">statMAP 0.0976 0.1356 0.2088 0.1917 0.2597</cell></row><row><cell>KLERUN2</cell><cell cols="4">statMPC 10 0.2778 0.2489 0.2870 0.4407 0.6027</cell></row><row><cell></cell><cell cols="4">statMNDCG 10 0.0719 0.1727 0.0844 0.1778 0.1525</cell></row><row><cell></cell><cell></cell><cell cols="3">statMAP 0.1302 0.1343 0.2488 0.1683 0.2684</cell></row><row><cell>KLERUN3</cell><cell cols="4">statMPC 10 0.4055 0.2063 0.4363 0.3342 0.6205</cell></row><row><cell></cell><cell cols="4">statMNDCG 10 0.1071 0.1498 0.1540 0.1293 0.1850</cell></row><row><cell cols="2">Run Type</cell><cell cols="3">alpha-nDCG@10 P-IA@10 nERR-IA@10</cell></row><row><cell></cell><cell>before</cell><cell>0.466517</cell><cell cols="2">0.156096 0.434547</cell></row><row><cell>KLE1</cell><cell>day week</cell><cell>0.462569 0.466335</cell><cell cols="2">0.164044 0.428372 0.166754 0.426638</cell></row><row><cell></cell><cell>amean</cell><cell>0.465140</cell><cell cols="2">0.162298 0.429852</cell></row><row><cell></cell><cell>before</cell><cell>0.458607</cell><cell cols="2">0.159821 0.422684</cell></row><row><cell>KLE2</cell><cell>day week</cell><cell>0.457419 0.466968</cell><cell cols="2">0.167190 0.423790 0.169818 0.429638</cell></row><row><cell></cell><cell>amean</cell><cell>0.460998</cell><cell cols="2">0.165610 0.425371</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,149.71,384.63,228.70,55.81"><head>Table 2 .</head><label>2</label><figDesc>News Blog Post Ranking TaskTable</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="7,144.73,645.50,182.73,8.07"><p>LIBSVM: http://www.csie.ntu.edu.tw/ cjlin/libsvm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="7,144.73,656.80,88.43,8.07"><p>http://www.nytimes.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>7 Acknowledgement This work is supported in part by <rs type="programName">Basic Science Research Program</rs> through the <rs type="funder">National Research Foundation of Korea (NRF)</rs> funded by the <rs type="funder">Ministry of Education, Science and Technology (MEST)</rs> (<rs type="grantNumber">2010-0012662</rs>), in part by the <rs type="projectName">BK</rs> 21 project in 2011, and in part by the <rs type="institution">POSTECH Information Research Laboratories (PIRL)</rs> project.</p><p>We appreciate providing the TRC2 newswire corpus from Thomson-Reuters.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MMqszPw">
					<orgName type="program" subtype="full">Basic Science Research Program</orgName>
				</org>
				<org type="funded-project" xml:id="_JnpWwTQ">
					<idno type="grant-number">2010-0012662</idno>
					<orgName type="project" subtype="full">BK</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.13,373.72,342.46,8.07;8,146.48,384.68,334.10,8.07;8,146.48,395.64,334.10,8.07;8,146.48,406.60,118.51,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,250.01,373.72,230.58,8.07;8,146.48,384.68,114.46,8.07">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,280.53,384.68,200.05,8.07;8,146.48,395.64,288.51,8.07">SIGIR &apos;98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,417.56,342.46,8.07;8,146.48,428.51,244.08,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,382.21,417.56,98.38,8.07;8,146.48,428.51,98.22,8.07">Kle at trec 2008 blog track: Blog post and feed retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,263.30,428.51,97.07,8.07">Proceedings of TREC 2008</title>
		<meeting>TREC 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,439.47,342.46,8.07;8,146.48,450.44,334.11,8.07;8,146.48,461.39,195.73,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,301.27,439.47,179.33,8.07;8,146.48,450.44,198.03,8.07">Diffpost: Filtering non-relevant content based on content difference between two consecutive blog posts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,453.20,450.44,27.38,8.07;8,146.48,461.39,95.64,8.07">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">5478</biblScope>
			<biblScope unit="page" from="791" to="795" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,472.35,342.46,8.07;8,146.48,483.31,334.11,8.07;8,146.48,494.27,334.12,8.07;8,146.48,505.23,13.45,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,214.22,472.35,266.37,8.07;8,146.48,483.31,61.53,8.07">Discovering evolutionary theme patterns from text: an exploration of temporal text mining</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,230.80,483.31,249.79,8.07;8,146.48,494.27,182.56,8.07">KDD &apos;05: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="198" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,516.19,342.46,8.07;8,146.48,527.14,334.12,8.07;8,146.48,538.11,334.12,8.07;8,146.48,549.07,13.45,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,298.02,516.19,182.57,8.07;8,146.48,527.14,31.02,8.07">Mining the blogosphere for top news stories identification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,199.57,527.14,281.02,8.07;8,146.48,538.11,181.60,8.07">SIGIR &apos;10: Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="395" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,560.02,342.46,8.07;8,146.48,570.98,212.03,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,224.08,560.02,256.51,8.07;8,146.48,570.98,44.69,8.07">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,197.56,570.98,79.52,8.07">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,581.95,342.46,8.07;8,146.48,592.90,275.36,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,227.63,581.95,252.96,8.07;8,146.48,592.90,73.07,8.07">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,238.15,592.90,97.57,8.07">Proceedings of SIGIR 2001</title>
		<meeting>SIGIR 2001</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
