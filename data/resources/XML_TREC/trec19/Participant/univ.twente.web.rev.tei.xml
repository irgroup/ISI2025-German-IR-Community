<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.73,164.85,253.78,15.12">MapReduce for Experimental Search</title>
				<funder ref="#_WHVmnJv">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,211.87,197.33,85.41,10.48"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
							<email>d.hiemstra@utwente.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.94,197.33,71.53,10.48"><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
							<email>c.hauff@utwente.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.73,164.85,253.78,15.12">MapReduce for Experimental Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">63A95AA48991D97F1AD9DB3CC8830A02</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This draft report presents preliminary results for the TREC 2010 adhoc web search task. We ran our MIREX system on 0.5 billion web documents from the ClueWeb09 crawl. On average, the system retrieves at least 3 relevant documents on the first result page containing 10 results, using a simple index consisting of anchor texts, page titles, and spam removal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information retrieval researchers are supposed to come up with new ideas that improve search systems. If such ideas are radically new, experimentally testing might require a non-trivial amount of coding to change existing systems. If for instance a new idea requires information that is not normally in the index (anchor texts, geographical locations, some complex natural language constructs, parsimonious language modeling probabilities, etc., etc.), then the researcher has to recode parts of the system's indexing facilities, and possibly query processing facilities that access this information. If the new idea requires query processing techniques that are not supported by the production system (sliding windows, phrases, structured query expansion, ontology matching, etc.) even more work has to be done.</p><p>We propose to use Hadoop MapReduce <ref type="bibr" coords="1,333.49,522.48,10.52,8.74" target="#b5">[6]</ref> to quickly test new retrieval approaches on a cluster of machines by sequentially scanning all documents. The system read each web page one at the time, and on each page we execute the 50 TREC topics. Sequential scanning allows us to do almost anything we like: sophisticated natural language processing, sliding windows, etc. If the new approach is successful, it will have to be implemented in a full blown inverted file search system, but there is no point in making a new inverted index if the experiment is unsuccessful. To give the reader an idea of the complexity of such an experiment: An experiment that needs two sequential scans of the data requires about 350 lines of code. The experimental code does not need to be maintained: In fact, it should not be altered anymore to provide data provenance and reproducibility of research results. Once the experiment is done, the code is filed in a repository for future reference. We call our code repository MIREX (MapReduce Information Retrieval EXperiments), and it is available as open source software from http://mirex.sourceforge.net Surprisingly, linear scanning for testing experimental search approaches seems to be mostly reported in industrial settings. Jeffrey Dean <ref type="bibr" coords="2,380.05,163.83,10.52,8.74" target="#b2">[3]</ref> describes how Map-Reduce <ref type="bibr" coords="2,169.20,175.78,10.52,8.74" target="#b3">[4]</ref> is used at Google for experimental evaluations. New ranking ideas are tested off-line on human rated query sets similar to topics at TREC. Running such off-line tests has to be easy for the researchers at Google, possibly at the expense of the efficiency of the prototype. So, it is okay if it takes hours to run for instance 10,000 queries, as long as the experimental infrastructure allows for fast and easy coding of new approaches. A similar line of reasoning was followed by Microsoft at TREC 2009: Nick Craswell et al. <ref type="bibr" coords="2,393.73,247.51,10.52,8.74" target="#b1">[2]</ref> use DryadLINQ <ref type="bibr" coords="2,133.77,259.47,10.52,8.74" target="#b6">[7]</ref> on a cluster of 240 machines to run their TREC experiments. The work at Google and Microsoft shows that sequential scanning over large document collections is a viable approach to experimental information retrieval. Some of the advantages are <ref type="bibr" coords="2,217.45,295.33,9.96,8.74" target="#b4">[5]</ref>: 1) Researchers spend less time on coding and debugging new experimental retrieval approaches; 2) It is easy to include new information in the ranking algorithm, even if that information would not normally be included in the search engine's inverted index; 3) Researchers are able to oversee all or most of the code used in the experiment; 4) Large-scale experiments can be done in reasonable time.</p><p>This draft report presents preliminary results for the TREC 2010 ad-hoc task. In Section 2 we briefly introduce MIREX, Section 3 presents our experimental results, and Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MIREX: MapReduce Information Retrieval Experiments</head><p>MapReduce is a framework for batch processing of large data sets on clusters of commodity machines <ref type="bibr" coords="2,227.15,475.63,9.96,8.74" target="#b3">[4]</ref>. Users of the framework specify a mapper function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reducer function that processes intermediate values associated with the same intermediate key. The pseudo code in Figure <ref type="figure" coords="2,338.25,511.49,4.98,8.74" target="#fig_0">1</ref> outlines our sequential search implementation. The implementation does a single scan of the documents, processing all queries in parallel.</p><p>The mapper function takes pairs of document identifier and document text (DocId, DocText). For each pair, it runs all benchmark queries and outputs for each matching query the query identifier as key, and the pair document identifier and score as value. In the code, Queries is a global constant per experiment. The MapReduce framework runs the mappers in parallel on each machine in the cluster. When the map step finishes, the framework groups the intermediate output per key, i.e., per QueryId. The reducer function then simply takes the top 1000 results for each query identifier, and outputs those as the final result. The reducer function is also applied locally on each machine (that is, the reducer is also used as a combiner Our experiments are made of several such MapReduce jobs: We extracted anchor texts from the full ClueWeb09 Category A data, we gathered global statistics for terms that occur in the TREC topics, and we run linear search as depicted in Figure <ref type="figure" coords="3,216.77,417.56,3.87,8.74" target="#fig_0">1</ref>. As a final step, we removed spam pages using the results from Gordon Cormack et al. <ref type="bibr" coords="3,262.73,429.52,9.96,8.74" target="#b0">[1]</ref>. The experiments were run on a cluster of 15 low cost machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head><p>Table <ref type="table" coords="3,160.75,496.24,4.98,8.74" target="#tab_0">1</ref> shows the precision at 5, 10 and 20 web pages retrieved for queries 1-50 of TREC 2009. Here, LM no smoothing denotes a language model with a length prior, that does not use any smoothing such that documents that do not contain all query terms are assigned zero probability (i.e., they are not retrieved). The experiment named LM λ = x denotes a language model with length prior and linear interpolation smoothing, where λ is the weight of the document language model, so λ = 0.95 uses minimal smoothing, and λ = 0.15 uses quite some smoothing. The experiment named LM Dirichlet uses Dirichlet smoothing with µ = 2500, and BM25 is Okapi's BM25 weighting with k 1 = 1.2 and b = 0.75 <ref type="bibr" coords="3,464.20,591.88,9.96,8.74" target="#b4">[5]</ref>.</p><p>Interestingly, on the anchor text representation, the best approach does not use smoothing, so it does not need global statistics to compute IDF-like (inverse document frequency) weights. Global statistics can be incorporated by doing one initial pass over the corpus to collect global statistics for all queries <ref type="bibr" coords="3,133.77,651.66,9.96,8.74" target="#b1">[2]</ref>. Presumably, on document collections of this scale, IDF-like weighting is unnecessary. We used the spam rankings kindly provided by Cormack et al. <ref type="bibr" coords="4,433.30,285.36,10.52,8.74" target="#b0">[1]</ref> -their fusion results -to remove the spammiest 50% of the web pages, i.e., we effectively removed half of the index. Removing spam pages helps early precision considerably on TREC 2009, although not as much as reported by Cormack et al. Searching the document titles in addition to the anchor texts seems to hurt performance a bit on TREC 2009, however, it improved recall (not shown), and in combination with spam page removal, the precision figures are almost the same as searching the anchor texts alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>P@5 P@10 P@20 anchors, LM λ = 0. Table <ref type="table" coords="4,177.37,499.85,4.98,8.74" target="#tab_1">2</ref> shows the precision at 5, 10, and 20 documents retrieved of the official TREC runs. The results are computed over 36 of the 50 topics for which relevance judgments were available at the time of writing these working notes. The held back topic numbers are 54, 61, 66, 68, 72, 78, 83, 86, 87, 90, 95, 98, 99, and 100. The official runs are fully judged until 20 documents retrieved, so all precision values are actual precision values. Of the unofficial run, the fraction of judged documents for 5, 10 and 20 documents retrieved was 1.00, 0.99, and 0.95 respectively. On TREC 2010, including the document titles seems to help a bit, even if spam pages are not removed. The three official runs do not differ substantially on the 36 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We showed how to use Hadoop MapReduce for large-scale information retrieval experiments using little effort. We evaluated a web search approach that uses anchor texts, page titles, and spam detection. If we assume the standard result page size of 10 results, then the system retrieves more than 3 relevant documents on average on the first result page. The code used in our experiment is open source and available to other researchers at: http://mirex.sourceforge.net</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,224.08,327.85,163.09,8.85"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pseudo code for linear search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,171.25,126.34,268.74,126.67"><head>Table 1 :</head><label>1</label><figDesc>Precision at 5, 10 and 20 on TREC 2009</figDesc><table coords="4,333.40,126.34,106.60,8.77"><row><cell>P@5</cell><cell>P@10 P@20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,141.32,409.23,325.85,66.18"><head>Table 2 :</head><label>2</label><figDesc>Precision at 5, 10 and 20 on TREC 2010</figDesc><table coords="4,141.32,409.23,325.85,44.71"><row><cell>95 (utwente3)</cell><cell>0.306</cell><cell>0.256</cell><cell>0.251</cell></row><row><cell>anchors+titles, LM λ = 0.95 (utwente4)</cell><cell>0.311</cell><cell>0.253</cell><cell>0.249</cell></row><row><cell>anchors-spam, LM λ = 0.95 (unofficial)</cell><cell>0.317</cell><cell>0.306</cell><cell>0.276</cell></row><row><cell>anchors+titles-spam, LM λ = 0.95 (utwente4SF)</cell><cell>0.317</cell><cell>0.308</cell><cell>0.303</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The research was funded in part by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs> grant <rs type="grantNumber">639.022.809</rs>). We are grateful to <rs type="person">Yahoo Research, Barcelona</rs>, for contributing to our Hadoop cluster.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WHVmnJv">
					<idno type="grant-number">639.022.809</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,149.27,354.34,328.21,8.74;5,149.27,366.29,286.42,8.74" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1004.5168</idno>
		<title level="m" coord="5,331.80,354.34,145.68,8.74;5,149.27,366.29,177.78,8.74">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,149.27,386.22,328.22,8.74;5,149.27,398.18,328.21,8.74;5,149.27,410.13,328.21,8.74;5,149.27,422.09,61.99,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,436.44,386.22,41.04,8.74;5,149.27,398.18,259.11,8.74">Microsoft Research at TREC 2009: Web and relevance feedback tracks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,427.82,398.18,49.65,8.74;5,149.27,410.13,206.35,8.74">Proceedings of the 18th Text REtrieval Conference (TREC)</title>
		<meeting>the 18th Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="500" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,149.27,442.01,328.21,8.74;5,149.27,453.97,328.22,8.74;5,149.27,465.92,235.95,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,193.92,442.01,283.55,8.74;5,149.27,453.97,48.52,8.74">Challenges in building large-scale information retrieval systems: invited talk</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,217.52,453.97,259.96,8.74;5,149.27,465.92,169.49,8.74">Proceedings of the second ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the second ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,149.27,485.85,328.21,8.74;5,149.27,497.80,328.21,8.74;5,149.27,509.76,144.04,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,268.74,485.85,208.74,8.74;5,149.27,497.80,31.61,8.74">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,202.15,497.80,275.32,8.74;5,149.27,509.76,112.95,8.74">Proceedings of the 6th Symposium on Operating System Design and Implemention (OSDI)</title>
		<meeting>the 6th Symposium on Operating System Design and Implemention (OSDI)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,149.27,529.68,328.22,8.74;5,149.27,541.64,328.22,8.74;5,149.27,553.59,328.21,8.74;5,149.27,565.55,154.56,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,268.18,529.68,209.30,8.74;5,149.27,541.64,176.92,8.74">MapReduce for information retrieval evaluation: let&apos;s quickly test this on 12 TB of data</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,351.91,541.64,125.58,8.74;5,149.27,553.59,133.63,8.74">Multilingual and Multimodal Information Access Evaluation</title>
		<title level="s" coord="5,291.97,553.59,158.47,8.74">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6360</biblScope>
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,149.27,585.47,280.88,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,196.58,585.47,129.61,8.74">Hadoop: The Definitive Guide</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,149.27,605.40,328.21,8.74;5,149.27,617.35,328.21,8.74;5,149.27,629.31,328.21,8.74;5,149.27,641.26,271.09,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,173.66,617.35,303.82,8.74;5,149.27,629.31,162.73,8.74">DryadLINQ: A system for general-purpose distributed data-parallel computing using a high-level language</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,332.02,629.31,145.46,8.74;5,149.27,641.26,240.01,8.74">Proceedings of the 8th Symposium on Operating System Design and Implemention (OSDI)</title>
		<meeting>the 8th Symposium on Operating System Design and Implemention (OSDI)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
