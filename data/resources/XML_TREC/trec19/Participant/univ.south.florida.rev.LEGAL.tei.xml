<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,75.26,38.62,461.38,13.17;1,127.94,54.70,356.03,13.17;1,72.02,70.90,459.61,13.17">Using Bag of Words (BOW) and Standard Deviations to Represent Expected Structures for Document Retrieval: A Way of Thinking that Leads to Method Choices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.94,98.80,94.10,11.26"><roleName>JD, MBA</roleName><forename type="first">H</forename><forename type="middle">S</forename><surname>Hyman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IS/DS Department</orgName>
								<orgName type="institution">of University of South Florida</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Warren Fridy III of Nova Southeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,75.26,38.62,461.38,13.17;1,127.94,54.70,356.03,13.17;1,72.02,70.90,459.61,13.17">Using Bag of Words (BOW) and Standard Deviations to Represent Expected Structures for Document Retrieval: A Way of Thinking that Leads to Method Choices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">919CF032AE37C5BE819FC5181E7185E2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses a theory and proposed design for a retrieval artifact using a bag of words (BOW) approach for terms and a standard deviation method for assigning weights. The paper is organized into three parts. The first part is an historical overview of the development of text mining techniques. It is intended to acquaint the reader with our perspectives and assumptions; it is not intended to be an exhaustive review of the literature. The second part discusses the application of text mining techniques to the legal domain, specifically eDiscovery.</p><p>The third part describes our approach to the 2010 TREC Legal Track problem set #301.</p><p>Part Three is sub-divided into three sections. Section one is a discussion of our approach to document retrieval. Section two is a description of our design approach and method specifically chosen for Problem #301. Section three discusses contributions, limitations, generalizability, and conclusions based on our experience with the initial results produced by the competing artifacts in the TREC proceeding. We include a discussion of the initial results as reported at the conference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>eDiscovery is an emerging problem domain that calls for solutions provided from two separate disciplines: Law and Information Systems <ref type="bibr" coords="1,320.45,526.87,69.71,12.00" target="#b6">(Hyman, 2010)</ref>. The term eDiscovery refers to electronically stored information (ESI) sought by an opposing party during litigation <ref type="bibr" coords="1,493.42,548.83,31.19,12.00;1,72.02,570.79,88.06,12.00">(TREC, 2008 Proceedings)</ref>.</p><p>For many years, lawyers and their clients have relied upon manual and physical methods for retrieving and providing requested documentation during the litigation process. These methods served their purpose when documentation was largely hard copy paper and stored almost entirely within physical files and storage containers.</p><p>As ESI began to have an increased share of the percentage of documentation sought in litigation, lawyers and their clients were forced to develop new approaches for seeking and providing such electronic documentation.</p><p>Early approaches to electronic discovery can be described as a "don't ask, don't tell" agreement between the parties -"I won't ask it from you, if you don't ask it from me." In many, if not most instances, lawyers have been intimidated by the prospect of eDiscovery largely due to several challenges that must be considered: There is no bright-line legal precedent in this area, so how should I advise my client? What documents am I looking for? How do I ask for them? How can I force the other party to comply? What arguments can I make to the court to assist me in forcing the other party to comply-will the court be as intimidated as the lawyers when drafting an order? Sometimes it is the clients who are intimidated by the prospect of eDiscovery. If there are 15 Gigs of information out there, and it costs a client $100 -$500 per Gig depending on the vendor, that can add up to a lot of money to reserve in a discovery budget already committed to depositions, process service, copies, and other ancillary items. As a result, many lawyers report that their clients are willing to "leave money on the table," rather than litigate a commercial dispute, that they may or may not win an award, that may or may not get reduced by the court, that may or may not get reversed upon appeal.</p><p>We believe these issues remain relevant today. It is important to think about the problem set in these terms, especially given this year's provision for acceptance of manual retrieval as a viable approach (TREC 2010 Legal Task Interactive Guidelines), and for one of the problem sets dedicated to privilege claims (Problem #304). This perspective is what drives our philosophy and our approach to the TREC 2010, Problem #301.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Historical Overview</head><p>There have been studies as early as the 1950s comparing automated methods for classification of documents <ref type="bibr" coords="2,207.16,668.74,69.60,12.00" target="#b6">(Hyman, 2010)</ref>. Early methods were founded upon NLP, natural language processing <ref type="bibr" coords="2,173.66,690.70,69.71,12.00" target="#b6">(Hyman, 2010)</ref>. Zellig Harris conducted studies on linguistics in the 1950s and published a paper entitled "Distributional Structure" in the journal Word in 1954. In 1964, Harold Borko published findings from an experiment comparing automated classification to human classification.</p><p>In the late 1960s, Goffman proposed an "indirect method" for document retrieval. The method is based on a theory of conditional relevance. The process takes a set of potentially relevant documents and assigns a probability to each document by evaluating the probability that document 'X' is relevant, if it is given that document 'Y' is relevant. <ref type="bibr" coords="3,422.35,216.26,71.08,12.00;3,72.02,238.22,77.60,12.00" target="#b2">(Croft and Van Rijsbergen, 1976</ref>).<ref type="foot" coords="3,159.98,235.67,4.08,8.04" target="#foot_0">1</ref> </p><p>In the 1980s we see greater development of probabilistic approaches and theories based on semantics. In 1986 Marcia Bates published a model for searching unstructured documents. The model design incorporated three principals: Uncertainty, Variety, and Complexity.</p><p>In 1990, Deerwester, Dumais, Furnas, and Landauer published the seminal work in the search approach known as Latent Semantic Analytics (LSA). According to Huang and Kuo, LSA<ref type="foot" coords="3,524.50,387.50,4.08,8.04" target="#foot_1">2</ref> has been a method proposed to deal with the issue of the poor fit associated with the limitation of synonyms, polymorphisms, and dictionaries. <ref type="foot" coords="3,301.25,431.56,4.08,8.04" target="#foot_2">3</ref>Ascertaining colloquial slang in an industry, sub-industry, or among operators in the field can be a difficult task. Many references in documents are based on local jargon often never considered by the drafter for the purpose of informing personnel outside the technical team loop. Hence the problem with relying upon industry dictionaries such as UMLS (Unified Medical Language System) in the medical domain, or the emerging LOIS project (Legal Information Sharing) in the legal domain.</p><p>The nature of document search is iterative. What search operator would feel confident performing a single pass and declaring that single set of results to be reliable? Initial search terms need to be refined. Parameters need to be re-evaluated. This brings us to query expansion.</p><p>In 2008, Wang and Oard published an article experimenting with three classes of "vocabulary enrichment." The classes were interactive relevance feedback, fully automatic relevance feedback, and thesaurus expansion. Their results supported their criticism of query expansion as "noisy." Also in 2008, Schweighofer and Geist proposed a method that made use of "lexical ontologies" and user feedback. They made use of a document oriented database askSam and an open text tool server Solr. Their implementation was based on a user generated query that is expanded based upon an ontology and relevance feedback. They formulated an iterative loop creating the new expanded query. Their approach is an excellent example of combining a first pass operator list of terms, augmented by a dictionary, and using feedback in an iterative loop.</p><p>Their approach addresses a significant issue with unstructured document/text mining, combining industry terms (a dictionary) with colloquial slang and relevance feedback. This can be implemented with an iterative loop that addresses two challenges in unstructured document/text mining, query expansion and probabilistic relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Mining in the Legal Domain</head><p>As of 2010, eDiscovery still represents a new and emerging challenge in the legal IR domain. The legal domain consists of complex terms and abstract concepts not easily represented <ref type="bibr" coords="4,134.78,575.95,141.62,12.00">(Scheighofer and Geist, 2008)</ref>. eDiscovery is a complete departure from the traditional reasons for using legal IR.</p><p>Traditionally lawyers have been used to searching for documents in the form of cases using key notes, an NLP/BOW (bag of words) approach. This method was well suited to the goals of the legal domain -searching with static terms for cases that established legal precedent. The method of search relied upon a manual indexing --cases announced in court reporters would be painstakingly catalogued by key note. This was done specifically so that documents catalogued could be retrieved at a later time. Compare this cataloguing to whatever method may be in place to catalogue a potential group of unknown number of documents sought in litigation; the challenge becomes apparent.</p><p>The documents retrieved using key words are cases that allow lawyers to be informed on a subject matter and predict a possible range of outcomes for a client's case. An example of this would be a lawyer who wants to advise a client who has recently been arrested for the possession of drugs resulting from a search of his car during a traffic stop. The lawyer could use one of the commercially available legal search engines and enter a simple key word search: automobile, drugs, seizure. The result produced would most likely be a reliable number of cases that the lawyer could use to advise the client about issues involved in the case and the range of outcomes.</p><p>By contrast, a search for discovery documents has no such reliable static terminology.</p><p>The documents existing in whatever industry being litigated contain dynamic, not static terminologies. Quite often terms, definitions and references vary from company to company, and division to division within a company. It is no wonder why many practitioners may choose to simply avoid the problem completely through an agreement that, "I won't ask for your eDocuments, if you don't ask for mine."</p><p>Given the above we believe that one of the foremost concerns of eDiscovery is to determine methods to search for documents that were never intended for later, third party retrieval. Searching for a case using the key term automobile has a very strong likelihood of capturing cases involving trucks, cars, motorcycles and most other forms of personal conveyance. The same cannot be said of searching for all documents in a person's hard drive that are associated with the term "oil" or "petroleum."</p><p>There are two foundational issues that must be addressed when searching an unstructured domain. The first is the synonym problem. When a user searches for an automobile, the search engine needs to include car, truck, motorcycle, etc. The second problem is known as "polysemy," many words having more than one meaning. <ref type="bibr" coords="6,413.86,96.38,109.20,12.00">(Deerwater, et al. 1990</ref>).</p><p>Synonyms and polysemies are two factors that reduce the power and accuracy of IR.</p><p>We observed in the TREC 2010 problem sets that a formidable challenge in eDiscovery is the nature of the documents themselves. Often, if not always, the documents sought during litigation have been created with no consideration for future, third party search. Many times documents are quick streams of thought jotted down and emailed to fellow workers. Quite often subject headings are misleading or attachments are mislabeled, both of which can result in a relevant document being missed or a non-relevant document being included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gap in Current Research and Our Approach</head><p>The current approaches to IR devote the majority of resources to complex classifier design. Our focus is on the primary purpose of IR -the successful retrieval of a maximum number of relevant documents to the user. To achieve this we advocate a "back to basics" approach to determine if recall success is being impacted by the complexity of the classifier or by the nature of the data set itself. We have framed our approach using hypotheses listed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypotheses</head><p>H1: BOW terms manipulated through iteration and weighting methods will produce recall results as good as a sophisticated classifier.</p><p>H2: The use of mean occurrences and standard deviations will produce precision results as good as a sophisticated classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Design Artifact and Method for Problem #301</head><p>Our approach to design of a retrieval system focuses on choice points along the search query building process. The first choice is the collection of terms to be used. The second choice is the weighting method and assignment of weights to the terms and occurrences. Choices have direct consequences to the width and depth of the net cast. These consequences should be taken seriously by the user and the system designer alike.</p><p>We suggest the assignment of weights should be based on the structure of the terms one would expect to see in a target document. In theory, the collection of target documents should closely resemble the expected structure of terms. From the expected structure we predict a mean number of term appearances. In this case, we assign a threshold for acceptance based on the standard deviation of the expected terms from the mean appearances. There are numerous alternative threshold assignments that may be used. We believe the key factor is how the expected structure of selected terms appears in targeted documents.</p><p>Our experimental design in this case is a bag of words (BOW) approach, where the terms are weighted using standard deviations from the mean number of occurrences of terms in the suspect document. Our first step is the development of a dictionary of terms that on their face are good candidates for inclusion, given the subject area and domain. The next step is to gather and review synonyms and colloquial slang. The initial terms, synonyms and slang, form the first pass query. The first pass is implemented with no weights. The results produced from the first pass are analyzed by mean number of appearances per term and standard deviation.</p><p>We find that user choices for the construction of the terms and the mechanism for scoring the relative importance of a particular term's appearance, impact the final set of documents targeted.</p><p>The nature of the domain searched and the use of language in that domain should not be underestimated. When a set of terms is agreed upon, some form of article construction along with an assigned weight is applied. For example, we have found through simple trial and error that a given term appearing several times in a document may be a good sign, and should receive a relative score to reflect such. However, a term that appears too many times could indicate an irrelevant document, and should receive a corresponding relative score to reflect such. We assign weights to the number of appearances of terms in a document. The choice of weighting has a direct impact on whether a document is included or excluded.</p><p>Our design assumes the purpose behind the first pass to be to capture as many documents as possible. The main concern at this stage is missing a document due to failure of synonyms.</p><p>The purpose behind the second pass in our design is to narrow the documents returned.</p><p>It is for this reason that we use a more targeted group of terms, comprised mostly from the collection of initial terms.</p><p>We implement an evaluation stage to validate the structural pattern of terms. There are variations that can be implemented at this choice point. One variation is to return the group of documents based on a count of all terms occurring in a document. Another variation is to return documents based on per term count and then weight each term according to its count. A third variation is to count all occurrences of nouns in the document. This is done similar to a grammar checker. A fourth variation is to return a word list. This list would contain all newly discovery terms in a document. What makes this option appealing is the ability to have the user review the pattern of terms for possible document elimination. In other words, if a suspect document has a particular pattern of terms, eliminate it. This method allows the system to account for the misuse of terms, and the overuse of terms.</p><p>Calculations based on counts of terms in the documents must be assigned whichever method is chosen for the evaluation stage. These calculations lead to scores for documents to be grouped for comparison. In this case we chose to use standard deviations from the mean number of appearances for weights. Just as in evaluation choices, there are numerous alternative methods for calculating term occurrences and scoring documents. One such example as mentioned earlier is to count all terms, or count per term. Whichever method is selected, the nature of our approach lies in the choice points the user makes in the article construction and scoring mechanism. Our design and choice points based on our theory has been graphically depicted in Figure <ref type="figure" coords="8,245.02,631.90,4.55,12.00">1</ref>. For the problem #301 experiment we chose to use the first pass and second pass as described, and implement Variation 1 and Evaluation method 1 for third pass and output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Set</head><p>The data set used in this case is a portion form the ENRON data set. It contains over 650,000 emails, some with attachments and some without. A document (email) can be deemed relevant due to the body of the email containing relevant information, or the attachment to the email containing relevant information. The data set has been previously validated by prior Text Retrieval (TREC) proceedings and is publically available through the Electronic Discovery Relationship Model (EDRM) web site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of Results</head><p>The initial results suggest that our method produced the highest recall of documents (.20). However, our method also produced the lowest level of precision (.20). Our F-score result, which is a function of recall and precision, was the second highest (.20), despite our extremely low performance in precision.</p><p>How should these results be interpreted? Well, like many experimental methods, the results are circumstance driven. The data set comprised approximately 650,000 documents, of which we retrieved approximately 23,000. That represents a lot of documents for a user to review, especially with precision as low as 20%. Consider the fact that the next highest amount of documents retrieved was around 13,000, albeit with a higher precision rate. Which method has proven to be better: The method that produced more documents, but also more irrelevant documents, or the method that produced fewer documents, but a higher percentage of relevant ones? Success and failure when described this way becomes a question of customer satisfaction -elusive to define, and difficult to satisfy. We suggest that the utility of our experiment lies in its ability to compete effectively with the more sophisticated approaches applied to problem #301. The utility of our artifact lies in the ability of the user to make iterative improvements to the classifier based on human in the loop feedback. This has the effect of allowing the decision maker to continue evaluating test run documents, and use that feedback to improve the next iteration by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>This paper contributes to the domain of unstructured document retrieval by offering a design artifact that allows a user to conduct, on the fly, retrieval experiments. Our artifact offers users the ability to choose terms, assign weights, and create a scoring mechanism. The user observes results based on choices made. The output of our system is a report in the form of an Excel spreadsheet in CSV format. The report affords the user the ability to select and filter, based upon columnar output, segregated by specific pass number. This output method allows the user the ability to add a human in the loop component to aid in recognizing emerging patterns that may escape an automated process. The power of our artifact lies in its simplicity. Our artifact can be used to benchmark results produced by more sophisticated classifiers that operate as black boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>One limitation of this paper is the fact that the artifact designed is still largely a work in progress. Additional experiments need to be performed to better assess the impact of article construction, use of threshold values for term counts, and alternative choices for weighting methods.</p><p>A second limitation in this study focuses on the nature of retrieval terms themselves.</p><p>The foundation of a search is based on the initial choices of terms and synonyms. If the initial terms are not well represented, even the best weighting system is not going to produce effective results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizability</head><p>Although the artifact in this case has limitations, the choices associated with the methods explored apply to all document retrieval problem sets and the decision to choose a particular classifier approach. We have found in our research that both the user and the designer of a retrieval artifact must give careful thought and consideration to the choice points discussed in this paper. Terms chosen, weights assigned, and word structure expected, all have impacts on the final result produced. Our artifact and experiment show one such example that we believe is prototypical in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Research</head><p>Additional experiments exploring the impacts of choice points associated with terms and weights will provide further insight for document retrieval in the unstructured domain.</p><p>Further experimentation will also provide greater guidance regarding the utility of using a BOW approach as an alternative to classifiers such as Latent Semantic Indexing when choosing terms and weights for designing an artifact for document retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this study we designed an artifact for document retrieval based on initial terms, synonyms and slangs. We chose standard deviation from mean number of appearances of terms in a document as a weighting method. We used a multiple step, iterative process to produce our results. We found choices made at several steps of the process have varied impacts on results produced. We found initial choice of terms and the expectation of how those terms are constructed in a document of interest had an impact on choice of weights applied and results produced.</p><p>The initial results support our hypothesis that a BOW approach can produce as good a recall as the more sophisticated classifiers. The initial results do not support our hypothesis that a BOW approach can produce as good a precision as the more sophisticated classifiers. Further experimentation is recommended to explore the significance of the impact of the choices made in designing the retrieval artifact, and determining how recall and precision are affected by the choice of classifier used.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,77.54,607.03,446.65,9.96;3,72.02,621.07,454.97,9.96;3,72.02,635.23,158.22,9.96"><p>This method should sound familiar to TREC participants in the Interactive Task -taking suspect documents 'Y', presenting them to the topic authority, receiving feedback regarding the documents, and using that feedback to assess the relevance of documents 'X.'</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,79.82,659.23,201.65,9.96"><p>Also referred to as Latent Semantic Indexing (LSI).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,77.54,683.35,401.90,9.96"><p>TREC participants can certainly relate to the difficulties encountered when proposing search terms.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,72.02,100.94,438.57,11.04;12,72.02,114.38,145.87,11.04" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,121.38,100.94,220.81,11.04">Subject Access in Online Catalogs: A Design Model</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,355.27,100.94,155.32,11.04;12,72.02,114.38,86.76,11.04">Journal of the American Society for Information Science</title>
		<imprint>
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.02,141.14,421.51,11.04;12,72.02,154.58,126.94,11.04" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,121.02,141.14,372.51,11.04;12,72.02,154.58,66.87,11.04">Measuring the Reliability of Subject Classification by Men and Machines</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964-10">Oct. 1964</date>
		</imprint>
	</monogr>
	<note>American Documentation</note>
</biblStruct>

<biblStruct coords="12,72.02,181.46,467.42,11.04;12,72.02,194.90,238.46,11.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,237.56,181.46,235.17,11.04">An Evaluation of Goffman&apos;s Indirect Retrieval Method</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,486.34,181.46,53.10,11.04;12,72.02,194.90,126.86,11.04">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">327</biblScope>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.02,221.78,449.00,11.04;12,72.02,235.22,353.19,11.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,395.16,221.78,125.86,11.04;12,72.02,235.22,34.80,11.04">Indexing by Latent Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,120.50,235.22,248.95,11.04">Journal of the American Society for Information Science</title>
		<imprint>
			<date type="published" when="1990-09">Sep. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.02,262.01,311.55,11.04" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Harris</surname></persName>
		</author>
		<title level="m" coord="12,118.72,262.01,103.39,11.04">Distributional Structure</title>
		<imprint>
			<date type="published" when="1954">1954</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.02,288.89,461.97,11.04;12,72.02,302.33,322.64,11.04" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,244.45,288.89,289.55,11.04;12,72.02,302.33,59.73,11.04">The Transformation and Search of Semi-Structured Knowledge in Organizations</title>
		<author>
			<persName coords=""><forename type="first">Chua-Che</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chia-Ming</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,143.54,302.33,155.80,11.04">Journal of Knowledge Management</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.02,329.21,466.31,11.04;12,72.02,342.65,28.54,11.04" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,133.71,329.21,251.39,11.04">Designing a Text Mining Artifact for eDiscovery Solutions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Hyman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Working Paper</note>
	<note>Submitted</note>
</biblStruct>

<biblStruct coords="12,72.02,369.53,460.83,11.04;12,72.02,382.97,84.80,11.04" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,273.63,369.53,171.26,11.04">Overview of the TREC 2008 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,456.34,369.53,76.52,11.04;12,72.02,382.97,80.03,11.04">TREC Conference 2008, Proceedings</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.02,409.73,461.37,11.04;12,72.02,423.19,138.67,11.04" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,209.69,409.73,288.64,11.04">Legal Query Expansion Using Ontologies and Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Schweighofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,510.83,409.73,22.57,11.04;12,72.02,423.19,133.91,11.04">TREC Conference 2008, Proceedings</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.02,450.07,434.03,11.04;12,72.02,463.51,247.32,11.04" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,282.35,450.07,157.87,11.04">The Selection of Good Search Terms</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,452.95,450.07,53.10,11.04;12,72.02,463.51,126.86,11.04">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="77" to="91" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.02,490.39,455.89,11.04" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,157.97,490.39,192.97,11.04">Query Expansion for Noisy Legal Documents</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,364.01,490.39,159.13,11.04">TREC Conference 2008, Proceedings</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
