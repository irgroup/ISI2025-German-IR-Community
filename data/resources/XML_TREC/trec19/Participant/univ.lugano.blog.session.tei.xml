<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.02,85.30,297.68,16.84">University of Lugano at TREC 2010</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,82.78,131.00,79.25,11.06"><forename type="first">Mostafa</forename><surname>Keikha</surname></persName>
							<email>mostafa.keikha@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,170.98,131.00,85.96,11.06"><forename type="first">Parvaz</forename><surname>Mahdabi</surname></persName>
							<email>parvaz.mahdabi@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.91,131.00,70.70,11.06"><forename type="first">Shima</forename><surname>Gerani</surname></persName>
							<email>shima.gerani@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.52,131.00,86.38,11.06"><forename type="first">Giacomo</forename><surname>Inches</surname></persName>
							<email>giacomo.inches@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,442.87,131.00,74.20,11.06"><forename type="first">Javier</forename><surname>Parapar</surname></persName>
							<email>†javierparapar@udc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.56,143.31,72.88,11.06"><forename type="first">Mark</forename><surname>Carman</surname></persName>
							<email>mark.carman@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.02,143.31,77.14,11.06"><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
							<email>fabio.crestani@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.02,85.30,297.68,16.84">University of Lugano at TREC 2010</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4EB3F72DF79E3B040B1BFC9AF971B38C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report on the University of Lugano's participation in the Blog and Session tracks of TREC 2010. In particular we describe our system for performing blog distillation, faceted search, top stories identification and session reranking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>User generated content has recently become one of the most important sources of information in the web. This data contains a lot of information regarding users' opinions and experiences, which can be useful in many applications. Forums, on-line discussions, community question answering sites and social networks are all examples of such content. Blogs are another important source of information in this category and are the content of interest for the TREC (Text REtrieval Conference) blog track <ref type="bibr" coords="1,187.80,438.14,9.72,7.86" target="#b5">[5,</ref><ref type="bibr" coords="1,200.21,438.14,6.47,7.86" target="#b8">8]</ref>, which involves tasks of faceted blog distillation and top story identification. We explain our approach to faceted blog distillation in section 2 and to top stories identification is explained in section 3. The methods we used for the TREC session track are described in section 4 and we provide conclusions in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BOG DISTILLATION</head><p>Blog distillation is the problem of retrieving blogs that are relevant to a given query. Retrieving blogs as collections of documents, as apposed to retrieving single documents in traditional IR tasks, is the main characteristic of this problem. We employ the Blogger Model for retrieving blogs for the distillation task <ref type="bibr" coords="1,134.63,590.12,9.20,7.86" target="#b1">[1]</ref>. In this model, blogs are ranked by : p(blog|q) ∝ p(q|blog)p(blog)</p><p>where the blog prior p(blog) is estimated by a uniform dis-.</p><p>tribution and the query likelihood p(q|blog) is calculated as follows:</p><formula xml:id="formula_1" coords="1,366.43,294.26,189.50,43.31">p(q|blog) = Y t∈q b p(t|blog) n(t,q) (2) b p(t|blog) = λp(t|blog) + (1 -λ)p(t)<label>(3)</label></formula><formula xml:id="formula_2" coords="1,355.86,353.02,200.07,20.63">p(t|blog) = X post∈blog b p(t|post)p(post|blog)<label>(4)</label></formula><p>where again p(post|blog) is assumed to be uniform.</p><formula xml:id="formula_3" coords="1,365.46,405.97,190.46,11.50">b p(t|post) = βp(t|post) + (1 -β)p(t)<label>(5)</label></formula><p>This post level smoothing is the only difference between our implementation and the original Blogger Model. p(t|post) is estimated by Maximum Likelihood estimation of the term probability the post. p(t) denotes the probability of the term in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Faceted Search</head><p>For the faceted rankings, we followed our approach from last year with a small modification which we explain below. We first generated positive and negative facet scores for each retrieved blog, denoted pos(b) and neg(b) respectively. Facet scores for each blog are calculated using only the posts that are relevant to the query, (i.e. those in the top 15000 posts for the query). A blog facet score is calculated based on the average of the facet score for the relevant posts:</p><formula xml:id="formula_4" coords="1,323.05,590.36,232.87,30.37">scorefacet(b) = E b [scorefacet(d)] = X d∈b p(d|b)scorefacet(d)<label>(6)</label></formula><p>We consider uniform the probability over posts for a blog, (i.e. p(d|b) = 1/|posts|). These facet scores induce a ranking, denoted rpos(b, q) and rneg(b, q), which we combined with the original relevance ranking rrel(b, q) using the Borda Fuse aggregation method as follows: 1 scoreBF(d, q) = α rrel(d, q) + (1 -α) rfacet(d, q) (7) 1 Note that whenever there are ties in the ranking, (i.e. blogs b1 and b2 have the same score), then the rank for those blogs is the average of the (total order) ranking. In order to enable fair comparison between different facet re-ranking methods, in TREC 2010, the organizers distributed three standard baselines which are the results of the first phase (blog distillation) among participants. Participants could submit up to four runs for each of the standard baselines. We re-ranked the standard baselines using our facet scoring methods and used the TREC 2009 data (i.e. relevance judgments) for training in order to set an appropriate value for the weighting coefficient α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">In-depth versus Shallow</head><p>For the in-depth versus shallow facet, we calculated the Cross Entropy (CE) between each retrieved document(post) and the collection as a whole. We used CE as the positive score for the positive (in-depth) facet value since high CE indicates that the document contains many rare and informative words:</p><formula xml:id="formula_5" coords="2,70.17,397.63,222.73,23.66">pos(d) = CE(p(.|d), p(.|c)) = X t∈d p(t|d) log 1 p(t|c)<label>(8)</label></formula><p>Here p(t|d) is the probability of a term t appearing within the document d, which we calculate using the relative term frequency as follows: p(t|d) = tf(t, d)/ P t tf(t , d), where tf(t, d) is the absolute term frequency. Meanwhile p(t|c) denotes the probability of a term across the whole collection c, for which we use a document frequency based estimate p(t|c) = df(t)/|c| where |c| is the number of documents in the collection. Our rational for using a df rather than tf based estimate is that the former appears less susceptible to noise from spam documents, which oftentimes include terms with very high frequency (high tf values).</p><p>For the negative (shallow ) facet score we simply use the negation of the CE, i.e. neg(d) = -pos(d). Table <ref type="table" coords="2,261.42,555.76,4.61,7.86" target="#tab_0">1</ref> shows the result of re-ranking the baselines with this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Opinion versus Factual</head><p>For the opinion versus factual facet, we built lexicons of opinionated and objective words using the TREC Blog06 collection and corresponding relevance/opionion judgments. In the lexicon, terms were weighted according to a document-frequency based version of the Mutual Information (MI) metric <ref type="bibr" coords="2,138.08,650.91,9.20,7.86" target="#b6">[6]</ref>. We then calculated (positive and negative) facet scores for each retrieved document by averaging over the lexicon weights for each word in the document (see equation 11 below.)</p><p>In order to calculate both positive (opinionated ) and negative (factual ) facet weights for terms we split the Mutual Information metric into two values as follows. Let T denote the event that a document contains the particular term t, and T the event that the document doesn't contain the term. Then let O denote the event that a document is classed as being (relevant and) opinionated about the query and Ō that it is (relevant but) not opinionated about the query. We calculate the positive facet score for a term by calculating the MI summation only over the two positively correlated quadrants (i.e. T ∩ O and T ∩ Ō) as follows:</p><formula xml:id="formula_6" coords="2,321.86,155.60,234.06,32.28">pos(t) = p(T , O) log p(T , O) p(T ), p(O) + p( T , Ō) log p( T , Ō) p( T ), p( Ō) (9)</formula><p>The negative facet score is calculated analogously as follows:</p><formula xml:id="formula_7" coords="2,321.27,204.00,228.98,22.41">neg(t) = p(T , Ō) log p(T , Ō) p(T ), p( Ō) + p( T , O) log p( T , O) p( T ), p(O)</formula><p>(10) We calculate the required joint and marginal probabilities using document frequency estimates using the sets of opinionated O and relevant R documents in the TREC Blog06 collection as:</p><formula xml:id="formula_8" coords="2,390.80,286.68,91.14,28.78">p(T , O) = df(t, O)/|R| p(T ) = df(t, R)/|R| p(O) = |O|/|R|</formula><p>Where df(t, O) is the number of opinionated documents containing the term t. The other joint and marginal probabilities required for equations 9 and 10 are estimated analogously.</p><p>Having calculated positive and negative weights for each term, we then averaged these lexicon weights over each document to calculate positive and negative facet scores for the document as follows:</p><formula xml:id="formula_9" coords="2,360.77,420.45,195.15,20.63">pos(d) = E d [pos(t)] = X t∈d p(t|d)pos(t)<label>(11)</label></formula><p>As an alternative to the lexicon built from the Blog06 collection, for another set of runs, we used a lexicon which is built from amazon review and specification corpus <ref type="bibr" coords="2,543.65,468.52,9.20,7.86" target="#b3">[3]</ref>. We considered p(subj|t) provided by the lexicon as pos(t) in equation 11.</p><p>We should note that we also investigated using the number of comments and emoticons in each post as two additional features (besides the opinion score) and trained a SVM classifier using last year's data, but they didn't show to be useful in training a good classifier.</p><p>Table <ref type="table" coords="2,351.35,552.20,4.61,7.86" target="#tab_1">2</ref> shows the result of re-ranking the baselines with the MI weight of terms in the Blog06 collection and the second method in which we used an opinion lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Personal versus Official</head><p>Finally for the personal versus official facet, the same scores were used as in the opinion case, since we believe that more "personal content" is on the whole more likely to contain opinions than more "official content".</p><p>Table <ref type="table" coords="2,351.35,646.04,4.61,7.86" target="#tab_2">3</ref> shows the result of re-ranking the baselines with the MI weight of terms in the Blog06 collection and the second method in which we used an opinion lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TOP STORIES IDENTIFICATION</head><p>Our method for the top stories task proceeded as follows. We first extracted time-stamped blog posts for each query date. Applying a clustering method on the extracted posts for each date, we generate different topics that have been discussed on each day. Then by considering the importance of each extracted topic in the Reuters headline corpus, we generate a ranked list of headlines for each day. In the following sections we outline our approach in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The algorithm</head><p>In this section we present details of our top stories identification algorithm. Our method for top stories task proceeds as follows.</p><p>1. For every query date we extract a set of blog posts that have a publish date that is the same as query.</p><p>2. We cluster the selected posts for each day in multiple clusters, by assuming each cluster would be about some topics that have been discussed on that date. Details of our clustering method are presented in Section 3.2.</p><p>3. Using the KL divergence between each cluster and the collection, we extract the most informative terms for each cluster.</p><p>4. The informative terms for each cluster are used as a query against the headlines collection in order to extract the most important headlines for each cluster.</p><p>5. Aggregating the ranked lists of headlines for each date, we generate our final ranking of the headlines.</p><p>6. We classify the headlines into the pre-defined classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clustering the blog posts</head><p>For each query date, we selected a random sample containing 10% of the blog posts published on the date of the query and then followed a simple clustering process based on k-means <ref type="bibr" coords="3,366.96,114.92,9.21,7.86">[7]</ref>. We decided to use k-means clustering because of its low computational requirements and the time constraints that we had. The clustering process was run for every query date independently. For each date, we randomly selected 100 documents from among the sample of blog posts to act as seeds for the clustering algorithm. After running the k-means algorithm over the 10% sample for every query, we had a set of 100 clusters from which we extracted 100 topics as explained in the next section.</p><p>In the k-means algorithm three factors need to be determined: the document representation, the similarity measure and the stop condition:</p><p>1. Every document d, a blog post, was represented by its tf idf vector, in which the value for each term t was calculated as follows:</p><formula xml:id="formula_10" coords="3,381.41,283.29,132.32,19.74">tf idf (d, t) = tf (d, t) × log( N df (t) )</formula><p>where tf (d, t) is the term frequency of the term t in the document d, df (t) is the document frequency of the term t in the collection sampling of size N .</p><p>2. In order to compute the documents similarities in the clustering method we used the cosine similarity:</p><formula xml:id="formula_11" coords="3,340.69,376.73,212.07,25.87">Sim(di, dj) = P m t=1 tf idf (di, t) × tf idf (dji, t) pP m t=1 tf idf (di, t) 2 P m t=1 tf idf (dj, t) 2</formula><p>where m is the size of the lexicon.</p><p>3. As a convergence condition, we checked the number of items whose assignments changed in a given iteration. We also limited the number of iterations to 100, although this maximum value was never reached during our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating the Cluster Representation</head><p>After clustering the posts, we use the Kullback-Leibler (KL) divergence between the cluster and the collection in order to select the most informative terms in the cluster:</p><formula xml:id="formula_12" coords="3,334.90,538.26,221.02,19.75">score(t, cluster) = p(t|cluster) log p(t|cluster) p(t)<label>(12)</label></formula><p>Based on this score, we select the top 100 terms for each cluster as the representation of the cluster. The cluster representation is used as a query, to retrieve the related headlines for that cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Aggregating the Ranked Lists</head><p>After retrieving the headlines for each cluster, we have multiple ranked-lists of headlines per day. By normalizing the scores in each ranked list and aggregating all the ranked lists per day, we will have one headline ranked list per query day. We employ two different approaches for aggregation. In the first approach, we use ComSum method which is a simple summation of the scores in different lists as the final score of the headline <ref type="bibr" coords="3,425.15,703.22,9.72,7.86" target="#b4">[4,</ref><ref type="bibr" coords="3,439.75,703.22,6.48,7.86" target="#b2">2]</ref>. In the second approach we use the Ordered Weighted Averaging (OWA) operators for aggregating scores <ref type="bibr" coords="3,408.43,724.14,9.20,7.86" target="#b9">[9]</ref>. The ordered weighted averaging operator, commonly called the OWA operator, was introduced by Yager <ref type="bibr" coords="4,120.12,81.05,9.21,7.86" target="#b9">[9]</ref>. OWA provides a parametrized class of mean type aggregation operators, that can generate an OR operator (M ax), an AN D operator (M in) and any other aggregation operator in between.</p><p>An OWA operator of dimension n is a mapping F : R n → R that has an associated weighting vector W ,</p><formula xml:id="formula_13" coords="4,53.80,149.96,168.48,60.56">W = [w1, w2, ..., wn] T such that n X i=1 wi = 1, 0 ≤ wi ≤ 1,</formula><p>and where</p><formula xml:id="formula_14" coords="4,126.74,230.01,162.08,26.84">F (a1, ..., an) = n X i=1 wibi (<label>13</label></formula><formula xml:id="formula_15" coords="4,288.81,239.18,4.09,7.86">)</formula><p>where bi is the ith largest element in the collection a1, ..., an.</p><p>There are different methods for indicating weighting vector W . We use a quantifier based method introduced by Yager [9].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Classifying the Headlines</head><p>Another part of the top stories identification task is to classify headlines into 5 pre-defined classes, namely: world, US, sport, scitech, business. Since we are not allowed to use any external resource for classification, we decided to use the Reuters corpus itself for classifying the headlines. To this end, we generates 5 short queries for each class manually. Submitting the generated queries to the headlines corpus, we retrieved the top 100 most relevant headlines for each class. We then used the KL divergence to extract the informative terms for each class. Later in order to classify each headline, we calculate its similarity to the 5 class representation and assign it to the most similar class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SESSION TRACK</head><p>The session track is held for the first time in TREC 2010. The objective is to take into account the interaction of the user with the search system during a session and not only one single query. User reformulates his query multiple times, in order to clarify what is his information need. To this end, by looking into the results of previous reformulations, search engine will be able to reveal aspects of his information need which were not explicitly stated at first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Collection</head><p>We used the Category B subset of the ClueWeb and indexed it using the Terrier information retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Our Approach</head><p>We first generated two ranked lists (RL1 and RL2) for the first and second query using the BM25 implementation of Terrier. We then implemented two different approaches.</p><p>1. In the first approach we generate the third ranking (RL3) by scoring documents according to the weighted summation of the reciprocal ranks of documents in RL1 and RL2, where the weight given to documents from RL1 is negative and RL2 is positive. Thus the score for a document d is computed as follows:</p><formula xml:id="formula_16" coords="4,339.23,84.80,217.77,29.62">score(d, RL3) = -α• 1 rank(d, RL1) +(1+α)• 1 rank(d, RL2)<label>(14)</label></formula><p>If a document was not present in one of the ranked lists, it's reciprocal rank was set to 0. We empirically set α to 0.2 and submit a run for this approach.</p><p>2. In the second approach we build the relevance model for the first and second query using the top N ranked documents from RL1 and RL2 as the pseudo-relevant documents (denoted PR1 and PR2). We estimate the relevance models R1 and R2 by averaging the relative frequencies for terms across the pseudo-relevant documents. We also smoothed the R1 estimate with a background language model based on collection term frequency estimates as follows: </p><formula xml:id="formula_17" coords="4,351.34,264.33,181.93,23.67">p(w|R1) = λ N X d∈PR1 tf (w, d) |d| + (1 -λ) ctf<label>(</label></formula><p>The normalizing constant in this case is simply the Kullback-Leibler divergence between R2 and R1. After obtaining the new term distribution, we select the top K terms from R3 and submit them as a weighted query to Terrier again using the BM25 retrieval function. In this way we generated two runs with the parameters: N = 10, K = 10 and λ set to either 0.9 or 0.5.  <ref type="table" coords="5,86.60,140.71,4.12,7.89">5</ref>: Evaluation scores for the entire session of our three submitted runs using the ns-DCG dupes@10 metric three runs for this task. Runid USIRR2010 belongs to our first approach, while USIML052010 and USIML092010 belong to our second approach where we set λ to 0.5 and 0.9 respectively. Our submitted runs were evaluated by three metrics nsDCG@10, nsDCG dupes@10, and nDCG@10. The evaluation scores of our three submitted runs with three mentioned metrics are presented in Table <ref type="table" coords="5,219.28,252.78,3.71,7.86" target="#tab_0">1</ref>-3. Table <ref type="table" coords="5,262.56,252.78,4.61,7.86">4</ref> shows the evaluation scores of our systems for task 1 which can be observed by comparing RL1-RL2 and RL1-RL3. Table <ref type="table" coords="5,288.30,273.71,4.61,7.86">5</ref> shows the evaluation scores of our systems for task 2. ns-DCG@10 for RL1-RL3 is considered as the official metric for Task 2. We see that USIRR2010 performs better than the two other approaches in both tasks. Table <ref type="table" coords="5,245.54,315.55,4.61,7.86" target="#tab_5">6</ref> shows the evaluation scores of our submitted runs using the nDCG@10 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Organizers of Session</head><p>Due to the absence data (query pairs and corresponding relevance judgements), we were unable to set parameters to maximize performance. Arbitrary setting of the parameters (α and λ) explains the relatively poor performance of the approaches. We intend to test parameter settings on relevance data when it becomes available.</p><p>Finally, Table <ref type="table" coords="5,123.28,409.70,4.61,7.86" target="#tab_6">7</ref> shows the performance of our best run with respect to different formulation types. By comparing RL12 and RL13 it can be seen that this method performs better on "Specification" and "Drifting" as opposed to "Generalization". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have described our participation in TREC 2010 Blog and Session track for faceted blog distillation, top stories identification and result re-ranking.</p><p>For the faceted rankings, we first generated positive and negative facet scores for each retrieved document and then combined the facet rankings with the relevance ranking using Borda Fuse.</p><p>For the top stories task we extracted time-stamped blog posts for each query date. Applying a clustering method on the extracted posts for each date, we generated different topics that have been discussed on each day. Then by considering the importance of each extracted topic in the Reuters headline corpus, we generated a ranked list of headline for each day.</p><p>In the session track, for re-ranking the documents, we first generated two rank lists for first and second query. We then implemented two different approaches for building the third ranked list. The first method is based on the reciprocal ranks in first two rank lists. In the second approach we build the relevance model for the first and second query using their top ranked documents. We then tried to select terms from the second ranked list which were rare in the first list using the Kullback-Leibler divergence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.80,68.45,239.11,113.83"><head>Table 1 :</head><label>1</label><figDesc>MAP of the In-depth vs. Shallow facet re-ranking over the three TREC baselines for all queries</figDesc><table coords="2,104.03,68.45,138.64,72.12"><row><cell></cell><cell cols="2">Indepth Shallow</cell></row><row><cell>stdBaseline1</cell><cell>0.3046</cell><cell>0.1425</cell></row><row><cell>CrossEntropy</cell><cell>0.3056</cell><cell>0.1280</cell></row><row><cell>stdBaseline2</cell><cell>0.2176</cell><cell>0.1033</cell></row><row><cell>CrossEntropy</cell><cell>0.2176</cell><cell>0.0957</cell></row><row><cell>stdBaseline3</cell><cell>0.1747</cell><cell>0.874</cell></row><row><cell>CrossEntropy</cell><cell>0.1747</cell><cell>0.0856</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.80,68.45,239.11,263.12"><head>Table 2 :</head><label>2</label><figDesc>MAP of the Opinionated vs. Factual facet re-ranking over the three TREC baselines for all queries</figDesc><table coords="3,97.86,68.45,150.98,263.12"><row><cell></cell><cell cols="2">Opinionated Factual</cell></row><row><cell>stdBaseline1</cell><cell>0.1888</cell><cell>0.2189</cell></row><row><cell>MI</cell><cell>0.1926</cell><cell>0.2155</cell></row><row><cell>Lexicon</cell><cell>0.1974</cell><cell>0.1888</cell></row><row><cell>stdBaseline2</cell><cell>0.1274</cell><cell>0.1757</cell></row><row><cell>MI</cell><cell>0.0976</cell><cell>0.1774</cell></row><row><cell>Lexicon</cell><cell>0.1415</cell><cell>0.1201</cell></row><row><cell>stdBaseline3</cell><cell>0.1085</cell><cell>0.1058</cell></row><row><cell>MI</cell><cell>0.0987</cell><cell>0.1058</cell></row><row><cell>Lexicon</cell><cell>0.1225</cell><cell>0.1020</cell></row><row><cell></cell><cell cols="2">Personal Official</cell></row><row><cell>stdBaseline1</cell><cell>0.1973</cell><cell>0.2457</cell></row><row><cell>MI</cell><cell>0.2573</cell><cell>0.2469</cell></row><row><cell>Lexicon</cell><cell>0.1920</cell><cell>0.2469</cell></row><row><cell>stdBaseline2</cell><cell>0.1442</cell><cell>0.1832</cell></row><row><cell>MI</cell><cell>0.1529</cell><cell>0.1763</cell></row><row><cell>Lexicon</cell><cell>0.1429</cell><cell>0.1583</cell></row><row><cell>stdBaseline3</cell><cell>0.0857</cell><cell>0.1956</cell></row><row><cell>MI</cell><cell>0.0839</cell><cell>0.1956</cell></row><row><cell>Lexicon</cell><cell>0.0848</cell><cell>0.1956</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.80,344.46,239.11,28.81"><head>Table 3 :</head><label>3</label><figDesc>MAP of the Personal vs. Official facet re-ranking over the three TREC baselines for all queries</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,339.23,264.33,216.69,191.57"><head></head><label></label><figDesc>Here tf (w, d) denotes the term frequency of word w in document d, ctf (w) is the collection term frequency and |d| denotes the length of document d.</figDesc><table coords="4,339.23,264.33,216.69,191.57"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>w) d |d| P (15)</cell></row><row><cell>p(w|R2) =</cell><cell>1 N</cell><cell>X d∈PR2</cell><cell>tf (w, d) |d|</cell><cell>(16)</cell></row><row><cell cols="5">It is desirable for this task to highlight words from</cell></row><row><cell cols="5">the term distribution of R2 that are rare in the term</cell></row><row><cell cols="5">distribution of R1 in order to reduce the number of</cell></row><row><cell cols="5">documents from RL1 that are present in RL3. To this</cell></row><row><cell cols="5">end, we weighted term probabilities in R2 by their rela-</cell></row><row><cell cols="5">tive information in R2 and R1 to calculate a new query</cell></row><row><cell>model R3:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">p(w|R3) ∝ p(w|R2) log</cell><cell>p(w|R2) p(w|R1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,53.80,469.68,239.11,163.16"><head>Table 6 :</head><label>6</label><figDesc>Evaluation scores of our three submitted runs using the nDCG@10 metric</figDesc><table coords="5,84.55,469.68,175.69,163.16"><row><cell></cell><cell></cell><cell cols="2">nDCG@10</cell></row><row><cell>Run</cell><cell>RL1</cell><cell>RL2</cell><cell>RL3</cell></row><row><cell cols="2">USIML052010 0.1896</cell><cell>0.2144</cell><cell>0.1645</cell></row><row><cell cols="2">USIML092010 0.1896</cell><cell>0.2144</cell><cell>0.1449</cell></row><row><cell>USIRR2010</cell><cell>0.1896</cell><cell>0.2144</cell><cell>0.2147</cell></row><row><cell></cell><cell></cell><cell cols="2">nsDCG@10</cell></row><row><cell cols="2">ReformulationType</cell><cell>RL12</cell><cell>RL13</cell></row><row><cell>Specification</cell><cell></cell><cell>0.1529</cell><cell>0.1532</cell></row><row><cell>Drifting</cell><cell></cell><cell>0.1967</cell><cell>0.2013</cell></row><row><cell cols="2">Generalization</cell><cell>0.2706</cell><cell>0.2652</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,53.80,654.90,239.11,18.35"><head>Table 7 :</head><label>7</label><figDesc>Performance of our best run across the different reformulation types.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">ACKNOWLEDGMENTS</head><p>We thank the <rs type="institution">TREC</rs> organizers for their hard work. We also want to thank <rs type="person">Thomson-Reuters</rs> for providing us with the headlines corpus.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,321.30,359.37,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.00,374.65,220.44,7.86;5,331.01,385.11,220.43,7.86;5,331.01,395.57,209.11,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,505.94,374.65,45.50,7.86;5,331.01,385.11,216.08,7.86">Bloggers as experts: feed distillation using expert retrieval models</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,342.50,395.57,106.41,7.86">Proceedings of SIGIR 2008</title>
		<meeting>SIGIR 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="753" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.00,407.03,191.22,7.86;5,331.01,417.49,191.23,7.86;5,331.01,427.95,195.48,7.86;5,331.01,438.41,157.21,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,369.14,417.49,153.10,7.86;5,331.01,427.95,195.48,7.86;5,331.01,438.41,26.26,7.86">University of Glasgow at TREC 2007: Experiments in Blog and Enterprise Tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,375.58,438.41,82.88,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.00,449.87,218.97,7.86;5,331.01,460.33,213.94,7.86;5,331.01,470.79,216.07,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,373.46,460.33,171.49,7.86;5,331.01,470.79,51.23,7.86">Kle at trec 2008 blog track: Blog post and feed retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,412.36,470.79,106.43,7.86">Proceedings of TREC 2008</title>
		<meeting>TREC 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.00,482.24,204.35,7.86;5,331.01,492.71,211.64,7.86;5,331.01,503.17,205.99,7.86;5,331.01,513.63,223.52,7.86;5,331.01,524.09,222.51,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,446.58,482.24,88.77,7.86;5,331.01,492.71,211.64,7.86;5,331.01,503.17,15.39,7.86">Voting for candidates: adapting data fusion techniques for an expert search task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,364.79,503.17,172.21,7.86;5,331.01,513.63,218.70,7.86">Proceedings of the 15th ACM international conference on Information and knowledge management</title>
		<meeting>the 15th ACM international conference on Information and knowledge management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.00,535.54,214.06,7.86;5,331.01,546.01,224.73,7.86;5,331.01,556.47,192.09,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,496.94,535.54,48.13,7.86;5,331.01,546.01,94.88,7.86">Overview of the trec-2007 blog track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,444.28,546.01,111.45,7.86;5,331.01,556.47,163.88,7.86">Proceedings of the Sixteenth Text REtrieval Conference (TREC 2007)</title>
		<meeting>the Sixteenth Text REtrieval Conference (TREC 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.00,567.92,184.95,7.86;5,331.01,578.38,201.16,7.86;5,331.01,588.84,69.93,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schtze</surname></persName>
		</author>
		<title level="m" coord="5,456.14,567.92,59.82,7.86;5,331.01,578.38,156.74,7.86">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.00,600.30,198.44,7.86;5,331.01,610.76,223.79,7.86;5,331.01,621.22,215.19,7.86;5,331.01,631.68,131.96,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,385.09,600.30,144.35,7.86;5,331.01,610.76,144.79,7.86">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mcqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,482.86,610.76,71.93,7.86;5,331.01,621.22,215.19,7.86;5,331.01,631.68,59.21,7.86">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.00,643.14,217.23,7.86;5,331.01,653.60,220.20,7.86;5,331.01,664.06,166.36,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,378.82,653.60,157.05,7.86">Overview of the TREC-2006 blog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,331.01,664.06,82.88,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="15" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.00,675.52,178.60,7.86;5,331.01,685.98,220.83,7.86;5,331.01,696.44,217.36,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,384.47,675.52,125.13,7.86;5,331.01,685.98,216.27,7.86">On ordered weighted averaging aggregation operators in multicriteria decision making</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Yager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,331.01,696.44,127.83,7.86">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="190" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
