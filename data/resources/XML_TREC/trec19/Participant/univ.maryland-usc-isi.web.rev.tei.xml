<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,228.14,72.35,153.43,16.84;1,110.92,92.27,387.89,16.84">UMD and USC/ISI: TREC 2010 Web Track Experiments with Ivory</title>
				<funder ref="#_aXBU2f5 #_kAGmj4Y">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
				<funder>
					<orgName type="full">Academic Cloud Computing Initiative</orgName>
					<orgName type="abbreviated">ACCI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,118.47,137.97,76.02,11.06"><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
							<email>telsayed@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.62,137.97,58.90,11.06"><forename type="first">Nima</forename><surname>Asadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.54,137.97,77.87,11.06"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
							<email>metzler@isi.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.12,137.97,59.82,11.06"><forename type="first">Lidan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,444.23,137.97,53.62,11.06"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,228.14,72.35,153.43,16.84;1,110.92,92.27,387.89,16.84">UMD and USC/ISI: TREC 2010 Web Track Experiments with Ivory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9331ECF8079E2EB491D11BF96260EC56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ivory is a web-scale retrieval engine we have been developing for the past two years, built around a cluster-based environment running Hadoop, the open-source implementation of the MapReduce programming model. Building on successes last year at TREC, we explored two major directions this year: more sophisticated retrieval models and large-scale graph analysis for spam detection. We describe results of ad hoc retrieval experiments with latent concept expansion and a greedily-learned linear ranking model. Although neither model is novel, our experiments provide some insight on the behavior of these two approaches at scale, on collections larger than those previously studied. We also discuss our link-based spam filtering algorithm that operated on the entire web graph of ClueWeb09. Unfortunately, results in the spam track were worse than the baseline provided by the track organizers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Compared to research in industry labs and search engine companies, academic information retrieval faces two major challenges: access to data and access to computing resources. The first issue has been alleviated to some extent by the availability of ClueWeb09, 1 a recent web crawl by CMU comprising approximately one billion pages. Indeed, the dataset has allowed researchers to explore, through the TREC framework, retrieval at web scale.</p><p>Clearly, web-scale datasets cannot be easily and efficiently processed by individual machines, necessitating bringing to bear the computation capacity made available by clusters. Several convergent trends have improved researchers' access to computing resources. At the hardware level, falling prices of commodity severs and alternative forms of access such as utility computing have made computing resources more affordable than ever. At the software level, emerging frameworks for large-scale distributed programming such as Map-Reduce <ref type="bibr" coords="1,86.36,598.72,9.71,7.86" target="#b6">[6]</ref> and DryadLINQ <ref type="bibr" coords="1,168.82,598.72,14.32,7.86" target="#b26">[26]</ref> allow IR researchers to focus on solving IR problems, as opposed to wrestling with system-level details such as scheduling and synchronization.</p><p>For the past two years, we have been rethinking various aspects of IR in the context of a cluster-based environment built around Hadoop, an open-source implementation of MapReduce. Last year, we successfully developed, largely from scratch, a web-scale retrieval engine called Ivory <ref type="bibr" coords="1,276.04,671.95,13.49,7.86" target="#b13">[13]</ref>. Accomplishments included a scalable, distributed indexer based on MapReduce <ref type="bibr" coords="1,145.12,692.87,9.20,7.86">[7]</ref>, a novel retrieval model whereby 1 http://boston.lti.cs.cmu.edu/Data/clueweb09/ postings are directly fetched from the Hadoop Distributed File System (HDFS), and integration of web page classifiers for quality, spam, and adult content. Ivory was effective in the ad hoc task in the TREC 2009 web track, on both the 50 million and 502 million subsets of ClueWeb09.</p><p>Building on Ivory and the successes of last year, we explored two major directions for TREC 2010: more sophisticated retrieval models and large-scale link analysis for spam detection. This paper is organized as follows: Section 2 describes improvements we made in the Ivory infrastructure. Section 3 discusses our ad hoc retrieval experiments with latent concept expansion and a greedily-learned linear ranking model. Although neither model is novel, our experiments provide some insight on the behavior of these two approaches at scale, on collections larger than those previously studied. Section 4 describes our link-based spam filtering algorithm that operates on the entire web graph of ClueWeb09. Unfortunately, our results were worse than the baseline provided by the track organizers. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">IVORY ENHANCEMENTS</head><p>We begin with a brief overview of indexing and retrieval in the previous implementation of Ivory deployed in TREC 2009 <ref type="bibr" coords="1,338.48,460.13,13.49,7.86" target="#b13">[13]</ref>. In the indexing process, the collection is divided into smaller document partitions; each is indexed separately, producing a document-partitioned index. The English portion of ClueWeb09, used in the evaluations, is divided into ten segments, each of which serves as a partition. Our inverted indexing algorithm is written in MapReduce: documents are processed in the mappers, which emit postings. Reducers receive postings grouped by term, and then write compressed postings lists to disk. For retrieval, we adopted a broker-mediated architecture: the user sends a query to the broker, which then forwards it to every partition server. Each server returns a ranked list over documents in its partition to the broker, which merges all of the ranked lists to produce the final results.</p><p>This year, we added a preprocessing stage prior to indexing. This stage consists of three major steps, all implemented as MapReduce jobs. In the first step, all documents are parsed into document vectors (with stemming and stopword removal), represented as associative arrays from terms to term frequencies (tfs). At the same time we build a table of document lengths, necessary for retrieval later. In the second step, we construct a mapping from terms to integers (term ids), sorted by descending document frequency (df), i.e., term 1 represents the term with the highest df, term 2 represents the term with the second highest df, etc. During this process, we discard all terms that occur ten or fewer times in the collection, since these rare terms are unlikely to be part of real-world user queries. The resulting dictionary is then compressed with front-coding <ref type="bibr" coords="2,240.46,89.02,13.49,7.86" target="#b22">[22]</ref>. Finally, in the third step a new set of document vectors are generated in which terms are replaced with corresponding integer term ids. Furthermore, within each document the terms are sorted in increasing term id, so that we are able to encode gap differences (using γ codes). The final result is a compact representation of the original document collection.</p><p>Why the addition of this preprocessing stage? Why go through the extra steps of materializing the document vectors? This is necessary to support relevance feedback, e.g., latent concept expansion (see Section 3.2). Relevance feedback requires access to document contents, so the two options are to reparse the documents on-the-fly or to store the document vectors for easy access. Storing document vectors with the actual vocabulary is inefficient, so this is why we create document vectors with terms represented by integers. Only these integerized document vectors are used in the retrieval process, but term document vectors may be useful for independent reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AD HOC RETRIEVAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Term proximity linear ranking models</head><p>This year, in addition to the traditional baseline BM25 model, we explored various linear models for ranked retrieval. Linear ranking models offer a simple, yet principled way to combine different query-document features to score documents effectively. Many widely used ranking models belong to this family of ranking functions <ref type="bibr" coords="2,251.29,387.27,14.32,7.86" target="#b17">[17,</ref><ref type="bibr" coords="2,270.81,387.27,7.16,7.86" target="#b1">1,</ref><ref type="bibr" coords="2,283.19,387.27,6.48,7.86" target="#b9">9]</ref>. Broadly, a linear ranking model is specified by a set of features F = f1, . . . , fN and the corresponding model parameters Λ = λ1, . . . , λN . Each feature fi is a function that maps a query-document pair (q, d) to a real value. The relevance score of document d with respect to query q is computed as:</p><formula xml:id="formula_0" coords="2,120.47,457.67,105.76,17.67">Score(q, d) = i λifi(q, d)</formula><p>From this general form, different linear models can be instantiated by specifying their corresponding feature sets and the associated weights. Among the possible linear model instantiations, we restricted our attention to the sequential dependence model (SD) <ref type="bibr" coords="2,150.85,524.05,14.32,7.86" target="#b17">[17]</ref> and the weighted sequential dependence model (WSD) <ref type="bibr" coords="2,151.85,534.51,9.20,7.86" target="#b1">[1]</ref>. Both of these models employ a combination of term-based and term proximity features to score documents. In addition to these features, since web documents vary in quality, we also employed a documentdependent spam feature provided by the University of Waterloo <ref type="bibr" coords="2,79.80,586.81,9.20,7.86" target="#b5">[5]</ref>. The complete feature set used by our linear models is listed below:</p><p>• Number of occurrences of each unigram in the document (weighted by BM25)</p><p>• Number of occurrences of the exact phrase "qj qj+1" in the document (weighted by BM25)</p><p>• Number of occurrences of an unordered window containing qj qj+1 (window span = 8) in the document (weighted by BM25)</p><p>• Waterloo spam score for each document <ref type="bibr" coords="2,240.27,711.19,9.72,7.86" target="#b5">[5]</ref> Feature Description g t 1 (q) # times q occurs in the collection g t 2 (q) # documents q occurs in the collection g t 3 (q) # times q occurs in ClueWeb09 g t 4 (q) # times q occurs in a Wikipedia title g t 5 (q) 1 (constant feature) There are certainly other ways to define term-based and term proximity features. However, the SD and WSD models serve as strong baselines given previous results on TREC datasets <ref type="bibr" coords="2,352.71,253.57,14.31,7.86" target="#b17">[17,</ref><ref type="bibr" coords="2,370.08,253.57,6.47,7.86" target="#b1">1]</ref>.</p><formula xml:id="formula_1" coords="2,322.19,122.73,22.71,10.63">g b 1 (qj,</formula><p>In the case of the SD model, the weight on a particular feature fi depends on its type. In our experiments, all term occurrence features receive a weight of 0.82, and the term proximity features (e.g., exact phrase and unordered window features) receive a weight of 0.09. These values reflect best-practice settings and have been used in our TREC runs from last year <ref type="bibr" coords="2,393.20,326.80,13.50,7.86" target="#b13">[13]</ref>. The document-dependent spam feature receives a weight of 0.02, which is learned from a line search-given the specified SD feature weights, we scan a set of possible spam weights ranging from 0 to 1 in increments of 0.01, and select the spam weight that results in maximum MAP score when combined with the SD model.</p><p>An advantage of the parameter tying between the querydocument features in the same type is simplicity in the ranking model. A drawback is that it cannot differentiate between "important" query concepts and less important query concepts, since they will be assigned the same weight in the ranking model if they are in the same feature class. For instance, for the query "Shenandoah Valley Tourist Attractions", the query concepts "Shenandoah Valley" and "Valley Tourist" clearly have different importance. Intuitively, we would consider "Shenandoah Valley" to be more important than "Valley Tourist". However, the SD model treats them as equally important, by assigning their term proximity features the same weight in the ranking model. The weighted sequential dependence model (WSD) <ref type="bibr" coords="2,464.82,525.56,9.72,7.86" target="#b1">[1]</ref> improves on the SD model by letting the feature weights (for query-dependent features) vary with the query concepts, such that features are assigned weights in accordance with the importance of the concepts they are defined over. Formally, the weight λi of feature fi(q) takes a parametric form:</p><formula xml:id="formula_2" coords="2,395.99,599.12,80.76,17.67">λi(q) = j wj gj(q)</formula><p>where gj's are meta-features defined over the query for each feature i, and wj's are the free parameters. Hence, λi depends on q via gj and wj. For the query-dependent metafeatures gj, we use both collection features (collection frequency and document frequency) and features from external sources (English Wikipedia and ClueWeb09). The metafeatures are summarized in Table <ref type="table" coords="2,454.05,690.26,3.58,7.86" target="#tab_0">1</ref>.</p><p>The free parameters wj's in the WSD model are trained on the first segment of the ClueWeb09 collection using all 50 TREC 2009 web track queries. We employed a simple line search to identify the values of the parameters <ref type="bibr" coords="3,276.03,68.10,13.50,7.86" target="#b19">[19]</ref>. This method iteratively optimizes the retrieval metric by performing a series of one-dimensional searches. At each iteration, the optimal value for a parameter is discovered while holding all other parameters fixed. The process continues until the improvement in the objective metric drops below a threshold. Given the trained values for the wj's, we then learned an optimal weight for the document spam feature (0.03 in this case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LCE</head><p>Another retrieval model we explored this year is the latent concept expansion model (LCE) <ref type="bibr" coords="3,183.46,194.63,13.49,7.86" target="#b18">[18]</ref>. LCE is a robust query expansion model that provides a mechanism for modeling term dependencies in query expansion. Since it is based on the Markov Random Field (MRF) framework <ref type="bibr" coords="3,239.62,226.01,13.50,7.86" target="#b17">[17]</ref>, it allows us to incorporate a wide range of retrieval features. In our experiments, we considered latent unigram concepts only. We used our trained WSD model as the baseline, and let the LCE model score the unigram concepts contained in the top 5 documents retrieved by WSD, and selected the top 4 terms with the highest scores to form an expanded WSD model. The weights of these unigram expansion features were computed in the same way as the WSD query-dependent features (e.g., as parametric functions of the meta-features described in Section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning to Rank</head><p>We also explored the use of a simple learning-to-rank approach <ref type="bibr" coords="3,85.13,373.47,14.32,7.86" target="#b15">[15]</ref> this year. Machine learning approaches have been shown to be effective for learning highly effective ranking functions, but lack of sufficient training data limits their applicability in the TREC context. We used the TREC 2009 web track ad hoc queries and judgments as our training data. Given their small size, we were forced to use a relatively simple model with a small number of features to avoid over-fitting.</p><p>We used the same linear ranking function defined earlier in Section 3.1:</p><formula xml:id="formula_3" coords="3,120.47,484.04,105.76,17.67">Score(q, d) = i λifi(q, d)</formula><p>where λi's are the model parameters we need to estimate from the training data.</p><p>By limiting the complexity of the model, we discourage over-fitting. In the future, we are interested in exploring automatic methods for obtaining large amounts of (possibly noisy) relevance judgments that can then be used with more sophisticated learning to rank approaches, such as Lamba-Rank <ref type="bibr" coords="3,78.21,581.84,9.72,7.86" target="#b2">[2]</ref> or gradient boosting <ref type="bibr" coords="3,175.69,581.84,13.49,7.86" target="#b27">[27]</ref>.</p><p>We were primarily interested in three types of features that are currently supported by Ivory. These include:</p><p>• Basic IR Scores: These features are simply the scores computed using traditional IR models. The two features of this type we used are the BM25 score and the language modeling retrieval score (specifically, query likelihood with Dirichlet smoothing).</p><p>• Term Proximity Features: Term proximity has been shown to be important for effective retrieval, especially for very large collections such as the web. Therefore, we considered a large family of term proximity scores as features. We used various combinations of ordered and unordered windows, with different window sizes and scoring approaches (i.e., BM25 and language modeling). Additional details can be found in <ref type="bibr" coords="3,509.24,226.93,13.49,7.86" target="#b16">[16]</ref>.</p><p>• Document Features: Query-independent features are useful for inferring the a priori importance of a given document. The document features that we computed are Anti-TrustRank (see Section 4 for more details), the Waterloo spam score <ref type="bibr" coords="3,441.90,291.22,9.20,7.86" target="#b5">[5]</ref>, and PageRank <ref type="bibr" coords="3,518.86,291.22,13.49,7.86" target="#b21">[21]</ref>.</p><p>This results in a total of 45 features (2 basic, 40 proximity, and 3 document). It should be noted that field-specific text matching scores (e.g., anchor text match, title match, etc.) were omitted because Ivory currently does not support indexing semi-structured documents.</p><p>The model parameters (i.e., the λi weights) are learned using a greedy feature selection strategy <ref type="bibr" coords="3,468.30,374.03,13.49,7.86" target="#b16">[16]</ref>. In this approach, features are added to the model, one at a time, according to a greedy selection criterion. At each iteration, the feature that yields the largest gain in effectiveness (as measured by ERR <ref type="bibr" coords="3,339.17,415.88,9.97,7.86" target="#b3">[3]</ref>) after being added to the existing model is selected. This results in a sequence of one-dimensional optimizations that can be solved using simple line search techniques. The learning algorithm halts when the difference in ERR between successive iterations drops below a given threshold (10 -4 ). This training procedure is simple, fast, and yields a model with minimal correlation/redundancy between features.</p><p>Table <ref type="table" coords="3,351.67,489.10,4.61,7.86" target="#tab_1">2</ref> shows the values learned for the model parameters from the 2009 TREC web track ad hoc queries and relevance judgments, where BM25(q, d) is the traditional BM25 score, BM25 bigram (q, d) is the BM25 score of the query bigrams, BM25pairs(q, d) is the BM25 score of all pairs of query terms, and Spam, AntiSpam, and PageRank are the three document features described previously.</p><p>It is interesting that the learned model contains just 6 features among a total of 45. The greedy feature selection strategy determined that the remaining features were mostly redundant, and thus not worth including in the model. The selected features do, in fact, capture different aspects of relevance, such as a term score (BM25), a phrase score (BM25 bigram ), a proximity score (BM25pair), and various query-independent scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation</head><p>We now provide an empirical evaluation of our experiments. All of the model parameters used in our runs were tuned using the TREC 2009 web track data. Furthermore, all statistical significance testing was performed using a onetailed paired t-test at the p &lt; 0.05 level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Ad Hoc Task</head><p>Table <ref type="table" coords="4,89.54,188.15,4.61,7.86" target="#tab_2">3</ref> summarizes the results of our ad hoc retrieval runs. The run identifiers denote the retrieval method (e.g., BM25, SD, WSD, LCE, and L2R for "learning to rank") and the subset of ClueWeb09 documents that were used for the run (the suffix "a" denotes Category A while the suffix "b" denotes Category B). The effectiveness of the runs are evaluated in terms of precision at 10 (P@10), mean average precision (MAP), normalized discounted cumulative gain at 20 (NDCG@20), and expected reciprocal rank at 20 (ERR@20).</p><p>The results show a natural progression of effectiveness from our baselines (IvoryBM25a and IvorySDa) to our more sophisticated approaches (IvoryLCEb and IvoryL2Rb). A deeper analysis reveals our results can be broken into three tiers of effectiveness. The bottom tier consists of BM25 and SD, which are statistically indistinguishable across all metrics. The WSD model, which is statistically significantly better than both BM25 and SD in terms of ERR@20, occupies the middle tier. Finally, LCE and L2R make up the top tier, as the two models are statistically indistinguishable from one another, but statistically significantly better than the WSD model with respect to ERR@20.</p><p>Our results also suggest that better effectiveness can be achieved by only retrieving documents from Category B of ClueWeb09. Similar results were also observed by ourselves and others at the TREC 2009 web track. These findings are in line with the analysis of the relative quality of Category A vs. Category B that we undertook previously <ref type="bibr" coords="4,254.05,470.59,13.49,7.86" target="#b13">[13]</ref>. Our analysis showed that Category B generally had higher quality documents, less spam, and fewer adult pages, and thus was generally a better source of relevant documents.</p><p>It is also interesting to note that our LCE run, which used the WSD model with a spam feature as its base ranking function, was highly effective. Previous attempts to apply pseudo-relevance feedback to web search have yielded inconsistent results. Our preliminary investigation suggests that highly-focused pseudo-relevance feedback (i.e., a few expansion terms selected from a few top-ranked documents), combined with an effective base ranking function, can be effective for web search. Additional analysis and experimentation are necessary to fully understand the observed effects.</p><p>Our most effective run, IvoryL2Rb, was a simple machinelearned ranking function that uses only 6 features. There are various ways that one could improve upon this model, such as using additional features (e.g., those based on anchor text and document structure), and perhaps even combining LCE, or some other form of pseudo-relevance feedback, with learning to rank. Now that we have described the positive aspects of our runs, we briefly describe several cases where our ranking models failed. In particular, there were four topics that none of our runs retrieved any relevant documents for. The four topics were "to be or not to be that is the question" (topic 70), "kiwi" (topic 74), "the wall" (topic 92), and "titan" (topic 94).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Query Types and Diversity</head><p>Each of the topics used in this year's track were manually annotated as being either "faceted" or "ambiguous". Faceted topics have a well-defined focus. The "facets" arise from the fact that different users may be interested in different aspects of the general focus. For example, "horse hooves" (topic 51) has facets that include "caring for hooves", "pictures of horse hooves", "diseases of horse hooves", and "anatomy of horse hooves". Ambiguous topics are less welldefined and it may be difficult to infer the actual intent from a single query alone. For example, "avp" (topic 52) can refer to "Association of Volleyball Players", "AVP anti-virus software", "Avon products company", and so on.</p><p>Figure <ref type="figure" coords="4,354.29,470.59,4.61,7.86" target="#fig_0">1</ref> compares the effectiveness of our runs for faceted and ambiguous queries. The results suggest that our baseline runs (BM25 and SD) are actually more effective on ambiguous queries than faceted ones, whereas our more advanced runs (WSD, LCE, and L2R) are markedly better for faceted queries. The WSD-based runs (WSDa, WSDb, and LCEb) show substantial gaps between faceted and ambiguous queries, which suggests that the methods may not adequately account for query ambiguity. On the other hand, the learning to rank approach shows a significant increase in ambiguous query effectiveness over these approaches, although the gap between ambiguous and faceted queries still remains.</p><p>Finally, we evaluated the effectiveness of our runs with respect to several diversity-aware metrics. It is important to note that we did not officially participate in the diversity task and none of our runs perform any diversity-specific processing. We include these results for completeness and to better understand the strengths and weaknesses of our approaches.</p><p>The results of the diversity evaluation are provided in Table 4. The diversity results closely track the ad hoc results presented in Table <ref type="table" coords="4,394.27,700.73,3.58,7.86" target="#tab_2">3</ref> aware metrics or if our more advanced approaches implicitly account for diversity in some way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SPAM FILTERING 4.1 Model</head><p>Given the large size of the ClueWeb09 collection, spam has led to rising concerns about the effectiveness of classic retrieval methods. Cormack et al. <ref type="bibr" coords="5,216.17,260.05,9.72,7.86" target="#b5">[5]</ref> presents a simple, minimally-trained, content-based classifier that postprocesses ranked lists to yield significant improvements on a variety of metrics. In an attempt to further explore the structure of spam in ClueWeb09 and to properly counterbalance its effect on retrieval, we tackled this problem from a different approach.</p><p>Web graphs provide important information about their components' interconnections and there has been a broad range of studies <ref type="bibr" coords="5,123.41,354.19,14.32,7.86" target="#b10">[10,</ref><ref type="bibr" coords="5,142.16,354.19,11.76,7.86" target="#b12">12,</ref><ref type="bibr" coords="5,158.34,354.19,11.77,7.86" target="#b25">25,</ref><ref type="bibr" coords="5,174.53,354.19,11.77,7.86" target="#b24">24,</ref><ref type="bibr" coords="5,190.71,354.19,7.16,7.86" target="#b8">8,</ref><ref type="bibr" coords="5,202.30,354.19,11.76,7.86" target="#b28">28,</ref><ref type="bibr" coords="5,218.49,354.19,11.76,7.86" target="#b23">23]</ref> on how to exploit link-based characteristics to detect more sophisticated forms of spam, e.g., link spamming. Link spamming involves boosting the rank of certain pages by setting up specific link structures among them. It cannot be properly modeled by content-based filters, as these filters are generally designed to recognize term spam (i.e., misleading document content) <ref type="bibr" coords="5,90.10,427.42,9.72,7.86" target="#b8">[8,</ref><ref type="bibr" coords="5,102.88,427.42,11.76,7.86" target="#b28">28,</ref><ref type="bibr" coords="5,117.70,427.42,10.73,7.86" target="#b20">20]</ref>.</p><p>Gyöngyi et al. <ref type="bibr" coords="5,122.46,437.88,14.31,7.86" target="#b10">[10]</ref> exploits the intuition that good pages, i.e., those of high quality, are unlikely to point to low quality or spam pages. Having collected a set of good pages as its seed in a supervised manner, their proposed algorithm, called TrustRank, then propagates "trust" throughout the web graph. The propagation is done iteratively with a biased PageRank algorithm. Pages are then labeled by applying a certain threshold to the TrustRank scores. Krishnan and Raj <ref type="bibr" coords="5,91.95,521.56,13.49,7.86" target="#b12">[12]</ref>, on the other hand, address the problem in a slightly different way and show their proposed algorithm outperforms TrustRank. Following the intuition that spam pages are unlikely to be pointed to by good pages, their algorithm, called Anti-TrustRank, starts with a set of spam pages as its seed and then runs a biased PageRank to propagate "anti-trust" in the reverse direction. Wu et al. <ref type="bibr" coords="5,278.59,584.33,14.32,7.86" target="#b24">[24]</ref> show that combining trust and anti-trust (i.e., distrust) is more effective than using only trust scores. Given the trust and distrust values, they propose a linear combination in the form below for a page Pi as a way to take into account both properties:</p><formula xml:id="formula_4" coords="5,83.30,652.99,180.10,7.86">Score(Pi) = α • Trust(Pi) -β • AntiTrust(Pi)</formula><p>where 0 ≤ α ≤ 1 and 0 ≤ β ≤ 1 are two coefficients that give different weights to trust and anti-trust scores.</p><p>Despite the fact that these two approaches are mathematically well-formulated and easy-to-implement, there are certain drawbacks to consider. As discussed by Gyöngyi et al., selection of seed pages can skew the final scores, which, after applying the threshold, will alter the effectiveness of the filter. Moreover, in most cases a set of seed pages does not guarantee full coverage of the entire web graph. This gap in coverage comes from the possibility that certain parts of the graph might not be reachable from the seeds. To reduce the effect of these problems, Jiang et al. <ref type="bibr" coords="5,478.56,345.06,14.32,7.86" target="#b11">[11]</ref> suggest the use of a large, automatically-generated seed set, as opposed to a small, carefully-selected seed set.</p><p>Instead of developing our own classifier, our goal for the spam task was to use Wu et al.'s approach <ref type="bibr" coords="5,501.76,386.90,14.32,7.86" target="#b24">[24]</ref> by interpolating between TrustRank and Anti-TrustRank scores for each page in the collection. Additionally, we studied the effects of seed page selection. After extracting the complete web graph from the ClueWeb09 collection, we developed our MapReduce implementation of PageRank <ref type="bibr" coords="5,491.99,439.21,14.31,7.86" target="#b14">[14]</ref> in order to have the necessary tools to efficiently run the TrustRank and Anti-TrustRank algorithms. In our experiments, we limit the number of iterations to 30. Based on changes between the iterations measured by the difference of the L1 norm of the score vectors, we can detect algorithm convergence. Figure <ref type="figure" coords="5,373.61,501.97,4.61,7.86" target="#fig_1">2</ref> plots the convergence (difference in L1 norm of score vectors) versus the iteration number.</p><p>To compute an interpolated score, we assign a value of 1 to both coefficients α and β in the equation above. The page scores are converted to a percentile rank for the purpose of comparing effectiveness <ref type="bibr" coords="5,413.86,554.27,9.20,7.86" target="#b5">[5]</ref>.</p><p>TrustRank creates a seed set by manually labeling pages with high PageRank scores in the inverse web graph <ref type="bibr" coords="5,539.05,575.20,13.50,7.86" target="#b10">[10]</ref>, while in Anti-TrustRank, pages with high PageRank scores in the original web graph are labeled manually <ref type="bibr" coords="5,517.86,596.12,13.49,7.86" target="#b12">[12]</ref>. For TrustRank, good pages are used as the seed set while in Anti-TrustRank, spam pages are used as the seed set. Due to the large size of our web graph, in lieu of manually labeling the pages, we define thresholds θTrust and θAntiTrust and apply them to Waterloo spam scores <ref type="bibr" coords="5,480.25,648.42,9.71,7.86" target="#b5">[5]</ref> to generate seed sets. Pages with a spam score of θTrust or higher are then used as the seed pages for TrustRank and pages with a spam score of θAnitTrust or lower are chosen to form the seed set for Anti-TrustRank. This allows us to easily generate larger seed sets that cover larger portions of the web graph.</p><p>In our experiments we used the pairs (70, 30) and (90, 10) as values for (θTrust, θAntiTrust). The first pair produces a larger seed set but leaves fewer unlabeled pages. However, the tradeoff is, as we lower θTrust and raise θAntiTrust, although we obtain larger seed sets, we increase the chance of having inaccurate seed sets, i.e., a good seed set that might contain spam pages and a bad seed set that might contain good pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We now provide an evaluation of our spam filters. Evaluation data provided by NIST use the metrics introduced by Cormack et al. <ref type="bibr" coords="6,116.64,261.42,9.20,7.86" target="#b4">[4]</ref>. More specifically, ham% is the percentage of misclassified non-spam pages (i.e., non-spam pages that are classified as spam), spam% is similarly the percentage of misclassified spam pages (i.e., spam pages that are classified as non-spam), and, logistic average misclassification percentage (lam%) defined as lam% = logit -1 logit(ham%) + logit(spam%) 2</p><p>where logit(x) = log( x 100%-x ) <ref type="bibr" coords="6,177.83,364.93,9.20,7.86" target="#b4">[4]</ref>. Additionally, since both ham% and spam% measure failure rather than effectiveness, the spam track reports the area above the Receiver Operating Characteristic (ROC) curve. This last metric is equivalent to (1 -ROCA%), where ROCA% is the area under the ROC curve. The dataset against which the filters are tested contains 16956 ham (non-spam) pages and 645 spam pages. Waterloo spam <ref type="bibr" coords="6,150.12,439.21,9.72,7.86" target="#b5">[5]</ref> scores are used as a baseline in the provided evaluation.</p><p>As mentioned earlier, our model takes two parameters θTrust and θAntiTrust as thresholds to form the seed sets. We submitted two runs, IVORY.70.30 and IVORY.90.10, corresponding to the (70, 30) and (90, 10) threshold settings, respectively. Table <ref type="table" coords="6,133.47,501.97,4.61,7.86" target="#tab_4">5</ref> summarizes the evaluation results for the baseline and the two variants of our spam filter. Unfortunately, neither of our runs beat the baseline (unaltered Waterloo spam scores).</p><p>A simple comparison between these two filters shows that IVORY.90.10 has a higher rate of misclassification but a lower (1-ROCA%). Looking at the ROC curves offers an explanation for the behavior of these filters. As depicted in Figure <ref type="figure" coords="6,82.49,585.66,3.58,7.86" target="#fig_2">3</ref>, IVORY.90.10 outperforms IVORY.70.30 when the false positive rate is low. This behavior lies in the choice of thresholds for our filter, (90, 10) versus (70, 30). A threshold of 70 for θTrust is relatively more likely to include more spam pages in the "good" seed set (set of non-spam pages used in the TrustRank algorithm). A threshold of 70 assigns high trust scores to some pages that are in fact spam. This explains why IVORY.90.10 is more effective when the false positive rate is low. However, a threshold of 90 is more likely to cover a smaller portion of the web graph when compared to a threshold of 70, thus propagating trust to fewer pages in the web graph. This might explain why IVORY.90.10 has a higher rate of misclassification than IVORY.70.30.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>Participation in both last and this year's TREC web tracks has taught us valuable lessons in working with web-scale collections. In addition to sharing experimental results, Ivory is publicly-available as an open-source software package. <ref type="foot" coords="6,551.77,311.67,3.65,5.24" target="#foot_0">2</ref>Code necessary to replicate most of the experiments here are included. We hope that this makes it easier for others to build on our results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,316.81,230.87,239.11,7.89;4,316.81,241.33,143.58,7.89;4,316.81,53.80,239.11,162.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of ERR@10 for faceted and ambiguous queries across runs.</figDesc><graphic coords="4,316.81,53.80,239.11,162.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,346.88,242.15,178.97,7.89;5,316.81,53.80,244.80,174.08"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convergence of spam scores.</figDesc><graphic coords="5,316.81,53.80,244.80,174.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,316.81,239.34,239.11,7.89"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ROC curves for the spam filtering systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,322.19,124.49,236.74,75.03"><head>Table 1 :</head><label>1</label><figDesc>Meta-features in the WSD model.</figDesc><table coords="2,322.19,124.49,236.74,55.25"><row><cell>qj+1)</cell><cell># times bigram occurs in the collection</cell></row><row><cell cols="2">g b 2 (qj, qj+1) # documents bigram occurs in the collection</cell></row><row><cell>g b 3 (qj, qj+1)</cell><cell># times bigram occurs in ClueWeb09</cell></row><row><cell>g b 4 (qj, qj+1)</cell><cell># times bigram occurs in a Wikipedia title</cell></row><row><cell>g b 5 (qj, qj+1)</cell><cell>1 (constant feature)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,316.81,55.49,239.11,117.61"><head>Table 2 :</head><label>2</label><figDesc>Learned parameter values for the learning to rank model.</figDesc><table coords="3,381.45,55.49,109.83,86.37"><row><cell>Feature</cell><cell>Weight</cell></row><row><cell>BM25(q, d)</cell><cell>0.7294</cell></row><row><cell cols="2">BM25 bigram (q, d) 0.0616</cell></row><row><cell>BM25pairs(q, d)</cell><cell>0.0433</cell></row><row><cell>Spam(d)</cell><cell>0.0674</cell></row><row><cell>AntiSpam(d)</cell><cell>0.0915</cell></row><row><cell>PageRank</cell><cell>0.0069</cell></row><row><cell>All other features</cell><cell>0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,61.22,55.49,224.27,94.20"><head>Table 3 :</head><label>3</label><figDesc>Summary of ad hoc task results.</figDesc><table coords="4,61.22,55.49,224.27,73.42"><row><cell>ID</cell><cell>P@10</cell><cell cols="3">MAP NDCG@20 ERR@20</cell></row><row><cell cols="3">IvoryBM25a 0.2313 0.1008</cell><cell>0.1183</cell><cell>0.0693</cell></row><row><cell>IvorySDa</cell><cell cols="2">0.2458 0.0992</cell><cell>0.1315</cell><cell>0.0711</cell></row><row><cell cols="3">IvoryWSDa 0.3354 0.1142</cell><cell>0.1579</cell><cell>0.0860</cell></row><row><cell cols="3">IvoryWSDb 0.3625 0.1309</cell><cell>0.1975</cell><cell>0.1045</cell></row><row><cell cols="3">IvoryLCEb 0.3937 0.1370</cell><cell>0.2143</cell><cell>0.1127</cell></row><row><cell>IvoryL2Rb</cell><cell cols="2">0.4000 0.1333</cell><cell>0.2255</cell><cell>0.1340</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,316.81,700.73,239.11,18.32"><head>Table 4 :</head><label>4</label><figDesc>. It is unclear if this is the result of intrinsic correlations between diversity-agnostic and diversity-Summary of diversity task results.</figDesc><table coords="5,67.78,55.49,211.14,73.42"><row><cell>Run ID</cell><cell cols="3">ERR-IA@20 α-NDCG@20 NRBP</cell></row><row><cell>IvoryBM25a</cell><cell>0.1448</cell><cell>0.2300</cell><cell>0.1049</cell></row><row><cell>IvorySDa</cell><cell>0.1459</cell><cell>0.2284</cell><cell>0.1087</cell></row><row><cell>IvoryWSDa</cell><cell>0.1716</cell><cell>0.2601</cell><cell>0.1320</cell></row><row><cell>IvoryWSDb</cell><cell>0.2214</cell><cell>0.3277</cell><cell>0.1777</cell></row><row><cell>IvoryLCEb</cell><cell>0.2201</cell><cell>0.3169</cell><cell>0.1799</cell></row><row><cell>IvoryL2Rb</cell><cell>0.2847</cell><cell>0.3934</cell><cell>0.2468</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,65.62,55.49,215.46,62.32"><head>Table 5 :</head><label>5</label><figDesc>Summary of spam filtering results.</figDesc><table coords="6,65.62,55.49,215.46,41.54"><row><cell>System</cell><cell cols="4">ham% spam% lam% 1-ROCA%</cell></row><row><cell>Baseline</cell><cell>24.32</cell><cell>18.60</cell><cell>21.36</cell><cell>13.6051</cell></row><row><cell cols="2">IVORY.70.30 29.49</cell><cell>20.93</cell><cell>25.00</cell><cell>21.52</cell></row><row><cell cols="2">IVORY.90.10 33.11</cell><cell>27.44</cell><cell>30.23</cell><cell>19.36</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="6,321.42,711.19,63.69,7.86"><p>http://ivory.cc/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">NSF</rs> under award <rs type="grantNumber">IIS-0836560</rs> and <rs type="grantNumber">IIS-0916043</rs>; <rs type="person">Google</rs> and <rs type="funder">IBM</rs>, via the <rs type="funder">Academic Cloud Computing Initiative (ACCI)</rs>. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors' and do not necessarily reflect those of the sponsors. The last author is grateful to <rs type="person">Esther and Kiri</rs> for their loving support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aXBU2f5">
					<idno type="grant-number">IIS-0836560</idno>
				</org>
				<org type="funding" xml:id="_kAGmj4Y">
					<idno type="grant-number">IIS-0916043</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.30,466.61,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,482.08,216.98,7.86;6,335.61,492.54,198.47,7.86;6,335.61,503.00,204.92,7.86;6,335.61,513.46,220.31,7.86;6,335.61,523.93,45.50,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,517.14,482.08,35.44,7.86;6,335.61,492.54,198.47,7.86;6,335.61,503.00,22.38,7.86">Learning concept importance using a weighted dependence model</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,377.01,503.00,163.52,7.86;6,335.61,513.46,116.98,7.86">Third ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,535.38,204.89,7.86;6,335.61,545.84,210.42,7.86;6,335.61,556.30,199.49,7.86;6,335.61,566.76,112.06,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,493.81,535.38,46.68,7.86;6,335.61,545.84,143.74,7.86">Learning to rank with nonsmooth cost functions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,497.82,545.84,48.20,7.86;6,335.61,556.30,199.49,7.86;6,335.61,566.76,20.92,7.86">Advances in Neural Information Processing Systems 20 (NIPS 2007)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,578.22,213.90,7.86;6,335.61,588.68,198.92,7.86;6,335.61,599.14,208.98,7.86;6,335.61,609.60,199.08,7.86;6,335.61,620.06,193.08,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,335.61,588.68,183.51,7.86">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,335.61,599.14,208.98,7.86;6,335.61,609.60,199.08,7.86;6,335.61,620.06,20.92,7.86">Proceedings of the 18th International Conference on Information and Knowledge Management (CIKM 2009)</title>
		<meeting>the 18th International Conference on Information and Knowledge Management (CIKM 2009)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,631.52,199.96,7.86;6,335.61,641.98,214.21,7.86;6,335.61,652.44,205.81,7.86;6,335.61,662.90,65.46,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,463.66,631.52,71.91,7.86;6,335.61,641.98,56.77,7.86">TREC 2005 spam track overview</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,411.09,641.98,138.72,7.86;6,335.61,652.44,143.18,7.86">Proceedings of the Fourteenth Text REtrieval Conference (TREC 2005)</title>
		<meeting>the Fourteenth Text REtrieval Conference (TREC 2005)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,674.36,215.71,7.86;6,335.61,684.82,220.31,7.86;6,335.61,695.28,196.14,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,335.61,684.82,220.31,7.86;6,335.61,695.28,74.96,7.86">Efficient and effective spam filtering and re-ranking for large Web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno>CoRR, abs/1004.5168</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,57.64,205.52,7.86;7,72.59,68.10,219.95,7.86;7,72.59,78.56,196.04,7.86;7,72.59,89.02,201.76,7.86;7,72.59,99.48,109.38,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,184.14,57.64,93.97,7.86;7,72.59,68.10,129.81,7.86">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,220.61,68.10,71.93,7.86;7,72.59,78.56,196.04,7.86;7,72.59,89.02,116.76,7.86">Proceedings of the 6th Symposium on Operating System Design and Implementation (OSDI 2004)</title>
		<meeting>the 6th Symposium on Operating System Design and Implementation (OSDI 2004)<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,110.94,176.94,7.86;7,72.59,121.40,214.36,7.86;7,72.59,131.86,187.49,7.86;7,72.59,142.32,215.15,7.86;7,72.59,152.78,101.02,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,204.35,110.94,45.18,7.86;7,72.59,121.40,214.36,7.86;7,72.59,131.86,114.46,7.86">Brute-force approaches to batch retrieval: Scalable indexing with MapReduce, or why bother?</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>HCIL-2010-23</idno>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
			<pubPlace>College Park, Maryland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="7,72.59,164.24,219.01,7.86;7,72.59,174.70,197.28,7.86;7,72.59,185.16,187.24,7.86;7,72.59,195.62,205.83,7.86;7,72.59,206.08,147.47,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,240.98,164.24,50.62,7.86;7,72.59,174.70,197.28,7.86;7,72.59,185.16,92.40,7.86">Spam, damn spam, and statistics: Using statistical analysis to locate spam Web pages</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.15,185.16,87.68,7.86;7,72.59,195.62,205.83,7.86;7,72.59,206.08,36.24,7.86">Proceedings of the 7th International Workshop on the Web and Databases (WebDB)</title>
		<meeting>the 7th International Workshop on the Web and Databases (WebDB)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,217.54,207.17,7.86;7,72.59,228.00,176.75,7.86;7,72.59,238.46,205.15,7.86;7,72.59,248.92,208.98,7.86;7,72.59,259.38,211.55,7.86;7,72.59,269.84,133.20,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,231.56,217.54,48.21,7.86;7,72.59,228.00,161.69,7.86">Dependence language model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,72.59,238.46,205.15,7.86;7,72.59,248.92,208.98,7.86;7,72.59,259.38,144.44,7.86">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004)</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004)<address><addrLine>Sheffield, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,281.30,193.01,7.86;7,72.59,291.76,219.47,7.86;7,72.59,302.22,206.99,7.86;7,72.59,312.68,203.60,7.86;7,72.59,323.14,57.26,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,72.59,291.76,154.47,7.86">Combating Web spam with TrustRank</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Gyöngyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,246.20,291.76,45.86,7.86;7,72.59,302.22,206.99,7.86;7,72.59,312.68,99.14,7.86">Proceedings of the 30th International Conference on Very Large Data Base (VLDB 2004)</title>
		<meeting>the 30th International Conference on Very Large Data Base (VLDB 2004)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="576" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,334.60,212.42,7.86;7,72.59,345.06,203.94,7.86;7,72.59,355.52,204.08,7.86;7,72.59,365.98,205.43,7.86;7,72.59,376.44,84.49,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,249.48,334.60,35.53,7.86;7,72.59,345.06,203.94,7.86;7,72.59,355.52,40.97,7.86">Larger is better: Seed selection in link-based anti-spamming algorithms</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,132.22,355.52,144.46,7.86;7,72.59,365.98,128.25,7.86">Proceeding of the 17th International Conference on World Wide Web</title>
		<meeting>eeding of the 17th International Conference on World Wide Web<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1065" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,387.90,205.95,7.86;7,72.59,398.36,210.66,7.86;7,72.59,408.82,220.31,7.86;7,72.59,419.28,97.96,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,175.69,387.90,102.85,7.86;7,72.59,398.36,57.81,7.86">Web spam detection with anti-trust rank</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,137.61,398.36,145.63,7.86;7,72.59,408.82,220.31,7.86;7,72.59,419.28,15.16,7.86">Proceedings of the 2nd International Workshop on Adversarial Information Retrieval on the Web</title>
		<meeting>the 2nd International Workshop on Adversarial Information Retrieval on the Web</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,430.74,218.60,7.86;7,72.59,441.20,215.97,7.86;7,72.59,451.66,197.87,7.86;7,72.59,462.12,152.08,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,257.15,430.74,34.04,7.86;7,72.59,441.20,215.97,7.86;7,72.59,451.66,42.20,7.86">Of Ivory and Smurfs: Loxodontan MapReduce experiments for web search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,133.27,451.66,104.04,7.86">Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,473.58,202.26,7.86;7,72.59,484.04,218.88,7.86;7,72.59,494.50,189.96,7.86;7,72.59,504.96,181.83,7.86;7,72.59,515.42,98.60,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,163.33,473.58,111.52,7.86;7,72.59,484.04,127.39,7.86">Design patterns for efficient graph algorithms in MapReduce</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,219.54,484.04,71.93,7.86;7,72.59,494.50,189.96,7.86;7,72.59,504.96,123.25,7.86">Proceedings of the Eighth Workshop on Mining and Learning with Graphs Workshop (MLG-2010)</title>
		<meeting>the Eighth Workshop on Mining and Learning with Graphs Workshop (MLG-2010)<address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,526.88,214.39,7.86;7,72.59,537.34,201.53,7.86;7,72.59,547.80,42.94,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,116.43,526.88,166.98,7.86">Learning to rank for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,72.59,537.34,197.70,7.86">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,559.26,219.89,7.86;7,72.59,569.72,198.75,7.86;7,72.59,580.18,208.98,7.86;7,72.59,590.64,199.08,7.86;7,72.59,601.10,184.98,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,121.33,559.26,171.15,7.86;7,72.59,569.72,183.68,7.86">Automatic feature selection in the Markov Random Field model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,72.59,580.18,208.98,7.86;7,72.59,590.64,199.08,7.86;7,72.59,601.10,20.92,7.86">Proceedings of the 16th International Conference on Information and Knowledge Management (CIKM 2007)</title>
		<meeting>the 16th International Conference on Information and Knowledge Management (CIKM 2007)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,612.56,212.32,7.86;7,72.59,623.02,204.98,7.86;7,72.59,633.48,220.06,7.86;7,72.59,643.94,209.22,7.86;7,72.59,654.40,214.88,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,190.77,612.56,94.14,7.86;7,72.59,623.02,114.26,7.86">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,205.64,623.02,71.93,7.86;7,72.59,633.48,220.06,7.86;7,72.59,643.94,209.22,7.86;7,72.59,654.40,54.06,7.86">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005)</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005)<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,665.86,220.32,7.86;7,72.59,676.32,209.38,7.86;7,72.59,686.78,174.58,7.86;7,72.59,697.24,179.60,7.86;7,72.59,707.70,211.55,7.86;7,335.61,57.64,143.33,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,190.15,665.86,102.76,7.86;7,72.59,676.32,133.68,7.86">Latent concept expansion using Markov random field model</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,225.30,676.32,56.66,7.86;7,72.59,686.78,174.58,7.86;7,72.59,697.24,179.60,7.86;7,72.59,707.70,144.44,7.86">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2007)</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2007)<address><addrLine>Amsterdam, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.60,69.09,214.81,7.86;7,335.61,79.55,60.37,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="7,453.73,69.09,92.65,7.86">Numerical optimization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.60,91.01,209.59,7.86;7,335.61,101.47,213.59,7.86;7,335.61,111.93,220.31,7.86;7,335.61,122.39,214.60,7.86;7,335.61,132.85,20.96,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="7,335.61,101.47,209.80,7.86">Detecting spam Web pages through content analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ntoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,347.08,111.93,208.84,7.86;7,335.61,122.39,67.49,7.86">Proceedings of the 15th International Conference on World Wide Web</title>
		<meeting>the 15th International Conference on World Wide Web<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.60,144.31,214.81,7.86;7,335.61,154.77,200.38,7.86;7,335.61,165.23,185.62,7.86;7,335.61,175.69,193.25,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="7,534.55,144.31,15.87,7.86;7,335.61,154.77,200.38,7.86;7,335.61,165.23,15.36,7.86">The PageRank citation ranking: Bringing order to the Web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<idno>SIDL-WP-1999-0120</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Stanford Digital Library Working Paper</note>
</biblStruct>

<biblStruct coords="7,335.60,187.15,203.27,7.86;7,335.61,197.61,217.03,7.86;7,335.61,208.07,219.07,7.86;7,335.61,218.53,20.96,7.86" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="7,499.60,187.15,39.28,7.86;7,335.61,197.61,217.03,7.86;7,335.61,208.07,26.18,7.86">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann Publishing</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.60,229.99,217.42,7.86;7,335.61,240.45,181.90,7.86;7,335.61,250.91,200.16,7.86;7,335.61,261.37,215.14,7.86;7,335.61,271.83,93.57,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="7,443.87,229.99,109.16,7.86;7,335.61,240.45,166.90,7.86">Extracting link spam using biased random walks from spam seed sets</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chellapilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,335.61,250.91,200.16,7.86;7,335.61,261.37,182.87,7.86">Proceedings of the 3rd International Workshop on Adversarial Information Retrieval on the Web</title>
		<meeting>the 3rd International Workshop on Adversarial Information Retrieval on the Web<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.60,283.29,220.32,7.86;7,335.61,293.75,208.69,7.86;7,335.61,304.21,217.60,7.86;7,335.61,314.67,132.56,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="7,484.17,283.29,71.75,7.86;7,335.61,293.75,132.76,7.86">Propagating trust and distrust to demote web spam</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,487.64,293.75,56.66,7.86;7,335.61,304.21,217.60,7.86;7,335.61,314.67,15.16,7.86">Proceedings of the WWW 2006 Workshop on Models of Trust for the Web</title>
		<meeting>the WWW 2006 Workshop on Models of Trust for the Web<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.60,326.13,178.32,7.86;7,335.61,336.59,214.03,7.86;7,335.61,347.05,208.98,7.86;7,335.61,357.51,214.60,7.86;7,335.61,367.97,20.96,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="7,484.51,326.13,29.41,7.86;7,335.61,336.59,197.82,7.86">Topical TrustRank: Using topicality to combat web spam</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,335.61,347.05,208.98,7.86;7,335.61,357.51,67.49,7.86">Proceedings of the 15th International Conference on World Wide Web</title>
		<meeting>the 15th International Conference on World Wide Web<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.60,379.43,159.51,7.86;7,336.76,388.94,186.89,10.13;7,335.61,401.67,218.68,7.86;7,335.61,412.13,210.19,7.86;7,335.61,422.59,202.94,7.86;7,335.61,433.05,200.75,7.86;7,335.61,443.51,161.76,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="7,335.61,401.67,218.68,7.86;7,335.61,412.13,205.99,7.86">DryadLINQ: A system for general-purpose distributed data-parallel computing using a high-level language</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Úlfar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">K</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gunda</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Currey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,347.10,422.59,191.45,7.86;7,335.61,433.05,196.56,7.86">Proceedings of the 8th Symposium on Operating System Design and Implementation (OSDI 2008)</title>
		<meeting>the 8th Symposium on Operating System Design and Implementation (OSDI 2008)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.60,454.97,213.24,7.86;7,335.61,465.43,191.89,7.86;7,335.61,475.89,196.95,7.86;7,335.61,486.35,216.21,7.86;7,335.61,496.81,125.91,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="7,387.29,465.43,140.20,7.86;7,335.61,475.89,196.95,7.86;7,335.61,486.35,23.52,7.86">A general boosting method and its application to learning ranking functions for web search</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,377.60,486.35,174.21,7.86;7,335.61,496.81,97.70,7.86">Advances in Neural Information Processing Systems 21 (NIPS 2008)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.60,508.27,220.32,7.86;7,335.61,518.73,220.32,7.86;7,335.61,529.19,213.70,7.86;7,335.61,539.65,156.84,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="7,456.90,508.27,99.02,7.86;7,335.61,518.73,78.12,7.86">A spamicity approach to web spam detection</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,432.14,518.73,123.79,7.86;7,335.61,529.19,208.96,7.86">Proceedings of the 2008 SIAM. International Conference on Data Mining (SDM&apos;08)</title>
		<meeting>the 2008 SIAM. International Conference on Data Mining (SDM&apos;08)<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="277" to="288" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
