<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.52,99.37,432.97,12.64">Top Stories Identification From Blog to News In TREC 2010 Blog Track</title>
				<funder ref="#_AQDxPMb">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.64,142.28,54.30,10.80"><forename type="first">Yu-Fan</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Cheng Kung University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.80,142.28,72.29,10.80"><forename type="first">Jing-Hau</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Cheng Kung University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.52,142.28,80.49,10.80"><forename type="first">Liang-Cheng</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Cheng Kung University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,387.84,142.28,68.28,10.80"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
							<email>hykao@mail.ncku.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Cheng Kung University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.52,99.37,432.97,12.64">Top Stories Identification From Blog to News In TREC 2010 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CA30A183EE016C278A6386FE2B6652EB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In 2010 Blog Track, there are two tasks including Faceted Blog Distillation Task and Top Stories Identification Task. We mainly focus on the Top Stories Identification Task. In this task, there are two issues to solve. The first issue is ranking the important news stories on the specified day, named Story Ranking Task. The second issue is named News Blog Post Ranking Task. News Blog Post Ranking Task is ranking the blog posts that are relevant to the news story and diversifying the topics of blog posts.</p><p>In Story Ranking Task, our team Ikm100 (NCKU_CSIE_IKMLAB) submitted three runs. In the first run, a news story is scored by its number of discussion posts. In the second run, our idea is that if the news story is discussed by more people and the supporting blog post is relatively important, the news story would be more important. In the last run, we use the "Relevant-Post Time-Entropy evaluation" to score the news story.</p><p>In News Blog Post Ranking Task, we use the cosine similarity between the news story and the blog post, and also use importance of posts to extract the supporting blog posts of the news query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In TREC 2010, Blog track contains two tasks: Faceted Blog Distillation Task and Top Story Identification Task. Our team participates in the Top Story Identification Task. In TREC 2009, the Top Story Identification Task was a pilot search task addressing the problem of using blog data to identify top news stories <ref type="bibr" coords="1,138.87,608.82,10.68,8.96" target="#b1">[2]</ref>.</p><p>In TREC 2010, the Top Story Identification Task has two stages [1]:</p><p>1. Story Ranking Task. 2. News Blog Post Ranking Task.</p><p>For the Story Ranking Task, it is different from the previous tasks in TREC2009. This task is treated as real-time event detection. We have the limitation of using information in Blog'08 data. With a given query date Q, all the information of Blog'08 we used must have the timestamp smaller than Q or equal Q. This limitation fit to mimic a real-time environment. Besides, for each query date we have to submit ranking of 100 news stories with 5 categories. The categories are "Business", "U.S.", "Sport", "SciTech" and "World".</p><p>For News Blog Post Ranking Task, the goal is to identify the top 50 relevant blog posts for each given news story with different period of time. Each ranking of blog posts should be diverse. It means that the blog posts should cover multiple aspects of the news stories. For each ranking, there are three different period of time described as follows:</p><p>1. The timestamp of blog posts should equal or smaller than the query timestamp 2. The timestamp of blog posts should equal or smaller than the query timestamp + 1 days 3. The timestamp of blog posts should equal or smaller than the query timestamp + 7 days</p><p>In this paper we first describe the data preprocessing in Section 2, then we introduce our method for the news stories ranking and the post ranking in Section 3 and Section 4, respectively. We report our performance in Section 5. Finally, the conclusion for our participation is in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Preprocessing</head><p>The dataset in the blog track provided by TREC is the Blogs08 collection. It has various blogs from different blogospheres. We can extract a lot of information from this dataset, like timestamp, post titles, post contents, and post comments, etc. The preprocessing is a laborious work to exactly extract the information we need from every different blogs.</p><p>Blogs08 dataset contains feed files, permalink files, and blog homepages, and it is crawled over a 13-month period from early 2008 to early 2009. The crawled results contain 1,303,520 feeds and 28,488,766 permalink documents. In our experiment, we only focus on all feed files because they contain main sentences of each blog post and we think it is enough for this task. For each blog post, we extract the corresponding information from feed files, including title, content and so on.</p><p>In the second preprocessing step we filter out all non-English posts and stop words and apply the stemming process. Then we index each blog post title and content.</p><p>In this year, the data of news story is the TRC2 newswire corpus. The TRC2 is a collection of 1,613,707 news stories from Thomson-Reuters. First, we filter out the news which contains the error messages like "SERVICE ALERT". Second, we remove the news stories in the document, "topNewsblacklist.docnos.txt.gz". Then we create a separated index for each news story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach for Story Ranking Task</head><p>Our three ranking methods are all based on the headline-post similarity network we build. We submitted three runs with run tags Run1 (ikm100jing), Run2 (ikm100bindog), and Run3 (ikm100ufan). In Run1, we use the Sum of Cosine Similarity Approach. In Run2, we use the Average TF-IDF Approach. And in Run3, we use the Relevant-Post Time-Entropy Evaluation Approach. After ranking by our approaches, we do duplicated detection and block list for each run tag, and then get a set of ranking results as shown in Figure <ref type="figure" coords="2,100.69,513.56,3.77,8.96" target="#fig_0">1</ref>. The first set of ranking results, NReR, will be re-rank into a new set of ranking results as named ReR. Finally, NReR and ReR should be fitted to format of the output in the last step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Headline-Post Network</head><p>For each news corpus in TRC2, first, we build the weight of each word by TF-IDF (Term Frequency-Inverse Document Frequency) technique. There are two types of TF-IDF of a word in our methods. One is to use the information of headlines to build the weight of each word and is called TF-IDF h . Another is to use the information of contents and is called TF-IDF c .</p><p>Furthermore, we consider that the information of headlines is more important than the contents. Thus, we give each word a new value of TF-IDF weight as shown in the following equation:</p><formula xml:id="formula_0" coords="2,349.93,236.84,157.31,11.12">TF-IDF new = 2 * TF-IDF h + TF-IDF c</formula><p>We also do the same TF-IDF calculating for the posts in Blogs08.</p><p>Second, for each "query date", we calculate pairwise cosine similarity between each news story and each blog post. For each query, the constructed network contains more than 100 million headline-post links. In order to filter out noises information in them, we only store the headline-post links which the similarity value is larger than 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Run1: Sum of Cosine Similarity Approach</head><formula xml:id="formula_1" coords="2,376.81,402.48,103.84,21.88">Score(h) = � sim(h, p) p∈T</formula><p>The score of each news story h is assigned as the sum of the cosine-similarity between news story h and post p that is posted during the timestamp T. In this run, we only consider the post whose cosine-similarity is larger than 0.3. The timestamp T is between the headline day d and d-1. For example, there are a news story h and three posts p1, p2 and p3. In Figure <ref type="figure" coords="2,441.24,624.20,3.77,8.96" target="#fig_1">2</ref>, the value of sim(h, p1) is 0.8 , sim(h, p2) is 0.5 , and sim(h, p3) is 0.4 . Then the news story gets the Score(h) that value is 1.7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Run2: Average TF-IDF Approach</head><p>The main idea in this run is that the news story is more important if it is discussed by more people, and a more important post has more important words. Here, we only use the posts on the query day T and only consider the post whose cosine-similarity is larger than 0.35. In next step, we calculate the average TF-IDF (post_avg_tfidf) for each blog post. The score is defined as:</p><formula xml:id="formula_2" coords="3,90.62,182.50,137.44,10.13">Score(h) = � post_avg_t�idf(p)</formula><p>p∈T * sim(h, p) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Run3: Relevant-Post Time-Entropy Evaluation Approach</head><p>In Run3, we propose a method that collects and analyzes the entropy value of the posts, called "Relevant-Post Time-Entropy Evaluation". For a query date T, we use all the posts in the range from T-5 to T. In this run, we also consider the post whose cosinesimilarity is larger than 0.35 to cut off the irrelevant posts.</p><p>In Figure <ref type="figure" coords="3,126.48,555.55,3.76,8.96" target="#fig_3">4</ref>, after extracting the publish time from dataset, the relevant posts are shown according to their published day on the time line. It means for the news story we can selected a set of relevant posts, and separate each posts by published day on the time line. We assume that before the hot story happened, this story may get higher attention and some bloggers would start to discuss the hot story in their posts. We call this case the posting-bursty behavior, as the example at 8/09 in Figure <ref type="figure" coords="3,177.45,659.02,3.76,8.96" target="#fig_3">4</ref>. We used the entropy value E to model the behavior as follows. </p><formula xml:id="formula_3" coords="3,379.68,197.80,98.09,34.80">E = -� q i * log(q i ) T i=T-5</formula><p>'E' means the entropy value of relevant posts. 'q i ' means the probability of relevant posts appearing in date i. Each news story h has a score with the ratio between the entropy value and the bursty distance D, defined as</p><formula xml:id="formula_4" coords="3,386.78,301.06,83.98,23.89">Score(h) = (1 -E) D ,</formula><p>where 'D' means the distance between the bursty date and the news date. Consider the example in Figure <ref type="figure" coords="3,532.29,347.68,3.77,8.96" target="#fig_3">4</ref>. Each black dot is a post. We find all the relevant posts with the news story which the date was from 8/06 to 8/11, and we find a bursty date 8/09. Then we get a distance D=2 between bursty date and news date in this example.</p><p>Then we calculate 'E' as follows:</p><formula xml:id="formula_5" coords="3,317.52,439.64,201.25,27.82">E = -�0 + � 2 8 � * �log 2 8 � + � 4 8 � * �log 4 8 � + � 1 8 � * �log 1 8 � + � 1 8 � * �log 1 8 � + 0�=0.53,</formula><p>and the score of h is calculated as follows:</p><formula xml:id="formula_6" coords="3,385.44,501.55,86.51,23.54">Score(h) = 1 -0.53<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Duplicated Detection and Block List</head><p>In the TRC2 corpus, we found that there are many duplicated news in one subject. That is, if a news article is partially updated, it becomes a new version of the original news. Our goal is to detect the duplicated news after any revision. Our approach is described as follows:</p><p>1.</p><p>We calculate the pair-wise similarity for each news story in query's ranking results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>For each news story h we extract the news story h' which has the higher similarity larger than 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>For each h we have a news story group that contains h and h'. Then we save the latest news from the news story group and add the other news stories into the block list B.</p><p>Repeating the third step, we have the block list B for all news stories. Finally, we remove the news stories in the block list B for each query's news story ranking results.</p><p>Then we got a set of three ranking results of news stories with different runs: Run1, Run2, Run3, and we call this set of ranking results as Non Re-ranking Results (NReR). NReR will be re-ranked by the effective terms described in Section 3.3, and we will have a new set of ranking results, called Re-ranking Results (ReR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effective Terms Re-ranking</head><p>In this section, we are curious about whether some effective terms in the past hot news will have a positive influence for news stories ranking in the future or not. We build monthly effective terms list M i for some month ID i, where i is the integer from 1 to 13. For example, M 3 present the effective terms list for March, 2008 and M 13 specially presents for January, 2009.</p><p>First, we do not consider the news stories which do not have any word that has its IDF value larger than 1.5. We then select top 40 news stories in each run of NReR for each query.</p><p>Second, in order to build M i , we select effective terms from the top 40 news stories of the queries in its previous month. For instance, the effective terms in M 5 is selected from the top 40 news stories from the queries in April, 2008. The queries in April from the dataset are 2008-4-2, 2008-4-19 and 2008-4-23. Besides, in order to build M 1 , we add two queries of 2008-1-1 and 2008-1-2. We then choose effective terms from extracted news. Each term is selected when we thought it can be an important term in news stories. Each member is assigned 57 news of each month in average.</p><p>Third, for each query in i-th month, the news stories will be rescoring by the equation in the following equation.</p><p>Score ′ (h) = � Score(h) * e TF-IDF(w,h) w∈M i Score(h) means the original score of the news story h. Score'(h) means h is enhanced by those effective terms by the formula illustrated for each w belongs to M i . The TF-IDF(w, h) means the TF-IDF value of the term w in the TRC2 news story h. If the w does not exist in h, then the value of TF-IDF(w, h) will be zero.</p><p>After the rescoring, we can re-rank the news stories and got a new set of re-ranking results (ReR). The reranking flow chart is also shown in Figure <ref type="figure" coords="4,489.12,120.17,3.76,8.96" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">News stories Classification</head><p>Blog task of this year is different from tasks in last year. The difference is that we have to judge the category of each extracted news story. For a given time query, we have to return the top 100 news stories ranking for each category. Unfortunately, the TRC2 dataset do not contains category information of news. Thus, we use the tool, "Libsvm", to build a classifier to classify our news.</p><p>First, we crawled the news stories during the period from 2008/04 to 2008/06 for each 5 category as training set. There have total 1037 news stories for each category. Then we construct the inverted index of the training set, and we use the vector of TF-IDF values to represent each news story. We train a model according to the training set, and then use this model to predict the category of each news story in TRC2 corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Format of Output File for Story Ranking Task</head><p>At the last step (D) shown in Figure <ref type="figure" coords="4,483.70,411.16,3.76,8.96" target="#fig_0">1</ref>, we have to fit the format of output files. For each numbered "query date", we select top 100 news stories ranking for each 5 categories. We already have the category of each news story from Section 3.4. Format of output file is like the sample [1] in Figure <ref type="figure" coords="4,442.80,468.66,3.76,8.96" target="#fig_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach for News Blog Post Ranking Task</head><p>In this stage, we use the cosine similarity value between the headline and the post, and also use the count of post's comment to score the posts.</p><formula xml:id="formula_7" coords="5,78.24,584.64,209.85,9.63">Score(p) = (α)�sim(h, p)� + (1 -α)(Norm_Comment)</formula><p>sim(h, p) means the value of cosine similarity between the news story h and the blog's post p. "Norm_Comment" means the normalized value of count of post's comment. Term α means the weight of the cosine similarity comparing with count of post's comment.</p><p>For each news story of query, we selected relevant posts with their cosine similarity larger than 0.35.</p><p>Before we score and rank the relevant post. We classify posts to 5 categories ("Business", "U.S.", "Sport", "SciTech" and "World"). After post ranking, we select the post which only has the same category with the news story to submit our results. For example, if the query of the news story belongs to business category, then we only rank those posts which belong to the business category. We submitted 3 runs with three different α in this stage. We set α = 0.25, 0.5, and 0.75 for each run respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results of Runs</head><p>Our team IKM100 submitted 3 runs named Run1 (ikm100jing), Run2 (ikm100bindog) and Run3 (ikm100ufan) in Stories Ranking Task. All runs are automatically generated and ranked. The performance  <ref type="table" coords="6,242.87,120.65,3.76,8.96" target="#tab_1">2</ref>. We select the evaluation of alpha-nDCG@10, P-IA@10 and nERR-IA@10 to show our performance as shown in Table <ref type="table" coords="6,97.30,156.18,3.76,8.96" target="#tab_1">2</ref>.</p><p>TREC organizers have provided test programs of evaluation on TREC website. We have download the program files of blog story ranking task and implement the evaluation process on Ubuntu OS. We also implement some extra evaluation. The comparison of statMAP between Standard-NReR and Standard-ReR is shown in Table <ref type="table" coords="6,145.43,236.72,3.76,8.96" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In TREC 2010, we focus on the Story Ranking of the Top Story Identification Task. We propose three general methods based on the headline-post network. In this network, we identify each cosine similarity between post and news story. For the category classification, our team crawl the news from Reuters website and use the "Libsvm" to classify news stories in TRC2 dataset. Additionally, we found something interesting that the effective term list we retrieved is not as useful as we expected. The performance of Standard-NReR outperformed Standard-ReR according to statMAP. This means that the influence of term list will shift with time and a better approach should be developed to find the useful effective terms.</p><p>In the News Blog Post Ranking, our method does not use diversity features to rank the posts. We use the blogger's attention feature, i.e. the number of post's comments. This feature helps us to identify popular post for the news story. We also use cosine similarity to judge the relevance between post and news story.</p><p>In the future work, we focus on exploring links between blogs and applying other suitable models to the Top Story Identification Task. The work will try to quantify blogger's attention to the top stories and see how we can use more characteristics of the blogosphere.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,103.22,683.86,159.90,8.96;2,73.50,568.76,222.35,101.39"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1 : Flow chart of NReR and ReR</figDesc><graphic coords="2,73.50,568.76,222.35,101.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,346.33,589.62,164.57,8.96;2,384.00,515.41,94.03,71.79"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2 : Example for approach in Run1</figDesc><graphic coords="2,384.00,515.41,94.03,71.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,100.80,307.59,164.57,8.96;3,84.24,330.64,210.17,8.96;3,72.01,342.05,224.85,8.96;3,72.02,353.57,222.37,8.96;3,72.01,365.09,222.58,8.96;3,72.01,376.62,222.40,8.96;3,72.00,388.14,32.64,8.96;3,97.00,215.06,163.77,90.30"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3 : Example for approach in Run2 Consider the example in Figure 3. The news story h has two associated posts p1 and p2. The value of sim(h, p1) is 0.8 and sim(h, p2) is 0.5. The post p1 contains Word1 and Word2. The post p2 contains Word3, Word4 and Word5. The score of h is then calculated as follows:</figDesc><graphic coords="3,97.00,215.06,163.77,90.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,328.53,164.20,200.47,9.05;3,407.03,175.72,43.26,8.96;3,319.56,71.04,217.92,91.92"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4 : Example of Relevant-Post Time-Entropy Evaluation</figDesc><graphic coords="3,319.56,71.04,217.92,91.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,366.24,544.15,125.01,8.96;4,329.76,567.19,210.18,8.96;4,317.50,578.60,222.38,8.96;4,317.48,590.12,177.33,8.96;4,319.56,488.64,231.12,53.76"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5 : Format of output file Finally, we have two set of final results, Standard-NReR and Standard-ReR. Each set of final results contains 3 runs for our different approaches.</figDesc><graphic coords="4,319.56,488.64,231.12,53.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,100.48,466.04,370.16"><head>Table 1 :</head><label>1</label><figDesc>Performance of submitted runs for Story Ranking Task</figDesc><table coords="5,72.00,117.99,466.04,352.65"><row><cell>Group</cell><cell>Runs</cell><cell>TRC2-Fields</cell><cell>Mean StatMAP</cell><cell>Business</cell><cell>Sci-Tech</cell><cell cols="2">StatMAP by Category Sport</cell><cell>U.S.</cell><cell>World</cell></row><row><cell>POSTECH_KLE</cell><cell>KLERUN1</cell><cell>HC</cell><cell>0.2206</cell><cell>0.1851</cell><cell>0.1821</cell><cell>0.1916</cell><cell cols="3">0.2458</cell><cell>0.2986</cell></row><row><cell>ICTNET</cell><cell>ICTNETTSRun2</cell><cell>HC</cell><cell>0.2138</cell><cell>0.0969</cell><cell>0.1898</cell><cell>0.2405</cell><cell cols="3">0.3025</cell><cell>0.2396</cell></row><row><cell></cell><cell>Run1</cell><cell>HC</cell><cell>0.2151</cell><cell>0.1141</cell><cell>0.2483</cell><cell>0.1725</cell><cell cols="3">0.3897</cell><cell>0.1504</cell></row><row><cell>ikm100</cell><cell>Run2</cell><cell>HC</cell><cell>0.2107</cell><cell>0.1146</cell><cell>0.2390</cell><cell>0.1451</cell><cell cols="3">0.3870</cell><cell>0.1715</cell></row><row><cell></cell><cell>Run3</cell><cell>HC</cell><cell>0.2043</cell><cell>0.0823</cell><cell>0.2425</cell><cell>0.1699</cell><cell cols="3">0.3827</cell><cell>0.1441</cell></row><row><cell>Runs</cell><cell></cell><cell cols="3">Alpha-nDCG@10</cell><cell cols="2">P-IA@10</cell><cell></cell><cell cols="2">nERR-IA@10</cell></row><row><cell>Run1,α = 0.25</cell><cell></cell><cell></cell><cell>0.3335</cell><cell></cell><cell cols="2">0.1049</cell><cell></cell><cell></cell><cell>0.2907</cell></row><row><cell>Run2,α = 0.5</cell><cell></cell><cell></cell><cell>0.3750</cell><cell></cell><cell cols="2">0.1211</cell><cell></cell><cell></cell><cell>0.3332</cell></row><row><cell>Run3,α = 0.75</cell><cell></cell><cell></cell><cell>0.4075</cell><cell></cell><cell cols="2">0.1309</cell><cell></cell><cell></cell><cell>0.3720</cell></row><row><cell></cell><cell>Runs</cell><cell cols="2">Mean StatMAP</cell><cell>Business</cell><cell cols="4">StatMAP of each category Sci-Tech Sport</cell><cell>U.S.</cell><cell>World</cell></row><row><cell>Standard-NReR</cell><cell>Run1</cell><cell cols="2">0.2339</cell><cell>0.1200</cell><cell>0.2809</cell><cell>0.1716</cell><cell></cell><cell cols="2">0.4662</cell><cell>0.1307</cell></row><row><cell></cell><cell>Run2</cell><cell cols="2">0.2243</cell><cell>0.1191</cell><cell>0.2725</cell><cell>0.1364</cell><cell></cell><cell cols="2">0.4393</cell><cell>0.1543</cell></row><row><cell></cell><cell>Run3</cell><cell cols="2">0.2139</cell><cell>0.0874</cell><cell>0.2732</cell><cell>0.1704</cell><cell></cell><cell cols="2">0.4192</cell><cell>0.1201</cell></row><row><cell>Standard-ReR</cell><cell>Run1</cell><cell cols="2">0.2151</cell><cell>0.1141</cell><cell>0.2483</cell><cell>0.1725</cell><cell></cell><cell cols="2">0.3897</cell><cell>0.1504</cell></row><row><cell></cell><cell>Run2</cell><cell cols="2">0.2107</cell><cell>0.1146</cell><cell>0.2390</cell><cell>0.1451</cell><cell></cell><cell cols="2">0.3870</cell><cell>0.1715</cell></row><row><cell></cell><cell>Run3</cell><cell cols="2">0.2043</cell><cell>0.0823</cell><cell>0.2425</cell><cell>0.1699</cell><cell></cell><cell cols="2">0.3827</cell><cell>0.1441</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,176.16,229.36,279.18,8.96"><head>Table 2 :</head><label>2</label><figDesc>Performance of submitted runs for News Blog Post Ranking</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,128.04,328.72,426.64,8.96"><head>Table 3 :</head><label>3</label><figDesc>The comparison of statMAP between Standard-NReR and Standard-ReR for Story Ranking Task of our runs and runs of other participants in Story Ranking Task is shown in Table1.In News Blog Post Ranking Task, our team submitted 3 runs named 'Run1', 'Run2' and 'Run3' with different α values shown in Table</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgement</head><p>We would like to appreciate the <rs type="grantNumber">TRC2</rs> newswire corpus from <rs type="person">Thomson-Reuters</rs>, and the technological guidance of using evaluation program from the organizers in blog track.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AQDxPMb">
					<idno type="grant-number">TRC2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,76.50,678.92,63.29,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,321.28,97.24,3.76,8.96;6,353.51,97.24,186.59,8.96;6,353.51,108.65,177.57,8.96" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<title level="m" coord="6,446.87,97.24,93.24,8.96;6,353.51,108.65,82.23,8.96">Blog track research at TREC. SIGIR Forum</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="58" to="75" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
