<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.59,71.87,292.54,16.84">MMCI at the TREC 2010 Web Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.38,116.90,97.31,11.06"><forename type="first">Andreas</forename><surname>Broschart</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,319.50,116.90,73.51,11.06"><forename type="first">Ralf</forename><surname>Schenkel</surname></persName>
							<email>schenkel@mmci.uni-saarland.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.59,71.87,292.54,16.84">MMCI at the TREC 2010 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9068087DE9A2601470C3C631B299BCEC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Term proximity scoring models incorporate distance information of query term occurrences and are an established means in information retrieval to improve retrieval quality. The integration of such proximity scoring models into efficient query processing, however, has not been equally well studied. Existing methods make use of precomputed lists of documents where tuples of terms, usually pairs, occur together, usually incurring a huge index size compared to term-only indexes. This paper uses a joint framework for trading off index size and result quality. The framework provides optimization techniques for tuning precomputed indexes towards either maximal result quality or maximal query processing performance under controlled result quality, given an upper bound for the index size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The MMCI group at Saarland University has participated in the 2010 Web Track Adhoc task. Our submission uses a joint framework described in <ref type="bibr" coords="1,120.76,397.47,10.45,7.77" target="#b1">[1]</ref> to tune index parameters to be used for later index construction. We impose an index size limit for the tuned index and keep an eye on the result quality at the same time. For our experiments in the TREC Web Track, we used the topics from the AdHoc task 2009 for training and optimizing index parameters. We considered only the 50% least spammy English ClueWeb09 documents from Category A according to the Waterloo Fusion spam scores (approximately 6 TB uncompressed size).</p><p>The remainder of the paper is structured as follows: Section 2 describes the score model our approach is based on, the employed index structures, and the query processing approach. Section 3 contains a detailed description of the runs we have submitted to the 2010 Web Track Adhoc task. We comment on some results and conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM 2.1 Score Model</head><p>The proximity score model we use is based on <ref type="bibr" coords="1,230.56,590.97,9.52,7.77" target="#b2">[2]</ref>, the modification was initially described in <ref type="bibr" coords="1,161.91,601.43,9.52,7.77" target="#b4">[4]</ref>. For a document d from a collection C of documents, we denote by the term frequency tf d (t) the number of times term t occurs in d, and the length l d = P tf d (t) of document d is the sum of the term frequencies of all terms it contains. We use the established BM25 content score <ref type="bibr" coords="1,247.70,643.27,9.52,7.77" target="#b3">[3]</ref>, which is computed for a query q = {t1, . . . , tn} of terms as</p><formula xml:id="formula_0" coords="1,57.38,669.29,231.94,23.58">score BM25 (d, q) = X t∈q tf d (t) • (k1 + 1) tf d (t) + k1 • (1 -b + b l d avgdl ) • idf (t)</formula><p>where k1 and b are tunable parameters, avgdl is the average length of all documents in the collection, and idf (t) is the inverse docu-ment frequency of t in the collection: Denoting by df (t) the number of documents in which t occurs and by N the number of documents in C, idf (t) is defined as</p><formula xml:id="formula_1" coords="1,392.58,246.31,68.44,19.74">idf (t) = log N df (t)</formula><p>We denote by pi(d) the term at position i of d, omitting d when the document is clear from the context. For a term t, P d (t) denotes the positions in d where t occurs. For a query q = {t1, . . . , tn}, P d (q) := ∪t i ∈q P d (ti) denotes the positions of those terms in d, and</p><formula xml:id="formula_2" coords="1,328.51,332.38,197.77,8.35">Q d (q) := {(i, j) ∈ P d (q) × P d (q) | i &lt; j ∧ pi = pj}</formula><p>denotes the position pairs of distinct terms from q in d. The score for a query q = {t1, . . . , tn} is then a linear combination of a standard BM25 content score and a BM25-style proximity score where term frequencies are replaced by per-term accumulators acc :</p><formula xml:id="formula_3" coords="1,316.81,396.61,243.13,36.96">score Büttcher (d, q) = score BM25 (d, q) + X t∈q min{1, idf (t)} acc d (t) • (k1 + 1) acc d (t) + 1</formula><p>Here, the accumulator for term t k ∈ q is defined as</p><formula xml:id="formula_4" coords="1,355.92,456.91,159.20,55.13">acc d (t k ) = X (i,j)∈Q d (q):p i =t k idf (pj) (i -j) 2 + X (i,j)∈Q d (q):p j =t k idf (pi) (i -j) 2</formula><p>This score shows two major differences from the original score developed by <ref type="bibr" coords="1,367.39,530.55,53.69,7.77">Büttcher et al.:</ref> (1) it does not include the document length in the proximity score, and (2) accumulators combine not only adjacent query term occurrences. It has been shown in <ref type="bibr" coords="1,545.46,551.48,10.45,7.77" target="#b4">[4]</ref> that these modifications do not have an impact on result quality, but allow for efficient precomputation and indexing. A simple reformulation of the definition of acc d (t k ) yields</p><formula xml:id="formula_5" coords="1,332.49,597.55,206.21,83.75">acc d (t k ) = X t∈q idf (t) • X (i, j) ∈ Q d (q) : (pi = t k , pj = t) ∨(pi = t, pj = t k ) 1 (i -j) 2 | {z } :=acc d (t k ,t) = X t∈q idf (t) • acc d (t k , t)</formula><p>Now acc d (t k ) is represented as a monotonous combination of perpair scores acc d (t k , t), which can be precomputed for all possible term pairs and stored in an inverted index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Description of the Index Structures</head><p>As described in <ref type="bibr" coords="2,120.79,70.87,10.45,7.77" target="#b1">[1]</ref> we use two different index structures:</p><p>• text index lists (short: TLs): each list stores, for a single stemmed term t and for each document d where this term occurs, an entry of the form (d.docid, score BM25 (d, t)) where d.docid is a unique numerical id for document d. The entries are ordered by descending score BM25 (i.e., BM25 score with b = 0.5 and k1 = 1.2).</p><p>• combined index lists (short: CLs): each list contains, for a single stemmed term pair (t1, t2) and for each document where this term pair occurs within a text window of 10 terms in the document, an entry of the form (d.docid,</p><formula xml:id="formula_6" coords="2,76.21,189.54,216.69,21.51">acc d (t1, t2), score BM25 (d, t1), score BM25 (d,<label>t2</label></formula><p>)). The resulting proximity contribution of (t1, t2) for d is stored in acc d (t1, t2).</p><p>Each index list is ordered by descending acc contribution.</p><p>The tuning process in this submission aims at finding appropriate tuning parameters for the later index construction such that index entries with the least impact in both TLs and CLs are not materialized.</p><p>We tune two parameters:</p><p>• the minimal score cutoff: We keep only those combined index list entries whose acc-score is not below a certain lower limit m.</p><p>• the list length cutoff: We keep at most the l entries from each text and combined index list that have the highest scores (score BM25 for TL, acc for CL).</p><p>The index tuning process results in optimal choices for m and l which are named m and l. More details are given in Section 5 of <ref type="bibr" coords="2,64.07,396.35,9.52,7.77" target="#b1">[1]</ref>. The resulting tuned indexes lead to efficient processing of queries using an n-ary merge join, still providing at least BM25 result quality and controlling the index space requirements. All indexes are compressed using delta-and v-byte encoding. Details about the different tuning approaches and the compression can be found in <ref type="bibr" coords="2,86.18,448.66,9.52,7.77" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Processing</head><p>We use an n-way merge join with pruned index lists to get rid of the overhead costs dynamic pruning algorithms incur (cp. Section 6 in <ref type="bibr" coords="2,70.50,500.96,9.41,7.77" target="#b1">[1]</ref>). Our merge-based processing architecture consists of the following components:</p><p>1. After pruning index lists to a fixed maximal number of entries (and, possibly, using a minimal score cutoff for combined lists), we resort each list in ascending order of document ids, and optionally compress it.</p><p>2. At query time, the n text and combined lists for the query are combined using an n-way merge join that combines entries for the same document and computes its score. If that score is higher than the current k th best score, the document is kept in a heap of candidate results, otherwise it is dropped.</p><p>3. Once all index entries have been read, the content of the heap is returned.</p><p>Instead of maintaining a heap with the currently best k results, an even simpler implementation could keep all results as result candidates and sort them at the end; however, this would increase the memory footprint of the execution as not k, but all encountered documents and their scores need to be stored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DESCRIPTION OF THE RUNS</head><p>For the 2010 Web Track Adhoc task we submitted the following three runs:</p><p>• MMCITLCLl20M: This run uses TLs and CLs pruned to the first 20 million entries (l = 20, 000, 000 and m = 0.00) as input to an NRA (a dynamic pruning algorithm) implementation. The input lists consist of TLs for all stemmed query terms and CLs for all pairs of stemmed query terms. Stopwords are removed. We exclude the 50% spammiest documents according to the Waterloo Fusion spam score. The judging precedence was set to medium. We had to limit the index lists to the first 20 million entries as building the unpruned CLs exceeded our disk capacity.</p><p>• MMCITLl20M: This run uses TLs pruned to the first 20 million entries (l = 20, 000, 000) as input to an NRA implementation. The input lists consist of TLs for all stemmed query terms. Stopwords are removed. We exclude the 50% spammiest documents according to the Waterloo Fusion spam score. The judging precedence was set to least important. To be able to compare to MMCITLCLl20M, we decided to limit the index lists to the first 20 million entries for the TLs as well.</p><p>• MMCIl410m1: This run aims at providing the retrieval quality of BM25 scores (i.e., similar early precision quality) for k = 10 result documents and at the same time efficient query processing, still considering the index size. The indexes are tuned for efficient evaluation but still aim at providing BM25 result quality with a maximum index size of 1 TB for the test topics from the 2010 Web Track Adhoc task. We have used the efficiency-oriented absolute index quality tuning approach as described in <ref type="bibr" coords="2,420.87,400.07,10.45,7.77" target="#b1">[1]</ref> to come up with appropriate index tuning parameters. A list length of l = 410 with a minimum score requirement of m = 1.00 for the proximity contribution acc turned out to be optimal. To allow for efficient query processing, this run uses pruned TL and CL indexes that are reordered by docid then. The pruned indexes are the input to a merge join implementation. The input lists consist of pruned and reordered TLs for all stemmed query terms as well as pruned and reordered CLs for all pairs of stemmed query terms. Stopwords are removed. We exclude the 50% spammiest documents according to the Waterloo Fusion spam score. Judging precedence was set to most important.</p><p>For the tuning process that produced the index parameters for MMCIl410m1, we used the 50 Web Track 2009 topics as training topics and compared the respective precision values for the top-10 results at many pruning levels (i.e., many (l, m) combinations) with the precision for the top-10 results of the MMCITLl20M evaluation. Intuitively, the tuning process selects the smallest list length that can still achieve at least BM25 quality for the training topics within the index space constraint of 1 TB. If there is more than one solution for that list length, we select the one with the highest m to minimize the required index space.</p><p>As MMCIl410m1 aims at providing the retrieval quality of BM25 and MMCITLl20M uses (almost) complete (i.e., non-pruned) BM25 score lists, MMCITLl20M is the baseline run for MMCIl410m1's retrieval quality. MMCITLCLl20M is a baseline run for the effectiveness-oriented index tuning approaches. Due to the limitation of submittable runs, we can only provide runtimes for some of the runs.</p><p>size <ref type="bibr" coords="3,238.11,56.08,12.59,6.05">[GB]</ref> prec@k reads•10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND CONCLUSION</head><p>Table <ref type="table" coords="3,84.80,170.24,4.48,7.77" target="#tab_0">1</ref> depicts our absolute index quality tuning results with an index size limit of 1 TB. 1 As described earlier, for our experiments in the Web Track Adhoc Task 2010, we used the topics from the Web Track AdHoc task 2009 as training topics and their relevance assessments to optimize the index parameters. We considered only the 50% least spammy English ClueWeb09 documents from Category A according to the Waterloo Fusion spam scores (approximately 6 TB uncompressed size).</p><p>We provide processing times of a single-threaded, Java-based implementation running on a single cluster node. These measurements were taken by running the complete batch of queries five times and taking the average. When we measure cold cache runtimes, we empty the filesystem cache before the evaluation of each query (not every query batch) which is a very conservative setting.</p><p>MMCITLl20M, the baseline run for indexes produced by efficiency-oriented tuning approaches, generates a precision@10 of 0.180, and a precision@100 of 0.1110 for the training topics. For the test topics, it produces a precision@10 of 0.2250 and a precision@100 of 0.1294.</p><p>With index parameters tuned for efficiency and top-10 document retrieval, the most efficient resulting index ( l, m) = (410, 1.00) (used in run MMCIl410m1) needs less than 500 GB of disk space. For the training topics, at a precision@10 of 0.190, it provides a result quality comparable to indexes that use pure BM25 scores (which generate a precision@10 of 0.180). Query processing with the index pruned at (l, m) = (410, 1.00) requires on average less than 1 ms for warm caches and about 80 ms for cold caches to process one training topic.</p><p>Due to shorter index lists for the topics from the Adhoc task 2010 which serve as test topics, query processing is even a bit faster than for the training topics, showing more performant warm cache times and cold cache times of about 70 ms. This is reflected in the average number of accessed entries per query: while for the training topics we need on average 1,302 reads per query, we only need 1,045 reads for the test topics. The precision@10 of 0.2292 for the tuned index MMCIl410m1 on the test topics slightly exceeds the precision@10 of 0.2250 for MMCITLl20M. The most efficient resulting index for top-100 document retrieval, ( l, m) = (400, 1.00) at a precision@100 of 0.1170 also meets the precision requirements for the training topics (MMCITLl20M has a precision@100 of 0.1110).</p><p>MMCITLCLl20M, the baseline run for indexes produced by effectiveness-oriented tuning approaches, generates a precision@10 of 0.198, and a precision@100 of 0.1324 for the training topics. For the test topics, it produces a precision@10 of 0.2250 and a precision@100 of 0.1358.</p><p>Our effectiveness-oriented index for top-10 document retrieval with ( l, m) = (1410, 1.00) aims at providing precision values which are comparable to MMCITLCLl20M. It requires 640 GB disk 1 Please note that the retrieval quality measured in this paper is based on relevance assessments for 48 topics while the evaluation in <ref type="bibr" coords="3,65.19,702.51,10.45,7.77" target="#b1">[1]</ref> was based on the older relevance assessments that only contained 36 topics, hence the different precision values.</p><p>space and provides a precision@10 value of 0.200. This precision is comparable to the precision for the (almost) non-pruned proximity score run MMCITLCLl20M that has a precision@10 value of 0.198. Query execution takes about 2 ms for the training topics and slightly more than 1 ms for the test topics for warm caches.</p><p>The precision@10 value of 0.2250 for MMCITLCLl20M is the same as for MMCITLl20M; we would have expected higher precision values for MMCITLCLl20M like for our previous experiments on the GOV2 collection with the TREC 2004 to 2006 Terabyte Track (detailed results are provided in <ref type="bibr" coords="3,458.15,251.44,9.41,7.77" target="#b1">[1]</ref>). This may be partially due to the sparser relevance assessments for the Web Track topics. As to the precision@100 values MMCITLCLl20M performs better (at 0.1358) than MMCITLl20M (at 0.1294) which meets our expectations.</p><p>Our effectiveness-oriented index for top-100 document retrieval with ( l, m) = (4600, 0.30) has longer index lists than for k=10 which leads to a slightly slower query processing; despite the longer lists, it still fits the 1 TB space limit.</p><p>In <ref type="bibr" coords="3,336.08,345.58,10.45,7.77" target="#b1">[1]</ref> we also describe experiments with the GOV2 collection: Although the indexed part of ClueWeb09 is one order of magnitude larger than GOV2 (6 TB vs. 426 GB uncompressed size), the required index space does not grow as fast as the collection size. For the efficiency-oriented index quality setting, the parameter tuning for k=10 on ClueWeb09 results in ( l, m) = (410, 1.00) whereas for GOV2 it results in ( l, m) = (310, 0.05). The final index size is 500 GB for ClueWeb09 and 95 GB for GOV2. Indexes tuned for the ClueWeb09 collection often have shorter list lengths which provides even faster query processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,78.85,54.21,452.02,71.91"><head>Table 1 :</head><label>1</label><figDesc>Index tuning for absolute index quality</figDesc><table coords="3,364.07,54.21,164.38,8.43"><row><cell>-5</cell><cell>bytes•10 -5</cell><cell>twarm[ms]</cell><cell>t cold [ms]</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,321.30,461.23,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,331.75,476.79,208.22,7.77;3,331.75,487.10,174.33,7.93;3,331.75,497.56,136.31,7.93;3,331.75,509.02,180.27,6.31" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="3,442.73,476.79,97.24,7.77;3,331.75,487.25,89.19,7.77">Real-time text queries with tunable term pair indexes</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Broschart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<idno>MPI-I-2010-5-006</idno>
		<ptr target="http://www.mpi-inf.mpg.de/reports" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">MPI Technical Report</note>
</biblStruct>

<biblStruct coords="3,331.75,519.63,224.07,7.77;3,331.75,530.09,215.26,7.77;3,331.75,540.40,105.59,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,499.41,519.63,56.41,7.77;3,331.75,530.09,202.12,7.77">Term proximity scoring for ad-hoc retrieval on very large text collections</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,331.75,540.40,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="621" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,331.75,552.01,219.68,7.77;3,331.75,562.32,211.01,7.93;3,331.75,572.78,155.77,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,454.27,552.01,97.16,7.77;3,331.75,562.47,109.07,7.77">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,447.14,562.32,95.63,7.72;3,331.75,572.78,76.56,7.72">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,331.75,584.39,219.13,7.77;3,331.75,594.70,217.57,7.93;3,331.75,605.31,56.04,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,377.35,594.85,107.62,7.77">Efficient text proximity search</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Broschart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,500.51,594.70,21.79,7.72">SPIRE</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="287" to="299" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
