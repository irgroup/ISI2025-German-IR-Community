<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.88,142.90,342.11,15.49">The Role of Anchor Text in ClueWeb09 Retrieval</title>
				<funder>
					<orgName type="full">Australian Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,229.80,176.73,42.03,10.76"><forename type="first">Vo</forename><surname>Ngoc</surname></persName>
						</author>
						<author>
							<persName coords="1,274.68,176.73,20.62,10.76;1,322.20,176.73,71.72,10.76"><forename type="first">Anh</forename><forename type="middle">Alistair</forename><surname>Moffat</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.88,142.90,342.11,15.49">The Role of Anchor Text in ClueWeb09 Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B83D1105A144126750B441FE8B776045</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We found that the impact-based retrieval model works well for the corpus, and that, along with some other factors, the use of an anchor text collection significantly boosts the retrieval effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This report describes work done at the University of Melbourne for TREC-2010. In addition, it also summarizes our TREC-2009 participation as that was not reported elsewhere. For these two years, we participated in the Ad-hoc and Diversity Tasks of the Web Track, and for 2010 alone we also submitted to the Session Track. In all of these, we employed the whole English portion of the ClueWeb09 corpus and hence participated in, by TREC definition, Category A.</p><p>Experiments were performed using our locally-developed software. The system has been developed for several years at the University of Melbourne, targeting both efficiency and effectiveness of ad-hoc retrieval. The system engages an impact-based retrieval model, impact-sorted indexes, and fast index compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Models</head><p>As a common point for all of our submissions in TREC 2009-2010, impact-based retrieval methods were employed. However, the 2010 submissions are distinguished by the employment of of spam filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Similarity Computation</head><p>For a document collection and a query, the similarity between the query and a collection document is computed using either IMP -the impact model <ref type="bibr" coords="1,318.60,645.09,105.27,9.82" target="#b0">[Anh and Moffat, 2005]</ref> or IBM25 -its BM25style formulation <ref type="bibr" coords="1,176.76,658.65,75.11,9.82" target="#b2">[Anh et al., 2008]</ref>.</p><p>In short, IMP and IBM25 can be considered as variations of the conventional vector-space model (see, for example, <ref type="bibr" coords="1,180.12,685.77,58.52,9.82" target="#b5">Salton [1989]</ref>) and the BM25 formulation <ref type="bibr" coords="1,361.08,685.77,101.27,9.82" target="#b4">[Robertson et al., 1994]</ref>, respectively. However, unlike their original counterparts, they operate over term impacts instead of term frequencies, and do not depend on any tuning parameter. At indexing time each document d is examined as an individual document, in isolation from any collection-wide statistics, and each distinct term t in the document in this process is associated with a document term impact value ω d,t . This impact value is an integral value between 1 and 8 inclusive, and can be regarded as a normalized value of the frequency of t in d. This process does not use any knowledge from outside of the document. The impact values are then stored in the index.</p><p>At query time, the query q is also considered as an independent document, but for each distinct term t ∈ q, the collection frequency of t in the collection is employed (instead of the frequency of t in q), and a similar process of normalization as in the case of documents is used to define query term impact ω q,t .</p><p>Finally, the similarity S d,q between q and a document d is computed using one of the two methods. The first method, IMP, is the original impact model, which specifies that</p><formula xml:id="formula_0" coords="2,249.60,222.32,104.54,23.05">S d,q = t∈d∩q ω d,t • ω q,t .</formula><p>(1)</p><p>The second method, IBM25, is a slightly modification of what described by <ref type="bibr" coords="2,446.40,257.37,73.22,9.82" target="#b2">Anh et al. [2008]</ref>. Its exact formulation is</p><formula xml:id="formula_1" coords="2,139.20,289.40,385.11,30.73">S d,q = t∈d∩q log N -f t + 0.5 f t + 0.5 • log(1 + ω d,t ) k 1 + log(1 + ω d,t ) • log(1 + ω q,t ) k 3 + log(1 + ω q,t ) ,<label>(2)</label></formula><p>where k 1 = 2 and k 3 = 1000, and are constant across document collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spam Filtering</head><p>Cormack et al. <ref type="bibr" coords="2,168.84,382.89,28.83,9.82">[2010]</ref> report that the use of spam filtering or re-ranking significantly improves retrieval effectiveness for most of the systems that participated in Web Track 2009. For our 2010 submissions, we employed their fusion spam score to remove the spammiest 30% pages from each retrieved list. We do not, however, evaluate the effect of spam filtering on our 2010 submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anchor Text and Links</head><p>For all of our submissions, the original ClueWeb09 was indexed and employed for retrieval. In addition, for the majority of submissions, we also created and indexed the incoming anchor text collection. For that purpose, the canonical anchor text (that is, not including any surrounding text) of each link in document was extracted and gathered if the destination document was also a document in ClueWeb09. The resultant anchor text collection was separately indexed and queried in the same way as the original ClueWeb09 collection.</p><p>For TREC-2009, our submissions were based on either the content-only or the anchor-only collections. For TREC-2010 we also employed the fusion between content-only scores, anchoronly scores, and, in some cases, PageRank scores. When fusion was used, the individual scores were normalized so that to share to maximal score of 1 on the per-query basic, and then normalized scores are weighted and then combined. Typically, the weight of a non-content score is 0.25. Our post-TREC experiments show that the contribution of the PageRank on retrieval effectiveness was marginal. While the problem seems interesting, we are not making any investigation in this report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Web Track 2009</head><p>There are two tasks in Web Track 2009 -Ad-hoc and Diversity. While the former is a conventional TREC task, the latter aims, for each query, to get a ranked list of pages that provide broad coverage of the query and avoid excessive redundancy within the list. In our submissions, we focused on the Ad-hoc Task and applied the same retrieval techniques to both of the tasks.</p><p>We assumed that in a large dataset like ClueWeb09, good documents would possess a nontrivial number of incoming links, and a reasonable volume of incoming anchor text. Moreover, for each web page, the content of its incoming anchor text would be better than the content of itself in objectively describing the page. That is, doing retrieval in the collection of incoming anchor text could yield more effective results than doing that in the original collection, especially when talking about the Diversity Task. Table <ref type="table" coords="3,140.88,392.85,5.45,9.82">1</ref> and Table <ref type="table" coords="3,196.44,392.85,5.45,9.82" target="#tab_0">2</ref> list our submissions for the Ad-hoc and for the Diversity Tasks, respectively. The tables show a strong correlation of the effectiveness performance across the two tasks. They also show that, as anticipated, the anchor text collection does work significantly better than the original text collection in terms of retrieval accuracy. One surprise is that Table <ref type="table" coords="3,466.80,433.53,5.45,9.82">1</ref> shows very marginal difference on performance between IMP and IBM25. While that is not consistent with <ref type="bibr" coords="3,99.24,460.65,74.42,9.82" target="#b2">Anh et al. [2008]</ref>, it might due to the change in IBM25 formulation for this work, which clearly targets the symmetrical usage of ω d,t and ω q,t .</p><p>Subsequently, <ref type="bibr" coords="3,178.20,487.77,98.79,9.82" target="#b3">Cormack et al. [2010]</ref> showed that the use of spam filtering significantly improved the accuracy of most of the TREC 2009 submissions, including ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Web Track 2010</head><p>For the Web Track 2010 we intended to compare the use of both content and anchor components with that of content only. We again employed the two similarity formulations defined by Formula 1 and Formula 2. Unlike the previous year, we applied spam filtering to all of our submissions. Any document that had fusion score (as defined by <ref type="bibr" coords="3,300.00,601.89,93.68,9.82" target="#b3">Cormack et al. [2010]</ref>) of at least 0.70 was removed from the result lists.</p><p>When both content and anchor text are employed, for each query q two separate searches are conducted, one over the original Web document collection, and the other over the anchor text collection. The search method and setting are similar for both of the cases. Each result list is then normalized linearly so that the top score of the list is 1. Then, the lists are merged, and for each document d the final score is computed as</p><formula xml:id="formula_2" coords="3,229.56,705.18,294.75,14.43">S d,q = (1 -α) • S C d,q + α • S A d,q ,<label>(3)</label></formula><p>where α = 0.25, S C d,q is the normalized content score, and S C d,q is the normalized anchor score. Table <ref type="table" coords="3,140.52,746.97,5.45,9.82" target="#tab_2">3</ref> shows the performance of our Ad-hoc submissions. Overall, the effectiveness performance is good, given that no extra information except for the spam scores was employed. The clearly shows the advantage of using fusion of content and anchor over that of content alone -by about 100% for P@5, and about 70% for P@10. Similar to our 2009 submissions, the difference in performance of IMP and IBM25 is marginal.</p><p>For the 2010 Diversity Task we argued that the anchor text collection might play a central role for effective retrieval. In fact, when different authors write outgoing anchor text to a particular web page, they might pay attention on different aspects of the page. Similarly, one author can also give a number of anchor text to the same page, likely to different coverage. In short, incoming anchor text for a page can objectively high-light various important aspects of web pages and hence is probably valuable for the Diversity search.</p><p>Our Diversity submissions are summarized in Table <ref type="table" coords="4,342.00,486.81,4.06,9.82" target="#tab_3">4</ref>. A comparison with Table 2 reveals that our argument in regard to the role of anchor text is not contradicted. Indeed the advantage of anchor text over content-only runs is dramatic. Moreover, although the fusion between anchor text and content works better than the anchor text alone, the performance gap is relatively small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Session Track 2010</head><p>The central point for this track is to measure the ability of retrieval systems to improve search accuracy after learning that users re-formulate their initial queries. It is supposed that in a search session, a user first issues a query RL1, obtains some results, then for various reasons (such as being unhappy with the results or realizing mistakes), re-formulates RL1 to RL2 and re-submits. The question is whether the retrieval systems can employ the history of RL1 to improve the overall quality of the results returned for RL2.</p><p>For the purpose of the track, each participating retrieval system submitted three output sets per session: set RL1 for the original query RL1; set RL2 for the re-formulated RL2 as a stand-alone query; and set RL3 for employing query RL3 which, in general, is a system's re-formulation of RL2 using the knowledge of both RL1 and RL2. Note that the output for RL2 is purely for the comparison purpose, that the system performance is assessed through the accuracy of the outputs of RL3 relative to that of RL1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>For our submissions we supposed:</p><p>• that the user re-formulates the query from RL1 to RL2 because the RL1 output is of poor quality, that is, its quality is considered to be inferior;</p><p>• that the user has enough patience that she or he issues RL2 after skimming a large number (say, 2,000, which the number of answers per query set by the Track's organizers) of items of the RL1 output; and</p><p>• that the user is an experienced searcher, and hence RL2 is a well-formulated and highlyaccurate query.</p><p>With these two suppositions, it is assumed that</p><p>• there is no need for the system to change the query RL2, and so the query RL2 will be used to generate RL3 output; and</p><p>• since the run RL1 is considered as a failure, items appear in the RL1 output should be discouraged from appearing in the RL3 output.</p><p>Based on these assumptions, we designed the following simple strategy for our submissions. The queries RL1 and RL2 are submitted to our search systems. No actual search is done for RL3. Instead, the RL3 output list is generated from the respective lists of RL2 and RL1. First, all documents d in RL2 and their respective scores s 2 d are included in the candidate list for RL3. Then the scores of the RL1 and RL3 lists are normalized so that they share a common maximal score (on a per-query basic). Next, for each d in the RL3 list, if d also appears in the RL1 list, the score s 3 d of d is modified to</p><formula xml:id="formula_3" coords="5,262.44,465.56,261.87,14.17">s 3 d = s 2 d -ψ • s 1 d ,<label>(4)</label></formula><p>where ψ is called the penalty coefficient, and 0 ≤ ψ ≤ 1. The setting ψ = 0 means that RL3 output is identical to that of RL2. When ψ = 1, the maximal penalty is set, and is equal to the score of d in RL1 output. Note that the penalty value is linearly and positively correlated to the RL1 scores, so the higher-evaluated documents in the RL1 list get larger penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submitted runs</head><p>For the Session Track, we altered our search system to allow the use of static PageRank score. In principle, the logarithm of PageRank scores was employed. Moreover, the PageRank scores were normalized locally for each query so that the maximal value of PageRank scores for the list of all documents that appear in either the result list of content-only search or anchor-only search is 1 (as in the case of normalized scores of the content-only and anchor-only results). When all content, anchor, and PageRank components are employed, the aggregated similarity score is calculated as</p><formula xml:id="formula_4" coords="5,196.56,673.26,327.75,14.31">S d,q = (1 -α -β) • S C d,q + α • S A d,q + β • S P d,q ,<label>(5)</label></formula><p>where α = β = 1, S C d,q , S A d,q are defined as in Formula 3, and S P d,q is the normalized pagerank score of d with respect to q.</p><p>We made use of three different run styles as listed below.</p><p>1. Style A, characterized by the retrieval model IBM25, the use of content, anchor, and pagerank as defined by Formula 5, and the penalty coefficient ψ = 0.25 for the Formula 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Style B, almost identical to</head><p>Style A except for the value of ψ, which is set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Style C, a content-only run, with the similarity score defined using the IMP model, and ψ in Formula 4 is set to 0.25 as in the case of Style A.</p><p>We submitted three runs -UM10SibmA, UM10SibmbB, and UM10SimpA, which followed exactly the three styles Style A, Style B, and Style C, respectively. The performance of these submissions is listed in Table <ref type="table" coords="6,197.40,399.57,5.45,9.82" target="#tab_4">5</ref> under the umbrella of Group A. The table also lists, under Group D, some statistics from all TREC submissions for this track.</p><p>Table <ref type="table" coords="6,141.36,426.69,5.45,9.82" target="#tab_4">5</ref> shows that, for our submissions, the runs that employ anchor text and pagerank in addition to the original content collection significantly outperform the content-only run. We also conducted another run (not reported here), which is content plus anchor as defined by Formula 3.</p><p>Unfortunately, our post-TREC analysis showed that this run has a performance which is very close to that of Style A, which means that our use of PageRank did not improve retrieval effectiveness. We will not make any further attempt in this report to track down the reasons for this failure.</p><p>The comparison of Style A and Style B in Table <ref type="table" coords="6,346.56,508.05,5.45,9.82" target="#tab_4">5</ref> reveals several interesting points with regards to the purpose of the Session Track and our approaches. First, the large change in the value of ψ between Style A and Style B did not bring too much difference in performance. Perhaps any value between 0.25 and 1.00 could give a similar effect.</p><p>Second, the policy of penalty actually hurt the retrieval performance, instead of improving it as we hoped for. And the larger is the penalty coefficient, the worse is the main system performance attribute, which is defined by the columns RL13. Perhaps:</p><p>• The supposition that RL1 failed is not totally correct. Actually, it performed well relative to RL2.</p><p>• The supposition that users are patient enough to read all 2,000 answers of RL1 before deciding to issue the reformulated queries is (of course) incorrect. Perhaps, users are impatient enough to read only top-10 results (as endorsed by the track's guideline). That is, the penalty should be applied only to these top-10 documents, and ones that are demonstrably similar to them, and not to any other. additional experiments are reported in Table <ref type="table" coords="7,293.16,358.89,5.45,9.82" target="#tab_4">5</ref> under the labels of Group B and Group C. In Group B we limited the penalty only to the top-10 documents of the RL1 output list, and hence all of other documents have scores as defined by the normalized scores of RL2. The table shows that this policy helps improve the performance of RL3, but the performance is still considerably worse than that of RL2. That means, penalty is likely not a good approach. Experiments in Group C were designed to check the reverse policy. Instead of penalties, rewards were given to the top-10 documents of the RL1 list. That is, the Formula 4 becomes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-TREC Experiments</head><formula xml:id="formula_5" coords="7,262.44,461.60,261.87,14.17">s 3 d = s 2 d + ψ • s 1 d .<label>(6)</label></formula><p>The two styles Style A and Style B under the section Group B were conducted in this manner, and with no change to the ψ values. It can be seen that this time, RL3 outperforms RL2 in terms of accuracy, although the gap is modest. The small success of the reverse policy confirm that the policy is not worthy, at least in the context of this year's Session Track. The problem is, of course, the relatively good initial performance of RL1. Overall, our submissions had excellent performance in terms of effectiveness of the Ad-hoc Task, but failed to address the main criterion of the Track -the RL13 scores. It is, however, difficult to analyze the reasons for this failure. On the one hand, it can be said that our methodology, especially the policy of applying a penalty to documents in the RL1 output, is unsupported, and we need to seek alternative approaches in order to have better performance for RL3. On the other hand, the failure is partly due to the assumption that the RL1 output is poor, and that the query RL2 is much better than the query RL1. None of these two assumptions is correct for this year. Given that over the whole Track the maximal score for RL3 is lower than that for RL2, we unfortunately face an uncertain question of whether the settings for this year's Track were appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Technical Notes</head><p>The experiments described in this paper were conducted using a HPC cluster located at RMIT University. Various parameters of the system are listed at http://its-ru-hpc-mgmt.cs.rmit. edu.au/doku.php?id=rmit_hpc_specifications. In short, the cluster consists of 34 machines and a storage unit. Each machine has eight 2.3 GHz CPUs and 32 GB RAM. We share the system with other users. For simplicity, we use the word "node" to refer to a CPU, not a physical machine. Our system employed 32 nodes for both indexing and querying. These 32 nodes normally belonged to only a few machines, but we were not able to choose or specify particular nodes or machines, or number of nodes per machine.</p><p>For a document collection, the principal component of its index is the inverted file, where each distinct term of the collection is associated with an inverted list. We made use of an impact-sorted index. The impact-sorted inverted list for a term t is a list of equal-impact blocks. Each block represents one distinct impact value k, and contains the sequence of document numbers in which t appears and has an impact score of k. Inside a block, document numbers are arranged in increasing order, to facilitate compression. The blocks are arranged in decreasing order of associated impacts, so as to support effective pruning.</p><p>Compression is applied to inverted files. In all of our experiments the word-synchronized compression scheme Simple8 <ref type="bibr" coords="8,234.72,266.01,108.51,9.82" target="#b1">[Anh and Moffat, 2010]</ref> was used for inverted list compression. This method provides a good balance between index space and decoding speed, and is especially good for skipping operation.</p><p>For the content-only collection, the wall-clock time for indexing was approximately 16 hours. Note that this time included time for creating a fully positional index, and then extracting a working, non-positional index. The query time was not recorded because of the small number of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>TREC-2009 and TREC-2010 marked the first time our team, as well as many other teams, worked with a large text collection of over ten terabytes. We managed to perform the tasks in a reasonable time for both indexing and querying. In terms of effectiveness performance, we got good results. Given that all of our runs did not rely on any external resources like external databases or commercial search engines, the results were quite encouraging. That shows that the impact-based retrieval is competitive, despite of the fact that it is simple and does not involve tuning parameters.</p><p>Across different tasks in the two years, we noticed the crucial role of the anchor text collection for effective retrieval. In one of the tasks, the use of anchor text alone yields the performance better than that of content alone, and almost as good as using both content and anchor text. In all tasks, the use of anchor text in addition to the content significantly improve the effectiveness performance.</p><p>There are a number of problems need to be addressed in the upcoming TRECs, including finding the way to effectively employ PageRank scores; designing new, more effective, approaches for the Session Track; dealing with the specific features of the Diversity Task; and improving both effectiveness and efficiency of the retrieval model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,99.24,102.33,425.09,212.99"><head>220 0.241 0.268 0.091 0.073 0.061Table 2 :</head><label>2</label><figDesc>Effectiveness performance of the 2009 Web Track Diversity submissions. Note that the run mudvibm5 is identical to muadibm5, and mudvimp -to muadanchor.</figDesc><table coords="3,99.24,102.33,425.09,164.63"><row><cell>Run</cell><cell></cell><cell>Description</cell><cell></cell><cell>MAP</cell><cell cols="2">P@5 P@10 P@20</cell></row><row><cell cols="2">muadimp</cell><cell cols="2">IMP, content only</cell><cell cols="3">0.044 0.108 0.116 0.125</cell></row><row><cell cols="7">muadanchor IMP, anchor text only 0.0256 0.296 0.250 0.179</cell></row><row><cell cols="2">muadibm5</cell><cell cols="2">IBM25, content only</cell><cell cols="3">0.044 0.108 0.118 0.125</cell></row><row><cell cols="7">Table 1: Effectiveness performance of the 2009 Web Track Ad-hoc submissions. Each figure in</cell></row><row><cell cols="3">bold is the highest in that column.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="2">Description</cell><cell>@5</cell><cell cols="2">α-nDCG @10 @20</cell><cell>@5</cell><cell>precision-IA @10 @20</cell></row><row><cell cols="3">mudvibm5 IBM25, content only</cell><cell cols="3">0.106 0.112 0.132</cell><cell>0.057 0.046 0.043</cell></row><row><cell>mudvimp</cell><cell cols="2">IMP, anchor text only</cell><cell>0.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,503.16,760.53,21.04,9.82"><head>table Run</head><label>Run</label><figDesc></figDesc><table coords="4,147.84,102.33,322.01,50.99"><row><cell></cell><cell>Description</cell><cell>MAP P@5 P@10 P@20</cell></row><row><cell>UMa10BSF</cell><cell>IBM25, content only</cell><cell>0.066 0.183 0.206 0.192</cell></row><row><cell cols="3">UMa10BASF IBM25, content + anchor 0.088 0.383 0.356 0.321</cell></row><row><cell cols="2">UMa10IASF IMP, content + anchor</cell><cell>0.087 0.394 0.358 0.319</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,99.24,178.17,425.01,102.35"><head>Table 3 :</head><label>3</label><figDesc>Effectiveness performance of the 2010 Web Track Ad-hoc submissions. In all of the runs, the 30% spammiest pages according to the fusion scores were discarded.</figDesc><table coords="4,101.88,215.72,413.82,64.80"><row><cell>Run</cell><cell>Description</cell><cell>@5</cell><cell>α-nDCG @10 @20</cell><cell>@5</cell><cell>precision-IA @10 @20</cell></row><row><cell>UMd10ASF</cell><cell>IBM25, anchor text only</cell><cell cols="2">0.236 0.260 0.293</cell><cell cols="2">0.127 0.109 0.086</cell></row><row><cell cols="2">UMd10BASF IBM25, content + anchor</cell><cell cols="2">0.275 0.336 0.379</cell><cell cols="2">0.162 0.152 0.131</cell></row><row><cell cols="2">UMd10IASF IMP, content + anchor</cell><cell cols="2">0.281 0.335 0.380</cell><cell cols="2">0.165 0.144 0.130</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,99.24,305.37,425.08,37.07"><head>Table 4 :</head><label>4</label><figDesc>Effectiveness performance of the 2010 Web Track Diversity submissions. In all of the runs, the 30% spammiest pages according to the fusion scores have been discarded. Note that the run UMd10BASF is identical to UMa10BASF, and UMd10IASF is identical to UMa10IASF.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,99.24,102.33,425.25,194.50"><head>Table 5 :</head><label>5</label><figDesc>Effectiveness performance of the 2010 Session Track submissions. In all of the runs, the 30% spammiest pages according to the fusion scores have been discarded. In the header, RL1, RL2, and RL3 accordingly refer to the score of RL1, RL2, and RL3 output. See the Session Track's overview paper for the meaning of RL12 and RL13.</figDesc><table coords="6,124.68,102.33,368.10,119.02"><row><cell>Run</cell><cell>nsDCG@10 RL12 RL13</cell><cell cols="2">nsDCG dupes@10 RL12 RL13</cell><cell>RL1</cell><cell>nDCG@10 RL2</cell><cell>RL3</cell></row><row><cell cols="2">Group A: unimelb submissions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Style A</cell><cell>0.249 0.221</cell><cell>0.245</cell><cell>0.229</cell><cell cols="3">0.235 0.266 0.178</cell></row><row><cell>Style B</cell><cell>0.249 0.216</cell><cell>0.245</cell><cell>0.225</cell><cell cols="3">0.235 0.266 0.165</cell></row><row><cell>Style C</cell><cell>0.214 0.189</cell><cell>0.217</cell><cell>0.198</cell><cell cols="3">0.201 0.236 0.165</cell></row><row><cell cols="4">Group D: Statistics from all TREC submissions</cell><cell></cell><cell></cell></row><row><cell>max</cell><cell>0.249 0.238</cell><cell>0.245</cell><cell>0.229</cell><cell cols="3">0.235 0.266 0.260</cell></row><row><cell>median</cell><cell>0.204 0.178</cell><cell>0.207</cell><cell>0.187</cell><cell cols="3">0.189 0.214 0.170</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,99.24,750.69,425.20,23.51"><head>Table 6 :</head><label>6</label><figDesc>Based on the above two arguments, we conducted a number of post-TREC experiments, by changing the way the RL3 output is generated. No change was made to the RL1 and RL2 output. The Effectiveness performance of the post-TREC experiments for Session Track. Some figures from the official submissions and the Track's statistics are also include under Group A and Group D. In all of the runs, the 30% spammiest pages according to the fusion scores have been discarded.</figDesc><table coords="7,124.68,102.33,368.10,160.54"><row><cell>Run</cell><cell>nsDCG@10 RL12 RL13</cell><cell cols="2">nsDCG dupes@10 RL12 RL13</cell><cell>RL1</cell><cell>nDCG@10 RL2</cell><cell>RL3</cell></row><row><cell cols="4">Group A: unimelb submissions, penalty to all items in RL1</cell><cell></cell><cell></cell></row><row><cell>Style A</cell><cell>0.249 0.221</cell><cell>0.245</cell><cell>0.229</cell><cell cols="3">0.235 0.266 0.178</cell></row><row><cell cols="4">Group B: Penalty applied only to the top-10 of RL1</cell><cell></cell><cell></cell></row><row><cell>Style A</cell><cell>0.249 0.241</cell><cell>0.245</cell><cell>0.250</cell><cell cols="3">0.235 0.266 0.243</cell></row><row><cell>Style B</cell><cell>0.249 0.241</cell><cell>0.245</cell><cell>0.250</cell><cell cols="3">0.235 0.266 0.242</cell></row><row><cell cols="6">Group C: Rewards instead of penalty, and only to those in top-10 of RL1</cell></row><row><cell>Style A</cell><cell>0.249 0.249</cell><cell>0.245</cell><cell>0.245</cell><cell cols="3">0.235 0.266 0.269</cell></row><row><cell>Style B</cell><cell>0.249 0.249</cell><cell>0.245</cell><cell>0.245</cell><cell cols="3">0.235 0.266 0.268</cell></row><row><cell cols="4">Group D: Statistics from all TREC submissions</cell><cell></cell><cell></cell></row><row><cell>max</cell><cell>0.249 0.238</cell><cell>0.245</cell><cell>0.229</cell><cell cols="3">0.235 0.266 0.260</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement This work was supported by the <rs type="funder">Australian Research Council</rs>. We thank <rs type="institution">RMIT University (Australia)</rs> for letting us to use their high-performance computing cluster for all the experiments described in this report.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,99.24,713.49,425.07,9.82;8,110.16,727.05,414.19,9.82;8,110.16,740.61,414.21,9.82;8,110.16,754.17,211.05,9.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,199.68,713.49,197.33,9.82">Simplified similarity scoring using term ranks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,331.56,727.05,192.79,9.82;8,110.16,740.61,290.09,9.82">Proc. 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ziviani</surname></persName>
		</editor>
		<meeting>28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,99.24,103.41,424.77,9.82;9,110.16,116.97,396.09,9.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,200.28,103.41,164.19,9.82;9,208.92,116.97,137.00,9.82">Index compression using 64-bit words</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<ptr target="cs.mu.oz.au/alistair/coders-64bit/" />
	</analytic>
	<monogr>
		<title level="j" coord="9,373.08,103.41,146.25,9.82">Software -Practice &amp; Experience</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="147" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Source code available from www</note>
</biblStruct>

<biblStruct coords="9,99.24,139.53,424.97,9.82;9,110.16,153.09,413.93,9.82;9,110.16,166.65,414.09,9.82;9,110.16,180.21,331.29,9.95" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,238.32,139.53,285.89,9.82;9,110.16,153.09,30.21,9.82">Term impacts as normalized term frequencies for BM25 similarity scoring</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-89097-3_7</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-540-89097-3_7" />
	</analytic>
	<monogr>
		<title level="m" coord="9,352.20,153.09,171.89,9.82;9,110.16,166.65,112.97,9.82">Proc. 15th Int. Symp. String Processing and Information Retrieval</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Amir</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Turpin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</editor>
		<meeting>15th Int. Symp. String essing and Information Retrieval<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008-11">November 2008</date>
			<biblScope unit="volume">5280</biblScope>
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,99.24,202.65,425.07,9.82;9,110.16,216.21,189.57,9.82" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,283.44,202.65,240.87,9.82;9,110.16,216.21,78.06,9.82">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<idno>srXiv:1004.5168</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,99.24,238.77,425.09,9.82;9,110.16,252.33,414.03,9.82;9,110.16,265.77,413.93,9.82;9,110.16,279.33,395.37,9.95" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,428.76,238.77,76.10,9.82">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<ptr target="http://potomac.ncsl.nist.gov:80/TREC/t3_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="9,193.92,252.33,211.28,9.82">Proc. Third Text REtrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>Third Text REtrieval Conference (TREC-3)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11">November 1994</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
	<note>Special Publication 500-225</note>
</biblStruct>

<biblStruct coords="9,99.24,301.89,424.85,9.82;9,110.16,315.45,274.05,9.82" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,145.68,301.89,378.41,9.82;9,110.16,315.45,54.00,9.82">Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison Wesley</publisher>
			<pubPlace>Reading, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
