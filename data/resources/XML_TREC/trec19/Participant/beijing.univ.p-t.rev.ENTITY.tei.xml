<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,107.28,77.06,397.55,14.36">PRIS at TREC 2010: Related Entity Finding Task of Entity Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,183.84,114.71,53.63,9.50"><forename type="first">Zhanyi</forename><surname>Wang</surname></persName>
							<email>wangzhanyi@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.51,114.71,63.81,9.50"><forename type="first">Chunsong</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.66,114.71,40.24,9.50"><forename type="first">Xueji</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.19,114.71,58.94,9.50"><forename type="first">Haoyi</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.52,134.75,29.06,9.50"><forename type="first">Ru</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.75,134.42,43.49,9.94"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.12,134.75,50.68,9.50"><forename type="first">Guang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.20,134.75,35.51,9.50"><forename type="first">Jun</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,107.28,77.06,397.55,14.36">PRIS at TREC 2010: Related Entity Finding Task of Entity Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6A47A12D4B2C5795026E49F2DF34EA44</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports the approaches to the task of Entity Track applied by PRIS lab of BUPT in TREC 2010. We used Document-Centered Model (DCM) and Entity-Centered Model (ECM) for the task. BM25 method was introduced into ECM besides indri retrieval model.</p><p>Another improvement aimed at entity extraction. Special web page, NER tool and entity list generated by some rules were all taken into account. Also, some external resources such as Google and CMU search engine were applied.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.12" lry="792.12"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.12" lry="792.12"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.12" lry="792.12"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.12" lry="792.12"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.12" lry="792.12"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.12" lry="792.12"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.12" lry="792.12"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.12" lry="792.12"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The overall aim of the 2010 TREC Entity track is to create a test collection for the evaluation of entity related searches on Web data. This year the Related Entity Finding (REF) is the main task of the track. The problem of related entity finding is defined as follows <ref type="bibr" coords="1,407.70,462.37,11.84,9.94" target="#b0">[1]</ref>: Given an input entity, by its name and homepage, the type of the target entity, as well as the nature of their relation, described in free text, find related entities that are of target type, standing in the required relation to the input entity. Compared with previous edition, following changes are introduced. In order to cope with the challenges, we mainly aimed at four areas: collecting relative documents, extracting named entities, constructing retrieval model and allocating homepages for entities. We also used some external resources, such as Stanford NER Tool, Indri 1 &amp; Lemur toolkit 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ English subset of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Collecting Relative Documents</head><p>The most significant change in TREC 2010 Entity Track is the enlargement of data set. The volume of English portion of uncompressed ClueWeb category A is about 15 TB. Being restricted by hardware resource, we used the category A search engine developed by CMU (http://boston.lti.cs.cmu.edu:8085/ clueweb09/search/cata_english/lemur.cgi?). We collected documents related to queries for named entity finding.</p><p>First, we analyzed all 70 topics with focus on the "entity_name" and "narrative" fields, extracted keywords from these fields and reformulated queries. Taking the 30th topic for example, the original topic is as the follows:</p><p>and Google and CMU search engine.</p><p>The report is organized as follows. Section 2 describes the process of collecting relative documents. Section 3 introduces our methods of named entity extraction. Section 4 proposes our retrieval models. Allocating homepages for entities is presented in Section 5. Submitted runs show in Section 6 and Section 7 gives the conclusion and future work. In this topic, "Ocean", "Spray" and "grower" can be treated as keywords. Then we rewrote it into the type of Indri query language, such as "#uw(Ocean+Spray+grower)". The actual query language was more complicated.</p><p>Second, we sent the queries to the CMU search engine one by one. Some useful information such as document number, title, URL, rank and score could be got from the result page. Generally, a query obtained less than 1,000 documents. This information was stored for the latter needs.</p><p>In order to indexing by Indri or Lemur, we parsed the documents and structure into the follow format: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Named Entities Extraction</head><p>Last year, named entities extraction in our system only adopted automatic NER tool. The precision was low. In contrast, we integrated multiple methods this year. Specifically, special web page, NER tool and entity list generated by some rules were taken into account.</p><p>First we refined key words from topics and retrieved in Google and Wikipedia. In related pages with high ranking score, we looked for special tables and lists, and then screened partial entities for the topic manually.</p><p>Then we extracted entities using NER tools from the web pages. We filtered the entities through several entity lexicons to gain the ultimate list. With the development of NLP, most NERs (Named Entity Recognizers) can utilize the context information to recognize the named entities, such as Stanford NER <ref type="foot" coords="3,227.64,585.53,3.48,6.26" target="#foot_0">3</ref> , LBJ NER<ref type="foot" coords="3,281.40,585.53,3.48,6.26" target="#foot_1">4</ref> </p><p>In the previous section, we gained the score, doc-number and URL of the pages with high relevance retrieved by searching engine. Then the first five pages were crawled and downloaded to the local. Then we picked out the related named entities in the simply-processed documents . After weighting the speed and accuracy of Stanford NER and LBJ NER, we decided to employ the former. using the Stanford NER. Certainly, the NER can identify all types of entities at one time, which means we should limit the condition to get the entities of target type. Finally, a list of entities in a particular format was gained under the following procedures.</p><p>However, not all the results extracted were entities, so we built lexicons using the Wikipedia resources to refine the list to gain a better result.</p><p>Wikipedia is kind of an encyclopedia including human knowledge in all fields, rather than a simple dictionary, an online forum or others. It is worth mentioning that Wikipedia is an open resource to the extent that anyone can copy and modify the materials. It offers convenience for people of different occupations to access to knowledge. On the other hand, users can enrich them by broadening their scope of knowledge.</p><p>Considering above features of Wikipedia, we downloaded and stored the pages as local texts.</p><p>Meanwhile, we discriminated the texts by rules, such as, starting with "Organization", starting with "Companies" for ORG type. We referred to the rules worked out by University of Amsterdam last year <ref type="bibr" coords="4,194.96,340.42,12.92,9.94" target="#b1">[2]</ref> and proposed some new ones. Then we obtained four different collections of documents including the four types. Figure <ref type="figure" coords="4,132.84,556.46,5.52,9.94" target="#fig_3">3</ref> shows the whole course. In each collection, we extracted a list of entities according to the labels in the documents, which is called lexicon. After the NER tool's result, we just need to justify whether the entity is in the corresponding sort of lexicon or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Retrieval Models</head><p>This year, we submitted four runs. The first two runs (PRIS1 and PRIS2) used Document-Centered Model (DCM) which is similar to the two-stage search model <ref type="bibr" coords="4,490.58,684.37,12.92,9.94" target="#b2">[3]</ref> we proposed last year. As figure <ref type="figure" coords="4,223.07,704.41,5.52,9.94" target="#fig_4">4</ref> shown, we call it DCM for emphasizing that documents link the queries and entities, as opposed to Entity-Centered Model (ECM). The difference is that entities extracted from special pages are given more weights in PRIS2. So the more reliable entities are, the early ranked they are. The query, entity, document and collection are denoted by q, e, d and D respectively. Then the formula of DCM is</p><formula xml:id="formula_0" coords="5,193.57,160.69,248.35,29.89">( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) , 1 , d D</formula><p>p q e score e q p e q p e d p q d p d p q p q âˆˆ = = = âˆ‘ (1) A new model was introduced in our task this year. In ECM, an entity is represented by snippets extracted from relevant documents. Unlike DCM, both probabilities of a query and entity with respect to a document are estimated by twice retrievals, the probability of an entity given by a query is got only once retrieval in ECM. The formula of ECM is</p><formula xml:id="formula_1" coords="5,130.21,205.76,349.90,113.59">q d 1 d 2 d l e 1 e 2 e m â€¦ â€¦ (q,d) (e,d) (q,e) q e 1 e 2 e m d 1 d 2 d l â€¦ â€¦ (q,e)</formula><formula xml:id="formula_2" coords="5,223.57,430.59,187.44,32.99">( ) ( ) ( ) ( ) ( ) ( )</formula><p>, e e e p q d p d score e q p e q p d q p q = = =</p><p>where d e is the new document composed of snippets of an entity e.</p><p>As is well known, the context of an entity in a document is considered to be the significant information. Words around it are called a snippet. In our task, the length of every snippet was 150 words. Some snippets extracted from different relevant documents of the entity were combined into a new document. Each new document contains original entity information. We built an index to the new documents for every topic. As long as the document is retrieved by a query, the entity is considered to be relevant. In ECM, we used the BM25 weight to rank documents.</p><p>BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document. One of the most prominent instantiations of the function is as follows.<ref type="foot" coords="5,485.88,656.57,3.48,6.26" target="#foot_2">5</ref> </p><formula xml:id="formula_4" coords="6,198.63,79.94,204.93,38.18">( ) ( ) ( ) ( ) ( ) 1 1 1 , 1 , ,<label>1</label></formula><formula xml:id="formula_5" coords="6,173.91,81.54,346.72,46.44">n i i i i f q D k score D Q IDF q D f q D k b b avgdl = â‹… + = â‹… ï£« ï£¶ + â‹… -+ â‹… ï£¬ ï£· ï£­ ï£¸ âˆ‘ (3)</formula><p>where f(q i ,D) is q i 's term frequency in the document D, |D| is the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn. k 1 and b are free parameters, usually chosen as k 1 = 2.0 and b = 0.75. IDF(q i ) is the IDF weight of the query term q i . The scores generated by retrieval system are probably not in (0, 1), thus normalization is necessary. The following formula was used in our runs: min</p><formula xml:id="formula_6" coords="6,265.82,258.83,254.82,26.77">max min 1 score s -ï£¯ ï£º ï£° ï£» = - + (4)</formula><p>Here, score is the original score, s is the final score. Max and min are the maximum and minimum of the original score respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Allocating Homepages for Entities</head><p>This stage was the last part of our task. Before this stage, four runs which came from the retrieving methods were produced. In each run, there were a number of entity results for 70 topics.</p><p>Those results in each topic were ranked by the score, like the follows: Figure <ref type="figure" coords="6,276.00,536.32,3.76,8.96">5</ref>. A example of a run.</p><p>Our four runs adopted the same processing method. It is as follows: First, for any line in a run, parse out the document id and entity name. Second, construct a URL using entity name and get the html by Google. The html contains tens of thousands of relevant results of this entity, but only the first five results are needed. Title and URL are extracted. Some character strings can be found in title and URL, which can be used to measure the possibility of a result being the homepage of the entity. We give a score for the first five results, then resort them in descending order according to the scores they get. Third, search docid from URL database. The homepage of the entity is allocated when a docid is found. The default homepage document id in the original entity run is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Submitted Runs</head><p>Our experimental data, the English subset of the "Category A" of ClueWeb09, contain about 500 million English pages. For each query, we returned up to 100 related entities. Supporting documents were omitted. The four runs and descriptions are shown in table <ref type="table" coords="7,423.87,208.45,4.14,9.94" target="#tab_1">1</ref>.    The improvement of this year is mainly due to some key details. The content of topics and collection were mined more deeply. Appropriate rules tools, and entity lists were constructed for entity extraction. More flexible retrieval models and ways of ranking entities were tried in the experiments. Several resources and methods were used for allocating primary homepages. In contrast to the improvement in the finding of primary homepages, relevant homepages were less found. This exposed the weak point of our work, putting focus only on the primary ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Work</head><p>It's the second year we participating the REF task. By integrating some new methods we improved our system. By CMU search engine, we eased the job of getting information from the huge data set ClueWeb. At the stage of entity extraction, special web page, NER tool and entity list generated by some rules were properly taken into account. ECM was also introduced for model improvement. Finally, we used Google and URL database to refine homepages of ranked entities. In the future, we'll pay more attention to mining the context of entities and finding relevant homepages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,197.17,542.40,66.32,9.94;1,111.96,562.44,149.97,9.94;1,111.96,582.36,120.35,9.94;1,111.96,602.39,118.24,9.94;1,111.96,622.43,246.28,9.94;1,111.96,642.35,154.01,9.94;1,111.96,662.39,179.65,9.94;1,111.96,682.42,293.93,9.94"><head></head><label></label><figDesc>ClueWeb cat A â€¢ Single record submission format â€¢ No supporting documents â€¢ New entity type: location â€¢ Revised definition of primary and relevant homepages â€¢ Wikipedia pages are not accepted â€¢ Primary homepages are rewarded more â€¢ Names are judged only for primary pages; the judgment is binary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,241.32,577.36,129.45,8.96"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example of a topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,231.60,340.60,148.64,8.96"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An example of a document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,225.60,537.16,160.77,8.96"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Build entity lists of four types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,216.00,332.80,179.96,8.96"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The framework of DCM and ECM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,164.64,444.04,19.92,8.96;6,164.64,459.63,276.33,8.96;6,164.64,475.23,246.86,8.96;6,164.64,490.83,276.24,8.96;6,164.64,506.43,19.92,8.96;7,90.00,80.42,432.24,9.94;7,90.01,100.46,248.45,9.94"><head>â€¦â€¦ 1 Q0</head><label>1</label><figDesc>clueweb09-en0012-54-05924 6 0.884194 PRIS1 T_Mobile_PO 1 Q0 clueweb09-en0010-63-31612 7 0.881037 PRIS1 E_Plus 1 Q0 clueweb09-en0012-54-05919 8 0.880843 PRIS1 T_Mobile_US â€¦â€¦ replaced with the new docid. Finally, keep the first docid-duplicated entity, and remove all the others if duplicates of document ids in a topic are found.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,89.98,229.12,432.24,226.27"><head>Table 1 .</head><label>1</label><figDesc>Runs and descriptions.</figDesc><table coords="7,89.98,248.90,432.24,206.49"><row><cell>Run Tag</cell><cell>Description</cell></row><row><cell>PRIS1</cell><cell>Document-Centered Model by Indri</cell></row><row><cell>PRIS2</cell><cell>Document-Centered Model by Indri,</cell></row><row><cell></cell><cell>giving priority to entities extracted manually</cell></row><row><cell>PRIS3</cell><cell>Entity-Centered Model by Indri</cell></row><row><cell>PRIS4</cell><cell>Entity-Centered Model by Lemur (BM25)</cell></row><row><cell cols="2">Because the evaluation results of other groups have not been published, we only compare our</cell></row><row><cell cols="2">runs to each other. In total 50 topics of 2010 task, 47 ones are evaluated. Here we list the results</cell></row><row><cell cols="2">of submitted runs in table 2. The PRIS2 run presents the best performance in both nDCG_R and</cell></row><row><cell>P@10.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,466.12,432.46,213.54"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="7,90.00,466.12,432.46,213.54"><row><cell></cell><cell cols="3">. nDCG_R and P@10 of submitted runs.</cell><cell></cell></row><row><cell></cell><cell>PRIS1</cell><cell>PRIS2</cell><cell>PRIS3</cell><cell>PRIS4</cell></row><row><cell>nDCG_R</cell><cell>0.2158</cell><cell>0.2846</cell><cell>0.216</cell><cell>0.1761</cell></row><row><cell>P@10</cell><cell>0.1745</cell><cell>0.2489</cell><cell>0.1766</cell><cell>0.1426</cell></row><row><cell cols="5">According to the baseline (best, median and worst), we mapped our result (PRIS2) into the</cell></row><row><cell cols="5">different intervals. Table 3 shows the distributions of nDCG_R. In 2010, 36 topics were above the</cell></row><row><cell cols="5">average level. More than 91% of all topics achieved or surpassed median values. While in 2009,</cell></row><row><cell cols="5">over half of them were below it. Table 4 proves that the improvement of finding primary</cell></row><row><cell cols="5">homepages. Most are in the [best, median] interval. Last year the topics equaling median took up</cell></row><row><cell cols="5">80%, but most of them were not allocated a correct homepage. 312 out of 779 primary</cell></row><row><cell cols="3">homepages were found, which is taken up over 40%.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,166.80,81.16,275.87,76.88"><head>Table 3 .</head><label>3</label><figDesc>The nDCG_R distribution of PRIS2 in the last two years.</figDesc><table coords="8,166.80,97.34,275.87,60.70"><row><cell></cell><cell>Best</cell><cell>Best~Median</cell><cell>Median</cell><cell>Others</cell></row><row><cell>2010</cell><cell>1 2.13%</cell><cell>35 74.47%</cell><cell>7 14.89%</cell><cell>4 8.51%</cell></row><row><cell>2009</cell><cell>0.00%</cell><cell>35.00%</cell><cell>10.00%</cell><cell>55.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,166.80,169.00,272.63,77.00"><head>Table 4 .</head><label>4</label><figDesc>The P@10 distribution of PRIS2 in the last two years.</figDesc><table coords="8,166.80,185.30,272.63,60.70"><row><cell></cell><cell>Best</cell><cell>Best~Median</cell><cell>Median</cell><cell>Others</cell></row><row><cell>2010</cell><cell>2 4.26%</cell><cell>29 61.70%</cell><cell>15 31.91%</cell><cell>1 2.13%</cell></row><row><cell>2009</cell><cell>5.00%</cell><cell>10.00%</cell><cell>80.00%</cell><cell>5.00%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,97.56,701.37,138.60,8.10"><p>http://nlp.stanford.edu/ner/index.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,97.56,711.81,188.60,8.10;3,167.52,173.44,32.26,8.96;3,167.52,189.03,214.17,8.96;3,167.51,204.63,53.39,8.96;3,167.51,220.23,260.95,8.96;3,167.51,235.83,56.14,8.96;3,167.51,251.42,236.12,8.96;3,167.51,267.02,36.93,8.96;3,167.51,282.62,73.45,8.96;3,167.51,298.22,39.68,8.96;3,167.52,313.84,35.02,8.96"><p>http://cogcomp.cs.illinois.edu/page/software_view/4 &lt;DOC&gt; &lt;DOCNO&gt;clueweb09-en0003-81-01650&lt;/DOCNO&gt; &lt;DOCHDR&gt; URL: http://shop.crackberry.com/content/smartphones/index.htm &lt;/DOCHDR&gt; &lt;TITLE&gt;Smartphone Connections -BlackBerry&lt;/TITLE&gt; &lt;TEXT&gt; Document content &lt;/TEXT&gt; &lt;/DOC&gt;</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="5,97.56,711.81,152.36,8.10;5,101.04,678.86,178.69,9.94;5,279.72,683.21,3.48,6.26;5,283.20,678.86,19.32,9.94;5,302.52,683.21,3.48,6.26;5,306.00,678.86,165.42,9.94"><p>http://en.wikipedia.org/wiki/Okapi_BM25 Given a query Q, containing keywords q 1 ,...,q n , the BM25 score of a document D is:</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.36,607.94,160.95,9.94" xml:id="b0">
	<monogr>
		<title level="m" coord="8,105.36,607.94,129.11,9.94">TREC Entity 2010 guidelines</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,106.32,623.54,415.95,9.94;8,90.00,639.14,432.20,9.94;8,90.00,654.74,25.76,9.94" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rianne</forename><surname>Kaptein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marijn</forename><surname>Koolen</surname></persName>
		</author>
		<title level="m" coord="8,253.93,623.54,268.35,9.94;8,90.00,639.14,397.11,9.94">Result Diversity and Entity Ranking Experiments: Anchors, Links, Text and Wikipedia. In proceedings of The Eighteenth Text REtrieval Conference</title>
		<meeting><address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,670.34,414.24,9.94;8,90.00,685.94,230.99,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,244.09,670.34,164.53,9.94">BUPT at TREC 2009: Entity Track</title>
		<author>
			<persName coords=""><forename type="first">Zhanyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongxin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,432.49,670.34,89.76,9.94;8,90.00,685.94,168.02,9.94">proceedings of The Eighteenth Text REtrieval Conference</title>
		<meeting>The Eighteenth Text REtrieval Conference<address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
