<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,94.27,72.35,421.19,16.84">Combination of evidence for effective web search</title>
				<funder ref="#_hYJhXQy">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.45,118.05,72.85,11.06"><forename type="first">Dong</forename><surname>Nguyen</surname></persName>
							<email>dongn@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Language Technologies Institute Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.41,118.05,68.85,11.06"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Language Technologies Institute Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,94.27,72.35,421.19,16.84">Combination of evidence for effective web search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">656E1896C2AF272FB38A288429F02998</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe Carnegie Mellon University's submission to the TREC 2010 Web Track. Our baseline run combines different methods, of which in particular the spam prior and mixture model were found the most effective. We also experimented with expansion over the Wikipedia corpus and found that picking the right Wikipedia articles for expansion can improve performance substantially. Furthermore, we did preliminary experiments with combining expansion over the Wikipedia corpus with expansion over the top ranked web pages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Carnegie Mellon University participated in the Ad Hoc task of the TREC 2010 Web track. Our experiments were carried out on the English subset of ClueWeb09 (Category A). We focused in particular on the ad hoc task, but also submitted to the diversity task. Our aim was to improve P@10 and MAP, we therefore did not employ special methods to improve the diversity.</p><p>We first investigate the effectiveness of different methods that have shown to work well in the past: priors, mixture model and the dependency model. These methods are then combined to provide a strong baseline. Next we experiment with different pseudo relevance feedback strategies. We explore expansion using the Wikipedia corpus and perform preliminary experiments to combine this with expansion over the top retrieved web pages.</p><p>We first describe related work and our retrieval framework. We then outline our submitted runs and present and discuss the results. The conclusion summarizes our findings and provides suggestions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The Web can be viewed as a graph where pages are connected through links. Both the connections as well as the text around these links (anchor text) differentiate web search from traditional text search and have been exploited in web search. The connections have been used to compute authority scores such as PageRank <ref type="bibr" coords="1,188.14,648.42,14.31,7.86" target="#b15">[16]</ref> and HITS <ref type="bibr" coords="1,249.13,648.42,13.49,7.86" target="#b9">[10]</ref>. Eiron and McCurley <ref type="bibr" coords="1,115.03,658.88,9.72,7.86" target="#b6">[7]</ref> performed an analysis of anchor text for web search. They found that anchor text behaves much like real world queries. Furthermore, they found that the homogeneity of results improved when using anchor text. The documents returned tended to focus on the most common meaning of the query.</p><p>Priors have shown to be very effective for web search. Not only authority priors such as PageRank, but also other kind of priors have been investigated in the past. In particular, a prior giving deeper urls less probability has shown to be effective in entry page search <ref type="bibr" coords="1,441.50,251.91,13.49,7.86" target="#b11">[12]</ref>, a common information need in web search, and in web search in general <ref type="bibr" coords="1,527.69,262.37,9.20,7.86">[9]</ref>. In the TREC 2009 Web Track, spam was a major issue. Experiments showed that applying a spam prior improved the performance of TREC 2009 Web Track's systems substantially <ref type="bibr" coords="1,340.58,304.21,9.20,7.86" target="#b4">[5]</ref>.</p><p>Web queries are often short and ambiguous. Therefore query expansion can help to increase performance. External expansion on a cleaner (e.g. Wikipedia) or larger (explored by <ref type="bibr" coords="1,330.63,356.52,9.97,7.86" target="#b5">[6]</ref>) dataset has proven to be effective in the past. In the TREC 2009 Web Track different approaches to expand queries were explored. Specifically, approaches expanding the query using external sources such as Wikipedia (University of Glasgow <ref type="bibr" coords="1,382.31,398.36,13.49,7.86" target="#b13">[14]</ref>, University of Amsterdam <ref type="bibr" coords="1,508.26,398.36,9.97,7.86" target="#b7">[8]</ref>) or commercial search engines (University of Waterloo <ref type="bibr" coords="1,515.41,408.82,14.32,7.86" target="#b16">[17]</ref>) were explored because the initial results can be very noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WEB TRACK 3.1 Tasks</head><p>The goal of the Web Track is to explore and evaluate Web retrieval technologies <ref type="bibr" coords="1,404.00,474.95,9.20,7.86" target="#b2">[3]</ref>. TREC 2010 Web Track contained three tasks: ad hoc, diversity and spam filtering. The ad hoc task ranks systems according to their performance based on manual relevance assessments. For every query, a specific information need was specified. For example, for 'iron' only pages about iron as an essential nutrient are considered as relevant for the ad hoc task. With the diversity task, the goal is to return a ranked list that provides a complete coverage of the query and avoids redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>The ClueWeb09 dataset contains about 1 billion web pages collected in January and February 2009. Systems can submit runs on category B (subset of 50 million documents) or category A (full dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>The ad hoc task is evaluated using expected reciprocal rank and standard measures such as P@10 and MAP. For the TREC 2009 Web Track, the MTC method was used to estimate the MAP. The diversity task is evaluated using measures such as intent aware ERR, Î±-nDCG and the noveltyand rank-biased precision (NRBP) measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RETRIEVAL FRAMEWORK</head><p>This section outlines the different retrieval components that we explored. Our final submitted runs combine these methods and are presented in the next section. We use the Lemur Project's search engine <ref type="bibr" coords="2,150.90,99.98,9.20,7.86">[1]</ref>. A stopword list is applied and words are stemmed with the Krovetz stemmer.</p><p>The results presented in this section are the results on the training set (Web Track 2009) and calculated using the Trec Eval program. Although in the Web Track 2009 evaluation MTC was used (eMAP and eP@10), we found the regular MAP and P@10 to be more robust when evaluating runs that were not included in the original pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Priors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Computing priors</head><p>We explore the effectiveness of three different priors: Page-Rank, Spam and an URL prior individually and combined with each other. For PageRank, we use the computed Page-Rank values provided by CMU <ref type="bibr" coords="2,182.02,263.69,9.20,7.86" target="#b1">[2]</ref>. For the Spam prior, we use the spam estimates made available by the University of Waterloo <ref type="bibr" coords="2,104.58,284.61,9.20,7.86" target="#b4">[5]</ref>. For every document, they provide an estimate of the percentile of documents in the corpus that are spammier than the particular document. The documents were mapped into two bins (percentile score less than or bigger than 50%) and log probabilities were computed for them. For the URL prior we followed the method described in Kamps et al. <ref type="bibr" coords="2,120.32,347.38,9.71,7.86">[9]</ref> (product squared variant).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Incorporating priors</head><p>Priors can be added in the Indri query language by adding the #PRIOR construct in the query. The following are two different variants for adding priors in the query. The first is the most straightforward way following the typical query likelihood model, by adding the priors directly in the query and treating it as a query term. In this way, the weight of the prior decreases when the query contains more terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#combine(#prior(PAGERANK) obama family tree)</head><p>When using multiple priors a composite prior is more suitable instead of adding all priors as query terms. Furthermore, a composite query can weight the composite prior and query terms. An example using two priors can be found below.</p><p>#weight( 0.2 #weight(0.1 #prior(PAGERANK) 0.9 #prior(SPAM2)) 0.8 #combine(obama family tree ) )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Results</head><p>In Table <ref type="table" coords="2,90.52,637.96,4.61,7.86" target="#tab_0">1</ref> we compare the different priors individually. We included the prior using the first method, because this required no parameter tuning but gave us a good way to assess the effect of the different priors. In our final submitted runs we use the second method to include a composite prior.</p><p>Table <ref type="table" coords="2,78.32,700.73,4.61,7.86" target="#tab_0">1</ref> shows that the Spam prior is the most effective, both in Precision at 10 and MAP, which can be explained by the high amount of spam normally dominating the results. The URL prior is also effective, perhaps partially because spam pages are often very deep and short URL pages are less likely to be spam. Surprisingly, the PageRank prior alone does not perform very well. Comparing the results of the baseline with and without a PageRank prior, it seems that some popular, but not so relevant sites are promoted too much with a PageRank prior. Furthermore, some other relevant pages such as Wikipedia pages get a lower rank, probably because that particular Wikipedia article is not linked to often. However this only means PageRank is not effective as the only prior, combining it with the other priors can have additional benefits. For example, a (tuned) combination of PageRank and the Spam prior gave better performance than the Spam prior alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mixture model</head><p>Our index contains title, inlink, heading, and document fields. In Table <ref type="table" coords="2,380.78,485.25,4.61,7.86">2</ref> the results per field are shown. We follow the Indri query reference to construct a mixture model<ref type="foot" coords="2,536.57,493.94,3.65,5.24" target="#foot_0">1</ref> , an example query is:</p><formula xml:id="formula_0" coords="2,316.81,533.07,230.23,28.39">#combine( #wsum( 5.0 espn.(title) 3.0 espn.(inlink) ) )</formula><p>After tuning the parameters in combination with the priors, we observed the best performance with title (0.1), inlink (0.2) and document (0.7).</p><p>Inlink has a very high precision, which was also observed by others <ref type="bibr" coords="2,344.54,631.89,13.49,7.86" target="#b10">[11]</ref>. However, the MAP is low compared with using the document field, because not all sites have inlinks. Furthermore when adding a spam prior, the precision for the document field improves substantially, while the inlink precision improves much less. This again indicates that inlink is resistant to spam most of the times, while the document field is very susceptible to spam. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sequential dependency model</head><p>We employ the dependency model as proposed by Metzler and Croft <ref type="bibr" coords="3,95.86,170.04,13.49,7.86" target="#b14">[15]</ref>. This model expands the original query with subqueries that add proximity constraints. Both ordered and unordered constraints are added. After parameter tuning, we observed that the unordered component was not effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Combining dependency &amp; mixture model</head><p>We combine the two models linearly by the #weight operator in Lemur and tune the weights using our training set.</p><p>#weight( 0.9 #combine( mixture model ) 0.1 #combine( dependency model ) )</p><p>In Table <ref type="table" coords="3,92.59,351.84,4.61,7.86" target="#tab_1">3</ref> the effectiveness of the mixture model, dependency model and their combination is presented. The baseline system of these runs uses the optimized priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Host collapse</head><p>We also experimented with host collapse: only allowing a certain number of pages of the same hosts. The results however were mixed. Although this decreases the risk of certain pages appearing numerous times in the results (for example with slight variants due to dynamic generated pages), this also collapses useful websites such as Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Query expansion</head><p>We use the Relevance Model (RM1) by Lavrenko and Croft <ref type="bibr" coords="3,53.80,507.72,14.32,7.86" target="#b12">[13]</ref> to expand the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Wikipedia expansion</head><p>Web pages are very noisy and even documents that are ranked highly in the results might not be suitable for query expansion. Therefore expansion over an external, cleaner corpus can give substantial performance benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages</head><p>Expansion over a Wikipedia corpus has several advantages over expansion using top ranked web pages. First, the corpus is very clean. Expanding over web pages has the risk of including noisy terms, such as spam terms or company names when searching for a commercial product (e.g. 'cheap internet'). Second, Wikipedia especially has a good coverage of named entities. Third, Wikipedia is objective. In contrary to expansion over the web, which has a risk of drifting to a particular viewpoint if the top-ranked pages are very biased (which we observed for example in the query 'Rick Warren'). Disadvantages However, expanding over Wikipedia also brings some disadvantages. Queries such as 'getting organized ', 'cheap internet', or 'travel information', are not well covered by Wikipedia. Furthermore, Wikipedia articles do not always represent what the average Web user might be interested in. For example, a user issuing the query Yahoo might be interested in its services (such as 'search', 'ask ', 'job' etc.), while expanding over Wikipedia might add terms related to the company itself, such as acquisition and its development over the years. And lastly, expanding over Wikipedia often adds Wikipedia specific terms such as 'article', 'edit' and 'Wikipedia' and thus biasing the results to Wikipedia pages which is sometimes not desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Picking Wikipedia articles for expansion</head><p>To overcome the noise that Wikipedia might bring in with queries for which Wikipedia is not suitable, we explored the following strategy. We only consider Wikipedia pages for expansion that appear in the top results (e.g. top 1000) of the main search. This reduces the risk of expanding over Wikipedia when Wikipedia pages do not match the query well. Furthermore, the number of documents to estimate the relevance model with is not fixed and depends automatically on the match between the Wikipedia articles and the query (although we do set a maximum number of pages to be included).</p><p>This strategy is compared with the baseline strategy, which searches in the Wikipedia corpus directly. The advantage the baseline brings is that it is possible to optimize the queries to search in the Wikipedia corpus. For example, we observed that the title extent is more effective in the Wikipedia corpus than in general web search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exact match</head><p>We also explore the strategy which we call 'exact match'. If the title of a Wikipedia article matches the query exactly and is not a disambiguation page, we only use that page to expand. This happens often with named entity queries (such as 'the secret garden' or 'starbucks'). However, when the query is ambiguous (such as 'kcs') no exact match is found and we use multiple pages. This strategy is often very accurate, because pages match only exactly when there is no ambiguity question or when there is a clear majority sense (for example for the query 'euclid ').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary variations</head><p>We explored the following strategies:</p><p>Expansion collection: Only use Wikipedia articles that have been returned in the top X results, or always expand a query by searching in the whole Wikipedia corpus.</p><p>Exact match: If the title of a Wikipedia article matches the query exactly, only use this page for expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Results are presented in Table <ref type="table" coords="3,439.80,679.80,3.58,7.86" target="#tab_2">4</ref>. We observe that both proposed strategies, 'exact match' and only using the Wikipedia articles retrieved in the top documents improve both MAP as well as P@10 significantly. Union: Expand by taking the union of the top terms of both expansion term sets.</p><p>Intersection: Expand by taking the intersection for both sets and averaging the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intersection</head><p>Using only terms that appear in the intersection yields the following advantages. If there is a mismatch between the pages returned by Wikipedia and Web expansion, (almost) no terms will be added. This thus decreases the risk of adding non-suitable terms. If there is a good match between two sets, more expansion terms are added. Furthermore, it automatically deletes terms that are noise (for example Wikipedia specific words such as 'edit', 'article' or noise from the Web expansion). However, the drawback is that if one of the expansion techniques performs poorly, (almost) no terms are added. Therefore difficult queries such as 'the music man' or 'kcs' are almost not expanded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Union</head><p>This approach takes the top terms from every query expansion. Weights are not modified, except if a term appears in both sets, the weight of that term is the sum of the weights in each set. This approach introduces more noise, but guarantees that every query will be expanded. Furthermore, terms are added even if a query expansion does not give good expansion terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples</head><p>In Table <ref type="table" coords="4,90.28,572.93,4.61,7.86" target="#tab_3">5</ref> two examples are presented of a (non optimized) run which takes the intersection. We see that for these queries the intersection removes almost all the noise. Note that the terms that appear in the intersection are not necessarily the terms that have the highest weight in the Wikipedia or web expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">Incorporating the query terms in the query</head><p>The expansion terms were added in our baseline model (as described in the next section) by weighting the original terms and the new expansion terms using the #weight operator. In preliminary experiments we experimented with weights 0.3, 0.5 and 0.7. The observed differences were small, therefore we continued with using 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">AD HOC: RESULTS &amp; DISCUSSION</head><p>Parameters were tuned using parameter sweeps with the data from Web Track 2009. For the computationally heavy parameter sweeps, we used a subset of 20 queries. We will discus results on the submitted runs for the Web Track of TREC 2009 and 2010. Results for 2010 are for 48 of the 50 topics, topics 95 and 100 have been dropped. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submission I: Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Discussion</head><p>Analyzing the queries for which our system performed poorly, we can make three observations. First, stop word removal definitely made some queries harder with the most extreme example the 2010 query 'to be or not to be that is the question' (only retaining 'question'). But even for more common queries stopword removal can make a big difference (for example 'music man' versus 'the music man' or 'wall ' versus 'the wall '). Second, stemming also decreased the performance heavily in some queries. For example for the 2010 query 'living in india', 'living' matched 'live' resulting in many results about live webcams, live sport results etc. A 2009 query example is 'the current' (with stemming 'currency' matched to the query). The last type of queries for which the performance was low was ambiguous queries, such as 'defender ' or 'kcs' or for 2010 the queries 'avp' or 'iron'. We found that 30 of the 48 queries were on or above median regarding P@10. For 2 queries it had the best P@10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Submission II: Wikipedia expansion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Run description</head><p>Our second submission only uses Wikipedia for query expansion. Only Wikipedia pages appearing in the top results (e.g. top 1000) are considered for expansion. Furthermore, we apply the 'exact match' strategy. We take the top 10 Wikipedia articles, extract 30 expansion terms and give the expansion query a weight of 0.5. We furthermore applied a stopword list with Wikipedia specific terms (such as 'edit').</p><p>Expansion terms are integrated in our baseline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Discussion</head><p>Compared to the baseline both MAP and P@10 increases a lot with this run. Examples for which the P@10 increased heavily are 'joints' (+0.8), 'korean language' (+1) and 'raffles' (+0.9). 'joints' and 'korean language' are two concepts both well represented by Wikipedia and the 'exact match' strategy was applied in these cases. 'raffles' is ambiguous, therefore multiple Wikipedia articles were used for expansion and terms such as 'hotel ','travel ' and 'singapore' were added, matching the right intent ('Find the homepage of Raffles Hotel in Singapore'). However, for some queries the performance also decreased. An example is 'discovery channel store' (-0.8), a typical example of queries for which the encyclopedic character of Wikipedia might not be suitable. 33 of the 48 queries were on or above median regarding P@10. For 10 queries it had the best P@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Submission III: Combining Wikipedia and Web expansion</head><p>Our third submission combines Wikipedia and expansion over the top retrieved web pages and uses the union approach. For our diversity track, we also submitted a run with the intersection approach and additional modifications (cmuComb10, see next section). However, because this run was also evaluated with the ad hoc measures, these are also reported in Table <ref type="table" coords="5,393.34,320.61,4.61,7.86" target="#tab_4">6</ref> and<ref type="table" coords="5,421.14,320.61,3.58,7.86" target="#tab_5">7</ref>. For Wikipedia expansion, we used the exact match strategy but searched in the Wikipedia corpus directly. We used the top 10 documents for both expansion methods and included the top 10 expansion terms of each method. We applied host collapse before doing expansion on the web documents. Due to time constraints, parameters for this run were not tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Discussion</head><p>Compared to only Wikipedia expansion, the performance is worse. A more effective approach could be to give Wikipedia terms more weight, or depending on the match of Wikipedia or web articles with the query to expand only using one method. Furthermore, simply leaving the weights unchanged might not be the most effective method. However, we did observe that some queries performed better than the baseline or Wikipedia expansion. For example 'sewing instructions' got a P@10 of 0.9 compared to 0.6 (baseline) and 0.1 (Wikipedia expansion). For this query, expansion over the web documents (adding 'manual ', 'book ', 'machine', 'pattern' etc.) gave more suitable terms than expansion over Wikipedia (adding 'home', 'company', 'style', 'gar ' etc.). On average, we found that the performance tended to be closer to the baseline than to Wikipedia expansion. 31 of 48 of the queries were on or above median regarding P@10. For 3 queries it had the best P@10.</p><p>For the intersection approach, the performance is also lower compared to Wikipedia expansion. Direct comparison with the union approach is not possible, because this run contains additional modifications. 35 of 48 of the queries were on or above median regarding P@10. For 4 queries it had the best P@10. This run has the highest number of queries on or above median, although the P@10 is much lower than the Wikipedia expansion run. This indicates that the Wikipedia run had a more extreme behavior (performing very well or very bad on some queries), which can be explained by the expansion approach. We submitted two runs that were also submitted to the ad hoc task: Wikipedia expansion and the combination of Wikipedia and web expansion. Our third run was also a combination of Wikipedia and Web expansion, but using the intersection method. Furthermore, for the third run we experimented with adding the expansion terms with a mixture model. In addition, when the query matched exactly with an Wikipedia article and the query contained articles (such as 'the'), we added all the expansion terms obtained by expansion over the Wikipedia corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Discussion</head><p>Results of the 2010 runs can be found in Table <ref type="table" coords="6,253.32,308.36,3.58,7.86" target="#tab_6">8</ref>. Unfortunately, it is difficult to draw hard conclusions, because the diversity of the queries also depends on the precision of the results. The Wikipedia run is the best according to all measures. However, this might be because Wikipedia has the highest precision. For some of the queries, adding expansion terms from both web and Wikipedia expansion improved diversity compared the baseline. For example for the query 'titan', the P@10 for the union approach is 0.3, the same as for the baseline. However, the Î±-nDCG@10 increases from 0.172 to 0.409. Added terms were: titan, nissan, internet, poker, review, luggage, cab, pickup, bed, tennessee, titan, teen, team, series, new, member, vol, comic, issue and 2008, covering a wide range of different topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper we described our participation in the TREC 2010 Web Track. We first explored several approaches that worked well in the past, such as priors, mixture model and dependency model. We found the most effective prior to be the spam prior, and the mixture model was more effective than the dependency model. The best baseline model combined all three components.</p><p>We observed that expansion over the Wikipedia corpus is very effective, dramatically increasing the performance over some queries for which the initial results were very poor. Furthermore, selecting Wikipedia articles for expansion is more effective when the documents that are picked appear high in the main search, instead of directly searching in the Wikipedia corpus. We also explored an approach that combines the query expansion over different collections (top results and Wikipedia). However, performance on average was lower than only using Wikipedia expansion, especially on queries for which the initial results were very poor.</p><p>For future work, we expect that a more sophisticated approach to combine Wikipedia expansion with expansion over the top documents can be more effective. For example, only relying on one expansion method when the expansion collection seem to match the query well, or giving a particular expansion method more weight.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,319.55,161.79,102.59,9.44;4,316.81,173.94,239.11,7.86;4,316.81,184.40,239.11,7.86;4,316.81,194.86,239.11,7.86;4,316.81,205.33,239.11,7.86;4,316.81,215.79,8.75,7.86;4,316.81,245.71,37.60,7.47;4,330.91,256.17,56.39,7.47;4,345.00,266.63,79.88,7.47;4,345.00,277.09,79.88,7.47;4,330.91,287.55,56.39,7.47;4,345.00,298.01,61.09,7.47;4,359.10,308.47,28.20,7.47;4,373.19,318.93,206.74,7.47;4,373.19,329.39,112.77,7.47;4,359.10,339.86,37.60,7.47;4,373.19,350.32,187.94,7.47;4,373.19,360.78,103.37,7.47;4,359.10,371.24,37.60,7.47;4,373.19,381.70,197.34,7.47;4,373.19,392.16,108.07,7.47;4,359.10,402.62,4.71,7.47;4,349.70,413.08,75.18,7.47;4,373.19,423.54,187.94,7.47;4,373.19,434.00,65.79,7.47;4,387.29,444.46,103.37,7.47;4,387.29,454.92,98.67,7.47;4,387.29,465.38,145.66,7.47;4,373.19,475.85,4.71,7.47;4,349.70,486.31,4.71,7.47;4,330.91,496.77,4.71,7.47;4,316.81,507.23,4.71,7.47"><head>5.1. 1</head><label>1</label><figDesc>Run descriptionOur final run makes use of the priors and combines the mixture and dependency model linearly. Note that this run does not contain PageRank and no unordered window with the dependency model. An example query of the baseline run is:#weight( 0.2 #weight( 0.9 #prior(SPAM2) 0.1 #prior(URL1)) 0.8 #weight( 0.9 #combine( #wsum( 0.1 milwaukee.(title) 0.2 milwaukee.(inlink) 0.7 milwaukee.(document) ) #wsum( 0.1 journal.(title) 0.2 journal.(inlink) 0.7 journal.(document) ) #wsum( 0.1 sentinel.(title) 0.2 sentinel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,316.81,64.30,239.11,208.27"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="2,316.81,64.30,239.11,208.27"><row><cell>: Priors calculated over all relevance assess-</cell></row><row><cell>ments of 2009.</cell></row><row><cell>Run MAP P@10</cell></row><row><cell>No prior 0.0647 0.1920</cell></row><row><cell>Spam 0.0745 0.2720</cell></row><row><cell>PageRank 0.0502 0.1820</cell></row><row><cell>Url 0.0657 0.2620</cell></row><row><cell>Table 2: Fields, calculated over all relevance assess-</cell></row><row><cell>ments of 2009.</cell></row><row><cell>Run MAP P@10</cell></row><row><cell>Title 0.0203 0.0880</cell></row><row><cell>Title, spam 0.0219 0.1040</cell></row><row><cell>Inlink 0.0291 0.2300</cell></row><row><cell>Inlink, spam 0.0224 0.2440</cell></row><row><cell>Heading 0.0086 0.0600</cell></row><row><cell>Heading, spam 0.0121 0.0820</cell></row><row><cell>Document 0.0535 0.1180</cell></row><row><cell>Document, spam 0.0712 0.2340</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.80,64.30,239.11,60.04"><head>Table 3 :</head><label>3</label><figDesc>Results dependency and mixture over all relevance assessments of 2009.</figDesc><table coords="3,98.60,83.57,149.50,40.77"><row><cell>Run MAP P@10</cell></row><row><cell>Dependency model 0.0627 0.2560</cell></row><row><cell>Mixture model 0.0751 0.3120</cell></row><row><cell>Combination 0.0881 0.3360</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,53.80,64.30,239.12,176.80"><head>Table 4 :</head><label>4</label><figDesc>Wikipedia expansion over all relevance assessments of 2009.</figDesc><table coords="4,53.80,83.57,239.12,157.53"><row><cell>Run MAP P@10</cell></row><row><cell>Only top documents, exact match 0.1399 0.4520</cell></row><row><cell>Top documents, no exact match 0.1169 0.3980</cell></row><row><cell>Whole wiki corpus, no exact match 0.1088 0.3540</cell></row><row><cell>4.6.2 Combining Wikipedia and expansion over top</cell></row><row><cell>retrieved Web pages</cell></row><row><cell>In this section we explore combining Wikipedia expansion</cell></row><row><cell>with expansion over the top retrieved web pages. Combining</cell></row><row><cell>relevance feedback over different document samples has been</cell></row><row><cell>investigated by Collins-Thompson and Callan [4]. However,</cell></row><row><cell>our document samples are not random, and are very differ-</cell></row><row><cell>ent in nature and quality from each other. We explore the</cell></row><row><cell>following strategies:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,59.18,64.30,521.01,144.97"><head>Table 5 :</head><label>5</label><figDesc>Expansion terms.</figDesc><table coords="5,59.18,74.85,521.01,134.41"><row><cell>Query Wikipedia expansion</cell><cell>Web expansion</cell><cell>Intersection</cell></row><row><cell>orange county center, convention, county, phase,</cell><cell>center, convention, orange, county,</cell><cell>convention, drive, new,</cell></row><row><cell>convention center orange, 2, space, ft, m, orlando, sq,</cell><cell cols="2">occc, hotel, feb, show, event, md, san, orange, florida, orlando, 1,</cell></row><row><cell>wikipedia, million, build, 1, tourist,</cell><cell>diego, international, rental, www,</cell><cell>county, occc, center</cell></row><row><cell>north, exhibition, florida, new,</cell><cell>1, com, new, service, florida, 4, 407,</cell><cell></cell></row><row><cell>state, complete, approve, occc,</cell><cell>near, net, 5, 00, home, restaurant,</cell><cell></cell></row><row><cell>tax, bcc, south, drive, january, 000</cell><cell>orlando, drive</cell><cell></cell></row><row><cell cols="2">dogs adoption animal, dog, greyhound, pet, adoption, dog, adoption, cat, shelter, rescue,</cell><cell>home, adopt, shelter, dog</cell></row><row><cell>wikipedia, race, shelter, group,</cell><cell>pet, event, animal, 1, adopt, ny,</cell><cell>adoption, pet, rescue</cell></row><row><cell>article, home, page, edit, adopt,</cell><cell cols="2">north, new, hempstead, home, 3, near, link, animal, care,</cell></row><row><cell>own, org, wiki, http, care, en,</cell><cell>york, washington, 4, 2, port, breed,</cell><cell>3</cell></row><row><cell>rescue, link, 3, state, need,</cell><cell>month, information, puppy, care,</cell><cell></cell></row><row><cell>category, free, work, marine, live</cell><cell>service, link, com</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,59.18,239.59,246.04,71.50"><head>Table 6 :</head><label>6</label><figDesc>Ad hoc: MAP and P@10.</figDesc><table coords="5,59.18,248.44,246.04,62.66"><row><cell>2009</cell><cell>2010</cell></row><row><cell cols="2">Run MAP P@10 MAP P@10</cell></row><row><cell cols="2">Baseline (cmuBase10) 0.0881 0.3360 0.0976 0.2833</cell></row><row><cell cols="2">Wikipedia (cmuWiki10) 0.1399 0.4520 0.1574 0.4208</cell></row><row><cell cols="2">Union (cmuFuTop10) 0.1040 0.3320 0.1177 0.3125</cell></row><row><cell cols="2">Int.+add. (cmuComb10) 0.1137 0.3780 0.1209 0.3250</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,61.80,334.88,223.10,71.50"><head>Table 7 :</head><label>7</label><figDesc>Ad hoc: ERR@20 and nDCG@20.</figDesc><table coords="5,61.80,343.73,223.10,62.66"><row><cell></cell><cell>2010</cell></row><row><cell cols="2">Run ERR@20 nDCG@20</cell></row><row><cell>Baseline (cmuBase10) 0.09130</cell><cell>0.14200</cell></row><row><cell>Wikipedia (cmuWiki10) 0.11206</cell><cell>0.21181</cell></row><row><cell>Union (cmuFuTop10) 0.10009</cell><cell>0.15825</cell></row><row><cell>Inters.+add. (cmuComb10) 0.09831</cell><cell>0.16899</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,53.80,64.83,423.65,110.35"><head>Table 8 :</head><label>8</label><figDesc>Diversity results for submitted runs (+ baseline) 2010.</figDesc><table coords="6,53.80,75.89,423.65,99.29"><row><cell cols="3">Run ERR-IA@20 Î±-nDCG@20 NRBP</cell><cell>MAP-IA</cell></row><row><cell>Baseline (cmuBase10) 0.201887</cell><cell>0.304172</cell><cell cols="2">0.163077 0.049794</cell></row><row><cell>Wikipedia (cmuWiki10) 0.248370</cell><cell>0.345176</cell><cell cols="2">0.214939 0.092600</cell></row><row><cell>Union (cmuFuTop10) 0.208400</cell><cell>0.309083</cell><cell cols="2">0.170817 0.062064</cell></row><row><cell>Inters.+add. (cmuComb10) 0.215057</cell><cell>0.323582</cell><cell cols="2">0.173160 0.064907</cell></row><row><cell>6. DIVERSITY: RESULTS &amp; DISCUSSION</cell><cell></cell><cell></cell></row><row><cell>6.1 Runs submitted</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,321.42,711.19,250.85,7.86"><p>http://www.lemurproject.org/lemur/IndriQueryLanguage.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">ACKNOWLEDGMENTS</head><p>We would like to thank <rs type="person">Liu Liu</rs> and <rs type="person">Minh Duong</rs> for providing the spam priors in the Lemur format. This paper is based on research funded by <rs type="funder">NSF</rs> grant <rs type="grantNumber">NSF EEC 0935127</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hYJhXQy">
					<idno type="grant-number">NSF EEC 0935127</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.30,249.28,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,273.33,171.11,7.86;6,335.61,283.79,201.03,7.86;6,335.61,294.25,108.68,7.86" xml:id="b1">
	<monogr>
		<ptr target="http://boston.lti.cs.cmu.edu/clueweb09/wiki/tiki-index.php?page=pagerank" />
		<title level="m" coord="6,335.60,273.33,164.98,7.86">PageRank ClueWeb09 provided by CMU</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,305.71,182.80,7.86;6,335.61,316.17,219.88,7.86;6,335.61,326.63,193.63,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,335.61,316.17,144.82,7.86">Overview of the trec 2009 web track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,498.82,316.17,56.66,7.86;6,335.61,326.63,165.35,7.86">Proceedings of the Eighteenth Text REtrieval Conference</title>
		<meeting>the Eighteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,338.08,211.06,7.86;6,335.61,348.55,204.96,7.86;6,335.61,359.01,179.81,7.86;6,335.61,369.47,205.73,7.86;6,335.61,379.93,198.56,7.86;6,335.61,390.39,175.66,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,484.61,338.08,62.05,7.86;6,335.61,348.55,189.33,7.86">Estimation and use of uncertainty in pseudo-relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,335.61,359.01,179.81,7.86;6,335.61,369.47,205.73,7.86;6,335.61,379.93,169.63,7.86">SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="303" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,401.85,215.71,7.86;6,335.61,412.31,220.31,7.86;6,335.61,422.77,193.83,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,335.61,412.31,220.31,7.86;6,335.61,422.77,72.66,7.86">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno>CoRR, abs/1004.5168</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,434.22,212.16,7.86;6,335.61,444.69,197.34,7.86;6,335.61,455.15,179.81,7.86;6,335.61,465.61,205.73,7.86;6,335.61,476.07,198.56,7.86;6,335.61,486.53,175.66,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,435.21,434.22,112.56,7.86;6,335.61,444.69,181.71,7.86">Improving the estimation of relevance models using large external corpora</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,335.61,455.15,179.81,7.86;6,335.61,465.61,205.73,7.86;6,335.61,476.07,169.63,7.86">SIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,497.99,216.84,7.86;6,335.61,508.45,201.90,7.86;6,335.61,518.91,220.06,7.86;6,335.61,529.37,211.78,7.86;6,335.61,539.83,200.77,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,459.01,497.99,93.44,7.86;6,335.61,508.45,56.28,7.86">Analysis of anchor text for web search</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Eiron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Mccurley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,410.37,508.45,127.14,7.86;6,335.61,518.91,220.06,7.86;6,335.61,529.37,207.95,7.86">SIGIR 2003: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="459" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,551.29,219.01,7.86;6,335.61,561.75,220.32,7.86;6,335.61,572.21,220.08,7.86;6,335.61,582.67,217.65,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,468.60,561.75,87.33,7.86;6,335.61,572.21,129.04,7.86">Heuristic ranking and diversification of web documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">J</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,483.75,572.21,71.93,7.86;6,335.61,582.67,150.07,7.86">Proceedings of the Eighteenth Text REtrieval Conference</title>
		<meeting>the Eighteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2010-02">February 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,594.12,201.13,7.86;6,335.61,604.59,218.93,7.86;6,335.61,615.05,193.61,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,498.26,594.12,38.48,7.86;6,335.61,604.59,143.59,7.86">Language models for searching in web corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,497.88,604.59,56.66,7.86;6,335.61,615.05,165.33,7.86">Proceedings of the Thirteenth Text REtrieval Conference</title>
		<meeting>the Thirteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,626.50,173.65,7.86;6,335.61,636.96,189.32,7.86;6,335.61,647.43,82.29,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,404.54,626.50,104.71,7.86;6,335.61,636.96,97.97,7.86">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,441.01,636.96,77.91,7.86">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,658.88,212.16,7.86;6,335.61,669.34,210.49,7.86;6,335.61,679.80,188.20,7.86;6,335.61,690.26,212.04,7.86;6,335.61,700.73,211.52,7.86;6,335.61,711.19,83.88,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,443.16,658.88,104.61,7.86;6,335.61,669.34,124.09,7.86">The importance of anchor text for ad hoc search revisited</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,477.92,669.34,68.18,7.86;6,335.61,679.80,188.20,7.86;6,335.61,690.26,212.04,7.86;6,335.61,700.73,46.67,7.86">Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2010</title>
		<meeting>eeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2010<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">July 19-23, 2010. 2010</date>
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,57.64,196.71,7.86;7,72.59,68.10,220.31,7.86;7,72.59,78.56,191.30,7.86;7,72.59,89.02,205.73,7.86;7,72.59,99.48,198.56,7.86;7,72.59,109.94,166.46,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,253.44,57.64,15.87,7.86;7,72.59,68.10,216.39,7.86">The importance of prior probabilities for entry page search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,84.09,78.56,179.81,7.86;7,72.59,89.02,205.73,7.86;7,72.59,99.48,169.63,7.86">SIGIR 2002: Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,121.40,189.97,7.86;7,72.59,131.86,210.52,7.86;7,72.59,142.32,220.06,7.86;7,72.59,152.78,211.78,7.86;7,72.59,163.24,200.77,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,197.16,121.40,65.40,7.86;7,72.59,131.86,64.47,7.86">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,155.97,131.86,127.14,7.86;7,72.59,142.32,220.06,7.86;7,72.59,152.78,207.95,7.86">SIGIR 2001: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,174.70,216.86,7.86;7,72.59,185.16,193.13,7.86;7,72.59,195.62,195.38,7.86;7,72.59,206.08,192.16,7.86;7,72.59,216.54,179.40,7.86;7,72.59,227.00,70.89,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,112.62,185.16,153.10,7.86;7,72.59,195.62,195.38,7.86;7,72.59,206.08,176.84,7.86">University of Glasgow at TREC 2009: Experiments with Terrier -Blog, Entity, Million Query, Relevance Feedback, and Web tracks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Iadh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">T S</forename><surname>Rodrygo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,72.59,216.54,179.40,7.86;7,72.59,227.00,42.61,7.86">Proceedings of the Eighteenth Text REtrieval Conference</title>
		<meeting>the Eighteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,238.46,212.32,7.86;7,72.59,248.92,184.04,7.86;7,72.59,259.38,205.15,7.86;7,72.59,269.84,208.98,7.86;7,72.59,280.30,217.55,7.86;7,72.59,290.76,72.87,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,190.77,238.46,94.14,7.86;7,72.59,248.92,114.26,7.86">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,205.64,248.92,50.99,7.86;7,72.59,259.38,205.15,7.86;7,72.59,269.84,208.98,7.86;7,72.59,280.30,85.82,7.86">SIGIR 2005: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,302.22,214.81,7.86;7,72.59,312.68,217.53,7.86;7,72.59,323.14,166.87,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="7,271.53,302.22,15.87,7.86;7,72.59,312.68,212.99,7.86">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="7,72.59,334.60,215.71,7.86;7,72.59,345.06,217.19,7.86;7,72.59,355.52,197.81,7.86;7,72.59,365.98,151.68,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,72.59,345.06,217.19,7.86;7,72.59,355.52,41.54,7.86">Experiments with clueweb09: Relevance feedback and web tracks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,132.49,355.52,137.90,7.86;7,72.59,365.98,84.11,7.86">Proceedings of the Eighteenth Text REtrieval Conference</title>
		<meeting>the Eighteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2010-02">February 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
