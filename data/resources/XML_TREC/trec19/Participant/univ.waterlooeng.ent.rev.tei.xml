<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,67.51,76.98,476.98,15.80;1,256.25,97.38,99.50,15.80">Related Entity Finding: University of Waterloo at TREC 2010 Entity Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,266.58,135.83,78.85,9.64"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
							<email>ovechtom@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,67.51,76.98,476.98,15.80;1,256.25,97.38,99.50,15.80">Related Entity Finding: University of Waterloo at TREC 2010 Entity Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E609856C5847189B26EA0E2BCF8CEB7C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The University of Waterloo participated in the Related Entity Finding task of the Entity track. Our goal is to investigate whether related entity finding problem can be addressed by unsupervised approaches that rely primarily on statistical methods and common linguistic tools, such as named-entity taggers and syntactic parsers. We approach the related entity finding problem by first retrieving documents in response to the query, and extracting an initial set of candidate entities from the text of the documents. As a separate step, we automatically construct a set of seed entities, which represent hyponyms of the target entity category specified in the narrative, and then rank the candidate entities by their similarity to the seeds. An example of the target entity category name is "authors", extracted from the narrative "Authors awarded an Anthony Award at <ref type="bibr" coords="1,113.91,365.99,91.05,9.64">Bouchercon in 2007</ref><ref type="bibr" coords="1,219.47,365.99,68.63,9.64">Bouchercon in " (2009 topic #14) topic #14)</ref>. The system extracts category names from the free-text narrative, finds seed entities belonging to each category, and computes the similarity of candidate entities to the seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>Figure <ref type="figure" coords="1,101.14,443.51,5.40,9.64" target="#fig_0">1</ref> provides an overview of the main components of our system. In the first stage (rectangle 1 in Figure <ref type="figure" coords="1,100.28,456.23,3.96,9.64" target="#fig_0">1</ref>), the system retrieves an initial set of documents for the query from the Web. Only the sentences containing query terms plus one preceding/following sentence are retained. Named Entity tagging is applied to these sentences, and candidate entities are extracted and ranked. In the second stage, the target category name is automatically identified from the topic narrative, and in stage 3 the system finds hyponyms of this category name, and selects seed entities from the hyponyms. In stage 4, the entities (candidates and seeds) are represented as vectors of weighted grammatical dependency relations, and pairwise (candidate-seed) similarity is calculated. In stage 5, candidate entities are ranked by similarity to all seeds. Stage 1 is described in Section 2.1, while stages 2-5 are presented in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extracting candidate entities</head><p>As the first step, the queries to retrieve top documents from the Web are generated from the "entity name" and "narrative" sections of the Entity track topics according to the algorithm in Figure <ref type="figure" coords="1,451.17,599.27,4.10,9.64">2</ref>. The objective is to extract named entities and other noun phrases from the topic. For this purpose we use a Part-Of-Speech tagger <ref type="bibr" coords="1,98.72,624.71,53.31,9.64" target="#b0">(Brill, 1995)</ref>, a Noun Phrase chunker <ref type="bibr" coords="1,266.30,624.71,131.53,9.64" target="#b5">(Ramshaw and Marcus, 1995)</ref>, and a list of titles of Wikipedia pages. The resulting queries are then used to retrieve the top 50 documents from a Web search engine. Our motivation to use 50 is to keep the number of documents for subsequent in-depth analysis reasonably small, and at the same time have sufficient amount of text to extract entities from. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Algorithm for processing queries</head><p>The retrieved 50 documents are parsed to remove HTML tags, script and style sections, and broken into sentences. We then extract sentences that contain at least one query term. If a query term is a noun, the system attempts to match its singular and plural forms. For each such sentence, we also extract one sentence before and one after. Let this set of sentences be {S}. The sentences are then processed by the LBJbased Named Entity Recognizer <ref type="bibr" coords="2,215.30,671.27,112.17,9.64" target="#b6">(Ratinov and Roth, 2009)</ref>. The NER tagger only assigns the categories of "Location", "Organization", "Person" and "Miscellaneous". In the Entity track 2010, the target entity type specified in the topic is one of the following: "organization", "person", "location" and "product". For topics belonging to the first three types, we extract all entities tagged with the corresponding labels. However, for topics of category "Product" we extract entities labeled as "Organization" and "Miscellaneous".</p><p>Having extracted candidate entities from the top 50 documents retrieved for each topic, we then rank them by TF*IDF, where TF is the frequency of the entity in the sentence set {S} extracted from the top 50 retrieved documents, and IDF is calculated by using the number of documents containing the entity in the ClueWeb09 Category B collection. Since the TF*IDF-ranked list can be quite large and may contain a lot of noise, we select the top 200 entities for the use in subsequent stages. In the following sections we will refer to this ranked list of 200 entities as "candidate entities".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ranking candidate entities by the similarity to the target entity category</head><p>Since the chosen NER tagger can only be used to identify entities of a few broad categories, such as organisations and people, the list of candidate entities can be noisy. This is further compounded by the NER tagger errors. To refine the list of entities, we decided to investigate the applicability of the distributional similarity principle, which is based on the observation that semantically close words occur in similar contexts. If we know a small number of correct seed entities, we can rank the candidate entities by the distributional similarity to them. Many methods reported in the literature that utilised the distributional similarity principle are semi-supervised methods, such as <ref type="bibr" coords="3,331.49,276.71,49.27,9.64" target="#b4">(Lin, 1998)</ref>. These start with the list of known seed words and then find other words that are distributionally similar, and therefore, likely to be semantically close to the seed words. The problem in the Related Entity Finding task is that the seed words are not given. However, the topic narratives have descriptions of the categories of entities that are to be retrieved. Our approach is to generate a list of seed entities based on the given entity category names. For example, the narrative of the 2009 training topic #3 is "Motorsport series that Bridgestone officially supports with tyres." We developed a method to extract the category name from the narrative, i.e. "motorsport series" in this topic, and adapted a method for the automatic acquisition of the hyponymy relation proposed by <ref type="bibr" coords="3,165.92,377.75,62.22,9.64" target="#b1">Hearst (1992)</ref> to find entities that belong to the category (in this case "motorsport series"), which we then use as seed entities. Furthermore, a new method was developed to compute the distributional similarity between seed entities and candidate entities, and rank the entities by similarity to all seed entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Extracting category names from topic narratives</head><p>To extract category names, the narratives are first processed using Brill's Part-of-Speech (POS) tagger <ref type="bibr" coords="3,68.16,457.67,55.97,9.64" target="#b0">(Brill, 1995)</ref> and a Noun-Phrase chunker <ref type="bibr" coords="3,256.77,457.67,133.64,9.64" target="#b5">(Ramshaw and Marcus, 1995)</ref>. Then a set of rules is applied to select one of the initial noun phrases (NPs) from the narrative. Generally, the first noun phrase in the narrative is selected as the category name, unless it is a personal pronoun or the noun "searcher". Other rules include splitting a NP containing conjunction (e.g. "and", "/", "or") into two or more NPs (see example for 2009 topic #4 in Table <ref type="table" coords="3,236.11,508.31,3.95,9.64" target="#tab_0">1</ref>). For each topic, six queries are constructed using the above templates and the category name is extracted from the topic narrative. For example, the query for the first template in Table <ref type="table" coords="4,415.47,385.91,5.40,9.64" target="#tab_1">2</ref> is: "operating systems such as". Each query is submitted to one of the commercial search engines as a phrase (i.e. quote-delimited). If the total number of pages retrieved by all six queries is fewer than 10, the first word in the category name is dropped and the search is repeated. If again it returned fewer than 10 pages, the first two words are dropped, and so on until either 10 or more pages are retrieved, or the remaining category name consists of only a single word, in which case we use whatever number of pages were found. If a category name is a single word, the query includes the topic title in addition to the template, in order to minimise the extraction of unrelated entities.</p><p>The documents retrieved for each query are processed to remove HTML tags, and split into sentences. The sentences containing the hyponymy lexico-syntactic patterns are then processed using the LBJ-based NER tagger <ref type="bibr" coords="4,99.04,516.23,112.36,9.64" target="#b6">(Ratinov and Roth, 2009)</ref>. Depending on the expected position of hyponyms in the lexico-syntactic pattern, NEs either immediately preceding, or following the pattern are extracted. If several NEs are used conjunctively, i.e., separated by a comma, "and" or "or", all of them are extracted. For each topic, we extract only NEs with tags corresponding to the entity type specified in the topic, i.e. NEs tagged with "ORG", "PER", "LOC" and ("MISC"|"ORG") are extracted for topics with entity types "organization", "person" "location" and "product" respectively. Below is an example of the output of the NER tagger for topic #7:</p><p>"…on some modern multi-user operating systems such as Since, the entity type for the 2009 training topic #7 is "product", the following entities are extracted as hyponyms of "operating system": "Windows XP", "Windows Vista", "Mac OS X", "Ubuntu" and "Fedora". In this sentence the tagger erroneously tagged "OpenSUSE" as person, therefore it is not extracted.</p><p>One problem with using all found hyponyms as seed entities is that they can be unrelated to the topic. Some category names extracted from topic narratives are very broad, such as "products". Applying the above hyponym acquisition algorithm based on such high-level hypernym is bound to produce a very large number of topically-unrelated hyponyms. Computing distributional similarity between candidate entities and these hyponyms is likely to be ineffective and possibly detrimental to performance. We therefore must ensure that we use only those hyponyms as seeds, for which there exists some evidence of relationship to the topic. For this purpose, we defined as seeds the intersection of found hyponyms and entities extracted from the top 50 documents retrieved for the initial query as described in Section 2.1. For example, for topic #7, the following hyponyms were identified as seeds: "FreeBSD", "Mac", "Linux", "Ubuntu", "Windows XP", "Microsoft Windows", "Unix", "Microsoft". If only one seed word is identified as a result of this process, we do not perform entity re-ranking on this topic, and keep the original rank order, i.e. by TF*IDF. We believe that one seed entity is an insufficient representation of the target entity category, and may substantially degrade performance if it is incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Computing distributional similarity between candidate and seed entities.</head><p>Distributional similarity between entities is computed based on how similar their contexts of occurrence are in text. In their simplest form, contexts could be words extracted from windows around entity occurrences. Alternatively, they could be grammatical dependency relations, with which an entity occurs in text. The use of grammatical dependency relations is more constraining in calculating entity similarity, and allows us to identify entities belonging to the same semantic category. For example, grammatical dependency relations in Table <ref type="table" coords="5,107.26,293.99,5.40,9.64">3</ref> tend to occur with noun phrases referring to sports people.</p><p>Table <ref type="table" coords="5,202.85,310.71,4.10,9.48">3</ref>. Examples of grammatical dependency relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammatical dependency relation Source sentence win V:subj:N Rubens Barrichello</head><p>Rubens Barrichello won the Italian Gran Prix. teammate N:appositive:N Rubens Barrichello His teammate, Rubens Barrichello, managed second with a 1:35.681 lap at the end of the session.</p><p>Since we are interested in identifying entities that are of the same semantic category as the seed words, we decided to use grammatical dependency relations for calculating entity similarity. For each seed entity we retrieve 200 documents from ClueWeb09 Category B using BM25 <ref type="bibr" coords="5,366.90,436.31,104.99,9.64" target="#b7">(Robertson et al., 1995)</ref> implemented in Wumpus<ref type="foot" coords="5,107.76,448.20,3.00,5.36" target="#foot_0">1</ref> . Each document is split into sentences, and sentences containing the entity are parsed using the Minipar<ref type="foot" coords="5,103.68,460.92,3.00,5.36" target="#foot_1">2</ref> syntactic parser <ref type="bibr" coords="5,185.11,461.75,51.46,9.64" target="#b2">(Lin, 1993)</ref> to extract grammatical dependency triples. Each dependency triple consists of two words/phrases and a grammatical relation that connects them (see examples in Table <ref type="table" coords="5,532.15,474.23,3.95,9.64">3</ref>). These dependency triples are transformed into features representing the context of each entity. To transform a triple into a feature, we replace the entity name in the triple with 'X', e.g., "win V:subj:N Rubens Barrichello" is transformed into "win V:subj:N X". To avoid using features that are specific to only one or few seed entities, only features that co-occur with at least 50% of all seed entities are retained. For each seed entity we build a vector consisting of these features and frequencies of their co-occurrence with this entity in the 200 retrieved documents.</p><p>A vector for each of the 200 candidate entities obtained in Stage 1 (Section 2.1) is generated following the same procedure as above for seed entities. Each vector consists only of those features (and frequencies of their co-occurrence with the candidate entity) that co-occur with at least 50% of all seed entities.</p><p>To compute the similarity between the vectors of seeds and candidate entities, we use BM25 with query weights. For each seed and candidate entity we calculate a query adjusted combined weight, QACW <ref type="bibr" coords="5,68.16,634.07,116.62,9.64" target="#b7">(Spärck Jones et al., 2000)</ref>. In the QACW formula, the vector of the seed entity is treated as the query and the vector of the candidate as the document:</p><formula xml:id="formula_0" coords="6,161.82,74.36,249.32,77.67">€ QACW c,s = TF(k 1 + 1) K + TF ⋅QTF ⋅ IDF f f =1 F ∑ (1)</formula><p>Where: F -the number of features that a candidate entity c and a seed entity s have in common; TFfrequency of feature f in the vector of candidate entity; QTF -frequency of feature f in the vector of the seed entity; K = k 1 *((1-b)+b*DL/AVDL); k 1 -feature frequency normalisation factor; b -document length normalisation factor; DL -number of features in the vector of the candidate entity; AVDL -average number of features in all candidate entities.</p><p>In order to calculate the IDF of a feature, we need to have access to a large syntactically parsed corpus, such as the ClueWeb09 Category B collection. Since we do not have such a resource, and it is computationally demanding to produce one, we approximate IDF of a feature with the IDF of its component word. For example, for the feature "win V:subj:N X" we calculate the IDF of "win" by using its document frequency in the ClueWeb09 Category B collection.</p><p>Arguably, when calculating the similarity of candidate entities to seed entities, we should take into account how strongly each seed entity is associated with the original TREC topic. Candidate entities similar to those seed entities, which have weak association with the topic, should be downweighted compared to those candidate entities, which are similar to seed entities strongly associated with the topic. We quantify this association by using the TF*IDF weight of the seed entity calculated as described in Section 2.1. Thus, the candidate entity matching score with all seed entities is calculated according to:</p><formula xml:id="formula_1" coords="6,167.08,331.52,237.06,80.25">€ EntitySeed c = TFIDF s × QACW c,s s=1 S ∑<label>(2)</label></formula><p>Only the entities with an EntitySeed weight of greater than zero are retained. The final score for a candidate entity is calculated as a linear combination of the candidate entity's TF*IDF and EntitySeed weights, as defined in the following equation:</p><formula xml:id="formula_2" coords="6,130.28,416.13,309.70,51.70">€ TFIDFEntitySeed c = β × TFIDF c + (1-β) × EntitySeed c (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>The runs submitted are UWAT1, which uses the TF*IDF method (Section 2.1), and UWAT2, which uses the seed similarity method described in Section 2.2.</p><p>Since the requirement of the Related Entity Finding task in 2010 is to return one homepage for each retrieved entity, we developed a simple homepage finding algorithm, which consists of retrieving the top 10 webpages for each entity from a major commercial Web search engine, filtering out a small number of common URLs, such as "dictionary.com", "facebook.com", "linkedin.com", and using as a homepage the top ranked page that exists in the ClueWeb09 Category A collection. The results are summarised in Table <ref type="table" coords="6,68.16,558.95,4.10,9.64" target="#tab_2">4</ref>, and the NDCG@R performance difference of UWAT2 from UWAT1 by topic is shown in Figure <ref type="figure" coords="6,512.40,558.95,4.10,9.64" target="#fig_2">3</ref>.  As can be seen from Table <ref type="table" coords="7,196.00,480.95,4.10,9.64" target="#tab_2">4</ref>, re-ranking and pruning candidate entities by calculating their similarity to seed entities improves performance significantly. Figure <ref type="figure" coords="7,338.41,493.67,5.40,9.64" target="#fig_2">3</ref> demonstrates that most of the topics are improved by using the re-ranking process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper we describe our approach to the Related Entity Finding task of the Entity track of TREC 2010. The method relies primarily on statistical and linguistic methods. In our approach, candidate entities are extracted using a NER tagger from the documents retrieved in response to the query. As a separate step a set of seed entities is generated by finding hyponyms of the entity category name given in the narrative. The candidate entities are then compared to the seeds by the similarity of the grammatical dependency relations they co-occur with. Entities that have a higher similarity to a larger number of seeds are ranked higher in the final list. The results suggest that when we apply the entity re-ranking by similarity to seed entities, we get a statistically higher performance compared to when only the first stage of entity ranking (by TF*IDF) is performed. A possible future extension of this work is bootstrapping additional seeds from the initial set of seeds identified using the hyponymy method described here.</p><p>* and ** are statistically significant at 0.05 and 0.01 levels (2-tailed paired t-test). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,169.84,340.23,272.48,9.48;2,125.64,72.10,362.70,261.20"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Components of the related entity finding system.</figDesc><graphic coords="2,125.64,72.10,362.70,261.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,327.23,608.87,216.81,9.64;4,68.16,621.59,339.64,9.64"><head></head><label></label><figDesc>[MISC Windows XP] , [MISC Windows Vista] , [ORG Mac OS X] , [PER OpenSUSE] , [ORG Ubuntu] and [ORG Fedora] ."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,143.65,447.75,324.79,9.48"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Difference of UWAT2 from UWAT1 in NDCG@R by topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,68.16,508.31,475.77,210.28"><head>Table 1 . Examples of NP-chunked narratives and extracted category names (2009 training topics)</head><label>1</label><figDesc>Table1lists examples of NP-chunked narratives of the training topics and the extracted category names. Identifying seed entities After the category name is extracted from the topic narrative, the next step is to find entities that belong to this category. We adapted the unsupervised hyponymy acquisition method proposed by<ref type="bibr" coords="4,478.35,116.87,61.08,9.64" target="#b1">Hearst (1992)</ref>. Hearst's method consists of using six domain-and genre-independent lexico-syntactic templates that indicate a hyponymy relation. The templates and examples of sentences found for the 2009 training topic #7 are shown in Table2.</figDesc><table coords="3,70.08,554.79,468.02,151.08"><row><cell>Topic</cell><cell>NP-chunked narrative</cell><cell>Extracted category</cell></row><row><cell></cell><cell></cell><cell>name</cell></row><row><cell>4</cell><cell>[ 7.30/CD report/NN presenters/NNS and/CC reporters/NNS ] ./.</cell><cell>7.30 report presenters</cell></row><row><cell></cell><cell></cell><cell>7.30 report reporters</cell></row><row><cell>5</cell><cell>[ Members/NNS ] of/IN [ John/NNP Key's/NNP cabinet/NN ] ./.</cell><cell>members</cell></row><row><cell>6</cell><cell>[ What/WP ] [ products/NNS ] did/VBD [ Nokia/NNP ] sell/VB</cell><cell>products</cell></row><row><cell></cell><cell>throughout/IN [ its/PRP$ history/NN ] ,/, [ that/WDT ] are/VBP still/RB</cell><cell></cell></row><row><cell></cell><cell>sold/VBN [ today/NN ] ?/.</cell><cell></cell></row><row><cell>7</cell><cell>[ I'd/NNP ] like/VB to/TO find/VB [ which/WDT ] [ operating/VBG</cell><cell></cell></row><row><cell></cell><cell></cell><cell>fashion labels</cell></row></table><note coords="3,107.76,670.55,299.37,9.64;3,107.76,683.27,35.99,9.64;3,442.56,657.83,79.06,9.64;3,70.08,696.23,10.90,9.64;3,107.76,696.23,304.13,9.64;3,107.76,708.95,124.56,9.64"><p>systems/NNS ] [ I/PRP ] can/MD use/VB on/IN [ the/DT EEE/NNP PC/NN] operating systems 10 [ Which/WDT fashion/NN labels/NNS ] belong/VBP to/TO [ the/DT LVMH/NNP group/NN ] ?/.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,71.52,171.51,466.52,181.56"><head>Table 2 . Hearst's patterns for hyponym acquisition Template Examples based on training topic #7</head><label>2</label><figDesc>NP such as {NP,}* {(or|and)} NP …on some multi-user operating systems such as Windows XP, Windows Vista, Mac OS X, OpenSUSE, Ubuntu and Fedora. such NP as {NP,}* {(or|and)} NP Experienced in such operating systems as MS-DOS, Windows, Windows NT/2K/XP/2K3, Solaris, Linux, FreeBSD. NP {, NP}*{,} or other NP If you want to run Windows, Linux, or other operating systems on… NP {, NP}*{,} and other NP PostScript font installation -Unix, Linux and other operating systems. NP{,} including {NP ,}* {or | and} NP We will cover each of the major operating systems, including DOS, Windows 9x/NT/2000/XP, and UNIX/Linux. NP {,} especially {NP,}* {or | and} NP …but the program is used mainly on other operating systems, especially Linux.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,102.96,75.03,404.24,107.16"><head>Table 4 . Results on the 50 topics.</head><label>4</label><figDesc></figDesc><table coords="7,102.96,94.71,404.24,87.48"><row><cell>Run</cell><cell>nDCG@R</cell><cell>P10</cell><cell>MAP</cell><cell>RPrec</cell><cell>Rel. Retr.</cell><cell>Pri. Retr.</cell></row><row><cell>UWAT1 (TF*IDF)</cell><cell>0.1264</cell><cell>0.0957</cell><cell>0.0608</cell><cell>0.1033</cell><cell>95</cell><cell>151</cell></row><row><cell>UWAT2 (TFIDFEntitySeed)</cell><cell>0.1393 ** (10.2%)</cell><cell>0.1106 (15.6%)</cell><cell>0.0722 * (18.8%)</cell><cell>0.1223 (18.4%)</cell><cell>96</cell><cell>154</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,73.91,697.67,142.33,9.64"><p>http://www.wumpus-search.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,73.91,710.15,220.91,9.64"><p>http://webdocs.cs.ualberta.ca/~lindek/minipar.htm</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,68.16,140.63,473.41,9.64;8,68.16,153.35,310.65,9.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,135.37,140.63,406.20,9.64;8,68.16,153.35,95.69,9.64">Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,171.11,153.35,115.46,9.64">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="543" to="565" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,68.16,169.91,456.61,9.64;8,68.16,182.63,459.72,9.64;8,68.16,195.11,19.20,9.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,163.17,169.91,260.69,9.64">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,443.12,169.91,81.66,9.64;8,68.16,182.63,251.05,9.64">Proceedings of the 14th Conference on Computational Linguistics -Volume</title>
		<meeting>the 14th Conference on Computational Linguistics -Volume<address><addrLine>Nantes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-08-23">1992. August 23 -28, 1992</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,68.16,211.91,468.58,9.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,134.46,211.91,216.24,9.64">Principle-Based Parsing Without OverGeneration</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,370.30,211.91,102.16,9.64">Proceedings of ACL-93</title>
		<meeting>ACL-93</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="112" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,68.16,224.39,69.31,9.64" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">H</forename><surname>Columbus</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,68.16,241.19,471.60,9.64;8,68.16,253.91,466.43,9.64;8,68.16,266.39,39.36,9.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,134.46,241.19,223.21,9.64">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,377.30,241.19,162.47,9.64;8,68.16,253.91,228.74,9.64">Proceedings of the 17th international Conference on Computational Linguistics -Volume</title>
		<meeting>the 17th international Conference on Computational Linguistics -Volume<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-08-10">1998. August 10 -14, 1998</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,68.16,283.19,470.71,9.64;8,68.16,295.67,247.97,9.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,228.24,283.19,237.51,9.64">Text Chunking Using Transformation-Based Learning</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,473.25,283.19,65.62,9.64;8,68.16,295.67,242.25,9.64">Proceedings of the Third ACL Workshop on Very Large Corpora, MIT</title>
		<meeting>the Third ACL Workshop on Very Large Corpora, MIT</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,68.16,312.47,458.47,9.64;8,68.16,324.95,423.63,9.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,206.26,312.47,303.92,9.64">Design Challenges and Misconceptions in Named Entity Recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,68.16,324.95,418.10,9.64">Proceedings of the Annual Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Annual Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,68.16,341.75,442.57,9.64;8,68.16,354.23,462.43,9.64;8,68.16,366.95,19.20,9.64;8,68.16,383.75,457.18,9.64;8,68.16,396.23,452.00,9.64;8,68.16,408.95,116.35,9.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,418.57,341.75,74.60,9.64;8,320.45,383.75,204.89,9.64;8,68.16,396.23,189.50,9.64">A probabilistic model of information retrieval: Development and comparative experiments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hankock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M ; Spärck</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,144.85,354.23,226.39,9.64">Proceedings of the Third Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Third Text Retrieval Conference<address><addrLine>Gaithersburg, U.S.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995. 2000</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="809" to="840" />
		</imprint>
	</monogr>
	<note>NIST. Part 1. Part 2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
