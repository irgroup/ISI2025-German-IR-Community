<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.04,164.95,321.17,15.49;1,285.85,186.86,39.54,15.49">The Melbourne Team at the TREC 2010 Legal Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-07-01">July 1, 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.25,219.36,78.58,10.76"><forename type="first">William</forename><surname>Webber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.30,219.36,60.59,10.76"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RMIT University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.34,219.36,66.69,10.76"><forename type="first">Mingfang</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RMIT University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.49,219.36,74.03,10.76"><forename type="first">Xiuzhen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RMIT University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,143.14,233.30,82.89,10.76"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.49,233.30,59.92,10.76"><forename type="first">Phil</forename><surname>Farrelly</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Potter Farrelly Consulting</orgName>
								<address>
									<addrLine>5 Mallesons Stephen Jaques 6 Wombat Technology</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.88,233.30,64.75,10.76"><forename type="first">Sandra</forename><surname>Potter</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Potter Farrelly Consulting</orgName>
								<address>
									<addrLine>5 Mallesons Stephen Jaques 6 Wombat Technology</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.10,233.30,58.29,10.76"><forename type="first">Steven</forename><surname>Dick</surname></persName>
						</author>
						<author>
							<persName coords="1,270.33,247.25,66.09,10.76"><forename type="first">Phill</forename><surname>Bertolus</surname></persName>
						</author>
						<title level="a" type="main" coord="1,145.04,164.95,321.17,15.49;1,285.85,186.86,39.54,15.49">The Melbourne Team at the TREC 2010 Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-07-01">July 1, 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">8024203EE1F4852B5259B6BD6F69060A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Overview</head><p>The Melbourne team was a collaboration between academic and industry groups. The team participated in both the learning and the interactive tasks of this year's Legal Track. The baseline run for the learning track employed true-relevance feedback, achieving respectable outcomes; the experimental runs added additional features and employed an SVM classifier, with poor results. The techniques developed for the learning task were then deployed in the interactive task. The classifier again achieved poor predictive quality, although final results place our production (non-significantly) first. We describe the learning task efforts in Section 2, and the interactive task in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Task</head><p>The team submitted three runs to the learning task. The first of these, rmitindA, built a query from topic keywords and performed retrieval using the BM25 similarity metric, along with true-relevance feedback (TRF) on the relevant seed documents. The remaining two runs trained an SVM classifier, using the TRF score as one amongst of seven features. One run, rmitmlfT, used all seven of the features; the other, rmitmlsT, used for each topic the set of features (except all features, or the TRF feature alone) which achieved the highest mean AUC on a ten-fold cross-validation.</p><p>Our philosophy this year was "we're not here to win; we're here to learn"; our results show that we have achieved at least the first of these objectives. The baseline TRF run performed respectably, coming at or above the median hypothetical F1 score for all but one topic. Our machine classification runs, though, performed poorly, often well below the TRF run, which is surprising, given that the TRF score was a feature. We also tried using Mechanical Turk for relevance assessment. The intention was to train a run with Turker-assessed documents added to the seed set. The Turkers performed our tasks very poorly, however, with 87% of them failing a simple trap question, making their output unusable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Method</head><p>We describe the baseline true-relevance feedback run first, then the experimental machine classification runs built on top of it. We also describe our unsuccessful attempt at using Mechanical Turk to produce seed documents for a run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>True relevance feedback</head><p>Indexing and searching for the TRF run were carried out using the Lemur toolkit.<ref type="foot" coords="2,473.49,487.02,3.49,6.28" target="#foot_0">1</ref> Stopwords were removed, and words were stemmed using the Porter stemmer. The Okapi BM25 model was used for matching and ranking documents. Keywords from the topic were used as the query. For feedback, all relevant documents from a topic's seed set were taken as positive examples, and the top 100 terms were added to the query. Unranked documents were assigned a minimal fixed score, and appended to the ranked search result. Result similarity scores were then normalised linearly to the range [0, 1], to estimate probability of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine classifier</head><p>The other two learning runs used an SVM classifier, employing the SVM perf <ref type="bibr" coords="2,434.59,617.49,42.88,8.97;2,133.77,629.44,23.23,8.97" target="#b0">[Joachims, 2006]</ref>  Ten-fold cross-validation was performed on the seed documents to determine the best feature subsets for each topic. Cross-validation results, expressed as area under the ROC curve (AUC), are given in Table <ref type="table" coords="3,298.71,420.10,3.73,8.97" target="#tab_1">2</ref>. The rmitmlfT run used all seven features, while the rmitmlsT run used the feature subset giving the best AUC values, excluding the set of all features and the set containing only feature 7, the TRF score. The features selected for rmitmlsT are reported in column 3 of Table <ref type="table" coords="3,360.75,455.98,3.73,8.97" target="#tab_1">2</ref>. As the table shows, adding features supplementary to the TRF score generally improved cross-validation accuracy on the seed documents, often by a wide margin; however, the team's results on the official relevance judgments failed to replicate this improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mechanical Turk</head><p>We intended to use Mechanical Turk (MT) to create one of our runs. A sample of documents were submitted to MT for relevance assessment; the plan was to use the assessments gathered to train a separate or enhance an existing machine classifier run. Failing that, the MT assessments could at least be used to calibrate the probability of relevance values assigned to submitted documents. The results gathered from MT were, however, too low in quality even for the latter usage.</p><p>Our human intelligence tasks (HITs) for MT were designed as follows. A topic statement was presented to the Turker, accompanied by six documents (emails or textversion attachments). The Turker was required to assess the relevance of each document to the topic. For each document, the Turker had the choice of Highly Relevant, Relevant, Irrelevant, and Not in English. In fact, almost all of the documents in the Enron corpus are in English. To set up a trap question, however, one of the six documents in every set was an email in German, taken from a technical mailing list. If the Turker failed to identify this document as not being in English, the HIT was rejected.</p><p>Unfortunately, the rejection rate turned out to be very high: 87% of HITs failed the trap question, worse even than would occur by purely random clicking. Such a high failure rate questions the quality of the few HITs that passed the trap question, too. As a result, we judged the Mechanical Turk assessments to be unusable. Our experience of Mechanical Turk is an interesting one. How can such a high failure rate occur on such an easy trap question? Was the payment offered too low (2 cents per hit)? Was the task too easy to automate (only buttons needed to be pressed)? Or did the task require too much attention from the Turkers?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Results</head><p>The hypothetical F1 scores, based upon an optimal cutoff depth, for the Melbourne runs are given in Table <ref type="table" coords="4,225.10,507.97,3.73,8.97">3</ref>, along with the best, median, and worst hypothetical F1 scores across all participants. It is evident, first, that the TRF run performed slightly above the median for most topics; and second, that the machine classification runs generally performed worse than the TRF ones, even though they used TRF scores as a feature. We investigate the poor performance of the classifier in Section 2.3</p><p>Learning task runs had to include a probability of relevance for each document. Relevance estimates for the TRF run were formed by linearly scaling retrieval similarity scores to the [0, 1] range; the resulting probabilities of relevance greatly overestimate yield. The classifier runs used the method proposed by <ref type="bibr" coords="4,390.64,603.61,48.07,8.97" target="#b1">Platt [1999]</ref> to derive probabilities from classifier predictions using cross-validation. The resulting sum of probabilities, however, was much too high. The sampling of seed documents appears to have been strongly biased in favour of relevant documents. If the sampling design had been available, it should have been possible to correct the estimates accordingly. In the absence of sampling information, the solution adopted was to scale probabilities by the number of relevant documents found in last year's interactive task, for which reason we designate the classifier runs as technology-assisted, rather than fully automated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Result analysis</head><p>The most striking result from our submission is the poor performance of the machine classifier runs. Cross-validation on the seed documents suggested that the addition of these features to the TRF score would in most cases lead to an improvement in accuracy (Table <ref type="table" coords="5,199.72,448.20,3.60,8.97" target="#tab_1">2</ref>). The AUC scores for some topics were above 0.9, suggesting very accurate classification. Furthermore, training and cross-validating using the official assessments, instead of the seed documents, produces comparable AUC scores, as the first two columns of Table <ref type="table" coords="5,243.43,484.07,4.98,8.97" target="#tab_3">4</ref> attest. If, however, we take the model developed on the seed documents, and use it to rank first the seeds and then the official assessments, we see a sharp fall in scores on official compared to seed assessments, as shown in the third and fourth columns of Table <ref type="table" coords="5,269.20,519.93,3.73,8.97" target="#tab_3">4</ref>. This fall suggests that the seed documents may be inaccurate examples for the official assessments. Evidence of seed bias is found in the low level of agreement between the seed and official assessors, on documents assessed by both. The second last column of Table <ref type="table" coords="5,472.50,555.80,4.98,8.97" target="#tab_3">4</ref> provides the Cohen's κ scores for this agreement (a κ of 1 means perfect agreement, whereas a κ of 0 means only random agreement), while the last column gives the proportion of assessments the two groups agreed on. Agreement ranges from fair to random to (in the case of Topic 206) almost adversarial. Such low agreement between seed data and official assessments suggests that this is a challenging data set on which to train a classifier, and indeed our classifier achieves its best results (Table <ref type="table" coords="5,433.36,627.53,4.15,8.97">3</ref>) on those topics (namely Topics 202 and 205) for which seed accuracy is the highest.</p><p>Seed bias cannot, however, be the only cause of poor classifier performance, since the classifier performed lamentably on some topics that have tolerable seed-qrel agree- ment, most notably Topics 200 and 207. Indeed, the classifier achieved an official AUC of 0.28 on Topic 207, meaning that it performed worse than a random ordering of documents.<ref type="foot" coords="6,190.64,406.20,3.49,6.28" target="#foot_1">2</ref> Moreover, the true-relevance feedback run itself learns from the seed documents, and was able to do so without a general calamitous drop in performance.</p><p>Part of the classifier's failure to use seed documents to predict official assessments may lie in the features or classifier used. Consider the recall-precision curve over assessed documents for Topic 200, shown in Figure <ref type="figure" coords="6,351.48,455.59,3.73,8.97" target="#fig_0">1</ref>. The classifier pushes a few relevant documents to the top of the list, but then precision falls precipitously, only to start rising again further down the ranking. Such a result is suggestive of over-fitting: a few relevant documents mislead the classifier into promoting a largely irrelevant class.</p><p>A likely culprit for such over-fitting is Feature 3 of Table <ref type="table" coords="6,388.83,503.41,3.73,8.97" target="#tab_0">1</ref>, the proportion of a custodian's seed documents which are relevant. Indeed, the upper ranks of the run displayed in Figure <ref type="figure" coords="6,200.20,527.32,4.98,8.97" target="#fig_0">1</ref> are dominated by the emails of a particular custodian, a few of which are relevant, the rest not; meanwhile, a quarter of the way down the ranking, a larger cluster of relevant emails from another custodian are located. Perhaps the former custodian has only relevant instances in the seed set, and the latter only irrelevant ones. Such an error could be caused by misleading assessments by the seed assessor; but it could equally be caused by the biased way in which the seed documents were selected, from runs returned by previous participants, not at random from the collection as a whole. The resulting over-fitting would not be caught by standard ten-fold cross-validation, which assumes that the seed examples are accurate and unbiased. The textual features used in true-relevance feedback may be more robust to errors of this sort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Interactive task</head><p>The Melbourne team also participated in this year's interactive task, submitting a run for Topic 302. The run was developed using a combination of human-directed Boolean keyword search and machine classification. Classifier effectiveness in locating relevant documents was low, however, perhaps in part because relevant documents were few. Appeals were selected by a three-fold review of conflicting assessments; this review offers insights into assessor agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method</head><p>A commercial e-discovery tool was used to perform keyword searches, browse the corpus, and tag documents as relevant or irrelevant. The assessed documents were then fed as seed documents to the classifier, using all the features listed in Table <ref type="table" coords="7,470.01,281.21,3.73,8.97" target="#tab_0">1</ref>. The output of the classifier was used to select new documents for assessment, while human-directed searches continued in parallel.</p><p>The original intention was to assess documents the classifier found ambiguous; under the SVM model, documents sitting close to the separating hyperplane. In practice, though, the classifier had low precision even amongst the top-ranked documents, and greatly overpredicted the proportion of relevant documents. We judged it unlikely that documents nominally close to the hyperplane (with a prediction score near 0.0) were truly ambiguous. <ref type="foot" coords="7,201.88,375.28,3.49,6.28" target="#foot_2">3</ref> Instead, the top few hundred unassessed documents were selected from the classifier for assessment at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Table <ref type="table" coords="7,158.56,436.01,4.98,8.97" target="#tab_4">5</ref> gives the precision of the classifier's top-n documents at each iteration of the run development, as measured by the team's own relevance assessments. Even though we were selecting the documents with the highest relevance prediction from the classifier, precision was low, ranging from 5% to 16%. The variation in precision at each iteration, though borderline significant (χ 2 = 12.2, p = 0.06), is likely due to using different assessors. The low precision of the top-n results is despite the fact that the classifier was giving a positive relevance prediction to roughly a quarter of the corpus.</p><p>Human-directed searches, primarily by iteratively refined Boolean queries, were carried out in parallel to the classifier runs. Precision cannot meaningfully be compared between human-directed and classifier searches, as the methods and aims differ. Nevertheless, the human-directed searches were not producing many new relevant documents. (The jump in the total number of relevant documents between September 8th and September 14th is due in part to reassessment of earlier results.)</p><p>A more controlled comparison between human-directed and classifier search was carried out as part of the September 13th iteration. Three query documents, addressing three different aspects of the topic, were created by extracting extended sections of highly relevant text from assessed relevant documents. These query documents were then submitted as queries to the BM25 system, without relevance feedback or the use The undeduplicated top n emails and top n attachments were taken from the classifier at each iteration. "Num" is the number selected, after deduplication, and "Rev" the number reviewed. "Rel" is the number found relevant, "TA" the number referred to the TA, and "!Rel" the number found irrelevant (a document could, erroneously, be assigned to more than one reviewed category). The precision at each iteration is given in the final column. The rows labelled "Incident", "Prevent", and "Other" give results for three query documents lodged on September 13; see the text for further explanation. The "All" rows give totals for their respective dates, including results from human-directed searches.</p><p>of machine classification. The top n results for each of these artificial query documents are shown in the eighth through tenth rows of Table <ref type="table" coords="8,346.46,492.65,3.73,8.97" target="#tab_4">5</ref>. The manually created queries were even less successful in locating relevant documents than the machine classifier. We originally intended to rank documents using the classifier, then truncate the ranking at a cutoff based on cross-validation or manual sampling. In the event, however, the classifier proved to be too inaccurate. Even at the top of the ranking, only 10% of documents were relevant; therefore, accepting unreviewed documents on the classifier's recommendation would damage our run's precision. Therefore, our final submission consisted solely of documents that had been manually reviewed and assessed as relevant by us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interim assessments</head><p>Our final submission consisted of 326 officially deduplicated documents, across 265 distinct messages. As our classifier was still yielding around 8% relevant documents in the top 100 at each iteration, we were confident that there were more relevant doc- Table <ref type="table" coords="9,159.07,340.77,3.87,8.97" target="#tab_8">7</ref>: Estimated, pre-appeal (row 1) and post-appeal (row 2) effectiveness scores for the Melbourne interactive run, and post-appeal rank of Melbourne run amongst six participants for the topic (row 3).</p><p>uments in the collection, but we believed that the number was not large. The official, post-appeal assessments found 326 relevant documents in the assessment sample; the official estimate for the population has not been provided, but (inferring from our scores the known data) is somewhere around 1,000.</p><p>The document-level agreement on the assessment sample between our returned run and the interim, pre-appeal assessments is shown in Table <ref type="table" coords="9,372.83,457.93,3.73,8.97" target="#tab_5">6</ref>. Agreement is only fair (κ = 0.35), although our failing to return a document should not be taken as an explicit judgment that the document was not relevant. The interim results confirmed our finding that there are few relevant documents for the topic large. At the message level, the sampled assessment estimates that there are 740 relevant messages in the collection, and that our run located 59 of them. The estimated effectiveness scores from the interim assessments are shown in the first row of Table <ref type="table" coords="9,323.12,529.66,3.73,8.97" target="#tab_8">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Appeals</head><p>There were altogether 387 document-level assessments that disagreed with our run: 235 documents we did not return were assessed relevant, and 152 documents we returned were assessed irrelevant. To determine which assessments to appeal, we performed a multi-assessor, blind re-assessment of the conflicting documents, on the basis of the topic authority's guidelines to the assessors. Seven of our team members took part in this review. Each document was randomly assigned to three different reviewers. Order of review was randomized, and reviewers were not told what the original return status or assessment of the document were.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interim assessment</head><p>Proportion reviewed as relevant For instance, the first row, fourth column shows that 27 of our appeals against interim assessments of "irrelevant" (0) were upheld by the topic authority. The -1 assessment means "unassessable"; we appealed no assessments of this type.</p><formula xml:id="formula_0" coords="10,292.89,146.14,80.70,9.96">0 1/3 1/2 2/</formula><p>The agreement between the threefold reviewers and the original assessors for each conflicting document is shown in Table <ref type="table" coords="10,296.52,472.69,3.73,8.97" target="#tab_6">8</ref>. The reviewers disagreed with relevant assessments more often than they did with irrelevant ones. The review result was taken as the majority assessment of the three assessors for a document; if this result disagreed with the official assessors, the document was appealed. Document-level assessments were not appealed, however, if the result would not change message-level assessments; for instance, we did not appeal relevant assessments for message bodies if we agreed that an attachment to the message was relevant.</p><p>Altogether, some 146 of the 387 document-level assessments that conflicted with our run were appealed, making up 106 of the 281 conflicting message-level assessments. The adjudication results of these appeals is reported in Table <ref type="table" coords="10,414.93,580.28,3.73,8.97" target="#tab_7">9</ref>. A surprising proportion (12%) of documents initially found assessable by the interim assessors were marked unassessable by the topic authority. Excluding these documents, 77% of our appeals against irrelevant judgments were upheld, as were 62% of our appeals against relevant judgments.</p><p>Our final, post-appeal scores is shown in the second row of precision. The relative showings are surprising: we only submitted documents that we had manually reviewed, which should boost precision at the cost of recall. These results suggest that other teams were even more conservative in their productions. In any case, the wide errors bounds on all scores (as reported in the official results) make it difficult to state which participants had conclusively superior productions. Part of the motivation for determining appeals by a randomized, multi-assessor review was to measure inter-assessor agreement. Table <ref type="table" coords="11,356.08,382.25,9.96,8.97" target="#tab_9">10</ref> shows the κ between each reviewer. Contested-relevance message bodies with agreed-relevance attachments are excluded, since these bodies are frequently near-empty, and reviewer agreement on rejecting a relevant assessment is misleadingly high. Three reviewers were from industry (though none had legal training), while four were from academia. Inter-reviewer agreement is variable, ranging from near-random at 0.05, to moderate at 0.52; there is no clear pattern of homogeneity within industry or academic reviewer groups. These agreement figures need to be interpreted with some care: on the one hand, the reviewers all took part in run development, and so might be expected to have a more coherent conception of relevance; on the other, though, the documents assessed are those whose relevance is contested, and may therefore be more difficult to determine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analysis</head><p>It appears that Topic 302 was a topic for which there were, in e-discovery terms, relatively few relevant documents. Even so, the performance of the machine classifier is disappointing. Only 10% of the top-ranked unassessed documents were actually relevant, and this proportion neither increased as the classifier was trained, nor decreased as the pool of relevant documents was depleted. The poor accuracy of the classifier meant that it could not be used to automatically classify relevant documents, but was only usable as a means of suggesting documents for manual review.</p><p>While the poor precision of the classifier's top ranks might be attributable to the sparsity of relevant documents, the waywardness of the classifier's relevance predictions is not. As mentioned previously, the classifier consistently gave a positive predic-tion to a quarter of the documents in the corpus, meaning that it regarded them as more likely to be relevant than not. It is possible that this error is due to the nature of the training instances. These instances were far from randomly chosen. Instead, they consisted on the one hand of a large number of irrelevant documents (see the bottom rows of Table <ref type="table" coords="12,168.44,175.88,3.60,8.97" target="#tab_4">5</ref>), belonging mostly to a few clearly irrelevant classes (for instance, historical calendar appointments regenerated as emails when Enron migrated mail servers); and on the other hand of a much smaller number of documents, chosen for review because of strong apparent evidence of relevance, though not in fact with a high proportion actually relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary</head><p>A collaborative team of academic and industry participants, based in Melbourne, took part in both the interactive and the learning tasks of this year's Legal Track. The core retrieval method was a true-relevance feedback (TRF) run using the BM25 retrieval model. The TRF result was then supplemented with six other features, and an SVM classifier trained.</p><p>Three runs were submitted to the learning task. The first, using only the TRF scores, performed respectably. The other two, using machine classification based on the TRF score plus all or a subset of the other features, performed much worse, despite promising AUC scores under cross-validation. An analysis suggests, first, that the seed documents are unreliable indicators of officially assessed relevance, and second, that the features chosen for the classifier were not robust to seed bias.</p><p>A single run was submitted for Topic 302 of the interactive task. The process was a blend of machine classification and of human-directed search using a commercial e-discovery tool. The classification method was the same as for the learning task, namely true-relevance feedback supplemented with other features and used to train an SVM classifier. Even at the top of its ranking, the classifier returned a low proportion of relevant documents, perhaps due in part to there being few relevant documents in the collection; additionally, the classifier grossly overpredicted the prevalence of relevance. A multi-assessor review process was employed to determine appeals, showing variable agreement between reviewers. Nevertheless, our interactive run achieved (non-significantly) top scores in recall and F1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,133.77,338.43,343.72,8.97;6,133.77,350.38,328.51,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Recall-precision curve for the full-feature SVM run on Topic 200, using the official qrels, and considering only those documents contained in the official qrels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,143.04,130.05,325.17,157.95"><head>Table 1 :</head><label>1</label><figDesc>Features used in the classifier runs.</figDesc><table coords="2,143.04,130.05,325.17,133.52"><row><cell cols="2">Id Name</cell><cell>Description</cell></row><row><cell>1</cell><cell>isAttach</cell><cell>Is the document an attachment (rather than an email)?</cell></row><row><cell>2</cell><cell>hasContent</cell><cell>Is the document (email body) non-empty?</cell></row><row><cell>3</cell><cell cols="2">custodianRel What proportion of seed documents from the custodian are</cell></row><row><cell></cell><cell></cell><cell>relevant?</cell></row><row><cell>4</cell><cell>relWithin</cell><cell>How many seed documents within t = 7 days are relevant?</cell></row><row><cell>5</cell><cell cols="2">externalProp What proportion of participants in an email are from non-</cell></row><row><cell></cell><cell></cell><cell>Enron addresses?</cell></row><row><cell>6</cell><cell>numRecip</cell><cell>How many recipients does the email have?</cell></row><row><cell>7</cell><cell>trfScore</cell><cell>What score does the document receive under Okapi-based</cell></row><row><cell></cell><cell></cell><cell>True Relevance Feedback?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,159.75,629.44,317.73,8.97"><head>Table 2 :</head><label>2</label><figDesc>implementation. Seven features, summarized in Table1, were implemented for Features selected for the rmitmlst run for each topic, with their AUC scores under ten-fold cross-validation, plus the AUC score of using the TRF feature by itself. The classifier did not converge for the TRF score feature alone on Topics 206 and 207.</figDesc><table coords="3,167.93,130.05,275.39,121.89"><row><cell cols="2">Topic Mnemonic</cell><cell>Subset features</cell><cell cols="3">Subset Full AUC AUC AUC TRF</cell></row><row><cell>200</cell><cell>Houses</cell><cell>3,4,7</cell><cell>0.72</cell><cell cols="2">0.71 0.63</cell></row><row><cell>201</cell><cell>Prepay transactions</cell><cell>1,2,3,4,5,7</cell><cell>0.91</cell><cell cols="2">0.92 0.62</cell></row><row><cell>202</cell><cell>FAS 140/125</cell><cell>3,5,7</cell><cell>0.91</cell><cell cols="2">0.91 0.79</cell></row><row><cell>203</cell><cell>Financial forecasts</cell><cell>3,5,7</cell><cell>0.83</cell><cell cols="2">0.79 0.79</cell></row><row><cell>204</cell><cell>Document shredding</cell><cell>3,5,7</cell><cell>0.82</cell><cell cols="2">0.79 0.83</cell></row><row><cell>205</cell><cell>Energy forecasts</cell><cell>3,5,7</cell><cell>0.90</cell><cell cols="2">0.90 0.85</cell></row><row><cell>206</cell><cell>Analyst reports</cell><cell>3,5,7</cell><cell>0.97</cell><cell>0.96</cell><cell>-</cell></row><row><cell>207</cell><cell>Fantasy football</cell><cell>3,5,7</cell><cell>0.95</cell><cell>0.92</cell><cell>-</cell></row></table><note coords="3,133.77,336.42,343.72,8.97;3,133.77,348.37,343.70,8.97;3,133.77,360.33,343.70,8.97;3,133.77,371.62,343.70,9.96;3,133.77,384.24,223.52,8.97"><p>the classifier. Other, possibly more powerful features were planned, but were not implemented due to lack of time. The count-based features 4 and 6, and the real-valued feature 7, were transformed by adding one and taking the natural logarithm. All feature values were normalized to the [-1, 1] range, based on the maximum and minimum unnormalized scores observed on the training examples.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,133.77,130.05,343.71,198.95"><head>Table 4 :</head><label>4</label><figDesc>Tenfold cross-validation AUC values for the full feature set classifier on the seed documents and on the official qrels (left); AUC results for ranking the assessed seed and qrel documents using a model trained on the seed documents (middle); and agreement between seed and qrel assessors for commonly assessed documents, measured by Cohen's κ and accuracy (agreed assessments as proportion of all) (right).</figDesc><table coords="5,155.43,130.05,300.38,126.70"><row><cell>Topic</cell><cell cols="2">Cross-validation</cell><cell cols="2">Ranking</cell><cell cols="2">Seed-qrel agreement</cell></row><row><cell></cell><cell>Seeds</cell><cell>Qrels</cell><cell cols="2">Seeds Qrels</cell><cell>κ</cell><cell>Accuracy</cell></row><row><cell>200</cell><cell>0.71</cell><cell>0.82</cell><cell>0.73</cell><cell>0.66</cell><cell>0.30</cell><cell>0.71</cell></row><row><cell>201</cell><cell>0.92</cell><cell>0.77</cell><cell>0.91</cell><cell>0.69</cell><cell>0.06</cell><cell>0.50</cell></row><row><cell>202</cell><cell>0.91</cell><cell>0.96</cell><cell>0.91</cell><cell>0.94</cell><cell>0.34</cell><cell>0.92</cell></row><row><cell>203</cell><cell>0.79</cell><cell>0.89</cell><cell>0.81</cell><cell>0.68</cell><cell>0.19</cell><cell>0.60</cell></row><row><cell>204</cell><cell>0.79</cell><cell>0.79</cell><cell>0.83</cell><cell>0.60</cell><cell>0.31</cell><cell>0.71</cell></row><row><cell>205</cell><cell>0.90</cell><cell>0.90</cell><cell>0.90</cell><cell>0.86</cell><cell>0.17</cell><cell>0.84</cell></row><row><cell>206</cell><cell>0.96</cell><cell>0.96</cell><cell>0.98</cell><cell>0.77</cell><cell>0.02</cell><cell>0.19</cell></row><row><cell>207</cell><cell>0.92</cell><cell>0.92</cell><cell>0.93</cell><cell>0.58</cell><cell>0.14</cell><cell>0.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,133.77,130.05,324.80,230.50"><head>Table 5 :</head><label>5</label><figDesc>Results of classifier runs at different dates.</figDesc><table coords="8,310.85,130.05,101.52,8.97"><row><cell>Proprietary deduplication</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,133.77,130.05,343.69,195.58"><head>Table 6 :</head><label>6</label><figDesc>Agreement between the submitted Melbourne run and the official, pre-appeal assessments on the documents included in the assessment sample. The 238 documents found unassessable by the assessors are excluded.</figDesc><table coords="9,194.62,130.05,222.01,195.58"><row><cell>Returned</cell><cell cols="2">Assessed</cell><cell></cell></row><row><cell></cell><cell cols="2">Relevant Not relevant</cell><cell>Total</cell></row><row><cell>Returned</cell><cell>113</cell><cell>152</cell><cell>265</cell></row><row><cell>Not returned</cell><cell>235</cell><cell>11,542</cell><cell>11,777</cell></row><row><cell>Total</cell><cell>348</cell><cell>11,694</cell><cell>12,042</cell></row><row><cell>Stage</cell><cell cols="3">Precision Recall F1</cell></row><row><cell>Pre-appeal</cell><cell>0.22</cell><cell cols="2">0.080 0.120</cell></row><row><cell>Post-appeal</cell><cell>0.45</cell><cell cols="2">0.200 0.277</cell></row><row><cell>֒→ Rank</cell><cell>4</cell><cell>1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,133.77,146.14,343.72,207.88"><head>Table 8 :</head><label>8</label><figDesc>Agreement between reviewers and assessors on the documents whose assessment disagreed with our run. Each cell counts the number of the 387 reviewed documents falling into that category. In the columns, we count the proportion of the three reviewers who regarded the document as relevant; this excludes reviews of "unassessable", making the proportion of 1/2 possible. Documents officially assessed as "unassessable" were likewise excluded from the review process.</figDesc><table coords="10,373.58,146.14,35.34,9.96"><row><cell>3</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,133.77,379.44,343.69,20.93"><head>Table 9 :</head><label>9</label><figDesc>Adjudication outcome for appealed documents. Counts are for documents appealed by the Melbourne team.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,133.77,640.06,343.71,32.88"><head>Table 7 .</head><label>7</label><figDesc>Success in appeals appreciably boosted our scores, though we lack the data to say what boost other teams received. We finished first out of six participants in recall and F1, but fourth in</figDesc><table coords="11,197.78,129.38,215.66,86.70"><row><cell>Reviewer</cell><cell>I 2</cell><cell>I 3</cell><cell>A 1</cell><cell>A 2</cell><cell>A 3</cell><cell>A 4</cell></row><row><cell>I 1</cell><cell cols="6">0.42 0.51 0.05 0.37 0.09 0.32</cell></row><row><cell>I 2</cell><cell></cell><cell cols="5">0.13 0.21 0.31 0.21 0.41</cell></row><row><cell>I 3</cell><cell></cell><cell></cell><cell cols="4">0.13 0.52 0.36 0.42</cell></row><row><cell>A 1</cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.22 0.22 0.16</cell></row><row><cell>A 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.45 0.49</cell></row><row><cell>A 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="11,133.77,240.51,343.71,45.50"><head>Table 10 :</head><label>10</label><figDesc>Inter-reviewer agreement (Cohen's κ) between the seven reviewers for conflicting assessments. Reviewers labelled I are from industry, A from academia. Reviews of messages where we disagree with a relevant assessment on the body, but agree that there is a relevant attachment, are excluded.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,148.11,648.82,90.96,7.17"><p>http://www.lemurproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,148.11,654.29,329.37,7.17;6,133.77,663.75,139.26,7.17"><p>The official AUC score is estimated over unassessed documents, whereas the AUC score in column 4 of Table4only considers assessed documents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="7,148.11,658.61,326.80,7.17"><p>These problems may have been due to our use of AUC, instead of loss, as the SVM objective function.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,133.77,571.67,343.70,8.07;12,143.73,582.63,333.75,8.07;12,143.73,593.58,333.75,8.07;12,143.73,604.55,48.31,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,211.90,571.67,133.20,8.07">Training linear SVMs in linear time</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,329.07,582.63,148.41,8.07;12,143.73,593.58,197.70,8.07">Proc. 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dimitrios</forename><surname>Gunopulos</surname></persName>
		</editor>
		<meeting>12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08">August 2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,133.77,623.47,343.70,8.07;12,143.73,634.43,333.74,8.07;12,143.73,645.40,329.64,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,187.08,623.47,290.39,8.07;12,143.73,634.43,68.06,8.07">Probabilistic outputs for support vector machines and comparison to regularized likelihood methods</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,221.44,645.40,134.56,8.07">Advances in Large Margin Classifiers</title>
		<editor>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
