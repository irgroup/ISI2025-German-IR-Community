<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.69,116.95,299.98,12.62;1,249.67,134.89,116.03,12.62">UCD SIFT in the TREC 2010 Web Track: Notebook Paper</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,209.30,172.56,62.75,8.74"><forename type="first">David</forename><surname>Leonard</surname></persName>
							<email>david.leonard@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.17,172.56,64.40,8.74"><forename type="first">Lusheng</forename><surname>Zhang</surname></persName>
							<email>lu-sheng.zhang@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.87,172.56,49.77,8.74"><forename type="first">David</forename><surname>Lillis</surname></persName>
							<email>david.lillis@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.82,184.51,59.76,8.74"><forename type="first">Fergus</forename><surname>Toolan</surname></persName>
							<email>fergus.toolan@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.57,184.51,67.34,8.74"><forename type="first">Rem</forename><forename type="middle">W</forename><surname>Collier</surname></persName>
							<email>rem.collier@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.55,184.51,61.99,8.74"><forename type="first">John</forename><surname>Dunnion</surname></persName>
							<email>john.dunnion@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.69,116.95,299.98,12.62;1,249.67,134.89,116.03,12.62">UCD SIFT in the TREC 2010 Web Track: Notebook Paper</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F4913D02EF9FDE666930AC18CFDB273B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The SIFT (Segmented Information Fusion Techniques) group in UCD is dedicated to researching Data Fusion in Information Retrieval. This area of research involves the merging of multiple sets of results into a single result set that is presented to the user. As a means of both evaluating the effectiveness of this work and comparing it against other retrieval systems, the group entered Category B of the TREC 2010 Web Track. This involved the use of freely-available Information Retrieval tools to provide inputs to the data fusion process. This paper outlines the strategies of the 3 candidate fusion algorithms entered in the ad-hoc task, discusses the methodology employed for the runs and presents a preliminary analysis of the provisional results issued by TREC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the second year of the SIFT (Segmented Information Fusion Techniques) project's participation in the TREC Web Track. In an effort to build on the experience gained in last year's competition and test some of the modifications made to our approach, it was once again decided to enter Category B. The principal aim of the SIFT group is to develop data fusion algorithms that combine the outputs of multiple Information Retrieval (IR) systems or algorithms in order to produce a single result-set that is of a superior quality. It should therefore be emphasised that the motivation behind our entry was not to evaluate novel IR systems or algorithms, but rather to investigate methods that may be used to combine these. In order to achieve this, the method employed uses implementations of standard, off-the-shelf IR algorithms (available as open source software) as the base systems for fusion and subsequently layers the fusion algorithms on top of these. This year's entry comprised three runs submitted to the ad-hoc task, with the result sets for each generated using a different fusion technique developed within the group.</p><p>The paper is organised as follows: Section 2 gives a short introduction to the area of Data Fusion. Sections 3, 4 and 5 provide implementation details for the three data fusion techniques that each constituted our entry for one of the runs. The procedures used to tune the parameters of these algorithms for the submitted runs, in addition to details of the component IR systems, are described in Section 6. Preliminary results are presented in Section 7. Possible directions for future entries are discussed in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Fusion</head><p>Data Fusion is an IR technique for combining the ranked lists returned by different component IR systems in response to a query. The goal is to produce an aggregate ranked list with improved performance over each of the individual lists. An inherent assumption within the data fusion context (as distinct from the related concept of collection fusion) is that each system retrieves from the same document collection. Techniques for fusion may be decomposed into two broad categories based on the level at which they access information:</p><p>1. Rank-based: the fusion algorithm is restricted to accessing the linearly scaled ranked lists output by the component systems and is not privy to the degrees of confidence underpinning these rankings. Such algorithms include approaches based on interleaving <ref type="bibr" coords="2,298.54,338.55,10.52,8.74" target="#b0">[1]</ref> and voting-based techniques [2, 3] 2. Score-based: the fusion algorithm may also take into account the relevance scores of the documents in the ranked list. These are the internally generated real numbers used by each IR system as a basis for calculating the rankings. Linear combination <ref type="bibr" coords="2,240.44,386.32,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,252.61,386.32,7.75,8.74" target="#b4">5]</ref> and the popular CombSum and CombMNZ algorithms <ref type="bibr" coords="2,183.45,398.28,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,195.63,398.28,7.75,8.74" target="#b6">7]</ref> are examples of methods based on relevance scores. These categories may be further sub-divided in accordance with whether they require training data to tune the parameters of the algorithm.</p><p>When choosing how to fuse the ranked lists returned by multiple IR systems, Vogt and Cottrell proposed certain intuitive "effects" that may be taken into consideration <ref type="bibr" coords="2,196.48,465.83,9.96,8.74" target="#b3">[4]</ref>. The first of these, the Skimming Effect, is based on the observation that relevant documents are more likely to appear at the top of result sets (where an IR system would place those documents it estimates to be most relevant). Thus, favouring early-ranked documents when compiling the final result set can result in improved fusion performance. Secondly, the Chorus Effect argues that if multiple input systems agree on the relevance of a document (by including it in each of their result sets) then this is increased evidence of relevance. This is also consistent with Lee's observation that IR systems tend to return the same relevant documents but different non-relevant ones <ref type="bibr" coords="2,435.19,561.47,9.96,8.74" target="#b6">[7]</ref>. Fusion algorithms that attach greater importance to documents that are returned by multiple input systems attempt to exploit this effect.</p><p>The fusion algorithms which were used to generate the results sets for the runs submitted to the ad-hoc task are part of a family of rank-based fusion techniques that may be termed "probabilistic". They are probabilistic in the sense that they attempt to build a model of the ranking behaviour of each component system, which may subsequently be used to estimate the probability that a document returned by that system at a particular rank will be relevant.</p><p>A training phase is utilised to gather statistics about the past performance of each system from which such a probability distribution may be approximated. At the fusion stage this probability information is used as a means to combine and re-rank the documents returned by each system in response to a query. The primary difference between the fusion strategies relates to the nature of the approximation of the probability distribution i.e. the degree to which it accurately reflects the true distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ProbFuse</head><p>Probfuse is a rank-based data fusion algorithm that attempts to model the characteristic ranking behaviour of each component system using a probability distribution <ref type="bibr" coords="3,177.74,273.71,9.96,8.74" target="#b7">[8]</ref>. It adopts a coarse-grained approach to the estimation of such a distribution, which is based on the notion of segmentation. The key idea is to divide a ranked list, often referred to as a result set, into a series of consecutive equal-sized segments spanning a range of rank positions. After segmentation, a training phase is undertaken to calculate the probability that a document returned at a position lying within a particular segment is relevant. These probability values are later used during the fusion phase to produce a single aggregated ranked list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Phase</head><p>The training phase is undertaken using a dataset consisting of a collection of result sets for which relevance judgments are available. The model of probability used by ProbFuse requires that each result set be divided into x segments of equal size. The division of a simple 12-document result set into segments is illustrated in Figure <ref type="figure" coords="3,227.49,457.32,3.87,8.74" target="#fig_0">1</ref>. Here, the leftmost result set is seen to be divided into two segments, with half of the documents appearing in each. Examples are also shown for increasing values for x, resulting in greater numbers of segments being created.</p><p>The objective of the training phase is to ascribe probabilities to each segment that will represent the likelihood that a document appearing in that segment will be relevant to any given topic. These probabilities may be calculated using the following formula:</p><formula xml:id="formula_0" coords="3,258.95,561.90,221.64,26.87">P (d k |S) = q∈Q R k,q K Q (1)</formula><p>where P (d k |S) represents the probability that a document d returned by the system S in segment k is relevant, R k,q is the number of documents in segment k that are judged to be relevant to topic q, K is the total number of documents in segment k and Q is the set of training topics. The outcome of the training phase is a set of probability values associated with the segments belonging to each of the systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusion Phase</head><p>Having obtained, for each system, a set of values estimating the probability that a document returned in each segment is relevant, the next step of the process is fusion. In this phase, each document is examined and its position in each of the result sets to be fused is noted. Depending on the segment the document is returned in, each system may then contribute towards that document's final ranking score, with no contribution occurring from any system that fails to return the document. The ranking score R d for each document d is given by the following equation:</p><formula xml:id="formula_1" coords="4,269.18,511.12,207.16,30.20">R d = M s=1 P (d k |s) k (<label>2</label></formula><formula xml:id="formula_2" coords="4,476.35,521.54,4.24,8.74">)</formula><p>where M is the number of retrieval models to be fused, P (d k |S) is as outlined above and k is the segment in which d is returned by system s (1 for the first segment, 2 for the second, etc.). Once R d has been calculated for each document, the documents are then merged into the final result set, sorted in descending order of R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SlideFuse</head><p>SlideFuse is a variation on the method adopted for modelling probability distributions used in ProbFuse <ref type="bibr" coords="4,248.91,657.11,9.96,8.74" target="#b8">[9]</ref>. In particular, it attempts a more fine-grained ap-proximation of the true underlying distribution (i.e. in contrast to the segmented approach, where the probability values apply to ranges of rank positions, it attempts to calculate the probability that a document returned at each position in a result set is relevant). For a training set of topics, this may be computed using the following formula:</p><formula xml:id="formula_3" coords="5,258.02,189.53,222.57,23.80">P (d p |s) = q∈Q R dp,q Q<label>(3)</label></formula><p>where, P (d p |s) is the probability that a document d returned by input system s in position p of a result set is relevant, R dp,q is the relevance of the document d, at position p, to the topic q (1 if the document is relevant, 0 if not) and Q is the set of training topics. In practice, however, a problem arises when using the above formula to calculate such probabilities, due to the presence of un-judged documents in the result sets i.e. documents for which no relevance information is available. During the training procedure, it is quite likely that there may be many positions at which only judged non-relevant or un-judged documents are returned. Unfortunately, this leads to a zero value for the probabilities of relevance calculated for these positions.</p><p>In order to address this problem and obtain a smoother, more representative, probability distribution the concept of a sliding window is introduced. Instead of focusing on individual positions, as above, the probability values for the surrounding positions are also taken into consideration and an average value calculated as follows:</p><formula xml:id="formula_4" coords="5,250.60,411.09,229.99,25.41">P (d p,w |s) = b i=a P (d i |s) b -a + 1<label>(4)</label></formula><p>In the above equation, P (d p,w |s) is the probability of relevance of a document d returned in position p using a window of size w either side of p, P (d i |s) is calculated using Equation refeqn:rank and a and b are, respectively, the beginning and end positions that delimit the window. The size of the window, or number of neighbouring positions that are taken into account on each side of a position, is fixed for each ranked list with a suitable value for this parameter being determined empirically. An illustration of the smoothing effect of the sliding window is shown in Figure <ref type="figure" coords="5,218.06,528.02,4.98,8.74" target="#fig_1">2</ref> for a sample input system.</p><p>The primary difference between the method adopted in ProbFuse lies in the fact that for SlideFuse the window or segment used to associate a probability value with each position is now always centred about the position. The combination strategy used to calculate the final ranking scores, R d , at the fusion stage is very similar to that given in equation 2, with the exception that P (d p,w |s) is now substituted for P (d k |s) and the scaling parameter k is no longer required. This function is presented in Equation <ref type="formula" coords="5,305.85,612.03,3.87,8.74" target="#formula_5">5</ref>. </p><formula xml:id="formula_5" coords="5,266.37,634.07,214.22,30.20">R d = M s=1 P (d p,w |s)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MAPFuse</head><p>MAPFuse is a fusion technique designed to address the extensive data/training demands of earlier probabilistic algorithms such as ProbFuse and SlideFuse <ref type="bibr" coords="6,462.33,411.77,14.61,8.74" target="#b9">[10]</ref>.</p><p>It attempts to formulate a universal probabilistic model that may be used to characterise the ranking behaviour of IR systems. The aim is to then estimate the parameters of this model using less data while simultaneously preserving fusion performance. At its core, it postulates a hyperbolic approximation of relationship between the position of a document in a ranked list returned by an IR system and the probability of relevance of the document to a query. In effect, a weight is associated with each input system based on past performance (similar in many respects to the score-based technique of Linear Combination <ref type="bibr" coords="6,466.20,507.41,10.79,8.74" target="#b3">[4]</ref>) which is then used in conjunction with the rank of each document to scale the contribution of the documents returned by that system to the fused result set.</p><p>In initial experiments carried out to explore MAPFuse, the MAP score achieved for training queries (M AP s ) was used as the weight associated with each system. The probability of relevance at a given position p was then estimated by</p><formula xml:id="formula_6" coords="6,269.02,589.67,211.57,22.31">P (d p |s) = M AP s p<label>(6)</label></formula><p>This was found to be correlated with a curve fitted to the probability of relevance when estimated at each individual position in the result set (as calculated using Equation <ref type="formula" coords="6,204.31,645.16,3.87,8.74" target="#formula_3">3</ref>). As such, the final ranking score R d of document d could be calculated as follows:</p><formula xml:id="formula_7" coords="7,271.27,127.79,209.33,26.80">R d = s∈S M AP s p s (d)<label>(7)</label></formula><p>where p s (d) is the position in which input system s returned document d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">TREC 2010 Experiments</head><p>In order to prepare for entry into the competition a number of decisions needed to be taken in relation to the experimental setup. In particular it was necessary to select both a suitable training dataset and also the input systems to be used during fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Training Data</head><p>As discussed above, each fusion algorithm requires a training phase to tune the parameters of the models that are built of the input systems. Ideally, for fusion to be successful, the data on which this training occurs should provide a representative sample that will be sufficient to capture the ranking behaviour of the models on future queries. In an effort to fulfil this requirement, the training strategy adopted was to use the ClueWeb09 Category B document collection in conjunction with the topics and relevance judgments available from TREC Web Track 2009 <ref type="bibr" coords="7,186.60,374.90,14.61,8.74" target="#b10">[11]</ref>. The parameters for segment and window size required respectively by ProbFuse and SlideFuse were chosen based on successful performance in previous empirical work <ref type="bibr" coords="7,250.33,398.81,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="7,262.50,398.81,7.75,8.74" target="#b8">9]</ref> i.e. the segment size used was 25 and the window size for was 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Input Systems</head><p>In order to focus development work on the design of fusion techniques the philosophy of the group is to use freely available open source IR software as a means for generating inputs to the fusion process. Two such packages provided the backbone for this year's entry.</p><p>-Terrier: Terrier (TERabyte RetrIEveR) is an open-source search engine developed at the University of Glasgow and released under the Mozilla Public License <ref type="bibr" coords="7,186.76,532.94,14.61,8.74" target="#b11">[12]</ref>. Terrier is specifically designed to be capable of handling largescale document collections, on the order of terabytes. This, coupled with the fact that it offers implementations of a variety of document ranking models, made it an attractive choice for providing the inputs to the fusion process. -Lemur: The Lemur Project<ref type="foot" coords="7,271.19,578.52,3.97,6.12" target="#foot_0">1</ref> was started in 2000 by the Center for Intelligent Information Retrieval (CIIR) at the University of Massachusetts, Amherst, and the Language Technologies Institute (LTI) at Carnegie Mellon University.</p><p>Indri is an open-source search engine, released as part of this project, which provides state-of-the-art text search and a rich structured query language for text collections of up to 50 million documents.</p><p>It was required that a subset of these IR algorithms be selected to generate the inputs to the fusion process. In order to accomplish this, each of the techniques provided by Terrier was run on topics from TREC Web Track 2009 and the 3 best performing systems chosen. These were DFR BM25, PL2 and TF IDF. In addition to these, the stand-alone Indri search engine was also selected. The same four systems were used as inputs to each of fusion algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Based on the preliminary results issued by TREC, computed over 36 of the 50 topics, a provisional measure of the performance of the 3 fusion algorithms may be gauged. Table <ref type="table" coords="8,212.87,262.40,4.98,8.74" target="#tab_0">1</ref> displays the average values for a selection of the evaluation metrics computed for the ad-hoc task, with the highest value for each highlighted in bold type. With reference to this table it may be seen that on average SlideFuse performs best according to 5 out of 6 of the metrics. Slidefuse was also generally the best performing technique across the range of average statistics calculated by the trec eval tool. The performance of our runs with respect to the other entrants is illustrated in Table <ref type="table" coords="8,174.21,501.40,3.87,8.74" target="#tab_1">2</ref>, which shows the number of queries for which a run did better than or was equal to the median value of the two primary metrics ERR@10 and nDCG@10 (It should be noted that that there were 5 queries for which the median value was 0 for both measures). On a per query basis ProbFuse performed marginally better than SlideFuse with both techniques recording results better than or equal to the median on half the queries. To put these results into some further perspective, it was observed that the average difference between SlideFuse and the best performing system across the 36 queries was 0.421 for ERR@10 and 0.248 for nDCG@10.</p><p>The relatively strong performance of ProbFuse was surprising, given that it is the least smooth attempt to approximate the probability distribution of the constituent systems to be fused. On the other hand the comparatively poor performance of MAPFuse, which has shown promise in previous experimental work <ref type="bibr" coords="8,158.87,657.11,14.61,8.74" target="#b9">[10]</ref>, was disappointing. A possible explanation for this may be its reliance on only a single summary statistic (MAP score) to characterise the behaviour of an IR system at individual rank level. It is also not clear whether the MAP score is the appropriate measure to use for parameterisation of the probability distribution on such large datasets. In contrast, SlideFuse exploits more detailed information/statistics about the behaviour of the system at each rank position and is therefore perhaps a more stable and accurate approximation of the underlying probability distribution. However, it should also be pointed out that the primary motivational scenario behind MAPFuse is for situations where such detailed information is not available. Although the three fusion algorithms are not explicitly designed to optimise the criteria for the diversity task, Table <ref type="table" coords="9,307.79,335.26,4.98,8.74">3</ref> presents the results of our runs for the nERR-IA@10, α-nDCG@10 and P-IA@10 metrics. As above, the figures represent the number of queries for which our algorithms were better than or equal to the median value (these statistics are computed over the 88 runs submitted for both the ad-hoc and diversity tasks).</p><p>Table <ref type="table" coords="9,171.76,415.94,4.13,7.89">3</ref>. Diversity task, % of queries better than or equal to median on each run ProbFuse SlideFuse MAPFuse nERR-IA@10 53% 33% 44% α-nDCG@10 47% 31% 42% P-IA@10 50% 47% 44%</p><p>8 Future Work</p><p>The selection procedure used to determine the inputs to be used in the fusion phase of the runs relied solely on the individual performance of systems with little attention paid to the relationship between interactions amongst the systems and combined performance. One area of interest to the group is the formulation of metrics to capture complementary characteristics of the input systems stemming from their methodological differences. Such metrics would form the basis of more sophisticated selection strategies and perhaps more intelligent fusion techniques. Similarly, it would also be interesting to investigate whether such metrics could be leveraged effectively in fusion algorithms tailored to the diversity task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,196.80,348.20,221.76,7.89;4,169.23,116.83,276.90,216.60"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Segmenting a result set for different values of x</figDesc><graphic coords="4,169.23,116.83,276.90,216.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,211.14,328.40,193.07,7.89;6,150.48,116.83,314.40,196.80"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Probability Distribution using SlideFuse</figDesc><graphic coords="6,150.48,116.83,314.40,196.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,154.95,354.26,305.45,96.81"><head>Table 1 .</head><label>1</label><figDesc>Evaluation Results, with the highest score for each metric in bold</figDesc><table coords="8,224.79,375.05,165.78,76.01"><row><cell></cell><cell cols="3">ProbFuse SlideFuse MAPFuse</cell></row><row><cell>ERR@10</cell><cell>0.135</cell><cell>0.156</cell><cell>0.129</cell></row><row><cell cols="2">nDCG@10 0.074</cell><cell>0.085</cell><cell>0.075</cell></row><row><cell>P5</cell><cell>0.328</cell><cell>0.333</cell><cell>0.244</cell></row><row><cell>P10</cell><cell>0.300</cell><cell>0.331</cell><cell>0.250</cell></row><row><cell>bpref</cell><cell>0.211</cell><cell>0.208</cell><cell>0.204</cell></row><row><cell>MAP</cell><cell>0.108</cell><cell>0.115</cell><cell>0.108</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,116.91,345.83,60.59"><head>Table 2 .</head><label>2</label><figDesc>The percentage of queries which did better than or was equal to the median value of the metrics</figDesc><table coords="9,224.79,146.92,165.78,30.58"><row><cell></cell><cell cols="3">ProbFuse SlideFuse MAPFuse</cell></row><row><cell>ERR@10</cell><cell>53%</cell><cell>50%</cell><cell>33%</cell></row><row><cell cols="2">nDCG@10 53%</cell><cell>50%</cell><cell>39%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,144.73,657.79,95.55,7.86"><p>http://lemurproject.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,143.58,337.64,7.86;10,151.52,154.54,329.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,352.79,143.58,123.24,7.86">The Collection Fusion Problem</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Johnson-Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,165.38,154.54,251.41,7.86">Proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<meeting>the Third Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,165.50,337.64,7.86;10,151.52,176.46,329.07,7.86;10,151.52,187.42,326.31,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,266.69,165.50,88.38,7.86">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,376.71,165.50,103.89,7.86;10,151.52,176.46,329.07,7.86;10,151.52,187.42,119.68,7.86">SIGIR &apos;01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,198.38,337.63,7.86;10,151.52,209.34,329.07,7.86;10,151.52,220.30,256.25,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,264.32,198.38,154.56,7.86">Condorcet fusion for improved retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,438.42,198.38,42.17,7.86;10,151.52,209.34,329.07,7.86;10,151.52,220.30,48.64,7.86">CIKM &apos;02: Proceedings of the eleventh international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="538" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,231.26,337.63,7.86;10,151.52,242.19,139.34,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,265.47,231.26,172.25,7.86">Fusion Via a Linear Combination of Scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,445.75,231.26,34.83,7.86;10,151.52,242.21,55.19,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,253.17,337.64,7.86;10,151.52,264.13,329.07,7.86;10,151.52,275.09,329.07,7.86;10,151.52,286.05,125.42,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,290.06,253.17,190.53,7.86;10,151.52,264.13,33.97,7.86">Searching distributed collections with inference networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,206.28,264.13,274.31,7.86;10,151.52,275.09,260.53,7.86">SIGIR &apos;95: Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,297.01,337.64,7.86;10,151.52,307.97,329.07,7.86;10,151.52,318.93,226.43,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,246.96,297.01,135.83,7.86">Combination of Multiple Searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<idno>Publication 500-215.</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,405.77,297.01,74.83,7.86;10,151.52,307.97,171.51,7.86">Proceedings of the 2nd Text REtrieval Conference (TREC-2)</title>
		<meeting>the 2nd Text REtrieval Conference (TREC-2)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology Special</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,329.86,337.63,7.89;10,151.52,340.84,32.25,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,193.29,329.89,167.98,7.86">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,368.92,329.89,54.64,7.86">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">SI</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,351.80,337.64,7.86;10,151.52,362.76,329.07,7.86;10,151.52,373.72,329.07,7.86;10,151.52,384.68,110.07,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,336.10,351.80,144.50,7.86;10,151.52,362.76,61.10,7.86">ProbFuse: A Probabilistic Approach to Data Fusion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lillis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Toolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,239.28,362.76,241.31,7.86;10,151.52,373.72,256.94,7.86">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,395.64,337.64,7.86;10,151.52,406.60,329.07,7.86;10,151.52,417.56,297.71,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,335.08,395.64,145.51,7.86;10,151.52,406.60,94.05,7.86">Extending Probabilistic Data Fusion Using Sliding Windows</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lillis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Toolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,274.42,406.60,206.18,7.86;10,151.52,417.56,134.05,7.86">Proceedings of the 30th European Conference on Information Retrieval (ECIR &apos;08)</title>
		<meeting>the 30th European Conference on Information Retrieval (ECIR &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008-04-02">31st March -2nd April 2008</date>
			<biblScope unit="page" from="358" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,428.52,337.97,7.86;10,151.52,439.47,329.07,7.86;10,151.52,450.43,329.07,7.86;10,151.52,461.39,110.94,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,461.47,428.52,19.12,7.86;10,151.52,439.47,182.27,7.86">Estimating Probabilities for Effective Data Fusion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lillis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Toolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,354.81,439.47,125.78,7.86;10,151.52,450.43,325.19,7.86">Proceedings of the 33rd Annual ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 33rd Annual ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,472.35,337.97,7.86;10,151.52,483.31,329.07,7.86;10,151.52,494.27,107.84,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,302.78,472.35,158.90,7.86">Overview of the TREC-2009 Web Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,162.67,483.31,249.76,7.86">TREC2009: Proceedings of the 18th Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,505.23,337.98,7.86;10,151.52,516.19,329.07,7.86;10,151.52,527.15,248.77,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,453.14,505.23,27.46,7.86;10,151.52,516.19,119.64,7.86">Terrier information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,294.53,516.19,186.06,7.86;10,151.52,527.15,144.03,7.86">Proceedings of the 27th European Conference on Information Retrieval (ECIR 05)</title>
		<meeting>the 27th European Conference on Information Retrieval (ECIR 05)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="519" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
