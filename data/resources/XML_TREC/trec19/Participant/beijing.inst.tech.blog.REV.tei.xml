<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,222.66,142.04,177.49,11.84;1,242.16,157.16,138.33,11.84">BIT at TREC 2010 Blog Track: Faceted Blog Distillation</title>
				<funder ref="#_ZYNmHqX">
					<orgName type="full">Chinese National Natural Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Beijing Institute of Technology Excellent Doctoral Dissertation Yu Miao Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.40,182.02,41.08,8.48"><forename type="first">Peng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.19,182.02,41.62,8.48"><forename type="first">Qing</forename><surname>Yang</surname></persName>
							<email>yangqing2005@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.49,182.02,58.37,8.48"><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
							<email>cxzhang@bit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.59,182.02,54.60,8.48"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
							<email>zniu@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,222.66,142.04,177.49,11.84;1,242.16,157.16,138.33,11.84">BIT at TREC 2010 Blog Track: Faceted Blog Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0AFF5E3B20432D07F236E74B228B0487</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the work done for the TREC 2010 faceted blog distillation task. As the approach used in TREC 2009, a mixture of language models based on global representation is employed to rank the entire blogs by relevance and facets. The parameters in our approach are adjusted according to the experimental results in TREC 2009. In addition, we make use of the results evaluated in TREC 2009 to train a SVM classifier. This classifier is used to filter and re-rank the results obtained by the mixture model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the second year that Beijing Institute of Technology (BIT) participates in TREC faceted blog distillation. We evaluated the effectiveness of combining different language models for faceted blog ranking and employing global representation for feed distillation <ref type="bibr" coords="1,204.58,412.12,9.98,8.48" target="#b0">[1]</ref>. The parameters in our approach are tuned to the best performance according to the experimental results in TREC 2009. In addition, we make use of last year's evaluated results to train a SVM classifier model, followed by filtering and reranking results obtained by the mixture model. With the improved approach and more integrated indexed data (In TREC 2009, the index for our submitted results does not contain the data of January 2008) experimental results have shown the significant improvement this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Our approach for Faceted Blog Distillation Task</p><p>The goal of faceted blog distillation task is to find blogs that are principally devoted to a given topic and a given facet. The retrieval unit is a blog but a single document. All retrieved blogs should not only be relevant to the given topic, but also to the given facet, such as opinionated, personal and in-depth facets. According to the approaches used in last year, this task will be run as two separate sub-tasks, namely the Baseline Blog Distillation and the Faceted Blog Distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Overview</head><p>Fig. <ref type="figure" coords="2,263.48,391.00,3.22,7.62">1</ref>. The System Architecture.</p><p>As shown in Fig. <ref type="figure" coords="2,212.62,412.12,3.66,8.48">1</ref>, our system consists of four main parts: query generator, blog retrieval component, language model building component and classification component. Query generator is responsible for parsing topic and query generation. The blog retrieval component indexes blog corpus, and finds all blog posts corresponding to each query. All posts are then combined to blogs according to their feedno. After this, the component retrieves all posts that have the same feedno. The language model building component is responsible for generating topic-facet mixture models. The topic-facet mixture model is a linear combination of topic relevance model and facet model. In order to build topic relevant language model, we index Wikipedia corpus<ref type="foot" coords="2,215.64,509.26,2.82,5.08" target="#foot_0">1</ref> , and finds relevant wiki pages about a given topic. The classification component is in charge of filtering and re-ranking the results generated by the retrieval system using mixture models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Generation</head><p>This year, we only choose title field of a topic for query string generation. According to the experiments on TREC 2009, we notice that using title, description and narrative fields (TDN) cannot achieve a better performance than using title field (T) alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset and Index</head><p>TREC Blog08 collection contains permalinks, feed xml and homepages. We use the permalinks and feed xml for the faceted blog distillation task. The feed xml corpus is used to train the facet classifiers. The feed xml file format is similar to that of permalinks file, but the content is encoded by xml. We modified some Indri code to index the feed xml corpus. Krovetz stemmer and a list with 450 stop words (e.g. a, about, above or many other common but useless words) are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Retrieval Model</head><p>We choose Global Representation Model to represent blog as last year. This model treats a blog as a virtual document which is comprised of all posts of the blog. Thus, this model can effectively reflect the recurring interest in a given topic over the time span of the feed. In addition, since we use language model based approach to rank feed, Global Representation model, which combines many posts into a large document, can avoid the problem of sparsity of words as far as possible. In our experiments in TREC 2009, we notice that global representation model can achieve a better performance than pseudo-cluster selection model mentioned in previous work <ref type="bibr" coords="3,159.67,342.22,12.25,8.48" target="#b1">[2]</ref>.</p><p>In order to combine all posts into one large virtual document, first find all relevant posts for a given topic, which are then combined into some feeds by their feedno fields. After this, metadata search is used to collect all posts of a given feed by the feed's feedno. Finally, we obtain all relevant feeds which contain not only relevant posts but irrelevant posts as well.</p><p>We rank feeds by the Kullback-Leibler Divergence of a feed language model and a topic-facet language model. In this solution, two different language models are defined: one for a topic and its' facet value (θ TF ); and another for the virtual document of a feed (θ D ). That is, we assume that θ TF represents the topic and facet information need, while θ D represents a feed. The KL-divergence of these two models is able to measure how close they are to each other. Thus, the distance can be used to rank feeds:</p><formula xml:id="formula_0" coords="3,188.82,468.23,274.38,22.38">( ) ( ) ( ) ( ) , | l o g | TF D TF w score D TF p w p w cons θ θ θ = + ∑<label>(1)</label></formula><p>Because the constant cons(θ TF ) (the entropy of θ TF ) does not affect the results of ranking feeds, we do not compute it in our system. Thus, the main task is to estimate θ TF and θ D . θ D can be estimate by the following formula:</p><formula xml:id="formula_1" coords="3,230.10,522.71,233.10,28.16">( ) ( ) ( ) , | | | | D c w D p w C p w D μ θ μ + = +<label>(2)</label></formula><p>where p(w|C) is a background language model, C(w,D) is the count of w occurs in D, and μ is a Dirichlet smoothing parameter. We use μ=2000, which is optimal in most case <ref type="bibr" coords="3,158.14,576.46,11.10,8.48" target="#b2">[3]</ref>. The θ TF in equation ( <ref type="formula" coords="3,261.38,576.46,3.68,8.48" target="#formula_0">1</ref>) is the language model which reflects not only the topic information need but facet information need. So we use a mixture of language model to estimate it. We assume the feed is generated from a mixture of topic relevant model θ T and facet model θ F . The topic-facet model θ TF is a linear combination of θ T , and θ F :</p><p>( )</p><formula xml:id="formula_2" coords="3,250.56,631.60,212.64,13.88">F 1 T T F θ β θ βθ = - +<label>(3)</label></formula><p>where β is used to control the influence of the facet model θ F . In equation ( <ref type="formula" coords="4,435.32,141.64,3.33,8.48" target="#formula_2">3</ref>), θ T is the topic relevance model that can be obtained by pseudo-relevance feedback method (PRF). Using Formula (1), we can obtain a ranked list for each facet. Then we use last year's results to train three facet classifiers. These classifiers are used to filtered some results with low facet confidence (according to the classification confidence) in the ranked list. Finally, we merge the rank score and classification confidence by the ratio according to experiments in TREC 2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>We submit 2 baseline runs and 14 faceted runs. Table <ref type="table" coords="4,352.68,259.96,4.71,8.48" target="#tab_0">1</ref> shows the results of our two baseline runs and three standard baseline results. Table <ref type="table" coords="4,358.47,270.76,4.71,8.48" target="#tab_1">2</ref> shows the overview of our all faceted runs results, best and median results over all submitted runs. Table <ref type="table" coords="4,438.19,281.56,4.71,8.48" target="#tab_2">3</ref> shows the improvement over our results in TREC 2009. Run id all opinion factual indepth shallow official personal Figures 2 and 3 show our system's per-topic performance for the faceted blog distillation in terms of MAP, alongside with the per-topic median and best performance over all groups' runs, respectively. All topics are sorted along the x axis in descending order of BIT best run performance.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,182.46,358.54,258.06,8.48"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. MAP of faceted blog distillation for the first faceted value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,143.34,312.46,306.81,79.05"><head>Table 1 . Baseline Results</head><label>1</label><figDesc></figDesc><table coords="4,143.34,328.48,306.81,63.02"><row><cell>Run id</cell><cell>Map</cell><cell>R-prec</cell><cell>bpref</cell><cell>P@5</cell><cell>P@10</cell></row><row><cell cols="2">BITblog10bl1 0.3519</cell><cell>0.3845</cell><cell>0.3332</cell><cell>0.5500</cell><cell>0.4750</cell></row><row><cell cols="2">BITblog10bl2 0.3025</cell><cell>0.3562</cell><cell>0.3059</cell><cell>0.4667</cell><cell>0.4000</cell></row><row><cell>stdbaseline1</cell><cell>0.3210</cell><cell>0.3749</cell><cell>0.3249</cell><cell>0.5083</cell><cell>0.4250</cell></row><row><cell>stdbaseline2</cell><cell>0.2117</cell><cell>0.2613</cell><cell>0.2057</cell><cell>0.3333</cell><cell>0.2958</cell></row><row><cell>stdbaseline3</cell><cell>0.1832</cell><cell>0.2625</cell><cell>0.2021</cell><cell>0.3667</cell><cell>0.3292</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,249.72,410.68,112.60,7.62"><head>Table 2 . Faceted Results (Map)</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,187.50,204.23,212.43,432.32"><head>Table 3 . Improvement over last year's results</head><label>3</label><figDesc>From the experimental results, we notice a significant improvement over last year's result, although overall performances of all other groups' approach are improved. In fact, the basic retrieval model, that is topic-facet mixture model, is similar to the model used in TREC 2009. In TREC 2009, if the missing index of blogs in January 2008 is added, we can achieve relatively low but similar experimental results. A significant difference is parameters adjustments according to the results in TREC 2009. Another difference is the use of SVM classifier for filter and re-ranking.</figDesc><table coords="5,187.50,204.23,212.43,432.32"><row><cell>Best</cell><cell>Median</cell><cell>BIT Best</cell></row><row><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell></cell><cell></cell></row><row><cell>Best</cell><cell>Median</cell><cell>BIT Best</cell></row><row><cell>1</cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="2">Baseline result Faceted result</cell></row><row><cell cols="2">BIT 09 best run 0.1165</cell><cell>0.1026</cell></row><row><cell cols="2">BIT 10 best run 0.3519</cell><cell>0.2367</cell></row><row><cell>improvement</cell><cell>202.06%</cell><cell>130.07%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,150.42,645.64,259.50,7.62"><p>Wikipedia corpus is obtained from ClueWeb09 Dataset (Category B subset).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is supported by the grant from <rs type="funder">Chinese National Natural Science Foundation</rs> (No: <rs type="grantNumber">60705022</rs>) and <rs type="funder">Beijing Institute of Technology Excellent Doctoral Dissertation Yu Miao Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZYNmHqX">
					<idno type="grant-number">60705022</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BIT10bl1fd1 0.1946 0.1590 0.2423 0.2232 0.1436 0.2239 0.1834 BIT10bl1fd2 0.2023 0.1748 0.2717 0.2232 0.1436 0.2384 0.1849 BIT10bl1fd3 0.2367 0.2129 0.3920 0.2232 0.1436 0.3411 0.1779 BIT10bl1fd4 0.2222 0.1636 0.3885 0.1515 0.1607 0.3510 0.1803 BIT10bl2fd1 0.1714 0.1868 0.2369 0.1814 0.124 0.1745 0.1651 BIT10bl2fd2 0.1801 0.2278 0.2513 0.1814 0.124 0.1888 0.1663 BIT10bl2fd3 0.2084 0.2395 0.3468 0.1976 0.1173 0.2833 0.1529 BIT10bl2fd4 0.1845 0.2021 0.2687 0.1814 0.124 0.2492 0.1341 BIT10std1fd1 0.2013 0.1968 0.2553 0.2232 0.1436 0.2092 0.2056 BIT10std1fd2 0.2094 0.224 0.2825 0.2232 0.1436 0.2242 0.2037 BIT10std1fd3 0.2002 0.2032 0.2663 0.2232 0.1436 0.2152 0.1843 BIT10std1fd4 0.1925 0.2009 0.2508 0.2232 0.1436 0.1597 0.2086 BIT10std2fd1 0.1246 0.1054 0.2341 0.1146 0.1084 0.1228 0.1056 BIT10std3fd1 0.1091 0.0886 0.1217 0.0951 0.0856 0.1689 0.0951 BITblog10bl1 0.2219 0.1636 0.3870 0.1529 0.1607 0.3487 0.1803 BITblog10bl2 0.1836 0.1982 0.2707 0.1804 0.1236 0.247 0.1341 median 0.1043 0.1373 0.1806 0.0725 0.1012 0.0894 0.1693 best 0.4411 0.4941 0.5512 0.2557 0.4247 0.4299 0.5606</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="6,146.52,334.00,322.20,7.62;6,143.34,343.78,325.34,7.62;6,143.34,353.44,42.34,7.62" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<idno>TREC 2009) Proceedings. 2009: NIST</idno>
		<title level="m" coord="6,378.62,334.23,90.11,7.38;6,143.34,344.01,56.86,7.38;6,217.90,343.78,151.95,7.62">TREC 2009 Faceted Blog Distillation Task</title>
		<imprint/>
	</monogr>
	<note>The Eighteenth Text REtrieval Conference</note>
</biblStruct>

<biblStruct coords="6,146.53,363.22,322.19,7.62;6,143.34,372.94,212.12,7.62" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,233.52,363.45,150.61,7.38">UMass at TREC 2007 Blog Distillation Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,398.82,363.22,69.90,7.62;6,143.34,372.94,207.94,7.62">The Eighteenth Text REtrieval Conference</title>
		<imprint/>
	</monogr>
	<note>TREC 2007) Proceedings. 2007: NIST</note>
</biblStruct>

<biblStruct coords="6,146.53,382.72,322.13,7.62;6,143.34,392.44,238.33,7.62" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,253.86,382.94,214.80,7.38;6,143.34,392.66,70.28,7.38">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chengxiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,218.82,392.44,74.87,7.62">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
