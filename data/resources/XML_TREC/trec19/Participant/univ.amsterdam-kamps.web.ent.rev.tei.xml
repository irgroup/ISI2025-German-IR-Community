<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,60.93,83.76,487.87,15.48;1,232.15,105.68,145.42,15.48">Using Anchor Text, Spam Filtering and Wikipedia for Web Search and Entity Ranking</title>
				<funder ref="#_C2xN3cc #_Jn4bMXG #_XUxjX4F">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.75,138.20,63.92,10.75"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.52,138.20,80.71,10.75"><forename type="first">Rianne</forename><surname>Kaptein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.49,138.20,75.75,10.75"><forename type="first">Marijn</forename><surname>Koolen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,60.93,83.76,487.87,15.48;1,232.15,105.68,145.42,15.48">Using Anchor Text, Spam Filtering and Wikipedia for Web Search and Entity Ranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">69395FE929338103F7E5EE5449E6825A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we document our efforts in participating to the TREC 2010 Entity Ranking and Web Tracks. We had multiple aims: For the Web Track we wanted to compare the effectiveness of anchor text of the category A and B collections and the impact of global document quality measures such as PageRank and spam scores. We find that documents in ClueWeb09 category B have a higher probability of being retrieved than other documents in category A. In ClueWeb09 category B, spam is mainly an issue for full-text retrieval. Anchor text suffers little from spam. Spam scores can be used to filter spam but also to find key resources. Documents that are least likely to be spam tend to be high-quality results. For the Entity Ranking Track, we use Wikipedia as a pivot to find relevant entities on the Web. Using category information to retrieve entities within Wikipedia leads to large improvements. Although we achieve large improvements over our baseline run that does not use category information, our best scores are still weak. Following the external links on Wikipedia pages to find the homepages of the entities in the ClueWeb collection, works better than searching an anchor text index, and combining the external links with searching an anchor text index.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the Web Track, we experiment with three anchor text variants from two indexes. One index contains all the incoming anchor text of the category A collection, the other index contains only the incoming anchor text of the category B collection. A third variant is derived from the category A index, where we filter on the category B results, to see if the extra anchor text for category B pages, from category A pages, improves the effectiveness. Further, we experiment with combining the retrieval score with PageRank scores and spam classification scores, and filtering results based on spam scores.</p><p>Our approach to the TREC Entity Ranking track is similar to the approach we took last year <ref type="bibr" coords="1,455.31,230.82,10.58,8.64" target="#b5">[6]</ref>. We adjusted our approach to fit in with the new result format. The TREC entity ranking track investigates the problem of related entity finding, where entity types are limited to people, organisations and products. We approach this task as an Entity Ranking task by not using the given input entity. Also we do not use the general entity types of people, organisations and products, instead we have manually assigned more specific target entity types which are also Wikipedia categories. To retrieve entities within Wikipedia, we exploit the category information which has been proven to work for this task. To find the corresponding web entity home pages, we follow the external links on the Wikipedia pages, and search an anchor text index for the page title.</p><p>This paper consists of two parts. In the first part, in Section 2, we discuss our experiments for the Web Track. The second part details our Entity Ranking experiments in Section 3. We summarise our findings in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Web Track</head><p>For the Web Track, we experiment with incoming anchor text representation based on either the category A or category B collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experimental Set-up</head><p>For the Web Track runs we used Indri <ref type="bibr" coords="1,486.38,559.79,11.62,8.64" target="#b2">[3]</ref> for indexing, with stopwords removed and terms are stemmed using the Krovetz stemmer. We built the following indexes:</p><p>Text B: contains document text of all documents in ClueWeb category B.</p><p>Anchor B: contains the anchor text of all documents in ClueWeb category B. All anchors are combined in a bag of words. 37,882,935 documents (75% of all documents) have anchor text and therefore at least one incoming link.</p><p>Anchor A: contains the anchor text of all documents in the English part of ClueWeb category A, kindly provided by the University of Twente <ref type="bibr" coords="2,191.12,57.28,10.58,8.64" target="#b1">[2]</ref>. In total 440,678,986 documents (87% of all English documents) have anchor text. There are 45,077,244 category B documents within this set (90% of all category B documents). We finished our index for category A after the official submission deadline, so we have no official runs based on this index.</p><p>For all runs, we use Jelinek-Mercer smoothing, which is implemented in Indri as follows:</p><formula xml:id="formula_0" coords="2,94.56,189.72,198.35,22.31">P (r|d) = (1 -λ) • tf r,d |d| + λ • P (r|D)<label>(1)</label></formula><p>where d is a document in collection D. We use little smoothing (λ = 0.1), which was found to be very effective for large collections <ref type="bibr" coords="2,99.46,247.06,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="2,112.74,247.06,7.19,8.64" target="#b4">5]</ref>.</p><p>For ad hoc search, pages with more text have a higher prior probability of being relevant <ref type="bibr" coords="2,171.60,271.69,10.58,8.64" target="#b6">[7]</ref>. Because some web pages have very little textual content, we use a linear document length prior β = 1. That is, the score of each retrieved document is multiplied by P (d):</p><formula xml:id="formula_1" coords="2,129.90,330.94,163.00,26.29">P (d) = |d| β d ∈D |d | β<label>(2)</label></formula><p>The final retrieval score S ret is computed as:</p><formula xml:id="formula_2" coords="2,129.97,390.60,162.93,9.65">S ret = P (d) • P (r|d)<label>(3)</label></formula><p>.</p><p>Using a length prior on the anchor text representation of documents has an interesting effect, as the length of the anchor text is correlated to the incoming link degree of a page.</p><p>The anchor text of a link typically consists of one or a few words. The more links a page receives, the more anchor text it has. Therefore, the length prior on the anchor text index promotes web pages that have a large number of incoming links and thus the more important pages.</p><p>We used the PageRank scores computed over the entire category A collection provided by CMU. <ref type="foot" coords="2,215.99,533.59,3.49,6.05" target="#foot_0">1</ref> To combat spam, we use the Fusion spam scores provided by Cormack et al. <ref type="bibr" coords="2,53.80,559.17,10.58,8.64" target="#b0">[1]</ref>. These spam scores are percentiles based on the logodds that a page is spam. Documents in the lower percentiles are most likely to be spam, while documents in the higher percentiles are least likely to be spam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Official Runs</head><p>We look at the impact of filtering spam pages and re-ranking retrieval results by multiplying the retrieval scores by either the PageRank score or the spam percentile. This is computed as:</p><formula xml:id="formula_3" coords="2,368.62,78.96,187.29,24.60">S P R (d) = P R(d) • S ret (d) (4) S SR (d) = Spam(d) • S ret (d)<label>(5)</label></formula><p>where </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>Results for the Ad hoc task are shown in Table <ref type="table" coords="2,501.04,567.32,3.74,8.64" target="#tab_1">1</ref>. We include a number of unofficial runs for further analysis. We make the following observations. Of the official runs, the baseline mixture run UAMSA10d2a8 has the highest MAP. Document quality indicators do not help average precision. However, the spam filter (UAMSA10mSF30) is effective for early precision. The official UAMSA10mSFPR run performs very poorly, because of a error in the multiplication of the retrieval and PageRank scores. The anchor text only run UAMSD10ancB is very similar to the mixture run. This is probably because, in the mixture run, the anchor text score dominates the full-text score. When we combine the anchor The category B anchor text index is more effective than the category A anchor text index. Although performance of the category A index improves when we filter out pages that do not occur in category B, it falls behind performance of the category B anchor text run. If we filter out the results that are not in category B, the results improve. It seems that the documents in category B have a higher probability of being relevant. We will further analyse this difference in the next section.</p><p>For the Diversity Tasks we report the official nERR-IA (normalised intent-aware expected reciprocal rank) and strec (subtopic recall) measures in Table <ref type="table" coords="3,199.63,529.02,3.74,8.64" target="#tab_3">2</ref>. The nERR-IA measure uses collection-dependent normalisation.</p><p>The performance of the mixture run UAMSA10d2a8 and the anchor text run UAMSD10ancB are similar. Again, this is probably due to high weight on the anchor text score in the mixture model. Filtering out pages below the 30th percentile (UAMSA10mSF30) has a small positive effect. Re-ranking the results by combining the anchor text score with the spam percentile (UAMSD10aSRfu) leads to bigger improvements at rank 5. Combining the anchor text run with PageRank (UAMSD10ancPR) hurts diversity performance.</p><p>The anchor text run (Anchor B) is clearly more diverse than the full-text run (Text B). But, because some of the top results of the Text B run are unjudged, these scores are a lower bound. The mixture run (Mix B) leads to a small improvement in nERR-IA@5 and strec@5. The anchor text index of category A has lower scores than the Anchor B run. If we filter on category B, the scores go up, again suggesting that the category B documents are of higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Analysis</head><p>In this section, we perform a further analysis of the results and look for reasons why the anchor text in category B is more effective than the anchor text in category A. We also look at the impact of spam on the performance of our runs. This year, judged documents were labelled as being either irrelevant, relevant, a key resource, a home page targeted by the query or junk/spam. We analyse our runs using these labels.</p><p>We first look at the relevance assessments themselves, in Table <ref type="table" coords="3,341.24,410.91,3.74,8.64" target="#tab_2">3</ref>. The category B part of ClueWeb09 is a 10% subset of category A. In total, 18,161 query-document pairs were judged, the majority of which are for documents in the category B collection. The top 20 results of the official runs seem to have mainly category B documents. This could be due to many participants submitting category B-only runs, or because documents in category B are ranked higher than the rest of the documents in category A. Of the pages judged as spam, only 50% comes from category B. This suggests that category B contains less spam. The relevant pages (including key resources and navigational pages) are as frequent in the judged documents of category B as in the judged documents of category A.</p><p>If we look at the top 100 results of the Anchor A run, we find that 53% of the results are category B documents. This shows that, at least for anchor text, the category B documents are more often retrieved than non-category B documents in category A. But why does the Anchor B run perform so much better than the Anchor A, even when we filter the Anchor A run on category B? In Figure <ref type="figure" coords="3,491.95,638.58,4.98,8.64">1</ref> we look at the percentage of non-judged results in the top 100. Because the Anchor B run is an official submission, the top 20 results are judged. For the other two runs, many of the top results or not judged, which, at least partially, explains why the Anchor A runs are score lower than the Anchor B run.</p><p>In the rest of this section, we look at the official submis-  sions. Next, we look at the percentage of results in the top 20 that are labeled as spam (Figure <ref type="figure" coords="4,467.19,261.10,3.60,8.64">2</ref>). <ref type="foot" coords="4,477.98,259.43,3.49,6.05" target="#foot_1">2</ref> Only the Text B run suffers from spam, and especially at the highest ranks, with 46% of the results at rank 1 being spam. At rank 2 the percentage is even higher (47%). The percentage gradually drops to 17% at rank 20. All other runs, which are mainly based on the anchor text index, hardly suffer from spam. At least in category B, anchor text seems not to be abused by spammers.</p><p>In Figure <ref type="figure" coords="4,367.27,357.94,4.98,8.64">3</ref> we look at the percentage of results labeled as relevant (including key resources and navigational target pages). Here we see that the precision of the Text B run increases with rank, which is probably due to the fact that the amount of spam at each rank gradually drops with increasing rank. Note that not all of the Text B results in the top 20 are judged (from 4% at rank 1 up to 18% at rank 20), so the actual percentage of relevant documents might be higher (as well as the percentage of spam). Of the official runs, the ranking based on both the anchor text score and the spam percentile (UAMSD10aSRfu) has the highest precision at rank 1. However, at rank 5 and beyond, the official runs have a very similar precision. Also, precision remains relatively stable after rank 10.</p><p>If we look at the percentage of results labeled as key resource (Figure <ref type="figure" coords="4,379.46,538.47,3.60,8.64">4</ref>), we see again that the UAMSD10aSRfu run has a slightly higher percentage at rank 1 (23%), but the percentage rapidly drops to around 8% for these runs. If we promote documents that are least likely to be spam, we find more key resources in the top of the ranking. This shows that the spam scores not only indicate whether a document is spam or not, but provide an overall indicator of document quality as well. The Text B run has a low percentage at rank 1 (again, possibly due to spam), but catches up with the anchor text based runs at rank 11 and from rank 12 outperforms them. This is in line with the higher MAP of the Text B run; beyond the first ranks, its precision is better than that of the anchor text and mixture runs. The percentage of results labeled as navigational target is shown in Figure <ref type="figure" coords="5,124.65,514.96,3.74,8.64" target="#fig_2">5</ref>. The Text B run finds no navigational targets before rank 9, whereas the official runs start with 4% navigational targets at rank 1. However, this percentage quickly drops to 1%. As with the key resources, anchor text easily finds one or a few highly linked home page and other important pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Entity Ranking</head><p>For the entity ranking track, we experiment with using Wikipedia as a pivot to retrieve entities. We participate only in the related entity finding task, we treat this task however as an entity ranking task, i.e. we do not use the given entities in the query topics, but only the narrative. To complete the task of entity ranking, we split the task up into three steps:</p><p>1. Rank all Wikipedia pages according to their match to the narrative from the query topic.</p><p>2. Rerank the top retrieved Wikipedia pages, according to their match with the target entity types 3. Find home pages belonging to the retrieved Wikipedia pages</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Retrieving entities in Wikipedia</head><p>Our approach exploits the category information in Wikipedia. The target entity types which are assigned during topic creation (people, organisations, products and locations) are too general for our purposes. Instead we have assigned manually more specific entity types to each query. These entity types can also be assigned automatically by pseudo-relevance feedback, i.e. take the top N results from the initial ranking created in step 1 of the entity ranking process, and assign the most frequently occurring category as the target entity type. Our initial run is a language model run with a document length prior created with Indri <ref type="bibr" coords="5,443.79,521.34,10.58,8.64" target="#b2">[3]</ref>. To rerank the pages according to their match with the target entity types, we use the following algorithm. KL-divergence is used to calculate distances between categories, and calculate a category score that is high when the distance is small, and the categories are similar as follows:</p><formula xml:id="formula_4" coords="5,323.63,597.46,232.29,20.31">Scat(C d |Ct) = - t∈D P (t|Ct) * log P (t|Ct) P (t|C d )<label>(6)</label></formula><p>where d is a document, i.e. an answer entity, C t is a target category and C d a category assigned to a document. The score for an answer entity in relation to a target category S(d|C t ) is the highest score, or shortest distance from any of the document categories to the target category. A linear combination of the initial score as calculated in step 1 and the category score produces the final score by which the Wikipedia pages are ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieving home pages for Wikipedia Entities</head><p>In the third and last step of our approach we retrieve home pages associated with the retrieved Wikipedia pages. In the Wikipedia context we consider each Wikipedia page as an entity. The Wikipedia page title is the label or name of the entity. We experiment with three methods to find Web pages associated with Wikipedia pages:</p><p>1. External links: Follow the links in the External links section of the Wikipedia page.</p><p>2. Anchor text: Take the Wikipedia page title as query, and retrieve pages from an anchor text index using a length prior.</p><p>3. Combined: When no external link is available search the anchor text.</p><p>For each Wikipedia page we only include the first result of the associated Web pages. In the 'External Links' method results are skipped if no external links exist in the document collection for the Wikipedia result. All our experiments are conducted using only the Category B part of the ClueWeb collection. <ref type="foot" coords="6,95.58,338.64,3.49,6.05" target="#foot_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head><p>For the TREC 2010 entity ranking track, Wikipedia pages are not judged and considered non-relevant by definition. The official results only report on finding the web homepages of the entities. In our approach however, identifying the relevant Wikipedia pages is key. We therefore generate an alternative assessment set. The names associated with the homepages are judged, so we can compare the relevant names to our found Wikipedia page titles to get an indication of the quality of our Wikipedia runs. The results of these runs can be found in Table <ref type="table" coords="6,186.39,496.66,3.74,8.64" target="#tab_4">4</ref>. When external links are used to find homepages, all Wikipedia results without external links to a page in the ClueWeb Category B collection are excluded from the ranking. In the table we show the results after removing these pages, so we get an idea of the number of relevant entities we are missing. The results for the run using the combined approach and the run searching the anchor text are very similar, differences only come from the removal of different duplicate results. Unfortunately, we cannot compare the runs to a baseline of full-text search on the ClueWeb collection. Since we have not submitted a fulltext search run to the TREC, a large amount of the results in this run would be unjudged, and the results would be underestimated. Instead we compare the Wikipedia runs using the category information to the runs not using the category information.</p><p>The baseline scores are weak, achieving NDCG@R scores of less than 0.05. For all but one of the measures and approaches large significant improvements are achieved when category information is used, some scores more than double. Although the run using the external links throws away all results without external links to the ClueWeb collection, resulting in a lower number of primary Wikipedia pages retrieved, the pages with external links still lead to reasonable P@10 and the best NDCG@R.</p><p>In Table <ref type="table" coords="6,359.92,164.87,4.98,8.64">5</ref> the results of the TREC entity ranking task 2010 are given, evaluating the primary homepages found. Again significant improvements are achieved when category information is used, except for the run using anchor text to find homepages. The approach based on following the external links gives the best results. For almost all Wikipedia pages with relevant titles the external link to a ClueWeb page is relevant. In addition, some Wikipedia entities which have not been judged relevant, still contain external links to relevant homepages. In contrast, the combined approach and the anchor text approach do not perform as well on finding homepages. Although these runs contain more relevant Wikipedia entities, less relevant homepages are found. The anchor text index finds less than half of the relevant entities. In contrast to the results of 2009 <ref type="bibr" coords="6,446.89,332.24,10.58,8.64" target="#b5">[6]</ref>, the combined approach does not lead to any improvements over the link based approach. This is probably caused by the fact that in 2009 one result can contain up to 3 webpages, whereas in 2010 each result contains one webpage. The success rate at rank 1 of the anchor text approach is obviously not as high as the success rate at rank 3, while for the external links, in most cases the first external link is is relevant.</p><p>Comparing our results to other approaches, our performance is not very impressive. One of the main shortcomings in our approach is that the task is actually a related entity finding task, but we are approaching it as an entity ranking task, that is we do not use the given entity to which the entities should be related. This given entity is in most cases a part of the narrative in the query topic, which we initially use to retrieve entities within Wikipedia. Another problem is that the narrative is phrased as a sentence, instead of a keyword query for which our approach is originally designed. So, although our using Wikipedia as a pivot to search entities is a promising approach, it should be adjusted to the specific characteristics of the related entity finding task to perform better on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we detailed our official runs for the TREC 2010 Web Track and Entity Ranking Track and performed an initial analysis of the results. We now summarise our preliminary findings.</p><p>For the Web Track, we wanted to compare the anchor text representations of ClueWeb09 category A and category B and look at the impact of spam scores.  The larger category A anchor text index covers many more documents than the category B anchor text index. It also increases the coverage and amount of anchor text of category B documents. However, the category B anchor text run outperforms the category A anchor text run, even if we filter the latter to retain only the category B results.</p><p>Our analysis of the relevance judgements shows that the majority of the Ad hoc judgements are for documents in category B, but relatively fewer of the documents labeled as spam are in category B. This shows that the top results of the official runs consist mainly of category B documents and also suggests that documents in category B are of higher quality than other documents in ClueWeb09. We also found that the category A anchor text run mainly has category B documents in the top 100 results, which suggests that category B documents have a higher probability of being retrieved. Another explanation for the lower scores of the category A anchor text run is that it has many non-judged results in the top ranks, so the evaluation scores might be underestimated.</p><p>In our experiments, only the full-text index suffers from spam, indicating that anchor text is less targeted by spammers. The Fusion spam scores can help reduce spam, but also used as indicators of document quality. If we rerank search results by combining the retrieval score with the spam score, we can improve the effectiveness of anchor textwhich, in our experiments, does not suffer from spam-for locating key resources.</p><p>For the Entity Ranking Track, we experimented with using Wikipedia as a pivot to find related entities in the larger Web. We find that using category information to retrieve entities in Wikipedia leads to large improvements over the baseline full-text search in Wikipedia, although it should be noted that the baseline performance is weak. To locate the web homepages of the entities found in Wikipedia following the external links on the Wikipedia page is better than searching an anchor text index, or combining the external links with searching an anchor text index. Our approach could be improved by exploiting the given entity to which the answer entities should be related.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,73.72,267.89,199.26,8.64"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Percentage of results that are not judged</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,63.46,64.07,219.77,8.64"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Percentage of results that are labeled relevant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,316.81,64.07,239.10,8.64;5,316.81,76.03,47.69,8.64"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Percentage of results that are labeled as navigational target</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,316.81,115.90,239.11,412.54"><head></head><label></label><figDesc>S ret (d) is the Indri retrieval score for document d, P R(d) is the PageRank score for d and Spam(d) is the spam percentile for d.</figDesc><table coords="2,316.81,152.13,239.11,376.32"><row><cell>We submitted three runs for the Adhoc Task:</cell></row><row><cell>UAMSA10d2a8: Mixture of document and anchor-text</cell></row><row><cell>runs of the category B indexes, with a linear length</cell></row><row><cell>prior probability for both document and anchor-text</cell></row><row><cell>representations. Scores are combined 0.2 document</cell></row><row><cell>score + 0.8 anchor-text score.</cell></row><row><cell>UAMSA10mSF30: Combination of category B document</cell></row><row><cell>and anchor-text runs with linear length priors for docu-</cell></row><row><cell>ment and anchor-text representations. Scores combined</cell></row><row><cell>as 0.2 document score + 0.8 anchor-text score. Results</cell></row><row><cell>are post-filtered on spam using the Waterloo spam rank-</cell></row><row><cell>ings, thresholded at the 30% spammiest pages.</cell></row><row><cell>UAMSA10mSFPR: Mixture of category B document and</cell></row><row><cell>anchor-text runs with linear length priors on document</cell></row><row><cell>and anchor-text representation. The mixture run scores</cell></row><row><cell>are multiplied by the CMU PageRank scores and spam-</cell></row><row><cell>filtered using the Waterloo Fusion spam percentiles,</cell></row><row><cell>thresholded at the 30% spammiest pages.</cell></row><row><cell>We submitted three runs for the Diversity Task:</cell></row><row><cell>UAMSD10ancB: Anchor-text run with linear length prior</cell></row><row><cell>on anchor-text representation using category B.</cell></row><row><cell>UAMSD10ancPR: Category B anchor-text run with linear</cell></row><row><cell>length prior on the anchor-text representation. Retrieval</cell></row><row><cell>scores are multiplied by the CMU PageRank scores.</cell></row><row><cell>UAMSD10aSRfu: Category B anchor-text run with linear</cell></row><row><cell>length prior on the anchor-text representation. Retrieval</cell></row><row><cell>scores are multiplied by the Fusion spam percentiles.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.80,64.07,239.10,327.64"><head>Table 1 :</head><label>1</label><figDesc>Results for the 2010 Ad hoc task. Best scores are in boldface.</figDesc><table coords="3,53.80,96.46,239.10,163.74"><row><cell>Run id</cell><cell>MAP</cell><cell>MRR</cell><cell>nDCG@10</cell></row><row><cell>UAMSA10d2a8</cell><cell cols="2">0.0435 0.4340</cell><cell>0.1736</cell></row><row><cell>UAMSA10mSF30</cell><cell cols="2">0.0429 0.4750</cell><cell>0.1790</cell></row><row><cell>UAMSA10mSFPR</cell><cell cols="2">0.0029 0.0624</cell><cell>0.0111</cell></row><row><cell>UAMSD10aSRfu</cell><cell cols="2">0.0406 0.5053</cell><cell>0.1881</cell></row><row><cell>UAMSD10ancB</cell><cell cols="2">0.0412 0.4333</cell><cell>0.1736</cell></row><row><cell>UAMSD10ancPR</cell><cell cols="2">0.0224 0.1912</cell><cell>0.0583</cell></row><row><cell>Text B length</cell><cell cols="2">0.0838 0.2134</cell><cell>0.1113</cell></row><row><cell>Mix B length</cell><cell cols="2">0.0435 0.4340</cell><cell>0.1736</cell></row><row><cell>Anchor B length</cell><cell cols="2">0.0412 0.4333</cell><cell>0.1736</cell></row><row><cell>Anchor A length</cell><cell cols="2">0.0264 0.3424</cell><cell>0.0989</cell></row><row><cell cols="3">Anchor A, filter B, length 0.0281 0.3592</cell><cell>0.1059</cell></row><row><cell cols="4">text score with the spam percentiles (UAMSD10aSRfu),</cell></row></table><note coords="3,53.80,263.53,239.10,8.64;3,53.80,275.48,239.10,8.64;3,53.80,287.44,239.10,8.64;3,53.80,299.39,239.10,8.64;3,53.80,311.35,239.10,8.64;3,53.80,323.30,239.10,8.64;3,53.80,335.26,239.10,8.64;3,53.80,347.21,239.10,8.64;3,53.80,359.17,239.10,8.64;3,53.80,371.12,239.10,8.64;3,53.80,383.08,63.36,8.64"><p>early precision increases. The spam scores are effective for ad hoc search. This is further discussed in Section 2.4. The PageRank scores (UAMSD10ancPR) are ineffective for the anchor text run. This is not due to any error as with the UAMSA10mSFPR run. As expected, the full-text run Text B has a higher MAP than the anchor text and mixture runs. While it has lower early precision, it finds many more relevant documents. Note that the Text B run was not submitted, and therefore has a substantial number of unjudged results in the top ranks; precision is probably underestimated.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,316.81,64.07,239.10,127.56"><head>Table 3 :</head><label>3</label><figDesc>Statistics on the TREC 2010 Ad Hoc assessments over categories A and B</figDesc><table coords="3,347.91,98.52,176.92,93.11"><row><cell cols="2">Description Category A</cell><cell>Category B</cell></row><row><cell>Documents</cell><cell>500M</cell><cell>50M (10%)</cell></row><row><cell>Judgements</cell><cell cols="2">25,329 15,845 (63%)</cell></row><row><cell>Spam</cell><cell>1,431</cell><cell>715 (50%)</cell></row><row><cell>Irrelevant</cell><cell cols="2">18,665 12,040 (65%)</cell></row><row><cell>Relevant</cell><cell>4,018</cell><cell>2,318 (58%)</cell></row><row><cell>Key</cell><cell>1077</cell><cell>682 (63%)</cell></row><row><cell>Nav</cell><cell>138</cell><cell>90 (65%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,99.45,64.07,407.72,173.87"><head>Table 2 :</head><label>2</label><figDesc>Impact of length prior on Diversity performance of baseline runs. Best scores are in boldface.</figDesc><table coords="4,140.23,86.55,329.26,151.39"><row><cell></cell><cell></cell><cell>nERR-IA</cell><cell></cell><cell>nNRBP</cell><cell></cell><cell>strec@</cell></row><row><cell>Run id</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell></cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>UAMSA10d2a8</cell><cell cols="3">0.246 0.262 0.277</cell><cell>0.240</cell><cell cols="3">0.379 0.483 0.600</cell></row><row><cell>UAMSA10mSF30</cell><cell cols="3">0.263 0.275 0.290</cell><cell>0.257</cell><cell cols="3">0.377 0.460 0.583</cell></row><row><cell>UAMSA10mSFPR</cell><cell cols="3">0.021 0.027 0.033</cell><cell>0.021</cell><cell cols="3">0.037 0.078 0.158</cell></row><row><cell>UAMSD10aSRfu</cell><cell cols="3">0.271 0.281 0.295</cell><cell>0.266</cell><cell cols="3">0.387 0.466 0.576</cell></row><row><cell>UAMSD10ancB</cell><cell cols="3">0.245 0.262 0.277</cell><cell>0.240</cell><cell cols="3">0.368 0.483 0.600</cell></row><row><cell>UAMSD10ancPR</cell><cell cols="3">0.089 0.102 0.117</cell><cell>0.084</cell><cell cols="3">0.189 0.282 0.450</cell></row><row><cell>Text B length</cell><cell cols="3">0.089 0.106 0.123</cell><cell>0.076</cell><cell cols="3">0.186 0.281 0.411</cell></row><row><cell>Anchor B length</cell><cell cols="3">0.245 0.262 0.277</cell><cell>0.240</cell><cell cols="3">0.368 0.483 0.600</cell></row><row><cell>Mix B length</cell><cell cols="3">0.246 0.262 0.277</cell><cell>0.240</cell><cell cols="3">0.379 0.483 0.600</cell></row><row><cell>Anchor A length</cell><cell cols="3">0.189 0.198 0.210</cell><cell>0.182</cell><cell cols="3">0.325 0.387 0.505</cell></row><row><cell cols="4">Anchor A, filter B, length 0.198 0.212 0.224</cell><cell>0.199</cell><cell cols="3">0.293 0.382 0.485</cell></row><row><cell>Text B SF30</cell><cell cols="3">0.200 0.218 0.230</cell><cell>0.189</cell><cell cols="3">0.312 0.409 0.512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,67.10,64.07,212.49,127.56"><head>Table 4 :</head><label>4</label><figDesc>TREC'10 Wikipedia Entity Ranking Results</figDesc><table coords="7,74.31,86.55,198.08,105.08"><row><cell>Approach</cell><cell cols="2"># Pri. WP NDCG@R</cell><cell>P10</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Links</cell><cell>77</cell><cell>0.0449</cell><cell>0.0511</cell></row><row><cell>Anchor text</cell><cell>83</cell><cell>0.0397</cell><cell>0.0447</cell></row><row><cell>Comb.</cell><cell>84</cell><cell>0.0405</cell><cell>0.0447</cell></row><row><cell cols="2">Using Category Information Links 79 -</cell><cell>0.1046 • •</cell><cell>0.0809 •</cell></row><row><cell>Anchor text</cell><cell>104 • •</cell><cell cols="2">0.0831 • 0.0851 • •</cell></row><row><cell>Comb.</cell><cell>104 • •</cell><cell cols="2">0.0836 • 0.0851</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,68.14,702.66,13.72,6.91;2,106.40,703.41,186.50,5.61;2,53.80,712.87,160.30,5.61"><p>See: http://boston.lti.cs.cmu.edu/clueweb09/ wiki/tiki-index.php?page=PageRank.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,331.16,693.19,224.76,6.91;4,316.81,702.66,239.10,6.91;4,316.81,712.12,117.02,6.91"><p>Because of the error with the UAMSA10mSFPR run and the poor performance of the UAMSD10ancPR run, we leave these runs out of our analysis, to keep the figures easy to read.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,68.14,693.19,224.76,6.91;6,53.80,702.66,239.10,6.91;6,53.80,712.12,119.97,6.91"><p>In our official runs we meant to include links to pages in Category A. Unfortunately, we made an error, and only links to the Category B part of the collection are used in all our runs.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs>, grant # <rs type="grantNumber">612.066.513</rs>, <rs type="grantNumber">639.072.601</rs>, and <rs type="grantNumber">640.001.501</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_C2xN3cc">
					<idno type="grant-number">612.066.513</idno>
				</org>
				<org type="funding" xml:id="_Jn4bMXG">
					<idno type="grant-number">639.072.601</idno>
				</org>
				<org type="funding" xml:id="_XUxjX4F">
					<idno type="grant-number">640.001.501</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,333.41,292.00,222.51,8.64;7,333.41,303.96,222.50,8.64;7,333.41,315.73,214.55,8.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,333.41,303.96,222.50,8.64;7,333.41,315.91,92.37,8.64">Efficient and Effective Spam Filtering and Re-ranking for Large Web Datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno>CoRR, abs/1004.5168</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,333.41,333.91,222.51,8.64;7,333.41,345.87,222.50,8.64;7,333.41,357.82,222.50,8.64;7,333.41,370.71,187.79,7.01" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,453.23,333.91,102.69,8.64;7,333.41,345.87,134.41,8.64">MIREX: MapReduce Information Retrieval Experiments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
		<ptr target="http://eprints.eemcs.utwente.nl/17797/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="7,333.41,387.78,21.86,8.64;7,374.14,387.78,181.78,8.64;7,333.41,399.73,222.50,8.64;7,333.41,412.62,38.36,7.01" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,374.14,387.78,181.78,8.64;7,333.41,399.73,22.28,8.64">Language modeling meets inference networks</title>
		<author>
			<persName coords=""><surname>Indri</surname></persName>
		</author>
		<ptr target="http://www.lemurproject.org/indri/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,333.41,429.68,222.50,8.64;7,333.41,441.46,222.50,8.82;7,333.41,453.41,222.51,8.82;7,333.41,465.55,222.50,8.64;7,333.41,477.50,125.36,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,376.84,429.68,163.53,8.64">Effective smoothing for a terabyte of text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,515.84,441.46,40.08,8.59;7,333.41,453.41,222.51,8.82;7,333.41,465.55,222.50,8.64;7,333.41,477.50,62.27,8.64">The Fourteenth Text REtrieval Conference (TREC 2005). National Institute of Standards and Technology. NIST Special Publication</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="500" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,333.41,495.50,222.50,8.64;7,333.41,507.46,222.50,8.64;7,333.41,519.23,222.50,8.82;7,333.41,531.19,222.51,8.82;7,333.41,543.32,222.50,8.64;7,333.41,555.28,42.34,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,379.50,495.50,176.42,8.64;7,333.41,507.46,135.91,8.64">Experiments with document and query representations for a terabyte of text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,444.74,519.23,111.17,8.59;7,333.41,531.19,45.56,8.59">The Fifteenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006. 2007</date>
			<biblScope unit="page" from="500" to="272" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,333.41,573.28,222.50,8.64;7,333.41,585.23,222.50,8.64;7,333.41,597.19,222.50,8.64;7,333.41,608.96,222.50,8.82;7,333.41,620.92,222.51,8.82;7,333.41,633.05,197.34,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,493.11,573.28,62.80,8.64;7,333.41,585.23,222.50,8.64;7,333.41,597.19,55.12,8.64">Result diversity and entity ranking experiments: Text, anchors, links, and wikipedia</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kaptein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,365.99,608.96,189.93,8.59;7,333.41,620.92,34.86,8.59">The Eighteenth Text REtrieval Conference Proceedings</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009. 2010</date>
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,333.41,651.05,222.51,8.64;7,333.41,663.00,222.50,8.64;7,333.41,674.78,222.50,8.59;7,333.41,686.74,222.50,8.59;7,333.41,698.69,222.50,8.82;7,333.41,710.82,65.40,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,522.60,651.05,33.32,8.64;7,333.41,663.00,205.82,8.64">The importance of prior probabilities for entry page search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,333.41,674.78,222.50,8.59;7,333.41,686.74,222.50,8.59;7,333.41,698.69,65.77,8.59">Proceedings of the 25th Annual International ACM SI-GIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SI-GIR Conference on Research and Development in Information Retrieval<address><addrLine>New York NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
