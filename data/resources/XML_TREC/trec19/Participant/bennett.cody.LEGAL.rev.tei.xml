<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.76,76.22,238.90,18.61;1,72.76,110.31,269.85,13.00">Auto-Relevancy Baseline A Hybrid System Without Human Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,72.76,131.26,60.54,9.19"><forename type="first">Cody</forename><surname>Bennett</surname></persName>
							<email>c_bennett@tcdi.com</email>
						</author>
						<title level="a" type="main" coord="1,72.76,76.22,238.90,18.61;1,72.76,110.31,269.85,13.00">Auto-Relevancy Baseline A Hybrid System Without Human Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">437AD6D0A7DB8DB992805E5C5985E9A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>On obtaining a Request for Production and automatically emulating a typical eDiscovery workflow 1 , a simple application of the classical Bayes algorithm upon the pseudo-hybridization of Semantic A and Latent Semantic Indexing BC systems should smooth out historically high yet noisy Recall of some LSI models and their derivatives and produce a tighter linear distribution when assessing relevant documents unsupervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>See the TREC website for details on the differences between Interactive and Learning tasks, the mock Requests for Production, and other information regarding scoring and assessing. Team TCD1's participation will be discussed without the repetition of most of that information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Participation</head><p>TCD1's submission assumes that by building a blind baseline mechanism, the result is an automated distribution useful as a statistical snapshot, part of a knowledge and/or eDiscovery paradigm, and ongoing quality assurance and control within large datasets and topic training strata. Further, corporations' Information Management architectures currently deployed can offer hidden insights of relevancy when historically divergent systems 2 are hybridized.</p><p>Therefore, TCD1's baseline submission considers a hybridization of Semantic and LSI 3 systems. The features are mostly conceptual, as strict keyword targeting was purposefully not used in order to ascertain the effectiveness of Semantic + LSI.</p><p>[STEP 0] For verbosity, the baseline was submitted to TREC Legal Learning track using:</p><p>• 685,592 de-duped Enron emails and attachments Semantically indexed 4</p><p>• A subset of 2010 TREC</p><p>1 Essentially, Collection, Processing, Review, Analysis and near-Production were automated based on the verbiage of the Request for Production and TREC provided exemplars.</p><p>2 Keyword vs. concept, concept vs. probabilistic, concept vs. semantic, etc. Esp. with IR systems, hybridization offers revitalization and ROI longevity. <ref type="bibr" coords="1,72.76,634.57,2.19,4.49">3</ref> The semantic and conceptual systems could be considered plug and play for different approaches. The approach is considered modular as long as a topic model is available and exemplar data is available specifying relevant and non-relevant information. <ref type="bibr" coords="1,72.76,671.29,2.19,4.49">4</ref> The Semantic engine is proprietary and therefore will not be dissected in detail; consists of a mixture of NLP and Semantic mapping with a prebuilt verbose English training strata -see Dahlgren.</p><p>seed values = {1,-1} LSI conceptually indexed<ref type="foot" coords="1,518.20,172.33,2.67,4.49" target="#foot_0">5</ref> see Seed Document Count • 8 topic iteration; 1 topic used as an attempted control • 1 run submission; most teams submitted the maximum 3</p><p>Data inputs were two-fold -Request for Production features and seed stratum. Output was relevancy and rank among other metadata described in TREC requirements.</p><p>The run was automatic with no intervention, no feedback loop and no previous TREC seed sets. The method used a black box approach absorbing a Request for Production and mechanically determined relevancy and rank as output. As part of the relevancy assessment, the black box emulated a machine learning topic expert. Similar to some Web methods, the initial topics within the legal document were expanded upon using a mixture of Natural Language Processing, Semantic indexing and targeted contextual hit building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High level of Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Query Expansion</head><p>[STEP 1] By using proprietary methods to locate topic request lines from the Request for Production and remove noise D , 8 simple queries were created and used to drive query expansion<ref type="foot" coords="1,349.72,589.93,2.67,4.49" target="#foot_1">6</ref> . As an example, initial topic truncation for topic 201 was: Topic "Expert" Relevancy (Rl) was then found using Bayes E :</p><p>Probability Rank (Ra) was proprietary function.</p><p>→ End for → End for 7 Further, this method directs the topic model build from the influence of the initial Request for Production. <ref type="bibr" coords="2,72.76,666.97,2.19,4.49">8</ref> The categorizer can be as simple as a vector cosine comparison across a conceptual index. 9 This type of human expert emulation simulates TREC's Legal Interactive track, except the role of the Legal expert is replaced by the Semantic knowledge of the machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All Run Results</head><p>TCD1's baseline run averaged higher than other "baseline" submissions save one during both preliminary result assessments (Oct. 2010) and full raw result assessments (Jan. 2011):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oct. 2010 Assessment Scores</head><p>On average, with 2 topics falling below simulated control (discussed below), the baseline was essentially the median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jan. 2011 Assessment Scores</head><p>TCD1 ranked top 2 in highest individual topic recall @ 200k documents (98.8%). Since TCD1 along with other teams appear to use topics 200 or 207 as controls and by removing the lowest score, a highest min-1 also shows TCD1 as median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Control</head><p>The baseline system's simulated control -topic 200 -used 0 "non-relevant" documents; all document P(D|H') were deliberately scored as .5. The reasoning follows the possibility that Legal Learning judges/assessors were purposefully skewing topic seeds with false positives/false negatives. Topic scores below the control in a production system i.e. topics 201, 205 due to deteriorated Hit Rate and/or False alarm rates (seed topics used to define these rates) would be targets for reassessment of exemplars.</p><p>Further:</p><p>• Seed documents were occasionally not semantic representatives of the topic and conceptually ambiguous and noisy, at least noisier than other topics and therefore caused anomalies during vector comparisons 10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The types of features derived from the linguistic search expansion used to train the topic expert appear to be critical to the system; features extracted highlight the importance of a smart and very clear topic "expert" model 11 .</p><p>Overall, the baseline system in current form:</p><p>• Is a good QC indicator of the salience of topic exemplars when comparing to a control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Is dynamic; the system produced scores below the control but also top tier results on various topics. The high entropy will be a focus on future system iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Averages up to 85% returned @ 200k, closer to desired automation (ignoring the control and results below the control -201, 205).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The algorithm's document ranking probability estimates were considerably off on average (55.8%).</p><p>In most cases, the rank algorithm needs to be adjusted upward.</p><p>Further work is needed to determine best next steps for increasing the recall at lower document cutoffs (increased precision). But it is clear from the entropy of scores delivered by the second raw assessment that topics require seed cleansing -something a statistical QC feature in future runs could determine automatically. Last, the baseline system while affected by errant seed exemplars did appear more robust in smoothing the LSI distribution due to the hybridization of a Semantic built expert in the application of Bayes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphical Comparison</head><p>Below is the amount of noise and recall of relevant documents @ 200k cutoff generated by top submissions for topic 207:</p><p>10 The use of seed topics is a mandatory step for this system since, in real rolling eDiscovery requests the exemplar training is iterative and dynamic.</p><p>11 Some semantic features may be missing but could be found through multiple recursions. Also, noise introduced during topic expert feature building (P|H) appears to cause a similar dilemma as conflicting P(D|H) and P(D|H').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic 207 -TREC Legal Presentation -TCD1 is Pink 12</head><p>For topic 207, TCD1's system achieves the highest recall (99+%) but at the cost of false positives 13 . The blue circle shows the baseline approaching congruency of most other systems at recall ~88% / precision ~75%. The green circle shows the approach to highest recall capability at the cost of noise. The initial false positive rate of TCD1's submission is the likely result of the lack of use of strict keywords and phrases during topic building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>While the hypothesis was proven to a point based on initial tests and other teams using LSI, more noise detection and elimination is needed to achieve both high R and P &gt; 98% @ &lt; 200k documents returned. For automation and QA/QC purposes, 88% of non-biased topics 14 may be an acceptable threshold for use in knowledge systems 15 compared to the accuracy of human review <ref type="bibr" coords="3,406.12,401.77,5.31,4.49">16</ref> . However, it is of direct interest to judge the cost of noise as a monetary value similar to valuations performed in TREC Legal Interactive task. Significantly lowering the noise will provide a cleaner plateau to begin questioning, "what/when is the probability that the system may produce an errant document in review and what is this cost?</p><p>17 "</p><p>Moving beyond cost, further enhancements to the system will improve the precision of the topic model expert. In future iterations, work will be done to ascertain at the time of seed building the viability of the seeds 18 ; checks to see if seed documents semantically overlap and cause inconsistency and/or tainting P(D|H) and P(D|H'). Also, a second and third pass before final scoring might be interesting to develop, where new (D|H) are learned from the semantic process.</p><p>Simply, the use of strict keyword features in addition to hybrid 12 Gordon Cormack, Maura Grossman 13 TREC is using F1 as the official yardstick. If F2 is used, recall is weighted dominantly. <ref type="bibr" coords="3,313.24,612.01,4.35,4.49">14</ref> The control (200) and 201, 205 <ref type="bibr" coords="3,313.24,621.13,4.35,4.49">15</ref> Grossman mentioned Xerox and TCDI topic 207 scores at TREC Legal Learning task presentation at Gaithersburg, Md. <ref type="bibr" coords="3,313.24,639.61,4.35,4.49">16</ref> See Inside Counsel, Jan. 2011 "Computerized E-Discovery Review is Accurate and Defensible" -a real world test showing machine vs. human review with machine @ ~83% while human review teams @ ~76% and ~72%. <ref type="bibr" coords="3,313.24,666.97,4.35,4.49">17</ref> The same question should be asked for human reviewers. <ref type="bibr" coords="3,313.24,676.33,4.35,4.49">18</ref> In an active review, people and systems try to improve their models, not deliberately try to break them with false positives / false negatives -although human assessors do make mistakes. QA/QC is critical in determining these issues.</p><p>features employed on the topic expert should increase relevancy scores and decrease false positives. Online topic learning may add more precision to the topic expert. Also, Latent Dirichlet Allocation F would add an automated topic feature set for juxtaposition. Even further, use of rough fuzzy hybridization appears as a promising black box approach to automated IR tasks and learning GH .</p><p>But regardless of future sophistications, TCD1's simplistic single-run hybrid baseline produced a peak topic score ~98.8% recall @ 200k directed by the Request for Production verbiage unsupervised. Next plans will reduce the document cutoff it takes to attain this recall "baseline".</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="1,317.34,643.59,213.02,7.41;1,313.24,652.71,203.51,7.41;1,313.24,661.83,195.79,7.41;1,313.24,670.95,63.42,7.41"><p>The values and feature sets for indexing are proprietary and therefore will not be dissected in detail. While repeating the experiment when using historical Dumais et al LSI, the relevancy results are assumed to be approximately similar.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="1,317.34,680.31,196.52,7.41;1,313.24,689.43,31.73,7.41;1,220.83,498.16,66.37,7.40"><p>This is counterintuitive to how eDiscovery typically handles keyword expansion.Seed Document Count</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,72.76,216.39,220.49,7.41;4,72.76,225.51,92.57,7.41" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Dahlgren</surname></persName>
		</author>
		<ptr target="http://www.cognitionsearch.com" />
		<title level="m" coord="4,133.87,216.39,156.21,7.41">Naïve Semantics for Natural Language Understanding</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.76,243.99,210.58,7.41;4,72.76,253.11,207.14,7.41;4,72.76,262.23,106.46,7.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,92.83,253.11,107.94,7.41">Indexing by Latent Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,207.35,253.12,72.55,7.40;4,72.76,262.24,57.95,7.40">Journal of the Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.76,280.71,169.27,7.41;4,72.76,289.83,88.92,7.41" xml:id="b2">
	<analytic>
		<title/>
		<ptr target="http://www.contentanalyst.com" />
	</analytic>
	<monogr>
		<title level="j" coord="4,72.76,280.71,88.64,7.41">C ContentAnalyst -current LSI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.76,308.31,225.40,7.41;4,72.76,317.43,164.44,7.41" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Manning</surname></persName>
		</author>
		<title level="m" coord="4,152.30,308.31,145.85,7.41;4,72.76,317.43,62.18,7.41">Hinrich Schütze, Foundations of Statistical Natural Language Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.76,335.67,218.24,7.41;4,72.76,345.03,47.35,7.41;4,72.76,365.67,197.22,7.41;4,72.76,374.79,163.72,7.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,137.50,335.68,116.46,7.40;4,197.23,365.67,66.43,7.41">Probability Theory: The Logic of Science</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,72.76,374.80,152.76,7.40">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2003">2003. 2006</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>Correlated topic models</note>
</biblStruct>

<biblStruct coords="4,72.76,393.03,216.53,7.41;4,72.76,402.39,202.35,7.41;4,72.76,411.51,141.17,7.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,152.76,393.03,136.54,7.41;4,72.76,402.39,121.62,7.41">Semantics-preserving dimensionality reduction: Rough and fuzzy-rough based approaches</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,202.61,402.40,72.51,7.40;4,72.76,411.52,12.24,7.40">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1457" to="1471" />
			<date type="published" when="2004-12">Dec. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.76,429.75,197.33,7.41;4,72.76,439.11,198.41,7.41;4,72.76,448.23,168.44,7.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,170.19,429.75,90.15,7.41">Rough and Fuzzy Hybridization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,72.76,439.11,198.41,7.41;4,72.76,448.23,33.48,7.41">Computational Intelligence and Feature Selection: Rough and Fuzzy Approaches</title>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
