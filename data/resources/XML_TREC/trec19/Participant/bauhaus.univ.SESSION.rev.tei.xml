<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,185.16,115.83,245.10,12.88">Webis at the TREC 2010 Sessions Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.68,153.59,63.54,9.03"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Media Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.00,153.59,47.74,9.03"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Media Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.32,153.59,63.34,9.03"><forename type="first">Michael</forename><surname>Völske</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Media Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,185.16,115.83,245.10,12.88">Webis at the TREC 2010 Sessions Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A37F844F09EB1E73E2EA9427582EF10B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we provide an overview of the Webis group's two-phase approach to the TREC 2010 Sessions track. In a preprocessing phase the queries are segmented to highlight contained concepts. In the final retrieval phase we treat Carnegie Mellon's ClueWeb search engine as a black box and apply the MAXIMUM QUERY framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC 2010 Sessions track offered the opportunity to apply our developed techniques for user experience improvement during web search sessions. Our framework is inspired by the observation that the interactions between web search users and search engines follow a classic scheme. The user comes up with a set of (in her opinion) appropriate keywords-or keyphrases-for a given information need. She submits a query containing some of these keywords and gets back a ranked result list. If the user does not find a match for her information need among the first results, she will hardly browse all the items but submit different queries based on her keywords until she is satisfied or decides to give up. This process forms a search session-the set of consecutive web queries a user submits to a search engine in order to satisfy a given information need.</p><p>The track itself has two tasks: (1) to improve retrieval performance for a given query by using the user's previous queries from the same session, and (2) to improve retrieval performance over an entire query session instead of a single query. Our framework for these tasks consists of two parts. In a first step we use a query segmentation approach from <ref type="bibr" coords="1,157.08,500.63,41.48,9.03" target="#b2">[HPSB10]</ref> to automatically detect and highlight concepts and phrases within the queries. In the second step-the retrieval process itself-we adopt a user perspective against an existing ClueWeb09 search engine and apply the MAXIMUM QUERY framework from <ref type="bibr" coords="1,179.52,536.50,27.23,9.03" target="#b4">[SH10]</ref>.</p><p>We apply the user perspective for the following reason. Experience shows that in many cases a user's first web query for her information need is answered reasonably well by existing commercial search engines (i.e., the query brings up an appropriate result). In case that the first query was not successful, commercial search engines provide different means of support (e.g., query expansion for queries returning lots of hits or spelling correction for queries returning no hits due to typos). However, there is no real session support yet such that respective techniques could also be implemented at user site (e.g., in a proxy process that handles a user's web queries).</p><p>Our approach has a more combinatorial flavor than current search engine's user support techniques but it is easily combinable with existing technology. We suggest to derive the maximum query for a given set of keywords (i.e., a query containing as many of the keywords as possible, while returning a reasonable number of results). The requirement to contain as many of the keywords as possible reflects the following rationale. Taken together, the keywords of a search session describe the user's information need. Some of the keywords might not be appropriate (e.g. typos) and should be omitted, but the more keywords are contained in a maximum query the better is the descriptiveness of the user's information need.</p><p>The rationale for requiring a reasonable number of hits per query also deserves closer consideration. Queries with empty result pages are useless and the same often applies to queries returning only a handful of hits. This gives a lower bound on the number of desired results. But there is also an upper bound since the number of results a user will consider for a single query is usually constrained by a processing capacity k, determined by the user's reading time etc. If the user faces a query with millions of hits, she can only check a fraction of the results-typically the top-ranked ones. Relevant entries below are missed. Consistent with our User-over-Ranking hypothesis <ref type="bibr" coords="2,448.80,286.31,27.33,9.03" target="#b5">[SH11]</ref>, we argue that the best queries are the ones that are sufficiently specific to not return millions of hits-but also not just one or two. For such queries the user can check the complete result list and will not miss any potential match for her information need due to search engine ranking issues that she cannot influence. That queries returning about k many results indeed can improve retrieval performance is underpinned by our experimental justification for the User-over-Ranking hypothesis <ref type="bibr" coords="2,391.20,358.07,29.36,9.03" target="#b5">[SH11]</ref> Hence, from the user's perspective, a maximum query contains a possible description of the information need and offers the chance to check all the results. However, finding a maximum query "by hand" is not that straightforward. Several queries have to be submitted to identify appropriate keyword combinations. Hardly any user will take the time for such a lengthy procedure. Therefore, we apply an algorithm. To be applicable at user site the algorithm is of external nature (i.e., it only uses the search engine's interfaces). The search engine is handled as a black box, acting like an oracle that answers queries. There is no need for the user to know the underlying retrieval model or implementation details. The black box we use for the Sessions track is Carnegie Mellon's ClueWeb search engine<ref type="foot" coords="2,250.68,476.04,3.49,6.32" target="#foot_0">1</ref> .</p><p>The paper is organized as follows. In Section 2 we describe the applied query segmentation process. The actual retrieval process is presented in Section 3. Achieved results of our system are shown in Section 4. A discussion and some concluding remarks follow in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preprocessing phase</head><p>The TREC 2010 Sessions track queries contained no phrase information. Hence, we decided to preprocess each single query by automatically detecting segments of words that should have been highlighted as phrases for improved retrieval. As a query segmentation procedure we use the s s -weighting scheme <ref type="bibr" coords="2,350.88,613.91,39.23,9.03" target="#b2">[HPSB10]</ref>.</p><p>The basic and major assumption is that phrases contained in web queries really exist on the web. The straightforward idea then is to use the web itself as a corpus to detect potential query segments. The approach uses phrase occurrence frequencies to decide which phrases are more likely segments than others. The largest obtainable collection of web phrases is the Google n-gram corpus <ref type="bibr" coords="3,319.44,142.91,27.06,9.03" target="#b0">[BF06]</ref>; it contains n-grams of length 1 to 5 from the 2006 Google index along with their frequencies.</p><p>A query q is viewed as a sequence (w 1 , w 2 , . . . , w n ) of n keywords. A valid segmentation S for q is a sequence of disjunct segments s, each a subsection of q, whose concatenation equals q. There are 2 n-1 possible valid segmentations for q, and (n<ref type="foot" coords="3,161.28,200.69,3.97,6.97" target="#foot_1">2</ref> n)/2 possible segments that contain at least two keywords from q. We derive a score for each of the possible segmentations as follows. First, the n-gram frequency count(s) for each possible segment s is retrieved. The frequencies of n-grams up to n = 5 can be obtained directly from the corpus; we use an index of the Google n-gram corpus in an inverted file from [SPT10] 2 . For longer n-grams up to n = 9, estimations are made analog to the set-based method described in <ref type="bibr" coords="3,354.96,262.43,26.41,9.03" target="#b7">[TP08]</ref>. Having these counts at hand, all valid segmentations are enumerated systematically, and for each segmentation S a score is computed according to the following function:</p><formula xml:id="formula_0" coords="3,231.36,307.37,152.52,23.53">score(S) = s∈S,|s|≥2 |s| |s| • count(s).</formula><p>The factor |s| |s| gives significant weight to longer segments compared to short ones in order to compensate the power law distribution of occurrence frequencies on the web. For example, "new york" has a much larger count than "new york times". The exponential scoring function should help to avoid segmentations like "new york" "times". For a query q we choose from all possible valid segmentations the segmentation S that maximizes score(S). This simple approach is competitive with more involved methods, as evaluation shows <ref type="bibr" coords="3,284.16,416.51,39.23,9.03" target="#b2">[HPSB10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Retrieval phase</head><p>In the real retrieval phase we treat the Carnegie Mellon ClueWeb search engine as a black box and submit segmented queries. When processing a query we adopt the MAXIMUM QUERY framework <ref type="bibr" coords="3,262.80,496.19,29.24,9.03" target="#b4">[SH10]</ref> that works as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic setting</head><p>Starting point is a set W = {w 1 , . . . , w n } of keywords and keyphrases (obtained by the preprocessing), where the indices correspond to the order of the keywords in the original query. Subsets Q ⊆ W can be submitted as queries (complete phrases would be highlighted) assuming the AND notion that requires all keywords from Q to be contained in every document. The ClueWeb engine's reply to a query Q consists of the head of an exhaustive, ranked list L Q of documents containing all the keywords from Q, and an estimation l Q for the result list length</p><formula xml:id="formula_1" coords="3,314.88,617.13,21.57,10.66">|L Q |.</formula><p>The task of MAXIMUM QUERY is to find a largest subset Q ⊆ W that satisfies l min ≤ l Q ≤ l max for given constant lower and upper bounds l min and l max . As for the Sessions track we set l min = 1 (we do not tolerate empty result lists) and l max = 1 000 (1 000 results should be reported per run). The size constraint on Q ensures Q to be as specific of the user's information need as possible while the result list constraints reflect the desired capacity (in the TREC Sessions track case no more than 1 000 results would be considered per run). Adopting the notation from <ref type="bibr" coords="4,344.04,190.67,30.44,9.03" target="#b1">[BG08]</ref> we say that for l Q &lt; l min the query Q is underflowing, whereas for l Q &gt; l max it is overflowing. Queries that are neither under-nor overflowing are valid. A valid query Q is maximal iff adding any keyword from W \ Q results in an underflowing query. The largest maximal queries are the maximum queries-the "target" of MAXIMUM QUERY.</p><p>To further explain our setting, consider the following example scenario with the ten indexed documents d 1 , . . . , d 10 and the set W = {w 1 , . . . , w 5 } of keywords with the keyword document relationship given in Table <ref type="table" coords="4,321.96,274.31,3.77,9.03" target="#tab_0">1</ref>. </p><formula xml:id="formula_2" coords="4,193.92,315.16,226.97,66.89">Keyword Document d1 d2 d3 d4 d5 d6 d7 d8 d9 d10 w1 • • • • • w2 • • • w3 • • • • • • • • w4 • • • • • • w5 • • • • • • •</formula><p>Note that, submitted as a query, the set W itself will not result in any hit on the ten document collection since none of the documents contain all keywords. Let l min = 3 and l max = 4 (i.e., we are looking for subsets of the keywords that are contained in at least 3 and at most 4 documents). Figure <ref type="figure" coords="4,313.44,446.99,4.98,9.03" target="#fig_0">1</ref> shows the hypercube of the possible 2 5 queries; valid queries are shown highlighted.  An example of an overflowing query is {w 3 , w 5 } (six results), whereas {w 1 , w 5 } is underflowing (two results). We have the four maximal valid queries {w 1 , w 3 }, {w 1 , w 4 }, {w 2 , w 3 }, and {w 3 , w 4 , w 5 } corresponding to the upper border in Figure <ref type="figure" coords="5,473.16,142.91,3.77,9.03" target="#fig_0">1</ref>. In our example scenario the unique maximum element is {w 3 , w 4 , w 5 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm for finding maximum queries</head><p>The pseudo-code listing of our approach to find maximum queries is given as Algorithm 1. A first pre-check removes underflowing keywords (lines 1 and 2 of the listing) because they cannot be contained in a maximum query. Note that validity checks (lines 2, 4, 15, and 17) are managed by submitting the query to the ClueWeb search engine. A second pre-check (line 4) ensures that the remaining set W of non-underflowing keywords itself is underflowing, since otherwise W itself is maximum or no valid query can be found at all. For the rather short queries and sessions of the TREC 2010 Sessions track very often no keyword was removed as underflowing and the complete query mostly still overflows. On such sessions we just report the first 1 000 results for the complete query W . As for the few other sessions we applied the following main part of our algorithm.</p><p>The basic idea can be characterized as a depth-first search on a search tree containing all possible queries. Revisiting nodes in the tree is prohibited by processing the keywords in the order of their indices. The algorithm starts trying to find a maximal </p><formula xml:id="formula_3" coords="5,134.76,615.26,192.55,43.70">Q ′ ← ENLARGE(Q ∪ {w}, W left ) 17: if Q ′ is valid and |Q ′ | &gt; |Qmax| then 18: Qmax ← Q ′ 19: return Q</formula><p>valid query containing the first keyword w 1 . It adds the keywords w 2 , w 3 etc. as long as the query remains non-underflowing. Whenever the query underflows, the last keyword is removed and the next one tried. If all keywords have been tried and the resulting query is valid, it is the current candidate to be a maximum query. The algorithm then backtracks to other possible paths in the search tree. Pruning is done whenever the current candidate cannot become larger than the currently stored maximum query. A valid query that contains more keywords than the maximum query so far is stored as the new maximum query. Since this strategy causes an exhaustive search, it is guaranteed to find a maximum query if there is one at all. Note that Algorithm 1 outputs the lexicographically first maximum query. Here lexicographically means the following. Let Q and Q ′ be two different queries and let w min be the keyword with lowest index in the symmetric difference Q△Q ′ = (Q∪Q ′ )\ (Q ∩ Q ′ ). We say that Q comes lexicographically before Q ′ with respect to the keyword ordering w 1 , . . . , w n iff w min ∈ Q. Computing the lexicographically first maximum query is a reasonable approach as it reflects the idea that users in their queries first type the keywords that are most descriptive of their information need. Hence, for the Sessions track we always use the lexicographically first maximum query.</p><p>However, it would not be difficult to compute all maximum queries submitting a few additional web queries and then select one of the maximum queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>The evaluation for both tasks of the Sessions track is done by comparing three rankings. One ranking has to be given for each of the two queries from the originally provided 150 two-query sessions. A third ranking could incorporate the knowledge that both queries form a session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Our runs</head><p>We have submitted two runs of our system, both with the query segmentation preprocessing that highlights contained phrases. As for the individual single queries from each session, both runs process the first and the second queries analogous as follows (respective result lists referred to as query 1 and query 2 ). For each single query we computed a maximum query. In case of more than one maximum query the lexicographically first with respect to the initial keyword ordering of the given queries is chosen. However, as already described earlier, the provided 300 queries are rather short such that very rarely terms were excluded from the queries in order to get back 1 000 results (chosen as the upper bound as the track required submission of 1 000 results). From the maximum queries for the first queries 138 out of 150 contain all phrases; for the second queries 141 out of 150 contain all phrases. For all queries the top 1 000 results where used for the run when available.</p><p>The two runs of our system treated the individual single queries in the same way but used a different scheme for the queries corresponding to the complete sessions. We had an unweighted and a weighted run (respective ranked lists referred to as session u for unweighted and session w for weighted) that worked as follows. In session u we applied the described MAXIMUM QUERY framework for the complete set W of phrases from both queries. In case of more than one maximum query the lexicographically first with respect to the initial keyword ordering of the given queries is chosen. Again, very often all keywords together still overflow; 128 out of 150 maximum queries contain all phrases. For all queries the top 1 000 results where used for the result list when available.</p><p>In session w we additionally pay attention to sessions with generalizations (second query originated by deleting keywords from the first) or specializations (second query contains additional keywords compared to first). Therefore, we weighted the query phrases as follows (and derived maximum queries with respect to that weighting). For phrases just appearing in the first query a term weight in Carnegie Mellon's ClueWeb09 search engine's Indri query language is set to 0.5; in case that it is present just in the second query, the weight is set to 2.0; for terms in both queries the weight is 1.0. The idea is that by submitting a second query the user figured out that her first query did not satisfy her information need and thus the new keywords in the second query should be treated as more important. Note that the weighting scheme also influences the "drifting" sessions that are neither specializations nor generalizations. Again, very often all keywords together still overflow; 128 out of 150 maximum queries contain all phrases. For all queries the top 1 000 results where used for then result list when available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>The runs were evaluated on 136 query sessions for which NIST provided relevance judgments. No judgments were given for the topics 24, 30, 35, 36, 58, 100, 114, 118, 120, 126, 130, and 136. Based on the provided relevance judgments, three evaluation metrics are used: nsDCG@10, nsDCG_dupes@10, and nDCG@10. The nsDCG@10 metric <ref type="bibr" coords="7,162.72,441.83,40.40,9.03" target="#b3">[JPDN08]</ref> is computed as: 10 r=1 2 rel(r,query 1 ) -1 (log 2 (r + 1)) * (log 4 (1 + 3)) + 10 r=1 2 rel(r,query 2 /session ) -1 (log 2 (r + 10 + 1)) * (log 4 (2 + 3))</p><p>,</p><p>where rel(r, query 1 ) is the relevance of the document in rank r in the query 1 result list and rel(r, query 2 /session) is the relevance of the document at rank r in the query 2 or the session result lists. The nsDCG_dupes@10 metric is similar to nsDCG@10 except that duplicate documents in the top-10 ranks of the query 2 or the session result lists that appeared in the top-10 ranks of the query 1 result list are considered non-relevant. Note that nsDCG@10 and nsDCG_dupes@10 evaluate the entire session and thus for each one of the two metrics there is an evaluation score for query 1 → query 2 and an evaluation score for query 1 → session (cf. Tables <ref type="table" coords="7,338.40,585.11,4.98,9.03" target="#tab_3">2</ref> and<ref type="table" coords="7,362.76,585.11,3.63,9.03" target="#tab_4">3</ref>). The nDCG@10 is the nDCG metric implemented as 10 r=1 2 rel(r,query 1 /query 2 /session) -1 (log 2 (r + 1))</p><p>for the three ranked lists for query 1 , query 2 , or session in isolation. Our nsDCG@10 results and for comparison the median and max of all systems are given in Table <ref type="table" coords="8,211.68,265.55,3.77,9.03" target="#tab_3">2</ref>. It is interesting to note that our runs as well as the median and maximum of all systems have better results query 1 → query 2 than for query 1 → session. This seems to suggest that the techniques to incorporate knowledge on the whole session do not really work for the test cases. This is not that surprising at least for our systems. Our approaches were not developed having such rather short queries and sessions in mind; the targeted use case are longer sessions with more keywords (cf. the discussion in Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Our obtained results</head><p>The nsDCG@10 for query 1 → session is the official metric for Task (2)retrieval performance over an entire query session. Our overall averaged nsDCG@10 query 1 → session score on all 136 topics is better for our weighted run compared to the unweighted run. Nevertheless, it is still slightly below the median of all systems. However, analyzed topicwise, our weighted run beats the median of all systems on 84 of 136 topics (61.76%) and we achieve the best (maximum) performance of all systems on 4 topcis (2.94%). This is quite surprising as we did not expect to perform that well on the rather short queries and sessions that do not really fit the use case of our system. The behavior for the duplicate free version of nsDCG (results in Table <ref type="table" coords="8,432.84,559.67,4.18,9.03" target="#tab_4">3</ref>) is similar to the pure nsDCG. Our weighted variant improves over the unweighted one but again is slightly below the median of all systems. However, seen topicwise, our weighted session w run beats the median of all systems on 86 out of 136 topics (63.24%) and yields the best (maximum) performance of all systems for 5 topics (3.68%).</p><p>As for Task (1)-retrieval performance for a given query by using the user's previous query-, performance can be evaluated by comparing the scores of query 1 → query 2 and query 1 → session for the nsDCG metrics. Our results on the session result lists are worse compared to the query 2 lists for both nsDCG metrics but this is the same with the median or the maximum of all systems. Again it should be noted that we expect our framework to perform better for longer sessions and queries where computing maximum queries makes more sense (cf. the discussion in Section 5).</p><p>As to compare the results "query-wise" we give our obtained nDCG values in Table 4. Again, our system (weighted and unweighted) and the median and best over all systems perform best on the second query alone not incorporating knowledge about the first query. At least for our system we hypothesize that we could perform better on longer sessions with more keywords; which gives rise to the following discussion and concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>As can be seen from the evaluation, our approach performs comparable to the median of all systems. This is quite encouraging, however, at least on average the retrieval performance tables show that yet our approach yields no improvement by considering two queries as a session compared to just processing the second query alone. But note that this holds for the median and maximum of all systems as well.</p><p>One reason might be the rather short queries and the session length of two queries. At least our approach is designed for longer sessions with more queries and keywords. In case of the 150 sessions of the 2010 TREC Sessions track often the complete query containing all keywords returned enough hits such that computing the maximum query does not remove any keywords. Furthermore, some topics are quite similar to each other (e.g., the various queries for the obama family tree) or rather "artificial" (to be or not to be meant to check stopword techniques?!).</p><p>Nevertheless, evaluating session processing techniques and not just single query retrieval is an important task from our perspective. As the Sessions track addresses exactly this problem it opens the evaluation perspective on a very interesting research area and we hope that it will be continued. An interesting future task might however involve longer and more diverse sessions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,190.80,631.84,233.60,8.16"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Hypercube of possible queries in the example scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,189.36,297.04,236.48,8.16"><head>Table 1 .</head><label>1</label><figDesc>Keyword document relationship in the example scenario.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,194.76,156.52,225.08,65.76"><head>Table 2 .</head><label>2</label><figDesc>Results for nsDCG@10 averaged over all 136 topics.</figDesc><table coords="8,194.76,173.92,225.01,48.36"><row><cell></cell><cell cols="2">query 1 → query 2 query 1 → session</cell></row><row><cell>unweighted</cell><cell>0.1796</cell><cell>0.1674</cell></row><row><cell>weighted</cell><cell>0.1796</cell><cell>0.1724</cell></row><row><cell>median all systems</cell><cell>0.2044</cell><cell>0.1784</cell></row><row><cell>max all systems</cell><cell>0.2488</cell><cell>0.2375</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,182.04,464.56,251.24,65.76"><head>Table 3 .</head><label>3</label><figDesc>Results for nsDCG@10_dupes averaged over all 136 topics.</figDesc><table coords="8,194.76,481.96,225.01,48.36"><row><cell></cell><cell cols="2">query 1 → query 2 query 1 → session</cell></row><row><cell>unweighted</cell><cell>0.1859</cell><cell>0.1654</cell></row><row><cell>weighted</cell><cell>0.1859</cell><cell>0.1748</cell></row><row><cell>median all systems</cell><cell>0.2067</cell><cell>0.1869</cell></row><row><cell>max all systems</cell><cell>0.2449</cell><cell>0.229</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,197.28,196.36,220.76,65.76"><head>Table 4 .</head><label>4</label><figDesc>Results for nDCG@10 averaged over all 136 topics.</figDesc><table coords="9,217.44,214.33,179.65,47.79"><row><cell></cell><cell cols="2">query 1 query 2 session</cell></row><row><cell>unweighted</cell><cell cols="2">0.1638 0.2014 0.1621</cell></row><row><cell>weighted</cell><cell cols="2">0.1638 0.2014 0.1776</cell></row><row><cell cols="2">median all systems 0.1894 0.2144</cell><cell>0.17</cell></row><row><cell>max all systems</cell><cell cols="2">0.2354 0.2658 0.2602</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.72,656.71,186.57,8.12"><p>http://boston.lti.cs.cmu.edu:8085/clueweb09/search/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.72,638.11,335.73,8.12;3,144.72,649.15,86.49,8.12"><p>A corresponding service that can also handle wildcard queries etc. is freely available online: http://www.netspeak.cc/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,172.32,643.87,308.27,8.12;9,172.32,654.79,142.88,8.12" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,294.00,643.87,186.59,8.12;9,172.32,654.79,66.36,8.12">Web 1T 5-gram Version 1. Linguistic Data Consortium LDC</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Franz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">13</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,172.32,119.59,308.12,8.12;10,172.32,130.63,119.48,8.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,309.12,119.59,167.67,8.12">Random sampling from a search engine&apos;s index</title>
		<author>
			<persName coords=""><forename type="first">Ziv</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-Yossef</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxim</forename><surname>Gurevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,172.32,130.81,68.16,7.86">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,177.12,141.55,303.46,8.12;10,172.32,152.47,308.27,8.12;10,172.32,163.51,308.20,8.12;10,172.32,174.61,308.20,7.86;10,172.32,185.35,308.24,8.12;10,172.32,196.39,37.28,8.12" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,441.36,141.55,39.23,8.12;10,172.32,152.47,106.41,8.12">The Power of Naïve Query Segmentation</title>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christof</forename><surname>Bräutigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,399.00,163.69,81.52,7.86;10,172.32,174.61,308.20,7.86;10,172.32,185.53,77.92,7.86">Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2010</title>
		<editor>
			<persName><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stéphane</forename><surname>Marchand-Maillet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Efthimis</forename><forename type="middle">N</forename><surname>Efthimiadis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</editor>
		<meeting>eeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2010<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-07-23">July 19-23, 2010. July 2010</date>
			<biblScope unit="page" from="797" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,176.16,207.31,304.40,8.12;10,172.32,218.23,308.26,8.12;10,172.32,229.45,308.36,7.86;10,172.32,240.19,278.72,8.12" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,172.32,218.23,274.62,8.12">Discounted cumulated gain based evaluation of multiple-query IR sessions</title>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lois</forename><forename type="middle">M L</forename><surname>Delcambre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marianne</forename><forename type="middle">Lykke</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,467.64,218.41,12.95,7.86;10,172.32,229.45,308.36,7.86;10,172.32,240.37,16.00,7.86">Advances in Information Retrieval , 30th European Conference on IR Research, ECIR 2008</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-04-03">March 30-April 3, 2008. 2008</date>
			<biblScope unit="page" from="4" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,172.32,251.11,308.08,8.12;10,172.32,262.33,308.13,7.86;10,172.32,273.07,197.96,8.12" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,295.92,251.11,150.34,8.12">Making the Most of a Web Search Session</title>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,462.60,251.29,17.80,7.86;10,172.32,262.33,308.13,7.86;10,172.32,273.25,94.05,7.86">2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2010)</title>
		<imprint>
			<date type="published" when="2010-08">August 2010</date>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,172.32,283.99,308.12,8.12;10,172.32,295.03,308.06,8.12;10,172.32,305.95,308.32,8.12;10,172.32,316.87,181.52,8.12" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,305.88,283.99,170.70,8.12">Introducing the User-over-Ranking Hypothesis</title>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,182.52,295.21,297.86,7.86;10,172.32,306.13,140.25,7.86">Advances in Information Retrieval, Proceedings of the 33rd European Conference on Information Retrieval (ECIR 2011)</title>
		<title level="s" coord="10,319.92,305.95,131.08,8.12">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="10,172.32,327.79,308.26,8.12;10,172.32,338.83,308.24,8.12;10,172.32,349.75,308.24,8.12;10,172.32,360.85,308.20,7.86;10,172.32,371.71,308.26,8.12;10,172.32,382.63,241.16,8.12" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,365.28,327.79,115.30,8.12;10,172.32,338.83,81.51,8.12">Retrieving Customary Web Language to Assist Writers</title>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Trenkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,172.32,360.85,308.20,7.86;10,172.32,371.89,125.97,7.86">Advances in Information Retrieval, Proceedings of the 32nd European Conference on Information Retrieval (ECIR 2010)</title>
		<title level="s" coord="10,365.40,371.89,115.18,7.86;10,172.32,382.81,14.85,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gabriella</forename><surname>Kazai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Suzanne</forename><surname>Little</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Roelleke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefan</forename><forename type="middle">M</forename><surname>Rüger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Keith</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5993</biblScope>
			<biblScope unit="page" from="631" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,172.32,393.55,308.26,8.12;10,172.32,404.59,308.06,8.12;10,172.32,415.51,308.00,8.12;10,172.32,426.43,20.00,8.12" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,277.20,393.55,203.38,8.12;10,172.32,404.59,105.32,8.12">Unsupervised query segmentation using generative language models and Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,298.32,404.77,182.06,7.86;10,172.32,415.69,117.52,7.86">Proceedings of the 17th International Conference on World Wide Web, WWW 2008</title>
		<meeting>the 17th International Conference on World Wide Web, WWW 2008<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">April 21-25, 2008. 2008</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
