<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,83.99,112.00,444.02,15.15;1,234.71,133.91,142.57,15.15">Indian Statistical Institute, Kolkata at TREC 2010 : Legal Interactive</title>
				<funder ref="#_7V92ddk">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.44,169.17,75.14,6.99"><forename type="first">Kripabandhu</forename><surname>Ghosh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Statistical Institute</orgName>
								<address>
									<settlement>Kolkata</settlement>
									<region>West Bengal</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.90,169.17,80.10,6.99"><forename type="first">Swapan</forename><forename type="middle">Kumar</forename><surname>Parui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Statistical Institute</orgName>
								<address>
									<settlement>Kolkata</settlement>
									<region>West Bengal</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.33,169.17,74.73,6.99"><forename type="first">Prasenjit</forename><surname>Majumder</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dhirubhai Ambani Institute of Information and Communication Technology</orgName>
								<address>
									<settlement>Gandhinagar</settlement>
									<region>Gujarat</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.83,183.12,81.26,6.99"><forename type="first">Ayan</forename><surname>Bandyopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Statistical Institute</orgName>
								<address>
									<settlement>Kolkata</settlement>
									<region>West Bengal</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.53,183.12,27.87,6.99"><forename type="first">S</forename><surname>John</surname></persName>
						</author>
						<author>
							<persName coords="1,346.22,183.12,50.81,6.99"><forename type="first">J</forename><forename type="middle">Raja</forename><surname>Singh</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Indian Institute Technology</orgName>
								<address>
									<settlement>Kharagpur</settlement>
									<region>West Bengal</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,83.99,112.00,444.02,15.15;1,234.71,133.91,142.57,15.15">Indian Statistical Institute, Kolkata at TREC 2010 : Legal Interactive</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">001D041DB2C6DF4E66F68E4C6C8897AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indian Statistical</head><p>Institute, Kolkata participated in TREC for the first time this year. We participated in TREC Legal Interactive task in two topics namely, Topic 301 and Topic 302. We reduced the size of the corpus by Boolean retrieval using Lemur 4.11 1 and followed it by a clustering technique. We chose members from each cluster (which we called seeds) for relevance judgement by the TA and assumed all other members of the cluster whose seeds are assessed as relevant to be relevant.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The motivation for our participation in TREC Legal track was to test the conventional tools of Information Retrieval on legal data. We chose the Interactive task <ref type="bibr" coords="1,322.80,427.93,10.52,8.74" target="#b2">[3]</ref> because of its uniqueness as in contrast with any other Ad Hoc task it allows the participants to interact with the reviewer as a process to understand the notion of relevance/responsiveness <ref type="bibr" coords="1,243.34,451.84,9.96,8.74" target="#b0">[1]</ref>. The data set this year was the EDRM Enron v2 dataset which consisted of Enron emails and their native attachments separately provided. There were two formats of the data on offer viz. XML and PST. Later on deduplicated text-only version was also available which we chose for our experiment. The data was available at http://durum0.uwaterloo.ca/trec/legal10/. The emails were of 596MB (compressed) and the native attachments were of 6GB (compresssed). We used Indri search engine of Lemur 4.11 toolkit for Boolean retrieval and Terrier 3.0<ref type="foot" coords="1,357.98,510.04,3.97,6.12" target="#foot_1">2</ref> for ranking the retrieved set using DFR <ref type="bibr" coords="1,72.00,523.57,15.00,8.74" target="#b1">[2]</ref>-BM25 <ref type="bibr" coords="1,116.07,523.57,10.52,8.74" target="#b3">[4]</ref> model. In section 2, we describe our approach. In section 3, we present the results and we conclude in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>We attempted to apply DFR-BM25 ranking model on the TREC legal corpus. We chose Terrier 3.0 as this toolkit has most of the IR methods implemented within. But as we received the TREC legal data set we realised that it would be difficult to manage such a large volume of data. So, we decided to reduce the corpus size by Boolean retrieval. We chose Lemur 4.11 as it supports various useful Boolean query operators which would suit our purpose. On the set obtained from Boolean retrieval we decided to apply ranked retrieval techniques. We decided to index only the original emails. The attachments were not indexed. We decided to add the attachments of the relevant parent emails in the final result because we assumed that the attachments of a relevant parent mail are likely to be relevant as well. The use of Boolean retrieval has the disadvantage that it will limit further search to the documents retrieved at this stage and have an adverse effect on our recall performance. But it would scale down the huge corpus size considerably (see Table <ref type="table" coords="2,531.14,111.02,4.43,8.74" target="#tab_0">1</ref>) and enable us to perform our experiments on a smaller set which would reduce processing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topic 301:</head><p>Topic 301 was as follows:</p><p>All documents or communications that describe, discuss, refer to, report on, or relate to onshore or offshore oil and gas drilling or extraction activities, whether past, present or future, actual, anticipated, possible or potential, including, but not limited to, all business and other plans relating thereto, all anticipated revenues therefrom, and all risk calculations or risk management analyses in connection therewith.</p><p>Based on the topic, we formed a Indri<ref type="foot" coords="2,256.01,259.34,3.97,6.12" target="#foot_2">3</ref> query which combines the result of the following eight queries:</p><p>1. #or( #band( oil gas onshore drilling anticipated revenue risk explosion fire) #band( oil gas offshore drilling anticipated revenue risk explosion fire) #band( oil gas onshore extraction anticipated revenue risk explosion fire) #band( oil gas offshore extraction anticipated revenue risk explosion fire))</p><p>2. #or ( #band( #1 (oil and gas) drilling onshore rig anticipated revenue risk) #band( #1 (oil and gas) extraction onshore rig anticipated revenue rig risk) #band( #1 (oil and gas) drilling offshore rig anticipated revenue risk) #band( #1 (oil and gas) extraction offshore rig anticipated revenue risk) )</p><p>3. #or ( #band( oil gas drilling onshore) #band( oil gas drilling offshore) #band( oil gas extraction onshore) #band( oil gas extraction offshore) ) narr Narrative: To be relevant, a document should describe onshore or offshore oil and gas drilling or extraction activities all business and other plans related, all anticipated revenues, and all risk calculations or risk management analyses</p><p>We applied DFR-BM25 model of Terrier 3.0 using the above query on the aforesaid set obtained by Boolean retrieval of Lemur. We chose the top 10 documents of the resulting ranked-list for Topic Authority assessment. In our first discussion with TA for Topic 301 (Mira Edelman), we offered these 10 documents for her opinion. We further requested for a clearer notion of relevance for the topic. Only three of the documents were judged relevant:</p><p>• Enron oil/gas drilling/extraction activities and revenues earned therefrom</p><p>• Enron oil/gas evaluation/audit report • Enron oil/gas transaction/ agreement/ transportation would be relevant.</p><p>Gas agreement documents: According to the TA, any document about a oil/gas agreement between Enron and another company would be relevant. One of the judged documents about Gas agreement as cited out by the TA was of the form:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ENFOLIO EXCESS GAS PURCHASE AGREEMENT (RESERVES COMITTED/INDEX PRICING)</head><p>Enron North America Corp. ......</p><p>To capture documents with a similar structure we formed a query : q301-1 : #5(#1(gas purchase agreement) reserves committed index pricing #syn(ena enron))</p><p>We used our clustering algorithm (to be discussed in section 2.3) to form clusters. We sent one member document from each, which we call a seed, for TA assessment. Five documents were judged relevant and their clusters were added to our set of relevant documents. This was a far too focussed query. To capture other agreement documents we formed another Boolean query (q301-2) : #uw10(#syn(oil gas) #syn(ena enron) #1(purchase agreement))</p><p>As we started interacting with the TA, we realised that instead of making a two stage retrieval (i.e. Boolean followed by ranked) we could get better results by Boolean retrieval alone if we manage to choose appropriate keywords. On the Boolean retrieved set, we decided to apply clustering instead (see section 2.3 for details). This technique seemed to work well as judging by TA feedback we seemed to get more relevant documents. In the wake of this, we decided to use TA advice as feedback and did not make use of any relevance feedback technique. So, in the remainder of our experiments we stuck to Boolean followed by clustering approach. The key terms "enterprise risk management", "operational risk" etc were suggested by both TA feedback and Rocchio Relevant feedback technique in Terrier 3.0.</p><p>Enron audited report documents: To obtain the documents related to Enron's audited report on net income related to oil/gas activities we formed the query: q301-5 : #band(#1(oil and gas) #syn(ena enron) #1(financial statement) asset liability equity income expense audit #1(consolidated net income))</p><p>Oil and gas transportation documents: As suggested by the TA, we looked for the documents about Enron oil and gas transportation activities by the following query : q301-6 : #band( #1(#syn(oil gas) #syn(drill extraction)) #syn(ena enron) #5(#syn(oil gas) transport))</p><p>The last three queries were not high yielding. So we decided to make a large set of all drilling-extraction documents. We formed the Lemur query : q301-7 : #band(#1(oil and gas) #syn(ena enron) #syn(drilling extraction))</p><p>This again shrunk the original corpus as given in table <ref type="table" coords="4,313.90,476.73,3.87,8.74" target="#tab_1">2</ref>.</p><p>As before, we made a ranked retrieval with DFR-BM25 using the Terrier query: title Topic: Enron Oil or Gas Drilling or Extraction desc Description: Document describes Enron oil and gas onshore drilling or extraction activities narr Narrative: To be relevant, a document should describe Enron onshore or offshore oil and gas drilling or extraction activities</p><p>We produced manually picked documents from the set thus obtained for TA assessment. The following subcollections evolved from the above set of retrieved documents after TA feedback:</p><p>1. Enron weekly summary -news about Enron's business news 2. Enron Btu weekly summary (news about Enron's internal affairs) 3. Enron SIC codes -This category encouraged fresh search from the whole collection by the Boolean query:</p><p>q301-8 : #band(#1(oil and gas) #syn(ena enron) #1(sic code))</p><p>4. Enron stock</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Enron Austin</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pad Gas documents :</head><p>A document retrieved by q301-6 (Oil and gas transportation) talked about Pad Gas which led to the query : q301-9 : #band(#syn(oil gas) #syn(ena enron) #1(pad gas))</p><p>Further analyses led to the following types:</p><p>Texas business plan documents: q301-10 : #band(#syn(oil gas) #syn(ena enron) #1(texas gas) #1(business plan))</p><p>Competitive analysis documents: Another document retrieved by q301-6 (Oil and gas transportation) led to q301-11 : #band(#syn(oil gas) #syn(ena enron) #1(competitive analysis))</p><p>California Energy Commission documents: Another document retrieved by q301-6 (Oil and gas transportation) prompted us to form q301-11 : #band(#syn(oil gas) #syn(ena enron) #syn(drilling extraction) revenue #1(california energy commission))</p><p>Global Contracts documents: We intended to look for the documents about the Global oil/gas contracts of Enron. So we came up with the query: q301-12 : #band(#syn(oil gas) #syn(ena enron) #1(global contract) #syn(purchase sale transport) financial) Confidential Enron documents: In one of the later calls, TA suggested that the documents containing Enron's confidential news or reports about oil/gas are to be relevant. This led us to form the following query: q301-13 : #band(#syn(oil gas) #syn(ena enron) #20(confidential propriety enron internal))</p><p>Oil purchase documents: Finally we looked to grasp the probable left out documents by a query: q301-14 : #band(enron #1(oil purchase) agreement)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic 302:</head><p>Topic 302 was as follows:</p><p>All documents or communications that describe, discuss, refer to, report on, or relate to actual, anticipated, possible or potential responses to oil and gas spills, blowouts or releases, or pipeline eruptions, whether past, present or future, including, but not limited to, any assessment, evaluation, remediation or repair activities, contingency plans and/or environmental disaster, recovery or clean-up efforts.</p><p>We started our experiments with Topic 301 where we noted that query formation merely from the keywords of the topic can be misleading in reaching the relevant documents. So, in case of Topic 302 we didn't form different combinations of Indri queries and instead tried to get hold of a few documents likely to be useful for our first interaction with the TA. So, we started with the Boolean-ranked strategy. We formed an Indri query : The Boolean query yielded 1350 documents. The number is tabulated in table <ref type="table" coords="6,431.87,197.79,3.87,8.74" target="#tab_2">3</ref>. Thus the corpus was reduced to 0.197%. We formed a ranked-list using Terrier 3.0 DFR-BM25 using the query: title Topic: Oil and Gas Spills desc Description: Document describes oil and gas spills, blowouts,releases,pipeline eruptions and assessments,repair or remediation narr Narrative: To be relevant, a document should describe oil and gas spills, blowouts,releases,pipeline eruptions assessment, evaluation,remediation or repair activities,contingency plans and/or environmental disaster,recovery or clean-up efforts Top 10 queries were presented to TA of Topic 302 (John Curran) on our first call. This interaction revealed a gross misinterpretation of the notion of relevance on our part as none of the 10 retrieved documents were deemed responsive! At this stage, we had decided on using our clustering algorithm and so, we will be using this strategy in the remaining part of Topic 302.</p><p>Clean up documents: TA opined that any document about "clean up effort" of oil/gas related to Enron would be responsive. These are the documents which narrate that some gas/oil spill incident caused by Enron has taken place and the consequent clean up efforts have been initiated. So we reformulated our query as: q302-1 : #band( enron #syn(oil gas) #syn(spill release) #uw10(#1(clean up) spill))</p><p>As clean-up measures would involve the application of tools like skimmers, booms and chemical dispersants, we formed a query as follows: q302-2 : #band(#syn(enron ena) #syn(oil gas) #uw100(spill #syn(skimmer boom dispersant))</p><p>River spill documents: To capture documents about oil/gas spills in river areas we formed query: q302-3 : #band(enron #syn(oil gas) spill river)</p><p>Transredes spill documents: To target documents about Transredes spill we came up with q302-4 : #band(enron #syn(oil gas) spill transredes) Litigation Memorandum documents: For the documents about litigations about oil/gas spill against Enron our query was: q302-5 :#band(enron #syn(oil gas) spill litigation memorandum)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Plan documents:</head><p>During one interation, the TA also suggested that the documents about action plan of Enron about the prevention/remedial efforts in case of oil/gas spills would be responsive. This prompted us to form the following query: q302-6 :#band(enron #syn(oil gas) spill #1(action plan) )</p><p>Spill Environmental documents: To retrieve the documents about environmantal hazards caused by oil/gas spill by Enron and the legal actions taken in this issue we formed the query: q302-7 :#band(enron oil spill #10(environmental #syn(law matter))) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Clustering Algorithm</head><p>On the basis of emperical studies we chose 0.3 as the threshold value. Initially, we provided most of the retrieved documents for judgement. But, gradually we observed that there exist many clusters of very similar documents and the relevance of all the documents in such a cluster can be decided by judging a few Let G(V, E) be an undirected graph, where V (the set of vertices) is the set of all documents in a given collection C. There is an edge e ∈ E between vertices v 1 (d 1 ), v 1 (d 2 ) ∈ V , d 1 , d 2 being documents of C, if the normalised cosine similarity between d 1 and d 2 is greater than threshold ( In our experiments, threshold is chosen as 0.3). Next, the connected components of G are found out. These components are our clusters. This is basically a single-linkage clustering which we thought would be appropriate for our experiment.</p><p>A cluster containing one or more judged relevant document(s) is considered as a "relevant cluster". In other words, each document of the cluster is assumed to be relevant. For a cluster not containing a judged relevant document, we send a few arbitrarily chosen documents as the representatives (or seeds) of the cluster for TA judgement. Such a cluster will be deemed relevant or nonrelevant according as its seeds are relevant or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We believe that the chances of achieving better understanding of the notion of relevance of a legal topic is directly proportional to the number of hours expended with the Topic Authority. Our team spent more time with TA of Topic 301 and managed to retrieve more useful documents. Table <ref type="table" coords="8,413.14,432.28,4.98,8.74" target="#tab_3">4</ref> illustrates this notion.</p><p>Table <ref type="table" coords="8,115.27,444.24,4.98,8.74" target="#tab_4">5</ref> shows the results of using the clustering algorithm and generation of more tentative relevant documents starting from a relatively small number of seeds. These results depict the performance of high yielding topics like q301-2, q301-13, q301-1. For the low yielding topics, clustering technique was of little use.</p><p>The results of Topic 301 are shown in Table <ref type="table" coords="8,288.36,492.06,3.87,8.74" target="#tab_5">6</ref>. After the first-pass sampling of Topic 301, 140 of the documents submitted by our team were selected for assessment. There were 42 documents on which we differed with the assessors. We appealed against 18 of them and 15 of them went in our favour. The sampling and adjudication details is presented in tabular form in table <ref type="table" coords="8,383.63,527.92,3.87,8.74" target="#tab_6">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This participation was a great learning experience for our team. Resource constraints and time constraints were major challenges on our part. We believe that we could have made much better use of TA assessment as we managed to interact with TAs for a period of one month. We came up with a clustering technique which was applied on the output of Boolean search to help us maximize the benefit of these interactions. We decided to submit clusters of relevant documents, about which we had high degree of confidence from TA feedback. Our high success in appeal (83.33% for Topic 301 and 59.46% for Topic 302) shows that we managed to capture the notion of relevance to a considerable degree. In a nutshell, we prepared a precision based system aimed at capturing the relevant documents. Our high precision values attest to the fact that we have succeeded in our approach to a great extent. But, it seems that this has come at the expense of recall. This reason behind this is the fact that we worked with a very small sub-collection of the given corpus. We feel that reducing expert dependence (here TA) would speed up the retrieval process. So, it is worthwhile to look to automate the process of query formation by query expansion tools that make use of legal knowledge. Doing away with a human expert competely may reduce system reliability and completeness. So, we may look to achieve more guidence in lesser number of interactions. We used a single linkage clustering which are not without its weaknesses. So, we hope to apply a better algorithm. Also, we hope to make a comparison of Boolean and ranked retrieval techniques. Finally, we would like to strike a balance between precision and recall values. This may be achieved if we work with the whole corpus instead of shrinking it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.00,402.38,468.00,265.77"><head>Table 1 :</head><label>1</label><figDesc>Collection statistics for Topic 301 Document describes oil and gas onshore drilling or extraction activities, business plans, revenues and risk management</figDesc><table coords="2,72.00,402.38,468.00,265.77"><row><cell>4. #filreq ( #band(oil and gas) #combine( onshore offshore drilling extraction revenue risk ) )</cell></row><row><cell>5. #or ( #band( #1 (oil drilling) onshore ) #band( #1 (oil extraction) onshore ) #band( #1 (oil drilling)</cell></row><row><cell>offshore ) #band( #1 (oil extraction) offshore ) #band( #1 (gas drilling) onshore ) #band( #1 (gas</cell></row><row><cell>extraction) onshore ) #band( #1 (gas drilling) offshore ) #band( #1 (gas extraction) offshore ))</cell></row><row><cell>6. #or ( #band ( #30(onshore oil drilling) revenue) #band( #30 (onshore oil extraction) revenue) #band</cell></row><row><cell>( #30 (onshore gas drilling) revenue ) #band ( #30 (onshore gas extraction) revenue) #band ( #30(off-</cell></row><row><cell>shore oil drilling) revenue ) #band ( #30 (offshore oil extraction) revenue ) #band ( #30 (offshore gas</cell></row><row><cell>drilling) revenue ) #band ( #30 (offshore gas extraction) revenue) )</cell></row><row><cell>7. #or ( #30(onshore oil drilling) #30 (onshore oil extraction) #30 (onshore gas drilling) #30 (onshore</cell></row><row><cell>gas extraction) #30(offshore oil drilling) #30 (offshore oil extraction) #30 (offshore gas drilling)#30</cell></row><row><cell>(offshore gas extraction) )</cell></row><row><cell>8. #band (#1 ( #syn(oil gas) #syn(drilling extraction) ) #syn(onshore offshore) )</cell></row><row><cell>This query returned a set of 2896 documents as compared to 685592 (a reduction to 0.42%) in the given</cell></row><row><cell>collection of emails. The comparison is given in table 1.</cell></row><row><cell>Then we had to select a sample from the retrieved set for TA judgement. Instead of picking random</cell></row><row><cell>samples, we decided to rank the retrieved set based on a Terrier query and select the top ranked documents</cell></row><row><cell>for TA assessment. So we formed a the following Terrier query :</cell></row><row><cell>title Topic: Oil or Gas Drilling or Extraction</cell></row><row><cell>desc Description:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,73.96,468.00,156.47"><head>Table 2 :</head><label>2</label><figDesc>Another collection statistics for Topic 301</figDesc><table coords="4,72.00,73.96,468.00,156.47"><row><cell>Collection</cell><cell cols="2">No. of documents Collection size</cell></row><row><cell>Original corpus</cell><cell>685592</cell><cell>3.8GB</cell></row><row><cell>Collection after Boolean retrieval</cell><cell>2715</cell><cell>225.7MB</cell></row><row><cell cols="3">Risk management documents: To retrieve documents regarding Enron risk management issues we</cell></row><row><cell>formed the following two Boolean Lemur queries:</cell><cell></cell><cell></cell></row><row><cell cols="3">q301-3 : #band(#1(enterprise risk management) #syn(ena enron) risk asset audit)</cell></row><row><cell cols="3">q301-4 : #band(#syn(oil gas) #syn(enron ena) risk #(operational risk) asset audit)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,73.96,388.36,108.65"><head>Table 3 :</head><label>3</label><figDesc>Collection statistics for Topic 302</figDesc><table coords="6,72.00,73.96,388.36,108.65"><row><cell>Collection</cell><cell cols="2">No. of documents Collection size</cell></row><row><cell>Original corpus</cell><cell>685592</cell><cell>3.8GB</cell></row><row><cell>Collection after Boolean retrieval</cell><cell>1350</cell><cell>17.4MB</cell></row><row><cell cols="2">#band(#syn(enron ena) #syn(oil gas) #syn(spill blowout release eruption))</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,104.78,244.86,402.44,336.43"><head>Table 4 :</head><label>4</label><figDesc>Time with TA and size of retrieved set</figDesc><table coords="7,104.78,244.86,402.44,336.43"><row><cell cols="4">Topic number Time expended with TA No of documents retrieved as relevant(tentative)</cell></row><row><cell>301</cell><cell cols="2">2hours 38 mins</cell><cell>693</cell></row><row><cell>302</cell><cell cols="2">1 hour 37 mins</cell><cell>109</cell></row><row><cell></cell><cell cols="3">query no no of seeds no of docs in relevant clusters</cell></row><row><cell></cell><cell>q301-1</cell><cell>4</cell><cell>79</cell></row><row><cell></cell><cell>q301-2</cell><cell>5</cell><cell>197</cell></row><row><cell></cell><cell>q301-3</cell><cell>4</cell><cell>8</cell></row><row><cell></cell><cell>q301-4</cell><cell>7</cell><cell>12</cell></row><row><cell></cell><cell>q301-5</cell><cell>2</cell><cell>12</cell></row><row><cell></cell><cell>q301-6</cell><cell>3</cell><cell>9</cell></row><row><cell></cell><cell>q301-8</cell><cell>4</cell><cell>21</cell></row><row><cell></cell><cell>q301-9</cell><cell>3</cell><cell>16</cell></row><row><cell></cell><cell>q301-10</cell><cell>1</cell><cell>3</cell></row><row><cell></cell><cell>q301-11</cell><cell>7</cell><cell>67</cell></row><row><cell></cell><cell>q301-12</cell><cell>1</cell><cell>20</cell></row><row><cell></cell><cell>q301-13</cell><cell>12</cell><cell>122</cell></row><row><cell></cell><cell>q301-14</cell><cell>2</cell><cell>62</cell></row><row><cell></cell><cell>q302-1</cell><cell>7</cell><cell>45</cell></row><row><cell></cell><cell>q302-2</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell>q302-3</cell><cell>6</cell><cell>30</cell></row><row><cell></cell><cell>q302-4</cell><cell>2</cell><cell>6</cell></row><row><cell></cell><cell>q302-5</cell><cell>1</cell><cell>6</cell></row><row><cell></cell><cell>q302-6</cell><cell>1</cell><cell>4</cell></row><row><cell></cell><cell>q302-7</cell><cell>1</cell><cell>19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,245.16,604.38,121.68,8.74"><head>Table 5 :</head><label>5</label><figDesc>Seeds and Clusters</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,72.00,73.96,468.00,144.91"><head>Table 6 :</head><label>6</label><figDesc>Results : Topic 301 members belonging to it. We believed that a document similar to a relevant document is likely to be relevant proportionally with the degree of similarity. So we formed clusters based on cosine similarity and tested out our assumption through relevance judgements. Positive results encouraged us to go on with it. The formal algorithm is as follows:</figDesc><table coords="8,159.50,73.96,292.50,45.40"><row><cell>Run status</cell><cell cols="3">Est. Recall Est. Precision Est. F 1</cell></row><row><cell>Pre-appeal</cell><cell>0.017</cell><cell>0.643</cell><cell>0.033</cell></row><row><cell>Post-appeal</cell><cell>0.027</cell><cell>0.867</cell><cell>0.052</cell></row><row><cell>% Improvement on appeal</cell><cell>58.82</cell><cell>34.84</cell><cell>57.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,72.00,527.92,468.00,147.31"><head>Table 7 :</head><label>7</label><figDesc>. Sampling and Adjudication : Topic 301The results of Topic 302 are shown in Table8. After first-pass sampling, 267 of the 274 documents submitted by us were chosen. There were 171 documents which we thought were Responsive and the assessors had deemed them as Non-Responsive. We appealed against 37 of them and got 22 overturned (see table9).</figDesc><table coords="8,85.22,550.58,441.57,33.05"><row><cell cols="6">Docs submitted Sampled for Disagreements Appealed against Appeals won % Appeals won</cell></row><row><cell></cell><cell>assessment</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1394</cell><cell>140</cell><cell>42</cell><cell>18</cell><cell>15</cell><cell>83.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,200.01,221.24,211.99,8.74"><head>Table 9 :</head><label>9</label><figDesc>Sampling and Adjudication : Topic 302</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,669.00,112.70,6.99"><p>http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,87.24,678.51,68.01,6.99"><p>http://terrier.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,87.24,678.90,230.67,6.99"><p>http://www.lemurproject.org/lemur/IndriQueryLanguage.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>We are honoured to acknowledge the kind contributions of <rs type="person">Dr. Mandar Mitra</rs>, <rs type="affiliation">Indian Statistical Institute, Kolkata, India</rs> for his valuable advice on IR methodologies and <rs type="person">Dr. Shreya Matilal</rs>, <rs type="person">Rajiv Gandhi School</rs> of <rs type="affiliation">Intellectual Property Law, Indian Institute of Technology, Kharagpur, India</rs> for his vision in the legal domain. We also heartily thank the <rs type="projectName">Topic Authorities -Mira Edelman (Topic 301</rs>) and <rs type="person">John Curran</rs> (<rs type="grantNumber">Topic 302</rs>) who were extremely cooperative with our team.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_7V92ddk">
					<idno type="grant-number">Topic 302</idno>
					<orgName type="project" subtype="full">Topic Authorities -Mira Edelman (Topic 301</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.50,623.53,259.86,8.74;9,383.32,623.53,79.85,8.74;9,488.78,623.53,51.22,8.74;9,87.50,635.48,187.95,8.74" xml:id="b0">
	<monogr>
		<ptr target="http://trec-legal.umiacs.umd.edu/itg10final.pdf" />
		<title level="m" coord="9,87.50,623.53,255.73,8.74">Trec2010 legal track interactive task guidelines</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.50,655.41,452.50,8.74;9,87.50,667.36,388.78,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,307.21,655.41,232.78,8.74;9,87.50,667.36,186.06,8.74">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cornelis Joost</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,282.87,667.36,97.04,8.74">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.50,75.16,452.50,8.74;10,87.50,87.11,285.83,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,328.39,75.16,161.68,8.74">Overview of the trec 2009 legal track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec18/t18proceedings.html" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.50,107.04,452.50,8.74;10,87.50,118.99,305.78,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,281.87,107.04,253.27,8.74">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,87.50,118.99,214.18,8.74">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
