<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,189.42,112.00,233.15,15.15;1,175.34,133.91,261.32,15.15">Learning Task Experiments in the TREC 2010 Legal Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-02-28">February 28, 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,264.32,167.81,83.35,8.74"><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
							<email>stomlins@opentext.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,189.42,112.00,233.15,15.15;1,175.34,133.91,261.32,15.15">Learning Task Experiments in the TREC 2010 Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-02-28">February 28, 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">3B01F9AC9752BCD0CFA3B4F5CC377223</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Learning Task of the TREC 2010 Legal Track investigated the effectiveness of e-Discovery search techniques at learning from examples to estimate the probability of relevance of every document in a collection. The task specified 8 test topics, each of which included a one-sentence request for documents to produce and several examples of relevant and non-relevant items from a new target collection of 685,592 e-mail messages and attachments. For our participation, we produced three retrieval sets to compare experimental feedback-based, topic-based and Booleanbased techniques. In this paper, we describe the experimental approaches and report the scores that each achieved on various set-based and rank-based measures. We report not just the mean scores of the experimental approaches but also the scores on each of the 8 individual test topics and the largest per-topic impacts of the techniques for several measures. Of the three experimental approaches compared, the experimental feedback-based approach had the highest score in the rank-based F 1 @R measure and set-based F 1 @K measure for a majority of the test topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>OpenText Search Server R , eDOCS Edition (formerly known as Open Text eDOCS SearchServer TM ) is a toolkit for developing enterprise search and retrieval applications. The eDOCS SearchServer kernel is also embedded in various components of the OpenText eDOCS Suite <ref type="foot" coords="1,352.89,484.91,3.97,6.12" target="#foot_0">1</ref> .</p><p>The eDOCS SearchServer kernel works in Unicode internally <ref type="bibr" coords="1,354.11,498.44,10.52,8.74" target="#b6">[7]</ref> and supports most of the world's major character sets and languages. The major conferences in text retrieval experimentation (TREC <ref type="bibr" coords="1,479.08,510.40,14.61,8.74" target="#b11">[12]</ref>, CLEF <ref type="bibr" coords="1,529.48,510.40,10.52,8.74" target="#b4">[5]</ref> and NTCIR <ref type="bibr" coords="1,128.07,522.35,10.79,8.74" target="#b8">[9]</ref>) have provided judged test collections for objective experimentation with the SearchServer kernel in more than a dozen languages.</p><p>This paper describes experimental work with the eDOCS SearchServer kernel (experimental post-6.0 builds) conducted in part by participating in the Learning Task of the TREC 2010 Legal Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Task</head><p>The Learning Task of the TREC 2010 Legal Track investigated the effectiveness of e-Discovery search techniques at learning from examples to estimate the probability of relevance of every document in a collection.</p><p>The Learning task was a successor task to the Ad Hoc, Relevance Feedback and Batch tasks of past Legal Tracks. We have participated in the 5 years of the Legal Track to date <ref type="bibr" coords="1,388.12,642.45,22.37,8.74">(2006)</ref><ref type="bibr" coords="1,410.48,642.45,4.47,8.74">(2007)</ref><ref type="bibr" coords="1,410.48,642.45,4.47,8.74">(2008)</ref><ref type="bibr" coords="1,410.48,642.45,4.47,8.74">(2009)</ref><ref type="bibr" coords="1,414.96,642.45,22.37,8.74">(2010)</ref>. (We also helped with coordinating the Legal Track in 3 of these years <ref type="bibr" coords="2,286.44,75.16,23.15,8.74">(2007)</ref><ref type="bibr" coords="2,309.59,75.16,4.63,8.74">(2008)</ref><ref type="bibr" coords="2,314.22,75.16,23.15,8.74">(2009)</ref> as described in <ref type="bibr" coords="2,409.69,75.16,14.61,8.74" target="#b18">[19]</ref>, <ref type="bibr" coords="2,431.61,75.16,15.50,8.74" target="#b9">[10]</ref> and <ref type="bibr" coords="2,470.33,75.16,9.96,8.74" target="#b5">[6]</ref>; however, we were not part of the coordination of this year's track.)</p><p>Compared to last year's Batch Task, the Learning Task used a new document collection (based on Enron e-mail instead of scanned documents from tobacco companies). The requirement to estimate the probability of relevance (of each e-mail or attachment) was also new this year.</p><p>The new document collection this year was called the "EDRM Enron Email Data Set v2" collection which consisted of 685,592 e-mail messages and attachments (approximately 4GB of text) from 159 mailbox directories. (By our count, there were 146 different employee mailboxes, with a few large mailboxes split into multiple directories.) We just used the "Deduplicated text-only" version of this collection available in a compressed file called edrmv2txt-v2.tar.bz2. (For binary attachments, this version contained the text extracted by a 3rd-party tool, which was of variable quality.) Uncompressed, the collection contained 685,592 .txt files, totaling 3,991,162,863 bytes. The document id was the part of the filename before the .txt suffix. Each attachment to a message was in a separate .txt file, numbered .1, .2, and so on. For example, container message "3.129461.NC5X5LNTR5XI1CBA3P4QVXG4YOWV5J0NB.txt" had 2 attachments called "3.129461.NC5X5LNTR5XI1CBA3P4QVXG4YOWV5J0NB.1.txt" and "3.129461.NC5X5LNTR5XI1CBA3P4QVXG4YOWV5J0NB.2.txt"; these were 3 of the 685,592 "documents" in the collection.</p><p>(The document set in 2010 was a substantial revision of the TREC 2009 Enron collection used in the Interactive task of the preceding year.)</p><p>To test the systems, there were 8 production requests, herein called "topics", numbered 200 to 207. 7 of these (numbers 201-207) were taken from the previous year's Interactive Task, which shared a (fictitious) 16-page background complaint regarding "securities fraud". The 8th topic (number 200) was a new one regarding real estate, with no background complaint, though it had 7 sentences of guidelines on what was responsive or not. Each topic included a one-sentence request for documents to produce for each topic. Furthermore, for each topic, several example relevant and non-relevant "seed" documents from the collection were provided (as described further below).</p><p>Please see the task guidelines <ref type="bibr" coords="2,220.11,385.99,15.50,8.74" target="#b20">[21]</ref> and track overview paper <ref type="bibr" coords="2,354.66,385.99,10.52,8.74" target="#b3">[4]</ref> for more details on the task and track. <ref type="bibr" coords="2,72.00,397.95,10.52,8.74" target="#b0">[1]</ref> and <ref type="bibr" coords="2,105.06,397.95,10.52,8.74" target="#b1">[2]</ref> have more background on e-Discovery in general. Also, background on our past participations in the track are in <ref type="bibr" coords="2,142.91,409.90,14.60,8.74" target="#b14">[15]</ref>, <ref type="bibr" coords="2,164.49,409.90,14.61,8.74" target="#b15">[16]</ref>, <ref type="bibr" coords="2,186.07,409.90,14.61,8.74" target="#b16">[17]</ref>, <ref type="bibr" coords="2,207.65,409.90,14.61,8.74" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>To index the collection, we processed the 685,592 .txt files as follows:</p><p>Firstly, for each container message (i.e. non-attachment messages, which were identified as those having just 2 dots in the document id instead of 3 dots), we discarded lines which appeared to be "noise" lines, which were those starting with "X-SDOC: ", "X-ZLID: zl-edrm-enron-v2-" or "EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc". (Note that, for these experiments, we did not bother to take advantage of any of the structure of the e-mail messages. In particular, the "Date:", "From:", "To:" and "Subject:" lines were just treated as plain text like any line of the body of the email.)</p><p>Then for each message (including attachments), we added a "&lt;record&gt;" tag before each message, followed by the document id inside "&lt;tid&gt;..&lt;/tid&gt;" tags, followed by the message text converted to an XML-safe form (e.g. special characters such as "&amp;" were converted to XML entities such as "&amp;amp;"), followed by a closing "&lt;/record&gt;" tag. The output of the re-formatting of the .txt files of each subdirectory was sent to one file, resulting in 228 .xml files (as some of the 159 mailbox directories had more than one subdirectory), but still comprising 685,592 records.</p><p>The reason for converting the collection to this XML format was that we could then index it with the same scripts we had used for the IIT CDIP collection of the previous 4 years. As in past years, for each record, we indexed from the "&lt;/tid&gt;" tag to the "&lt;/record&gt;" tag. Any tags themselves were indexed (we just didn't bother to discard them; a minor side effect is that this meant the term "record" matched every document). Entities (e.g. "&amp;amp;") were converted back to the character they represented (e.g. "&amp;").</p><p>We did not use a stopword list, though unlike the past few years, we did not bother to index punctuation characters this year. The index supported both searching on just the surface forms of the words and also searching on inflections from English lexical stemming. The documents were assumed to be in the Windows-1252 character set when converted to Unicode. Words were normalized to upper-case and any accents were dropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Examples</head><p>The training examples (also known as "training judgments", "training qrels" or "seed documents") for all 8 test topics were provided in a file called "seed.csv". For 7 of the topics (201-207), these were based on documents judged in the previous year's Interactive task, when the organizers were able to find the documents in the new collection (there apparently was no straightforward mapping from the document ids used in one collection to the other).</p><p>We reported some issues with the seed.csv file to the track mailing list (July 31, 2010), most notably that some messages were listed multiple times, with different judgments. We discarded lines with duplicate document ids as follows:</p><p>First, some lines were obviously "broken" in that they included an md5 sum in the first column (which was supposed to just be the case for attachments) but they did not include the attachment number (e.g. ".1", ".2", etc.) in the 4th column. So we dicarded these lines (there were 97 such lines).</p><p>Then, for any other case of a duplicated document id for a topic, we just kept the first judgment. This caused 25 more lines to be removed for topic 200, 10 more lines to be removed for topic 202, and 4 or fewer more lines to be removed for each of the other topics.</p><p>We also re-formatted the remaining lines to the traditional 4-column qrels format used by the trec eval utility, which is also readable by the l07 eval utility used by the track in the previous 3 years. Our output of all this processing was a new file called "qrelsL10.seed".</p><p>The following list shows, for each of the 8 topics, the count of the number of relevance judgments in qrelsL10.seed, the number judged relevant, and the number judged non-relevant. Note that the "count" may exceed the sum of the "rel" and the "non" because some documents were "gray" (had a label of -1 or -2, indicating that the assessor didn't render a judgment for some reason):</p><p>Topic 200: count=822, rel=205, non=617 Topic 201: count=724, rel=168, non=516 Topic 202: count=1430, rel=990, non=393 Topic 203: count=1024, rel=65, non=878 Topic 204: count=1219, rel=59, non=1122 Topic 205: count=1951, rel=330, non=1499 Topic 206: count=352, rel=18, non=324 Topic 207: count=583, rel=80, non=492</p><p>As can be seen from the above list, the number of example relevant documents (after our arbitrary processing to remove duplicate messages) ranged from 18 (for topic 206) to 990 (for topic 202).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feedback Run -otL10FT</head><p>The submitted experimental otL10FT run was a pure feedback run that did not make any use of the topic statements. Instead, the feedback technique was just based on the set of documents that were previously judged relevant (the "feedback set").</p><p>In the case of topic 202, which had a large number of example relevant documents (990 as mentioned earlier), we reduced the number in the feedback set to 305 by taking a random sample. This was just to guard against potential buffer overflow in some later steps.</p><p>Then documents of 10,000 bytes or more (in the XML formatting described earlier) were excluded from the feedback set in hopes of reducing the percentage of input text that was not relevant. (This step seemed to have been helpful last year for the otL09F run.) The resulting number of relevant documents, in order by topic, were 197, 115, 193, 36, 50, 232, 15 and 75. Also, we created an alternate feedback set that further excluded documents of 1000 bytes or more. Its resulting number of relevant documents, in order by topic, were 96, 24, 24, 6, 6, 12, 6 and 26.</p><p>The documents from each feedback set for each topic were used as the input to the SearchServer IS ABOUT predicate which created a vector query from the highest weighted terms (based on a tf.idf calculation after appending the input documents together). English inflections were enabled, and stems in more than 5% of the collection's documents were omitted.</p><p>To decide which result set to use for each topic (the one based on documents less than 1000 bytes in size, or the one based on documents less than 10,000 bytes in size), we scored each result set using the full qrelsL10.seed judgments with the l07 eval utility, and picked the result set for each topic with the higher score in the induced average precison measure (indAP), which is denoted ":mapJudged:" in l07 eval; (indAP is like the traditional average precisiion measure, except that unjudged documents are omitted). It turned out that the more restrictive feedback set scored higher for the first 5 topics (200-204) and the less restrictive set scored higher for the latter 3 topics (205-207).</p><p>To assign a probability of relevance to each result, our main input was the relevance() function score from SearchServer, which is a score between 0 and 1000 for each document (though most documents score between 0 and 500). These scores were found to have some value for predicting query difficulty in the TREC 2004 Robust Track <ref type="bibr" coords="4,135.06,266.44,14.61,8.74" target="#b19">[20]</ref>. Roughly speaking, the relevance() score is higher when there are rare terms (i.e. terms of high inverse document frequency) that match, which intuitively is a good reason to be more confident of the relevance of the document.</p><p>(The relevance ranking approach was the same for all runs, and also the same as in past years. The relevance function dampened the term frequency and adjusted for document length in a manner similar to Okapi <ref type="bibr" coords="4,100.41,326.22,15.50,8.74" target="#b10">[11]</ref> and dampened the inverse document frequency using an approximation of the logarithm. For runs which used inflectional matching (which was the case for all 3 submitted runs this year), these calculations were based on the stems of the terms.)</p><p>Our experimental probability formula for these experiments was to take the raw relevance() score (which, again, was usually between 0 and 500), multiply it by 0.002, square it, divide by 0.75, and enforce a max of 0.75 and min of 0.0001. Furthermore, for this feedback run, the seed relevant documents (from qrelsL10.seed) were moved to the front and assigned probability 0.75. And any documents unmatched by the feedback query were appended to the end with the min probability of 0.0001.</p><p>The resulting sum of the probabilities for each topic was 10990, 5752, 7478, 4404, 4466, 15743, 6346 and 8721, which corresponds to the predicted number of relevant documents for the topic. We compared this to the estimated number of relevant documents for the 7 Interactive topics of last year (the latter 7 topics, 201-207), which were 2454, 9514, 1831, 3242, 33614, 26343 and 26420, which was a factor in deciding on the 0.002 and squaring parts of the formula. While the fit was not all that close, the collection was said to be quite different this year, with duplicates removed, and new messages added, so we did not attempt to fit individual topics any more closely.</p><p>The max probability of 0.75 was chosen based on looking at the past assessor agreement on relevant documents. The min probability of 0.0001 seemed small enough to not affect the overall estimates much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Request Run -otL10rvlT</head><p>The submitted experimental otL10rvlT run did not make any use of the training examples; instead, it just used the one-sentence request. Common instruction words (e.g. "please", "produce", "documents") were manually removed. For example, for topic 202, for which the request text was "All documents or communications that describe, discuss, refer to, report on, or relate to the Company's engagement in transactions that the Company characterized as compliant with FAS 140 (or its predecessor FAS 125).", the WHERE clause of the corresponding SearchSQL statement was</p><formula xml:id="formula_0" coords="4,82.46,658.66,392.28,20.25">WHERE FT_TEXT CONTAINS 'engagement'|'transactions'|'compliant'|'FAS'|'140'| 'predecessor'|'FAS'|'125'</formula><p>Linguistic expansion from English inflectional stemming was applied, e.g. the search for 'engagement' also matched 'engagements'.</p><p>The experimental probability formula was the same as for the other runs, i.e. start with the raw relevance() score (which was usually between 0 and 500), multiply by 0.002, square it, divide by 0.75, and enforce a max of 0.75 and min of 0.0001. As mentioned earlier, the density of relevant documents on last year's collection was a factor in settling on this (experimental) general-purpose probability formula, so one could argue there was a modest feedback influence in this sense, though again, this run did not make any use of the provided training examples, and there was no tuning of the probabilities for individual topics.</p><p>Any documents unmatched by the request query were appended to the end with the min probability of 0.0001. This otL10rvlT run is meant to be considered as a baseline run representing what can be done by a mostly-automatic approach without using the training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Boolean Run -otL10bT</head><p>The submitted experimental otL10bT run was a Boolean-based run.</p><p>There was no organizer-provided reference Boolean query for the 8 topics (unlike for the Ad Hoc tasks of past Legal Tracks, which included a mock Boolean negotiation for each topic). So we attempted to quickly create our own Boolean query for each topic. The approach was to look at some of the example relevant documents matched by the feedback run (described earlier), with the feedback terms highlighted in those documents. We then manually picked keywords or phrases that seemed like they might be particularly useful for identifying relevant documents (which may or may not have included terms in the feedback query). Of course, this Boolean term selection was subjective, and we had a bias to making the Boolean query fairly short, and we did not want to spend a lot of time fine-tuning the query. One can perhaps think of this run as a hastily produced manual or interactive run.</p><p>The resulting Boolean queries were as follows: Note that linguistic expansion from English inflectional stemming was also applied, e.g. the search for "house" also matched "housing", "houses", "housed" and "house's". Also, the hyphenated search for "email" also matched non-hyphenated forms such as "email", "emails", "emailed", "emailing" and "email's" (along with hyphenated forms such as "e-mails", "e-mailing", "e-mailed", and so on).</p><p>Note also that the matches were still relevance-ranked. (For terms in phrases of Boolean queries, only occurrences of the term satisfying the phrase counted towards term frequency.) The same experimental probability formula was used as for the other runs, i.e. start with the raw relevance() score (which was usually between 0 and 500), multiply it by 0.002, square it, divide by 0.75, and enforce a max of 0.75 and min of 0.0001.</p><p>Any documents unmatched by the Boolean query were appended to the end with the min probability of 0.0001. (It was required to submit all of the documents for each topic.)</p><p>Note that the sum of the probabilities was typically different than the number of matches for the Boolean query. e.g. for topic 200, the Boolean query had 17,917 matches, but the sum of the probabilities from the (experimental) general-purpose formula was just 3430.</p><p>We submitted our 3 experimental runs (otL10FT, otL10rvlT and otL10bT) by the August 25, 2010 deadline. The task organizers then had a sample of the test collection judged for relevance as the basis for estimating the various scores, such as recall, precision and F 1 . The details presumably will be in the track overview paper <ref type="bibr" coords="6,99.60,132.84,9.96,8.74" target="#b3">[4]</ref>, but our understanding from the discussion on the track mailing list is that it proceeded as follows.</p><p>The test collection was divided into 4 strata, called stratum 100, stratum 1000, stratum 10000, and stratum 1000000. Our understanding is that any document that was ranked in the top-100 by any participant submission was in stratum 100 (and this stratum was completely judged). Any remaining document that was ranked in the top-1000 by any participant submission was in stratum 1000, but only 5-10% of this stratum was judged. And so on for the other 2 strata.</p><p>The task organizers produced a preliminary set of judgments (qrels.t10legallearn.prelim) on October 12, 2010, in time for the October 25 notebook paper deadline and November 16-19 conference. The final set of judgments (qrels.t10legallearn) were released January 19, 2011. In this paper, we just use the final set of judgments.</p><p>Based on the final judgments in qrels.t10legallearn, we produced our own counts of the number of documents in each stratum, the number judged in each stratum, and the ratio (which is the probability of each document in that stratum being chosen for judging), which are listed here: Topic 200: stratum 100: count=918, judged=918, prob=1.000000000000 stratum 1000: count=8707, judged=600, prob=0.068910072356 stratum 10000: count=74838, judged=600, prob=0.008017317406 stratum 1000000: count=601129, judged=602, prob=0.001001448940 Topic 201: stratum 100: count=1050, judged=1050, prob=1.000000000000 stratum 1000: count=7271, judged=556, prob=0.076468161188 stratum 10000: count=73807, judged=556, prob=0.007533160811 stratum 1000000: count=603464, judged=558, prob=0.000924661620 Topic 202: stratum 100: count=1146, judged=1146, prob=1.000000000000 stratum 1000: count=5439, judged=524, prob=0.096341239198 stratum 10000: count=64236, judged=524, prob=0.008157419516 stratum 1000000: count=614771, judged=526, prob=0.000855603143 Topic 203: stratum 100: count=1201, judged=1201, prob=1.000000000000 stratum 1000: count=8586, judged=506, prob=0.058933146983 stratum 10000: count=80978, judged=506, prob=0.006248610734 stratum 1000000: count=594827, judged=507, prob=0.000852348666 Topic 204: stratum 100: count=964, judged=964, prob=1.000000000000 stratum 1000: count=9995, judged=585, prob=0.058529264632 stratum 10000: count=84066, judged=585, prob=0.006958818072 stratum 1000000: count=590567, judged=586, prob=0.000992266754 Topic 205: stratum 100: count=1172, judged=1172, prob=1.000000000000 stratum 1000: count=6428, judged=516, prob=0.080273802116 stratum 10000: count=45913, judged=516, prob=0.011238647006 stratum 1000000: count=632079, judged=516, prob=0.000816353652 Topic 206: stratum 100: count=1077, judged=1077, prob=1.000000000000 stratum 1000: count=9367, judged=547, prob=0.058396498345 stratum 10000: count=77038, judged=547, prob=0.007100392014 stratum 1000000: count=598110, judged=549, prob=0.000917891358 Topic 207: stratum 100: count=976, judged=976, prob=1.000000000000 stratum 1000: count=6714, judged=581, prob=0.086535597259 stratum 10000: count=84580, judged=581, prob=0.006869236226 stratum 1000000: count=593322, judged=582, prob=0.000980917613</p><p>The counts for each topic should sum to 685,592 (the number of documents in the collection). The number of judged documents added to 2720 for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">L07 vs. L10 measures</head><p>The task organizers developed a new approach to estimating scores from the samples, based on individually estimating the precision on each stratum and then extrapolating over the entire stratum. We call this approach the "L10" approach, in contrast to the "L07" approach that was used the previous 3 years (for which we led the design when helping to coordinate the task). The L07 approach essentially assigned a fixed weight to each judged document based on the reciprocal of its probability of being judged. We reported on the track mailing list (Oct 18, 2010) that the new L10 approach had some anomalies, such as that if set D1 was a strict superset of set D2, it could still estimate recall(D1) to be less than recall(D2). Furthermore, the recall of a set could be estimated to be greater than 100%. (These particular anomalies could not happen with the L07 approach.) In this paper, we generally just report the L07-based scores (re-using scripts that we had set up in past years).</p><p>To compute the L07-based scores, we created a qrelsL10.probs file (for use with the l07 eval scoring utility) by taking the judged documents from qrels.t10legallearn and assigning them the probability as listed in the previous section.</p><p>To get an idea of how similar or different the L07 and L10 approaches are, Table <ref type="table" coords="7,430.92,384.97,4.98,8.74" target="#tab_0">1</ref> compares the estimated F 1 @K score from each approach for the experimental otL10FT run. For the "K" value, i.e. the retrieval depth at which to estimate F 1 , we use the "Cutoff Estimate" reported by the task organizers in otL10FT.sum, which was the depth at which the (L10) F 1 would be expected to be maximized if the run's probabilities of relevance were accurate. The L10 F 1 @K score was reported in otL10FT.sum as "F1". We see in Table <ref type="table" coords="7,535.02,432.79,4.98,8.74" target="#tab_0">1</ref> that the F 1 @K scores are almost the same for all 8 topics, suggesting that both estimation approaches are likely to lead to similar conclusions.</p><p>Note: The detailed L07 formulas for estimating the number of relevant and non-relevant documents for each topic, and also for estimating precision and recall, were reported in the 2007 Ad Hoc task section of <ref type="bibr" coords="7,72.00,492.57,14.61,8.74" target="#b18">[19]</ref>, and the detailed formulas for estimating F 1 were reported in the 2008 Ad Hoc task section of <ref type="bibr" coords="7,500.23,492.57,14.61,8.74" target="#b9">[10]</ref>. The l07 eval software used to compute the L07 evaluation measures is online at http://trec.nist.gov/data/ legal09.html .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">L07 measures</head><p>For each topic, we report a table of set-based scores and a table of rank-based scores. Of the various measures, probably the most informative set-based measure is F 1 @K and most informative rank-based measure is F 1 @R.</p><p>F 1 @R is easy to interpret because depth R (where R is the estimated number of relevant documents) is the special depth at which recall, precision and F 1 all have the same value. For F 1 @R, just the relative probabilities of relevance matter (i.e., the ranking) not the absolute probabilities.</p><p>F 1 @K is a more challenging measure because the system has to choose the depth K at which to be evaluated. This K value is implied by the absolute probabilities of relevance calculated by the system. Normally the best K value is approximately the same as R (which, of course, is not known to the system in advance). If F 1 @K is substantially less than F 1 @R, then the system likely substantially overestimated or underestimated the probabilities of relevance. The F 1 measure requires both high precision and high recall to achieve a high score. Overestimating the probabilities leads to too high a K value and typically lowers precision substantially, which in turn lowers F 1 . Underestimating the probabilities leads to too low a K value and typically lowers recall substantially, which in turn lowers F 1 .</p><p>In the tables of set-based scores, we include not just the 3 experimental submissions (otL10FT, otL10rvlT, otL10bT), but also 3 reference runs as follows:</p><p>• "seeds-rel" is the set of relevant seed documents.</p><p>• "seeds-non" is the set of non-relevant seed documents.</p><p>• "fullsetL10" is the set consisting of the entire document collection.</p><p>The tables of set-based measures have the following columns:</p><p>• "K": For the 3 submitted runs, K came from the organizer-provided "Cutoff Estimate" as described earlier, i.e. the cutoff at which the (L10) F 1 would be expected to be maximized if the run's probabilities of relevance were accurate. (Note that the K value is a property of the run and is computable before the relevance judgments are known.) For the 3 reference runs, K is the size of the set.</p><p>• "R@K": The estimated recall at depth K. Recall is the estimated number of relevant documents retrieved (at depth K) divided by the estimated total number of relevant documents (in the entire collection). (From ":est K-Recall:" in l07 eval output.)</p><p>• "P@K": The estimated precision at depth K. Precision is the estimated number of relevant documents retrieved (at depth K) divided by the sum of the estimated number of relevant and non-relevant documents (at depth K). (From ":est K-Prec:" in l07 eval output.)</p><p>• "F 1 @K": The estimated F 1 score at depth K. F 1 is 2*Precision*Recall/(Precision+Recall) or 0 if both Precision and Recall are 0. (Note that this F 1 formula is only applicable for individual topics; the mean F 1 across topics may differ from plugging the mean precision and recall into the formula.) (From ":est K-F1:" in l07 eval output.)</p><p>• "Num. Judged@K" is the actual number of judged documents in the top-K, followed in parentheses by the actual number of judged relevant (r), non-relevant (n) and gray (g) documents. Note that because not all documents were drawn for judging with the same probability, the estimated numbers of relevant and non-relevant documents in a result set are not in general exactly proportional to the drawn numbers. (From ":K-jg ret:", ":K-rel ret:", ":K-nonrel ret:" and ":K-gray ret:" in l07 eval output respectively.)</p><p>The table caption reports the estimated number of relevant documents for the topic. The tables of rank-based measures have the following columns: • "P@B" and "R@B": Estimated Precision and Recall at Depth B (where B is the number of documents matching our experimental Boolean query (used for otL10bT), which is listed in the table caption).</p><formula xml:id="formula_1" coords="9,151.08,76.95,248.18,9.65">Run K R@K P@K F 1 @K Num.</formula><p>(From ":est PB:" and ":est RB:" in l07 eval output respectively.)</p><p>• "F 1 @R": Estimated F 1 at Depth R (where R is the estimated number of relevant documents, which is listed in the table caption). (From ":est R-F1:" in l07 eval output.)</p><p>• "indAP": Induced Average Precision (the popular "average precision" after discarding unjudged documents; the sampling probabilities are not used for this measure, i.e. indAP is not infAP or statAP).</p><p>(From ":mapJudged:" in l07 eval output.)</p><p>• "GS10J": Generalized Success@10 on Judged Documents (1.08 1-r where r is the rank of the first relevant document, only counting judged documents, or zero if no relevant document is retrieved). GS10J is a robustness measure which exposes the downside of blind feedback techniques <ref type="bibr" coords="9,459.82,464.10,14.61,8.74" target="#b12">[13]</ref>. "Generalized Success@10" was originally introduced as "First Relevant Score" (FRS) in <ref type="bibr" coords="9,427.08,476.05,14.61,8.74" target="#b13">[14]</ref>. Intuitively, GS10J is a predictor of the percentage of topics for which a relevant document is returned in the first 10 rows. (From ":GS10J:" in l07 eval output.)</p><p>• "First 10 Ret": The judgments of the top-10 ranked documents of the run. 'R' indicates judged relevant. 'N' indicates judged non-relevant. (From ":relstring:" in l07 eval output.)</p><p>• "S1J": Success of the First Judged Document (in table of mean scores only).</p><p>The highest scores of each measure are in bold; however, see Table <ref type="table" coords="9,377.84,573.69,9.96,8.74" target="#tab_1">20</ref> for which mean differences may be statistically significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><formula xml:id="formula_2" coords="10,229.34,98.41,169.92,9.65">K R@K P@K F 1 @K Num.</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.00,397.64,303.36,8.30;5,72.00,409.60,198.75,8.30;5,72.00,421.55,428.89,8.30;5,72.00,433.51,407.97,8.30;5,72.00,445.46,455.04,8.30;5,98.15,457.42,439.35,8.30;5,72.00,469.37,428.89,8.30;5,72.00,481.33,298.13,8.30;5,72.00,493.28,230.14,8.30"><head></head><label></label><figDesc>200: FT_TEXT CONTAINS 'house'|'rental'|'apartment'|'condo' 201: FT_TEXT CONTAINS 'pre-pay'|'swap' 202: FT_TEXT CONTAINS 'FAS'|'transaction'|'swap'|'trust'|'Transferor'|'Transferee' 203: FT_TEXT CONTAINS 'forecast'|'earnings'|'profit'|'quarter'|'balance sheet' 204: FT_TEXT CONTAINS 'retention'|'compliance'|'preserve'|'discard'|'destroy'|'delete'| 'clean'|'eliminate'|'shred'|'schedule'|'period'|'documents'|'file'|'policy'|'e-mail' 205: FT_TEXT CONTAINS 'electricity'|'electric'|'loads'|'hydro'|'generator'|'power' 206: FT_TEXT CONTAINS 'analyst'|'credit'|'rating'|'grade' 207: FT_TEXT CONTAINS 'football'|'Eric Bass'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,72.00,76.95,468.00,157.72"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the Estimated F 1 @K Scores of the Experimental otL10FT Run using the l07 eval Approach of 2007-2009 ("L07") and New Approach of 2010 ("L10")</figDesc><table coords="8,237.20,76.95,137.60,123.90"><row><cell></cell><cell></cell><cell>L07</cell><cell>L10</cell></row><row><cell>Topic</cell><cell>K</cell><cell cols="2">F 1 @K F 1 @K</cell></row><row><cell>200</cell><cell cols="2">19097 0.080</cell><cell>0.082</cell></row><row><cell>201</cell><cell cols="2">9305 0.068</cell><cell>0.068</cell></row><row><cell>202</cell><cell cols="2">9578 0.316</cell><cell>0.332</cell></row><row><cell>203</cell><cell cols="2">7553 0.261</cell><cell>0.258</cell></row><row><cell>204</cell><cell cols="2">11780 0.099</cell><cell>0.095</cell></row><row><cell>205</cell><cell cols="2">26615 0.409</cell><cell>0.406</cell></row><row><cell>206</cell><cell cols="2">12628 0.059</cell><cell>0.062</cell></row><row><cell>207</cell><cell cols="2">15588 0.175</cell><cell>0.169</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,143.48,76.95,325.03,187.94"><head>Table 2 :</head><label>2</label><figDesc>Set-based Scores for Topic 200 (2543.5 Est. Relevant Documents)</figDesc><table coords="9,402.58,76.95,46.63,8.74"><row><cell>Judged@K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,169.14,278.30,273.72,8.74"><head>Table 3 :</head><label>3</label><figDesc>Rank-based Scores for Topic 200 (B=17917, R=2544)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,143.48,98.41,325.03,226.88"><head>Table 4 :</head><label>4</label><figDesc>Set-based Scores for Topic 201 (1885.9 Est. Relevant Documents)</figDesc><table coords="10,402.58,98.41,46.63,8.74"><row><cell>Judged@K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,151.08,338.70,309.84,161.95"><head>Table 5 :</head><label>5</label><figDesc>Rank-based Scores for Topic 201 (B=25561, R=1886)</figDesc><table coords="10,151.08,413.80,309.84,86.84"><row><cell>Run</cell><cell>K R@K</cell><cell cols="2">P@K F 1 @K</cell><cell>Num. Judged@K</cell></row><row><cell>otL10FT</cell><cell>9578 0.436</cell><cell cols="3">0.247 0.316 1033 (895r, 138n, 0g)</cell></row><row><cell>otL10bT</cell><cell>17225 0.456</cell><cell>0.171</cell><cell>0.249</cell><cell>1209 (865r, 344n, 0g)</cell></row><row><cell>seeds-rel</cell><cell cols="3">990 0.144 0.827 0.245</cell><cell>376 (345r, 31n, 0g)</cell></row><row><cell>otL10rvlT</cell><cell>8299 0.155</cell><cell>0.110</cell><cell>0.129</cell><cell>651 (441r, 210n, 0g)</cell></row><row><cell cols="3">fullsetL10 685592 1.000 0.009</cell><cell cols="2">0.018 2720 (996r, 1724n, 0g)</cell></row><row><cell>seeds-non</cell><cell>393 0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>9 (0r, 9n, 0g)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,143.48,514.32,325.03,126.36"><head>Table 6 :</head><label>6</label><figDesc>Set-based Scores for Topic 202 (6312.4 Est. Relevant Documents)</figDesc><table coords="10,154.04,589.42,303.93,51.26"><row><cell>Run</cell><cell>P@B</cell><cell cols="4">R@B F 1 @R indAP GS10J First 10 Ret</cell></row><row><cell>otL10FT</cell><cell cols="5">0.069 0.675 0.358 0.904 1.000 RRRRNRRRRR</cell></row><row><cell>otL10bT</cell><cell>0.057</cell><cell>0.627</cell><cell>0.291</cell><cell>0.748</cell><cell>0.429 NNNNNNNNNN</cell></row><row><cell cols="2">otL10rvlT 0.059</cell><cell>0.584</cell><cell>0.109</cell><cell>0.543</cell><cell>1.000 RNRRNNRRRR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,169.14,654.08,273.72,8.74"><head>Table 7 :</head><label>7</label><figDesc>Rank-based Scores for Topic 202 (B=66770, R=6313)</figDesc><table coords="11,151.08,98.41,309.84,86.84"><row><cell>Run</cell><cell>K R@K</cell><cell cols="2">P@K F 1 @K</cell><cell>Num. Judged@K</cell></row><row><cell>otL10FT</cell><cell>7553 0.432</cell><cell cols="2">0.187 0.261</cell><cell>835 (409r, 426n, 0g)</cell></row><row><cell>otL10bT</cell><cell>11005 0.352</cell><cell>0.127</cell><cell>0.187</cell><cell>860 (381r, 479n, 0g)</cell></row><row><cell>otL10rvlT</cell><cell>44131 0.554</cell><cell>0.042</cell><cell>0.078</cell><cell>1018 (394r, 624n, 0g)</cell></row><row><cell>seeds-rel</cell><cell cols="3">65 0.012 0.569 0.023</cell><cell>65 (37r, 28n, 0g)</cell></row><row><cell cols="3">fullsetL10 685592 1.000 0.005</cell><cell cols="2">0.009 2720 (481r, 2239n, 0g)</cell></row><row><cell>seeds-non</cell><cell>878 0.001</cell><cell>0.005</cell><cell>0.002</cell><cell>15 (4r, 11n, 0g)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,143.48,198.94,325.03,126.36"><head>Table 8 :</head><label>8</label><figDesc>Set-based Scores for Topic 203 (3124.6 Est. Relevant Documents)</figDesc><table coords="11,154.04,274.04,303.93,51.26"><row><cell>Run</cell><cell>P@B</cell><cell cols="4">R@B F 1 @R indAP GS10J First 10 Ret</cell></row><row><cell>otL10FT</cell><cell cols="4">0.071 0.829 0.303 0.568</cell><cell>0.857 NNRRRRNRRR</cell></row><row><cell>otL10bT</cell><cell>0.057</cell><cell>0.798</cell><cell>0.266</cell><cell>0.523</cell><cell>0.926 NRRNNRNNRN</cell></row><row><cell cols="2">otL10rvlT 0.044</cell><cell>0.541</cell><cell>0.111</cell><cell>0.312</cell><cell>0.270 NNNNNNNNNN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,151.08,338.70,309.84,161.95"><head>Table 9 :</head><label>9</label><figDesc>Rank-based Scores for Topic 203 (B=41609, R=3125)</figDesc><table coords="11,151.08,413.80,309.84,86.84"><row><cell>Run</cell><cell>K R@K</cell><cell cols="2">P@K F 1 @K</cell><cell>Num. Judged@K</cell></row><row><cell>otL10FT</cell><cell>11780 0.143</cell><cell cols="2">0.076 0.099</cell><cell>631 (292r, 339n, 0g)</cell></row><row><cell>otL10rvlT</cell><cell>17481 0.128</cell><cell>0.049</cell><cell>0.070</cell><cell>586 (252r, 334n, 0g)</cell></row><row><cell>otL10bT</cell><cell>21896 0.141</cell><cell>0.043</cell><cell>0.065</cell><cell>689 (306r, 383n, 0g)</cell></row><row><cell cols="3">fullsetL10 685592 1.000 0.009</cell><cell cols="2">0.018 2720 (475r, 2245n, 0g)</cell></row><row><cell>seeds-rel</cell><cell cols="3">59 0.006 0.678 0.013</cell><cell>59 (40r, 19n, 0g)</cell></row><row><cell>seeds-non</cell><cell>1122 0.000</cell><cell>0.005</cell><cell>0.000</cell><cell>9 (1r, 8n, 0g)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="11,140.99,514.32,330.02,126.36"><head>Table 10 :</head><label>10</label><figDesc>Set-based Scores for Topic 204 (6361.8 Est. Relevant Documents)</figDesc><table coords="11,154.04,589.42,303.93,51.26"><row><cell>Run</cell><cell>P@B</cell><cell cols="3">R@B F 1 @R indAP GS10J First 10 Ret</cell></row><row><cell>otL10FT</cell><cell>0.021</cell><cell cols="3">0.680 0.108 0.551</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell cols="4">otL10rvlT 0.027 0.817 0.074</cell><cell>0.407</cell><cell>1.000 RRRRRRNRRN</cell></row><row><cell>otL10bT</cell><cell>0.016</cell><cell>0.494</cell><cell cols="2">0.064 0.603 1.000 RRRRRRRRRR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="11,164.16,654.08,283.68,8.74"><head>Table 11 :</head><label>11</label><figDesc>Rank-based Scores for Topic 204 (B=203212, R=6362)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,660.96,452.76,6.99;1,72.00,670.43,468.00,6.99;1,72.00,679.89,468.00,6.99;1,72.00,689.36,340.59,6.99"><p>OpenText, Open Text eDOCS SearchServer and Open Text eDOCS Suite are trademarks or registered trademarks of Open Text Corporation in the United States of America, Canada, the European Union and/or other countries. This list of trademarks is not exhaustive. Other trademarks, registered trademarks, product names, company names, brands and service names mentioned herein are property of Open Text Corporation or other respective owners.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>K R@K P@K F 1 @K Num. <ref type="bibr" coords="12,402.58,98.41,34.97,8.74">Judged</ref>  We also include a comparision table (Table <ref type="table" coords="14,279.84,436.78,9.22,8.74">20</ref>) to highlight the differences between the submitted runs in various measures. Its columns are as follows:</p><p>• "Expt" specifies the experiment (the codes of the two runs being compared are listed, indicating first run minus second run).</p><p>• "∆" is the difference of the mean scores of the two runs being compared (the column heading says for which retrieval measure).</p><p>• "95% Conf" is an approximate 95% confidence interval for the mean difference (calculated from plus/minus twice the standard error of the mean difference; strictly speaking, for 8 topics, the multiplier should be greater than 2.0, but we did not update our scripts for this paper). If zero is not in the interval, the result is "statistically significant" (at the 5% level), i.e. the feature is unlikely to be of neutral impact (on average), though if the average difference is small (e.g. &lt;0.020) it may still be too minor to be considered "significant" in the magnitude sense.</p><p>• "vs." is the number of topics on which the first run scored higher, lower and tied (respectively) compared to the second run. These numbers should always add to the number of topics.</p><p>• "3 Extreme Diffs (Topic)" lists 3 of the individual topic differences, each followed by the topic number in brackets. The first difference is the largest one of any topic (based on the absolute value). The third difference is the largest difference in the other direction (so the first and third differences give the range of differences observed in this experiment). The middle difference is the largest of the remaining differences (based on the absolute value).</p><p>From the tables, we see that the experimental feedback-based approach (otL10FT) had the highest score in the rank-based F 1 @R measure and set-based F 1 @K measure for a majority of the test topics. Of course, the experiments described in this paper just scratch the surface of the research that should be possible with the standard set of test topics, documents and relevance judgments that have been created by the collaborative efforts of the task organizers and participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Assessor Consistency Study</head><p>The set-based tables in the previous section included "seeds-rel" and "seeds-non" entries which show the precision of the training example relevant and non-relevant documents respectively for each topic. On average, just 62% of the example relevant documents were judged relevant by this year's assessors (based on the estimated precision of "seeds-rel" for each topic, weighting each topic equally, as per Table <ref type="table" coords="15,493.11,207.63,8.30,8.74">18</ref>). 4% of the example non-relevant documents were judged relevant by this year's assessors (based on the estimated precision of "seeds-non" for each topic, weighting each topic equally, as per Table <ref type="table" coords="15,438.67,231.54,8.30,8.74">18</ref>). These consistency results are not much different than expectations, except perhaps for Topic 206, where just 2 of the 18 examples of relevant documents were considered relevant by this year's assessors (as per Table <ref type="table" coords="15,487.08,255.45,8.30,8.74">14</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The Learning Task of the TREC 2010 Legal Track investigated the effectiveness of e-Discovery search techniques at learning from examples to estimate the probability of relevance of every document in a collection. The task specified 8 test topics, each of which included a one-sentence request for documents to produce and several examples of relevant and non-relevant items from a new target collection of 685,592 e-mail messages and attachments. For our participation, we produced three retrieval sets to compare experimental feedbackbased, topic-based and Boolean-based techniques. In this paper, we described the experimental approaches and reported the scores that each achieved on various set-based and rank-based measures. We reported not just the mean scores of the experimental approaches but also the scores on each of the 8 individual test topics and the largest per-topic impacts of the techniques for several measures. Of the three experimental approaches compared, the experimental feedback-based approach had the highest score in the rank-based F 1 @R measure and set-based F 1 @K measure for a majority of the test topics. The test data gathered should enable us to further study ranking and probability estimation techniques.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="15,86.71,484.22,453.29,7.86;15,82.52,495.18,457.48,7.86;15,82.52,506.14,20.99,7.86" xml:id="b0">
	<monogr>
		<title level="m" coord="15,228.71,484.22,311.30,7.86;15,82.52,495.18,342.19,7.86">The Sedona Conference R Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery. The Sedona Conference Journal</title>
		<editor>
			<persName><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">VIII</biblScope>
			<biblScope unit="page" from="189" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,86.71,525.07,453.28,7.86;15,82.52,536.03,320.24,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<title level="m" coord="15,154.79,525.07,385.21,7.86;15,82.52,536.03,190.69,7.86">Toward A Federal Benchmarking Standard for Evaluating Information Retrieval Products Used in E-Discovery. The Sedona Conference Journal</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">VI</biblScope>
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,86.71,554.96,453.29,7.86;15,82.52,565.92,20.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,310.10,554.96,135.81,7.86">TREC-2006 Legal Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,452.97,554.96,87.03,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,86.71,584.85,453.29,7.86;15,82.52,595.80,202.48,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<title level="m" coord="15,403.10,584.85,136.90,7.86;15,82.52,595.80,21.14,7.86;15,172.92,595.80,107.87,7.86">Overview of the TREC 2010 Legal Track</title>
		<imprint/>
	</monogr>
	<note>Proceedings of TREC 2010</note>
</biblStruct>

<biblStruct coords="15,86.71,614.73,317.80,8.12" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><surname>Cross-Language</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/" />
		<title level="m" coord="15,153.52,614.73,107.63,7.86">Evaluation Forum web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,86.71,633.66,453.29,7.86;15,82.52,644.62,140.50,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,395.85,633.66,144.15,7.86;15,82.52,644.62,21.14,7.86">Overview of the TREC 2009 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,110.95,644.62,88.01,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,86.71,663.55,453.29,7.86;15,82.52,674.51,20.99,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,160.04,663.55,196.98,7.86">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,363.63,663.55,172.11,7.86">Sixteenth International Unicode Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,86.71,75.84,453.28,7.86;16,82.52,86.80,246.24,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="16,378.16,75.84,161.83,7.86;16,82.52,86.80,136.07,7.86">Building a Test Collection for Complex Document Information Processing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Agam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="665" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,86.71,105.73,421.60,9.85" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="16,124.30,105.73,154.83,7.86">NII-Test Collection for IR) Home Page</title>
		<author>
			<persName coords=""><surname>Ntcir</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/~ntcadm/index-en.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,124.66,448.68,7.86;16,82.52,135.61,140.50,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,397.24,124.66,142.76,7.86;16,82.52,135.61,21.14,7.86">Overview of the TREC 2008 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,110.95,135.61,88.01,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,154.54,448.67,7.86;16,82.52,165.50,60.66,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<title level="m" coord="16,402.18,154.54,137.82,7.86;16,82.52,165.50,31.38,7.86">Okapi at TREC-3. Proceedings of TREC-3</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,184.43,297.17,8.11" xml:id="b11">
	<monogr>
		<ptr target="http://trec.nist.gov/" />
		<title level="m" coord="16,91.32,184.43,190.87,7.86">Text REtrieval Conference (TREC) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,203.36,448.68,7.86;16,82.52,214.32,49.14,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="16,173.53,203.36,308.73,7.86">Early Precision Measures: Implications from the Downside of Blind Feedback</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="705" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,233.25,375.86,7.86;16,467.18,231.48,11.72,5.24;16,481.64,233.25,58.36,7.86;16,82.52,244.21,185.11,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<title level="m" coord="16,172.39,233.25,294.79,7.86;16,467.18,231.48,11.72,5.24;16,481.64,233.25,58.36,7.86;16,82.52,244.21,180.36,7.86">European Ad Hoc Retrieval Experiments with Hummingbird SearchServer TM at CLEF 2005. Working Notes for the CLEF 2005 Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,263.14,448.68,7.86;16,82.52,274.09,112.07,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,172.69,263.14,363.09,7.86">Experiments with the Negotiated Boolean Queries of the TREC 2006 Legal Discovery Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,82.52,274.09,88.01,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,293.02,448.68,7.86;16,82.52,303.98,112.07,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,172.69,293.02,363.09,7.86">Experiments with the Negotiated Boolean Queries of the TREC 2007 Legal Discovery Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,82.52,303.98,107.87,7.86">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,322.91,448.68,7.86;16,82.52,333.87,79.92,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="16,173.21,322.91,366.79,7.86;16,82.52,333.87,55.85,7.86">Experiments with the Negotiated Boolean Queries of the TREC 2008 Legal Track. Proceedings of TREC</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,352.80,448.68,7.86;16,82.52,363.76,79.92,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="16,173.21,352.80,324.70,7.86">Experiments with the Negotiated Boolean Queries of the TREC 2009 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Proceedings of TREC</note>
</biblStruct>

<biblStruct coords="16,91.32,382.69,448.68,7.86;16,82.52,393.65,140.50,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,401.42,382.69,138.58,7.86;16,82.52,393.65,21.14,7.86">Overview of the TREC 2007 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,110.95,393.65,107.87,7.86">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,412.58,370.25,7.86;16,461.57,410.81,11.72,5.24;16,477.35,412.58,62.65,7.86;16,82.52,423.53,112.07,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,210.10,412.58,251.47,7.86;16,461.57,410.81,11.72,5.24;16,477.35,412.58,58.45,7.86">Web and Terabyte Retrieval with Hummingbird SearchServer TM at TREC 2004</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Robust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,82.52,423.53,88.01,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,91.32,442.46,448.68,7.86;16,82.52,454.07,197.71,9.21" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="16,143.42,442.46,116.39,7.86">Legal Track -Learning Task</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<ptr target="http://plg.uwaterloo.ca/~gvcormac/legal10/" />
	</analytic>
	<monogr>
		<title level="m" coord="16,91.32,442.46,48.45,7.86;16,267.77,442.46,73.54,7.86">Task Coordinators</title>
		<imprint/>
	</monogr>
	<note>TREC 2010</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
