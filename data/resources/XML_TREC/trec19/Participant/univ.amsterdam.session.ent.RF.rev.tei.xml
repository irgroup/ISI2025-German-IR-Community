<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.55,83.76,328.62,15.48;1,177.21,107.63,255.30,12.90">The University of Amsterdam at TREC 2010 Session, Entity, and Relevance Feedback</title>
				<funder ref="#_sCnVkfH">
					<orgName type="full">PROMISE Network of Excellence</orgName>
				</funder>
				<funder ref="#_NYW3FqW #_a6QzGED">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_QqtYJkP">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_SS9ztdx #_5p9SY3y #_bMfph5q">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_NyqbCUp">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder>
					<orgName type="full">CIP ICT-PSP</orgName>
				</funder>
				<funder ref="#_yBWeFwn">
					<orgName type="full">Dutch and Flemish Governments</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.11,138.20,56.34,10.75"><forename type="first">Marc</forename><surname>Bron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.40,138.20,42.85,10.75"><forename type="first">Jiyin</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.19,138.20,80.70,10.75"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.83,138.20,58.77,10.75"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,154.67,152.15,90.19,10.75"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.80,152.15,82.71,10.75"><forename type="first">Manos</forename><surname>Tsagkias</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.45,152.15,97.60,10.75"><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.55,83.76,328.62,15.48;1,177.21,107.63,255.30,12.90">The University of Amsterdam at TREC 2010 Session, Entity, and Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F143CDB6B165D9CB108BF243F2C5363</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the participation of the University of Amsterdam's ILPS group in the session, entity, and relevance feedback track at TREC 2010. In the Session Track we explore the use of blind relevance feedback to bias a follow-up query towards or against the topics covered in documents returned to the user in response to the original query. In the Entity Track REF task we experiment with a window size parameter to limit the amount of context considered by the entity co-occurrence models and explore the use of Freebase for type filtering, entity normalization and homepage finding. In the ELC task we use an approach that uses the number of links shared between candidate and example entities to rank candidates. In the Relevance Feedback Track we experiment with a novel model that uses Wikipedia to expand the query language model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year the Information and Language Processing Systems (ILPS) group of the University of Amsterdam participated in the session, entity and relevance feedback tracks. In this paper, we describe our participation for each of these tracks, in three largely independent sections: Section 3 on our session track participation, Section 4 on our participation in the entity track, and Section 5 on our work in the relevance feedback track. We detail the runs we submitted, present the results of the submitted runs, and, where possible, provide an initial analysis of these results. Before doing so, we describe the shared retrieval approach in Section 2. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Framework</head><p>In this section we describe our general approach for each of the tracks in which we participated this year. We employ a language modeling approach to IR and rank documents by their log-likelihood of being relevant given a query. Without presenting details here, we only provide our final formula for ranking documents, and refer the reader to <ref type="bibr" coords="1,504.06,238.83,51.86,8.64;1,316.81,250.78,23.24,8.64" target="#b1">(Balog et al., 2008)</ref> for the steps of deriving this equation: log P(D|Q) ∝ log P(D) + ∑ t∈Q P(t|θ Q ) • log P(t|θ D ). (1)</p><p>Here, both documents and queries are represented as multinomial distributions over terms in the vocabulary, and are referred to as document model (θ D ) and query model (θ Q ), respectively. The third component of our ranking model is the document prior (P(D)), which is assumed to be uniform, unless stated otherwise. Note that by using uniform priors, Eq. 1 gives the same ranking as scoring documents by measuring the KL-divergence between the query model θ Q and each document model θ D , in which the divergence is negated for ranking purposes <ref type="bibr" coords="1,401.75,401.86,99.76,8.64" target="#b6">(Lafferty and Zhai, 2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modeling</head><p>Unless indicated otherwise, we smooth each document model using a Dirichlet prior:</p><formula xml:id="formula_0" coords="1,372.61,482.06,183.30,23.95">P(t|θ D ) = n(t, D) + µP(t) ∑ t n(t, D) + µ ,<label>(2)</label></formula><p>where n(t, D) indicates the count of term t in D and P(t) indicates the probability of observing t in a large background model such as the collection:</p><formula xml:id="formula_1" coords="1,369.58,560.80,186.34,22.18">P(t) = P(t|C) = ∑ D n(t, D) |C| .<label>(3)</label></formula><p>µ is a hyperparameter that controls the influence of the background corpus which we set to the average document length.</p><p>As to the query model θ Q , we adopt the common approach to linearly interpolate the initial query with an expanded part <ref type="bibr" coords="1,334.79,643.44,76.65,8.64" target="#b1">(Balog et al., 2008;</ref><ref type="bibr" coords="1,413.94,643.44,96.67,8.64" target="#b7">Zhai and Lafferty, 2001)</ref>:</p><formula xml:id="formula_2" coords="1,346.51,662.80,209.40,11.78">P(t|θ Q ) = λ Q P(t| θQ ) + (1 -λ Q )P(t|Q),<label>(4)</label></formula><p>where P(t|Q) indicates the MLE on the initial query, P(t| θQ ) indicates the MLE of the expanded part, and the parameter λ Q controls the amount of interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Significance testing</head><p>Throughout the paper we use the Wilcoxon signed-rank test to test for significant differences between runs. We report on significant increases (or drops) for p &lt; .01 using (and ) and for p &lt; .05 using (and ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Clueweb</head><p>All the tracks we participated in this year make use of the Clueweb document collection. We do not use any form of stemming and remove a conservative list of 588 stopwords. We index the headings, titles, and contents as searchable fields and do not remove any HTML tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Session Track</head><p>The goal of the TREC Session track is to find out how retrieval systems perform over the course of a session and whether taking a previous query into account can help improve retrieval performance for a follow-up query. The current setting considers sessions consisting of an original query and one follow-up query that constitutes either a generalization, specialization, or a parallel move.</p><p>Our submission for the TREC Session track explores the use of blind relevance feedback to bias a follow-up query towards or against the topics covered in documents that were returned to the user in response to the original query. Blind relevance feedback takes the most discriminative terms from a set of documents retrieved for a query, and uses these to build a query model that incorporates information about the topic underlying the documents. We apply this method to an initial, diverse result list. Below we explain our approach in detail, and give an overview of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach</head><p>Currently, little is known about users' expectations about retrieval systems' behavior throughout a session. Therefore, we based our submission on the following intuitions. Without contextual information about the users' preferred interpretation of a query, a retrieval system can return a standard retrieval run (cf. Retrieval approach, below). If the query is ambiguous, it may be better to provide a diverse result list to increase the likelihood of providing at least some relevant documents for different possible interpretations of the query (cf. Diversification). When additional information from a previous query can be taken into account, search results can be more focused. In our submission we use pseudo relevance feedback to combine information about the topics covered by the two queries (cf. Pseudo Relevance Feedback).</p><p>Retrieval approach Our retrieval system uses the framework explained above (cf. §2). We use a Dirichlet prior with µ = 1600. Queries are constructed to emphasize phrases, as these are often found in web queries. Phrases and individual terms are combined with equal weights. An example query is shown below. &lt;query&gt; &lt;number&gt;1&lt;/number&gt; &lt;text&gt;#weight( 0.5 #1(legal advice) 0.5 #combine(legal advice) )&lt;/text&gt; &lt;/query&gt; Retrieval runs are post-processed to filter out category and redirection pages from wikipedia, and to place the top result from wikipedia at the top of the result list.</p><p>Diversification We diversify runs following a topic modelbased approach. It models documents as a mixture of topics and constructs a final result list by re-ranking an initial list so that as many topics as possible are represented in the top ranked documents.</p><p>Our approach is inspired by previous work on diversifying a ranked list with Maximal Marginal Relevance (MMR) by <ref type="bibr" coords="2,331.08,334.00,131.91,8.64" target="#b4">Carbonell and Goldstein (1998)</ref> and based on a topic modeling approach, i.e., LDA <ref type="bibr" coords="2,437.09,345.95,67.32,8.64" target="#b2">(Blei et al., 2003)</ref>. It treats the re-ranking problem as a procedure of selecting a sequence of documents, where a document is selected depending on both its relevance with respect to the query and the documents that have already been selected before it, so as to have a set of documents that (i) are most relevant to the query and (ii) represent most if not all topical aspects.</p><p>We proceed as follows. First, we use LDA to extract 10 topics from the top 500 documents of the baseline retrieval run described above, so that each document is represented as a mixture of these 10 topics. We then start the re-ranking procedure by selecting the top relevant document in the initial list as the first document in the new ranked list. Then, we select a next document that can maximize the expected joint probability of presence of all topics in the selected result set. Since the sum of topic proportions within a document equals 1, the maximum joint probability (i.e., product of the probabilities of presence of each topic) occurs when the topics have equal proportion in the selected set. On the other hand, we use the retrieval score from the initial run as a prior probability that a document is selected as the next one, so as to take into account the relevance relation between the document and the original query.</p><p>Formally, given a query Q, a set of candidate documents Ca = {D j } n j=1 and a set of latent topics T = {t i } m i=1 , a document is selected from Ca for inclusion in the ranked list S such that arg max</p><formula xml:id="formula_3" coords="2,374.24,672.84,181.68,27.16">D∈Ca P(Q|D) m ∏ i=1 P(t i ∈ S ∪ {D}),<label>(5)</label></formula><p>where P(Q|D) is the query likelihood between the query Q and document D calculated as in a standard language modeling framework. The term P(t i ∈ S ∪ {D}) denotes the probability of a topic being present in the set S = S ∪ {D}, which is estimated by</p><formula xml:id="formula_4" coords="3,103.99,114.55,188.91,19.99">P(t i ∈ S ) = ∑ D j ∈S P(t i ∈ D j )P(D j ).<label>(6)</label></formula><p>Pseudo relevance feedback RL3 runs are generated using blind relevance feedback as follows. First, retrieval runs for the original and the follow-up query individually are generated, using the baseline method and diversification as described above. From the top-10 documents of these runs, the 10 most discriminative relevance feedback terms are extracted to form the sets of expansion terms E O (expansion terms extracted from results for the original query) and E F (expansion terms extracted from results for the follow-up query). These are combined to form the query expansion E as follows:</p><p>RF1 E = E O -only use feedback terms extracted from the top-ranked results of the original query.</p><p>RF2 E = E F \ E F -take feedback terms generated from results for the follow-up query and remove terms that were also extracted from results for the original query.</p><p>RF3 E = E O ∪ E F -combine relevance feedback terms of both queries.</p><p>These three approaches implement the following intuitions. First, we assume that results returned for the original query were helpful and can be used to focus or disambiguate results for the follow-up query. Second, we cover the assumption that results for the original query were not helpful. Finally, we consider the possibility that the underlying topic may best be represented by both queries.</p><p>As a final step in generating results when taking an original query into account, we remove documents that have been displayed in the top-10 of the response to the original query from the result list shown for the follow-up query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Runs</head><p>All submitted runs were automatic category A runs. uvaExt*.RL1 standard retrieval run using the original query + diversification using LDA uvaExt1.RL2 standard retrieval run using the follow-up query uvaExt1.RL3 combines uvaExt1.RL1 and uvaExt1.RL2 using RF1.</p><p>uvaExt2.RL2 standard retrieval run using the follow-up query + blind relevance feedback using the follow-up query uvaExt2.RL3 combines uvaExt1.RL1 and uvaExt1.RL2 using RF2.</p><p>uvaExt3.RL2 standard retrieval run using the follow-up query + diversification using LDA uvaExt3.RL3 combines uvaExt1.RL1 and uvaExt1.RL2 using RF3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Results for our submissions are listed in Table <ref type="table" coords="3,508.57,181.88,3.74,8.64" target="#tab_0">1</ref>. Listed is nsDCG@10.RL 12,13 with and without taking duplicate documents into account. nsDCG@10.RL 12 measures session performance when the original query was not taken into account. nsDCG@10.RL 13 measures session performance when the original query was taken into consideration. We find that the overall best performance is achieved by run uvaExt1 when duplicates are removed and the original query was not taken into account. This run retrieves document lists for each query. In all other cases, the followup run was diversified and in these cases, performance improves when taking the original query into account. In one case (uvaExt2, no duplicates), this improvement is statistically significant.</p><p>Performance when measured after removing duplicate documents improves in all cases. This is expected, as we remove duplicate documents that were previously displayed to the user at high ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Entity Track</head><p>The Entity Track consists of two tasks this year. The main task is the Related Entity Finding (REF) task introduced last year, where the goal is to find homepages of entities given a source entity, relation and target type. New this year is the second task: Entity List Completion (ELC). In the ELC task the goal is to find entities in structured data given a source entity, relation, target type and example entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Related Entity Finding Approach</head><p>In the REF task, we continue our experiments with cooccurrence models. This year we use a generative model to rank candidate entities that combines the co-occurrence between the source entity and candidate entities with evidence for relevance to the relation from the snippets in which they co-occur <ref type="bibr" coords="4,92.48,69.23,75.56,8.64" target="#b3">(Bron et al., 2010)</ref>. To the ranking provided by this model we apply type filtering based on Freebase and homepage finding using candidate entity names as queries to a web search engine. We briefly recall the derivation of our co-occurrence model below, followed by a description of each of the components.</p><p>We formulate the entity ranking problem as follows: rank candidate entities (e) according to P(e|E, T, R), where E is the source entity, T is the target type, and R is the relation described in the narrative.</p><p>Instead of estimating this probability directly, we use Bayes' rule and reformulate it into:</p><formula xml:id="formula_5" coords="4,97.36,222.87,191.67,22.31">P(e|E, T, R) = P(E, T, R|e) • P(e) P(E, T, R) . (<label>7</label></formula><formula xml:id="formula_6" coords="4,289.03,229.93,3.87,8.64">)</formula><p>Next, we drop the denominator as it does not influence the ranking of entities, and derive our final ranking formula as follows:</p><p>P(E, T, R|e) • P(e) = P(E, R|e) </p><formula xml:id="formula_7" coords="4,162.14,320.19,126.89,8.96">• P(T |e) • P(e)<label>(8</label></formula><p>In <ref type="bibr" coords="4,65.02,406.99,11.62,8.64" target="#b0">(8)</ref> we assume that the type is independent of the source entity E and the relation R. Next, we rewrite P(E, R|e) to P(R|E, e) so that it expresses the probability that relation R is generated by the two (co-occurring) entities (e and E).</p><p>Finally, we rewrite P(E, e) to P(e|E)•P(E) in (9) as the latter is a more convenient form for estimation, and we drop P(E) in (10) as it does not influence the ranking (for a fixed source entity E). Given equation ( <ref type="formula" coords="4,161.36,490.68,8.30,8.64" target="#formula_8">10</ref>) we are left with the following components:</p><p>• P(e|E): pure co-occurrence model,</p><p>• P(R|E, e): context dependent model, and</p><p>• P(T |e): type filtering.</p><p>Pure co-occurrence model We use χ 2 to express the strength of associations between the source entity and candidates, without considering the nature of their relation:</p><formula xml:id="formula_9" coords="4,73.72,635.68,223.09,23.93">cooc χ 2 (e, E) = N • ( c(e, E) • c(e, E) -c(e, E) • c(e, E) ) 2 c(e) • c(E) • ( N -c(e) ) • ( N -c(E) ) ,</formula><p>where N is the total number of documents, and e, E indicate that e, E do not occur, respectively (i.e., c(e, E) is the number of documents in which neither e or E occurs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-dependent model</head><p>We take the context surrounding a source entity and candidate entity into account by constructing a co-occurrence language model (θ Ee ) from the contexts in which a source entity and candidate co-occur. By assuming independence between the terms in the relation R we arrive at the following estimate:</p><formula xml:id="formula_10" coords="4,354.51,148.21,201.41,20.53">P(R|E, e) = P(R|θ Ee ) = ∏ t∈R P(t|θ Ee ) n(t,R) ,<label>(11)</label></formula><p>where n(t, R) is the number of times t occurs in R.</p><p>To estimate the co-occurrence language model θ Ee , we collect the snippets of a certain window size (w) in which the two entities co-occur and obtain term probabilities as follows:</p><formula xml:id="formula_11" coords="4,384.32,248.36,167.45,24.28">P(t|θ Ee ) = n(t, E, e) + µ • P(t) ∑ t n(t , E, e) + µ , (<label>12</label></formula><formula xml:id="formula_12" coords="4,551.77,255.42,4.15,8.64">)</formula><p>where µ is set to the average length of all the snippets in which E and e co-occur. Finally we define n(t, E, e) as:</p><formula xml:id="formula_13" coords="4,388.43,318.96,167.49,18.64">n(t, E, e) = ∑ d n(t, E, e, d, w),<label>(13)</label></formula><p>where n(t, E, e, d, w) is the number of times term t cooccurs with both entities E and e in document d within a distance of at most w term positions from either E or e. The window size w parameter will be determined empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type filtering</head><p>In last year's Entity Track DBpedia was used heavily to obtain type information for filtering. One issue when using type information from a closed vocabulary is handling entities outside the vocabulary. We experimented with Freebase<ref type="foot" coords="4,449.69,459.69,3.69,6.39" target="#foot_0">1</ref> as alternative knowledge source. Freebase is a collection of entities gathered from multiple structured data sources, i.e., DBpedia and WordNet, and therefore it covers more entities than DBpedia. Freebase contains information about properties of entities and relations between entities encoded in RDF (Antoniou and <ref type="bibr" coords="4,418.24,533.35,83.23,8.64" target="#b0">Van Harmelen, 2004)</ref>. RDF data is structured as subject, predicate, and object triples. A relation between two entities is represented by having the entities as subject and object and the predicate specifies the type of relation. A property is represented by having a text string (literal) as object instead of an entity. One property of Freebase is that it maintains a link to the original data source an entity originates from, i.e., the DBpedia URI of an entity.</p><p>To perform filtering we first created a manual mapping of the Freebase category labels to the target types T f b .</p><p>Then for each candidate entity with an entry in Freebase we obtain the category labels associated with the candidate (e f b ). Given this mapping we estimate P(T |e) as follows:</p><formula xml:id="formula_14" coords="5,115.95,77.32,128.55,20.91">P(T |e) = 1 if e f b ∩ T f b = / 0 0 otherwise,</formula><p>Hompage finding Another property associated with some of the entities in Freebase is the URL of the entity homepage. To obtain additional URLs we submit entity strings to a web search engine and collect the top 10 URLs. If we find a match for the Freebase URL in ClueWeb then we use it as the entity homepage, otherwise we use the highest ranked URL returned by the search engine that is found in ClueWeb. If no homepage is found we remove the entity from the candidates.</p><p>Entity normalization We perform a preliminary experiment with entity normalization and again turn to Freebase to provide a mapping of variants to a canonical entity. We consider all strings that are linked to the same Freebase ID with a name predicate as variants of the same entity. Normalization requires the following changes to the pure co-occurrence component:</p><formula xml:id="formula_15" coords="5,73.72,314.76,199.17,48.05">cooc χ 2 (V e ,V E ) = N • ( c(V e ,V E ) • c(e, E) -c(V e , E) • c(e,V E ) ) 2 c(V e ) • c(V E ) • ( N -c(V e ) ) • ( N -c(V E ) ) ,</formula><p>where V E is the set of variants association with E and c(V E ,V e ) is defined as:</p><formula xml:id="formula_16" coords="5,166.84,386.18,93.47,11.40">∑ v E ∈V E ∑ v e ∈V e c(v e , v E ).</formula><p>The context dependent model becomes: R) .</p><formula xml:id="formula_17" coords="5,88.70,421.68,179.86,20.53">P(R|V E ,V e ) = P(R|θ V E V e ) = ∏ t∈R P(t|θ V E V e ) n(t,</formula><p>We follow the estimation in Equation 12 but define n(t,V E ,V e ) as: <ref type="bibr" coords="5,216.31,487.88,18.22,8.59">d, w)</ref>.</p><formula xml:id="formula_18" coords="5,127.68,487.74,87.54,19.37">∑ v E ∈V E ∑ v e ∈V e ∑ d (t, v E , v e ,</formula><p>For the typefiltering component we only consider the type of the canonical form of the variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Pre-processing</head><p>The main challenge this year was to construct a related entity finding system that runs on the complete ClueWeb Cat A collection. For Named Entity Recognition (NER) we used the Stanford tagger <ref type="bibr" coords="5,136.11,612.13,82.22,8.64" target="#b5">(Finkel et al., 2005)</ref> which resulted in almost 2 billion unique entities. Removing entities with frequency lower than 5, replacing diacritics and removing entities longer than 128 characters, left us with 148 million entities. <ref type="foot" coords="5,75.67,658.02,3.69,6.39" target="#foot_1">2</ref> We then replaced entities by a unique identifier and indexed the documents using the Indri toolkit<ref type="foot" coords="5,240.20,669.97,3.69,6.39" target="#foot_2">3</ref> resulting in 10 indexes one for each part of ClueWeb Cat A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">REF Experiments</head><p>In our experiments we focus on the window size parameter and our entity normalization approach. We submitted the following four official runs:</p><p>ilpsA500 An automatic run with a window size of 500.</p><p>ilpsM50 A manual run with a window size of 50.</p><p>ilpsM50var A run with entity normalization.</p><p>ilpsM50agfil A run with strict type filtering.</p><p>In addition we generated the following un-official run:</p><p>ilpsA50 An automatic run with a window size of 50.</p><p>Note that in manual runs, apart from manually removing stop words and adding the base forms of verbs and singular forms of plural terms to the narrative, there was no manual interaction with the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">REF Results</head><p>We first look at the affect of the window size parameter. The first two rows in Table <ref type="table" coords="5,413.07,340.21,4.98,8.64" target="#tab_1">2</ref> show the results for a run with a window size of 500 (ilpsA500) and a run with a window size of 50 (ilpsA50). We observe that using a smaller widow size leads to a small gain in performance. This is in accord with the intuition that entities that occur close to the source entity are more strongly associated with it and that restricting context is effective in removing irrelevant entities. We plan further experiments in which we determine the optimal value of the window size parameter. The intuition to perform entity normalisation is that by collapsing variants we obtain more reliable co-occurrence statistics and more complete co-occurrence models. Row 4 and 5 in Table <ref type="table" coords="5,374.74,483.68,4.98,8.64" target="#tab_1">2</ref> show the results for a run with (ilpsM50var) and a run without (ilpsM50) entity normalisation. The drop in performance we observe when using normalisation is caused by abbreviations of entities identical to common terms, e.g., us and US (United States), which distort the cooccurrence statistics and context models.</p><p>Our best run (ilpsM50agfil) uses a more aggressive form of type filtering, i.e., entities that belong to the target type but also to other types are ranked lower proportional to the number of non-target types they belong to.</p><p>Finally, we note that although our scores approach the average of the per topic median nDCG R (0.08301), both are low compared to the average of the per topic maximum. In part this is due to the change to the Category A corpus. Our co-occurrence models take all contexts in which a source and candidate entity co-occur into account. While this is a competitive approach when applied to a high quality corpus such as Wikipedia and to a certain degree Category B <ref type="bibr" coords="5,508.94,686.91,46.97,8.64;5,316.81,698.87,21.44,8.64" target="#b3">(Bron et al., 2010)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity List Completion Approach</head><p>In our participation of the ELC task we investigate two intuitions. First, candidate entities that are more similar to the example entities in terms of the number of links they share should be ranked higher than entities that are less similar. Second, entities that link to objects that share words with the narrative, source entity and target type should be ranked higher than entities that do not.</p><p>The ELC task uses the Billion Triple Corpus (BTC) a set of structured data consisting of RDF triples. Both entities and relations are represented by URIs. The source entity and candidate entities given in the topic are represented by URIs as well, while the relation is given as a text string. In order to collect candidate entities we first obtain all objects that have one of the example entities as subject to form the set of example objects O x . Candidates are all entities that have an object in common with the example entities: based its links we take the predicates and objects from the set of triples which have the candidate as subject and store them as predicate-object tuples:</p><formula xml:id="formula_19" coords="6,107.69,511.20,151.25,8.96">T (c) = {(p, o) : triple(s, p, o), s = c}.</formula><p>We do this analogously for each example entity and calculate the candidate ranking score as the average Jaccard coefficient between the candidate entity and the example entities: set representation. A candidate word set is the set of unique terms obtained by parsing an entity's predicateobject tuples (W (c)). The set of unique terms from the source entity, narrative and target type (W (E, R, T )) form the topic representation. We calculate the word overlap as follows:</p><formula xml:id="formula_20" coords="6,111.78,594.13,139.11,25.09">avgJ(c, X) = 1 |X| ∑ x∈X |T (c) ∩ T (x)| |T (c) ∪ T (x)</formula><formula xml:id="formula_21" coords="6,353.22,224.03,186.22,22.54">wo(W (c),W (E, R, T )) = |W (c) ∩W (E, R, T )| |W (E, R, T )| • 2 .</formula><p>We combine the word overlap score with the average Jaccard coefficient to obtain our ranking score:</p><formula xml:id="formula_22" coords="6,353.10,296.60,186.46,8.74">score(c) = avgJ(c, Ex) • wo(W (c),W (E, R, T ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Table <ref type="table" coords="6,340.87,344.11,4.98,8.64" target="#tab_2">3</ref> shows the results of our two ELC runs. There is only a small difference in performance between the runs. This suggests that the links of the example entities already cover the relation expressed by the topic and that word overlap of the candidate entities with the topic relation does not add additional discriminative information on top of that. We note that our approach finds the most relevant entities, which is about half of the total number of relevant entities. One of the limitations is that we only consider entities as candidates when they link to the same objects as the example entities.</p><p>In future work we plan to loosen this constraint by also considering entities that are relevant to the terms in the relation without any links to the example entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Relevance Feedback Track</head><p>Typical relevance feedback algorithms consider feedback documents as generative models from which to sample terms. We find that simply applying out-of-the-box relevance feedback algorithms to the single example document is not effective; such feedback algorithms degrade retrieval performance. To address this issue, we have implemented a novel model and our focus in our TREC participation this year is to evaluate its performance.</p><p>No results have been provided to the participants at the time of writing. Because of this, we limit our discussion to describing our algorithm.</p><p>Our algorithm makes use of the moderated contents of Wikipedia as a pivot language. Wikipedia articles can be created by anyone, but they are typically moderated by a relatively small group of volunteers. Moreover, Wikipedia has extensive guidelines in place, pertaining to the correct use of grammar and style. As a consequence (and unlike common web pages), the language used in each article tends to be "clean" and to the point. It is this particular feature of Wikipedia that we use to influence the estimation of the language model of web pages. The expanded query language model is interpolated with the initial query to obtain a final representation of the user's information need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have described the participation of the University of Amsterdam's ILPS group in the session, entity, and relevance feedback track at TREC 2010. We arrived at the following preliminary conclusions. For the Session Track we find that using blind relevance feedback to extend a follow-up query with terms extracted from original results gives mixed results. Future work will be needed to select high-quality expansion terms more consistently, and to address differences between different (types of) queries.</p><p>For the Entity Track REF task we find that using a smaller window size for the contexts considered by our cooccurrence model leads to a small increase in performance. The inclusion of all context in which a candidate and source entity co-occur in our model leads to worse results when using a larger (more noisy) corpus. We also explore the use of Freebase for type filtering, homepage finding and entity normalization. Of which only the latter proved unsuccessful due to abbreviations identical to common terms distorting the cooccurrence counts and context models. For the ELC task we find that ranking candidate entities based on the links shared with the example entities is effective, especially in terms of recall.</p><p>No results are available for the Relevance Feedback Track at the time of writing and consequently we can provide no analyses or conclusions about our participation in the track.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,316.81,164.01,239.10,8.64;2,316.81,175.97,32.37,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example query, combining individual terms and phrases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,289.03,320.51,3.87,8.64;4,113.03,335.14,82.17,8.74;4,113.03,350.08,115.51,8.74;4,113.03,365.03,140.57,8.74;4,281.29,365.35,11.62,8.64;4,113.03,380.52,12.39,6.05;4,115.36,383.89,116.73,8.74"><head></head><label></label><figDesc>) = P(E, R, e) • P(T |e) = P(R|E, e) • P(E, e) • P(T |e) = P(R|E, e) • P(e|E) • P(E) • P(T |e) (9) rank = P(R|E, e) • P(e|E) • P(T |e)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,109.46,398.20,127.28,9.72;6,53.80,420.07,239.10,8.82;6,53.80,432.02,98.41,8.82;6,53.80,453.54,239.10,9.03"><head>C</head><label></label><figDesc>= {s : triple(s, p, o), o ∈ O x }, where triple(s, p, o) is a triple in the BTC with subject s, predicate p and object o. ilpsSetOL: baseline run To rank a candidate entity (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,336.51,271.60,196.61,77.19"><head>Table 1 :</head><label>1</label><figDesc>Results. nsDCG@10 for RL 12 and RL 13 .</figDesc><table coords="3,338.00,287.16,188.36,61.64"><row><cell></cell><cell>with duplicates</cell><cell>w/o duplicates</cell></row><row><cell>runID</cell><cell cols="2">n..RL 12 n..RL 13 n..RL 12 n..RL 13</cell></row><row><cell cols="3">uvaExt1 0.1356 0.1320 0.1416 0.1398</cell></row><row><cell cols="3">uvaExt2 0.1260 0.1297 0.1317 0.1373</cell></row><row><cell cols="3">uvaExt3 0.1262 0.1279 0.1311 0.1356</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,316.81,698.87,239.10,20.59"><head>Table 2 :</head><label>2</label><figDesc>, it suffers from low quality contexts introduced by the increase in corpus size. Results of the related entity finding runs.</figDesc><table coords="6,59.78,59.27,139.16,73.42"><row><cell>run</cell><cell>pri ret nDCG R</cell></row><row><cell>ilpsA500</cell><cell>88/779 0.0460</cell></row><row><cell>ilpsA50</cell><cell>94/779 0.0684</cell></row><row><cell>ilpsM50</cell><cell>94/779 0.0692</cell></row><row><cell>ilpsM50var</cell><cell>77/779 0.0571</cell></row><row><cell cols="2">ilpsM50agfil 99/779 0.0718</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,53.80,59.27,482.23,660.20"><head>Table 3 :</head><label>3</label><figDesc>| , where x is an example entity from the set of examples X and |X| is the size of the example set. Given this set we used two ranking approaches given below. Results of the entity list completion runs.</figDesc><table coords="6,53.80,674.57,239.10,44.89"><row><cell>ilpsSetOLnar: baseline combined with word overlap</cell></row><row><cell>In our second run we combine the predicate-object</cell></row><row><cell>tuple set overlap with the word set overlap between</cell></row><row><cell>an entities word set representation and a topics word</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,331.16,712.12,77.48,6.91"><p>http://wiki.freebase.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,68.14,702.31,206.14,6.91"><p>Available at http://ilps.science.uva.nl/resources/cikm2010-entity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,68.14,712.12,108.26,6.91"><p>http://www.lemurproject.org/indri</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>This research was partially supported by the <rs type="funder">European Union</rs>'s <rs type="programName">ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme</rs>, <rs type="funder">CIP ICT-PSP</rs> under grant agreement nr 250430, by the <rs type="funder">PROMISE Network of Excellence</rs> co-funded by the 7th <rs type="programName">Framework Programme</rs> of the <rs type="funder">European Commission</rs>, grant agreement no. <rs type="grantNumber">258191</rs>, by the <rs type="projectName">DuOMAn</rs> project carried out within the <rs type="programName">STEVIN programme</rs> which is funded by the <rs type="funder">Dutch and Flemish Governments</rs> under project nr <rs type="grantNumber">STE-09-12</rs>, by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project nrs <rs type="grantNumber">612.066.512</rs>, <rs type="grantNumber">612.061.814</rs>, <rs type="grantNumber">612.-061.815</rs>, <rs type="grantNumber">640.004.802</rs>, <rs type="grantNumber">380-70-011</rs>, and by the <rs type="institution">Center for Creation, Content and Technology (CCCT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QqtYJkP">
					<orgName type="program" subtype="full">ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_sCnVkfH">
					<orgName type="program" subtype="full">Framework Programme</orgName>
				</org>
				<org type="funded-project" xml:id="_NyqbCUp">
					<idno type="grant-number">258191</idno>
					<orgName type="project" subtype="full">DuOMAn</orgName>
					<orgName type="program" subtype="full">STEVIN programme</orgName>
				</org>
				<org type="funding" xml:id="_yBWeFwn">
					<idno type="grant-number">STE-09-12</idno>
				</org>
				<org type="funding" xml:id="_NYW3FqW">
					<idno type="grant-number">612.066.512</idno>
				</org>
				<org type="funding" xml:id="_a6QzGED">
					<idno type="grant-number">612.061.814</idno>
				</org>
				<org type="funding" xml:id="_SS9ztdx">
					<idno type="grant-number">612.-061.815</idno>
				</org>
				<org type="funding" xml:id="_5p9SY3y">
					<idno type="grant-number">640.004.802</idno>
				</org>
				<org type="funding" xml:id="_bMfph5q">
					<idno type="grant-number">380-70-011</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,338.33,53.86,66.65,12.90;7,316.81,79.36,239.10,8.82;7,326.78,91.32,95.45,8.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,493.33,79.36,62.58,8.59;7,326.78,91.32,25.38,8.59">A semantic web primer</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>References Antoniou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Van Harmelen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,316.81,111.42,239.10,8.64;7,326.78,123.38,229.14,8.64;7,326.78,135.15,175.12,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,531.12,111.42,24.80,8.64;7,326.78,123.38,229.14,8.64;7,326.78,135.33,113.48,8.64">A few examples go a long way: constructing query models from elaborate query formulations</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,458.74,135.15,39.22,8.59">SIGIR &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,316.81,155.26,239.10,8.64;7,326.78,167.03,218.47,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,530.46,155.26,25.45,8.64;7,326.78,167.21,73.68,8.64">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,407.84,167.03,80.95,8.59">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,316.81,187.14,239.10,8.64;7,326.78,198.91,220.02,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,508.20,187.14,47.71,8.64;7,326.78,199.09,158.45,8.64">Ranking related entities: Components and analyses</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,503.63,198.91,39.22,8.59">CIKM &apos;10</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,316.81,219.02,239.10,8.64;7,326.78,230.97,229.14,8.64;7,326.78,242.75,229.14,8.82;7,326.78,254.88,92.13,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,481.32,219.02,74.59,8.64;7,326.78,230.97,229.14,8.64;7,326.78,242.93,83.59,8.64">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,428.08,242.75,38.83,8.59">SIGIR &apos;98</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,316.81,274.81,239.10,8.64;7,326.78,286.77,229.14,8.64;7,326.78,298.54,225.23,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,521.61,274.81,34.30,8.64;7,326.78,286.77,229.14,8.64;7,326.78,298.72,105.26,8.64">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,450.75,298.54,32.29,8.59">ACL &apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,316.81,318.65,239.10,8.64;7,326.78,330.60,229.14,8.64;7,326.78,342.38,92.72,8.82" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,451.38,318.65,104.54,8.64;7,326.78,330.60,229.14,8.64;7,326.78,342.56,31.67,8.64">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<idno>SIGIR &apos;01</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,316.81,362.48,239.10,8.64;7,326.78,374.44,229.14,8.64;7,326.78,386.21,53.96,8.82" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,453.64,362.48,102.28,8.64;7,326.78,374.44,225.62,8.64">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>In CIKM &apos;01</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
