<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,132.62,80.24,348.13,18.00;1,150.86,111.44,311.62,18.00">A Multiple-Stage Framework for Related Entity Finding: FDWIM at TREC 2010 Entity Track</title>
				<funder ref="#_EV74ryn">
					<orgName type="full">863 Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.50,138.55,49.76,9.21"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<email>wangdong@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.85,138.55,35.81,9.21"><forename type="first">Qing</forename><surname>Wu</surname></persName>
							<email>qingwu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.49,138.55,66.70,9.21"><forename type="first">Haiguang</forename><surname>Chen</surname></persName>
							<email>hgchen@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.27,138.55,44.48,9.21"><forename type="first">Junyu</forename><surname>Niu</surname></persName>
							<email>jyniu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,132.62,80.24,348.13,18.00;1,150.86,111.44,311.62,18.00">A Multiple-Stage Framework for Related Entity Finding: FDWIM at TREC 2010 Entity Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">11070A45B082C294483011C9B8E5B94F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a multiple-stage retrieval framework for the task of related entity finding on TREC 2010 Entity Track. In the document retrieval stage, search engine is used to improve the retrieval accuracy. In the entity extraction and filtering stage, we extract entity with NER tools, Wikipedia and text pattern recognition. Then stoplist and other rules are employed to filter entity. Deep mining of the authority pages is proved to be effective in this stage. In entity ranking stage, many factors including keywords from narrative, page rank, combined results of corpus-based association rules and search engine are considered. In the final stage, an improved feature-based algorithm is proposed for the entity homepage detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the rapid development of World Wide Web, web data is a huge wealth for many fields. In the past, users will get a lot of documents when they want to find some information. There is still a lot of work to do if users want to find exact information, for example exact entities or their homepage. In entity track of TREC 2010, the main task is related entity finding, which is defined as follows:</p><p>Given an input entity, by its name and homepage, the type of the target entity, as well as the nature of their relation, described in free text, find related entities that are of target type, standing in the required relation to the input entity. To finish this task, we design a multiple-stage framework which consists of the following five stages: document retrieval, entity extraction, entity filtering, entity ranking, and homepage detection. We raise some new conceptions such as authority pages and employ some methods including the combination of corpus-based association rules and search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">APPROACHES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Retrieval</head><p>For the whole framework, the initial stage is document retrieval. We consider the entities extracted from those authority pages to be more reliable. We do parsing for the narrative, employ stoplist, extract keywords, and finally generate query. These queries are put into Google and the top 10 return pages in the clueweb09 are selected as our document collection. The document weight is designed according to Google's rank, which will be explained in detail in the entity ranking section. We also add authority pages to the document collection, of which we do deep mining. In our experiments, authority pages are defined as source entity homepages given by the query and their Wikipedia pages. We extract entities from tables and list of the authority pages using both text content and DOM tree, and then give these entities higher score in the entity ranking stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Entity Extraction</head><p>Entity Extraction is the second stage of the framework. We mainly use Stanford Named Entity Recognition<ref type="foot" coords="2,141.38,258.22,3.48,6.26" target="#foot_0">1</ref> to extract entity. Also we adopt text pattern recognition methods to improve the accuracy of the entity extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Entity Filtering</head><p>Entity filtering is the third stage in the framework. It mainly depends on stoplist. Other rules include the length of the entity and whether the entity is a Wikipedia title. We filter each kind of entity with a corresponding upper and lower limit length. After entity filtering, we get the candidate entity set E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Entity Ranking</head><p>Entity Ranking is the fourth stage of the framework. We focus on how to measure the accuracy of those candidate entities. For a candidate entity e in the set E, its score is computed as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Entity Frequency</head><p>Entity frequency in the document collection D is calculated by: where is the maximum frequency of the entity extracted from the document collection, and is the minimum, is the frequency of entity e, is the score of the entity which has the maximum frequency (usually 1.0 for normalized), and is the minimum. Besides, entities extracted from authority page are counted twice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Document Ranking</head><p>As we use Google's return pages as our document collection, page rank by Google can be a good reference. We rank the document according to calculated by where is the weight parameter between 0 and 1. is the page number(in the experiment, ), and is the rank of page e extracted from. For multiply pages, we use the top one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Confidence of e to Source Entity</head><p>We combine corpus-based association rules and search engine to calculate the relationship between e and source entity. This method is originally proposed by Richard Chow for privacy leak detecting <ref type="bibr" coords="3,127.00,146.53,12.33,9.50" target="#b0">[1]</ref>. The confidence of an inference e to source can be computed by: where is the result number returned by search engine for the query source entity and candidate entity , and is for candidate entity only. E.g., for the source entity "Microsoft" and a candidate entity "Windows 7", Google return 74,700,000 results for "Microsoft Window 7", and 483,000,000 results for "Windows 7", so the confidence of an inference "Windows 7" to "Microsoft" is .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Keywords</head><p>To check the consistency between e and query narrative, we employ keyword. This is similar to <ref type="bibr" coords="3,90.02,310.36,11.16,9.50" target="#b1">[2]</ref>. They define the first term or phrase of each query narrative as the only keyword, and in our system, we use multiple keywords. The keywords are extracted from the query narrative, which can be plural noun, organization, location, date and other term or phrase. For example , "Authors awarded an Anthony Award at Bouchercon in 2007" , the keywords can be "Authors" , "Anthony Award", " Bouchercon", "2007". We parse the query narrative with WordNet and other method. It is similar to do an entity extraction to generate query narrative, but not the same (In run FDwimET3, we extract keywords manually instead of using WordNet). Due to the variety of keywords, we believe that the frequency or category can not reflect the confidence well, so we only use the distance. If one keyword repeats for several times in the document, we use the one nearest to e. The minimum distance between the entity and keyword is calculated by: where is the weight parameter between 0 and 1, is the distance between e and keyword, the length of document, , the . If keyword and e do not co-occurrence at any document, then . We use two keywords in our experiments, the results are marked as and separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.5">Final Ranking</head><p>Consider the above factors, we use a linear combination of each score to yield the ranking score for e as follow:</p><p>where are the combination coefficients between 0 and 1 for the scores , , , , , and . For entities extracted by text pattern recognition method described in section 2.2, where C is a constant greater than 1. For other entities, is the final ranking score for e. Then we rank the entity list according to .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Homepage Detection</head><p>In the final stage, an improved feature-based algorithm is proposed for the entity homepage detection, in which features includes URL features, page content features and others. For every entity in the entity list, we just use its name as a query, and get Google's top 5 return non-Wikipedia pages that are in the clueweb09 as its candidate homepages. Then each page can get a score by the feature-based algorithm. The highest scoring page is selected as the homepage of the entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>We submitted the following four runs: FdWimET1: Run the complete process described above; FdWimET2: Run with different value of combination coefficients in Section 2.4.5 and fewer filtering rules compared to FdWimET1 FdWimET3: Run with keyword selected manually described in Section 2.4.4 FdWimET4: Run without using Wikipedia in the stage of entity filtering. Table1: Performance of all submitted runs for P@10 ,NGCG@R, Map, R-prec scores, and the number of relevant and primary entities (related homepage returned and primary homepage returned, respectively) retrieved Table <ref type="table" coords="4,116.18,545.34,5.28,9.50">1</ref> lists the results for our four runs. From the table, we can see that all P@10 and nDCG@R scores are over 0.3, which proves that deep mining of authority pages and employing text pattern recognition in the entity extraction stage is effective for related entity finding. For FdWimET1 vs FdWimET2, we can see that strict filtering rules result in fewer return pages but higher P@10 score. For FdWimET4 in which Wikipedia is not used for filtering entities, more entities are extracted compared to other runs. That is the reason why FdWimET4 returns the most primary and related homepages. Figure <ref type="figure" coords="4,120.04,654.57,5.28,9.50" target="#fig_0">1</ref> shows the NDCG@R scores of our best run and best run of all TREC2010 runs for each topic. For topics 35, 46, and 59 are not judged official, so the evaluation results are over 47 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>According to the figure, our system returns highest NDCG@R scores for 8 topics. Also there are 6 out of 47 queries got zero. The problem might be in the document retrieval stage and/or the entity extraction stage(depending too much on NER tools). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,117.98,311.21,359.46,10.56;5,91.50,73.15,415.30,231.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NDCG@R scores of our best run and best runs of TREC2010 for each topic</figDesc><graphic coords="5,91.50,73.15,415.30,231.70" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,97.58,760.77,178.24,9.00"><p>http://nlp.stanford.edu/software/CRF-NER.shtml</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">ACKNOWLEDGMENTS</head><p>We would like to sincerely thank <rs type="person">Lin Bao</rs> and <rs type="person">Jiachen Chen</rs>. In addition, our work is supported by <rs type="funder">863 Program of China</rs> <rs type="grantNumber">2009AA01Z429</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EV74ryn">
					<idno type="grant-number">2009AA01Z429</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,111.02,686.66,394.21,8.96;5,90.02,702.26,109.86,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,247.20,686.66,243.16,8.96">Detecting privacy leaks using corpus-based association rules</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Golle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Staddon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,90.02,702.26,17.61,8.96">KDD</title>
		<meeting><address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,111.02,717.86,383.67,8.96;5,90.02,733.48,404.57,8.96;5,90.02,749.08,43.58,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,283.82,717.86,210.87,8.96;5,90.02,733.48,280.89,8.96">Entity Retrieval with Hierarchical Relevance Model, Exploiting the Structure of Tables and Learning Homepage Classifiers</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,388.03,733.48,45.09,8.96">TREC 2009</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
