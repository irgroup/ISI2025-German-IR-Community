<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,114.66,81.22,365.96,16.65">UIC at TREC 2010 Faceted Blog Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,232.68,113.39,48.37,10.46"><forename type="first">Lifeng</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>at Chicago 851 S Morgan St</addrLine>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.42,113.39,58.28,10.46"><forename type="first">Clement</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>at Chicago 851 S Morgan St</addrLine>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,114.66,81.22,365.96,16.65">UIC at TREC 2010 Faceted Blog Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">475F4469BB57F9BFEA6370D99DDFB8DB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our system consists of a concept-based retrieval subsystem which performs the baseline blog distillation, an opinion identification subsystem and an opinion-in-depth analysis subsystem which performs the faceted blog distillation task. In the baseline task, documents which are deemed relevant are retrieved by the retrieval system with respect to the query, without taking into consideration of any facet requirements. The feeds are ranked in descending order of the sum of the relevance scores of retrieved documents. In order to improve the recall of the retrieval subsystem, we recognize proper nouns or dictionary phrases without requiring matching all the words of the phrases. In the opinionated vs. factual and personal vs. official faceted tasks, the opinion identification subsystem is employed to recognize query-relevant opinions within the documents. Personal documents are more likely to be opinionated than official documents. In the in-depth vs. shallow faceted task, the depth of the opinion within a document is measured by the number of concepts which are related with the query the document contains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the prevalence of Internet, more and more people express their opinions by writing online. Internet provides various textual resources covering a broad range of topics. Therefore, many researchers are interested in analyzing online text corpuses in depth, such as Blogosphere. Since 2006, TREC <ref type="bibr" coords="1,479.11,497.55,11.69,8.74" target="#b6">[8]</ref> has started a new track which provides Blogosphere collections to perform various text analysis techniques such as opinion retrieval <ref type="bibr" coords="1,188.59,528.81,69.25,8.74">[8, 9, 10, 17，15]</ref>, polarity classification <ref type="bibr" coords="1,354.51,528.81,10.86,8.74" target="#b7">[9,</ref><ref type="bibr" coords="1,367.87,528.81,12.54,8.74" target="#b8">10,</ref><ref type="bibr" coords="1,382.97,528.81,13.42,8.74" target="#b11">15]</ref> and blog distillation <ref type="bibr" coords="1,481.72,528.81,10.81,8.74" target="#b7">[9,</ref><ref type="bibr" coords="1,495.09,528.81,12.54,8.74" target="#b8">10,</ref><ref type="bibr" coords="1,87.66,544.35,11.91,8.74" target="#b9">11]</ref>. Blog distillation was introduced in 2007 <ref type="bibr" coords="1,270.69,544.35,11.69,8.74" target="#b7">[9]</ref> and continued from 2008 to 2010 <ref type="bibr" coords="1,422.45,544.35,15.85,8.74" target="#b8">[10,</ref><ref type="bibr" coords="1,440.91,544.35,11.91,8.74" target="#b9">11]</ref>. In 2007 and 2008, blog distillation only focuses on the ad-hoc retrieval of feeds according to topical relevance. Since 2009, the facet blog distillation has been introduced. It addresses not only the topical relevance but also the quality of a given facet. These facets are paired into three groups: 1) Opinionated vs. Factual: Some bloggers may make opinionated comment on the topics of interest, while others report factual information. A user may be interested in blogs which show prevalence to opinionatedness. For this group, the values of facets are "opinionated' vs.</p><p>"factual" blogs. <ref type="bibr" coords="1,194.99,659.55,16.64,8.74" target="#b9">[11]</ref> 2) Personal vs. Official: Companies are increasingly using blogging as an activity for PR purposes. However, a user may not wish to read such mostly marketing or commercial blogs, and prefer instead to keep to blogs that appear to be written by individuals without commercial influences. For this group, the values of facets are "personal" vs. "official" blogs. <ref type="bibr" coords="1,129.66,743.55,16.72,8.74" target="#b9">[11]</ref> 3) In-depth vs. Shallow: Users might be interested to follow bloggers whose posts express in-depth thoughts and analysis on the reported issues, preferring these over bloggers who simply provide quick bites on these topics, without taking the time to analyze the implications of the provided information. For this group, the values of facets are "in-depth" vs. "shallow" blogs (in terms of their treatment of the subject). <ref type="bibr" coords="2,326.44,139.17,16.68,8.74" target="#b9">[11]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BASELINE BLOG DISTILLATION TASK</head><p>To fulfill the baseline task which addresses only the topical relevance, an improved concept-based retrieval subsystem is utilized. The feeds are ranked in descending order of the weighted sum of topical relevance scores of retrieved documents belonging to them. The information retrieval subsystem has four components: concept identification, query expansion, concept based retrieval and document filters.</p><p>We improved our last two components to enhance recall and precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Concept Identification</head><p>A concept in a query is a multi-word phrase or a single word that denotes an entity. Four types of concepts are defined: proper nouns, dictionary phrases, simple phrases and complex phrases. The proper nouns are names of people, place, event, organization etc, such as "Hugo Chavez". A dictionary phrase is a phrase that has an entry in a dictionary such as Wikipedia [13], but is not a proper noun, such as "laser eye surgery". A simple phrase is a 2-word phrase, which is grammatically valid but is not a dictionary entry or a proper noun, e.g. "genealogical sources". A complex phrase has 3 or more words but is neither a proper noun nor a dictionary phrase, such as "United States future decline". We developed an algorithm that combines several tools to identify the concepts in a query. We use Minipar <ref type="bibr" coords="2,87.66,430.77,10.66,8.74">[7]</ref>, WordNet [14], and Wikipedia [13] and Google for proper noun and dictionary phrase identification.</p><p>Collins Parser <ref type="bibr" coords="2,147.34,446.37,11.70,8.74" target="#b2">[2]</ref> is used to find the simple phrase and complex phrase. Web search engine (Google) is also used for identifying simple phrases within complex phrases. The details of the algorithm can be found in <ref type="bibr" coords="2,123.76,477.57,15.35,8.74" target="#b12">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Expansion</head><p>Query expansion is another technique in the retrieval component. Two types of expansions are obtained: concept expansion and term expansion. In concept expansion, query concepts are recognized, disambiguated, if necessary and their synonyms are added. For example, for the query "gun control DC", there are many possible interpretations of "DC", according to Wikipedia. But, by using the query words "gun control", "DC" is disambiguated to "Washington DC", because "gun control" appears only in the Wikipedia entry of "Washington DC". As an example for concept expansion, consider the query "alternative treatments for ADHD". Proper noun "ADHD" has the synonym "Attention Deficit Hyperactivity Discord". Thus, the query becomes "alternative treatments for ADHD" OR "alternative treatments for Attention Deficit Hyperactivity Discord". Term expansion is carried out by the pseudo-feedback process in which terms in the vicinities of query terms in the top retrieved documents are extracted. We apply this technique to three different collections and take the union of the extracted terms. Specifically, the TREC documents and Web documents (via the use of Google) are employed. In addition, if a page in Wikipedia is found to represent a query concept, frequent words in that page are extracted. The union of terms extracted from these three sources is taken as the set of expanded query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Concept-Based Information Retrieval</head><p>After concepts identification and query expansion, an original query will be augmented with a list of concepts and their synonyms (if exists) and additional words. In our information retrieval module, a query-document similarity consists of two parts: the concept similarity and the term similarity (concept-sim, term-sim). The concept-sim is computed based on the identified concepts in common between the query and the document. The term-sim is the usual term similarity between the document and the query using the Okapi formula <ref type="bibr" coords="3,244.90,176.37,15.33,8.74" target="#b10">[12]</ref>. Each query term that appears in the document contributes to the term similarity, irrespective of whether it occurs in a concept or not. The concept-sim has a higher priority than the term-sim, since we emphasize that the concept is more important than individual terms.</p><p>Consider, for a given query, two documents D 1 and D 2 having similarities (x 1 , y 1 ) and (x 2 , y 2 ), respectively, where a x component represents concept similarity and a y component represents a term similarity.. D 1 will be ranked higher than D 2 if either (1) x 1 &gt; x 2 , or (2) x 1 = x 2 and y 1 &gt; y 2 . Note that if x i &gt;0, then the individual terms which contribute to concept-sim will ensure that y i &gt;0. The calculation of concept-sim is described in <ref type="bibr" coords="3,198.80,285.57,10.56,8.74" target="#b4">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Relaxed Recognition of Proper Nouns and Dictionary Phrase</head><p>In this subsection, we present a new technique to retrieve documents which contain some, but not necessarily all component words of a proper noun query concept or a dictionary query concept. It is known that proper nouns and dictionary phrase appear frequently in user queries. For example, in TREC blog queries collected from 2006 to 2008, there are 114 proper noun queries and 14 dictionary phrase queries among 150 queries. We first give an example to illustrate the idea. Consider the dictionary phrase C = "Genome sequences". Suppose there is a document containing the word S = "Genome", but without the word "sequences" next to it. This document may refer to a novel, instead of the hereditary information in molecular biology and genetics. To determine whether S in the document can be used to represent C, we proceed as follows:</p><p>(1) S is a prefix of C, if C represents a non-person proper noun or a dictionary phrase; if C is the name of a person, S is the last name;</p><p>(2) Both C and S are defined in Wikipedia;</p><p>(3) If S and C refer to the same entry in Wikipedia, then S can be used to represent C unambiguously; if S can refer to multiple entries in Wikipedia but the words in C -S can be used to uniquely identify the same entry as C in Wikipedia, then S is an ambiguous representation of C.</p><p>(4) If S is an ambiguous representation of C, a document D containing S must also contain at least one of the top two expanded terms of the documents containing C initially retrieved using the pseudo-feedback process or the terms in C -S; both S and one of these terms must be within a small window in D. If these conditions are satisfied, then C is assumed to be present in the document D.</p><p>For example, C = "New York Philharmonic Orchestra" and its prefix S = "New York Philharmonic" unambiguously refers to C according to Wikipedia. Therefore, "New York Philharmonic" can represent C without any constraints. However, in the "Genome sequences" example, Wikipedia has a lot of ambiguous entries about "Genome" but only the entry titled "Genome" has "sequences" in its content.</p><p>Therefore "Genome" can refer to "Genome sequences" if at least one of the top two expanded terms which are "DNA" and "genetics" or the query term "sequences" appears in close proximity with "Genome". As another example, suppose the user query is "Hugo Chavez". A document containing "Chavez" may or may not refer to the Venezuela President. It can be assumed to contain the query concept, if "Chavez" co-occurs in close proximity with at least one of the top two expanded terms "Venezuela" or "President" or the query term "Chavez". In the next subsection, we assign weights to such proper nouns, which are recognized in documents without exact matching all its component terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Relaxed Recognition of Proper Nouns</head><p>After a query concept is recognized in a document, it contributes a concept similarity to the document. If the document contains an ambiguous query concept, which can represent the query concept, the contribution will be reduced by a small value ∆, because there is a possibility that the representation is incorrect. We now determine the value of ∆ such that the following condition is satisfied. Let D 1 be a document containing a subset S i = {C k } of the set C of query concepts, D 2 be a document containing essentially the same set of query concepts S i '= {C k '} where each C k ' is either an original query concept C k in S i or an ambiguous representation of C k and at least one C k ' in S i ' is an ambiguous representation of C k in S i and D 3 be a document containing another subset S j of C. If the concept similarity of D 1 is greater than that of D 3 , then the concept similarity of D 2 is greater than that of D 3 while the concept similarity of D 1 is greater than that of D 2 . D 1 should be ranked higher than D 2 , because of the uncertainty of concept representation. D 2 should be ranked higher than D 3 , because the ordering between D 1 and D 3 should be preserved by replacing D 1 by D 2 , as D 2 has essentially the same set of query concepts as that of D 1 .</p><p>Supposed that the query "Lance Armstrong, Alexander Vinokourov" is submitted, the document D 1 containing both proper nouns will be ranked ahead of the document D 3 containing only "Lance Armstrong". The document D 2 containing "Armstrong" and "Alexander Vinokourov" and satisfying the relaxed form of "Lance Armstrong" should be ranked lower than D 1 but higher than D 3 .</p><p>The following formula guarantees the above property. The justification is not given due to the lack of space. Given a query topic with a set of concepts, C = {C 1 , C 2 , … , C n }, let the concept weight due to C i be W i . For an ambiguous representation C i ', its concept weight is W i ' = W i -∆, where ∆ is computed by the formula below, where S i and S j are two subsets of C.</p><formula xml:id="formula_0" coords="4,171.84,521.22,249.72,76.14">1 min , 0 , 2 , 2 | | , 1 2 C S S i j m S S S S i j m W W m S S n C W n ∈ ∈ ⎧ ⎧ ⎫ ⎪ ⎪ ⎪ = - &gt; ∈ ⎨ ⎬ ≥ ⎪ ⎪ ⎪ ⎩ ⎭ ⎪ Δ = ⎨ ⎪ ⎪ = ⎪ ⎩ ∑ ∑</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Document Filters</head><p>Some techniques, such spam filters, are utilized to improve performance (especially precision) of concept-based document retrieval system. A spam component <ref type="bibr" coords="4,348.33,660.75,16.69,8.74" target="#b11">[15]</ref> is incorporated to filter out those spam documents, such as pornographic documents and non-English spam documents. Moreover, irrelevant documents where query terms only appear in some irrelevant portions of documents, such as advertisement or navigation bar, are also removed. <ref type="bibr" coords="4,296.29,707.55,11.67,8.74" target="#b5">[5]</ref> validated that the removal of irrelevant portions, such as advertisement, from a blog document can significantly improve the retrieval effectiveness within blogosphere. Advertisements usually have the following characteristics: 1) each advertisement is the contents of a leaf node in an HTML tree; 2) a number of advertisements are in common among documents of the same feed, because a feed of documents is disseminated by the same content distributor. Thus, if the contents of a leaf node in a document are identical to that of a leaf node of another document in the same feed, then it is considered to be an advertisement. (In contrast, if a sentence in the main text of a document is identical to a sentence in a different document, but the sentence is not the entire contents of a leaf node, then the sentence is not recognized as an advertisement.</p><p>In the main text, usually a paragraph or a sequence of paragraphs forms the contents of a leaf node.)</p><p>Navigation bars within a document are adjacent hyperlinks and there are usually three or more such hyperlinks within the document. If query terms appear in advertisements or navigation bar portions of a document, they will not be used for retrieving the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Topical Relevance Ranking of Feeds</head><p>After documents are retrieved with respect to a query topic, a ranking of feeds will be generated. To demonstrate the rationale of ranking feeds according to the information of retrieved documents, some annotations are introduced first. Let q and f be a query topic and a feed respectively; D q denotes the set of documents retrieved with respect to q and D f is the documents of f; IR D is the IR score of document D.</p><p>For each feed, an aggregated score, S f , is calculated as below and feeds are ranked according to descending order of this score.</p><formula xml:id="formula_1" coords="5,229.80,350.26,133.48,27.59">| | | | f q f q f D D D D f D D S I R D ∈ = × ∑ I I</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FACETED BLOG DISTILLATION TASK</head><p>In faceted blog distillation task, six facets are identified and paired into three groups: opinionated vs. factual, personal vs. official and in-depth vs. shallow. In this section, we describe our opinion identification system. Then we propose the technique to measure the depth of an opinion within a document. Finally, we present the faceted feed ranking strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Opinionated vs. Factual</head><p>A document is a relevant opinionated document with respect to a query topic, if it consists of at least one sentence, which is opinionated and is directed toward the query topic. We adopt the opinion analysis system from <ref type="bibr" coords="5,141.86,558.99,15.84,8.74" target="#b11">[15,</ref><ref type="bibr" coords="5,161.15,558.99,11.88,8.74" target="#b13">17]</ref>. The documents retrieved from the document retrieval system are classified into three categories: (1) factual documents; (2) opinionated documents but without topic-relevant opinion; and (3) opinionated documents with topic-relevant opinion. The opinion analysis system first utilizes a support vector machine (SVM-Light <ref type="bibr" coords="5,242.94,605.79,11.25,8.74" target="#b3">[3]</ref>) classifier to distinguish the documents of ( <ref type="formula" coords="5,442.27,605.79,3.89,8.74">2</ref>) and (3) from those of (1). Then, it employs a heuristic-based classifier to differentiate documents of (3) from those of (2). The opinion score of a document is the sum of the scores of its subjective relevant sentences provided by the SVM classifier and its similarity score. This yields an aggregate score. Then, opinionated documents are ranked in descending aggregate scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">SVM-Based Opinion Classifier</head><p>A document is decomposed into sentences. Each sentence is classified by the SVM classifier to be either subjective (opinionated) or objective (factual). The document is opinionated if it has at least one opinionated sentence. In order to build the classifier, training data consisting of subjective data and objective data are collected. Given a topic, topic relevant subjective documents are collected from review web sites such as Rateitall.com and Epinions.com. Additional documents are collected from the retrieved results of a search engine (Google) by submitting the query topic plus some "opinion indicator phrases" such as "I like" or "I don't like". The objective training documents are collected from Wikipedia. The dictionary entry pages of Wikipedia are considered to be high-quality objective data sources, as these pages describe things without opinion. The unigrams (individual words) and bigrams (two adjacent words form a bigram) extracted from the training data are the potential features to train the SVM classifier. We adopt the Pearson's Chi-square Test <ref type="bibr" coords="6,317.28,185.97,11.67,8.74" target="#b1">[1]</ref> to select the features. After the features are determined, each sentence from the training data is presented in a presence-of-feature vector, i.e. only the presence or absence of each feature is recorded in the vector, but not the number of occurrences of the feature. Then an opinion SVM classifier is established over that set of labeled vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">The NEAR Operator</head><p>After an opinionated sentence is identified by the opinion classifier, the opinion in the sentence may or may not be directed toward the query topic. The NEAR operator determines whether the opinionative sentence is pertinent to the query topic. Intuitively, an opinionative sentence has a good chance of being pertinent to the query topic, if the query terms appear in close proximity to the sentence. To be more specific, for each opinionative sentence, a text window of five sentences is set. The window consists of the opinionative sentence, two sentences preceding it and two sentences following it. In this paper, we give five conditions which determine whether an opinionated sentence is toward the query topic.</p><p>(1) The opinionative sentences which occur before the first appearance of a query concept are not considered as they are not pertinent to the query topic.</p><p>(2) If the query topic consists of one type of concepts which is either proper noun concepts or dictionary phrase concepts but not both types of concepts, then at least one concept must be matched and one term for each of the unmatched concepts must also be found in the text window.</p><p>For example, for the query "Drug Wars in Mexico" with two proper noun concepts, "Drug Wars" and "Mexico", the opinionated sentence, "Mexico experiences a campaign of prohibition and foreign military aid being undertaken by the United States government, with the assistance of participating countries, intended to both define and reduce the illegal drug trade. … ", is relevant although it can only match "Mexico" and only a content term, "drug" from unmatched "drug wars".</p><p>(3) If the query topic contains both proper noun concepts and dictionary phrase concepts, at least one proper noun concept must be matched and at least one term for each unmatched proper noun or dictionary phrase concept must be found in the text window. For example, for the query "gun control DC" with a proper noun concept, "DC" and a dictionary concept "gun control", the opinionated sentence about "…Washington, D.C., has enacted a number of strict gun restriction laws…" is relevant although it can only match "D.C." and only a content term, "gun" from unmatched "gun control".</p><p>(4) If the query topic contains one type of concepts which is either proper noun or dictionary phrase concepts and additional content words, at least one proper noun or dictionary phrase concept and the content words must be matched and at least one term for each of unmatched concept must be found within the text window. For example, the query "sciatica remedies" with a dictionary phrase concept "sciatica" and one content word, "remedies", only the opinion concerning the treatment aspect of "sciatica" is relevant to the query. Therefore, besides the dictionary phrase, "remedies" must be matched to guarantee the opinion is about the specific aspect of "sciatica".</p><p>(5) If the query topic without any proper noun or dictionary concepts, all query content words must matched. For example, a simple phrase, "budget travel", both content terms, "budget" and "travel", must be matched to guarantee the opinion is about the low-cost travel form, instead of general travel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Personal vs. Official.</head><p>In our opinion, personal documents are more likely to be opinionated that official documents. Therefore, the same opinion identification system was employed in differentiating personal documents from official documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">In-depth vs. Shallow.</head><p>A document which provides in-depth analysis about the query topic should not only be opinionated but also contains many concepts related with the query topics. A procedure is designed to obtain the set of concepts closely related to the query topic. For the convenience of introducing our technique, let us assume that any query topic can be represented by a set of proper noun or dictionary concepts, C and a set of content words, T. RCC denotes the set of related concepts candidates. The procedure defined below returns those k concepts which are most closely related to q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function Related_Concept_Recognition</head><p>Input: A query topic q = {C, T}; Parameter k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output: k concepts related to q</head><p>1. RCC = ∅ ; // initialize 2. for each concept c ∈ C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>If c is defined in Wikipedia,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RCC = RCC U Wikipedia_Concept_Extractor(c).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">If RCC is not empty</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>for each related concept candidate c ∈ RCC</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">PMI c = Pointwise_Mutual_Information_by_Google(c, q).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">else // RCC is empty, q have no concepts defined in Wikipedia</head><p>9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RCC = Google_within_Wikipedia(q)</head><p>10.</p><p>for each related concept candidate c ∈ RCC</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">PMI c = Pointwise_Mutual_Information_by_Google(c, q).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Rank related concept candidates according to descending order of PMI scores</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">Return top k related concept candidates.</head><p>The rationale of the function, "Related_Concept_Recognition", is to first locate a set of related concept candidates and then select those concepts which are closely related to the query topic, q, from those candidates by the Pointwise Mutual Information (PMI).</p><p>In line 1, RCC which stores all related concept candidates is initialized. In line 4, the function Wikipedia_Concept_Extractor(c) extracts the various types of concepts from the Wikipedia entry c and stores them in RCC. Proper noun and dictionary phrase concepts can heuristically be identified by those anchor texts which point to entries in Wikipedia. Moreover, simple and complex phrases are identified from the subtitles of Wikipedia entry of c, because subtitles normally convey related information concerning various aspect of the concept c.</p><p>From lines 5 to 7, each related concept candidate is assigned a weight equal to the PMI score which is estimated using documents retrieved by Google. The formula below is utilized to estimate the PMI score of a related concept candidate, rc, and the query topic q.</p><formula xml:id="formula_2" coords="8,109.02,141.02,392.45,51.22">( ) ( ) . . . | | ( , ) log , ( ) ( ) | | | |</formula><p>GD rc AND q where GD x is the set of documents retrieved by Google w r t x D PMI rc q GD rc GD q D is the whole set of documents indexed by Google D D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= ×</head><p>Lines 8-11 are intended for the situation when the query topic q has no concept defined in Wikipedia.</p><p>For example, "Budget Travel" is a simple phrase query, but it is not defined in Wikipedia. The heuristic rule to locate the related concepts is to employ the parameterized Google search to retrieve documents from the site of Wikipedia and extract related concept candidates from the top 10 Wikipedia documents retrieved by Google. Then each related concept candidate is weighted by the PMI score of the query topic and it. For example, the top 10 Wikipedia documents retrieved by Google w.r.t. "Budget Travel" are shown and explained below. Among these 10 documents, 7 of them are related to the travel with budget or low cost and 3 of them are generally related with travel.</p><p>1) Backpacking(a form of low-cost, independent international travel);</p><p>2) Arthur Frommer (a travel writer whose writes a guide about budge travel);</p><p>3) Let's Go Travel Guides(the first travel guide series aimed at the student traveler); 4) CityPASS (A company that produces and sells booklets contain entrance tickets which is deeply discounted from the regular admission prices); After a set of related concepts is obtained, the extent of the depth of opinion within a blog document is measured by the sum of the normalized weights of related concepts which appear in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Facet Feed Ranking Strategy</head><p>A blog document is retrieved by the concept-based retrieval subsystem w.r.t. a query topic and then assigned a facet score w.r.t. the interested facet value. For the facet of "opinionated" (or "personal"), the relevant opinionated sentences within the blog document are identified by the opinion system and the facet score of "opinionated" (or "personal") is the sum of their SVM scores. For the facet of "factual" (or "official"), the facet score is the inverse of its facet score of "opinionated" (or "personal"). For the facet of "in-depth", the facet score is the sum of normalized weights of related concepts which appear in the document and the facet score for the "shallow" facet is the inverse of the "in-depth" score. After the facet score is calculated for a blog document, d, an aggregated score is obtained by linearly combining of its IR score and facet score as below.</p><p>(</p><formula xml:id="formula_3" coords="8,250.14,707.13,176.87,10.33">) d d d<label>1</label></formula><p>A ggregatedScore a IR S core a F a cetScore = ⋅ + -⋅</p><p>Let q and f be a query topic and a feed respectively; D q denotes the set of documents retrieved with respect to q and D f is the documents of f; AG D is the aggregated score of document D. For each feed, a facet aggregated score, FS f , is calculated as below and feeds are ranked according to descending order of this score.</p><formula xml:id="formula_4" coords="9,223.32,96.59,146.71,28.99">| | | | f q f q f D D D D f D D FS A G D ∈ = × ∑ I I</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT EVALUATION</head><p>In this section, we evaluate the concept-based retrieval subsystem, opinion identification subsystem and opinion-in-depth system by Blogs08 Blogosphere collection and 63 of 100 queries released from 2009 to 2010. Only 39 of 50 queries from TREC 2009 contain at least one feed in both of two interested facets assigned to the queries. By now only 37 of 50 queries has been manually judged and only 24 of these manually-judged queries contain at least one feed in both two facets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Blog Distillation</head><p>Table <ref type="table" coords="9,113.71,290.23,5.25,9.16" target="#tab_0">1</ref> shows the baseline performance of our concept-based retrieval subsystem. We utilize MAP, P@10, bPref and rPrec to measure the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Facet Blog Distillation</head><p>In this section, we report the performance of our opinion identification system and opinion-indepth system. We evaluate the MAP scores of rankings for six facets in Table <ref type="table" coords="9,414.62,442.03,3.94,9.16" target="#tab_1">2</ref>. TREC coordinators also provide three topical baselines and suggest all participants employ their techniques on these baselines to measure the effectiveness of their techniques. In Table <ref type="table" coords="9,499.34,543.48,4.16,9.57" target="#tab_2">3</ref>, we report the MAP scores of six faceted feed rankings on baselines with respect to 39 queries from TREC 2009. In table 4, the MAP scores of six faceted feed rankings on baselines are presented with respect to 24 queries from TREC 2010. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCULUSION</head><p>In this paper, we introduce the improved concept-based retrieval subsystem. A new technique is presented to improve the recall of the system by matching a concept within a document without requiring matching all content terms of that concept. An opinion identification subsystem and a technique to measure the depth of the opinions w.r.t. a query topic are demonstrated. The relevant opinions are identified by a SVM classifier and several heuristic rules. The depth of an opinion within a document is measured by the sum of weights of related concepts the document contains.</p><p>The performances of the various systems are reported in detail.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,87.66,404.36,321.23,8.74;8,87.66,419.96,249.03,8.74;8,87.66,435.56,120.15,8.74;8,87.66,451.16,198.28,8.74;8,87.66,466.76,152.37,8.74;8,87.66,482.36,336.85,8.74"><head>5 ) 8 )</head><label>58</label><figDesc>Hostel(provide information about budget oriented, social accommodation); 6) Low-cost carrier (airlines that generally has lower fares); 7) List of travel magazines; Guide book(a book for tourists or travelers); 9) Rofl Potts(another travel writer); 10) Primera (an Icelandic Charter airline which provides budget travel operations);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,180.42,321.12,255.39,63.77"><head>Table 1 . The Performance of Baseline Blog Distillation</head><label>1</label><figDesc></figDesc><table coords="9,193.02,343.51,209.23,41.38"><row><cell>MAP P@10 bPref</cell><cell>rPrec</cell></row><row><cell cols="2">TREC 2009 0.2841 0.3974 0.3209 0.3459</cell></row><row><cell cols="2">TREC 2010 0.2036 0.3083 0.1947 0.2500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,115.14,457.32,365.00,63.77"><head>Table 2 . The Performance of Facet Blog Distillation</head><label>2</label><figDesc></figDesc><table coords="9,115.14,479.71,365.00,41.38"><row><cell cols="5">Average Opinion Factual Personal Official In-depth Shallow</cell></row><row><cell>TREC 2009 0.1920</cell><cell>0.2175</cell><cell>0.1801 0.2190</cell><cell>0.1390 0.2678</cell><cell>0.1284</cell></row><row><cell>TREC 2010 0.1289</cell><cell>0.1080</cell><cell>0.1644 0.1088</cell><cell>0.1712 0.1125</cell><cell>0.1086</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,115.14,611.70,365.12,79.85"><head>Table 3 . Facet Blog Distillation on Baselines by TREC 2009 Queries</head><label>3</label><figDesc></figDesc><table coords="9,115.14,634.09,365.12,57.46"><row><cell cols="6">TREC 2009 Average Opinion Factual Personal Official In-depth Shallow</cell></row><row><cell>Baseline1</cell><cell>0.2193</cell><cell>0.2459</cell><cell>0.2183 0.2517</cell><cell>0.1603 0.2965</cell><cell>0.1433</cell></row><row><cell>Baseline2</cell><cell>0.1907</cell><cell>0.1970</cell><cell>0.1571 0.2221</cell><cell>0.1679 0.2725</cell><cell>0.1275</cell></row><row><cell>Baseline3</cell><cell>0.1698</cell><cell>0.1609</cell><cell>0.1442 0.1599</cell><cell>0.1754 0.2549</cell><cell>0.1233</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,115.14,698.22,365.12,63.77"><head>Table 4 . Facet Blog Distillation on Baselines by TREC 2010 Queries</head><label>4</label><figDesc></figDesc><table coords="9,115.14,720.61,365.12,41.38"><row><cell cols="6">TREC 2010 Average Opinion Factual Personal Official In-depth Shallow</cell></row><row><cell>Baseline1</cell><cell>0.1542</cell><cell>0.1650</cell><cell>0.1898 0.1192</cell><cell>0.2094 0.1259</cell><cell>0.1158</cell></row><row><cell>Baseline2</cell><cell>0.1435</cell><cell>0.1205</cell><cell>0.1729 0.1247</cell><cell>0.1966 0.1305</cell><cell>0.1160</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,92.16,257.48,97.14,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.43,279.74,370.39,8.74;10,461.82,277.57,3.24,5.65;10,471.66,279.74,36.01,8.74;10,105.66,295.34,246.89,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,247.86,279.74,213.96,8.74;10,461.82,277.57,3.24,5.65;10,471.66,279.74,36.01,8.74;10,105.66,295.34,59.58,8.74">The use of maximum likelihood estimates in χ 2 tests for goodness-of-fit</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chernoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,171.75,295.34,152.22,8.74">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.43,314.96,410.88,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,153.54,314.96,246.14,8.74">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>PhD Dissertation</note>
</biblStruct>

<biblStruct coords="10,91.43,334.58,416.37,8.74;10,105.66,350.18,93.39,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,161.95,334.58,178.19,8.74">Making large-scale SVM learning practical</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,347.85,334.58,159.96,8.74;10,105.66,350.18,64.08,8.74">Advances in Kernel Methods: Support Vector Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.43,369.74,416.23,8.74;10,105.66,385.34,238.98,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,261.51,369.74,246.16,8.74;10,105.66,385.34,138.70,8.74">An Effective Approach to Document Retrieval via Utilizing WordNet and Recognizing Phrases</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,261.79,385.34,77.98,8.74">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.43,404.96,416.19,8.74;10,105.66,420.56,353.07,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,254.60,404.96,253.02,8.74;10,105.66,420.56,195.47,8.74">DiffPost: Filtering Non-relevant Content Based on Content Difference between Two Consecutive Blog Posts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,318.45,420.56,64.48,8.74">Proc. of ECIR</title>
		<meeting>of ECIR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="791" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.43,461.78,416.28,8.74;10,108.66,477.38,89.82,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,362.11,461.78,145.60,8.74;10,108.66,477.38,21.57,8.74">Overview of the TREC-2006 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,147.87,477.38,25.56,8.74">TREC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,91.43,496.94,410.10,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,270.86,496.94,162.50,8.74">Overview of the TREC-2007 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<idno>TREC 2007</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.01,516.56,408.12,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,273.49,516.56,162.47,8.74">Overview of the TREC-2008 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<idno>TREC 2008</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.01,536.18,405.50,8.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,273.45,536.18,162.44,8.74">Overview of the TREC-2009 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<idno>TREC 2009</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.01,555.74,250.80,8.74" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Walker</forename><surname>Okapi</surname></persName>
		</author>
		<title level="m" coord="10,285.44,555.74,31.18,8.74">TREC-8</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.01,620.54,392.90,8.74;10,108.66,636.14,359.44,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,267.04,620.54,221.87,8.74;10,108.66,636.14,123.17,8.74">Improve the Effectiveness of the Opinion Retrieval and Opinion Polarity Classification</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,249.00,636.14,60.25,8.74">Proc. of CIKM</title>
		<meeting>of CIKM<address><addrLine>Napa Valley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">2008. October 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.01,657.74,393.76,8.74;10,108.66,673.34,333.14,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,330.92,657.74,158.86,8.74;10,108.66,673.34,165.53,8.74">Recognition and Classification of Noun Phrases in Queries for Effective Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,291.43,673.34,119.63,8.74">proceedings of the 16th CIKM</title>
		<meeting>the 16th CIKM</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.01,694.94,411.63,8.74;10,108.66,710.54,106.19,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,245.88,694.94,120.64,8.74">Opinion Retrieval from Blogs</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,385.93,694.94,83.52,8.74">Proc. of CIKM 2007</title>
		<meeting>of CIKM 2007<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
