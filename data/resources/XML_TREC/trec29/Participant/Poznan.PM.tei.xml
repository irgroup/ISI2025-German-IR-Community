<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.42,115.96,288.29,12.62">Poznań Contribution to TREC-PM 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.57,153.75,78.04,8.74"><forename type="first">Jakub</forename><surname>Dutkiewicz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computing</orgName>
								<orgName type="institution">Poznań University of Technology</orgName>
								<address>
									<addrLine>Plac Marii Sk lodowskiej-Curie 5</addrLine>
									<postCode>60-965</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.77,153.75,78.54,8.74"><forename type="first">Czes</forename><surname>Law Jȩdrzejek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computing</orgName>
								<orgName type="institution">Poznań University of Technology</orgName>
								<address>
									<addrLine>Plac Marii Sk lodowskiej-Curie 5</addrLine>
									<postCode>60-965</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.42,115.96,288.29,12.62">Poznań Contribution to TREC-PM 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">95891E3C6735B654F203E1756730E255</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our contribution to TREC PM 2020. We discuss the retrieval architecture used for our contribution. We split our system into four layers -preprocessing, representation, baseline and neural layer. We go over goals and specification of each layer. We conclude the section with description of our hardware setup. Then, we describe experiments conducted within this contribution -we discuss used data and retrieval models.The reranking gives little but noticeable improvement. Our results are significantly better than the median. Paper is concluded with a discussion over weaknesses and strengths of our approach, we briefly formulate what has to be done in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC PM 2020 track continues on lines of PM <ref type="bibr" coords="1,347.77,384.85,26.57,8.74">(2017,</ref><ref type="bibr" coords="1,377.91,384.85,22.69,8.74">2018,</ref><ref type="bibr" coords="1,404.17,384.85,23.80,8.74">2019)</ref> tracks with topics asking for precision medicine-related evidence to clinicians treating cancer patients. Documents in the form of The MEDLINE 2019 baseline (roughly mid-December 2018) of PubMed abtracts. This is the same corpus as last year. Topics are in a form of XML statements tagger with: Disease, Gene and Treatment. We have quite an experience in the medical search. We participated in TREC CDS 2016 <ref type="bibr" coords="1,157.82,456.58,9.96,8.74" target="#b5">[6]</ref>, TREC PM 2017, TREC PM 2018 track <ref type="bibr" coords="1,349.71,456.58,9.96,8.74" target="#b1">[2]</ref>, TREC PM 2019 <ref type="bibr" coords="1,439.83,456.58,12.57,8.74" target="#b2">[3]</ref>;and in bioCADDIE 2016 <ref type="bibr" coords="1,214.82,468.53,9.96,8.74" target="#b0">[1]</ref>. There are two basic approaches to Information Retrieval:</p><p>-Classical: -Neural networks (NN). Straight-forward implementation faces difficulty with only keywords. Lack of sentences prevents to use sequence approach as is dowe with query answering. Lack of or a small number of annotated cases (15-400) makes learning inefficient.</p><p>In this paper we expand our unsupervised approaches, which up to this point was using two steps:</p><p>-Baseline step: Divergence from Randomness, language models, within-document term-frequency (tf-idf), -Query expansion (with word embedding -obtained by neural network based autoencoders),</p><p>Supported by PUT statutory funds. One of the authors (CJ) acknowledges the nVIDIA GPU Grant of Quadro P6000 card.</p><p>The expansion comprises of the recently popular machine learning based extension. We use data from the previous competition -TREC PM 2019 in order to train a neural network, which is capable of enhancing our results. For the first time, we use the attention type network to rerank the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Architecture</head><p>We propose a dedicated Retrieval Architecture, which consists of four major layers. These layers compute the output for the following tasks:</p><p>1. Preprocessing Layer: document parsing, preprocessing and initial extraction of information. 2. Baseline Layer: indexing and retrieval with classical methods. 3. Word Space Layer: generating a word vector space. 4. Neural layer: training and reranking.</p><p>Layers are executed separately, and each of them generates data specified by an interface. The general retrieval architecture is illustrated in Fig. <ref type="figure" coords="2,449.61,354.02,3.87,8.74" target="#fig_1">2</ref>. This section comprises of specification of each layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing Layer</head><p>Goal of this layer is to prepare the raw XML data for further processing. In this layer we extract essential data and use it to create corpora with a suitable structure. We use a dedicated XML processor. This software was implemented by our team for TREC-PM 2019.Source code for this layer can be obtained via a public repository <ref type="foot" coords="3,178.60,185.35,3.97,6.12" target="#foot_0">1</ref> . It is a fast processor, implemented in C++ language. Processor is CPU based, engages all available system resources. The parser executes three tasks:</p><p>1. it reads the XML document and recognizes names of mark-up key, 2. it compares values of mark-up key names within a document with a list of informative mark-up names; it processes values of the fields within the list, 3. it extracts and tokenizes values of the processed fields.</p><p>The tokenizator is an integral part of the processor. The tokenizator executes the following rules:</p><p>1. special characters, punctuation, space and tabulator are treated as a separator, 2. new line is saved as a token, 3. all digits and numbers are converted into the same token.</p><p>A text field, after tokenization is ready for further processing. In this particular setup, we use the following mark-up keys:</p><formula xml:id="formula_0" coords="3,140.99,410.97,93.23,68.73">-PMID, -ArticleTitle, -AbstractText, -Keyword, -NameOfSubstance, -DescriptorName.</formula><p>Based on the value of these fields, two types of documents are created. Flatstructured document, which is an input to the Baseline layer and Neural layer, and a plain text document, which is an input to the Word Space Layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Baseline Layer</head><p>Goal of this layer is to create a baseline ranking list. It uses structured data in order to create an index and performs the classical Information Retrieval process. We employ Terrier <ref type="bibr" coords="3,254.30,589.02,15.50,8.74" target="#b9">[10]</ref> tool in order to create baseline ranking. We have conducted several intermediate experiments and picked the DFR BM25 model. We believe the experiments should be abstract in relation to the task, so we have chosen the model, which has been performing best on the Trec-Covid (CORD-19) dataset. Conducted experiments were in the Pseudo Relevance Feedback setting, with the parameters value of d ∈ {10, 30} and t ∈ {100, 300, 1000}, where d is a number of documents for the pseudo relevance feedback, and t is a number of terms extending the query. Results of the experiment are presented in table 1. It should be noted, that classical model performance has relatively high variance related to task characteristics. In our previous works, LGD model in some cases performed better <ref type="bibr" coords="4,276.22,178.77,9.96,8.74" target="#b0">[1]</ref>. Model performance function, based on the task characteristics is a very interesting venue of research and requires further investigation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word Space Layer</head><p>This layer is used as a model for language artifacts representation. Thanks to this layer we can take documents, written in natural language and transform them into points or shapes in multidimensional spaces. Such representation of language artifacts works very well with machine learning techniques, which otherwise we wouldn't be able to use. We employ a classical Word2Vec <ref type="bibr" coords="4,422.93,563.26,14.95,8.74" target="#b8">[9]</ref> approach in order to create a word space model. This model is used as a dictionary in a process of creating document matrices. Word vector v w is a multidimensional representation of a word w. Document matrix M is an organized list of word vectors.</p><p>M D = {v w : ∀w ∈ D}</p><p>We use L2 normalized version of the vector space, length of each vector within the space is one. In order to calculate similarity matrix S between two documents D 1 and D 2 , one just needs to multiply the document matrices.</p><formula xml:id="formula_1" coords="5,258.32,138.67,97.71,12.69">S(D 1 , D 2 ) = M D1 M T D2</formula><p>We use the similarity matrices as an input to the Neural Layer. Implementation of the Word Space Layer is done in Python3 language with use of the Gensim library <ref type="bibr" coords="5,249.02,186.41,14.61,8.74" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Neural Layer</head><p>This is the final layer. Here we take processed documents, train the neural model and generate the final result. We follow recent works in the field of machine learning enhanced Information Retrieval to manage this layer. We look forward to implement the currently best performing Attention based Networks. Before use of attention networks <ref type="bibr" coords="5,245.12,283.78,15.50,8.74">[11]</ref> there have been few positive results of deep models on IR tasks, especially ad-hoc retrieval tasks. Generally, there are two ways for solving a matching problem between a query and a document or a passage. One is the representation-focused model, which tries to build a good representation for a single text with a deep neural network, and then conducts matching between the compositional text representations built on word embeddings. The other is the interaction-focused model, appropriate for IR as argued by <ref type="bibr" coords="5,402.91,355.51,9.96,8.74" target="#b6">[7]</ref>, <ref type="bibr" coords="5,420.54,355.51,12.82,8.74" target="#b7">[8]</ref>.which first builds local interactions (i.e., local matching correlations) between two pieces of text, and then uses deep neural networks to learn hierarchical interaction patterns for matching. It has been established that or IR the attention should be in the lowest layer. We roughly follow <ref type="bibr" coords="5,314.65,403.33,10.52,8.74" target="#b4">[5]</ref> and <ref type="bibr" coords="5,347.48,403.33,10.52,8.74" target="#b3">[4]</ref> for the system architecture.</p><p>Here we propose a simplified version of the Neural Layer. This layer uses Word Space component in order to create document matrices for both documents and queries. In order to simplify the computation, we limit the vocabulary V . With use of the word space, we find 150 most similar terms for each query word. Words, which are not within the vocabulary are skipped. Then we create the similarity matrices. Similarity matrices are limited to a size of 100 highest values for each similarity vector. Then each vector is fed into three layer neural network. First layer in the network performs Max Pooling. Second and Third layer comprise of Dense layers, first one uses ReLU as an activation function; second one employs softmax. Network objective is set as a categorical cross entropy between the network prediction and query-document relevance vector (relevance is expressed as a one-hot vector).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">System Specification</head><p>We run the system on a machine with information retrieval and machine learning dedicated specification. For physical memory, we employ a disk array, which uses five disks directly connected to peripheral component interface with Non-Volatile Memory Express technology. We use this technology, to create a fast link between graphical processing unit, operating memory and physical memory. It is vital in information retrieval, as the textual corpora tend to get very large (e.g. the corpus used in Trec PM 2020 comprises of 220 GB of unprocessed data). We employ 126 GB of operational memory -it is convenient to store corpus indices directly in the operational memory for research purposes. Here, we use Elasticsearch tool for instant access to the documents within the corpus. Medium sized operational memory allows us to work on relatively large document and query matrices. We use nVIDIA P6000 graphics card for calculationsdistributed, GPU based machine learning allows us to process 20000 samples per minute. We use classical driver-CUDA-CudNN-tensorflow-keras software stack. The intermediate layer, which multiplies Document matrices by Query matrices is still implemented in Python3 with numpy library and at this moment is our bottleneck. Currently we can process roughly 400 documents per minute in this layer. Considering the size of Medline, and other IR corpora, this part requires a redo. Our system is maintained on a Clear Linux machine with remote access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conducted Experiments</head><p>In this section, we describe the conducted experiments and provided results. We go over the evaluation of our approach and compare ourselves with the Median Trec Result. In this paper we discuss our two main runs -the baseline run and the reranked run. The baseline run uses our preprocessing architecture combined with the Terrier tool in order to create a ranking. We choose DFR BM25 model in a Pseudo Relevance Feedback setting with 30 documents and 100 terms used for feedback. We use a simple concatenation of every field within the topic to create a query. We obtain a solid baseline, it outperforms median result in every available evaluation measure.</p><p>We use the remainder of our retrieval procedure to create a reranked list. Here, to create a query, we use only gene and disease fields from the topic. These fields are consistent for training data and data used in TREC PM 2020. We use concatenation of all document fields in the document matrix. Final score s f is calculated as a sum of neural network prediction s p and the baseline score</p><formula xml:id="formula_2" coords="7,134.77,486.08,232.57,22.70">s b . s f = s b -w • s neg p + w • s pos p</formula><p>The weight parameter w is arbitrarily set to 3. It should be noted, that documents are heavily shortened in the process and the queries are fairly short. Thanks to this approach, we obtain relatively cheap method of training and using network predictions. We observe, that our reranking approach outperforms original ranking in general measures -infAP, infNDCG, bpref and R-prec. The reranked list however, performs worse in a head of the list, as it is shown by P@5 and P@10 metrics. A detailed list of evaluation values is presented in Tab. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We describe experiments conducted within this contribution -we discuss used data and retrieval models. For the first time, we use the attention type network to 0,19 0,19 0,88 0,88 0,89 1,00 1,00 1,00 1,00 1,00 0,80 0,75 0,66 0,63 0,61 4 0,56 0,60 0,65 0,67 0,51 0,80 1,00 0,90 0,90 0,90 0,66 0,70 0,67 0,73 0,52 5 0,01 0,00 0,04 0,03 0,08 0,00 0,00 0,10 0,00 0,00 0,02 0,00 0,09 0,00 0,00 6 0,12 0,12 0,34 0,35 0,24 0,60 0,60 0,40 0,40 0,20 0,17 0,18 0,20 0,20 0,16 7 0,41 0,41 0,65 0,67 0,63 1,00 1,00 1,00 1,00 0,90 0,74 0,71 0,60 0,66 0,60 8 0,39 0,37 0,67 0,65 0,58 0,80 1,00 0,70 0,70 0,70 0,48 0,46 0,51 0,51 0,46 9 0,03 0,03 0,12 0,11 0,12 0,20 0,60 0,30 0,30 0,20 0,08 0,08 0,11 0,10 0,13 10 0,17 0,18 0,45 0,47 0,46 0,60 0,80 0,50 0,70 0,70 0,39 0,39 0,42 0,44 0,42 11 0,64 0,59 0,80 0,77 0,73 1,00 1,00 0,90 0,90 0,90 0,78 0,75 0,72 0,73 0,67 12 0,61 0,47 0,72 0,67 0,70 0,80 0,80 0,90 0,80 0,80 0,80 0,62 0,74 0,61 0,62 13 0,42 0,22 0,60 0,44 0,44 0,60 0,60 0,70 0,50 0,50 0,62 0,36 0,61 0,42 0,42 14 0,52 0,40 0,73 0,62 0,60 1,00 1,00 1,00 1,00 0,90 0,82 0,66 0,72 0,61 0,59 15 0,18 0,19 0,87 0,85 0,82 1,00 1,00 1,00 1,00 1,00 0,81 0,77 0,66 0,64 0,61 16 0,33 0,26 0,87 0,79 0,80 1,00 1,00 1,00 0,90 0,90 0,80 0,74 0,66 0,65 0,64 17 0,21 0,14 0,82 0,73 0,65 1,00 1,00 1,00 0,80 0,80 0,78 0,69 0,59 0,53 0,50 20 0,07 0,11 0,27 0,40 0,26 0,20 0,20 0,20 0,40 0,20 0,08 0,16 0,10 0,20 0,15 22 0,05 0,04 0,23 0,21 0,18 0,00 0,00 0,10 0,00 0,00 0,02 0,00 0,14 0,00 0,00 24 0,22 0,29 0,43 0,53 0,48 0,80 1,00 0,60 0,70 0,50 0,21 0,29 0,21 0,32 0,21 25 0,13 0,13 0,40 0,40 0,33 0,40 0,20 0,30 0,20 0,20 0,13 0,14 0,17 0,20 0,17 26 0,38 0,31 0,56 0,49 0,51 0,80 0,60 0,70 0,70 0,50 0,46 0,43 0,50 0,44 0,38 27 0,25 0,36 0,54 0,60 0,47 1,00 1,00 0,60 0,90 0,40 0,23 0,35 0,25 0,36 0,29 28 0,01 0,07 0,10 0,23 0,16 0,00 0,20 0,00 0,20 0,00 0,00 0,11 0,00 0,17 0,00 29 0,06 0,17 0,22 0,35 0,29 0,40 0,80 0,40 0,60 0,50 0,16 0,27 0,19 0,30 0,25 32 0,14 0,13 0,35 0,27 0,34 0,20 0,00 0,20 0,20 0,20 0,06 0,06 0,17 0,17 0,17 33 0,01 0,01 0,05 0,06 0,06 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 35 0,16 0,15 0,43 0,37 0,42 0,60 0,40 0,30 0,40 0,30 0,15 0,19 0,16 0,21 0,21 36 0,16 0,06 0,46 0,27 0,24 0,20 0,20 0,10 0,10 0,10 0,25 0,13 0,25 0,25 0,25 37 0,12 0,16 0,37 0,46 0,35 0,40 0,20 0,20 0,20 0,30 0,14 0,12 0,20 0,20 0,30 39 0,68 0,63 0,68 0,67 0,56 1,00 1,00 0,60 0,50 0,40 0,64 0,57 0,60 0,50 0,40 40 0,33 0,32 0,66 0,64 0,47 0,80 1,00 0,50 0,50 0,40 0,49 0,50 0,50 0,50 0,40 all 0,24 0,23 0,48 0,47 0,44 0,59 0,62 0,52 0,53 0,48 0,38 0,36 0,37 0,36 0,34 Table <ref type="table" coords="8,163.86,564.22,4.13,7.89">2</ref>. Comparison of evaluation measures for our runs (r -reranked; b -baseline) with median run. rerank the result. Overall, the reranking gives little but noticeable improvement. TREC PM 2020 contains a Treatment tag in queries, whereas this feature was missing in the TREC PM 2019 track.</p><p>Specifically, that our reranking approach outperforms original ranking in general measures -infAP, infNDCG, bpref and R-prec. The reranked list however, performs worse in a head of the list, as it is shown by P@5 and P@10 metrics.Our results are significantly better than the median. Due to a computing power limitations we adopted serious decrease of a size of a system.</p><p>Upon overcoming these limitations we expect some improvement of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Differences between Runs</head><p>In this appendix we highlight differences between our runs and Trec median with colours -in general green means that we achieved an improvement, red means that results are worse and yellow means that there is no significant difference. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,180.56,631.09,254.24,7.89;2,134.77,399.92,376.40,216.40"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General retrieval architecture used in this contribution.</figDesc><graphic coords="2,134.77,399.92,376.40,216.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,189.69,583.01,235.97,7.89;6,143.88,187.84,327.60,380.40"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Neural layer architecture used in this contribution.</figDesc><graphic coords="6,143.88,187.84,327.60,380.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,134.77,584.82,345.82,7.89;11,134.77,595.80,157.56,7.86;11,134.77,198.23,361.69,371.81"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. This table shows the differences between runs and Trec median. r -reranked run; b -baseline run; m -Trec median.</figDesc><graphic coords="11,134.77,198.23,361.69,371.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,185.72,237.11,243.91,200.49"><head>Table 1 .</head><label>1</label><figDesc>Terrier results for the Trec Covid benchmark.</figDesc><table coords="4,185.72,237.11,243.91,189.58"><row><cell cols="3">PRF parameters: d = documents; t = terms</cell></row><row><cell></cell><cell>d=30</cell><cell>d=10</cell></row><row><cell>model</cell><cell cols="2">t=1000 t=300 t=100 t=1000 t=300 t=100</cell></row><row><cell>DPH</cell><cell cols="2">0.1565 0.1602 0.1636 0.1521 0.1592 0.1580</cell></row><row><cell>BM25</cell><cell cols="2">0.1745 0.1743 0.1722 0.1724 0.1711 0.1734</cell></row><row><cell>PL2</cell><cell cols="2">0.1717 0.1716 0.1707 0.1592 0.1654 0.1624</cell></row><row><cell>DFR BM25</cell><cell cols="2">0.1815 0.1815 0.1819 0.1802 0.1798 0.1782</cell></row><row><cell>BB2</cell><cell cols="2">0.1710 0.1719 0.1721 0.1750 1.752 0.1764</cell></row><row><cell>DLH</cell><cell cols="2">0.1642 0.1640 0.1616 0.1652 0.1640 0.1638</cell></row><row><cell>DLH13</cell><cell cols="2">0.1744 0.1770 0.1787 0.1704 0.1724 0.1731</cell></row><row><cell>DFRee</cell><cell cols="2">0.1441 0.1507 0.1464 0.1452 0.1462 0.1471</cell></row><row><cell>IFB2</cell><cell cols="2">0.1751 0.1756 0.1772 0.1747 0.1772 0.1791</cell></row><row><cell>In expB2</cell><cell cols="2">0.1760 0.1762 0.1760 0.1758 0.1748 0.1732</cell></row><row><cell>In expC2</cell><cell cols="2">0.1764 0.1766 0.1766 0.1762 0.1750 0.1740</cell></row><row><cell>InL2</cell><cell cols="2">0.1784 0.1787 0.1796 0.1774 0.1722 0.1712</cell></row><row><cell cols="3">LemurTF IDF 0.1556 0.1558 0.1556 0.1556 0.1556 0.1558</cell></row><row><cell>LGD</cell><cell cols="2">0.1515 0.1570 0.1532 0.1535 0.1542 0.1552</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,656.80,153.39,7.86"><p>https://github.com/dudenzz/myindex</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,272.08,337.64,7.86;9,151.52,283.04,329.07,7.86;9,151.52,293.98,329.07,7.89;9,151.52,304.96,168.05,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,334.27,272.08,146.32,7.86;9,151.52,283.04,329.07,7.86">Baseline and extensions approach to information retrieval of complex medical data: Poznan&apos;s approach to the biocaddie</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cieslewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dutkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jedrzejek</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/bax103</idno>
		<ptr target="https://doi.org/10.1093/database/bax103" />
	</analytic>
	<monogr>
		<title level="j" coord="9,176.48,294.00,37.30,7.86">Database</title>
		<imprint>
			<biblScope unit="page">103</biblScope>
			<date type="published" when="2016">2016. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,316.08,337.64,7.86;9,151.52,327.04,329.07,7.86;9,151.52,338.00,329.07,7.86;9,151.52,348.96,329.07,7.86;9,151.52,359.92,329.07,7.86;9,151.52,370.88,233.48,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,343.24,316.08,137.36,7.86;9,151.52,327.04,118.68,7.86">POZNAN contribution to TREC PM 2018 (notebook paper)</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cieslewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dutkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jedrzejek</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec27/papers/Poznan-PM.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,445.37,327.04,35.23,7.86;9,151.52,338.00,226.76,7.86">Proceedings of the Twenty-Seventh Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>the Twenty-Seventh Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">November 14-16, 2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>TREC</note>
</biblStruct>

<biblStruct coords="9,142.96,382.00,337.63,7.86;9,151.52,392.96,329.07,7.86;9,151.52,403.92,329.07,7.86;9,151.52,414.88,329.07,7.86;9,151.52,425.84,329.07,7.86;9,151.52,436.80,232.96,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,374.07,382.00,106.52,7.86;9,151.52,392.96,44.16,7.86">Poznan contribution to TREC-PM</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cieslewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dutkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jedrzejek</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec28/papers/Poznan.PM.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,415.75,392.96,64.85,7.86;9,151.52,403.92,211.06,7.86">Proceedings of the Twenty-Eighth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>the Twenty-Eighth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-13">2019. November 13-15, 2019. 2019</date>
			<biblScope unit="volume">2019</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>TREC</note>
</biblStruct>

<biblStruct coords="9,142.96,447.92,337.63,7.86;9,151.52,458.85,329.06,7.89" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,230.79,447.92,249.80,7.86;9,151.52,458.88,60.11,7.86">Deeper text understanding for IR with contextual neural language modeling</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno>CoRR abs/1905.09217</idno>
		<ptr target="http://arxiv.org/abs/1905.09217" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,470.00,337.64,7.86;9,151.52,480.96,329.07,7.86;9,151.52,491.92,329.07,7.86;9,151.52,502.88,329.07,7.86;9,151.52,513.84,329.07,7.86;9,151.52,524.80,166.72,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,317.50,470.00,163.09,7.86;9,151.52,480.96,145.15,7.86">Convolutional neural networks for softmatching n-grams in ad-hoc search</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3159652.3159659</idno>
		<ptr target="https://doi.org/10.1145/3159652.3159659" />
	</analytic>
	<monogr>
		<title level="m" coord="9,193.09,491.92,287.51,7.86;9,151.52,502.88,101.77,7.86">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining<address><addrLine>Marina Del Rey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-02-05">2018. February 5-9, 2018. 2018</date>
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,535.92,337.64,7.86;9,151.52,546.88,329.07,7.86;9,151.52,557.84,329.07,7.86;9,151.52,568.80,329.07,7.86;9,151.52,579.76,329.07,7.86;9,151.52,590.72,238.27,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,417.10,535.92,63.49,7.86;9,151.52,546.88,114.86,7.86">PUT contribution to TREC CDS 2016</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dutkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jedrzejek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Frackowiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Werda</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec25/papers/IAIIPUT-CL.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,445.37,546.88,35.22,7.86;9,151.52,557.84,223.98,7.86">Proceedings of The Twenty-Fifth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>The Twenty-Fifth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">November 15-18, 2016. 2016</date>
			<biblScope unit="volume">2016</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>TREC</note>
</biblStruct>

<biblStruct coords="9,142.96,601.84,337.63,7.86;9,151.52,612.77,303.55,7.89" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno>CoRR abs/1711.08611</idno>
		<ptr target="http://arxiv.org/abs/1711.08611" />
		<title level="m" coord="9,303.42,601.84,177.18,7.86;9,151.52,612.80,32.07,7.86">A deep relevance matching model for ad-hoc retrieval</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,623.92,337.64,7.86;9,151.52,634.88,329.07,7.86;9,151.52,645.81,329.07,7.89;9,151.52,656.80,171.58,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,198.40,634.88,261.46,7.86">A deep look into neural ranking models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2019.102067</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2019.102067" />
	</analytic>
	<monogr>
		<title level="j" coord="9,466.77,634.88,13.82,7.86;9,151.52,645.84,65.00,7.86">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">102067</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,119.67,337.64,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,329.07,7.86;10,151.52,152.55,329.07,7.86;10,151.52,163.51,175.32,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,260.94,119.67,219.65,7.86;10,151.52,130.63,27.03,7.86">Distributed representations of sentences and documents</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/le14.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,207.45,130.63,273.14,7.86;10,151.52,141.59,95.96,7.86;10,421.56,141.59,59.03,7.86;10,151.52,152.55,140.26,7.86">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014. 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct coords="10,142.62,174.47,337.97,7.86;10,151.52,185.43,306.13,7.86;10,134.77,196.39,11.78,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,378.30,174.47,102.29,7.86;10,151.52,185.43,131.47,7.86">From puppy to maturity: Experiences in developing terrier</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,289.53,185.43,97.48,7.86">Proc. of OSIR at SIGIR</title>
		<meeting>of OSIR at SIGIR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,151.52,196.39,329.07,7.86;10,151.52,207.34,329.07,7.86;10,151.52,218.30,329.07,7.86;10,151.52,229.26,329.07,7.86;10,151.52,240.22,329.07,7.86;10,151.52,251.18,312.00,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,224.29,207.34,94.62,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need" />
	</analytic>
	<monogr>
		<title level="m" coord="10,442.65,218.30,37.94,7.86;10,151.52,229.26,329.07,7.86;10,151.52,240.22,109.52,7.86">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,259.87,337.97,10.13;10,151.52,273.10,243.51,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,245.14,262.14,231.32,7.86">Software framework for topic modelling with large corpora</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Řehůřek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<idno type="DOI">10.13140/2.1.2393.1847</idno>
		<ptr target="https://doi.org/10.13140/2.1.2393.1847" />
		<imprint>
			<date type="published" when="2010-05">05 2010</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
