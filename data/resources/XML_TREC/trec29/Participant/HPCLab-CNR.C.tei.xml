<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,229.38,116.95,156.60,12.62;1,188.45,134.89,238.45,12.62;1,160.94,154.73,293.48,10.86">Topical Enrichment of Conversational Search Utterances Participation of the HPCLab-CNR Team in CAsT 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.48,190.76,38.19,8.74"><forename type="first">Ida</forename><surname>Mele</surname></persName>
							<email>ida.mele@iasi.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="institution">IASI-CNR</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.22,190.76,104.42,8.74"><forename type="first">Cristina</forename><forename type="middle">Ioana</forename><surname>Muntean</surname></persName>
							<email>cristina.muntean@isti.cnr.it</email>
							<affiliation key="aff1">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.49,202.71,94.87,8.74"><forename type="first">Franco</forename><forename type="middle">Maria</forename><surname>Nardini</surname></persName>
							<email>francomaria.nardini@isti.cnr.it</email>
							<affiliation key="aff1">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.91,202.71,67.28,8.74"><forename type="first">Raffaele</forename><surname>Perego</surname></persName>
							<email>raffaele.perego@isti.cnr.it</email>
							<affiliation key="aff1">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.13,202.71,75.27,8.74"><forename type="first">Nicola</forename><surname>Tonellotto</surname></persName>
							<email>nicola.tonellotto@unipi.it</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,229.38,116.95,156.60,12.62;1,188.45,134.89,238.45,12.62;1,160.94,154.73,293.48,10.86">Topical Enrichment of Conversational Search Utterances Participation of the HPCLab-CNR Team in CAsT 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E0641B81429BBA97E4A34803B132B528</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Conversational Assistant Track (CAsT) provides test collections for open-domain conversational search systems with the purpose of pursuing research on Conversational Information Seeking (CIS). For our participation in CAsT 2020, we implemented a modular architecture consisting of three steps: (i) automatic utterance rewriting, (ii) first-stage retrieval of candidate passages, and (iii) neural re-ranking of candidate passages. Each run is based on a different utterance rewriting technique for enriching the raw utterance with context extracted from the previous utterances in the conversation. Two of our approaches are completely unsupervised, while the other two rely on utterances manually classified by human assessors. These approaches also employ the canonical responses for the automatically rewritten utterances provided by CAsT 2020.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conversational Information Seeking (CIS) has recently gained interest due to the popularity of conversational assistant systems and to the recent advances in automatic speech recognition and understanding. Conversational assistant systems help users in a wide range of activities such as checking the weather forecast, managing music streaming services, or performing e-commerce transactions. They are used in chatbots and smart home devices (e.g., Google Home, Amazon Alexa) as well as in wearable devices and smartphones (e.g., Apple Siri, Google Assistant, Microsoft Cortana).</p><p>Although conversational assistants are good at performing simple well-defined actions, their ability to support conversational information seeking is still very limited. Indeed, the seeking of information evolves as a dialogue between the user and the system, and the search goes on as turns of user natural-language questions, i.e., utterances. The retrieval of documents relevant to an utterance is difficult due to the informal use of natural language in the speech and the lacking of context. Moreover, adding context to ambiguous utterances is tricky due to the complexity of understanding the semantic meaning of previous utterances and their answers.</p><p>Thanks to CAsT data, researchers can experiment with their methodologies that aim to improve the automatic understanding of the users' information needs and to find the relevant responses using contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The TREC Conversational Assistant Track<ref type="foot" coords="2,326.88,193.44,3.97,6.12" target="#foot_0">4</ref> (CAsT) 2020 provided a dataset including search conversations and document collections. Compared to last year (CAsT 2019), CAsT 2020 is based on two collections: (1) TREC CAR (Complex Answer Retrieval) containing ∼ 29M of passages extracted from approximately 5M Wikipedia articles, and (2) MS-MARCO (MAchine Reading COmprensation) made of ∼ 8M passages from answer candidates of the Bing search engine.</p><p>CAsT 2020 dataset provides 25 conversations, each having from 6 to 13 utterances for a total of 216 utterances. Compared to CAsT 2019, the CAsT 2020 conversations do not include topic titles or descriptions, so topics unfold throughout the dialogue. Also, CAsT 2020 dataset provides a canonical system response of the previous turn utterance.</p><p>An example of conversation is as follows: (1) "How do you make Japanese Yakiniku?", (2) "Can the sauce be used in other dishes?", (3) "What are the best Yakiniku restaurants in Tokyo?", and (4) "Tell me about three star Michelin sushi restaurants there". While the first and third utterances are relatively easy to process by an Information Retrieval (IR) system, in the second utterance there is a reference to the subject of the conversation, i.e., Japanese Yakiniku, and the fourth utterance refers to a specific location, i.e., Tokyo, just introduced in the conversation. Even in this short piece of a conversation, it is possible to identify different kinds of utterances. The first and third utterances do not require any rewriting to be successfully answered. On the other hand, the second utterance lacks context and adding the keywords "Japanese Yakiniku" is mandatory for retrieving relevant documents. We also observe a topic shift in the third utterance, i.e., Japanese Yakiniku → Tokyo, that makes the fourth utterance referring to a newly introduced topic (Tokyo) and not to the previous one (Japanese Yakiniku). Even in this case, the utterance needs to be rewritten and enriched with the keyword "Tokyo" to be successfully answered by an automatic IR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search Conversations</head><p>By carefully inspecting the utterances in the CAsT 2020 dataset, we noticed some common patterns in the conversations: (a) Some utterances are self-explanatory as they do not lack context. In particular, the first utterance of each conversation is self-explanatory, but sometimes we can observe them in the middle of the conversation. Very often self-explanatory utterances in the middle of the conversation introduce a subtopic exploration or even a topic shift.</p><p>(b) In some cases, the topic of the first utterance dominates the conversation. Follow-up utterances are not self-explanatory and refer to the topic introduced at the beginning of the conversation. These utterances depend on the first topic of the conversation. (c) In other cases, utterances are not self-explanatory and refer to some topics mentioned in a previous utterance (different from the first utterance). Hence, these utterances need to be enriched with some context extracted from the previous utterances in the conversation. (d) Similarly to (c), the utterances are not self-explanatory and refer to some topics mentioned in previous utterances and/or in their answers. These utterances are even more tricky as they cannot be enriched with context extracted from only the previous utterances but rather from their results. We will refer to these utterances as depending on previous responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Utterance Labeling</head><p>Given our previous observations, we asked human assessors to manual label raw utterances. In particular, assessors familiar with the challenges in conversational search evaluated 216 utterances from 25 conversations using four labels:</p><p>• Self-Explanatory (SE): the utterance is self-explanatory, so the context is fully provided; • First Topic (F T ): the utterance misses context which depends on the first utterance; • Previous Topic (P T ): the utterance misses context which depends on the previous utterance; • Previous Response (P R): the utterance misses context which depends on the previous utterance as well as its canonical response.</p><p>For most of the utterances (i.e., 162 out of 216) we could see a full agreement among the assessors. To measure the inter-annotator agreement, we also computed the Fleiss' Kappa <ref type="bibr" coords="3,261.72,475.73,10.52,8.74" target="#b1">[2]</ref> that gives a measure of how consistent are the assessors' ratings. It is computed as κ = P -Pe 1-Pe , where 1 -Pe gives the degree of agreement that is attainable above chance, and P -Pe gives the degree of agreement actually achieved above chance. If κ = 1 the agreement is complete, while κ ≤ 0 means no agreement. We registered a value of 0.82 which, according to the table for interpreting κ values provided in <ref type="bibr" coords="3,349.68,538.97,9.96,8.74" target="#b2">[3]</ref>, corresponds to an "almost perfect" agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodologies</head><p>Our framework consists of three steps: (1) utterance rewriting, (2) candidate passage retrieval, and (3) neural re-ranking.</p><p>All our methods employ a Python NLP toolkit for extracting various linguistic features from the utterances 5 and perform utterance rewriting to enrich the raw utterance with the missing context. After utterance rewriting, in the first-stage retrieval, we use the rewritten utterances to retrieve the candidate passages and narrow down the search space. Then, neural re-ranking exploits a contextualized language model based on BERT to re-rank the passages <ref type="bibr" coords="4,448.17,155.86,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Utterance Rewriting</head><p>We assume that a user has an information need that intends to fulfill by issuing utterances to a conversational IR system. A raw utterance, u i , represents the natural language question issued by the user to the system. This is the input of our automatic utterance rewriting module whose output is an enriched utterance, ûi , used to retrieve candidate passages from the document collection. The purpose of the utterance rewriting module is to add the missing context to the raw utterance so that the user can get a good response to her request.</p><p>Runs with Unsupervised Utterance Rewriting. These runs are inspired by our work on investigating topic propagation in multi-turn conversations <ref type="bibr" coords="4,467.30,312.64,9.96,8.74" target="#b3">[4]</ref>. They use the raw utterances from the previous turns of the conversation.</p><p>-HPCLab-CNR-run2 rewrites the raw utterance by using the topics extracted from all the raw utterances of the previous turns. These topics are noun chunks, different from pronouns, which are either subjects or objects, while verbs are not used. The drawback of this approach is that propagating all the context seen during the conversation can lead to noisy results, especially for those conversations where the focus of interest may change. -HPCLab-CNR-run4 tries to address the main weakness of run2 as it rewrites the utterance by using the topics extracted from two main sources: (i) the raw utterance of the previous turn (previous topic); (ii) the raw utterance that has introduced a topic shift (focus topic). To detect the possible topic shifts, our approach employs some clue phrases (e.g., "tell me about", "what about"). Namely, at the beginning of the conversation, the focus topic is represented by the topic extracted from the first utterance, and when a topic shift occurs the focus topic is updated with the topic extracted from the utterance where the topic shift is detected. The topics in run4 are noun chunks (objects or subjects) plus the full verbs.</p><p>Runs with Utterance Rewriting based on Classification. These runs perform the automatic rewriting of raw utterances using the utterance classification explained in Section 2.2. The classification is used to determine the best enrichment for the current utterance. In particular:</p><p>• If the raw utterance is labeled as SE, no rewriting is applied.</p><p>• If the raw utterance is labeled as F T , it is enriched with the topic extracted from the first utterance of the conversation. • When the utterance label is P T , the rewriting is performed using the topic extracted from the previous enriched utterance.</p><p>• When the label is P R, the utterance is rewritten using the topic extracted from the previous enriched utterance. Plus, the context (e.g., topics or keywords) from the canonical response of the previous automatically rewritten utterance is added at the end of the enriched utterance.</p><p>These runs differ from each other because they extract the topics from utterances and the context from canonical responses in different ways:</p><p>-HPCLab-CNR-run1 extracts the topics from the enriched utterance using the noun chunks (objects or subjects). The context from the canonical response for the automatically rewritten utterance of the previous turn (provided by CAsT 2020) is represented by the keywords of the canonical response (after stopword removal). -HPCLab-CNR-run3, topics are the noun chunks (objects or subjects) that are recognized as named entities by TagMe<ref type="foot" coords="5,326.08,272.05,3.97,6.12" target="#foot_2">6</ref> (with threshold = 0.1). They are extracted from both the enriched utterances and their canonical responses. Using only named entities has the advantage to clean a noisy context, although, in some cases, the set of recognized named entities can be empty which may lead to poor context enrichment.</p><p>Compared to run2 and run4, these approaches are based on manual labels, they use context from canonical responses (when needed), and they extract topics from the enriched utterances of the previous turn. As we will see in Section 5, the approaches implemented for run1 and run3 perform better compared to the completely unsupervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setting</head><p>Metrics. The effectiveness of the rewriting techniques is evaluated with traditional TREC metrics. In particular, Mean Average Precision (map) and normalized Discounted Cumulative Gain (nDCG) for cutoffs at 3, 5, and 1K. The use of small cutoffs, such as 3 and 5, is common for the conversational search task since the user expects to receive one crisp answer rather than a long list of potentially relevant results.</p><p>First-stage retrieval. For indexing and querying the CAsT dataset, we used Indri<ref type="foot" coords="5,156.10,525.85,3.97,6.12" target="#foot_3">7</ref> . We indexed the two datasets by removing stopwords and we applied the Krovetz stemmer for stemming. As Indri querying method we used the Indri language model based on Dirichlet smoothing with parameter µ = 2500. We also applied pseudo-relevance feedback (PRF) based on the RM3 algorithm <ref type="bibr" coords="5,470.08,563.29,10.52,8.74" target="#b0">[1]</ref> using 20 keywords taken from the top 20 results and γ = 0.5.</p><p>Neural re-ranking. We used the model by Nogueira and Cho <ref type="bibr" coords="5,407.85,592.87,10.52,8.74" target="#b4">[5]</ref> to re-rank the results from the previous stage. The model fine-tunes the BERT base pre-trained model for re-ranking on the MS-MARCO passage retrieval dataset. For each query, Indri retrieves 1K results which are the input for the re-ranking step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In Table <ref type="table" coords="6,176.50,146.75,3.87,8.74" target="#tab_0">1</ref>, we report the values of the following metrics map@1K and nDCG@k (with k = 3, 5, and 1K) for our four runs. As we can see, the worst results are achieved by run2 and run4 as they do not use any utterance classification and any context from the canonical response for the previous utterance.</p><p>Better performances are achieved by run1 and run3 as they enrich the raw utterances leveraging the utterance classification and add the context extracted from the previous canonical response.</p><p>CAsT 2020 also provided for each query/utterance the worst, median, and best performance for 35 raw runs, 8 canonical runs, and 12 manual runs. We computed the average over all the queries, and the results are shown in Table <ref type="table" coords="6,472.84,255.49,3.87,8.74" target="#tab_1">2</ref>.</p><p>As expected, the performances of the two unsupervised runs (run2 and run4) are close to the raw median values reported in Table <ref type="table" coords="6,358.05,279.97,3.87,8.74" target="#tab_1">2</ref>. While the performances of run1 and run3 are close to the canonical median values. Moreover, the nDCG@3 values achieved with run1 and run3 are 0.313 and 0.331, respectively. They are slightly better than the CAsT 2020 BERT baseline for automatic-canonical results (nDCG@3 = 0.3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Utterance Rewriting Limits</head><p>By inspecting the rewritten utterances, we could notice some limitations of our automatic approaches. First of all, we need to replace acronyms to have a better understanding of the utterance, e.g., in "Why did it replace PB?" → PB refers to a variety of oranges Parson Brown. Also, some topic shifts are hard to find and simple clues do not help much. For example, consider this part of a conversation (1) "What is the climate like in Utah?", (2) "How does Salt Lake City differ?", (3) "What is its main economic activity?" → its refers to Salt Lake City rather than Utah, but this topic shift may be undetected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this report, we have presented the methodologies implemented for our participation in CAsT 2020.</p><p>As future work, we plan to develop an improved rewriting approach that automatically replaces acronyms (e.g., PB with Parson Brown). Also, we will improve our function for detecting topic shifts and extracting context from canonical responses. Lastly, we would like to experiment with automatic classifications of utterances where classifiers can be trained on a set of manually labeled utterances to predict the labels of future ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,160.89,360.98,281.29,79.19"><head>Table 1 .</head><label>1</label><figDesc>Performance of our runs</figDesc><table coords="6,160.89,383.20,281.29,56.98"><row><cell>Run</cell><cell>nDCG@3</cell><cell>nDCG@5</cell><cell>nDCG@1K</cell><cell>map@1K</cell></row><row><cell>Run1 (canonical)</cell><cell>0.313</cell><cell>0.304</cell><cell>0.403</cell><cell>0.220</cell></row><row><cell>Run2 (raw)</cell><cell>0.275</cell><cell>0.270</cell><cell>0.360</cell><cell>0.185</cell></row><row><cell>Run3 (canonical)</cell><cell>0.331</cell><cell>0.319</cell><cell>0.422</cell><cell>0.236</cell></row><row><cell>Run4 (raw)</cell><cell>0.292</cell><cell>0.275</cell><cell>0.358</cell><cell>0.194</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,152.21,491.13,301.72,145.48"><head>Table 2 .</head><label>2</label><figDesc>Performance of CAsT 2020 runs: averaged over all queries</figDesc><table coords="6,152.21,515.08,301.72,121.53"><row><cell>Run</cell><cell></cell><cell>nDCG@3</cell><cell>nDCG@5</cell><cell>nDCG@1K</cell><cell>map@1K</cell></row><row><cell></cell><cell>worst</cell><cell>0.006</cell><cell>0.007</cell><cell>0.046</cell><cell>0.006</cell></row><row><cell>Raw</cell><cell>median</cell><cell>0.279</cell><cell>0.273</cell><cell>0.375</cell><cell>0.180</cell></row><row><cell></cell><cell>best</cell><cell>0.733</cell><cell>0.687</cell><cell>0.710</cell><cell>0.492</cell></row><row><cell></cell><cell>worst</cell><cell>0.058</cell><cell>0.066</cell><cell>0.142</cell><cell>0.057</cell></row><row><cell cols="2">Canonical median</cell><cell>0.337</cell><cell>0.328</cell><cell>0.426</cell><cell>0.233</cell></row><row><cell></cell><cell>best</cell><cell>0.634</cell><cell>0.606</cell><cell>0.657</cell><cell>0.440</cell></row><row><cell></cell><cell>worst</cell><cell>0.016</cell><cell>0.023</cell><cell>0.174</cell><cell>0.029</cell></row><row><cell>Manual</cell><cell>median</cell><cell>0.414</cell><cell>0.398</cell><cell>0.489</cell><cell>0.263</cell></row><row><cell></cell><cell>best</cell><cell>0.724</cell><cell>0.683</cell><cell>0.698</cell><cell>0.478</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,657.79,105.03,7.86"><p>http://www.trecCAsT.ai/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,144.73,657.79,307.77,8.37"><p>spacy library available at https://spacy.io/usage/linguistic-features.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="5,144.73,646.84,155.74,7.86"><p>https://pypi.org/project/tagme/0.1.2/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="5,144.73,657.79,163.21,7.86"><p>https://www.lemurproject.org/indri.php</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,418.55,342.24,7.86;7,146.91,429.51,216.37,7.86" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<title level="m" coord="7,342.69,418.55,137.90,7.86;7,146.91,429.51,138.61,7.86">Information Retrieval: Implementing and Evaluating Search Engines</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,440.47,342.24,7.86;7,146.91,451.43,119.94,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,194.45,440.47,226.87,7.86">Measuring Nominal Scale Agreement among Many Raters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,428.01,440.47,52.58,7.86;7,146.91,451.43,30.62,7.86">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,462.39,342.24,7.86;7,146.91,473.35,187.78,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,271.84,462.39,208.76,7.86;7,146.91,473.35,47.63,7.86">The Measurement of Observer Agreement for Categorical Data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,203.11,473.35,41.90,7.86">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,484.31,342.24,7.86;7,146.91,495.27,333.68,7.86;7,146.91,506.22,48.38,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,146.91,495.27,181.52,7.86">Topic Propagation in Conversational Search</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">I</forename><surname>Muntean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,353.60,495.27,47.75,7.86">SIGIR 2020</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2057" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,517.18,342.24,7.86;7,146.91,528.14,97.10,7.86" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="7,270.20,517.18,132.91,7.86">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
