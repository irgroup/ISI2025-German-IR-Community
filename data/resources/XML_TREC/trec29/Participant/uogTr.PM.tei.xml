<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.06,84.23,415.89,15.44;1,162.62,104.15,286.74,15.44">University of Glasgow Terrier Team and UFMG at the TREC 2020 Precision Medicine Track</title>
				<funder ref="#_vApqFnt">
					<orgName type="full">Coordena√ß√£o de Aperfei√ßoamento de Pessoal de N√≠vel Superior, Brazil (Capes)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,109.69,130.33,64.90,10.59"><forename type="first">Alberto</forename><surname>Ueda</surname></persName>
							<email>ueda@dcc.ufmg.br</email>
							<affiliation key="aff0">
								<orgName type="institution">UFMG</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.95,130.33,101.09,10.59"><forename type="first">Rodrygo</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
							<email>rodrygo@dcc.ufmg.br</email>
							<affiliation key="aff1">
								<orgName type="institution">UFMG</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.20,130.33,80.66,10.59"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,488.49,130.33,53.61,10.59"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.06,84.23,415.89,15.44;1,162.62,104.15,286.74,15.44">University of Glasgow Terrier Team and UFMG at the TREC 2020 Precision Medicine Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DE48F11D6D42CD3660DBC65F07C975B5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For TREC 2020, we explore different re-ranking strategies by integrating PyTerrier -a framework which allows us to build advanced retrieval pipelines -with state-of-the-art BERT-based contextual language models. To produce the initial ranking, we use traditional retrieval models, such as Divergence From Randomness (DFR) weighting models. Then, we assign new scores for each document with BERT-based models, such as SciBERT and ColBERT. Finally, we re-rank the documents using combinations of those scores, via linear combination or learning-to-rank. We also conduct experiments with models able to leverage the structure information of the documents, i.e., by assigning different scores for their individual sections (e.g., Background, Results, Conclusions). We submitted five official runs, uog_ufmg_DFRee, uog_ufmg_s_dfr0, uog_ufmg_s_dfr5, uog_ufmg_sb_df5, uog_ufmg_secL2R. Our results in TREC 2020 confirm our main observations in our tests using the TREC 2019 test collection, showing that re-ranking strategies such as simple linear combinations of DFR and SciBERT scores (uog_ufmg_sb_df5) are competitive and outperform the TREC median performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In our first participation in the TREC Precision Medicine track, we opt to evaluate the effectiveness of traditional retrieval models integrated with state-of-the-art contextual language models, in the domain of medical literature. Inspired by our participation in the TREC Deep Learning Track <ref type="bibr" coords="1,154.96,467.13,13.40,7.94" target="#b15">[15,</ref><ref type="bibr" coords="1,170.60,467.13,10.05,7.94" target="#b16">16]</ref>, we leverage our existing team infrastructure, while enhancing it with the integration of new deep learning techniques -some of them specific to scientific literature, such as the SciBERT pre-trained model. In particular, we use PyTerrier <ref type="bibr" coords="1,69.70,510.96,13.49,7.94" target="#b11">[11]</ref>, <ref type="foot" coords="1,86.56,508.81,3.38,6.44" target="#foot_0">1</ref> a Python framework which allows advanced retrieval pipelines to be expressed, and evaluated, in a declarative manner similar to their conceptual design. We integrate PyTerrier with a toolkit which allows us to straightforwardly handle BERT-based models (CEDR <ref type="bibr" coords="1,107.86,554.80,9.27,7.94" target="#b8">[9]</ref>, described in Section 4) and then conduct several experiments on the application of different standard IR models (e.g., BM25, DPH, DFRee) and BERT-based models (e.g., BERT ùê∂ùêøùëÜ <ref type="bibr" coords="1,278.24,576.72,13.43,7.94" target="#b12">[12]</ref>, SciBERT <ref type="bibr" coords="1,86.92,587.68,9.27,7.94" target="#b2">[3]</ref>, CEDR_pacrr <ref type="bibr" coords="1,148.81,587.68,9.27,7.94" target="#b8">[9]</ref>, and the more recently proposed Col-BERT <ref type="bibr" coords="1,76.13,598.63,8.98,7.94" target="#b4">[5]</ref>). To evaluate the performance of those models and choose which of them would be submitted, we use the datasets provided by previous editions of the track, namely, TREC-PM 2017-2019 <ref type="bibr" coords="1,262.06,620.55,14.91,7.94" target="#b13">[13]</ref>. We combine the different scores given by each model using linear combinations and the LightGBM implementation of LambdaMART. <ref type="foot" coords="1,290.17,640.32,3.38,6.44" target="#foot_1">2</ref>Moreover, we conduct initial experiments using models that leverage the inherited structure of biomedical abstracts, i.e., the presence of named sections such as Background, Results, and Conclusions.</p><p>The remainder of this paper is structured as follows: Section 2 describes our settings for indexing the TREC-PM 2020 data; Section 3 discusses the models used to produce the initial candidate set of documents, i.e., the first-phase retrieval, while in Section 4 we show detailed information about the contextual language models used in the second-phase retrieval; Section 5 provides the full list of the official submissions we made to TREC and the strategies used in each submission; Section 6 and Section 7 discuss our results and provide concluding remarks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INDEXING</head><p>To index the 29,138,919 documents (i.e., biomedical abstracts) of the Pubmed/MEDLINE 2019 Baseline, we use Terrier <ref type="bibr" coords="1,510.63,310.66,14.65,7.94" target="#b9">[10]</ref> v5.3. For each document, we index only its identifier (tag PMID), title (tag articletitle) and text (tag abstracttext). We initially also indexed its MeSH Terms (found in tags such as descriptorname inside a (meshheadlinglist)), however, in our experiments they did not improve the first-phase nor the second-phase retrieval, hence they were discarded. For all documents, we identify named sections such as Background and Results (e.g., NlmCategory="BACKGROUND" inside an abstracttext), and we index them as Terrier fields. We discard all the other information from the original corpus.</p><p>We use the following indexing configurations:</p><p>‚Ä¢ Positions: to record the positions that terms occur in;</p><p>‚Ä¢ Fields: to record the frequencies of terms occurring in different parts of the document: abstract title, text, and named sections (as described previously); ‚Ä¢ Stemming &amp; Stopwords: to improve retrieval recall and precision, we applied Porter's stemmer and Terrier standard stopwords removal at indexing time.</p><p>We apply the classical two-pass indexing of Terrier<ref type="foot" coords="1,519.48,511.49,3.38,6.44" target="#foot_2">3</ref> to create both a direct and an inverted index of the corpus, to support query expansion and other retrieval techniques. We also record the raw text of the document titles and content as metadata, to allow deep learning upon these textual representations, as discussed further in Section 4. In the next section, we describe how we use that index to create the initial candidate set of documents in our first-phase retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FIRST-PHASE RETRIEVAL</head><p>To avoid computing expensive ranking models for large collections such as Pubmed/MEDLINE, with millions of documents, we take a common approach in most modern search engines, namely the cascading architecture: where an initial ranking of ùëò documents with respect to a query ùëû is thereafter re-ranked by the application of more advanced and complex ranking techniques, such as neural rankers. The initial ranking is the result of the referred first-phase retrieval. Using PyTerrier, we employ three standard models of Information Retrieval (IR) -BM25, DPH, and DFRee -in two different versions of each of them, with/without query expansion (QE):</p><p>‚Ä¢ DFRee is a hyper-parameter-free implementation of the Divergence From Randomness hyper-geometric weighting model <ref type="bibr" coords="2,103.03,168.11,9.50,7.94" target="#b1">[2]</ref>; ‚Ä¢ DPH <ref type="bibr" coords="2,98.13,179.07,14.72,7.94" target="#b14">[14]</ref> is the DFR-based weighting model with Popper's normalization (the 'P' in the DPH name). DPH is the default weighting model of PyTerrier; ‚Ä¢ BM25 (i.e. Okapi BM25) is a popular weighting model to address various retrieval tasks. In contrast to DPH and DFRee, we note that the BM25 hyper-parameters can be fine-tuned, providing corpus-fitted models. However, we use the PyTerrier default parameter values in our experiments.</p><p>For each topic, the query consists of the concatenation of the contents of the disease, gene, and treatment tags provided in the TREC topics file. To expand the original queries, we used the default parameters of query expansion in PyTerrier, in which, for each query ùëû, the top ten most informative terms of the top three documents retrieved were added to ùëû with the weighting scores given by the Bo1 model <ref type="bibr" coords="2,145.31,336.10,9.52,7.94" target="#b0">[1]</ref>. Although query expansion (QE) improved the recall of BM25 and DPH, as expected, the DFRee ranking model without QE, surprisingly, outperformed all of the models considered, including those with QE. Thus, we decided to use DFRee without QE as our first-phase retrieval model in all further experiments. We also experimented with different values for the size ùëò of the initial candidate set, from 200 to 5,000 documents per query. In these experiments, we found that using ùëò = 1000 provided similar performances to higher values of ùëò in the final ranking, not only with respect to precision but also in terms of recall. Therefore, we use this setting in all experiments described in this paper. We easily deploy all the aforementioned ranking models using PyTerrier.</p><p>Next, we describe the second-phase retrieval, where we employ more tailored ranking models to assign new scores to each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SECOND-PHASE RETRIEVAL</head><p>Since it has been proposed, the BERT contextual language model <ref type="bibr" coords="2,283.62,526.15,10.43,7.94" target="#b3">[4]</ref> (LM) and many of its extensions such as ALBERT <ref type="bibr" coords="2,226.74,537.11,10.42,7.94" target="#b5">[6]</ref> and RoBERTa <ref type="bibr" coords="2,288.50,537.11,10.43,7.94" target="#b7">[8]</ref> have showed significant improvements in several natural language processing (NLP) tasks, such as question answering, named entity recognition, and passage retrieval <ref type="bibr" coords="2,175.92,569.99,9.23,7.94" target="#b8">[9,</ref><ref type="bibr" coords="2,187.10,569.99,10.05,7.94" target="#b12">12]</ref>. In particular, in the BERTbased text re-ranking approach of <ref type="bibr" coords="2,182.91,580.94,13.49,7.94" target="#b12">[12]</ref>, the query and document are concatenated with a [SEP] in between, and a [CLS] token at the end. The final relevance estimate of the document for a query is obtained from the embedding of the [CLS] token. We also use this approach in all BERT-based experiments for TREC-PM.</p><p>More recently, some variants of the BERT language model have been proposed for the scientific literature domain, such as Sci-BERT <ref type="bibr" coords="2,76.67,657.66,10.48,7.94" target="#b2">[3]</ref> and BioBERT <ref type="bibr" coords="2,139.91,657.66,9.32,7.94" target="#b6">[7]</ref>. In practice, each LM variant is focused on capturing the specific context of texts in the domain to better represent the semantics of words in a vocabulary. In this work, we apply the SciBERT LM, which we have found to be more effective in biomedical search tasks compared to BERT ùê∂ùêøùëÜ and BioBERT <ref type="bibr" coords="2,283.07,701.49,9.27,7.94" target="#b2">[3]</ref>.</p><p>Listing 1: Example how to apply a BERT-based model using PyTerrier and CEDR. In particular, in this example we create and test the SciBERT-pm, a SciBERT model fine-tuned in previous TREC-PM track editions.</p><p>1 # 1 s t -p ha s e r e t r i e v a l : DFRee 2 dfree = pt . BatchRetrieve ( index , wmodel = " DFRee " , 3 metadata =[ " docno " ," articletitle " ," abstracttext " ]) 4 5 # 2 nd -p ha s e r e t r i e v a l : SciBERT re -r a n k i n g DFRee 6 scibert_pm = dfree &gt;&gt; CEDRPipe ( modelname = " SciBERT ") To use SciBERT in our experiments, we add a simple integration between PyTerrier and the CEDR <ref type="bibr" coords="2,441.93,312.26,10.55,7.94" target="#b8">[9]</ref> toolkit, described next.</p><formula xml:id="formula_0" coords="2,319.69,191.01,50.09,15.26">7 8 # f i n e -t u</formula><p>CEDR: Contextualized Embeddings for Document Ranking. The CEDR toolkit <ref type="foot" coords="2,368.47,337.71,3.38,6.44" target="#foot_3">4</ref> allows us to use state-of-the-art BERT language models, including the original BERT ùê∂ùêøùëÜ model (with no additional fine-tuning, and known as "vanilla BERT" in CEDR) and the PACRR model <ref type="bibr" coords="2,343.97,372.74,9.52,7.94" target="#b8">[9]</ref>. Moreover, it provides us with the possibility of finetuning the BERT-based models for different corpora. We made experiments with fine-tuned versions of the BERT ùê∂ùêøùëÜ , CEDR-PACRR, and SciBERT models, but the former two were discontinued as they were observed to exhibit less effectiveness than using a fine-tuned SciBERT model, described next.</p><p>SciBERT. We implement an integration class for SciBERT in CEDR, based on the BERT ùê∂ùêøùëÜ model, differing only by the pretrained models for the vocabulary and text tokenizer which are loaded. Then, we fine-tune the SciBERT and other BERT-based models in our corpus. To fine-tune the pretrained SciBERT model, we use the 80 queries from TREC-PM 2017 and 2018 (72 for training and 8 for validation) and the top 1,000 articles retrieved by DFRee for each query, over a maximum number of epochs equal to 100 with an early-stopping of the fine-tuning process after 20 iterations without validation improvements (i.e., parameter ùëùùëéùë°ùëñùëíùëõùëêùëí = 20). For all BERT-based text re-rankings in this work, we use the simple concatenation of the article's title and abstract as the document representation. To change the resulting fine-tuned model from the original one, we name it SciBERT-pm, in contrast to the original SciBERT model. Listing 1 illustrates an example of how we create and apply the SciBERT-pm model using PyTerrier. Note that in Line 6 we use PyTerrier's operator &gt;&gt; -known as "Then" -which indicates that we are chaining two pipelines, i.e., applying one PyTerrier transformation to re-rank the results of the proceeding transformer. For instance, in this example, the scibert_pm pipeline is deployed as the SciBERT re-ranking of the initial documents retrieved by the DFRee model. However, it is noteworthy to mention that both the first-phase and second-phase retrieval will be executed only when it is necessary: for instance, in our case, in Line 9, when PyTerrier needs the results to start the fine-tuning process.</p><p>After fine-tuning the model (with scibert_pm.fit(), we may store it for reuse in other experiments later. The output of pt.Experiment() will be a table comparing the results of the DFRee and SciBERTpm retrieval pipelines, according to the given TREC qrels file, in terms of MAP (Mean Average Precision) and nDCG (normalized Discounted Cumulative Gain) .</p><p>ColBERT. We also employ ColBERT <ref type="bibr" coords="3,202.55,181.61,9.52,7.94" target="#b4">[5]</ref>, a recently proposed contextual embedding with promising results for IR ad hoc tasks. ColBERT has been shown to be faster and at least as effective as BERT ùê∂ùêøùëÜ , which was confirmed in our experiments. However, in our tests for TREC-PM, ColBERT was not as effective as our SciBERT-pm model, so the former was not used in further experiments. The reason for this disappointing performance might be related to the fine-tuning process applied to SciBERT-pm. At the time of writing this paper, we did not fine-tune ColBERT in our TREC-PM data, leaving this as future work.</p><p>Using Document Structure. This approach builds on the observation that different biomedical abstract sections -e.g., Background and Conclusions -may convey different semantics. We hypothesize that the linguistic nuances of each section can be modelled individually with SciBERT and combined to improve our ranking models. To test this hypothesis, we once again use the top 1,000 abstracts retrieved by DFRee for each of the TREC-PM 2017 and 2018 queries. However, in contrast to the SciBERT-pm model, which is fine-tuned only once using the entire abstract, we produce one fined-tuned Sci-BERT model per each identified abstract section. We denote these novel models as SciBERT-s (i.e., section-based SciBERT). Thus, besides the original SciBERT-pm score, in this approach, we also have additional SciBERT-s scores, one score for each abstract section. In our initial experiments for TREC, we assume that all biomedical abstracts inherited have four sections (namely, Background, Methods, Results, and Conclusions). For the documents without the named sections, we use logistic regression classifier 5 to determine the section of each abstract sentence, trained on 30k sentences from the corpus that were separated into section by their respective authors.</p><p>Re-Ranking. In order to leverage both standard retrieval (firstphase) and semantic (second-phase) scores, we use as simpler score aggregation alternatives, for each document ùëë and query ùëû, linear combinations between the DFRee and SciBERT-pm scores, varying the weighting parameter alpha (ùõº) from 0 to 1, as follows:</p><formula xml:id="formula_1" coords="3,71.16,571.16,204.71,10.05">ùë†ùëêùëúùëüùëí (ùëë, ùëû) = ùõº √ó ùë†ùëêùëúùëüùëí ùëö 1 (ùëë, ùëû) + (1 -ùõº) √ó ùë†ùëêùëúùëüùëí ùëö 2 (ùëë, ùëû)</formula><p>where ùëö 1 are statistic-based models -such as DFRee -and ùëö 2 are semantic-based models, such as SciBERT-pm. In particular, the combination of DFRee and SciBERT-pm was the best we found in our experiments based on the past editions of TREC-PM, hence we focus on this combination in our TREC submissions this year. In general, our experiments showed that these combinations are consistently better than the models evaluated separately, also considering other statistic-based models, such as BM25 and DPH, and other semantic-based models, such as PACRR and ColBERT. We 5 Namely, SKLearn's LogisticClassifier: https://scikit-learn.org/stable/modules/ generated/sklearn.linear_model.LogisticRegression.html aim to investigate and explain the reasons for such a behavior in future studies.</p><p>For our structure-based experiments, in which we have more than two features as input -given by the section-based modelswe use a listwise learning-to-rank (LTR) method to combine the features. Specifically, we adopt the LightGBM 3.0<ref type="foot" coords="3,494.87,140.44,3.38,6.44" target="#foot_4">6</ref> implementation of LambdaMART. To train the LTR model, we use 80 queries (TREC-PM 2017 and 2018), of which the first 72 queries we use as training set and the last 8 queries as our validation set. For the configuration settings, we use the default parameter values of LightGBM ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SUBMITTED RUNS</head><p>We submitted the following five runs to the TREC 2020 Precision Medicine Track:</p><p>‚Ä¢ uog_ufmg_DFRee: this run is the result of the first-phase retrieval with PyTerrier's batch retrieval with DFRee, as detailed in Section 3; ‚Ä¢ uog_ufmg_s_dfr0: this run is the result of the second-phase retrieval with the SciBERT-pm re-ranking, using DFRee as the initial ranking model (c.f. uog_ufmg_DFRee), as detailed in Section 4; ‚Ä¢ uog_ufmg_s_dfr5: this run is the linear combination (ùõº = 0.5) of the scores of DFRee and SciBERT-pm (uog_ufmg_DFRee and uog_ufmg_s_dfr0, respectively); ‚Ä¢ uog_ufmg_sb_df5: this run is the result of the same methodology used in the previous uog_ufmg_s_dfr5 run, but with a different query tokenization; ‚Ä¢ uog_ufmg_secL2R: this run is the result of the second-phase retrieval with the structured-based SciBERT-s re-ranking, as detailed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS &amp; ANALYSIS</head><p>Table <ref type="table" coords="3,339.52,445.40,4.13,7.94" target="#tab_1">1</ref> summarizes the results of our submitted runs in TREC-PM 2020 for the evaluation metrics Inferred Discounted Cumulative Gain (infNDCG), Precision@10 (P@10), and R-precision (R-prec), the official evaluation metrics of the track. Firstly, we note that uog_ufmg_DFRee is better than TREC median for P@10 and their performances are close to each other for infNDCG and R-prec. This shows that our first-phase retrieval with DFRee is a reasonably good baseline and a reliable starting point to subsequent re-rank approaches. As expected by our experiments in TREC-PM 2019, the pure re-ranking with SciBERT-pm (uog_ufmg_s_dfr0) is less effective than the DFRee model. The goal with this approach is to allow us to combine DFRee scores with semantic-based models scores, in particular, SciBERT-pm.</p><p>However, our linear combination uog_ufmg_s_dfr5 in TREC-PM 2020 did not reproduce the same improvements over its baselines as observed in TREC-PM 2019, although it slightly outperformed the DFRee run for P@10. We will investigate the differences between the TREC-PM 2019 and 2020 results as future work. Nonetheless, our best run uog_ufmg_sb_df5 followed the same experimental methodology of uog_ufmg_s_df5, but with a different query tokenization. The run uog_ufmg_sb_df5 outperformed not only all our submitted runs, but also the TREC median on all measures. Finally, the results of run uog_ufmg_secL2R reflect our first experiments with the structured-based framework, i.e., when we use multiple SciBERT-s models to re-rank the different abstract sections. This approach outperformed the SciBERT-pm model (i.e., uog_ufmg_s_dfr0) on all metrics, which suggests that this is a promising research direction. However, the structure-based run was not consistently better than the other runs or the TREC median. To overcome this, we aim to further study the exact benefits we can achieve when we use structure-based information.</p><p>It is noteworthy to mention that the reported results above concern Phase 1 of the TREC-PM 2020 evaluation, i.e., for the relevance assessment. At the time of writing this paper, Phase 2, i.e., the evidence assessment, was not yet available to participants. The goal of this second phase of evaluation is as follows: given a pair of a disease-treatment as a query and a biomedical article retrieved, the aim is to determine how strong is the evidence brought by the article with respect to the pair disease-treatment. Articles with strong evidence should be ranked over articles with weaker evidence (whether positive or negative). In fact, we did not implement any specific methods to model the evidence strength of the articles. Instead, our approaches were purely relevance-oriented. We will present our obtained results in Phase 2 in the final version of the notebook paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In our participation in the TREC 2020 Precision Medicine Track we investigated the integration of domain-specific state-of-theart contextual language models, such as SciBERT, with PyTerrier retrieval pipelines, in a two-phase retrieval process. In terms of effectiveness, we observed promising improvements using simple linear combinations such as uog_ufmg_sb_df5, outperforming the median performance of the TREC participants. Moreover, part of the experimental methodology built for this task was used in the critical and urgent task provided by TREC-Covid, within the context of the ongoing pandemic. Finally, as discussed in this paper, several interesting research directions were raised during the experiments that we conducted this year for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.50,85.73,242.15,130.46"><head>Table 1 :</head><label>1</label><figDesc>Results of the five submitted runs for TREC-PM 2020. The last two rows show the median and best performances over all submissions, aggregated by average.</figDesc><table coords="4,84.81,130.73,178.22,85.45"><row><cell>Run</cell><cell cols="2">infNDCG P@10 R-prec</cell></row><row><cell>uog_ufmg_DFRee</cell><cell>0.418</cell><cell>0.500 0.315</cell></row><row><cell cols="2">uog_ufmg_s_dfr0 0.280</cell><cell>0.332 0.197</cell></row><row><cell cols="2">uog_ufmg_s_dfr5 0.408</cell><cell>0.513 0.309</cell></row><row><cell cols="2">uog_ufmg_sb_df5 0.498</cell><cell>0.548 0.391</cell></row><row><cell cols="2">uog_ufmg_secL2R 0.401</cell><cell>0.455 0.311</cell></row><row><cell>Median</cell><cell>0.432</cell><cell>0.456 0.326</cell></row><row><cell>Best</cell><cell>0.696</cell><cell>0.722 0.568</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,56.72,694.38,112.86,6.18"><p>https://github.com/terrier-org/pyterrier</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,56.84,702.79,89.02,6.18"><p>https://lightgbm.readthedocs.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,321.00,702.79,160.23,6.18"><p>https://github.com/terrier-org/terrier-core/blob/5.x/doc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,320.88,702.79,126.80,6.18"><p>https://github.com/Georgetown-IR-Lab/cedr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="3,321.00,702.79,113.81,6.18"><p>https://github.com/microsoft/LightGBM</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This study was financed in part by the <rs type="funder">Coordena√ß√£o de Aperfei√ßoamento de Pessoal de N√≠vel Superior, Brazil (Capes)</rs>, <rs type="grantNumber">Finance Code 001</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vApqFnt">
					<idno type="grant-number">Finance Code 001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,333.39,221.45,224.81,6.23;4,333.39,229.42,224.81,6.23;4,333.39,237.44,89.24,6.18" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,414.66,221.45,143.54,6.23;4,333.39,229.42,86.66,6.23">Probability models for information retrieval based on divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">Giambattista</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation.</note>
</biblStruct>

<biblStruct coords="4,333.39,245.41,224.81,6.18;4,333.39,253.38,225.88,6.18;4,333.39,261.30,120.48,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,492.50,245.41,65.70,6.18;4,333.39,253.38,222.41,6.18">Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cornelis Joost</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,333.39,261.30,58.77,6.23">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,269.32,224.81,6.18;4,333.39,277.24,179.65,6.23" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m" coord="4,464.77,269.32,93.43,6.18;4,333.39,277.29,66.48,6.18">SciBERT: A pretrained language model for scientific text</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,285.26,225.63,6.18;4,333.39,293.23,224.81,6.18;4,333.39,301.15,97.21,6.23" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,541.50,285.26,17.53,6.18;4,333.39,293.23,214.32,6.18">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,333.39,301.15,60.31,6.23">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,309.17,224.81,6.18;4,333.39,317.09,225.88,6.23;4,333.39,325.11,18.33,6.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,445.10,309.17,113.09,6.18;4,333.39,317.14,153.70,6.18">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</title>
		<author>
			<persName coords=""><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,500.07,317.09,56.29,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,333.08,224.81,6.18;4,333.39,341.05,224.81,6.18;4,333.39,348.97,204.97,6.23" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="4,433.89,341.05,124.30,6.18;4,333.39,349.02,108.24,6.18">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName coords=""><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno>1909.11942</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,356.99,225.58,6.18;4,333.39,364.96,224.81,6.18;4,333.39,372.88,225.58,6.23;4,333.23,380.90,31.30,6.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,435.80,364.96,122.40,6.18;4,333.39,372.93,140.64,6.18">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,480.92,372.88,41.16,6.23">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,388.87,224.94,6.18;4,333.39,396.84,225.05,6.18;4,333.39,404.76,224.81,6.23;4,333.18,412.78,18.66,6.18" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>1907.11692</idno>
		<title level="m" coord="4,523.05,396.84,35.40,6.18;4,333.39,404.81,142.40,6.18">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,420.75,225.64,6.18;4,333.39,428.67,225.57,6.23;4,333.23,436.69,14.51,6.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,540.39,420.75,18.64,6.18;4,333.39,428.72,138.64,6.18">CEDR: Contextualized embeddings for document ranking</title>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,484.00,428.67,54.06,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1101" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,444.66,225.88,6.18" xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ounis</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,452.58,224.81,6.23;4,333.39,460.55,67.86,6.23" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="4,333.39,452.63,170.45,6.18">From Puppy to Maturity: Experiences in Developing Terrier</title>
	</analytic>
	<monogr>
		<title level="m" coord="4,518.33,452.58,39.88,6.23;4,333.39,460.55,65.01,6.23">Proceedings of OSIR workshop at SIGIR</title>
		<meeting>OSIR workshop at SIGIR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,468.57,224.81,6.18;4,333.39,476.49,203.54,6.23" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="4,467.53,468.57,90.67,6.18;4,333.39,476.54,106.45,6.18">Declarative Experimentation in Information Retrieval Using PyTerrier</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Tonellotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,451.96,476.49,55.51,6.23">Proceedings of ICTIR</title>
		<meeting>ICTIR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,484.51,225.88,6.18;4,333.39,492.43,108.33,6.23" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="4,467.87,484.51,87.86,6.18">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,500.45,225.58,6.18;4,333.39,508.42,224.81,6.18;4,333.18,516.34,174.46,6.23" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="4,511.33,508.42,46.87,6.18;4,333.18,516.39,103.97,6.18">Overview of the TREC 2019 Precision Medicine Track</title>
		<author>
			<persName coords=""><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shubham</forename><surname>Pant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,449.47,516.34,54.74,6.23">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,524.36,225.99,6.18;4,333.39,532.28,224.81,6.23;4,333.39,540.25,50.71,6.23" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="4,463.57,524.36,95.81,6.18;4,333.39,532.33,180.66,6.18">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,525.95,532.28,32.25,6.23;4,333.39,540.25,21.31,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,548.27,225.07,6.18;4,333.18,556.19,218.91,6.23" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="4,496.61,548.27,61.85,6.18;4,333.18,556.24,148.42,6.18">University of Glasgow Terrier Team at the TREC 2019 Deep Learning Track</title>
		<author>
			<persName coords=""><forename type="first">Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,493.92,556.19,54.74,6.23">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,564.21,224.81,6.18;4,333.39,572.13,224.81,6.23;4,333.39,580.10,17.16,6.23" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="4,521.19,564.21,37.01,6.18;4,333.39,572.18,173.58,6.18">University of Glasgow Terrier Team at the TREC 2020 Deep Learning Track</title>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaxiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,519.23,572.13,38.97,6.23;4,333.39,580.10,13.73,6.23">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
