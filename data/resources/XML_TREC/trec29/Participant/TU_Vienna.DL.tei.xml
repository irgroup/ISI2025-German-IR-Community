<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.76,84.23,306.49,15.44;1,194.19,104.15,223.63,15.44">Evaluating Transformer-Kernel Models at TREC Deep Learning 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.13,134.31,98.82,10.59"><forename type="first">Sebastian</forename><surname>Hofstätter</surname></persName>
							<email>s.hofstaetter@tuwien.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">TU</orgName>
								<address>
									<settlement>Wien</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.89,134.31,73.25,10.59"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
							<email>hanbury@ifs.tuwien.ac.at</email>
							<affiliation key="aff1">
								<orgName type="institution">TU</orgName>
								<address>
									<settlement>Wien</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.76,84.23,306.49,15.44;1,194.19,104.15,223.63,15.44">Evaluating Transformer-Kernel Models at TREC Deep Learning 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FD5F229E4C5DD65E16927340377DAF8D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tested multiple hypotheses using the Transformer-Kernel neural ranking pattern. The TK model family sits between BERT and previous ranking model in terms of the efficiency-effectiveness trade-off, faster than BERT albeit less effective.</p><p>In the passage re-ranking task we tested the effectiveness of contextualized stopwords, introduced with TK-Sparse and find that removing 19% of terms after contextualization even slightly increases the model's effectiveness. In the document re-ranking task we tested if a long-text TKL model is better with 2,000 or 4,000 document tokens and find that our 2,000 token instance outperforms the other.</p><p>Our results confirm the path for new storage saving methods for interpretable ranking models, and give an interesting insight into the questions of how many tokens of a document we need to read for a relevance assessment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the 2020 installment of the TREC Deep Learning track, our group tested multiple hypotheses centered around the Transformer-Kernel neural ranking pattern. We introduced the TK model last year <ref type="bibr" coords="1,272.58,407.37,9.27,7.94" target="#b4">[5]</ref>, as a very efficient alternative to BERT based ranking approaches <ref type="bibr" coords="1,275.06,418.33,9.23,7.94" target="#b5">[6,</ref><ref type="bibr" coords="1,286.20,418.33,6.15,7.94" target="#b6">7]</ref>. The TK pattern fills the gap in the efficiency-effectiveness tradeoff map between BERT and non-Transformer based text ranking models <ref type="bibr" coords="1,81.90,451.20,9.35,7.94" target="#b1">[2]</ref>. Our shallow Transformers are faster than BERT, albeit less effective. In Table <ref type="table" coords="1,135.56,462.16,4.14,7.94">1</ref> we give a summary of our submitted runs corresponding to our two research questions.</p><p>Last year at TREC-DL'19 <ref type="bibr" coords="1,159.49,484.08,10.68,7.94" target="#b0">[1]</ref> we tested the general concept of our TK family, to understand whether the proposed concept works in general. For this year's TREC-DL passage task we tested the effectiveness of contextualized stopwords, introduced with TK-Sparse <ref type="bibr" coords="1,80.85,527.92,9.52,7.94" target="#b2">[3]</ref>, as a way to reduce the number of terms saved when pre-computing document representations against a full TK model. We studied the question: RQ1 How do space-saving contextualized stopwords effect the re-ranking quality of TK?</p><p>We found that the TK-Sparse run is slightly more effective than the plain TK run, suggesting that not only do space saving methods improve efficiency, our contextualized stopwords apparently reduce noise and improve the effectiveness of the TK model.</p><p>In the document task we tested our hypothesis of how much of a document we need to read to effectively train and run a neural ranking model. For this purpose we submitted two runs of our TKL model (Transformer-Kernel for Long documents) <ref type="bibr" coords="1,232.63,665.21,10.45,7.94" target="#b3">[4]</ref> one receiving the first 2,000 and the other the first 4,000 tokens of a document and studied: RQ2 How effective is the 2K vs. 4K document length TKL model? Table <ref type="table" coords="1,361.30,178.03,3.45,7.70">1</ref>: Summary of our submitted TREC'20 runs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run Description</head><p>Passages TUW-TK-2Layer A plain TK <ref type="bibr" coords="1,448.33,221.84,10.55,7.94" target="#b4">[5]</ref> model with GloVe embeddings and pre-trained Transformer layers on the collection TUW-TK-Sparse TK-Sparse <ref type="bibr" coords="1,445.96,254.72,10.55,7.94" target="#b2">[3]</ref> with 19% sparsity of stored passage representations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Documents TUW-TKL-2k</head><p>A TKL model <ref type="bibr" coords="1,456.50,292.08,10.45,7.94" target="#b3">[4]</ref> using 2,000 document tokens TUW-TKL-4k</p><p>A TKL model <ref type="bibr" coords="1,456.50,314.00,10.45,7.94" target="#b3">[4]</ref> using 4,000 document tokens</p><p>Here we found that the 2K model outperforms the 4K model in the TREC 2020 evaluation. This an interesting, as we observed reverse results using the TREC 2019 query set. For efficiency, of course computing fewer tokens is beneficial, so these TREC 2020 results are encouraging for models using not all available tokens to locate relevance.</p><p>We used our PyTorch <ref type="bibr" coords="1,408.53,421.96,10.55,7.94" target="#b7">[8]</ref> implementations available at: github.com/sebastian-hofstaetter/transformer-kernel-ranking</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In the following we give a quick overview of the methodology, for more details we refer to the respective papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TK</head><p>Our TUW-TK-2Layer run is an instance of the TK model. The Transformer-Kernel (TK) model <ref type="bibr" coords="1,442.52,524.97,10.68,7.94" target="#b4">[5]</ref> is not based on BERT pretraining, but rather uses shallow Transformers. TK independently contextualizes query 𝑞 1:𝑚 and passage 𝑝 1:𝑛 based on pre-trained word embeddings, where the intensity of the contextualization (Transformers as TF) is set by a gate 𝛼:</p><formula xml:id="formula_0" coords="1,380.84,582.00,177.37,23.50">q𝑖 = 𝑞 𝑖 * 𝛼 + TF(𝑞 1:𝑚 ) 𝑖 * (1 -𝛼) p𝑖 = 𝑝 𝑖 * 𝛼 + TF(𝑝 1:𝑛 ) 𝑖 * (1 -𝛼)<label>(1)</label></formula><p>The sequences q1:𝑚 and p1:𝑛 interact in a match-matrix with a cosine similarity per term pair and each similarity is activated by a set of Gaussian kernels <ref type="bibr" coords="1,404.61,634.13,9.50,7.94" target="#b8">[9]</ref>:</p><formula xml:id="formula_1" coords="1,376.51,646.66,181.69,24.72">𝐾 𝑘 𝑖,𝑗 = exp - cos( q𝑖 , p 𝑗 ) -𝜇 𝑘 2 2𝜎 2<label>(2)</label></formula><p>Kernel-pooling is a soft-histogram, which counts the number of occurrences of similarity ranges. Each kernel 𝑘 focuses on a fixed range with center 𝜇 𝑘 and width of 𝜎. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TK-Sparse</head><p>The adaption of contextualized stopwords used in the TUW-TK-Sparse run augment the TK model with a sparsity component, that filters out passage terms after the contextualization. We give an overview of this procedure in Figure <ref type="figure" coords="2,194.43,552.67,3.13,7.94" target="#fig_0">1</ref>. Every vector d𝑗 is transformed by two feed forward layers, followed by a ReLU activation, to compute the stopword removal gate 𝑟 𝑗 :</p><formula xml:id="formula_2" coords="2,140.91,590.52,153.13,10.42">𝑟 𝑗 = ReLU(FF( d𝑗 )<label>(4)</label></formula><p>This removal gate is applied to the kernel activations, to filter out activations that become 0 because of the gate 𝑟 𝑗 .</p><formula xml:id="formula_3" coords="2,147.37,638.62,146.68,9.75">𝐾 𝑘 𝑖,𝑗 = 𝐾 𝑘 𝑖,𝑗 * 𝑟 𝑗<label>(5)</label></formula><p>Now, we only need to store non-zero passage terms for the querydependent element-wise kernel activation and aggregation. The sparsity is trained with an augmented loss and L1-norm regularization of 𝑟 , forcing 𝑟 to become small and with the ReLu activation become zero at as many positions as possible without </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">TKL</head><p>Our document runs TUW-TKL-2k and TUW-TKL-4k are based on TKL. The TKL model utilizes the same building blocks as TK, namely shallow transformers and an enhanced kernel pooling, but applies them in sliding windows to form local-attention regions and then selects the most relevant regions to form the document score. The exact definitions are detailed in Hofstätter et al. <ref type="bibr" coords="2,494.09,329.74,9.39,7.94" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>To answer our RQ1 How do space-saving contextualized stopwords effect the re-ranking quality of TK? we show the passage re-ranking task results in Table <ref type="table" coords="2,393.72,386.52,3.09,7.94" target="#tab_0">2</ref>. We see that TK-Sparse outperforms TK on nDCG and MRR, however on MAP TK shows better results. The main take-away here is that TK-Sparse is at least as effective as TK, showing the applicability of our contextualized stopwords to save pre-computation storage. We view this technique as an interesting path forward in combination with other vector compression techniques.</p><p>Our document re-ranking task results concerning RQ2 How effective is the 2K vs. 4K document length TKL model? are shown in Table <ref type="table" coords="2,339.25,485.15,3.01,7.94" target="#tab_1">3</ref>. The results are unexpected, yet interesting, as the previous TREC-DL'19 results for those two model instances showed the reverse outcome in Hofstätter et al. <ref type="bibr" coords="2,446.71,507.07,9.36,7.94" target="#b3">[4]</ref>. The 2K model outperforms the 4K instance on all metrics. This shows that apparently 2,000 document tokens are enough in the re-ranking scenario. For next year's TREC we plan to test this hypothesis also in the retrieval stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.80,342.87,241.85,7.70;2,53.80,353.83,240.25,7.70;2,53.80,364.79,240.25,7.70;2,53.80,375.75,76.03,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TK-Sparse architecture diagram: The offline precomputed document representations are filtered through the sparse stopword component before being saved and used in the aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,332.04,85.73,211.78,51.75"><head>Table 2 :</head><label>2</label><figDesc>Official TREC'20 passage re-ranking results.</figDesc><table coords="2,336.54,103.13,203.07,34.35"><row><cell>Run</cell><cell cols="3">nDCG@10 MRR@1K MAP@1K</cell></row><row><cell>TUW-TK-2Layer</cell><cell>0.6539</cell><cell>0.7654</cell><cell>0.4179</cell></row><row><cell>TUW-TK-Sparse</cell><cell>0.6610</cell><cell>0.7970</cell><cell>0.4164</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,317.96,154.90,241.76,93.12"><head>Table 3 :</head><label>3</label><figDesc>Official TREC'20 document re-ranking results.</figDesc><table coords="2,317.96,172.30,241.76,75.72"><row><cell>Run</cell><cell cols="3">nDCG@10 MRR@100 MAP@100</cell></row><row><cell>TUW-TKL-2k</cell><cell>0.5852</cell><cell>0.9296</cell><cell>0.3810</cell></row><row><cell>TUW-TKL-4k</cell><cell>0.5749</cell><cell>0.9185</cell><cell>0.3749</cell></row><row><cell cols="4">compromising the effectiveness of the model. For the exact proce-</cell></row><row><cell cols="4">dure and training adaptions we refer to Hofstätter et al. [3].</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="4">CONCLUSION</head><p>We submitted 4 runs to the <rs type="institution">TREC-DL</rs>'20 campaign, and studied 2 research questions concerning the effectiveness of the TK model family. We find, that our TK-Sparse model does not negatively impact the effectiveness when reducing the passage terms with contextualized stopwords. Furthermore, we observe that 2,000 document tokens seem to be enough to re-rank documents with TKL.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="2,330.15,670.91,228.31,6.18;2,330.15,678.83,134.51,6.23" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="2,531.19,670.91,27.28,6.18;2,330.15,678.88,105.33,6.18">Overview of the TREC 2019 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In TREC</note>
</biblStruct>

<biblStruct coords="2,330.15,686.85,228.05,6.18;2,329.90,694.82,229.47,6.18;2,329.65,702.79,171.88,6.18" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sophia</forename><surname>Althammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mete</forename><surname>Sertkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<idno>arXiv:cs.IR/2010.02666</idno>
		<title level="m" coord="2,395.44,694.82,163.94,6.18;2,329.65,702.79,102.27,6.18">Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,65.99,89.10,229.13,6.18;3,65.99,97.02,201.12,6.23" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,65.99,97.07,148.67,6.18">Learning to Re-Rank with Contextualized Stopwords</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aldo</forename><surname>Lipani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Markus</forename><surname>Zlabinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,227.16,97.02,36.38,6.23">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,65.99,105.04,228.05,6.18;3,65.99,113.01,228.05,6.18;3,65.99,120.93,75.55,6.23" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,116.17,113.01,177.87,6.18;3,65.99,120.98,24.30,6.18">Local Self-Attention over Long Text for Efficient Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,102.33,120.93,36.37,6.23">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,65.99,128.95,228.05,6.18;3,65.78,136.87,221.20,6.23" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,251.41,128.95,42.63,6.18;3,65.78,136.92,170.47,6.18">Interpretable &amp; Time-Budget-Constrained Contextualization for Re-Ranking</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Markus</forename><surname>Zlabinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,248.76,136.87,34.99,6.23">Proc. of ECAI</title>
		<meeting>of ECAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,65.99,144.89,228.88,6.18;3,65.99,152.81,196.84,6.23" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,276.19,144.89,18.68,6.18;3,65.99,152.86,145.12,6.18">CEDR: Contextualized Embeddings for Document Ranking</title>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,223.61,152.81,36.37,6.23">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,65.99,160.83,229.13,6.18;3,65.99,168.75,108.33,6.23" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="3,202.88,160.83,88.70,6.18">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="3,65.99,176.77,228.82,6.18;3,65.99,184.74,229.12,6.18;3,65.99,192.66,178.11,6.23" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="3,83.01,192.71,103.40,6.18">Automatic differentiation in PyTorch</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,198.89,192.66,41.91,6.23">Proc. of NIPS-W</title>
		<meeting>of NIPS-W</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,65.99,200.68,229.13,6.18;3,65.99,208.60,211.08,6.23" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="3,65.99,208.65,159.60,6.18">End-to-End Neural Ad-hoc Ranking with Kernel Pooling</title>
		<author>
			<persName coords=""><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,237.86,208.60,36.37,6.23">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
