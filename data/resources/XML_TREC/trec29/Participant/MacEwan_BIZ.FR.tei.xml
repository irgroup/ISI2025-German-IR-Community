<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,89.20,364.94,14.49">MacEwan University at the TREC 2020 Fair Ranking Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,72.00,111.33,75.79,11.68"><forename type="first">Brian</forename><surname>Almquist</surname></persName>
							<email>almquistb@macewan.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">MacEwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,89.20,364.94,14.49">MacEwan University at the TREC 2020 Fair Ranking Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">937C4F200900FEF8403B2E473AD11B30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The MacEwan University School of Business submitted two runs for the TREC 2020 Fair Ranking Track. For this task, we indexed the document abstracts and the associated metadata from the provided Semantic Scholar dataset into a single Solr<ref type="foot" coords="1,216.96,187.87,3.48,6.74" target="#foot_0">1</ref> node using a standard Tokenizer chain.</p><p>For each of the evaluation queries, we executed a query for each of the documents that required reranking. For each query-document pairing, we collected the BM25 similarity score for the "paperAbstract" and "title" fields. Each of the documents are ranked for each field based on the similarity score with the query text, with ties sharing their combined rank. This resulted in two ranked lists for each query.</p><p>For the baseline run, MacEwan-base, the reranking occurs through merging the two ranked lists using a weighted sum of the ranks.<ref type="foot" coords="1,191.28,318.91,3.48,6.74" target="#foot_1">2</ref> For the normalized run, MacEwan-norm, the weighted sum uses the normalized similarity scores for each field instead of the similarity rank. Each sequence is assigned a set of starting weights. As queries are repeated within the sequence, the weights are adjusted incrementally to allow for emphasis or de-emphasis on different ranked lists. The amount of the adjustment is based on the frequency of the query within the sequence. This ensures that a complete shift in the weights for a given query will occur over the sequence if the query is repeated at least once. With only two ranked fields, the weights start equalized, and then the weight of the paperAbstract rank/score is increased while the weight of the title is decreased.</p><p>Due to time constraints, we were not able to incorporate information from the training grouping and any potential alternative groupings. As a result, these runs only assess whether fairness goals can be achieved with simple perturbations of result, given that the documents have met some threshold for relevance based on query similarity. We note that, while not universally inferior, the normalized run does not perform as well as the baseline run. Assuming a goal of minimizing the difference from the expected exposure, the baseline run achieves a better score in 54% of the queries. The difference in the mean scores between the two runs suggests that the distribution of similarity scores used for reranking the normalized run can lead to greater deviation from expected exposure. One possibility is that using a linear adjustment of weights on a linear distribution of scores, such as ranks, may produce a more frequent reranking than on normalized similarity scores. If there is a large gap in similarity scores, greater adjustments in weights will be required to force a reranking of the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Again, other document metadata, such as citation numbers, or external information gathered about the authors, may also be used as inputs for these merged rank scores.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,78.00,676.61,116.56,9.77"><p>http://lucene.apache.org/solr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,78.00,688.13,445.61,9.77;1,72.00,699.65,203.67,9.77"><p>Shaw, J. A. and Fox, E. A. Combination of multiple searches. In Harman, D. K., ed. The Second Text REtrieval Conference (TREC-2). (Gaithersburg, MD, August</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="1,278.15,699.65,251.20,9.77;1,72.00,711.17,286.91,9.77"><p>31-September 2, 1993). The Department of Commerce and the National Institute ofStandards and Technology (NIST), 1993, 243-252.   </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
