<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,56.69,107.73,420.11,19.92;1,56.69,129.65,209.17,19.92">MRG_UWaterloo Participation in the TREC 2020 Precision Medicine Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,56.69,171.77,114.11,7.59"><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.59,171.77,113.23,7.59"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,56.69,225.57,57.24,7.59"><forename type="first">Ba</forename><forename type="middle">'</forename><surname>Pham</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,56.69,107.73,420.11,19.92;1,56.69,129.65,209.17,19.92">MRG_UWaterloo Participation in the TREC 2020 Precision Medicine Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6C59D04952DC597EF23B52DF630A21C2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>The MRG_UWaterloo group from the University of Waterloo, in collaboration with Ba' Pham of the University of Toronto Health Economics and Assessment Collaborative, participated for the rst time in the TREC 2020 Precision Medicine Track.</p><p>Our baseline run (uwbm25) used the BM25 relevance ranking method, as implemented by the Wumpus Search Engine, 1 , with default parameters. The remaining runs examined the impact of various forms of pseudo-relevance feedback and manual relevance feedback, using relevance assessments by one author (Pham) who has extensive experience in conducting rapid systematic reviews, but not with precision medicine. To our surprise, we found that none of the feedback methods diered substantially in eectiveness from our baseline run. Based on preliminary NIST feedback, the eectiveness of all runs was near the median of all TREC submissions. Our Continuous Active Learning (CAL) approach <ref type="bibr" coords="1,302.94,547.01,10.51,6.64" target="#b0">[1]</ref> (uwman) started with the result of uwrn, repeatedly selecting the top-20 scoring documents for assessment, adding them to the training set, creating a new model, and re-scoring the documents. Our expert assessor also consulted external resources such as Web of Science to nd other relevant documents that should be included. The nal ranking consisted of those documents assessed relevant during assessment, ranked by score, followed by all other documents also ranked by score. Documents assessed non-relevant were ranked no dierently from documents that were never assessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Discussion</head><p>It is apparent that our expert's assessments diered substantially from the ocial TREC assessments, and that these dierences thwarted our attempts to leverage them for relevance feedback. It may be that our expert's assessments were too conservative. It has been observed elsewhere that more liberal assessments, even by less skilled reviewers, may yield better results for relevance feedback <ref type="bibr" coords="1,262.48,713.80,9.96,6.64" target="#b1">[2]</ref>. 1 http://stefan.buettcher.org/cs/wumpus/index.html. 2 https://code.google.com/archive/p/soa-ml/ with parameters --learner_type logreg-pegasos --loop_type roc --lambda 0.0001. The precision medicine topics may present retrieval diculties similar to the intersection topics that were problemetic in the TREC 2002 Filtering Track <ref type="bibr" coords="2,271.01,200.33,9.96,6.64" target="#b2">[3]</ref>. When relevance is dened to be the conjunction of several criteria, documents meeting some, but not all, of the criteria are considered to be non-relevant. Our working hypothesis is that when the learning algorithm is trained on a number of documents meeting some, but not all of the criteria, it may infer that each is an indicator of non-relevance, while failing to infer that, in combination, they indicate relevance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,71.64,403.55,483.67,6.64;1,56.69,415.50,498.62,6.64;1,56.69,427.46,375.16,6.64;1,431.86,422.87,3.97,7.94;1,438.82,427.46,116.49,6.64;1,56.69,439.41,498.62,6.64;1,56.69,451.37,42.94,6.64;1,71.64,463.32,483.67,6.64;1,56.69,475.28,498.62,6.64;1,56.69,487.23,498.61,6.64;1,56.69,499.19,93.35,6.64;1,71.64,511.14,483.67,6.64;1,56.69,523.10,498.61,6.64;1,56.69,535.05,360.58,6.64"><head></head><label></label><figDesc>Our pseudo-relevance feedback run (uwpr) selected the 20 highest-ranked documents from uwbm25 as positive training examples, and 100 randomly selected documents from the corpus as negative training examples. These training examples were converted to tf-idf feature vectors and used as input to Soa-ML 2 to compute a log-likelihood score for each document. Documents were sorted by score, and the top-scoring 1,000 documents were submitted as run uwpr. Our positive-only manual feedback run (uwr) started with the same documents as uwpr, but the 20 highestranked documents from uwbm25 were assessed for relevance by our expert (Pham), and only those assessed to be relevant were included as positive training examples. Those documents assessed to be non-relevant were excluded from the training set. Our postive-and-negative manual feedback run (uwrn) used the documents and relevance assessments from uwpr, treating documents assessed to be relevant as positive training examples, and documents assessed to be non-relevant, as well as the 100 random documents, as negative training examples.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="2,72.19,298.93,483.11,6.64;2,72.19,310.88,226.67,6.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="2,289.54,298.93,265.76,6.64;2,72.19,310.88,112.83,6.64">Autonomy and reliability of continuous active learning for technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grossman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06868</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="2,72.19,330.81,483.12,6.64;2,72.19,342.76,101.14,6.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="2,257.31,330.81,297.99,6.64;2,72.19,342.76,25.14,6.64">Impact of review-set selection on human assessment for text classication</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="2,118.38,343.16,28.25,6.11">SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="2,72.19,362.69,457.15,6.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="2,238.43,362.69,214.56,6.64">Building a ltering test collection for TREC 2002</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="2,474.39,363.09,50.32,6.11">SIGIR 2003</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
