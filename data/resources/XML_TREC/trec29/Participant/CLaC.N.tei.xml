<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,218.88,115.96,177.60,12.62;1,221.93,133.89,171.50,12.62">The CLaC System at the TREC 2020 News Track</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,229.86,171.63,72.37,8.74"><forename type="first">Pavel</forename><surname>Khloponin</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,324.92,171.63,60.58,8.74"><forename type="first">Leila</forename><surname>Kosseim</surname></persName>
							<email>leila.kosseim@concordia.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ClaC Lab Department of Computer Science and Software Engineering</orgName>
								<orgName type="department" key="dep2">School of Engineering and Computer Science</orgName>
								<orgName type="institution">Gina Cody</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Concordia University Montreal QC</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,218.88,115.96,177.60,12.62;1,221.93,133.89,171.50,12.62">The CLaC System at the TREC 2020 News Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">51E7F3C4D7C65672CF3E6C49132E2F2D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>News TREC</term>
					<term>Text embedding</term>
					<term>Proximity measures</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our approach to the TREC 2020 News Track. The goal of the News Track is to provide background links and entity linking to target news articles within a collection of articles. We submitted 4 runs, 2 of which achieved scores higher than the median. The first approach is based on the classic Okapi BM25 using the entire article as a query. This run obtained an nDCG@5 of 0.5924. The second approach is based on a combination of BM25 with GPT-2 embeddings which lead to an nDCG@5 of 0.5873. These top models were chosen after exploring a variety of representations of documents as embedding vectors and proximity measures. Combining document embeddings and Okapi BM25 is shown to diversify the results without much impact on quality, and can help to overcome the limitations of a single model system implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given the sheer number of electronic sources of news available today, it is important to develop approaches for the automatic recommendation of contextual information for users to better understand a news article.</p><p>In order to address this need, since 2018, the News Track at TREC has proposed two related shared tasks: background linking and entity ranking <ref type="bibr" coords="1,452.09,548.45,23.75,8.74;1,134.77,560.41,134.28,8.74" target="#b0">(Soboroff, Huang, and Harman 2018</ref><ref type="bibr" coords="1,269.05,560.41,164.19,8.74">, Soboroff, Huang, and Harman 2019</ref><ref type="bibr" coords="1,433.24,560.41,47.35,8.74;1,134.77,572.36,112.55,8.74">, Soboroff, Huang, and Harman 2020)</ref>. The goal of the background linking task is to provide relevant background information to news articles through the identification of related articles. On the other hand, entity ranking focuses on providing a list of names, concepts, artifacts, etc. mentioned in news articles, which will help readers better understand the news. This year, we only participated to the first task: background linking.</p><p>For the 2020 background linking task, the data set provided by NIST is an extension of the collection used in the previous years and consists of about 671,000 news articles from the Washington Post. Given a search topic, which itself is an article from the corpus selected by the TREC organizers, participants need to select up to 100 related articles from the corpus and output them as a ranked list from the most related to the least related. For evaluation purposes, only the top 5 articles from each list are considered. A 5 point rank is manually assigned by NIST assessors for each of the top 5 articles during the evaluation phase. The score assigned to each search topic is an integer between 0 (little or no useful information) and 4 (must appear in recommendations or critical context will be missed). The total score of the system is then computed using the nDCG@5 metric <ref type="bibr" coords="2,226.38,226.59,135.81,8.74" target="#b3">(Järvelin and Kekäläinen 2002)</ref> with the gain metric 2 rank .</p><p>One important criterion for judging related articles is diversity. Because we did not have a clear understanding of the notion of article diversity in the news recommendation context, we did not specifically address this aspect of the task, but we discuss this issue in Section 5.</p><p>For the submission, 50 search topics were used; however, for the official evaluation, only 49 search topics were considered. One topic with no relevant backlinks was dropped. Prior to the evaluation, the organizers also provided search topics and their corresponding manually evaluated results (that is, all articles evaluated manually with a rank from 0 to 4) from the TREC News 2018 (50 topics) and TREC News 2018 (60 topics), which used the same article collection. We used these two sets of topics (110 in total) and evaluated backlinks (24,163 in total) for validation purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Collection</head><p>The TREC News 2020 organizers provided a document collection of 671,934 news articles from The Washington Post published between 2012 and 2019. This document collection is the same as the one provided at the 2018 and 2019 editions of the task but with duplicate articles removed and with new articles from 2017 to 2019 added. Each news article is stored in "JSON-lines" format and represented as a single line of JSON. Each document contains 8 types of meta-information:</p><p>1. id 2. article URL 3. title 4. author 5. publication date 6. type (blog post or article) 7. news source 8. content field</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>Apart from the id (#1) field, which was used for identification purposes, only the article title (#3) and the text extracted from the content field (#8) were considered. The title was prepended to the content and processed as a single document. The content itself was stored in a form of content blocks, where each content block can be a text paragraph, an image, a video, a tweet, a citation, etc. Each content block itself may contain meta-information (up to 133 different fields), such as MIME-type, type, kicker (category), content, subtype, source, URLs, etc. Based on this meta-information we identified blocks with frequently appearing content types and checked if they have any useful text descriptions, and kept for further processing only blocks with paragraphs, image captions, headers, and quotes. Blocks were further cleaned from embeds, links, images, and other HTML tags, preserving only plain text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Statistics</head><p>NIST required participants to ignore wire articles, editorial content and opinion posts, which have "Opinion", "Letters to the Editor", or "The Post's View" values in the content meta-information block with type "kicker". Due to this, the initial set of 671,947 articles was reduced by 2,057 to 669,890 items. This is shown in Table <ref type="table" coords="3,204.01,339.21,3.87,8.74">1</ref>.</p><p>As shown in Table <ref type="table" coords="3,238.00,352.66,3.87,8.74">1</ref>, many sandbox articles (content previews or articles demonstrating website functionality) were discovered during data exploration: 1,112 articles with a URL path starting with "/test/wp/", presumably indicating content taken from the section of the website not intended to be public (content playground), and 95 documents with "Lorem ipsum..." (common placeholder text) content in the article text. They were preserved in the dataset.</p><p>original number of articles 671,947 articles to ignore 2,057 articles used to build the models 669,890 "lorem ipsum" articles 95 sandbox articles 1,112 average size of article before pre-processing (characters) 10,391 average size after pre-processing (characters) 4,533</p><p>Table <ref type="table" coords="3,205.42,547.91,4.13,7.89">1</ref>. Statistics of the 2020 TREC News document collection</p><p>As shown in Table <ref type="table" coords="3,239.07,572.43,3.87,8.74">1</ref>, the 671,947 documents considered have an average length of 10,391 characters prior to preprocessing, but only 4,533 after preprocessing. Figure <ref type="figure" coords="3,215.71,596.34,4.98,8.74" target="#fig_0">1</ref> shows the distribution of the article lengths before and after pre-processing. Some articles are composed almost entirely of meta-information and have very little text inside (short statements, video players, cited tweets, etc.), while others have very long texts (transcripts of debates, conferences, testimony, crime reports). As Figure <ref type="figure" coords="3,282.47,644.16,4.98,8.74" target="#fig_0">1</ref> shows, the majority have between 1,000 and 10,000 characters.</p><p>3 Our approach  The rationale for using such document representations was our expectation that related articles would be closer to each other in vector space than unrelated articles. Hence, document vector distance would be a good approximation of document content relatedness. Search topics and manually ranked documents from 2018 and 2019 (see Section 1) were used as a validation set to select among different combinations of parameters for text processing and model parameters. Before creating the document vectors, meta-information from the text was removed and Unicode characters were normalized. Pre-trained models were used off the shelf without fine-tuning. Taking into account the deep convolution nature of the models we evaluated embeddings pulled from the last layer as well as from the hidden layers. Max, min and mean pulling were calculated for each embedding. We also checked if normalizing vector components would make a difference. In total, we experimented with 195 types of embeddings per document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proximity Measures</head><p>After obtaining the embeddings for the articles, the proximity between two document vectors is computed. We decided to explore a broad range of proximity measures. Following <ref type="bibr" coords="5,227.13,209.51,49.78,8.74" target="#b10">(Cha 2007)</ref> we experimented with 65 different measures. Taking into account different parameters in some of these measures, in total we experimented with 174 proximity measures.</p><p>The main measures include cosine, Jacard and Minkowsky L p . Given two document vectors α and β of size n, distances from Table <ref type="table" coords="5,396.03,257.33,4.98,8.74" target="#tab_0">2</ref> between α and β are computed as follows:</p><p>cosine distance (inverse of cosine similarity)</p><formula xml:id="formula_0" coords="5,140.99,301.24,300.65,124.18">||α||||β|| α • β = n i=1 α 2 i n i=1 β 2 i n i=1 α i β i -Jaccard distance (inverse of Jaccard simmilarity for dense vecrors) n i=1 (α i -β i ) 2 n i=1 α 2 i + n i=1 β 2 i - n i=1 α i β i -Minkowsky L p distance p n i=1 |α i -β i | p</formula><p>Using different embedding models, pulling methods, output layers and proximity measures gave us a total of 2,677 runs to evaluate. To speed up the experiments of proximity measures were implemented directly in Elasticsearch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Validation</head><p>In order to select the best performing combination of parameters, the 2,677 models were evaluated with the 2018 and 2019 data. To evaluate our models, we generated the top 5 backlinks for each topic and computed nDCG@5 when compared with the 2018 and 2019 datasets. If a generated backlink was not in the validation sets, we assigned it a rank of 0 (equivalent to an irrelevant backlink) to calculate nDCG@5.</p><p>As shown in Table <ref type="table" coords="5,231.02,572.43,3.87,8.74" target="#tab_0">2</ref>, the model with GPT-2 embedding and the Minkovwsky L 3 distance achieved an average nDCG@5 of 0.466, which is higher than GPT-2 without normalization. Doc2Vec on other hand performed better without normalization with an average nDCG@5 of 0.449, compared to an average nDCG@5 of 0.238.</p><p>The top performing embedding algorithm and proximity measure were selected to build the final models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Choice of Runs</head><p>Given our results with the validation set, the top run was chosen to be submitted to this year's shared task, in addition to the classic Okapi BM25, our model from last year, and a combined approach. In the end, the following four runs were submitted: clac gpt2 norm, clac es bm25, clac combined, and clac d2v2019.</p><p>1. clac gpt2 norm is based on GPT-2 (normalized) embeddings. Each article's text was split in chunks 512 words, embeddings were computed and averaged for each article. Minkowski L 3 norm was used for proximity measure. This configuration was the best one among the embedding methods and proximity measures we explored (see Table <ref type="table" coords="6,361.44,390.49,3.87,8.74" target="#tab_0">2</ref>). 2. clac es bm25 is based on the Elasticsearch (Lucene) implementation of the Okapi BM25 ranking algorithm. The entire article text was used as a query. The parameter indices.query.bool.max clause count value was increased to 10240 to allow a longer query article. This approach was selected as it is widely used in the text retrieval field, and its implementation in Elasticsearch already has a reasonable configuration. 3. clac combined is a combination of two previous runs, where the BM25 score between the query and each target document is multiplied by the inverted distance between corresponding GPT-2-embeddings. 4. clac d2v2019 is based on Dov2Vec embeddings computed from News TREC 2019 and cosine similarity as a proximity measure. This model was used because we used this approach last year in our participation to the track, and, for this year, we wished to compare last year's method to novel ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Official Scores</head><p>As indicated in Section 3.4, we have submitted four runs. For each topic, NIST provided us with our official score as well as the minimum, maximum and median scores across all submitted runs. Table <ref type="table" coords="6,300.99,656.12,4.98,8.74">4</ref> shows the overall scores for all topics. As shown in Table <ref type="table" coords="7,203.32,244.69,3.87,8.74">4</ref>, two of our runs outperformed the collective median nDCG@5 of 0.5250. Among our submissions clac es bm25 achieved the highest score with an nDCG@5 of 0.5924, clac combined performed slightly worse reaching an nDCG@5 of 0.5873, while clac gpt2 norm and clac d2v2019 performed below the collective median with nDCG@5 of 0.4541 and 0.4481 respectively. These results are inline with our expectations from our validation runs on the 2019 data (see Section 3.3). Figure <ref type="figure" coords="8,181.54,238.27,4.98,8.74" target="#fig_2">2</ref> shows the scores for each topic, in decreasing order of the median score. Difficult topics for all participants appear on the left-hand side of Figure <ref type="figure" coords="8,472.84,250.22,3.87,8.74" target="#fig_2">2</ref>, while easier topics are on the right-hand side. For a few topics our approach had the lowest performance among all runs (7 topics for clac d2v2019 and clac gpt2 norm, 3 topics for clac combined, and only 2 topics for clac es bm25), they are all situated on the minimum line. On the other hand, for a few other topics, all points are situated on the maximum line (4 topics for clac d2v2019, 6 for clac gpt2 norm and for clac combined, and 8 topics for clac es bm25) hence we achieved the best performance among all News TREC runs on these topics.</p><p>Table <ref type="table" coords="8,176.48,357.88,4.98,8.74">5</ref> shows over all topics the percentage of times that each run was above or equal to the median. Table <ref type="table" coords="8,177.50,468.38,4.13,7.89">5</ref>. The number of topics ranked ≥ or &lt; the overall median for each run</p><p>Figure <ref type="figure" coords="8,182.15,492.32,4.98,8.74" target="#fig_3">3</ref> shows the scores per topic in increasing order of nDCG@5. This figure helps to compare our runs with the collective median and show that clac es bm25 and clac combined, in general, perform better than clac gpt2 norm and clac d2v2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>As shown in Table <ref type="table" coords="8,222.65,578.54,3.87,8.74">4</ref>, clac es bm25 and clac combined are rather similar in terms of overall median nDCG@5, however when looking at individual topics, their results are significantly different. Figure <ref type="figure" coords="8,329.60,602.45,4.98,8.74">4</ref> shows their difference graphically. The model clac combined returned the best result over all runs submitted for the topics #912<ref type="foot" coords="8,206.67,624.79,3.97,6.12" target="#foot_0">1</ref> when clac es bm25 missed the most relevant article<ref type="foot" coords="8,451.54,624.79,3.97,6.12" target="#foot_1">2</ref> . The All our models failed to return any relevant backlinks for topic #907. Only one relevant link was returned on topic #902 by the clac gpt2 norm model. Topics #900, #904, #896 and #888 were also challenging for our systems and for other participants as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Despite being a classic approach to information retrieval, Okapi BM25, good results on the backlinking task, but there is still an opportunity for improvement, since it is not providing the ideal order for backlinks and is missing some articles entirely.</p><p>On the other hand, our approach of using document embeddings based on pre-trained models such as BERT, GPT, GPT-2 or XLNet with various proximity measures is showing potential, even though by itself these performed below the median. In combination with BM25 these showed very similar nDCG@5 but Diffe ence in nDCG@5 nDCG@5(clac-es-bm25) -nDCG@5(clac-combined)</p><p>Fig. <ref type="figure" coords="10,166.60,407.27,4.13,7.89">4</ref>. The difference in nDCG@5 scores for clac es bm25 and clac combined on closer look we can see significant variety on results from clac es bm25 and clac combined. This fact shows a potential step towards returning more diverse backlinks, by relying on different models with different implementations we can return topics potentially not visible to a single model system.</p><p>In this work we did not do any fine-tuning of the models we used. Tuning for our specific dataset and task itself might improve the representation of the documents in vector-space, potentially providing a better ranking for backlinks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,142.38,157.25,8.77;4,134.77,161.84,345.82,8.74;4,134.77,173.79,345.82,8.74;4,134.77,185.75,345.83,8.74;4,134.77,197.70,345.82,8.74;4,134.77,209.66,26.57,8.74"><head>3. 1</head><label>1</label><figDesc>Document Representation After pre-processing, we experimented with six different representations to represent each document: Doc2Vec distributed representation (Le and Mikolov 2014), GPT (A. Radford and Narasimhan 2018), GPT-2 (Alec Radford et al. 2019), XLNet (Yang et al. 2020), BERT (Devlin et al. 2019) and RoBERTa (Liu et al. 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,190.05,515.54,235.27,7.89;4,134.77,241.38,345.84,259.38"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Length distribution of the articles in the collection</figDesc><graphic coords="4,134.77,241.38,345.84,259.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,167.94,629.08,279.47,7.89;7,134.77,354.93,345.84,259.38"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. 2020 results per topic in increasing order of median nDCG@5</figDesc><graphic coords="7,134.77,354.93,345.84,259.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,134.77,389.99,345.83,7.89;9,134.77,400.98,217.18,7.86;9,134.77,115.84,345.84,259.38"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results sorted individually by their nDCG@5. The topics do not necessarily correspond to the data points with the same abscissa.</figDesc><graphic coords="9,134.77,115.84,345.84,259.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,644.16,345.82,20.69"><head>Table 2 .</head><label>2</label><figDesc>Table 3 displays their performance on the 2019 data. Top performing models on the 2019 data</figDesc><table coords="6,190.32,117.75,231.65,110.11"><row><cell>Embedding</cell><cell>Distance</cell><cell>nDCG@5</cell></row><row><cell cols="2">GPT-2 with normalization Minkowsky L3</cell><cell>0.466</cell></row><row><cell>GPT-2</cell><cell>Minkowsky L0.15</cell><cell>0.457</cell></row><row><cell>Doc2Vec</cell><cell>Jaccard</cell><cell>0.449</cell></row><row><cell>GPT</cell><cell>Jaccard</cell><cell>0.440</cell></row><row><cell>Doc2Vec</cell><cell>Cosine</cell><cell>0.428</cell></row><row><cell>XLNet large cased</cell><cell>Minkowsky L0.1</cell><cell>0.354</cell></row><row><cell>XLNet base cased</cell><cell>Minkowsky L0.5</cell><cell>0.337</cell></row><row><cell>RoBERTa base</cell><cell>Minkowsky L0.5</cell><cell>0.323</cell></row><row><cell cols="2">Doc2Vec with normalization Jaccard</cell><cell>0.238</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,233.18,117.75,149.00,106.27"><head>Table 3 .</head><label>3</label><figDesc>Results with the 2019 data</figDesc><table coords="7,251.32,117.75,109.64,85.40"><row><cell>Run</cell><cell>nDCG@5</cell></row><row><cell>clac es bm25</cell><cell>0.5689</cell></row><row><cell>clac combined</cell><cell>0.5668</cell></row><row><cell cols="2">clac gpt2 norm 0.4660</cell></row><row><cell>clac d2v2019</cell><cell>0.4280</cell></row><row><cell>TREC max</cell><cell>0.7737</cell></row><row><cell>TREC median</cell><cell>0.5295</cell></row><row><cell>TREC min</cell><cell>0.1002</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="8,144.73,645.84,264.74,7.86"><p>How to ditch the boring trail mix and eat well while camping out</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="8,144.73,656.80,246.82,7.86"><p>Move beyond skewers and s'mores on your next camping trip</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was financially supported by a grant from the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,134.77,636.27,345.82,8.74;10,149.71,648.22,330.88,8.74;11,149.71,118.99,330.88,9.30;11,149.71,130.95,178.92,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,405.26,636.27,75.33,8.74;10,149.71,648.22,38.76,8.74">2018 News Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec27/papers/Overview-News.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,369.12,648.22,111.47,8.74;11,149.71,118.99,207.02,8.74">The Twenty-Seventh Text REtrieval Conference Proceedings (TREC 2018)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">500</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,134.77,142.90,345.82,8.74;11,149.71,154.86,330.88,9.30;11,149.71,166.81,271.03,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,209.85,142.90,91.25,8.74">News Track Overview</title>
		<ptr target="https://trec.nist.gov/pubs/trec28/papers/OVERVIEW.N.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,327.49,142.90,153.10,8.74;11,149.71,154.86,302.24,8.74">NIST Special Publication 1250: The Twenty-Eighth Text REtrieval Conference Proceedings (TREC 2019)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,134.77,178.77,345.83,8.74;11,149.71,190.72,330.88,9.30;11,149.71,202.68,255.34,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,212.73,178.77,183.38,8.74">News Track Overview (Notebook version)</title>
		<ptr target="https://trec.nist.gov/actpart/conference/papers/OVERVIEW.N.PDF" />
	</analytic>
	<monogr>
		<title level="m" coord="11,424.11,178.77,56.48,8.74;11,149.71,190.72,245.77,8.74">The Twenty-Ninth Text REtrieval Conference (TREC 2020) Notebook</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,134.77,214.64,345.83,8.74;11,149.71,226.59,330.88,8.74;11,149.71,238.55,211.90,9.30" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,379.97,214.64,100.62,8.74;11,149.71,226.59,124.60,8.74">Cumulated Gain-based Evaluation of IR Techniques</title>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,303.72,226.59,176.87,8.74;11,149.71,238.55,56.04,8.74">ACM Transactions on Information Systems (TOIS)</title>
		<idno type="ISSN">1046-8188</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,134.77,250.50,345.82,8.74;11,149.71,262.46,330.88,8.74;11,149.71,274.41,330.88,8.74;11,149.71,286.37,330.88,8.74;11,149.71,298.32,241.63,9.30" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,347.85,250.50,132.74,8.74;11,149.71,262.46,109.52,8.74">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/le14.html" />
	</analytic>
	<monogr>
		<title level="m" coord="11,286.88,262.46,193.71,8.74;11,149.71,274.41,110.35,8.74">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014-06">Jun 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
	<note>Bejing</note>
</biblStruct>

<biblStruct coords="11,134.77,310.28,345.83,8.74;11,149.71,322.23,330.88,9.30;11,149.71,334.19,326.95,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,335.47,310.28,145.13,8.74;11,149.71,322.23,132.89,8.74">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://cdn.openai.com/research-covers/language-unsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct coords="11,134.77,346.14,345.82,8.74;11,149.71,358.10,330.38,9.30;11,149.71,370.05,287.03,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,272.92,346.14,207.67,8.74;11,149.71,358.10,36.06,8.74;11,183.96,370.05,231.79,8.74">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<ptr target="https://cdn.openai.com/better-language-models/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>language models are unsupervised multitask learners. pdf</note>
</biblStruct>

<biblStruct coords="11,134.77,382.01,345.83,8.74;11,149.71,393.96,234.33,9.02" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,256.74,382.01,223.85,8.74;11,149.71,393.96,105.52,8.74">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237[cs.CL</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,134.77,405.92,345.82,8.74;11,149.71,417.87,287.16,9.02" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,261.94,405.92,218.65,8.74;11,149.71,417.87,158.35,8.74">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805[cs.CL</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,134.77,429.83,345.83,8.74;11,149.71,441.78,167.49,9.02" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692[cs.CL</idno>
		<title level="m" coord="11,251.04,429.83,229.55,8.74;11,149.71,441.78,38.58,8.74">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,134.77,453.74,345.83,8.74;11,149.71,465.69,330.88,8.74;11,149.71,477.65,82.66,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,275.77,453.74,204.82,8.74;11,149.71,465.69,213.46,8.74">Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions</title>
		<author>
			<persName coords=""><forename type="first">Sung</forename><forename type="middle">-</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,391.11,465.69,89.48,8.74;11,149.71,477.65,71.58,8.74">Int. J. Math. Model. Meth. Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
