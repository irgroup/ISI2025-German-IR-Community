<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,57.04,84.23,499.09,15.44;1,194.09,104.15,224.32,15.44">Glasgow Representation and Information Learning Lab (GRILL) at TREC 2020 Podcasts Track</title>
				<funder ref="#_8PM9nNy">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.08,130.29,68.61,10.59"><forename type="first">Paul</forename><surname>Owoicho</surname></persName>
							<email>paulowoicho@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.30,130.29,52.42,10.59"><forename type="first">Jeff</forename><surname>Dalton</surname></persName>
							<email>jeff.dalton@glasgow.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,57.04,84.23,499.09,15.44;1,194.09,104.15,224.32,15.44">Glasgow Representation and Information Learning Lab (GRILL) at TREC 2020 Podcasts Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6138428B15A9F50AA559A48CC37AB8C6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we discuss our participation in the Summarization Task of the TREC 2020 Podcasts Track. Our submission consists of summaries generated by (i) an abstractive model based on finetuning T5 on the Spotify Podcasts Dataset, (ii) an ensemble model where the first 15 sentences from the podcast transcript are extracted and passed as input to a fine-tuned T5 model, and (iii) another ensemble model where we use a SpanBERT and K-Means pipeline to extract the 15 most important sentences from the podcast transcript and pass them as input to a fine-tuned T5 model. Official results demonstrate that out of 179 evaluated summaries, our best performing model (ii) generated 42 good-quality summaries -on par with the average across all other submissions. This provides evidence that focusing on the first part of the podcast episode is a strong baseline for podcast summarization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Although podcasts are a fairly new form of audio media, its popularity and rate of consumption has rapidly grown over the years <ref type="bibr" coords="1,283.07,400.69,9.27,7.94" target="#b3">[4]</ref>. To better understand the content within podcasts, the Summarization Task of the TREC 2020 Podcasts Track asked participants to summarise a podcast episode using its audio and/or transcription. Returned summaries were to be grammatically complete and capture the most important parts of the podcast episode. In our participation, we focused on generating summaries for podcast episodes using their transcripts. Our three submissions were generated using summarization models that leveraged a T5 model fine-tuned on the Spotify Podcasts Dataset. For two of our submissions, we first ran each transcript through an extractive pipeline before generating the final summaries with T5. Further details of the task, datasets, baselines, and evaluation methods used are discussed in <ref type="bibr" coords="1,260.30,532.20,9.39,7.94" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODELS AND IMPLEMENTATION</head><p>For our experiments, we studied extractive and abstractive summarization methods. In addition to the official track baselines, the models we experimented with were:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT Extractive Summarizer</head><p>In <ref type="bibr" coords="1,73.40,624.75,9.29,7.94" target="#b2">[3]</ref>, Miller proposes an approach to extractive summarization where a Transformer model is first used to generate text embeddings from the sentences in a source text. Next, these embeddings are passed as input to a K-Means clustering model. The output summary from this pipeline then consists of sentences closest to the cluster centroids found by the K-Means model. This technique was proposed for use in a lecture summarization service where end-users could summarize the content of their lecture transcripts.</p><p>As spoken documents, lectures bear similarities with podcasts, supporting our intuition that Miller's approach was viable for the podcasts dataset. In building this model, we closely followed the project's open-sourced code on Github<ref type="foot" coords="1,458.25,218.84,3.38,6.44" target="#foot_0">1</ref> and leveraged its available APIs. Hyperparameters we could change included the number of sentences (also K-means clusters) in the final summary, the Transformer model used for generating text embeddings, and what layer of the Transformer model to generate the embeddings from. We set the model's output to two sentences in order to standardise the comparison between it and the results of the track's baseline approaches. Furthermore, even though the original pipeline was based on the BERT Transformer model, we examined the results of using SpanBERT in our experiments. We generated text embeddings using the second-to-last layer of our Transformer models and aggregated them using a mean pooling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5</head><p>In <ref type="bibr" coords="1,337.14,374.41,9.27,7.94" target="#b4">[5]</ref>, <ref type="bibr" coords="1,351.36,374.41,39.23,7.94">Raffel et al.</ref> propose T5 based on the results of a large scale evaluation of transfer learning techniques used for NLP tasks. T5 achieves state-of-the-art performance on many NLP tasks including abstractive summarisation. T5 also represents an improvement over the abstractive baseline model for the Summarization task. Summaries generated by the model on the CNN/DailyMail Dataset can be found in <ref type="bibr" coords="1,376.98,440.16,9.39,7.94" target="#b4">[5]</ref>. <ref type="bibr" coords="1,458.19,473.04,10.48,7.94" target="#b5">[6]</ref> due to the lack of Transformer based models with an abstractive text summarisation objective. PEGASUS is a large encoder-decoder model where indicative sentences in a piece of text are first removed and then regenerated from the remaining sentences. Despite being fine-tuned on a small number of samples, PEGASUS achieves state-of-the-art performance as measured by ROUGE scores on 12 summarization tasks. Some sample summaries generated with PEGASUS on a wide range of datasets can be found in <ref type="bibr" coords="1,441.19,560.71,9.39,7.94" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEGASUS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zhang et al. introduce PEGASUS in</head><p>Our implementation of T5 and PEGASUS followed a similar approach. We loaded T5 (t5-base) and PEGASUS (google/pegasus-cnn_dailymail) directly from the HuggingFace library. Both models were fine-tuned on the official training set of 65K podcast episodes using their creator descriptions as training summaries. Important hyperparameters used to train the models were:</p><p>• early stopping: True. This was necessary to prevent the models from overfitting the training summaries. • length penalty 2.0: This was necessary to discourage the models from generating long summaries.</p><p>• max length: 150: This was a stipulation of the maximum length of our models' summaries. We decided on 150 so that the models could generate meaningful summaries under the official 200-character limit. • min length: 30: This was a stipulation of the minimum length of our models' summaries. We hypothesised that 30 characters was the minimum necessary to generate a meaningful summary.</p><p>• no repeat ngram size: 3: This was necessary to discourage the models from generating summaries where it repeated itself. We hypothesised that a trigram was a reasonable threshold for this. • num beams: 5: In beam search, this hyperparameter determines the number of steps that are kept track of while a model generates a sequence. Typically, larger values generate better summaries at the expense of speed. We settled on 5 as this is typically used for text generation tasks. • learning rate: 1e-4: We kept the learning rate low enough to allow our models the chance to converge. • epochs: 3: We trained our models over 3 epochs. As our models were pre-trained, there was no need to fine-tune them over a longer epochs since we could leverage transfer learning.</p><p>We set our hyperparameter values to commonly used defaults without tuning or optimisations. This was because the provided creator summaries varied in quality. Moreover, a single piece of text can be summarised meaningfully in multiple ways. Thus, optimising hyperparameters to make the generated summaries resemble the creator summaries was not worthwhile. We validated our models using the remaining 35K episodes from the podcasts dataset.</p><p>Due to resource constraints, we truncated the input transcripts for both models before summary generation. For T5, this threshold was set to 7000 tokens, and for PEGASUS it was set to 1024. The undesired effect of this was that PEGASUS was only able to summarise the first few sentences of the podcast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble Models</head><p>We experimented with combining our best extractive and abstractive techniques to form an ensemble summariser. These were implemented by first generating a summary with an extractive summariser and passing that as input to an abstractive summariser. Our approach was based on findings in <ref type="bibr" coords="2,199.10,548.04,9.39,7.94" target="#b1">[2]</ref>.</p><p>Our two ensemble models are:</p><p>• First 15 Sentences + T5: For this model, we extract the first 15 sentences of a podcast transcript and pass that as input to a finetuned T5 model. • SpanBert + T5: For this model, we use SpanBERT in the BERT Extractive Summariser pipeline to select the top 15 sentences from a podcast transcript and pass that as input to a finetuned T5 model.</p><p>We use 15 sentences for our experiments with the presumption that it would contain enough context for an abstractive summariser to generate an adequate, meaningful summary. This was due to the results we observed from a simple baseline extractive summariser where we used the first five sentences of a podcast as its summary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Our candidate models were assessed using qualitative and quantitative methods, following which the top three performing models were used to generate summaries on the official evaluation set for our submissions.</p><p>For our assessments, we used the models to generate summaries on the same evaluation set of 150 podcasts used by the track coordinators to assess the task's baseline models <ref type="bibr" coords="2,496.96,202.93,9.52,7.94" target="#b0">[1]</ref>. Our models' summaries were evaluated using the ROUGE metric with the creator descriptions as reference. We also recruited six volunteers, each of whom rated 25 of the 150 summaries using episode titles, creator descriptions, and other metadata for context. Although it would have been better for podcast transcripts to be used for context, the transcripts were lengthy and volunteers were unwilling to read them. Volunteers' ratings were based on the official EGFB scale and demonstrated how well they thought the generated summary conveyed the gist of the podcast based on its metadata. When we compared our volunteers' ratings with those of the official baseline qualitative ratings for the track's baseline models, we observed a Cohen Kappa coefficient of 0.41, indicating a moderate agreement between the two sets of raters. This was due to the variation in the way the evaluations were carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We observed that our participants preferred T5's summaries more often than those of the other models. Furthermore, we noticed that the extractive pipeline had the fewest good-quality summaries of all models, showing that abstractive methods outperform extractive methods for podcast summary generation. For the ensemble models, our assessors preferred the summaries generated by the First 15 Sentences + T5 model over those generated by the SpanBERT + T5 model. While this may be representative of the fact that the first parts of a podcast are its most important for summary generation, it also shows that an effective way to combine extractive and abstractive podcast summarisation methods is by first retrieving indicative sentences from the beginning of a podcast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SUBMITTED RUNS</head><p>Our submitted runs were based on summaries generated by the following models:</p><p>• T5: This model generates a summary using the T5 model fine-tuned on the podcasts dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">NIST EVALUATIONS</head><p>We focus our discussions on the results of our qualitative evaluations since the Summarisation Task was about summary quality and not affinity to the provided creator descriptions. NIST evaluated participants' returned summaries on the official EGFB scale and on 8 yes/no questions discussed in <ref type="bibr" coords="3,235.96,414.19,10.43,7.94" target="#b0">[1]</ref> and itemised below:</p><p>• Q1: Does the summary include names of the main people (hosts, guests, characters) involved or mentioned in the podcast? • Q2: Does the summary give any additional information about the people mentioned (such as their job titles, biographies, personal background, etc)? • Q3: Does the summary include the main topic(s) of the podcast? • Q4: Does the summary tell you anything about the format of the podcast; e.g. whether it's an interview, whether it's a chat between friends, a monologue, etc? • Q5: Does the summary give you mode context on the title of the podcast? • Q6: Does the summary contain redundant information? • Q7: Is the summary written in good English? • Q8: Are the start and end of the summary good sentence and paragraph start and endpoints?</p><p>Over all evaluated summaries, all submissions seemed to struggle with generating good-quality ("Excellent" and "Good" ratings) summaries as shown in Table <ref type="table" coords="3,162.99,646.67,3.05,7.94" target="#tab_0">1</ref>. Our SpanBERT + T5 model generally performed worse than the average of all submissions, while the First 15 Sentences + T5 model was our best performing submission.</p><p>On the 8 yes/no questions, all submissions seemed to perform reasonably well in generating well-structured, grammatically correct summaries and omitting redundant information as shown in Table <ref type="table" coords="3,339.53,356.66,3.04,7.94" target="#tab_1">2</ref>. Our SpanBERT + T5 model however struggled with entity recognition/identification more often than other models, indicating that the 15 important sentences extracted with the extractive pipeline may have not always contained that information. Overall, while all models were fairly able to recognise the topic and format of podcasts, these are areas that need to be explored further in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Behaviour Analysis</head><p>We also examined how our models performed on episodes where most submissions excelled and struggled in. We found that:</p><p>For episode 04Llj5QjDot46TEDGj7lbI, 59% of submissions received an 'Excellent' rating. Our T5 and SpanBERT + T5 models received a 'Fair' rating, while the First 15 Sentences + T5 model received an 'Excellent' rating. The summaries generated by all our models are shown in Table <ref type="table" coords="3,376.11,526.12,3.07,7.94" target="#tab_2">3</ref>.</p><p>Of the three models, the First 15 Sentences + T5 model generates the most detailed summary that captures the most important parts of the episode. Unsurprisingly, this summary was also given a 'yes' on all questions asides question 6. This provides evidence that effective summary generation for a podcast can be done just using the beginning of the podcast. This may also explain the discourse structure of podcasts in which all entities and topics in the podcast are introduced at the start.</p><p>For episode 45hMVOlYDHoX0E92qKjKTW, over 93% of submissions were rated "Bad". Our models also struggled with this episode as they were also rated 'Bad' and received a 'no' on all questions that submissions were evaluated on. The summaries generated by our models are shown in Table <ref type="table" coords="3,432.90,668.59,3.07,7.94" target="#tab_3">4</ref>.</p><p>Evidently, the summaries our models generated are incoherent and repetitive (in the case of the SpanBERT + T5 model), and, being a short episode, we also observed that the summaries generated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5</head><p>This episode is from a conversation I had with Vanessa. We talk about her life and how she was able to get out of the house without being seen again. You can find Vanessa on Instagram at @VanessaJass, youtube.com/VanessaJass or email me at findingperspectivepodcast@gmail.com.  You had five masks ten percent polyester use your best friend as a tester 10 linen. You kept this one locked away hidden and Rib knit 7 to 10. Just a kid get out quick 60 cotton weighted used this at the very bottom, but that last 10% when that's all you sick. What do you hook with pigment died and fries fried that is rayon and crayons your OCD banded your Rainbow of coded sized and alphabetized. Legs Pistons coal powered steam engine freight train that clicker clacker of refrain is so hip jugular hit the lens flipped to admit of is our fish bowl shape distorted Vape the smoke cloud and moved and changed its shape make a defensive Pact. with a world map trace the curvy line across the small of your back.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,317.96,96.58,3.38,6.44"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,333.92,602.55,224.51,74.16"><head>Table 1 :</head><label>1</label><figDesc>Count of summaries from all submissions that received each rating</figDesc><table coords="2,333.92,602.55,224.51,74.16"><row><cell>• First 15 Sentences + T5: This model generates a summary</cell></row><row><cell>of the first 15 sentences of a podcast using the T5 model</cell></row><row><cell>fine-tuned on the podcasts dataset.</cell></row><row><cell>• SpanBERT + T5: This model generates a summary of the 15</cell></row><row><cell>most important sentences in a podcast (determined by the</cell></row><row><cell>SpanBERT + KMeans pipeline) using the T5 model fine-tuned</cell></row><row><cell>on the podcasts dataset.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,126.73,316.76,358.24,7.70"><head>Table 2 :</head><label>2</label><figDesc>Count of summaries from all submissions that received a 'yes' for each question</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,130.19,147.70,352.86,306.41"><head>Table 3 :</head><label>3</label><figDesc>Summaries generated for episode 04Llj5QjDot46TEDGj7lbI</figDesc><table coords="4,130.19,147.70,352.86,180.36"><row><cell></cell><cell>This episode we have Vanessa Jazz. Vanessa is a toronto based fam-</cell></row><row><cell></cell><cell>ily and real estate lawyer, motivational speaker and upcoming au-</cell></row><row><cell></cell><cell>thor. Vanessa has recently launched her own charity called Survivors</cell></row><row><cell>First 15 Sentences + T5</cell><cell>Unleashed. Vanessa shares her incredible story of being coerced into the sex trafficking world in Canada. She also discusses how she was forced into the sex trafficking world as a teenager. Vanessa's website:</cell></row><row><cell></cell><cell>www.survivorsunleashed.com/ Website: www.survivorsunleashed.com</cell></row><row><cell></cell><cell>Facebook: www.facebook.com/groups/findingperspectivepodcast/ Insta-</cell></row><row><cell></cell><cell>gram: @findingperspect.</cell></row><row><cell></cell><cell>This is a story of how I was coerced into the sex trafficking world and what</cell></row><row><cell>SpanBERT + T5</cell><cell>it was like to be in the sex industry. It's not about money, it's about being</cell></row><row><cell></cell><cell>happy. Listen to hear my story!</cell></row><row><cell>Model</cell><cell>Summary</cell></row></table><note coords="4,130.46,346.73,9.52,7.94;4,212.55,335.77,268.99,7.94;4,212.55,346.73,268.99,7.94;4,212.55,357.69,85.72,7.94;4,130.46,374.53,72.96,7.94;4,130.19,385.49,9.52,7.94;4,212.55,369.05,268.99,7.94;4,212.55,380.01,268.99,7.94;4,212.55,390.97,95.25,7.94;4,130.46,424.24,57.78,7.94;4,212.55,402.32,268.99,7.94;4,212.55,413.28,269.22,7.94;4,212.55,424.24,268.99,7.94;4,212.55,435.20,269.16,7.94;4,212.55,446.16,172.78,7.94"><p>T5 This episode is for you if you haven't already. If you haven't already, be sure to download the Spotify app and search for DKM/H or browse podcasts in your library tab. Enjoy! First 15 Sentences + T5 This episode is for you if you haven't already. If you haven't already, be sure to download the Spotify app and search for DKM/H or browse podcast in your library tab. Enjoy! SpanBERT + T5 This episode is for you if you haven't done already. Be sure to download the Spotify app and search for DKM/H or browse podcasts in your library tab. You can also share this episode with your friends on Instagram. If you haven't done already, be sure to download the Spotify app and search for DKM/H or browse podcasts in your library tab.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,53.15,460.68,481.62,248.76"><head>Table 4 :</head><label>4</label><figDesc>Summaries generated for episode 45hMVOlYDHoX0E92qKjKTW by the T5 and the First 15 Sentences + T5 models are the same. We believe this episode may have been challenging because no clear entities or topics were introduced. Furthermore, the GCP-ASR transcription reduced the episode's clarity -even for human readers. This would have negatively impacted the effectiveness of any summarisation model. The transcript for the episode is provided below: Hey there. Thanks for listening to the decaying major podcast. Make sure you follow your favorite podcast. So you never miss an episode like this one or if you become a premium user you can download the episode so you can listen to them offline like I do when you're on a plane or wherever you're traveling and also you can share this with your friends on Instagram. If you haven't done so already be sure to download the Spotify app and search for D. Km/h or browse podcast in your library tab. Control control is the thing wherein I'll catch the conscience of the king control is the Whirlwind wrapped in a puzzle words are the pieces and your mouth the muzzle bumping. Seeped in thick and I see my blood cold heart dicey. That's sodium vapor your heart seemed safer -thin like paper. rolls, like this old sharpened like a fossil whittled like a stick rhubarb crumble sweet and thick Mercury disc bite like allergen opaque like a curtain speckled like a freckle tested like a medal.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,320.88,702.77,170.59,6.18"><p>https://github.com/dmmiller612/bert-extractive-summarizer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,321.00,702.77,106.01,6.18"><p>https://castos.com/podcast-structure/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we discussed our participation in the Summarisation Task of the <rs type="programName">TREC 2020 Podcasts Track</rs>. Our results provide evidence that effective podcast summarisation can be done by only focusing on the beginning parts of the podcast episode. However, areas such as entity recognition and discourse analysis should be investigated for future work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8PM9nNy">
					<orgName type="program" subtype="full">TREC 2020 Podcasts Track</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,330.15,99.78,228.05,6.18;5,330.15,107.75,228.05,6.18;5,329.94,115.70,228.70,6.23;5,330.15,123.67,59.86,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,509.72,107.75,48.48,6.18;5,329.94,115.72,77.09,6.18">Overview of the TREC 2020 Podcasts Track</title>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,422.10,115.70,99.32,6.23">The 29th Text Retrieval Conference</title>
		<imprint/>
	</monogr>
	<note>TREC 2020) notebook. NIST, 2020</note>
</biblStruct>

<biblStruct coords="5,330.15,131.66,228.05,6.18;5,330.15,139.63,144.68,6.18" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,432.79,131.66,125.41,6.18;5,330.15,139.63,125.31,6.18">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName coords=""><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,330.15,147.60,229.13,6.18;5,330.15,155.55,78.53,6.23" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,372.27,147.60,184.31,6.18">Leveraging BERT for extractive text summarization on lectures</title>
		<author>
			<persName coords=""><forename type="first">Derek</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906">1906.04165, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,330.15,163.55,214.56,6.18" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,406.10,163.55,107.58,6.18">Audio on demand: the rise of podcasts</title>
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
			<publisher>Office of Communication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,330.15,171.52,228.05,6.18;5,330.15,179.49,228.19,6.18;5,330.15,187.43,228.82,6.23;5,330.15,195.43,14.51,6.18" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,466.60,179.49,91.73,6.18;5,330.15,187.46,133.74,6.18">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,330.15,203.40,229.23,6.18;5,330.15,211.37,212.51,6.18" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,520.19,203.40,39.19,6.18;5,330.15,211.37,193.10,6.18">Pegasus: Pretraining with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName coords=""><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
