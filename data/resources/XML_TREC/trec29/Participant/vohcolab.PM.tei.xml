<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,205.04,120.17,205.72,10.21;1,215.12,138.10,178.88,10.21;1,394.01,132.69,5.73,8.77">VOH.CoLAB at TREC 2020 Precision Medicine Track ⋆</title>
				<funder>
					<orgName type="full">FCT -Fundação para a Ciência e a Tecnologia</orgName>
				</funder>
				<funder ref="#_4e6QBzq">
					<orgName type="full">DSAIPA project FrailCare.AI</orgName>
				</funder>
				<funder ref="#_cEykhz3">
					<orgName type="full">NOVA LINCS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,209.07,172.73,82.14,8.80"><forename type="first">Miguel</forename><forename type="middle">D</forename><surname>Cardoso</surname></persName>
							<email>msd.cardoso@campus.fct.unl.pt</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">VOH.CoLAB</orgName>
								<orgName type="institution" key="instit2">Universidade NOVA de Lisboa</orgName>
								<address>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade NOVA de Lisboa</orgName>
								<address>
									<settlement>Caparica</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.71,172.73,64.44,8.80"><forename type="first">Flávio</forename><surname>Martins</surname></persName>
							<email>flavio.martins@vohcolab.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">VOH.CoLAB</orgName>
								<orgName type="institution" key="instit2">Universidade NOVA de Lisboa</orgName>
								<address>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade NOVA de Lisboa</orgName>
								<address>
									<settlement>Caparica</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Comprehensive Health Research Centre (CHRC)</orgName>
								<address>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,205.04,120.17,205.72,10.21;1,215.12,138.10,178.88,10.21;1,394.01,132.69,5.73,8.77">VOH.CoLAB at TREC 2020 Precision Medicine Track ⋆</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D6E9F1C9111911217667349C0D8C30F2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Precision Medicine</term>
					<term>Information Retrieval</term>
					<term>TREC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the Scientific Abstracts task of the TREC 2020 Precision Medicine Track. We present our approach and the methods implemented, including both submitted runs and several post-mortem experiments using different methods. We performed experiments with Drugbank-based synonym expansion, Rocchio-based pseudo-relevance feedback, and neural re-ranking using the BioBERT biomedical pre-trained language models. In our evaluation, the Rocchio-based pseudo-relevance feedback method was the best performing method. Finally, we found that metadata and other textual fields in the document (e.g., journal name), are useful for retrieval and, when indexed, can improve recall-oriented metrics considerably leading to improvements in retrieval performance across the board.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Precision Medicine is a medical model that proposes the customization of healthcare, with medical decisions, treatments, practices, or products being tailored to a subgroup of patients, instead of a one-drug-fits-all model. This medical model leverages research on diseases and treatments to promote such customized healthcare to a patient. However, poor accessibility to the growing amount of research prevents a quick and easy use of such information creating obstacles for a successful application of Precision Medicine.</p><p>The TREC Precision Medicine Track (TREC PM), was created to try to close the gap between the conceptualization and the practice of the model. It challenges participants to explore methods that leverage existing document collections and patient information and build search engines to aid clinical staff in the tailoring of patient treatment. TREC has hosted several different challenges in related clinical tracks throughout the years, such as TREC Clinical Decision Support Track (TREC CDS), which ran for three years between 2014-2016, which was the precursor of the TREC Precision Medicine Track (TREC PM) introduced in 2017.</p><p>In the 2020 edition of TREC PM, the challenge is to rank scientific abstracts from PubMed according to their relevance for supporting a given treatment for a given patient's disease and a known gene variant.</p><p>In this paper we describe our two submitted runs to the TREC 2020 PM track. In addition, we present a number of post-mortem experiments and describe our findings with a brief discussion of possible future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Track Description</head><p>In contrast with the 2019 edition, which promoted two tasks, one on Clinical Trials and another on Scientific Abstracts, this years' TREC Precision Medicine track focused only on a Scientific Abstracts task. The overall task formulation is similar to the 2019 edition of the task. However, this year, the challenge was to rank scientific abstracts from PubMed according to their relevance for supporting a given 1) treatment, for a 2) patient's disease, and a 3) known gene variant.</p><p>In order to evaluate the performance of our experimental methods, we dropped the field "treatment" from the 2019 topics. This was necessary for a better fit between the 2019 topics and the new task formulation in 2020 allowing it to be used as a training set and tuning any hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topics</head><p>For the 2020 edition, assessors initially developed a total of 40 topics. Each topic, represented using a XML document, describes a patient using the following structure with three fields: Disease name; Gene affected; Treatment proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scientific Abstracts Task</head><p>The document collection used for the 2020 edition is the same 2018 mid-december snapshot of PubMed abstracts used for the 2019 edition. It contains more than 30M (31,677,119) abstracts. These PubMed abstracts hold the information about its authors, its MeSH terms, the abstract itself and other fields of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Drugbank</head><p>Drugbank is a website where anyone can retrieve information regarding treatments, genes and other multitude of health related concepts. We used the data available from this website and automatically retrieved synonyms, descriptions and indications of the treatments presented in the topics. Ultimately we only used synonyms information, with an average of 1.27 synonyms per treatment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation and Methods</head><p>With the goal of creating a Precision Medicine search engine, we explored several strategies described in this section. We used Elasticsearch as our database to store the documents, and also as our primary search engine, since it has built-in information retrieval methods.</p><p>Elasticsearch was our go-to decision since it is widely used for Information Retrieval and it is open-source. With Elasticsearch as our primary search engine, we then defined strategies to expand the queries but also explored re-rank methods. We used stop words used in the PubMed search engine, pubmed list<ref type="foot" coords="3,436.90,343.06,3.99,6.18" target="#foot_0">4</ref> , and the Galago rmstop list<ref type="foot" coords="3,226.13,355.02,4.23,6.18" target="#foot_1">5</ref> when computing relevance models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hardware</head><p>All of the development occurred in JupyterHub hosted on a server. Furthermore, the dataset and Elasticsearch were also hosted on the same machine with the following hardware specifications:</p><p>Motherboard ASUS P9X79 PRO CPU Intel Core i7-3930K Processor (6 cores, 12 threads, 12M) RAM 64 GB DDR3 (1333Mhz)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collection Pre-processing</head><p>The dataset contains several compressed XML files (.gz), that were parsed and indexed in two different ways into Elasticsearch. We named these indexes, filtered corpus and unfiltered corpus. However, the latter was only created post-mortem. We note that unfiltered corpus index was later implemented due to the poor recall of the methods applied on the filtered corpus index. Thus we speculated that we were removing relevant information.</p><p>The filtered corpus Here we only indexed the abstracts that did not had animal or animals on its mesh terms. This filtering reduced the number of documents to 22,826,528 from 31,677,119, which means that 8,850,591 were documents that related to animals. Besides filtering out, we only used id, title, abstract text, mesh terms and substances fields from the original documents.</p><p>The Elasticsearch documents had the following schema:</p><formula xml:id="formula_0" coords="4,155.34,228.42,46.44,44.80">id text title text body text mesh text</formula><p>The unfiltered corpus Here we did not filter out any documents. We indexed all the text in the PubMed Abstract concatenated into the body field. The total documents present in this index are 31,677,119.</p><p>The Elasticsearch documents had the following schema: id text title text body text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Runs</head><p>We used Okapi BM25 as the retrieval function to get the documents from Elasticsearch. BM25 is a bag-of-words function that ranks a set of documents according to the given query, ignoring the proximity of its terms in the document itself. BM25 Equation (1) makes use of well-known notion of inverse document frequency (IDF) Equation ( <ref type="formula" coords="4,253.50,550.59,4.31,8.80">2</ref>) to score word importance in the collection (rarity).</p><p>We did not perform any type of tuning on its parameters and used the recommended default parameters of k1 = 1.2 and b = 0.75</p><formula xml:id="formula_1" coords="4,176.26,595.37,304.33,73.74">BM25(D, Q) = n i=1 IDF(q i ) • f (q i , D) • (k 1 + 1) f (q i , D) + k 1 • 1 -b + b |D| avgdl (1) IDF(q i ) = ln N -n(q i ) + 0.5 n(q i ) + 0.5 + 1 (2)</formula><p>Run run_bm25 For our baseline, we queried Elasticsearch by all the elements in the presented topic, (treatment, gene, disease). Giving no specific weight to any element of the topic, besides stating that the documents found must have both the gene and disease but also should (not must) have the treatment in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run bm25_synonyms</head><p>This run expands the information of the query with all the synonyms found for given treatment in each topic using Drugbank. The remaining execution of this run is similar to run_bm25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Not Submitted Experiments</head><p>BioBERT We experimented with BioBERT<ref type="foot" coords="5,334.24,258.61,4.35,6.18" target="#foot_2">6</ref>  <ref type="bibr" coords="5,342.42,260.12,11.12,8.80" target="#b2">[3]</ref> which is a fine-tuned version of the well-known language model Bert <ref type="bibr" coords="5,317.07,272.07,10.44,8.80" target="#b1">[2]</ref>. BioBERT was fined-tuned using PubMed abstracts, therefore, the use of this language model apriori appeared to be quite useful for this task. First, we used this pre-trained model to compute both the embeddings of the retrieved documents and the query so that can we later use them. These initial documents were retrieved using the same method as in run_bm25. Here we experimented with up to 10000 initial retrieved documents, starting at 1000 with a step of 250. The embeddings were used to re-rank the retrieved documents according to their cosine similarity to the query. Afterwards, we decided to linearly combine the bm25 scores that elastic search produces, and the cosine similarity scores. The BM25 scores had to be normalized between [0, 1] since the cosine similarity also ranges from [0, 1]. We used this final combined score to re-rank the retrieved documents.</p><p>In order to get the weights for the linear combination, we performed a linear search starting with α = 0, increasing it by 0.1 in each step, totaling in 10 steps. At each step we validated the results against the 2019 relevance judgments. The parameter α defines the weight given to the cosine similarity score. Henceforth, the higher the α, the higher the weight given to BioBERT cosine similarity between the query and the given document.</p><p>We optimized the weights and number of documents retrieved based on Rprec, map and num_rel_ret, in this exact order. We found that α = 0.4 and a number of initial retrieved documents over 2000, those metrics were optimal.</p><p>In this experiment, we also tested different ways of building the query inspired by Dai et al. <ref type="bibr" coords="5,189.93,535.27,9.13,8.80" target="#b0">[1]</ref>. However, we did not notice any improvement. We also attempted to re-rank the documents based on the top-k initially retrieved documents and not only on the query, which also did not yield any improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rocchio Expansion</head><p>We leverage the Rocchio formula as seen in Eq. ( <ref type="formula" coords="5,449.92,589.31,4.45,8.80" target="#formula_2">3</ref>)</p><formula xml:id="formula_2" coords="5,153.14,609.72,327.45,36.36">⃗ Q m = (α • ⃗ Q o ) +   b • 1 |D r | • ⃗ Dj ∈Dr ⃗ D j   -   c • 1 |D nr | • ⃗ D k ∈Dnr ⃗ D k   ,<label>(3)</label></formula><p>where Q o , D r , D n r are the original query, top documents and bottom documents, respectively. In order to get D j and D k , we retrieved two sets of 1000 documents using the methods described in Section 3.3. We then selected the top 100 and bottom 5 documents, defined the parameters a,b and c, to compute Q m .</p><p>Having ⃗ Q m calculated, we retrieved the top 75 terms of the query with the highest score, joined all of those terms into a query boosting each term with its score and queried Elasticsearch once again, retrieving another 1000 documents. We fined tuned Equation (3) parameters a,b and c using the 2019 results. We used a = 6, b = 5 and c = 5 as the final parameters.</p><p>Rocchio Rerank Alternatively to Section 3.4, after the initial retrieval and consequent ⃗ Q m computation, here we decided to use rocchio to re-rank the initially retrieved documents. In this experiment we used Q m as the ground truth vector and computed the similarity scores of all documents against this vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RM3 Expansion</head><p>For this experiment we created a feedback relevance model using the top 20 documents and then selected the top 20 terms. Having these 20 terms, we normalized their weights between [0,1], and queried Elasticsearch once again, with all of these 20 terms boosting them with their respective weights. This feedback relevance model, P (w | θ F ), is computed using the following equation:</p><formula xml:id="formula_3" coords="6,213.93,388.25,266.66,54.30">P (w | θ F ) ∝ d∈R P (w | d) • P (q | d) • P (d) document score (4) ∝ 1 |R| d∈R P (w | d) • BM25(q, d),</formula><p>where R is the top N relevant documents and BM25(q, d) is the BM25 score returned by Elasticsearch for document d matching query q. The relevance model P (w | R) ≈ P (w | θ F ) for query q is a weighted average of the terms in the top documents retrieved, where the weights are the BM25 scores for the query q. After Eq. ( <ref type="formula" coords="6,183.26,500.32,4.34,8.80">4</ref>) is computed, we interpolate the original query model weights, θ q , with the estimated relevance model, θ F , using parameter λ = 0.5 as follows,</p><formula xml:id="formula_4" coords="6,205.18,532.40,204.99,9.71">P (w | θ q ′ ) = λ • P (w | θ q ) + (1 -λ) • P (w | θ F ).</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section we display the benchmarks of all of our methods against the judgments of the 2020 edition. On this edition, the evaluation was divided into two phases. The phase 1 judgments evaluate if the documents retrieved are on the precision medicine topic and how much they match towards the query tokens individually. While the phase 2 judgments evaluate if the documents retrieved are relevant towards the relevant query as an whole. The phase 2 judgments were constructed by manually labeling the documents up to 100 documents by topic. As seen in Table <ref type="table" coords="7,223.39,452.99,5.34,8.80" target="#tab_1">2</ref> none of our runs and post-mortem experiences were above the median using the filtered corpus index. However, when we used the unfiltered corpus index, the rocchio based method showed better results. However, in phase 2 as shown in Table <ref type="table" coords="7,226.80,488.86,4.27,8.80" target="#tab_2">3</ref>, the better results were on the filtered corpus index. In addition we can also note that the Rocchio methods outperforms RM3 relevance model in all collections and displayed metrics.</p><p>The synonym-based query expansion methods always yields worse results due to the fact that some of the synonyms presents on Drugbank are their chemical names. For example, Alectinib had the synonym 6-dimethyl-8-[4-(morpholin-4yl)piperidin-1-yl]-11-oxo-6, which after tokenization creates unnecessary tokens that promote worse results. In the previous results tables we decided to not show the results of these methods since run_bm25_syn already performs worse than run_bm25. Thus, applying rocchio cosine or expansion, RM3 or BioBERT methods would consequently yield poorer results than the same methods that used the run_bm25 as the first step.</p><p>It is a common pattern in Information Retrieval to expand the information presented in the query or even in the text to retrieve before storing it. However, in our early experiments with this approach we did not see any improvement on the results. Henceforth, we did not explore further besides expanding the query with synonyms of the treatments. In this experiments we also concluded that big queries slow down the system significantly. Expanding the document itself with related data from SNOMED CT<ref type="foot" coords="8,277.73,154.29,3.88,6.18" target="#foot_3">7</ref> or other healthcare sources at indexing time, instead of expanding the query, may yield a more efficient system.</p><p>Initially, we hypothesized that pre-filtering the corpus could help and make the search more effective. However, this was not the case, as shown in Table <ref type="table" coords="8,473.65,194.26,4.16,8.80" target="#tab_1">2</ref>, where we were merely interested on how well the system matched the topics as individual tokens. We believe that in phase 1, the filtering removed metadata that could have been helpful. In phase 2 results, shown in Table <ref type="table" coords="8,410.67,230.13,4.11,8.80" target="#tab_2">3</ref>, the behaviour was mostly similar, except on the non-submitted experiments rocchio_exp and RM3, where the filtered corpus showed a slight improvement. The reason for these results deserves further investigation, nonetheless, we suspect that it is mostly due to the relevance judgments two-phase annotation model of the task. For phase 2, accessors labeled the documents manually by reading them, thus the documents were labeled solely by their content, which is the same information that the methods applied for the filtered corpus had access to.</p><p>In addition, we experimented with BioBERT but it did not yield good results when compared to the Rocchio Expansion method. BioBERT has a maximum limit of context it can work with, which is a limiting factor when working with long documents. We did not employ any method to compensate this limitation and just cropped the document so that it could fit in BioBERT's context window. Furthermore, in our experiments, BioBERT promotes BM25 low-scoring documents, which ultimately yields poorer results if we increase the number of re-ranked documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Finally, we conclude that in this multi-stage retrieval pipeline, the initial method of retrieval is of the utmost importance. Most of the methods that we employed a the second stage of retrieval, especially Rocchio and other pseudo-relevant feedback methods, require a good first stage retrieval to get an initial set of useful and relevant documents. Therefore, to improve the chances of the second stage method, the first retrieval should be tuned for P5, P10, and P20. In addition, we also conclude that no pre-filtering should be done since there is meta-data and other fields that are relevant as they improve the results.</p><p>As future work we look towards document expansions methods at indexing time and adapting BioBERT methods to longer documents. We will explore methods making use of medical knowledge-bases as well as semantic search using neural-based retrieval methods to improve the second stage retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,136.16,118.92,310.79,73.36"><head>Table 1 .</head><label>1</label><figDesc>TREC PM 2020 Scientific Abstracts task example topics.</figDesc><table coords="2,136.16,139.34,310.79,52.94"><row><cell></cell><cell>Patient 1</cell><cell>Patient 2</cell><cell>Patient 3</cell></row><row><cell>Disease</cell><cell cols="3">colorectal cancer non-small cell carcinoma acute myeloid leukemia</cell></row><row><cell>Gene</cell><cell>ABL1</cell><cell>ALK</cell><cell>ALK</cell></row><row><cell cols="2">Treatment Regorafenib</cell><cell>Alectinib</cell><cell>Gilteritinib</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,144.81,118.92,317.53,146.24"><head>Table 2 .</head><label>2</label><figDesc>TREC PM 2020 Scientific Abstracts task Phase 1 results.</figDesc><table coords="7,144.81,141.28,317.53,123.88"><row><cell></cell><cell cols="2">filtered corpus</cell><cell cols="2">unfiltered corpus</cell></row><row><cell></cell><cell>infNDCG P10</cell><cell>Rprec</cell><cell>infNDCG P10</cell><cell>Rprec</cell></row><row><cell>median</cell><cell cols="4">0.4316 0.4645 0.3259 0.4316 0.4645 0.3259</cell></row><row><cell>run_bm25*</cell><cell cols="4">0.3587 0.4452 0.2521 0.4538 0.5000 0.3486</cell></row><row><cell>run_bm25_syn*</cell><cell cols="4">0.2357 0.2839 0.1706 0.3587 0.3677 0.2862</cell></row><row><cell>rocchio_cosine</cell><cell cols="4">0.3567 0.4452 0.2478 0.4538 0.5000 0.3486</cell></row><row><cell>rocchio_exp</cell><cell cols="4">0.3941 0.4710 0.2892 0.4843 0.5032 0.3698</cell></row><row><cell cols="5">BioBERT_40%_2000 0.3567 0.4452 0.2477 0.4518 0.5000 0.3486</cell></row><row><cell>RM3</cell><cell cols="4">0.3024 0.3742 0.2148 0.4048 0.4226 0.2803</cell></row><row><cell>* official runs</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,144.81,280.09,301.76,146.24"><head>Table 3 .</head><label>3</label><figDesc>TREC PM 2020 Scientific Abstracts task Phase 2 results.</figDesc><table coords="7,144.81,302.45,301.76,123.88"><row><cell></cell><cell cols="2">filtered corpus</cell><cell cols="2">unfiltered corpus</cell></row><row><cell></cell><cell>ndcg@30</cell><cell>ndcg@5</cell><cell>ndcg@30</cell><cell>ndcg@5</cell></row><row><cell>median</cell><cell>0.2857</cell><cell>0.2529</cell><cell>0.2857</cell><cell>0.2529</cell></row><row><cell>run_bm25*</cell><cell>0.3009</cell><cell>0.2706</cell><cell>0.3042</cell><cell>0.2612</cell></row><row><cell>run_bm25_syn*</cell><cell>0.2476</cell><cell>0.2242</cell><cell>0.2701</cell><cell>0.2249</cell></row><row><cell>rocchio_cosine</cell><cell>0.3039</cell><cell>0.2706</cell><cell>0.3042</cell><cell>0.2612</cell></row><row><cell>rocchio_exp</cell><cell>0.3332</cell><cell>0.3045</cell><cell>0.3228</cell><cell>0.2714</cell></row><row><cell cols="2">BioBERT_40%_2000 0.3019</cell><cell>0.2706</cell><cell>0.3042</cell><cell>0.2612</cell></row><row><cell>RM3</cell><cell>0.2731</cell><cell>0.2597</cell><cell>0.2602</cell><cell>0.2391</cell></row><row><cell>* official runs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,144.73,636.36,315.39,7.62"><p>https://github.com/igorbrigadir/stopwords/blob/master/en/pubmed.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,144.73,647.40,335.89,7.62;3,144.73,658.36,14.12,7.62"><p>https://github.com/igorbrigadir/stopwords/blob/master/en/galago_rmstop. txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="5,144.73,658.36,165.26,7.62"><p>https://github.com/dmis-lab/biobert</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="8,144.73,659.69,101.68,6.47"><p>http://www.snomed.org/</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work is supported by <rs type="funder">DSAIPA project FrailCare.AI</rs> (<rs type="grantNumber">DSAIPA/0106/2019/02</rs>) and by <rs type="funder">NOVA LINCS</rs> (<rs type="grantNumber">UIDB/04516/2020</rs>) with the financial support of <rs type="funder">FCT -Fundação para a Ciência e a Tecnologia</rs>, through national funds.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4e6QBzq">
					<idno type="grant-number">DSAIPA/0106/2019/02</idno>
				</org>
				<org type="funding" xml:id="_cEykhz3">
					<idno type="grant-number">UIDB/04516/2020</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,139.75,143.84,340.84,8.80;9,148.13,155.80,332.47,8.80;9,148.13,167.75,333.85,8.80;9,148.13,180.45,214.45,8.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,233.78,143.84,246.81,8.80;9,148.13,155.80,82.20,8.80">Deeper text understanding for ir with contextual neural language modeling</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331303</idno>
		<ptr target="http://dx.doi.org/10.1145/3331184.3331303" />
	</analytic>
	<monogr>
		<title level="m" coord="9,244.05,155.80,236.54,8.80;9,148.13,167.75,284.11,8.80">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,139.03,191.66,341.56,8.80;9,148.13,203.62,266.32,8.80" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,367.93,191.66,112.66,8.80;9,148.13,203.62,169.08,8.80">Bert: Pre-training of deep bidirectional transformers for language</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.95,215.57,341.63,8.80;9,148.13,227.53,334.13,8.80;9,148.13,239.48,332.47,9.16;9,148.13,252.19,31.38,8.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,428.47,215.57,52.12,8.80;9,148.13,227.53,334.13,8.80;9,148.13,239.48,11.80,8.80">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j" coord="9,167.82,239.48,62.06,8.80">Bioinformatics</title>
		<imprint>
			<biblScope unit="issue">09</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
