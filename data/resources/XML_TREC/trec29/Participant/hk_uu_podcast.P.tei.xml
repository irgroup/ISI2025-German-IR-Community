<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.35,87.50,405.29,23.25;1,135.34,118.88,341.79,23.25;1,255.06,150.27,101.88,23.25">Abstractive Podcast Summarization using BART with Longformer attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.97,183.40,99.16,11.50"><forename type="first">Hannes</forename><surname>Karlbom</surname></persName>
							<email>hannes.karlbom@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.35,183.40,73.73,11.50"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
							<email>aclifton@spotify.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,103.35,87.50,405.29,23.25;1,135.34,118.88,341.79,23.25;1,255.06,150.27,101.88,23.25">Abstractive Podcast Summarization using BART with Longformer attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9C4AA38906DEE7E998F4CD8CBB098F7B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our model submitted to the TREC (Text REtrieval Conference) summarization part of the Podcasts track 2020 edition. The goal of this task is to summarize podcast episodes using 100k open-domain podcast transcripts. The challenge lies in the long length of the transcript documents, diverse structures of the podcast format and that neither the creator descriptions nor the transcripts are a perfect gold truth of an episode. We propose a combined model that tackles the length challenge, by using a drop in replacement of the Longformer attention mechanism in a pre-trained BART model and fine-tuning the model on the podcasts dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>Podcasts have become an increasingly popular form of audio media, and they represent a rich potential source of data. Thus, the TREC podcasts dataset and shared task was created with the purpose of better understanding podcast content. The shared task was split into two objectives, summarization and segment retrieval <ref type="bibr" coords="1,133.05,500.51,89.86,9.58" target="#b1">(Clifton et al., 2020)</ref>. The overview paper provided by the organizers of this years podcasts track gives a more in depth look at the tasks and the dataset <ref type="bibr" coords="1,204.56,538.17,75.34,9.58">(Jones et al., n.d.)</ref>.</p><p>This work focuses on the summarization task. In particular, we train deep neural abstractive summarization models on the largescale set of podcast transcripts. Neural summarization models are typically trained on relatively short documents, such as news articles; podcast transcripts, however, tend to be significantly longer (often several thousand tokens long). Thus, popular approaches to neural abstractive summarization do not tend to scale to the podcast transcripts in this dataset. Our approach uses a combination of well es-tablished a seq2seq models to overcome this length challenge and generate summaries that can meaningfully take into account the entire input transcript document.</p><p>In recent years neural models in Natural Language Processing have greatly improved state of the art by pre-training on language modeling tasks and using the attention mechanism to better generate tokens with respect to the input context. To create a model for summarizing the podcast transcripts we use <ref type="bibr" coords="1,459.66,539.55,60.53,9.58;1,315.96,552.10,47.54,9.58">BART (Lewis et al., 2019)</ref>, a model which has been shown to have state of the art performance on other summarization tasks such as the CNN/DailyMail dataset <ref type="bibr" coords="1,352.19,589.76,102.40,9.58">(Hermann et al., 2015)</ref>, and we finetune it on the podcast transcript dataset. To tackle the length challenge of the input documents, we extend the model by replacing the attention layers with attention mechanism used in the Longformer <ref type="bibr" coords="1,399.09,652.53,122.36,9.58;1,315.96,665.08,21.29,9.58" target="#b0">(Beltagy, Peters, and Cohan, 2020)</ref>, which uses a combination of global and local windowed attention that scales linearly with the input sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Background i. Previous Research</head><p>There are numerous models for summarization <ref type="bibr" coords="2,112.02,151.69,81.25,9.58">(Lewis et al., 2019;</ref><ref type="bibr" coords="2,196.16,151.69,68.31,9.58">Yan et al., 2020;</ref><ref type="bibr" coords="2,267.36,151.69,28.68,9.58;2,91.80,164.25,49.04,9.58">Zhang et al., 2019;</ref><ref type="bibr" coords="2,143.72,164.25,78.49,9.58">Raffel et al., 2019)</ref> and summarization of long texts, however none of them have been a applied to spoken language transcripts. The methods for dealing with longer inputs range from using extractive methods first and the summarizing the result (Liu and Lapata, 2019) whilst others use a divide and conquer approach (Gidiotis and Tsoumakas, 2020).</p><p>Recent work aimed at dealing with problem of long input texts for transformers has seen multiple approaches that have been shown to improve the computational complexity of the attention mechanism and at the same time keeping not losing predictive performance <ref type="bibr" coords="2,279.80,327.43,13.42,9.58;2,91.80,339.99,75.18,9.58">(Zaheer et al., 2020;</ref><ref type="bibr" coords="2,171.10,339.99,126.18,9.58;2,91.80,352.54,22.86,9.58" target="#b0">Beltagy, Peters, and Cohan, 2020;</ref><ref type="bibr" coords="2,118.04,352.54,159.36,9.58">Kitaev, Kaiser, and Levskaya, 2020;</ref><ref type="bibr" coords="2,280.77,352.54,15.66,9.58;2,91.80,365.09,48.12,9.58">Tay et al., 2020)</ref>.</p><p>In this work, we combine some of the these improvements in attention computation with a state of the art model for abstractive summarization, starting from a pre-trained model and directly finetuning on the podcast dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ii. BART &amp; Longformer</head><p>The basis for the model we used is a large BART model trained for summarization on the CNN/DailyMail dataset. In this way, the model can leverage the significantly larger CNN/DailyMail dataset to learn the summarization task before adapting to the spoken language podcast transcript domain. BART is a denoising autoencoder for training sequence-to-sequence models and has been shown to be effective for finetuning on text generation generation tasks such as summariazation <ref type="bibr" coords="2,127.03,614.87,80.51,9.58">(Lewis et al., 2019)</ref>.</p><p>Longformer, or the "The long document transformer", is a model which aims to address the problem of quadratic time complexity in the attention mechanism which many transformer models, including BART, suffer from. This attention scaling limits the how long input sequences models can handle and therefore could possibly include in a summary.</p><p>The attention mechanism in the Longformer model offers a drop-in replacement for the standard self-attention by using a local windowed attention with the option to introduce global attention to important tokens as well. The windowed attention scales linearly with the input sequence length allowing for much longer input lengths <ref type="bibr" coords="2,368.97,232.17,148.68,9.58" target="#b0">(Beltagy, Peters, and Cohan, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Combined Model</head><p>Our combined Longformer-BART model is initialized from an already trained checkpoint of BART and replaces the attention layers in the BART model with the Longformer attention layers. In the original Longformer paper, the model is created starting from a RoBERTa checkpoint's pretrained weights. In order to capture much longer sequences, instead of randomly initializing new position embeddings in the Longformer, it reuses RoBERTa's learned absolute position embeddings by copying the position embeddings multiple times.</p><p>The authors show that BERT's attention heads have a strong learned bias toward attending to local context and therefore initialization by copying keeps this local structure everywhere except at the partition boundaries <ref type="bibr" coords="2,368.17,501.51,147.74,9.58" target="#b0">(Beltagy, Peters, and Cohan, 2020)</ref>.</p><p>We use the same method for BART, copying the positional embeddings 4 times, with a max position of 1024, thus reaching a max input sequence length of 4096, which we use when finetuning the model on the podcast transcripts.</p><p>Parallel and independent to this work the authors of the Longformer paper have released a revision of the paper where they introduce the Longformer-Encoder-Decoder (LED) <ref type="bibr" coords="2,498.22,627.42,19.70,9.58;2,315.96,639.97,126.87,9.58" target="#b0">(Beltagy, Peters, and Cohan, 2020)</ref>. The LED model uses the same method as we have have in the combined model and manages to achieve state of the art performance on the arXiv summarization dataset <ref type="bibr" coords="2,386.89,690.18,84.52,9.58">(Cohan et al., 2018)</ref> with a 16k token input length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Methodology i. Data Preprocessing</head><p>The creator descriptions are noisy and are often too short, too long or do not capture what is relevant to the episodes. In addition, many descriptions contain boilerplate texts such as which platform was used to upload the episode. To improve our supervised fine-tuning, we have done the following:</p><p>• We removed examples where creator descriptions are either too long or too short with the boundary conditions set to between 10 and 1300 characters.</p><p>• We applied a TF-IDF vectorization of the descriptions which were compared to each other using the cosine distance. Any data points with too similar descriptions (threshold 0.95) were filtered out.</p><p>• Finally a BERT <ref type="bibr" coords="3,195.19,416.38,101.85,9.58">(Devlin et al., 2018)</ref> based sentence classifier was used identify and remove boilerplate sentences. The boilerplate classifier was trained using a small set of 1000 manually labelled episodes <ref type="bibr" coords="3,153.03,479.15,83.48,9.58">(Reddy et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ii. Experimental setup</head><p>To create our models, we combined the Huggingface transformers library <ref type="bibr" coords="3,246.00,547.06,51.29,9.58;3,91.80,559.61,23.71,9.58">(Wolf et al., 2019)</ref> with the Longformer attention source code 1 . The initial BART model's starting point was "facebook-bart-larg-cnn" 2 .</p><p>The models where trained using 4 x NVIDIA Tesla P100. We used a validation set of 1000 episodes, randomly sampled after the prepossessing step, to do early stopping and picking of the final model.</p><p>1 https://github.com/allenai/longformer 2 https://huggingface.co/facebook/bart-large-cnn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Results</head><p>The submissions were ranked according to a four-point quality score and an additional eight yes/no questions to further estimate the quality of the summary. The following descriptions of the quality score and yes/no questions were sent to the evaluators and act as a reference points for the results in the graphs.</p><p>Four-point quality scoring:</p><p>(3) Excellent: The summary accurately conveys all the most important attributes of the episode, which could include topical content, genre, and participants. In addition to giving an accurate representation of the content, it contains almost no redundant material which is not needed when deciding whether to listen. It is also coherent, comprehensible, and has no grammatical errors.</p><p>(2) Good: The summary conveys most of the most important attributes and gives the reader a reasonable sense of what the episode contains with little redundant material which is not needed when deciding whether to listen. Occasional grammatical or coherence errors are acceptable.</p><p>(1) Fair: The summary conveys some attributes of the content but gives the reader an imperfect or incomplete sense of what the episode contains. It may contain redundant material which is not needed when deciding whether to listen and may contain repetitions or broken sentences.</p><p>(0) Bad: The summary does not convey any of the most important content items of the episode or gives the reader an incorrect or incomprehensible sense of what the episode contains. It may contain a large amount of redundant information that is not needed when deciding whether to listen to the episode.</p><p>Yes/No Questions:</p><p>Q1: Does the summary include names of the main people (hosts, guests, characters) involved or mentioned in the podcast?</p><p>Q2: Does the summary give any additional information about the people mentioned (such as their job titles, biographies, personal background, etc)?</p><p>Q3: Does the summary include the main topic(s) of the podcast?</p><p>Q4: Does the summary tell you anything about the format of the podcast; e.g. whether it's an interview, whether it's a chat between friends, a monologue, etc?</p><p>Q5: Does the summary give you more context on the title of the podcast?</p><p>Q6: Does the summary contain redundant information?</p><p>Q7: Is the summary written in good English?</p><p>Q8: Are the start and end of the summary good sentence and paragraph start and end points?</p><p>We submitted one run referred to as Long-formerBart in Figure <ref type="figure" coords="4,192.05,266.10,5.08,9.58" target="#fig_0">1</ref> and Figure <ref type="figure" coords="4,256.13,266.10,3.81,9.58" target="#fig_1">2</ref>, which show the model's performance in comparison to the two creator descriptions (filtered and non-filtered). Figure <ref type="figure" coords="4,194.63,303.76,5.08,9.58" target="#fig_0">1</ref> displays a count of episode summaries for each quality rank totaling 179 episodes. Figure <ref type="figure" coords="4,214.68,328.86,5.02,9.58" target="#fig_1">2</ref> shows a percentage of episode summaries which received the answer yes to each respective question.</p><p>We see from Figure <ref type="figure" coords="4,188.78,373.98,4.88,9.58" target="#fig_0">1</ref> that the filtered creator descriptions were not always better than the unfiltered, and that our model's summaries were often better than both types of creator descriptions, with fewer bad outputs and more good/excellent outputs.</p><p>Table <ref type="table" coords="4,117.20,456.75,4.88,9.58">1</ref> shows the average ROUGE-L scores for recall, precision and f-measure for our model's outputs against the noisy creator summaries, along with values for the 95% confidence interval.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Conclusion and Discussion</head><p>This paper describes our submission to the summarization task of the TREC 2020 podcast track in which we produced summaries for podcast episodes using the transcribed texts as input. Our method uses the summarization state of the art seq2seq model BART and extends it with a sparse attention mechanism from the Longformer to be able to take into account much long input sequences. Our approach copies the learned embeddings in the BART model to the Longerformer attention layers and finetunes the model on the podcast dataset. This method is effective at capturing the entirety of the long input sequences and thus outperforms the descriptions made by the podcast creators themselves when evaluating over both the question set and the quality score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,91.80,673.74,204.24,9.55;4,91.80,686.29,138.17,9.50;4,91.80,522.22,204.24,139.67"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Quality scores reported for the run in comparison to creator descriptions</figDesc><graphic coords="4,91.80,522.22,204.24,139.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,315.96,233.21,204.24,9.55;4,315.96,245.76,162.53,9.50;4,315.96,90.71,204.24,130.66"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of yes answers per questions with creator descriptions for comparison</figDesc><graphic coords="4,315.96,90.71,204.24,130.66" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="4,315.96,555.34,205.90,9.58;4,325.93,567.89,195.94,9.58;4,325.93,580.31,194.27,9.71;4,325.93,592.87,75.55,9.50" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,418.95,567.89,102.91,9.58;4,325.93,580.44,102.33,9.58">Longformer: The longdocument transformer</title>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,315.96,605.55,205.48,9.58;4,325.54,618.10,195.91,9.58;4,325.62,630.52,194.59,9.50;4,325.93,643.08,170.95,9.50" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,442.23,605.55,79.21,9.58;4,325.54,618.10,169.63,9.58">100,000 Podcasts: A Spoken English Document Corpus</title>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,325.62,630.52,194.59,9.50;4,325.93,643.08,165.69,9.50">Proceedings of the 28th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 28th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
