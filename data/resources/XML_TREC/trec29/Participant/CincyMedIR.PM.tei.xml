<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,73.53,87.28,464.90,16.00;1,116.30,106.80,378.98,16.00">Retrieving Scientific Abstracts using Medical Concepts and Learning to Rank: CincyMedIR at TREC 2020 Precision Medicine Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.83,139.55,59.87,12.00"><forename type="first">Piyush</forename><surname>Sahu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cincinnati College of Medicine</orgName>
								<address>
									<settlement>Cincinnati</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.59,139.55,48.38,12.00"><forename type="first">Hoang</forename><surname>Vu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cincinnati College of Medicine</orgName>
								<address>
									<settlement>Cincinnati</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.62,139.55,124.80,12.00"><roleName>PhD, MSI</roleName><forename type="first">Danny</forename><forename type="middle">T Y</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cincinnati College of Medicine</orgName>
								<address>
									<settlement>Cincinnati</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,73.53,87.28,464.90,16.00;1,116.30,106.80,378.98,16.00">Retrieving Scientific Abstracts using Medical Concepts and Learning to Rank: CincyMedIR at TREC 2020 Precision Medicine Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8904BFA5660BA3AB900FE9C8FC7C5EC1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The CincyMedIR team led by Dr. Danny T.Y. Wu at the University of Cincinnati College of Medicine (UC CoM) participated in the Text Retrieval Conference 2020 Precision Medicine Track (TREC-PM). CincyMedIR only worked on the scientific abstracts this year with two main objectives: 1) to experiment learning to rank (LTR) models, a supervised machine-learning approach to adjust ranking based on the text features in the relevant documents, and 2) to develop a configurable pipeline for TREC-like tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CincyMedIR continued using ElasticSearch (ES) as the information retrieval (IR) platform. Through Python 3.6 and ES Application Programming Interfaces (APIs), an IR pipeline was developed and tested. Our team has been using the same set of tools for previous TREC tasks and gained working knowledge. In terms of the IR pipeline, first, all scientific abstracts were downloaded from the website and indexed using ES on Amazon Web Services (AWS). Second, the queries were formed by combining the disease and gene terms in the topic files (no treatment terms). No query expansion or modifications were done due to the limited improvement shown in our previous experiments. Third, the top 2,000 documents were retrieved for each query in TREC-PM 2020 and 2019 and combined with the annotated documents in the evaluation (qrel) files in 2019 and 2018. Fourth, these documents were parsed by Metamap to get their medical concepts; the terms and concepts were indexed for LTR. Then, eight LTR models were trained on 2018 documents and validated on the 2019 documents. Next, the top 1,000 relevant documents for each query were retrieved using BM25 and the best performed LTR model (random forest). In this step, queries may be modified to include the treatment terms. Lastly, five runs were generated for submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Our team implemented a configurable pipeline with multiple steps to conduct the experiment for TREC-like tasks. The steps included index preparation, query preparation, metamap parsing, LTR model training and validation, search retrieval, and evaluation on qrels files. The pipeline was able to support the experiment this year in an efficient manner. Table <ref type="table" coords="1,101.26,614.70,6.08,12.00" target="#tab_0">1</ref> shows that our submitted runs did not performed well. None of their Rprec, Precision at 10 (P_10) and infNDCG were above the median scores of all teams. The best performed run (dgt_trec_eval) did not use LTR nor re-rank methods at all but only added the treatment terms in the queries in a later step. We suspect that the poor performance resulted from the exclusion of treatment terms in the beginning of the process. Even though the re-rank methods considered the treatment terms, they did not improve the performance much. Therefore, we created another scenario by including the treatment terms in the beginning and generated additional four runs. The re-ranking was changed from term-based to concept-based methods, which were proven to be effective in the last year's runs. The results show that one of the addition runs was able to achieve a reasonable performance with infNDCG slightly above the median score. However, this run did not use any LTR and re-ranking methods. * above the median score of all teams; Rprec (0.325), P_10 (0.464), and infNDCG (0.431).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>CincyMedIR developed a python-based pipeline on ES to quickly respond to the TREC-PM tasks this year. However, the submitted and additional runs did not achieve high performance using LTR and medical concepts. It seems that retrieval with plain keywords and a classic ranking algorithm can generate a reasonable baseline. Due to the time constraint, we were not able to run more experiments to increase the performance from the baseline results. We will learn from other teams' techniques in the conference and continue to refine our results and the pipeline using the TREC-PM data in all four years between 2017 and 2020. This is the last year of TREC-PM, which means a new medically related track will be created in TREC 2021. We thank the organizers of TREC-PM for their effort and look forward to participating in the new track next year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.03,136.05,461.19,317.85"><head>Table 1 .</head><label>1</label><figDesc>Performance of Submitted and Additional Runs.</figDesc><table coords="2,72.03,157.05,461.19,296.85"><row><cell cols="2">CincyMedIR filename</cell><cell>Description</cell><cell>map</cell><cell>bpref</cell><cell>Rprec</cell><cell>P_10</cell><cell>infNDCG</cell></row><row><cell></cell><cell>dgt.trec_eval</cell><cell>Q_DGT</cell><cell cols="2">0.2266 0.2491</cell><cell>0.2678</cell><cell>0.4452</cell><cell>0.3877</cell></row><row><cell></cell><cell>28_t.trec_eval</cell><cell>Q_DG, LTR, RR_T</cell><cell cols="2">0.1495 0.1742</cell><cell>0.1822</cell><cell>0.3839</cell><cell>0.2721</cell></row><row><cell>Submitted</cell><cell cols="4">28dgt.trec_eval Q_DG, LTR, RR_DGT 0.0302 0.0659</cell><cell>0.0664</cell><cell>0.0742</cell><cell>0.0900</cell></row><row><cell>Runs (N=5)</cell><cell>28.trec_eval</cell><cell>Q_DG, LTR.</cell><cell cols="2">0.0281 0.0646</cell><cell>0.0603</cell><cell>0.0710</cell><cell>0.0852</cell></row><row><cell></cell><cell>20.trec_eval</cell><cell>Q_DG</cell><cell cols="2">0.0210 0.0483</cell><cell>0.0394</cell><cell>0.0516</cell><cell>0.0621</cell></row><row><cell></cell><cell></cell><cell>Q_DGT.</cell><cell cols="2">0.2859 0.2978</cell><cell>0.3124</cell><cell>0.4548</cell><cell>0.4338*</cell></row><row><cell>Additional</cell><cell>Not submitted</cell><cell>Q_DGT, RR_CID</cell><cell cols="2">0.2830 0.3003</cell><cell>0.3133</cell><cell>0.4581</cell><cell>0.4221</cell></row><row><cell>Runs (N=4)</cell><cell></cell><cell>Q_DGT, RR_CT</cell><cell cols="2">0.2711 0.2851</cell><cell>0.2931</cell><cell>0.4516</cell><cell>0.4114</cell></row><row><cell></cell><cell></cell><cell>Q_DGT, LTR</cell><cell cols="2">0.1782 0.1883</cell><cell>0.1823</cell><cell>0.3290</cell><cell>0.3137</cell></row><row><cell cols="6">Q_DG(T): queries were formed using disease, gene, (and treatment) in the XML.</cell><cell></cell></row><row><cell cols="6">LTR: Learning to Rank models trained on TREC-PM 2018 and validated on TREC-PM 2019</cell><cell></cell></row><row><cell cols="8">RR_(DGT|T|CID|CT): re-ranked methods; DGT) re-rank the results by moving a document up if it matches all</cell></row><row><cell cols="8">disease, gene, and treatment terms; T) matches treatment terms; CID) matches concept ids; CT) matches concept</cell></row><row><cell>terms.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
