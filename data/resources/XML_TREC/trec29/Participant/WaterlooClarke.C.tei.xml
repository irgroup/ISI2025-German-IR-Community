<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,83.11,114.94,445.78,15.12">WaterlooClarke at the Trec 2020 Conversational Assistant Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.98,148.84,89.08,10.48"><forename type="first">Negar</forename><surname>Arabzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.12,148.84,105.90,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,83.11,114.94,445.78,15.12">WaterlooClarke at the Trec 2020 Conversational Assistant Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F4634A667737A3A3C976F7A398E8236</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This report describes the methodology and results of the runs submitted by the WaterlooClarke group to TREC Conversational Assistant Track (CAsT) 2020. Our runs this year were based solely on the raw utterances. We did not submit any runs using the manually rewritten utterances or canonical response. All in all, our team submitted the four following runs :</p><p>1. WatACBase</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">WatACBaseRe</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WatACGPT2Re</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">WatACReAll</head><p>The overall approach is based on last year's approach <ref type="bibr" coords="1,351.33,404.54,10.91,9.57" target="#b0">[1]</ref>: 1) Refining the query, 2) retrieving the passages and 3) reranking the passages. We did not apply the reranking step for the WatACBase run. Compared to last year, we tried to improve our performance by: 1) expanding the pool of the retrieved documents by merging the retrieved documents from two query variations and 2) re-ranking the passages with Bert <ref type="bibr" coords="1,243.39,458.74,10.91,9.57" target="#b1">[2]</ref>. Based on preliminary experiments on the TREC CAsT 2019 data set, employing these two approaches showed statistically significant improvement in performance. In the following, we will explain the details of our methodology and discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We created a combined pool of retrieved documents using two distinct query variants; Base and GPT2, which we will explain below. We used the first query variant for WatACBase, WatACBaseRe and WatACReAll. We used the second query variant for WatACGPT2Re and WatACReAll .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Generation</head><p>Base Query Variant: This query variant was based on our query construction approach from last year <ref type="bibr" coords="1,111.17,648.57,13.52,9.57" target="#b0">[1]</ref> . Similar to last year, we first filtered terms appearing in our track-specific stopword lists. Then, we employed a simple, yet surprisingly well-performing, trick to maintain the topic of the conversation. We prepend the first utterance in each conversation to all of the utterances. What are the EU rules on GMO food labeling?</p><p>What are the GMO food labeling rules in the EU?" 82-5 Tell me more about traceability tools.</p><p>I would like to learn about GMO Food labeling. traceability tools . traceability tools.</p><p>Tell me more about traceability tools.</p><p>Tell me about traceability tools for GMO foods in the EU.</p><p>In addition, this year we appended the sentence "Tell me more about it" to the end of each utterance. We then applied AllenNLP coreference resolution to each conversation to the point of each utterance.</p><p>GPT2 Query Variant: We used the automatically rewritten queries based on GPT-2 model trained on TREC CAst 2019 data as an alternative to the Base Query Variant. While these automatic rewritten utterances work amazingly well most of the time, but there are some cases where they still do not maintain the thread of conversation. We determined the Base query variant a suitable complement for this variant.</p><p>Table <ref type="table" coords="2,118.87,332.56,5.45,9.57" target="#tab_0">1</ref> illustrates a few examples of the two query generation methods applied to a set of raw queries. As shown, appending the first utterance to the rest of the utterances in the conversation is helpful for keeping track of the topic. For example in utterance #82 -2, the raw query "What are the pros and cons?" has been modified to "I would like to learn about GMO Food labeling. What are the pros and cons? GMO labeling." In other words, the Base Query Variant helped to find out what is the topic for which we are seeking pros and cons. While the GPT2-rewriter also addressed this problem, in some cases such as utterance #82 -5, the Base Query refinement provides more details and is closer to the manually rewritten version of the queries. One of the limitations of the Base Query Variant appears when the main topic changes in a conversation. In those cases, appending the initial utterance to other queries, will not help and may have harmful effects on the performance. We hope to tackle this problem for next year's track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval</head><p>n order to retrieve the document at the very first stage, we utilized BM25 ranking with pseudo relevance feedback with the same exact experimental setup as in last year's experiments <ref type="bibr" coords="2,500.81,531.40,10.91,9.57" target="#b0">[1]</ref>. The second column in Table <ref type="table" coords="2,194.69,544.95,5.45,9.57" target="#tab_1">2</ref> explains the query variations we utilized for each of the runs. For WatACReAll run, we retrieved documents for both variations of query and then merged the pool of documents retrieved by both together. On the TREC CAsT 2019 dataset, broadening the pool of the retrieved documents led to a significant improvement in the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Passage Re-ranking</head><p>We applied passage re-ranking with Bert <ref type="bibr" coords="2,277.70,635.41,11.52,9.57" target="#b1">[2]</ref> on all the runs except WatACBase. In each of the re-ranked runs, we re-ranked the pool of the documents retrieved by different query variants by Pygaggle library<ref type="foot" coords="2,150.46,660.56,4.23,6.99" target="#foot_0">1</ref> . Based on recent literature and our experiments on TREC CAst 2019 data, the re-ranking has a significant improvement on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Table <ref type="table" coords="3,101.27,230.92,5.45,9.57" target="#tab_1">2</ref> compares our four different runs based on ndcg@5 and map@5. In addition, we investigated whether the improvement made in each of the runs is statistically significant. As it is demonstrated in Table <ref type="table" coords="3,113.88,258.02,4.24,9.57" target="#tab_1">2</ref>, the proposed reranking method in <ref type="bibr" coords="3,290.06,258.02,10.91,9.57" target="#b1">[2]</ref>, showed a statistically significant improvement on WatACBaseRe compared to WatACBase. More interestingly, map@5 did not have statistically significant improvement when using GPT2-writer compared to Base Query refinement ( WatACGPT2Re Vs WatACBaseRe ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In TREC CAst 2020, not only we made re-ranking work, also we ran experiments on two different query variations and compared their performance along with combining them. In future, we will consider generating different query variations to expand the pool of the retrieved documents. Moreover, detecting topic changes in the conversations will be helpful to modify our proposed Base Query Refinement methods. We look forward to participating in TREC CAst 2021.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,80.43,82.09,453.29,85.76"><head>Table 1 :</head><label>1</label><figDesc>An example of Base Query Refinement and GPT2 Query Refinement I would like to learn about GMO Food labeling. What are the pros and cons ? GMO labeling.</figDesc><table coords="2,80.43,96.89,453.29,70.96"><row><cell># Raw utterance</cell><cell>Base query refinement:</cell><cell>GPT2 Query refinement:</cell><cell>Manually Rewritten Queries</cell></row><row><cell>82-1 I would like to learn about</cell><cell>I would like to learn about GMO Food labeling.</cell><cell>I would like to learn about GMO</cell><cell>I would like to learn about GMO</cell></row><row><cell>GMO Food labeling.</cell><cell></cell><cell>Food labeling.</cell><cell>Food labeling.</cell></row><row><cell>82-2 What are the pros and</cell><cell></cell><cell>What are the pros and cons of GMO</cell><cell>What are the pros and cons of</cell></row><row><cell>cons?</cell><cell></cell><cell>Food labeling?</cell><cell>GMO food labeling?</cell></row><row><cell>82-3 And what about the</cell><cell>I would like to learn about GMO Food labeling.</cell><cell>What about the GMO Food labeling</cell><cell>What are the cons of GMO food</cell></row><row><cell>cons?</cell><cell>And what about the cons ? GMO labeling.</cell><cell>pros and cons?</cell><cell>labeling?</cell></row><row><cell>82-4 What are the EU rules?</cell><cell>I would like to learn about GMO Food labeling.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>What are the EU rules ? GMO labeling.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,164.14,82.09,285.49,65.04"><head>Table 2 :</head><label>2</label><figDesc>Comparing the results of our four submitted runs</figDesc><table coords="3,165.28,94.60,284.35,52.53"><row><cell>Run</cell><cell cols="4">Query Refinement method ndcg@5 map@5 ∆ndcg@5</cell></row><row><cell>WatACBase</cell><cell>Base</cell><cell>0.1547</cell><cell>0.0300</cell><cell></cell></row><row><cell cols="2">WatACBaseRe Base</cell><cell>0.2913</cell><cell>0.0680</cell><cell>+0.1366</cell></row><row><cell cols="2">WatACGPT2Re GPT-2</cell><cell>0.3161</cell><cell>0.0681</cell><cell>+0.1614</cell></row><row><cell>WatACReAll</cell><cell>Base + GPT2</cell><cell>0.3265</cell><cell>0.0720</cell><cell>+0.1718</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.59,684.23,174.17,7.47"><p>https://github.com/castorini/pygaggle</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,88.97,470.31,451.03,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,155.37,470.31,292.54,9.57">Waterlooclarke at the trec 2019 conversational assistant track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,470.60,470.31,30.97,9.57">TREC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,88.97,492.83,451.03,9.57" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="3,204.55,492.83,134.35,9.57">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
