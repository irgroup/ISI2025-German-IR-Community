<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.49,118.82,368.46,12.62">TREC 2020 NEWS Track Background Linking Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,182.84,150.81,72.83,10.48"><forename type="first">Rahul</forename><surname>Gautam</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision and Pattern Recognition Unit Indian Statistical Institute 203</orgName>
								<address>
									<addrLine>B.T.Road</addrLine>
									<settlement>Kolkata</settlement>
									<country>India -700108</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.99,150.81,70.72,10.48"><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
							<email>mandar.mitra@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision and Pattern Recognition Unit Indian Statistical Institute 203</orgName>
								<address>
									<addrLine>B.T.Road</addrLine>
									<settlement>Kolkata</settlement>
									<country>India -700108</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.98,150.81,80.63,10.48"><forename type="first">Dwaipayan</forename><surname>Roy</surname></persName>
							<email>dwaipayan.roy@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision and Pattern Recognition Unit Indian Statistical Institute 203</orgName>
								<address>
									<addrLine>B.T.Road</addrLine>
									<settlement>Kolkata</settlement>
									<country>India -700108</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.49,118.82,368.46,12.62">TREC 2020 NEWS Track Background Linking Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">02BC914DCAFF933CCA3EECA2B2368BBA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Background Linking task is a problem that focuses on providing users with suggestions for articles to read next, when the user is reading a news article. The suggested articles should provide adequate context and background information for the article that the user is currently reading. In this paper, we describe several methods that we explored for this task, and report their results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the third iteration of the News Track at TREC. The News Track consists of two tasks, viz., Background Linking and Entity Ranking. We explore the Background Linking (BL) task. The goal of the BL task is: given a news article, suggest similar articles that provide context and background for the given article. In order to solve the problem, we need to address the following issues:</p><p>• how to extract the most useful keywords from a reasonably long document (i.e., a news report)</p><p>• how to compute appropriate weights for the extracted terms</p><p>• how to use the extracted keywords to search accurately</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our method</head><p>The dataset provided in this task is the TREC Washington Post Collection (version 3). It consists of 671,947 Washington Post articles from the years 2012-2019. Each article consists of the following fields: id, title, content, author, article type, date, and url. Given an article as input, our method has to retrieve a list of articles, ranked according to their estimated usefulness in providing context and background for the given article. A ranked list is evaluated on the basis of a known list of useful articles (as judged by experts). Methods are compared on the basis of their NDCG@5 scores. All our methods use Lucene 8.3.4 (http://lucene.apache.org) to index and query the dataset. For all submissions, we tune our parameters using data from the TREC News 2018 and 2019 tasks. Below, we describe the methods corresponding to our submissions for the TREC 2020 Task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">IRISINews1: Boosted terms from content</head><p>This method consists of extracting K key terms from the query article, and issuing a weighted lucene query against the index to get the result list. We use a simple tf-idf formula to compute weights for terms from the document. For a term t in a document field d, the weight of term is given by tf (t, d)×idf (t), where tf (t, d) is the number of occurrences of t in d, and idf (t) is given by 1 + log((1 + N )/(1 + df )). We pick the top words from the content field, as it proved to be more useful than the title field in our experiments. By varying the number of words (K) extracted from the content field, we find that K = 80 yields the best performance for us.</p><p>Using the above query, we compute the score for a document using the BM25 <ref type="bibr" coords="2,461.46,149.21,17.21,9.57" target="#b1">[2]</ref> formula . Additionally, for each query term, we assign a numerical weight as specified previously. The BM25 score for each matching query term is multiplied by the weight of that term. These scores are added up to get the total similarity score for the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">IRISINews2: Document embedding with ranked list merging</head><p>In this method, we represent each news article as a vector in an embedding space. For every query article, we prepare a ranked list of articles based on the cosine similarity between the query article and all other articles in the index. We merge the ranked list generated by the method described here with the list produced by IRISINews1 to obtain the final ranked list.</p><p>We train a word2vec <ref type="bibr" coords="2,203.44,293.35,11.52,9.57" target="#b2">[3]</ref> model on the dataset. Each word in the corpus is represented as a 300 dimensional vector. An article is represented as the sum of the vectors corresponding to the top K words contained in the article. For extracting the top terms, we resort to the tf-idf scoring described for the previous method. We find that extracting K = 25 words and summing their vector representations yields reasonably good performance. For scoring, we rank the documents by their cosine similarity with the query embedding. We merge the ranked list from IRISINews1 with the list produced by the cosine similarity ranking using the method described below.</p><p>We retrieve 100 documents with the best performing parameters of each method. Let l 1 , l 2 denote, respectively, the list of documents retrieved by IRISINews1, and by the embeddingbased method. We sum normalize the scores in both the lists. Let the i-th entry in the list l be called l[i], and let the score for it be denoted as l <ref type="bibr" coords="2,335.11,442.39,12.52,9.57">[i]</ref>.score , Next we do the following.</p><formula xml:id="formula_0" coords="2,227.70,463.31,296.71,42.64">l 1 [i].score = λ × l 1 [i].score l 2 [i].score = (1 -λ) × l 2 [i].score where(0 ≤ λ ≤ 1) (1)</formula><p>After that we merge the two lists by simply taking their union; if a document appears in both the lists, the score for the document is the sum of the scores from the individual lists. By varying λ, we find that λ = 0.6 yields the best performance on the 2018 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">IRISINews3: Use of named entities</head><p>In the previous approaches, we used only words picked from the content part of the article. To use the title part, we applied the Stanford corenelp <ref type="bibr" coords="2,346.85,606.68,11.52,9.57" target="#b0">[1]</ref> Named Entity Recognition (NER) module to pick core terms from the title.</p><p>We generate two queries, one generated by method 1 (IRISINews1) with best performing parameters. Let us call this query q 1 . We generate another query by weighting the named entities by their tf-idf score. We call this query q 2 . We merge them in the following way, Let q[i].weight be the weight of the ith term of query q, we change the weight of the terms as</p><formula xml:id="formula_1" coords="2,220.27,695.69,168.91,42.64">q 1 [i].weight = λ * q 1 [i].weight q 2 [i].weight = (1 -λ) * q 2 [i].weight where(0 ≤ λ ≤ 1)</formula><p>We join the two queries to form the new query with λ = 0.8 and issue it against the index to get the final ranked list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">IRISINews4: Merging 3 ranked lists</head><p>Here we merge 3 ranked lists, l 1 , l 2 and l 3 . The list l 1 was generated by issuing a weighted query with top 20 terms extracted from the title. l 2 Was generated by a similar method, here top 50 terms from the content was used to construct the query. The third list was generated by ranking the documents with respect to their cosine similarity with the query document as done in IRISINews2.</p><p>We merge l 1 and l 2 using equation ( <ref type="formula" coords="3,282.58,169.80,4.65,9.57">1</ref>) with λ = 0.1. After getting the new list form merging l 1 and l 2 , we merged the new list with l3 with λ = 0.6 to get the final ranked list. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In below tables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we have explained our approach for the TREC 2020 News Track for background linking tasks. The results show that our system performs reasonably well using simple methods. There are further scope of improvement in our methods to obtain even better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,85.04,263.91,401.75,172.44"><head>Table 1 :</head><label>1</label><figDesc>Comparison between all methods * Best performance among our methods ** Best performance among all TREC submissions</figDesc><table coords="3,122.65,263.91,364.14,122.34"><row><cell cols="2">Submission Name Description</cell><cell cols="2">2018 Score 2019 Score</cell></row><row><cell>IRISINews1</cell><cell>Boosted query</cell><cell>0.3964</cell><cell>0.5263*</cell></row><row><cell>IRISINews2</cell><cell>Document vector approach</cell><cell>0.4041*</cell><cell>0.5131</cell></row><row><cell>IRISINews3</cell><cell>Using named entities</cell><cell>0.3993</cell><cell>0.5253</cell></row><row><cell>IRISINews4</cell><cell>Merging 3 ranked lists</cell><cell>0.3997</cell><cell>0.5160</cell></row><row><cell>htwsaar4</cell><cell>verbose query comprising of</cell><cell>0.4619**</cell><cell>-</cell></row><row><cell></cell><cell>title and content</cell><cell></cell><cell></cell></row><row><cell>UDInfolab all</cell><cell>All words and Entities used as</cell><cell>0.438</cell><cell>0.606**</cell></row><row><cell></cell><cell>query</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,107.46,598.96,329.09,9.57" xml:id="b0">
	<monogr>
		<ptr target="https://stanfordnlp.github.io/CoreNLP/" />
		<title level="m" coord="3,107.46,598.96,128.00,9.57">Stamford CoreNLP library</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,107.46,621.48,416.95,9.57;3,107.46,635.02,60.62,9.57" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>The Probabilistic Relevance Framework: BM25 and Beyond</note>
</biblStruct>

<biblStruct coords="3,107.46,657.54,416.94,9.57;3,107.46,671.09,171.70,9.57" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dean</forename><surname>Corrado</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Distributed Representations of Words and Phrases and their Compositionality</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
