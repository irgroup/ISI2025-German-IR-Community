<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,109.63,114.89,392.74,15.15;1,149.96,136.80,312.08,15.15">Impact of Tokenization, Pretraining Task, and Transformer Depth on Text Ranking</title>
				<funder ref="#_Hqaq6Jd">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO STW</orgName>
				</funder>
				<funder ref="#_9HARNbq">
					<orgName type="full">Innovation Exchange Amsterdam</orgName>
				</funder>
				<funder ref="#_RBeUmsF">
					<orgName type="full">Facebook Research (Computationally Efficient NLP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.01,170.76,63.95,10.48"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.22,170.76,102.22,10.48;1,361.45,169.14,1.88,6.99"><forename type="first">Nikolaos</forename><surname>Kondylidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.72,170.76,55.27,10.48"><forename type="first">David</forename><surname>Rau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,109.63,114.89,392.74,15.15;1,149.96,136.80,312.08,15.15">Impact of Tokenization, Pretraining Task, and Transformer Depth on Text Ranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2529ACAEF579B916E3387C8486BA804B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper documents the University of Amsterdam's participation in the TREC 2020 Deep Learning Track. Rather than motivated by engineering the best scoring system, our work is motivated by our interest in analysis, informing our understanding of the opportunities and challenges of transformers for text ranking. Specifically, we focus on the passage retrieval task where we try to answer three of sets of questions.</p><p>First, transformers use different tokenization than traditional IR approaches such as stemming and lemmatizing, leading to different document representations. What is the effect of modern preprocessing techniques on traditional retrieval algorithms? Our main observation is that the limited vocabulary of the BERT tokenizer is affecting many long-tail tokens, which leads to large gains in efficiency at the cost of a small decrease in effectiveness.</p><p>Second, the effectiveness of transformers is a result of the self-supervised pre-training task promoting general language understanding, ignorant of the specific demands of ranking tasks. Can we make further correlate queries and relevant passages in the pre-training task? Our main observation is that there is a whole continuum between the original self-supervised training task of BERT and the final interaction ranker, and isolating ranking-aware pre-training tasks may leads to gains in efficiency (as these pretrained models can be reused for many tasks) as well as to gains in effectiveness (in particular when limited data on the target task is available).</p><p>Third, transformers combine large sequence length with many layers, with unclear what this deep semantics adds in the context of ranking. How complex do the models need to be in order to perform well on this task? Our main observation is that the deep layers of BERT lead to some, but relatively modest, gains in performance, but that the exact role of the presumed superior language understanding for search is far from clear.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper documents our participation in the TREC 2020 Deep Learning Track <ref type="bibr" coords="1,465.18,564.17,74.82,9.57;1,72.00,577.72,23.23,9.57" target="#b2">[Craswell et al., 2021]</ref>. The Deep Learning Track started at TREC 2019 <ref type="bibr" coords="1,335.59,577.72,104.81,9.57">[Craswell et al., 2020b]</ref>, and is in it's second year. The TREC Deep Learning Track is providing very large training data for its task, which is a necessity for training deep neural networks. The track consists of four tasks in total; two tasks for two collections. Full-Ranking and Re-Ranking tasks applied in collections of documents and passages. The Re-Ranking task focuses on ranking the top 1,000 candidates from the collection for each query. We focused on the collection of passages and performed experiments for both tasks. TREC provides training, evaluation and test queries, the collection of passages and training triplets that consist of a query, a couple of candidates and an indicator that defines the most relevant passage among the two candidates. "what", "tissue", "compose", "##s", "the", "h", "##yp", "##oder", "##mis" Stem "what tissu compos the hypodermi" BERT+Stem "what", "tis", "##su", "com", "##po", "##s", "the", "h", "##yp", "##ode", "##mi" This paper is structured in the following way. Our first experiment described in Section 2 studies the effects of BERT tokenizer and Porter stemmer as a preprocessing step before applying BM25 for the Full-Ranking task. The following two experiments in Sections 3 and 4 are using the Re-Ranking task. Specifically, Section 3 describes a proposed additional preprocessing step applied on BERT, which aims on further correlating tokens among queries and relevant passages. Finally, Section 4 evaluates the performance of BERT used as a re-ranker, when only some of its layers are being used and fine-tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Impact of New Preprocessing Techniques</head><p>In this section we detail our tokenization experiments, motivated by the observation that modern transformers use different tokenization than traditional IR approaches such as stemming and lemmatizing, leading to different document representations. What is the effect of modern preprocessing techniques on traditional retrieval algorithms?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tokenization in BERT</head><p>All retrieval systems are functioning under a multi-stage ranking setting. The first steps are using traditional IR techniques taking advantage of their efficiency and ability to be applied on large collections. The final stages are using modern ML techniques on a small subset of candidates. This is the case due to their superior text comprehension abilities that comes with a high computational cost which prohibits their application on large scale. The two approaches utilize documents differently and we wanted to study how the modern BERT tokenizer <ref type="bibr" coords="2,414.25,503.64,96.32,9.57" target="#b3">[Devlin et al., 2018]</ref> alone is affecting the BM25 term based similarity scores. Therefore we preprocessed text with BERT tokenizer and then run Full-Ranking experiments on the passage retrieval task. Additionally, we study the effect of the NLTK Porter stemmer <ref type="bibr" coords="2,293.13,544.29,64.75,9.57" target="#b6">[Porter, 1980]</ref> on both applications.</p><p>Pretrained BERT comes with a predefined vocabulary of size 30,522 which is also enforced on the pre-trained BERT tokenizer that was applied. The BERT tokenizer applies WordPiece tokenization <ref type="bibr" coords="2,72.00,584.94,92.68,9.57" target="#b3">[Devlin et al., 2018]</ref>. An example of word piece tokenization is that "composes" will produce two tokens, namely "compose" and "##s". The double # character indicates that this token is part of the previous word. This ability of BERT tokenizer allows it to be very descriptive taking into account its very small vocabulary size. Nevertheless, the vocabulary size of BERT tokenizer is still limiting the tokenized text, compared to traditional term based retrieval algorithms were there is no vocabulary size limitation. The words that cannot be represented by the BERT tokenizer are resulting to UNK tokens, which are being removed from the resulting text. A full example of the different tokenization techniques is given in Table <ref type="table" coords="2,312.46,679.78,4.24,9.57" target="#tab_0">1</ref>.</p><p>We try to quantify these effects by reporting the number of (uniquely) affected tokenized words in Table <ref type="table" coords="2,114.40,706.88,4.24,9.57" target="#tab_1">2</ref>. This clearly demonstrates that the limited BERT vocabulary has clear impact. Recall, in standard IR approaches essentially all tokens (except for stop-words) are indexed. These absolute numbers of unknown tokens are massive. To put them into perspective, the untokenized corpus has 6,620,939 terms in our index, making also the fraction very large. The impact of this requires further research, the overwhelming majority of them occurs only once, and inspection of a sample reveals noisy character strings of unclear benefit to the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Results</head><p>The BM25 Full-Ranking results are presented in Table <ref type="table" coords="3,334.83,406.67,4.24,9.57" target="#tab_2">3</ref>. Although all scores are reasonably close, applying ther BERT tokenizer decreases the performance of BM25 across all metrics. In addition, Porter stemming leads to a performance increase under normal conditions, but if we first apply Porter stemmer and then the BERT tokenizer, there is a further performance drop. Above, in Table <ref type="table" coords="3,101.61,460.87,5.45,9.57" target="#tab_1">2</ref> we quantified the effects of the BERT tokenizer. Hence, the very large number of unknown words might be the main cause of the lower BM25 performance when BERT tokenizer is applied. Furthermore, by applying Porter stemmer before tokenization, the number of total unknown word instances increases by 400%, even though the number of unique unknown words slightly decreases. This suggests that by applying Porter stemmer initially, we affect a smaller subset of the vocabulary that is used more frequently, which can also explain the further performance drop of this technique.</p><p>Our general conclusion is that the limited vocabulary of the BERT tokenizer is affecting many long-tail tokens, which leads to large gains in efficiency at the cost of a small decrease in effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relevance Embedding Pre-training Task</head><p>In this section we detail our ranking-aware pre-training task experiments, motivated by the observation that modern transformers are pre-trained using a self-supervised pre-training task promoting general language understanding, ignorant of the specific demands of ranking tasks. Can we make further correlate queries and relevant passages in the pre-training task?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ranking-aware Pre-training</head><p>The success of the BERT model in various tasks can, next to its self-attention mechanism, be attributed to massive pre-training on large amounts of unlabeled text. Generally, pre-training allows to first build up a general representation that can later be helpful in the specialization of a down-stream task; particularly, in cases where the data for the down-stream task is very scarse and would not be sufficient to train a model on.</p><p>For the pre-training of BERT two special tasks have been developed: The Mask-LM (MLM) task where the model predicts random masked words in a sentence and the Next Sentence Prediction (NSP) task that requires the model to predict whether two sentences appear next to each other. While those pre-training tasks equip the model with a general language understanding that has been shown to significantly improve ranking <ref type="bibr" coords="4,283.29,352.40,124.37,9.57">[Nogueira and Cho, 2019b]</ref>, they are not tailored upon ranking specific demands. Specifically, capturing the semantic relevance between query and passage, which plays an essential role for QA-focused datasets such as MS MARCO <ref type="bibr" coords="4,439.02,379.50,84.73,9.57" target="#b7">[Qiao et al., 2019]</ref>, is not incorporated into the current pre-training of interaction-based rankers and is only indirectly learned during down-stream training. As pointed out by <ref type="bibr" coords="4,343.35,406.60,76.13,9.57" target="#b8">Yan et al. [2020]</ref>, we also feel the urge for the design of appropriate pre-training tasks for ranking with BERT. To this end we propose a new relevance embedding (RelEmb) pre-training task for interaction-based rankers to better model the interaction between query and passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methodology</head><p>Given a query and a relevant document, we form the combined input in the standard form of "CLS + query + SEP + relevant passage." Additionally, we add segmentation embeddings to allow the model to disambiguate query and the relevant passage. We truncate the query to a maximum 64 tokens and the total length of to input to 512. As the goal of this pre-training task is to model strong interactions between query and passage, we randomly mask tokens in the query and try to predict the words only from the passage and vice versa (as is shown in Figure <ref type="figure" coords="4,459.05,564.80,4.24,9.57" target="#fig_1">1</ref>). This way we explicitly learn a query and a document model given the other respectively. In 80% of the times mask tokens from the query, and 20% tokens from the document to account for the reason that predicting words in the document given the query is more challenging. We apply this additional pretraining step on the already pre-trained BERT, as a first preliminary experiment. We experimented with extracting relevant query/passage pairs from the ORCAS click data <ref type="bibr" coords="4,430.08,632.55,109.92,9.57">[Craswell et al., 2020a]</ref> and the provided MS MARCO training triples. After the pre-training we trained the BERT model following <ref type="bibr" coords="4,118.06,659.64,122.99,9.57">Nogueira and Cho [2019b]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The results of the ranker pre-trained of our proposed RelEmb task alongside the vanilla BERT ranker can be found in Table <ref type="table" coords="5,217.66,258.50,4.24,9.57" target="#tab_3">4</ref>. The RelEmb pre-training task increases the performance across all metrics compared to the vanilla pre-trained BERT model for both datasets. Particularly, NDCG@10 increases around 2.5 points for the BERT RelEmb trained on MS Marco, and MRR increases over 3 points when pre-trained on Orcas. These preliminary experiments show that designing new training tasks that go beyond general language understanding and target down-stream task specific characteristics holds potential and are worth exploring more in the future.</p><p>The proposed RelEmb pre-training task can especially be useful when the amount of query/ passage pairs is insufficient to fine-tune a ranker on. In this case the model can be first pretrained on available query/passage pairs (from another dataset) and then later be fine-tuned on the available ranking labels. Further, pre-training a ranker-like model on a large dataset has the advantage that its learned representations can be leveraged to many different tasks. Ranker that are trained directly on the down-stream task can not be reused have to be trained every single time from "scratch" requiring computational resources, time and data. In additional preliminary experiments we observed that after the RelEmb pre-training on Orcas the model is already closer to a BERT ranker and starts at a higher performance than the vanilla BERT. This experiment outlines the most naive way of modeling query-passage interaction and has to be seen as an initial effort in designing a ranking-aware pre-training task that needs refinement in future work.</p><p>Our general conclusion is that there is a whole continuum between the original self-supervised training task of BERT and the final interaction ranker, and isolating ranking-aware pre-training tasks may leads to gains in efficiency (as these pretrained models can be reused for many tasks) as well as to gains in effectiveness (in particular when limited data on the target task is available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Impact of the Deep Layers of BERT</head><p>In this section, we detail our experiments with the depth of the transformer model, motivated by trying to understand the what this deep semantics adds in the context of ranking. How complex do the models need to be in order to perform well on this task?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Impact of Model Depth</head><p>Fine-tuning BERT on the binary classification task for identifying whether a passage is relevant or irrelevant proved to perform very well, as proposed by <ref type="bibr" coords="5,349.00,678.67,122.72,9.57">Nogueira and Cho [2019a]</ref>. The model's input is given in the form [CLS] + query + [SEP] + passage. The hidden activations of the CLS token are given to a binary classifier. The relevance probability was used as a relevance score. We  extended this approach and question whether the full 12-layer BERT model has to be used for this task. The intuition is that this task might be simple enough, not requiring high level comparisons between the query and the passage that is provided by all 12 BERT layers of the pre-trained BERT. Instead, semantic matching of lower level might be enough. Our training scheme is the same with <ref type="bibr" coords="6,125.00,422.02,125.21,9.57">Nogueira and Cho [2019a]</ref> and in all cases the final binary classification linear layer is always randomly initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The results of our experiments are displayed in Table <ref type="table" coords="6,341.07,485.37,4.24,9.57" target="#tab_4">5</ref>. Top line (1-12) represents the vanilla approach were all 12 layers were utilized, as proposed by <ref type="bibr" coords="6,362.28,498.92,127.05,9.57">Nogueira and Cho [2019a]</ref>. We can see that when using only the first 2 or three layers of BERT the performance drops significantly (comparing 1-2 &amp; 1-3 to 1-12). On the other hand, if we use the first six or nine layers, the performance does not deteriorate that much. Specifically, with respect to MRR the performance difference is almost insignificant. Regarding NDCG@10 and MAP@1000 the performance drops by around 3 absolute percentage points. Taking into account that the models 1-9 and 1-6 use roughly 75% and 50% of the parameters used by the vanilla 1-12 model, one can argue that this is a very beneficial trade-off, since these models can be trained and applied much faster. Whether this trade-off is beneficial depends on the task where the models will be applied.</p><p>In Table <ref type="table" coords="6,133.88,620.87,5.45,9.57" target="#tab_5">6</ref> we show a selection of the topics that gained the most in performance comparing BERT 12 to BERT 2. All of these requests are more complex (so no simple navigational search) and have compound multi-term concepts, and all of them have specific relations between multiple concepts as signaled by the prepositions and verbs. Arguably, here the language understanding encoded in BERT can be potentially helpful, and this may explain the gains in performance. However, when inspecting topics that don't gain from BERT's depth, we see very similar query characteristics, defying the naive explanation that these gains are due to the required language understanding of these particular topics. Further research is needed to better understand the impact of transformer depth on search.</p><p>Our general conclusion is that the deep layers of BERT lead to some, but relatively modest, gains in performance, but that the exact role of the presumed superior language understanding for search is far from clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper documented our participation in the TREC 2020 Deep Learning Track. We conducted three sets of relatively independent experiments, analyzing the impact of the tokenization of modern transformers, experimenting with ranking-aware pre-training tasks, and analyzing the impact of the deep layers of BERT on ranking. Our main conclusions were the following. First, the limited vocabulary of the BERT tokenizer is affecting many long-tail tokens, which leads to large gains in efficiency at the cost of a small decrease in effectiveness. Second, there is a whole continuum between the original self-supervised training task of BERT and the final interaction ranker, and isolating ranking-aware pre-training tasks may leads to gains in efficiency (as these pretrained models can be reused for many tasks) as well as to gains in effectiveness (in particular when limited data on the target task is available). Third, the deep layers of BERT lead to some, but relatively modest, gains in performance, but that the exact role of the presumed superior language understanding for search is far from clear.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.00,162.70,468.00,9.57;4,72.00,176.25,236.37,9.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RelEmb pre-training task. Predicting randomly tokens in the passage from the query (left) or tokens in query from the passage (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.00,84.00,468.00,69.68"><head>Table 1 :</head><label>1</label><figDesc>Examples of the different tokenization techniques applied on query "1108466" from TREC 2020 test set.</figDesc><table coords="2,72.00,115.55,241.76,38.12"><row><cell cols="2">Tokenization Resulting text</cell></row><row><cell>-</cell><cell>"what tissue composes the hypodermis"</cell></row><row><cell>BERT</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,73.36,84.00,465.28,60.61"><head>Table 2 :</head><label>2</label><figDesc>Number of terms identified as unknown by BERT tokenizer, also after Porter stemming.</figDesc><table coords="3,77.98,102.47,456.04,42.14"><row><cell>Tokenizer</cell><cell>Stemmer</cell><cell cols="2">Total Unknown Terms Unique Unknown Terms</cell></row><row><cell>BERT</cell><cell>-</cell><cell>15,401,102</cell><cell>4,172,669</cell></row><row><cell>BERT</cell><cell>Porter</cell><cell>74,231,878</cell><cell>3,980,886</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,77.98,173.26,462.02,103.65"><head>Table 3 :</head><label>3</label><figDesc>Preprocessing effects using BM25 on various tokenization methods.</figDesc><table coords="3,77.98,191.73,462.02,80.79"><row><cell>Tokenizer</cell><cell>Stemmer</cell><cell cols="3">MAP% Rec@100% NDCG@10%</cell><cell>P@10%</cell></row><row><cell>Anserini</cell><cell>-</cell><cell>27.75</cell><cell>51.22</cell><cell>50.53</cell><cell>37.96</cell></row><row><cell>Anserini</cell><cell>Porter</cell><cell>28.19</cell><cell>55.92</cell><cell>48.06</cell><cell>35.19</cell></row><row><cell>BERT  †</cell><cell>-</cell><cell>26.51</cell><cell>53.61</cell><cell>46.58</cell><cell>32.96</cell></row><row><cell>BERT</cell><cell>Porter</cell><cell>25.38</cell><cell>50.03</cell><cell>45.98</cell><cell>32.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="3,79.69,267.28,1.72,5.24;3,81.91,268.94,156.91,7.96"><p>† Official submission as bm25 bert token.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,72.00,84.00,468.00,114.60"><head>Table 4 :</head><label>4</label><figDesc>Passage re-ranking (top 1,000) performance for the relevance embedding (RelEmb) pretraining task evaluated on the TREC 2020 test topics.</figDesc><table coords="5,77.98,116.02,462.02,82.58"><row><cell>Model</cell><cell>Pre-training dataset</cell><cell cols="2">MAP% NDCG@10%</cell><cell>MRR%</cell></row><row><cell>BERT Vanilla</cell><cell>-</cell><cell>44.92</cell><cell>66.71</cell><cell>80.08</cell></row><row><cell>BERT RelEmb</cell><cell>MS Marco</cell><cell>45.33</cell><cell>69.29</cell><cell>80.93</cell></row><row><cell>BERT RelEmb  †</cell><cell>Orcas</cell><cell>45.32</cell><cell>67.48</cell><cell>83.36</cell></row><row><cell cols="5">† Official submission as relemb mlm 0 2 (NDCG@10 66.62%, MRR 76.77%), scores differ as the model was not put</cell></row><row><cell>into evaluation mode.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,77.98,84.00,462.02,128.15"><head>Table 5 :</head><label>5</label><figDesc>Utilizing some of BERT's layers to be used as a passage re-ranker.</figDesc><table coords="6,77.98,102.47,82.82,9.57"><row><cell>Layers Finetuned</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,76.05,238.43,460.32,101.26"><head>Table 6 :</head><label>6</label><figDesc>Selection of topics with the most gain in performance comparing BERT 12 to BERT 2.</figDesc><table coords="6,77.98,256.90,29.43,9.57"><row><cell>Query</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the track organizers for their amazing service and effort in making realistic benchmarks for neural ranking available. This research is funded in part by the <rs type="funder">Netherlands Organization for Scientific Research (NWO STW</rs> # <rs type="grantNumber">17752</rs>; <rs type="grantNumber">NWO CI # CISC.CC.016</rs>), <rs type="funder">Facebook Research (Computationally Efficient NLP</rs> grant), and the <rs type="funder">Innovation Exchange Amsterdam</rs> (<rs type="grantName">POC grant</rs>). Views expressed in this paper are not necessarily shared or endorsed by those funding the research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Hqaq6Jd">
					<idno type="grant-number">17752</idno>
				</org>
				<org type="funding" xml:id="_RBeUmsF">
					<idno type="grant-number">NWO CI # CISC.CC.016</idno>
				</org>
				<org type="funding" xml:id="_9HARNbq">
					<orgName type="grant-name">POC grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,72.00,505.78,468.00,9.57;7,82.91,519.33,209.00,9.57" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,387.91,505.78,152.08,9.57;7,82.91,519.33,170.41,9.57">Orcas: 18 million clicked querydocument pairs for analyzing search</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Billerbeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,541.84,468.01,9.57;7,82.91,555.39,457.09,9.57;7,82.91,568.94,184.70,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,402.93,541.84,137.08,9.57;7,82.91,555.39,87.69,9.57">Overview of the TREC 2019 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,190.52,555.39,344.42,9.57">TREC 2019: Proceedings of the Twenty-Eighth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,591.46,468.00,9.57;7,82.91,605.01,457.09,9.57;7,82.91,618.56,86.21,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,330.64,591.46,209.36,9.57;7,82.91,605.01,22.75,9.57">Overview of the TREC 2020 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,126.51,605.01,343.12,9.57">TREC 2020: Proceedings of the Twenty-Ninth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,641.07,468.00,9.57;7,82.91,654.62,457.09,9.87;7,82.91,668.95,83.21,9.09" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,312.59,641.07,227.40,9.57;7,82.91,654.62,169.78,9.57">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,690.68,468.00,9.57;7,82.91,705.02,180.57,9.09" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,201.76,690.68,146.63,9.57">Passage re-ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR, abs/1901.04085</idno>
		<ptr target="http://arxiv.org/abs/1901.04085" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,75.51,463.68,9.57" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="8,196.71,75.51,136.12,9.57">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,72.00,98.02,468.01,9.57;8,82.91,111.57,264.47,9.87" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,146.48,98.02,158.91,9.57">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<idno type="DOI">10.1108/eb046814</idno>
		<ptr target="https://doi.org/10.1108/eb046814" />
	</analytic>
	<monogr>
		<title level="j" coord="8,318.34,98.02,37.67,9.57">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,134.09,468.00,9.57;8,82.91,147.64,316.99,9.87" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,261.73,134.09,235.67,9.57">Understanding the behaviors of BERT in ranking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1904.07531</idno>
		<ptr target="http://arxiv.org/abs/1904.07531" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,170.15,468.00,9.57;8,82.91,183.70,457.09,9.57;8,82.91,197.25,457.09,9.57;8,82.91,210.80,148.94,9.57" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,368.76,170.15,171.24,9.57;8,82.91,183.70,457.09,9.57;8,82.91,197.25,41.22,9.57">IDST at TREC 2019 deep learning track: Deep cascade ranking with generation-based document expansion and pre-trained language modeling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,149.23,197.25,355.65,9.57">TREC 2019: Proceedings of the Twenty-Eighth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
