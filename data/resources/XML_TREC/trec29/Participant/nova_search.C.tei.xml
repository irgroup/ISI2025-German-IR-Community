<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.83,115.96,343.68,12.62;1,287.90,133.89,39.56,12.62">NOVA at TREC 2020 Conversational Assistance Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,193.73,172.06,63.52,8.74"><forename type="first">Rafael</forename><surname>Ferreira</surname></persName>
							<email>rah.ferreira@campus.fct.unl.pt</email>
							<affiliation key="aff0">
								<orgName type="institution">NOVA University of Lisbon</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.66,172.06,60.04,8.74"><forename type="first">David</forename><surname>Semedo</surname></persName>
							<email>df.semedo@campus.fct.unl.pt</email>
							<affiliation key="aff0">
								<orgName type="institution">NOVA University of Lisbon</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.52,172.06,69.11,8.74"><forename type="first">Joao</forename><surname>Magalhaes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NOVA University of Lisbon</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.83,115.96,343.68,12.62;1,287.90,133.89,39.56,12.62">NOVA at TREC 2020 Conversational Assistance Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AD4085AFFBB960D9DF79ED18BB703BBD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conversational Search</term>
					<term>Multi-turn Question Answering</term>
					<term>Query Rewriting</term>
					<term>Information Retrieval</term>
					<term>Passage Ranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use of conversational assistants to search for information is becoming more popular among the general public. In particular, in the last few years, the interest in conversational search is increasing, being this a step forward in allowing a more natural interaction with search systems. In this paper, we describe our work and submitted runs to TREC Conversational Assistance Track (CAsT) 2020 [4]. This track is mainly focused on Passage Conversational Information Seeking, being the context of the conversation key to retrieve relevant information. Our approach leverages a three-stage architecture composed of: (a) context tracking via query rewriting, (b) retrieval, and (c) re-ranking using a transformer model. The results obtained with this architecture achieved state-of-the-art results when compared to TREC CAsT 2019 baselines <ref type="bibr" coords="1,201.61,380.87,9.22,7.86" target="#b4">[5]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In CAsT 2019 <ref type="bibr" coords="1,196.52,475.79,9.96,8.74" target="#b4">[5]</ref>, the conversational search task is formally defined by a sequence of natural language conversational turns for a topic T , with queries q. For each conversation turn T = q 1 , . . . q i , . . . q n , the task is to find relevant passages p k for each query q i , satisfying the user's information need for that turn according to the conversational context.</p><p>This task presents challenges that typical information retrieval systems do not have to address. One of the primary examples is that the current query may not include all the information needed to retrieve the answer that the user is searching for. This is evidenced in table 1 through the use of "one" (explicit coreference) in turn 2, which refers to physician's assistant, and in turn 4, where the "starting salary" is for the physician's assistant position, although there is no direct evidence (implicit coreference). Another challenge is to re-rank the passages according to the conversational context, pushing to the top the passages that are more relevant according to the context of the conversation.</p><p>In this work, we describe our submission to TREC CAsT 2020 <ref type="bibr" coords="1,432.68,644.16,9.96,8.74" target="#b3">[4]</ref>. In particular, we use a three-stage architecture composed of (a) context tracking by means of a query rewriting transformer model, (b) retrieval using typical information retrieval techniques, and (c) re-ranking via another transformer model. This architecture achieved state-of-the-art results in TREC CAsT 2019 when compared to the submitted runs <ref type="bibr" coords="2,278.84,323.56,9.96,8.74" target="#b4">[5]</ref>.</p><p>The rest of the paper is organized as follows: next we discuss the developed architecture, in section 3, we discuss the evaluation on the 2019 dataset. Section 4 specifies the results of the delivered runs to TREC CAsT 2020, and concluding remarks are presented in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our complete architecture is composed by three major components that we will introduce in this section: (a) query rewriting, to track the conversational context and retrieve relevant information, (b) retrieval, to efficiently retrieve passages from the millions present in the index, and (c) re-ranking, to obtain a better rank than the one given by the retrieval stage, by applying a more complex model, trained on large amounts of data which has a better language representation of the text in the query and passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query-Rewriting</head><p>To perform query rewriting, the source of the context are the previous (historical) queries and answers. These will allow us to rewrite the conversational queries to form context-independent queries.</p><p>Incorporation of previous queries. Concatenating previous queries is a simple way of adding context but adds irrelevant terms if the current query is not conversational, or if a topic shift occurred. We propose two simple methods:</p><p>-Prefixing (Pref ) -Prefixes the first query of the conversation to the current query. We use the first query since it is not conversational, and it usually sets the topic for the rest of the conversation.</p><p>-Full-Union -This method performs the union of the current query with all of the previous queries, creating a longer query, based on the number of turns in the conversation. This is a baseline approach that aims to show that choosing the right context is more important than having all of the context in the query.</p><p>Neural Coreference Resolution Model. Many conversational queries have mentions to other entities referenced in previous turns (coreferences). To perform the search in the index, we need to resolve these coreferences to retrieve relevant information. To do this, we used AllenNLP's coreference resolution model <ref type="bibr" coords="3,467.30,231.82,9.96,8.74" target="#b8">[9]</ref>. We use as input the sequence of queries in the conversation and use the output of the model for the last query as the resolved query. With this model, we developed two approaches:</p><p>-Coref -All identified mentions are replaced by the first one. After analyzing some of the outputs, we saw that the model mistakenly replaces mentions that are not coreferent. -Coref-Pronoun -To mitigate the problem identified in the previous approach, we only replace the mention if it is needed, i.e., when pronouns are present in the mention.</p><p>Conversational Query-Rewriting Transformer Model. Another approach that we developed uses the pre-trained, text-to-text transformer model T5 <ref type="bibr" coords="3,462.33,387.36,14.61,8.74" target="#b13">[14]</ref>. This model requires an input sequence and a target given as strings. We followed <ref type="bibr" coords="3,163.28,411.27,15.50,8.74" target="#b9">[10]</ref> and fine-tuned a T5 BASE model using the CANARD dataset <ref type="bibr" coords="3,467.31,411.27,9.96,8.74" target="#b7">[8]</ref>, using as input the concatenation of the current query, a separator token, and the previous turns (query-answer pairs), and as target the query rewritten. The model was fine-tuned according to <ref type="bibr" coords="3,289.19,447.13,15.49,8.74" target="#b9">[10]</ref> for 4000 steps, using a maximum input sequence length of 512 tokens, a maximum output sequence length of 64 tokens, a learning rate of 0.0001, and batches of 256 sequences. During evaluation on TREC CAsT 2019, since historical utterances don't depend on the responses of the system <ref type="bibr" coords="3,313.96,494.96,9.96,8.74" target="#b4">[5]</ref>, the input structure uses only the concatenation of historical queries separated by the same separator token. From the analysis of model's output, we saw that it was capable of performing both implicit and explicit coreference resolution like presented in table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Indexing and Retrieval</head><p>To index and search, we used Anserini <ref type="bibr" coords="3,302.47,579.14,14.61,8.74" target="#b16">[17]</ref>, and in particular, the Python implementation Pyserini 1 . We indexed the dataset removing stop words using Lucene's default list, and applied the stemming algorithm Kstem 2 . In our preliminary experiments using TREC CAsT's 2019 dataset, the use of stemming improved retrieval performance by a large margin. Our initial experiments also showed that LMD (Language Model Dirichlet) was the best performing method when compared to LMJM (Language Model Jelinek-Mercer) and BM25, confirming previous knowledge <ref type="bibr" coords="4,221.61,142.90,15.50,8.74" target="#b17">[18]</ref> that stated that LMD performs best for shorter queries, which are common in a conversational search setting. So the developed methods use LMD as the retrieval model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Passage Re-ranking</head><p>Transformer Re-ranking As re-ranking model, we used the pre-trained neural language model BERT <ref type="bibr" coords="4,254.79,226.23,9.96,8.74" target="#b5">[6]</ref>. This model is capable of generating contextual embeddings for a sentence and each of its tokens. This approach allows us to go beyond simple term matching thanks to the model's understanding of the interactions between the terms in the query and passage, being able to judge more thoroughly if a passage is relevant to a query.</p><p>The model used to perform passage re-ranking, was a BERT BASE model with a linear layer on top, fine-tuned on the query-passage binary relevance estimation task on the MS MARCO dataset following <ref type="bibr" coords="4,371.12,309.91,14.61,8.74" target="#b12">[13]</ref>. Our implementation used Huggingface's Transformer Library <ref type="bibr" coords="4,313.38,321.87,15.50,8.74" target="#b14">[15]</ref> and the fine-tuned model nboost <ref type="foot" coords="4,473.36,320.29,3.97,6.12" target="#foot_2">3</ref> . This model is used to calculate the probability of a passage being relevant given a query. We do this for the top-100 passages retrieved and order them by the resulting probabilities.</p><p>Rank Fusion. Another way of producing a new rank is by combining (fusing) different ranks, where each one can capture different aspects. As fusion algorithm, we used the Reciprocal Rank Fusion (RRF) <ref type="bibr" coords="4,359.36,410.35,10.52,8.74" target="#b2">[3]</ref> defined as:</p><formula xml:id="formula_0" coords="4,241.69,428.75,234.65,26.65">RRF score(d) = i 1 k + r i (p) , (<label>1</label></formula><formula xml:id="formula_1" coords="4,476.35,435.49,4.24,8.74">)</formula><p>where k is a hyperparameter, for which we used the default value of 60, and r i is the rank of passage p in list i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup and Protocol</head><p>CANARD dataset <ref type="bibr" coords="4,231.09,548.75,12.09,8.77" target="#b7">[8]</ref> -This dataset was created by manually rewriting the queries in QuAC <ref type="bibr" coords="4,211.35,560.73,10.52,8.74" target="#b0">[1]</ref> to form non-conversation queries. This amounts to 31.538, 3.418, and 5.571 query-rewrites for the training, validation, and test sets, respectively. We used this dataset to train and evaluate the T5 query-rewriting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC CAsT 2019 dataset [4]</head><p>-This dataset was used to evaluate the performance of the conversational search system. In the evaluation set, each passage's relevance to each query was graded using a value that ranges from 0 (not relevant) to 4 (highly relevant). The passage collection is composed by MS MARCO <ref type="bibr" coords="5,175.92,130.95,14.61,8.74" target="#b10">[11]</ref>, TREC CAR <ref type="bibr" coords="5,252.33,130.95,9.96,8.74" target="#b6">[7]</ref>, and WaPo <ref type="bibr" coords="5,315.17,130.95,15.50,8.74" target="#b11">[12]</ref> datasets, which creates a complete pool of close to 47 million passages (although in the final assessment WaPo was removed from the judgments due to a deduplication problem).</p><p>Protocol. To analyze the performance of the retrieval system, we used the official TREC CAsT 2019 metrics, nDCG@3 (normalized Discounted Cumulative Gain at 3), MAP (Mean Average Precision), and MRR (Mean Reciprocal Rank), as well as, Recall and P@3 (Precision at 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Retrieval. In table 2, we show the results of the various query rewriting techniques developed using LMD as the retrieval model. The first thing that becomes evident is the need for query rewriting methods evidenced by the low scores in the original conversational queries. All the query rewriting methods had the desired effect, increasing all metrics by a considerable margin, approximating the results to the ones obtained using the manually rewritten queries (non-conversational queries). Pref is a simple approach but proved to be useful, even getting better results than Coref and Coref-Pronoun because the model in these last two is not able to detect all of the coreferences. The combination Pref+Coref and Pref+Coref-Pronoun, further improved the results by combining both coreference resolution and concatenation of previous queries. The technique that uses the union of all previous queries and Coref-Pronoun was one of the worst-performing methods because of the long queries with irrelevant terms, showing the importance of choosing the correct context for each turn. With these models, the best query rewriting method was the T5 model achieving the best results in the metrics that evaluate the earlier positions of the rank, which are the most useful for this task.</p><p>After obtaining these results in the re-ranking step, we opted to use only the best-performing methods, so we used the Pref+Coref-Pronoun and T5 methods.</p><p>Re-ranking. In table <ref type="table" coords="5,237.04,500.70,3.87,8.74" target="#tab_2">3</ref>, we show the results on the TREC CAsT 2019 evaluation dataset with the best query rewriting methods discovered in the previous experiment, in conjunction with the BERT BASE model re-ranker on the top-100 passages retrieved. Adding to this, we also present some baselines from TREC CAsT 2019 <ref type="bibr" coords="5,190.36,548.52,9.96,8.74" target="#b4">[5]</ref>. In particular, clacBase <ref type="bibr" coords="5,316.29,548.52,10.52,8.74" target="#b1">[2]</ref> is a method that uses AllenNLP coreference resolution <ref type="bibr" coords="5,233.23,560.48,10.52,8.74" target="#b8">[9]</ref> and a fine-tuned BM25 model with pseudo-relevance feedback, and HistoricalQE <ref type="bibr" coords="5,259.09,572.43,15.50,8.74" target="#b15">[16]</ref> is a method that uses a query expansion algorithm based on session and query words together with a BERT LARGE model for re-ranking. The latter was the best performing method in terms of nDCG@3 in TREC CAsT 2019 <ref type="bibr" coords="5,230.57,608.30,9.96,8.74" target="#b4">[5]</ref>.</p><p>The results show the effectiveness of the re-ranker achieving an improvement over the simple retrieval method in all metrics. This is due to the better understanding that the fine-tuned BERT model has of the interactions between the query and passage terms. With the architecture that uses T5 for query-rewriting  and BERT for re-ranking, we were able to achieve state-of-the-art results. We attribute this to the better query-rewriting method that allows the retrieval model to retrieve passages given the conversational context, providing the reranker with more relevant passages.</p><p>In figure <ref type="figure" coords="6,189.13,560.48,3.87,8.74" target="#fig_0">1</ref>, we show the results of the Original, Manual, and T5 queries with BERT re-ranking in the top-100 in each turn until turn depth 8. As expected, the performance in the original queries drops significantly after the first turn because of the lack of conversational context. In the Manual queries, although we see differences in performance in each turn, these are not directly affected by turn depth. With respect to the T5 queries, we observe a decrease in performance in turn 2 because of the inclusion of conversational elements (coreferences and mentions to previous context), after this, performance increases in turn 3, and in most metric proceeds to decrease the deeper we go into the conversation. This is the expected behaviour and one of the main challenges of conversational search, keeping track of the context in long conversations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>In TREC CAsT 2020 <ref type="bibr" coords="7,230.72,496.60,9.96,8.74" target="#b3">[4]</ref>, the task is similar to our experimental setup but with the added challenge that each query in a conversation can be about any of the previous queries or answers (in 2019 only previous queries were considered). Also provided in this year's edition besides the raw and manually rewritten queries are automatic queries (AUTO). These automatic queries were developed by the organizers <ref type="bibr" coords="7,181.92,556.37,10.52,8.74" target="#b3">[4]</ref> using a query rewriting method comparable to ours.</p><p>With all these elements and the results obtained in section 3, the submitted runs were the following:</p><p>-AUTO BERT-100 -This run uses the automatic rewritten queries provided by the organizers in conjunction with the fine-tuned BERT BASE model to re-rank the top-100 passages retrieved by LMD. -T5 BERT-100 -Uses the queries generated by our fine-tuned T5 model in conjunction with the fine-tuned BERT BASE model to re-rank the top-100 passages retrieved by LMD. -AUTO-T5 BERT-100 -Performs retrieval with both the automatic rewritten queries and T5 generated queries and re-ranks the results of both lists with the fine-tuned BERT BASE model on the top-100 passages. We then join these lists using the Reciprocal Rank Fusion (RRF) algorithm.</p><p>With the first two runs, we want to make a comparison between our query rewriting model, T5, and the query rewriting model developed by the track organizers. With the last run, we want to evaluate if it is possible to use a combination of both queries to achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Submitted Runs Results</head><p>Overall performance. Table <ref type="table" coords="8,276.79,374.08,4.98,8.74" target="#tab_3">4</ref> shows the results of the official metrics obtained in TREC CAsT 2020 with the submitted runs, as well as the median.</p><p>Our submitted runs are superior to the median except nDCG@1000 in AUTO BERT-100 and T5 BERT-100, and in MAP@1000 in T5 BERT-100. As we can see, comparing both AUTO BERT-100, which uses query-rewrites made available by the organizers using another model, and T5-BERT 100, that uses our developed method, we obtain similar results. This shows the competitiveness of the query rewriting component developed. Our last run, which combines the two query-rewriting models using RRF, also achieved similar results to previous runs but achieved a higher value for metrics that evaluate the entire list of returned passages (@1000 metrics). This is due to the algorithm's ability to combine the scores of a passage in both lists in an effective manner. "Loosely" comparing (the queries are not the same) the results in 2019 (table 3) and 2020 (table 4) editions of TREC CAsT in terms of nDCG@3, we see a large decrease in metrics. This demonstrates that the 2020 dataset is more complex and challenging, mainly due to the inclusion of queries that depend on previous system responses (retrieved passages), which our model didn't specifically handle.</p><p>Performance per turn. Figure <ref type="figure" coords="8,282.23,608.30,4.98,8.74" target="#fig_1">2</ref> shows the performance until turn depth 8 of the submitted runs and median of all runs. As it happened in last year's data, performance drops significantly in the second turn due to the conversational aspect. Performance suffers an unexpected increase in our submitted runs in the sixth turn and proceeds to decrease until the last turn. Our results are superior to the median in most turns, with the exception of the first turn, where there is no conversational context. By analyzing these results, we are able to see that having correct rewritten queries is imperative to achieve good performance, being this even more evident in the 2020 edition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we presented a conversational search architecture composed by a three-stage pipeline. We demonstrated the need for a conversational query rewriting component and achieved good results using a T5 model fine-tuned on this task. This model showed good capabilities for coreference resolution while also having the ability to provide context to a query when this is not explicit. We also showed that a BERT-based re-ranking model can be used to further improve the results of conversational information retrieval systems. To summarize, the full architecture composed by the T5 query re-writing model, LMD retrieval model, and BERT re-ranker achieved state-of-the-art results in the TREC CAsT 2019 dataset <ref type="bibr" coords="9,169.69,484.75,9.96,8.74" target="#b3">[4]</ref>.</p><p>The delivered runs for TREC CAsT 2020 showed that the 2020 dataset is more challenging than the previous year and that our query rewriting model is on par with the model developed by the track organizers. The results also showed the need for a model capable of using previous system answers to rewrite the current query.</p><p>As future work, we aim to improve the query rewriting component by adding the previous answers in a way that does not comprise the integrity of the output, such as selecting only previous relevant terms. Concerning the passage ranking model, we want to explore models that can attend to the full conversational context instead of only considering the current query and list of retrieved passages for that turn.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,134.77,376.45,345.83,8.74;7,134.77,388.40,345.83,8.74;7,134.77,400.36,112.14,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Results in TREC CAsT 2019 evaluation set by turn depth of Original, Manual, and T5 queries, using LMD as the retrieval model and BERT BASE re-ranking in the top-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,146.31,249.27,322.74,8.74"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Results by turn depth of the submitted runs to TREC CAsT 2020.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,127.36,345.82,133.07"><head>Table 1 :</head><label>1</label><figDesc>Example of a conversation retrieved from TREC CAsT 2019 training set.</figDesc><table coords="2,135.93,153.98,343.50,106.46"><row><cell cols="2">Turn Type of Query</cell><cell>Conversational Query</cell></row><row><cell cols="3">1 Context-Independent What is a physician's assistant?</cell></row><row><cell>2</cell><cell cols="2">Conversational Context-Independent What are the educational requirements required to become a What are the educational requirements required to become one?</cell></row><row><cell></cell><cell></cell><cell>physician's assistant?</cell></row><row><cell>3</cell><cell cols="2">Conversational Context-Independent What does becoming a physician's assistant cost? What does it cost?</cell></row><row><cell>4</cell><cell cols="2">Conversational Context-Independent What's the average starting salary in the UK for a physician's assistant? What's the average starting salary in the UK?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,127.36,345.82,150.73"><head>Table 2 :</head><label>2</label><figDesc>Results of retrieval on the TREC CAsT 2019 evaluation set. All implemented methods use LMD as the retrieval model.</figDesc><table coords="6,172.23,154.81,270.90,123.28"><row><cell>Queries</cell><cell cols="2">Recall P@3 MAP MRR nDCG@3</cell></row><row><cell>Original</cell><cell>0.454 0.262 0.141 0.336</cell><cell>0.167</cell></row><row><cell>Pref</cell><cell>0.667 0.432 0.227 0.547</cell><cell>0.284</cell></row><row><cell>Coref</cell><cell>0.573 0.360 0.179 0.445</cell><cell>0.238</cell></row><row><cell>Coref-Pronoun</cell><cell>0.619 0.380 0.203 0.486</cell><cell>0.258</cell></row><row><cell>Pref+Coref</cell><cell>0.670 0.420 0.218 0.540</cell><cell>0.281</cell></row><row><cell>Pref+Coref-Pronoun</cell><cell>0.715 0.462 0.246 0.571</cell><cell>0.304</cell></row><row><cell cols="2">Coref-Pronoun+Full-Union 0.623 0.389 0.178 0.528</cell><cell>0.255</cell></row><row><cell>T5</cell><cell cols="2">0.697 0.474 0.251 0.597 0.322</cell></row><row><cell></cell><cell>Manual Baselines</cell><cell></cell></row><row><cell>Manual</cell><cell>0.820 0.590 0.327 0.694</cell><cell>0.406</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,307.11,345.82,173.44"><head>Table 3 :</head><label>3</label><figDesc>Results of retrieval on the TREC CAsT 2019 evaluation set. All implemented methods use LMD as the retrieval model.</figDesc><table coords="6,159.17,334.56,297.01,145.99"><row><cell>Queries</cell><cell cols="5">Re-ranker Recall P@3 MAP MRR nDCG@3</cell></row><row><cell>Original</cell><cell>-</cell><cell cols="3">0.454 0.262 0.141 0.336</cell><cell>0.167</cell></row><row><cell>Original</cell><cell>BERT</cell><cell cols="3">0.454 0.382 0.167 0.463</cell><cell>0.276</cell></row><row><cell cols="2">Pref+Coref-Pronoun -</cell><cell cols="3">0.715 0.462 0.246 0.571</cell><cell>0.304</cell></row><row><cell cols="2">Pref+Coref-Pronoun BERT</cell><cell cols="3">0.715 0.565 0.282 0.692</cell><cell>0.418</cell></row><row><cell>T5</cell><cell>-</cell><cell cols="3">0.697 0.474 0.251 0.597</cell><cell>0.322</cell></row><row><cell>T5</cell><cell>BERT</cell><cell cols="4">0.697 0.611 0.297 0.724 0.461</cell></row><row><cell></cell><cell></cell><cell cols="4">TREC CAsT 2019 baselines</cell></row><row><cell>clacBase [2]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.246 0.640</cell><cell>0.360</cell></row><row><cell>HistoricalQE [16]</cell><cell>BERT</cell><cell>-</cell><cell>-</cell><cell>0.267 0.715</cell><cell>0.436</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Manual Baselines</cell><cell></cell></row><row><cell>Manual</cell><cell>-</cell><cell cols="3">0.820 0.590 0.327 0.694</cell><cell>0.406</cell></row><row><cell>Manual</cell><cell>BERT</cell><cell cols="3">0.820 0.726 0.372 0.874</cell><cell>0.569</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,138.78,127.36,337.79,72.62"><head>Table 4 :</head><label>4</label><figDesc>Performance of submitted runs on TREC CAsT 2020 evaluation set.</figDesc><table coords="8,157.80,142.86,299.76,57.13"><row><cell>Run</cell><cell cols="4">nDCG@3 nDCG@5 nDCG@1000 MAP@1000</cell></row><row><cell>Median</cell><cell>0.280</cell><cell>0.274</cell><cell>0.375</cell><cell>0.180</cell></row><row><cell>AUTO BERT-100</cell><cell>0.304</cell><cell>0.296</cell><cell>0.364</cell><cell>0.182</cell></row><row><cell>T5 BERT-100</cell><cell>0.301</cell><cell>0.298</cell><cell>0.353</cell><cell>0.177</cell></row><row><cell>AUTO-T5 BERT-100</cell><cell>0.302</cell><cell>0.296</cell><cell>0.377</cell><cell>0.185</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,645.84,152.25,7.86"><p>https://github.com/castorini/pyserini</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,656.80,166.54,7.86"><p>http://lexicalresearch.com/kstem-doc.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,656.80,251.89,7.86"><p>https://huggingface.co/nboost/pt-bert-base-uncased-msmarco</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,142.36,337.64,7.86;10,151.52,153.29,329.07,7.89;10,151.52,164.28,111.70,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,165.91,153.32,152.66,7.86">Quac : Question answering in context</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR abs/1808.07036</idno>
		<ptr target="http://arxiv.org/abs/1808.07036" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,175.19,337.64,7.86;10,151.52,186.15,329.07,7.86;10,151.52,197.11,329.07,7.86;10,151.52,208.07,329.07,7.86;10,151.52,219.03,329.07,7.86;10,151.52,229.99,87.85,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,216.36,175.19,260.39,7.86">Waterlooclarke at the TREC 2019 conversational assistant track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec28/papers/WaterlooClarke.C.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,310.48,186.15,170.11,7.86;10,151.52,197.11,86.39,7.86">Proceedings of the Twenty-Eighth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>the Twenty-Eighth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-13">2019. November 13-15, 2019. 2019</date>
			<biblScope unit="volume">1250</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>TREC</note>
</biblStruct>

<biblStruct coords="10,142.96,240.90,337.64,7.86;10,151.52,251.86,329.07,7.86;10,151.52,262.82,329.07,7.86;10,151.52,273.78,329.07,7.86;10,151.52,284.74,329.07,7.86;10,151.52,295.70,329.07,7.86;10,151.52,306.66,67.07,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,350.56,240.90,130.04,7.86;10,151.52,251.86,216.96,7.86">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>BÃ¼ttcher</surname></persName>
		</author>
		<idno type="DOI">10.1145/1571941.1572114</idno>
		<ptr target="https://doi.org/10.1145/1571941.1572114" />
	</analytic>
	<monogr>
		<title level="m" coord="10,328.59,262.82,151.99,7.86;10,151.52,273.78,329.07,7.86;10,151.52,284.74,110.24,7.86">Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">July 19-23, 2009. 2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
	<note>SIGIR 2009</note>
</biblStruct>

<biblStruct coords="10,142.96,317.57,337.64,7.86;10,151.52,328.53,136.00,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,290.00,317.57,190.59,7.86">The trec conversational assistance track CAsT</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<ptr target="http://www.treccast.ai/" />
		<imprint>
			<date type="published" when="2020">1 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,339.44,337.64,7.86;10,151.52,350.37,329.07,7.89" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,288.08,339.44,192.52,7.86;10,151.52,350.40,56.13,7.86">TREC cast 2019: The conversational assistance track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno>CoRR abs/2003.13624</idno>
		<ptr target="https://arxiv.org/abs/2003.13624" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,361.31,337.63,7.86;10,151.52,372.25,329.07,7.89;10,151.52,383.23,131.41,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,338.64,361.31,141.95,7.86;10,151.52,372.27,189.90,7.86">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,394.15,337.63,7.86;10,151.52,405.10,180.34,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,295.55,394.15,185.04,7.86;10,151.52,405.10,33.07,7.86">Trec car 2.1: A data set for complex answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gamari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<ptr target="http://trec-car.cs.unh.edu" />
		<imprint>
			<date type="published" when="2018">7 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,416.02,337.63,7.86;10,151.52,426.98,329.07,7.86;10,151.52,437.94,329.07,7.86;10,151.52,448.89,329.07,7.86;10,151.52,459.85,329.07,7.86;10,151.52,470.81,329.07,7.86;10,151.52,481.77,318.31,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,352.97,416.02,127.62,7.86;10,151.52,426.98,144.21,7.86">Can you unpack that? learning to rewrite questions-in-context</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Peskov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1605</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1605" />
	</analytic>
	<monogr>
		<title level="m" coord="10,180.18,437.94,300.41,7.86;10,151.52,448.89,329.07,7.86;10,151.52,459.85,104.27,7.86">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="5917" to="5923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,492.69,337.64,7.86;10,151.52,503.62,286.35,7.89" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,329.90,492.69,150.70,7.86;10,151.52,503.64,14.75,7.86">End-to-end neural coreference resolution</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR abs/1707.07045</idno>
		<ptr target="http://arxiv.org/abs/1707.07045" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,514.56,337.97,7.86;10,151.52,525.52,329.07,7.86;10,151.52,536.45,302.08,7.89" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,396.85,514.56,83.74,7.86;10,151.52,525.52,329.07,7.86;10,151.52,536.48,26.15,7.86">Conversational question reformulation via sequence-to-sequence architectures and pretrained language models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/2004.01909</idno>
		<ptr target="https://arxiv.org/abs/2004.01909" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,547.39,337.98,7.86;10,151.52,558.35,329.07,7.86;10,151.52,569.28,236.95,7.89" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,151.52,558.35,297.47,7.86">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno>CoRR abs/1611.09268</idno>
		<ptr target="http://arxiv.org/abs/1611.09268" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,580.22,337.97,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,178.81,580.22,114.55,7.86">Trec washington post corpus</title>
		<author>
			<persName coords=""><surname>Nist</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/data/wapost/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,591.11,337.98,7.89;10,151.52,602.09,162.64,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,247.90,591.13,123.73,7.86">Passage re-ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR abs/1901.04085</idno>
		<ptr target="http://arxiv.org/abs/1901.04085" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,613.01,337.97,7.86;10,151.52,623.97,329.07,7.86;10,151.52,634.90,317.45,7.89" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,209.74,623.97,270.85,7.86;10,151.52,634.93,45.40,7.86">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR abs/1910.10683</idno>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,645.84,337.98,7.86;10,151.52,656.80,329.07,7.86;11,151.52,119.65,329.07,7.89;11,151.52,130.63,79.43,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,332.27,656.80,148.32,7.86;11,151.52,119.67,139.29,7.86">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno>CoRR abs/1910.03771</idno>
		<ptr target="http://arxiv.org/abs/1910.03771" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,141.59,337.97,7.86;11,151.52,152.55,329.07,7.86;11,151.52,163.51,329.07,7.86;11,151.52,174.47,329.07,7.86;11,151.52,185.43,329.07,7.86;11,151.52,196.39,107.06,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,341.41,141.59,139.18,7.86;11,151.52,152.55,78.16,7.86">Query and answer expansion from conversation history</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsai</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec28/papers/CFDACLIP.C.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,372.63,152.55,107.96,7.86;11,151.52,163.51,138.67,7.86">Proceedings of the Twenty-Eighth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>the Twenty-Eighth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-13">2019. November 13-15, 2019. 2019</date>
			<biblScope unit="volume">1250</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>TREC</note>
</biblStruct>

<biblStruct coords="11,142.62,207.34,337.97,7.86;11,151.52,218.30,329.07,7.86;11,151.52,229.26,329.07,7.86;11,151.52,240.22,329.07,7.86;11,151.52,251.18,329.07,7.86;11,151.52,262.14,166.72,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,266.98,207.34,213.61,7.86;11,151.52,218.30,66.98,7.86">Anserini: Enabling the use of lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Vries</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080721</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080721" />
	</analytic>
	<monogr>
		<title level="m" coord="11,201.13,229.26,279.46,7.86;11,151.52,240.22,194.34,7.86">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>White</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">W</forename></persName>
		</editor>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">August 7-11, 2017. 2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,273.10,337.97,7.86;11,151.52,284.06,329.07,7.86;11,151.52,295.02,329.07,7.86;11,151.52,305.98,329.07,7.86;11,151.52,316.93,329.07,7.86;11,151.52,327.89,92.67,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,245.02,273.10,235.57,7.86;11,151.52,284.06,154.22,7.86">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.384019</idno>
		<ptr target="https://doi.org/10.1145/383952.384019" />
	</analytic>
	<monogr>
		<title level="m" coord="11,329.84,284.06,150.75,7.86;11,151.52,295.02,329.07,7.86;11,151.52,305.98,53.58,7.86;11,259.82,305.98,39.32,7.86">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;01</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
