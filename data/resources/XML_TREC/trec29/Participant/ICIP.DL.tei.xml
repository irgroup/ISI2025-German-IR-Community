<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.43,115.96,296.50,12.62">ICIP at TREC-2020 Deep Learning Track</title>
				<funder>
					<orgName type="full">University of Chinese Academy of Sciences</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.20,155.49,65.03,8.74"><forename type="first">Xuanang</forename><surname>Chen</surname></persName>
							<email>chenxuanang19@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Laboraroty of Chinese Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.12,155.49,32.25,8.74"><forename type="first">Ben</forename><surname>He</surname></persName>
							<email>benhe@ucas.ac.cn</email>
						</author>
						<author>
							<persName coords="1,315.26,155.49,30.59,8.74"><surname>Le Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Laboraroty of Chinese Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.77,155.49,50.92,8.74"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
							<email>yfsun@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Laboraroty of Chinese Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.43,115.96,296.50,12.62">ICIP at TREC-2020 Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3EF2BAB138CF95AFBC40E550A281307</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the ICIP participation in TREC 2020 Deep Learning Track. We apply BERT [1]  to the re-ranking subtask of the document ranking task, with an adoption of the passage-level BERT re-ranker <ref type="bibr" coords="1,202.25,309.52,9.22,7.86" target="#b1">[2]</ref>. We utilize both the passage and document ranking dataset for model training, and the noisy training samples in generated document training set will be filtered, to guarantee and boost the ranking effectiveness. Additionally, we also distill smaller BERT models, on top of the recent knowledge distillation (KD) method on BERT, called Simplified TinyBERT [3], to investigate the influence of KD on the document ranking task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ICIP participation in the TREC 2020 Deep Learning track aims to study how to learn neural IR models out of the large-scale training data, and how to improve the effectiveness of BERT-based ranking models, and additionally how to distill the large and expensive BERT-based ranking model to the small, efficient and effective one. In our experiments, we only focus on the re-ranking subtask of the document ranking task.</p><p>Recently, contextual pre-trained language model, like BERT <ref type="bibr" coords="1,408.29,522.74,9.96,8.74" target="#b0">[1]</ref>, has advanced the state-of-the-art results on several ranking tasks <ref type="bibr" coords="1,359.23,534.70,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="1,370.85,534.70,4.10,8.74" target="#b3">[4]</ref><ref type="bibr" coords="1,374.95,534.70,4.10,8.74" target="#b4">[5]</ref><ref type="bibr" coords="1,374.95,534.70,4.10,8.74" target="#b5">[6]</ref><ref type="bibr" coords="1,379.04,534.70,8.19,8.74" target="#b6">[7]</ref>. Besides, the Knowledge Distillation (KD) technique <ref type="bibr" coords="1,279.88,546.65,10.52,8.74" target="#b7">[8]</ref> has been applied to compress BERT model for fast inference while maintaining the comparable performance on multiple NLP tasks <ref type="bibr" coords="1,186.65,570.56,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,199.30,570.56,4.24,8.74" target="#b8">[9]</ref><ref type="bibr" coords="1,203.54,570.56,4.24,8.74" target="#b9">[10]</ref><ref type="bibr" coords="1,203.54,570.56,4.24,8.74" target="#b10">[11]</ref><ref type="bibr" coords="1,203.54,570.56,4.24,8.74" target="#b11">[12]</ref><ref type="bibr" coords="1,203.54,570.56,4.24,8.74" target="#b12">[13]</ref><ref type="bibr" coords="1,207.79,570.56,12.73,8.74" target="#b13">[14]</ref>. Therefore, we utilize BERT model as the document reranker to score and re-rank the candidate documents, and meanwhile apply the KD method to the BERT re-ranker, to investigate the effectiveness of distilled BERT models on the document ranking task.</p><p>The rest of the paper is organized as follows. Section 2 gives a detailed introduction to the approach used in our experiments. Section 3 presents the experimental settings, models and ranking results. Finally, Section 4 concludes our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we give a detailed introduction to our approach for the document re-ranking subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Split and Passage Filter</head><p>Due to the maximum sequence length limitation of BERT, all documents are split into up to 30 overlapping passages (150 whole words and 75 overlapping words), and the title is added to the beginning of every passage if it is available. Simply, for the training set, the passages from a relevant document can be considered as relevant, and the passages from a irrelevant document as irrelevant <ref type="bibr" coords="2,447.11,263.09,9.96,8.74" target="#b1">[2]</ref>. But this simple process may produce a number of noisy training examples, which will negatively affect training a robust model.</p><p>Different from this simple process in previous work <ref type="bibr" coords="2,375.11,299.66,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,386.87,299.66,11.62,8.74" target="#b14">15]</ref>, we add a passage filter step to clean the training samples generated by the document split step, as the document is labeled relevant if it contains a relevant passage (paragraph or chunk) in the MS MARCO document dataset. In the passage filter step, we use a passage-level BERT re-ranker to filter out the irrelevant passages split from relevant documents for a query. More specifically, we only preserve the top-ranked passages in a relevant document according the relevance scores of passages given by a BERT ranker fine-tuned on passage ranking dataset. But note that we still preserve all overlapping passages for the validation and test set, to really judge the relevance of a query-document pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage-level BERT Re-ranker</head><p>For the passage-level BERT re-ranker <ref type="bibr" coords="2,296.81,464.13,9.96,8.74" target="#b1">[2]</ref>, we adopt the BERT-Large model (bertlarge-uncased, 24 layers), which also behaves as the teacher model in the KD training procedure where its ranking knowledge will be distilled to the smaller BERT model (12 layers). The passage text could be truncated such that the concatenation of query, passage, and separator tokens has the maximum length of 256 tokens. The input format of BERT re-ranker</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>is [CLS] [query] [SEP] [passage]</head><p>[SEP], and the final pooled hidden vector of the [CLS] token is fed into a single layer feed-forward network to obtain the probability (score) of the passage being relevant. We adopt the same settings in <ref type="bibr" coords="2,319.38,559.77,10.52,8.74" target="#b3">[4]</ref> to fine-tune BERT model to the document re-ranking task.</p><p>The BERT re-ranker predicts the relevance of each passage with a query independently. After that, we generate the score of a document according to the scores of its split passages, namely, the max score of the passages (MaxP) or the average score of the top-K passages (K-Max-AvgP). Eventually, all candidate documents are re-ranked by the document scores received. In our experiments, K is set as 2, and we demonstrate the better effectiveness of the 2-Max-AvgP compared to MaxP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Knowledge Distillation on BERT</head><p>Due to the expensive computation cost of BERT during inference, some knowledge distillation methods on BERT have been proposed, such as DistilBERT <ref type="bibr" coords="3,462.32,153.92,14.61,8.74" target="#b9">[10]</ref>, BERT-PKD <ref type="bibr" coords="3,191.51,165.88,14.61,8.74" target="#b10">[11]</ref>, TinyBERT <ref type="bibr" coords="3,265.28,165.88,9.96,8.74" target="#b8">[9]</ref>, MobileBERT <ref type="bibr" coords="3,343.21,165.88,15.50,8.74" target="#b11">[12]</ref> and MiniLM <ref type="bibr" coords="3,421.57,165.88,14.61,8.74" target="#b13">[14]</ref>, to distill a large BERT model (teacher) into a smaller BERT model (student), which not only has a faster inference speed but also maintains the comparable performance.</p><p>In our experiments, we adopt the Simplified TinyBERT method <ref type="bibr" coords="3,437.84,202.35,10.52,8.74" target="#b2">[3]</ref> to distill smaller BERT re-ranker. Simplified TinyBERT simultaneously distills the representation of embedding and hidden states, the attention behavior and the probability output (soft label) of the teacher BERT model, and also adds hard label (supervision signals from the training examples) to further boost the document ranking performance of smaller BERT models. We refer the readers to the paper <ref type="bibr" coords="3,179.90,274.08,10.52,8.74" target="#b2">[3]</ref> for further details of the KD procedure of Simplified TinyBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, the implementation details of our experiments are described, followed by the ranking performance of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>The document corpus contains about 3.2 million documents, 367,013 training queries, 5,193 validation queries, and 200 queries for test. We use the official top-100 result file of queries to generate train triples, validation and test pairs. For the passage corpus, we choose the Train Triples Small as the training samples, and there is no need to apply the split and filter step here.</p><p>In generating the training triples, for a query, after splitting the top-100 documents, we sample a negative passage in the passage pool of irrelevant documents for every passage in the relevant document. Based on this preliminary training triples (query, positive passage, negative passage, about 4,343k), we utilize a passage filter (BERT-Large model released in <ref type="bibr" coords="3,335.33,512.05,10.79,8.74" target="#b3">[4]</ref>) to remove the training triples which contains a fake positive passage, and only retain the training triples which contains the five top-ranked positive passages. Eventually, the total training data contains about 1,646k training triples for model fine-tuning and distillation. But for query-passage pairs in validation and evaluation sets, we preserve all passages of a document, in order to guarantee getting the real performance of the BERT re-ranker.</p><p>We carry out our experiments on three TITAN RTX 24G GPUs with Mixed Precision Training <ref type="bibr" coords="3,220.04,608.30,14.61,8.74" target="#b15">[16]</ref>. We use Adam optimizer with a weight decay of 0.01 with a learning rate 1e-06 for fine-tuning and a learning rate 5e-05 for distillation. Models are fine-tuned and distilled with batch size of 32 and 64, respectively. The model with the best MRR@10 metric on validation set is chosen, and evaluated on test sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models and Results</head><p>The details of different models are summarized in Table <ref type="table" coords="4,386.62,451.49,3.87,8.74" target="#tab_0">1</ref>, which contains the models that produced our three submitted runs. There is a lit difference on the effectiveness of 2-Max-AvgP aggregation way on TREC 2019 and 2020 DL Test set, compared to MaxP aggregation way.</p><p>From Table <ref type="table" coords="5,189.77,166.81,3.87,8.74" target="#tab_1">2</ref>, on TREC 2019 DL Test set, we can see that Model 2 significantly outperforms Model 1 in terms of MRR and NDCG@10, but 2-Max-AvgP aggregation way fails to improve Model 4; meanwhile, 2-Max-AvgP aggregation way behaves better than MaxP aggregation way on TREC 2020 DL Test set (Model 1 vs. Model 2, and Model 3 vs. Model 4). There is still a bit large of margin on the ranking performance between distilled models (Model 5 and Model 6) and the teacher model (Model 3) on TREC 2020 DL Test set. The two distilled models (Model 5 and Model 6) behave similarly, and adding hard label could boost the NDCG@10 metric on both test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we describe the system based on BERT model for the document re-ranking subtask in TREC 2020 Deep Learning track. In the passage-level BERT ranker setting, the BERT ranker fine-tuned on passage ranking dataset can be transferred to the document ranking task effectively. The superiority of K-Max-AvgP aggregation way seems to be related to the model settings and test data, compared to the MaxP aggregation way. Distilled BERT models do not behave as well as in <ref type="bibr" coords="5,243.94,387.44,9.96,8.74" target="#b2">[3]</ref>, may mainly because of the different setting of the teacher model. We plan to investigate these issues in future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,115.91,345.83,127.68"><head>Table 1 .</head><label>1</label><figDesc>The summary of models. Aggregation refers the way to get the score of a document according to the scores of its split passages during inference, MaxP means the max score of passages and 2-Max-AvgP means the average score of the top-2 ranked passages.</figDesc><table coords="4,140.69,170.77,338.82,72.82"><row><cell>Model ID</cell><cell>Model (Config)</cell><cell>Training Dataset</cell><cell>Distillation</cell><cell>Aggregation</cell></row><row><cell>1 2</cell><cell>BERT Large (L24 H1024 A16)</cell><cell>Passage</cell><cell>-</cell><cell>MaxP 2-Max-AvgP</cell></row><row><cell>3 4</cell><cell cols="2">BERT Large (L24 H1024 A16) Passage &amp; Document</cell><cell>-</cell><cell>MaxP 2-Max-AvgP</cell></row><row><cell>5 6</cell><cell>BERT Distilled (L12 H1024 A16)</cell><cell>Document</cell><cell>One Step One Step + Hard Label</cell><cell>MaxP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,260.29,345.83,133.77"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results on TREC 2019 &amp; 2020 DL test queries in document reranking subtask, which contains 43 and 45 queries, respectively. Statistical significance at p-value &lt; 0.05 is marked with Model ID for comparisons to each model.</figDesc><table coords="4,147.81,302.98,319.23,91.08"><row><cell>Model ID</cell><cell cols="5">TREC 2019 DL Test MRR NDCG@10 MAP MRR NDCG@10 MAP TREC 2020 DL Test</cell></row><row><cell>1</cell><cell>0.9225 2</cell><cell>0.6793 2</cell><cell>0.2877 6 0.9444</cell><cell>0.6440</cell><cell>0.4274 2</cell></row><row><cell cols="4">2 (ICIP run3) 0.9729 1 0.7086 1 0.2907 4,6 0.9667</cell><cell>0.6528</cell><cell>0.4360 1,5,6</cell></row><row><cell cols="2">3 (ICIP run1) 0.9554</cell><cell>0.6886</cell><cell cols="2">0.2815 0.9630 0.6623 5,6</cell><cell>0.4333</cell></row><row><cell>4</cell><cell>0.9496</cell><cell>0.6857</cell><cell cols="3">0.2752 2 0.9667 0.6685 5,6 0.4389 5,6</cell></row><row><cell cols="2">5 (ICIP run2) 0.9535</cell><cell>0.6823</cell><cell cols="2">0.2785 0.9407 0.6322 3,4</cell><cell>0.4206 2,4</cell></row><row><cell>6</cell><cell>0.9477</cell><cell>0.6871</cell><cell cols="2">0.2734 1,2 0.9333 0.6353 3,4</cell><cell>0.4192 2,4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,134.77,463.42,345.83,201.44"><head>Table 2 .</head><label>2</label><figDesc>Model 1 is the BERT-Large model fine-tuned on passage ranking dataset (Train Triples Small), adopting MaxP aggregation way to get the score of a document from the scores of its split passages. Model 2 is the same as Model 1, but it adopts 2-Max-AvgP aggregation way, and also produces our submission ICIP run3. Model 3 and Model 4 are first fine-tuned on passage ranking dataset, then fine-tuned on document ranking dataset (training triples generated as in Section 2.1), adopting MaxP and 2-Max-AvgP aggregation way, respectively. Model 5 and Model 6 are 12-layer models distilled from the BERT-Large model (Model 3 or Model 4) in the Simplified TinyBERT distillation setting on document ranking dataset, adopting MaxP aggregation way. Model 3 and Model 5 produce our submission ICIP run1 and ICIP run2, respectively.The evaluation results of our models for document re-ranking are shown in For a more comprehensive comparison, we present the evaluation results on both TREC 2019 and 2020 DL Test set. The best values are highlighted in boldface and statistical significance for paired two-tailed t-test is reported. From the results above, we find that Model 2 outperforms other models on TREC DL 2019 Test set, and meanwhile Model 4 behaves better than other models on TREC DL 2020 Test set.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">University of Chinese Academy of Sciences</rs>.</p></div>
<div><head>References</head></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,142.96,505.27,337.63,7.86;5,151.52,516.23,329.07,7.86;5,151.52,527.19,222.80,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,338.64,505.27,141.95,7.86;5,151.52,516.23,190.54,7.86">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,364.76,516.23,68.98,7.86">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,537.83,337.63,7.86;5,151.52,548.79,218.03,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,230.79,537.83,249.80,7.86;5,151.52,548.79,60.93,7.86">Deeper text understanding for IR with contextual neural language modeling</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,233.95,548.79,23.62,7.86">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,559.44,337.63,7.86;5,151.52,570.37,249.22,7.89" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,329.88,559.44,150.70,7.86;5,151.52,570.39,114.77,7.86">Simplified tinybert: Knowledge distillation for document retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/2009.07531</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,581.01,337.64,7.89;5,151.52,592.00,25.60,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,247.90,581.04,123.73,7.86">Passage re-ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR abs/1901.04085</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,602.64,337.63,7.86;5,151.52,613.57,250.72,7.89" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,312.29,602.64,168.30,7.86;5,151.52,613.60,115.86,7.86">Investigating the successes and failures of BERT for passage re-ranking</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Padigela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno>CoRR abs/1905.01758</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,624.24,337.63,7.86;5,151.52,635.17,163.92,7.89" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,309.37,624.24,171.21,7.86;5,151.52,635.20,28.91,7.86">Understanding the behaviors of BERT in ranking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR abs/1904.07531</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,645.84,337.64,7.86;5,151.52,656.77,166.51,7.89" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,274.73,645.84,205.86,7.86;5,151.52,656.80,32.07,7.86">Simple applications of BERT for ad hoc document retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/1903.10972</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,119.67,337.63,7.86;6,151.52,130.61,127.81,7.89" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,299.16,119.67,177.10,7.86">Distilling the knowledge in a neural network</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,141.59,337.64,7.86;6,151.52,152.52,329.07,7.89;6,151.52,163.51,25.60,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,442.43,141.59,38.17,7.86;6,151.52,152.55,218.66,7.86">Tinybert: Distilling BERT for natural language understanding</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR abs/1909.10351</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,174.47,337.98,7.86;6,151.52,185.40,304.51,7.89" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,349.56,174.47,131.03,7.86;6,151.52,185.43,170.07,7.86">Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno>CoRR abs/1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,196.39,337.97,7.86;6,151.52,207.34,329.07,7.86;6,151.52,218.30,98.02,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,294.42,196.39,186.17,7.86;6,151.52,207.34,47.05,7.86">Patient knowledge distillation for BERT model compression</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,220.35,207.34,71.92,7.86">EMNLP/IJCNLP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4322" to="4331" />
			<date type="published" when="2019">2019</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,229.26,337.97,7.86;6,151.52,240.20,327.37,7.89" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="6,387.39,229.26,93.20,7.86;6,151.52,240.22,192.61,7.86">Mobilebert: a compact task-agnostic BERT for resource-limited devices</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR abs/2004.02984</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,251.18,337.98,7.86;6,151.52,262.11,329.07,7.89;6,151.52,273.10,25.60,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="6,392.29,251.18,88.30,7.86;6,151.52,262.14,218.07,7.86">Distilling task-specific knowledge from BERT into simple neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/1903.12136</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,284.06,337.98,7.86;6,151.52,295.02,329.07,7.86;6,151.52,305.95,127.81,7.89" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,404.72,284.06,75.88,7.86;6,151.52,295.02,324.98,7.86">Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR abs/2002.10957</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,316.93,337.98,7.86;6,151.52,327.89,329.07,7.86;6,151.52,338.85,106.24,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,293.06,316.93,169.17,7.86">UCAS at TREC-2019 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,151.52,327.89,134.16,7.86">TREC. NIST Special Publication</title>
		<imprint>
			<biblScope unit="volume">1250</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,349.81,337.97,7.86;6,151.52,360.77,329.07,7.86;6,151.52,371.73,210.26,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,415.57,360.77,65.03,7.86;6,151.52,371.73,30.51,7.86">Mixed precision training</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,202.99,371.73,130.13,7.86">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
