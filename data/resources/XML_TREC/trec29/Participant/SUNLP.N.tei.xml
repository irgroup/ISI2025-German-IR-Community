<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,193.60,172.49,223.04,15.20">SU-NLP at TREC NEWS 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,129.15,206.34,59.43,10.56"><forename type="first">Ali</forename><forename type="middle">Eren</forename><surname>Ak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sabancı University</orgName>
								<address>
									<settlement>İstanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.55,206.34,75.29,10.56"><forename type="first">Çağhan</forename><surname>Köksal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sabancı University</orgName>
								<address>
									<settlement>İstanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.13,206.34,77.88,10.56"><forename type="first">Kenan</forename><surname>Fayoumi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sabancı University</orgName>
								<address>
									<settlement>İstanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,393.50,206.34,87.60,10.56;1,481.10,204.79,1.41,6.99"><forename type="first">Reyyan</forename><surname>Yeniterzi</surname></persName>
							<email>reyyan.yeniterzi@sabanciuniv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sabancı University</orgName>
								<address>
									<settlement>İstanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,193.60,172.49,223.04,15.20">SU-NLP at TREC NEWS 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B450AF814AE4DA4E4F833182CF0E020</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our work and submissions for the TREC 2020 News Track: background linking and wikification tasks. For the background linking task, we investigated utilization of transformer-based architectures as either to map news articles to sentence embeddings, or to extract summaries of queried news articles or to re-rank the candidate documents. Even though none of these approaches outperformed our regular full-text baseline search approach, they provided some insights and future research directions. In the wikification task, after extracting and linking the entities to their corresponding entities, we analyzed two ranking approaches; one depends on entities positions within the text and the other depends on vector similarities between the news article and entity's linked page.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Use of the online news services has been dramatically increasing in the last couple of years. Pew Research studies states that in 2018 93% of American adults get at least some of their news online <ref type="bibr" coords="1,340.76,531.45,10.84,9.63" target="#b0">[1]</ref>. Although the common assumption is that the readers have the necessary background knowledge about the context and entities mentioned in any news article, this expectation may not be valid for many readers. TREC News Track is specifically focusing on this problem by introducing two tasks, background linking and wikification. The purpose of the background linking task is to retrieve relevant news articles which help readers to understand the context and obtain background knowledge of a given news article (the query article). On the other hand, the wikification task goes one step further and uses the Wikipedia articles too, in order to provide more context to the readers on important short passages within the news article.</p><p>We participated to both of these tasks. For the background linking task, we specifically investigated the capabilities of recent transformer-based architectures on representing news articles. We explored different ways to represent the news articles, such as encoding them to vectors or using the extracted summaries. Even though, these proposed approaches did not outperform some classical baseline approaches, they provided some useful insight on their capabilities in this background linking task.</p><p>We approached to the wikification task as it is an entity linking and ranking task. Therefore, we started by applying the standard entity linking pipeline. Initially identified the entity mentions by using a NER tool and then retrieved and ranked the entity corresponding Wikipedia pages at the same time by using a search engine. Finally, we ranked the linked Wikipedia pages based on their relevance to the news article. In this final ranking part, we explored both the position of entities in the news article and document vector similarity between the news article and entities' wiki-pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>This year, third version of the TREC Washington Post Collection was released. This version is different from its priors as it contains more documents (around 150K) and the near duplicates were eliminated. The same Wikipedia dump from previous years TREC News was also used this year.</p><p>The background linking task contains 50 new topics along with their Washington Post articles. These 50 topics are also used for the Wikification task. Topics from previous years were used to fine-tune and evaluate our models during development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background Linking Task</head><p>This is the third year of the background linking task. Last year only one participant group <ref type="bibr" coords="2,211.89,575.84,11.45,9.63" target="#b1">[2]</ref> explored BERT for this task. This year we used transformer architectures in different ways to better understand their capabilities in this task. More specifically, we examined the effectiveness of sentence embeddings by creating dense vector representations of passages using the Universal Sentence Encoder. Additionally, we explored the prominent NLP model BERT in two ways, <ref type="bibr" coords="2,262.58,643.59,13.86,9.63" target="#b0">(1)</ref> for creating summaries of the topic news articles and (2) for re-ranking news articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our Approach</head><p>In this section, we briefly describe our baseline bag-of-words and transformerbased approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Baseline (FullText)</head><p>As a baseline approach, we used the bag-of-words approach with full-text indexing and querying. ElasticSearch's<ref type="foot" coords="3,300.48,228.39,4.23,7.04" target="#foot_0">1</ref> default settings were used for indexing and ranking. Only a date filter was applied in order to limit the retrieval to the news documents which were published before the queried news article. The same date filtering was used in all the following approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Universal Sentence Encoder (USE)</head><p>Universal Sentence Encoder <ref type="bibr" coords="3,260.84,320.37,11.45,9.63" target="#b2">[3]</ref> is a neural network model which can encode any varying length text into a 512-dimensional dense vector. Transformer or Deep Averaging Network (DAN) models are used as the encoder architecture.</p><p>In this paper, all Washington Post news articles were mapped to their USE vectors by using the available pretrained models<ref type="foot" coords="3,374.03,372.63,4.23,7.04" target="#foot_1">2</ref> and then these vectors were indexed with ElasticSearch. Article vectors' cosine similarity scores with respect to the query news article's vector were used to retrieve and rank them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">BERT Summarization (BERT Summ)</head><p>The goal of extractive summarization is to extract a subset of sentences from a provided text which are the most important and key sentences for summarizing the corresponding input text. Since background linking task's queries consist of long news articles, extractive summarization can be useful in identification of the useful sub-parts of the text that can be used for querying.</p><p>In order to investigate BERT's capabilities on identification of the key sentences from a text, BERT Extractive Summarization tool<ref type="foot" coords="3,407.30,557.53,4.23,7.04" target="#foot_2">3</ref> was used. This tool is based on a model which creates sentence embeddings and then clusters them in order to choose the central ones <ref type="bibr" coords="3,320.81,586.56,10.85,9.63" target="#b3">[4]</ref>. With this tool, on average 800 word long news articles were reduced to 180 words long summaries. Then these extracted summaries were used as queries to retrieve and rank the news articles which were indexed as full-text (without the summarization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">BERT Re-Ranking (BERT RR)</head><p>Both Universal Sentence Encoder and BERT Extractive Summarization tools were used as they are without any fine-tuning for our background linking task. In order to investigate the effects of task specific fine-tuning, we adapted the next sentence prediction task used during pre-training of BERT. This idea was successfully applied to ad-hoc document ranking before <ref type="bibr" coords="4,458.20,250.38,11.45,9.63" target="#b4">[5,</ref><ref type="bibr" coords="4,473.00,250.38,7.64,9.63" target="#b5">6]</ref>. The authors used to re-rank a set of retrieved candidate documents by using the query-document similarity scores calculated after processing the query as first text segment and document as the next one.</p><p>In this paper, we follow the work of CEDR <ref type="bibr" coords="4,345.35,304.57,10.85,9.63" target="#b4">[5]</ref>. We used the news articles from our baseline full-text approach as the initial candidates. Later on BERT with a next sentence prediction layer was used to re-rank these news articles based on their similarity to the query news article. In order to better analyze the affects of fine-tuning, this re-ranking part was performed both with finetuning and without it. During the one with fine-tuning, the official 2018 and 2019 background linking relevance judgements (qrels) were used.</p><p>As a technical note, since our queries are much longer than regular keyword queries, in order to work around BERT's maximum sequence length, both the query and candidate news articles were cropped or padded to the same length (half of 512). For our experiments, we used HuggingFace BERT's implementation<ref type="foot" coords="4,244.12,451.68,4.23,7.04" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>All these approaches were applied to both previous years' and this year's background linking topics. The results of these experiments are summarized in Table <ref type="table" coords="4,170.61,530.51,4.22,9.63" target="#tab_0">1</ref>. NDCG@5 is used as a primary metric in background linking task. Our baseline FullText approach performed very similar to the median score of all runs. That is expected since using full text for both indexing and querying is a general baseline.</p><p>Among the proposed architectures, the Universal Sentence Encoder returned the lowest scores. There may be several reasons for this. One reason can be that these models encode the semantic characteristics of texts in general but maybe those characteristics are not the right ones for this particular task. For example, many countries perform some type of elections which are discussed in news articles. All these news articles discussing different elections probably contain similar words and context related to the elections in general, such as polls, vote, count etc. Therefore, these news articles are most likely mapped to similar vectors but it is not the case that these elections of different countries are related to each other. Fine-tuning these models for this particular task can return much better results. In our submission, we used the USE model trained with the DAN architecture. Due to the time constraints, experiments with the transformer architecture could not be done. DAN architecture generally returns lower scores compared to the transformers. This may be yet another reason why this system received a lower score compared to other transformer-based approaches.</p><p>Extracting useful key sentences with BERT and using those summaries to search relevant news articles generally performed better than USE, but still returned lower scores than the baseline. Even though the summaries got better rankings on 9 topics compared to using the full article as query, it is still far away from a general out-performance. Similar to the USE, the model used in here was not fine-tuned for the background linking task, therefore the selected sentences may be useful overall but may not be the right ones to use for searching the relevant news articles.</p><p>BERT re-ranking was applied in two ways, with or without fine-tuning. Each year 50 new topics are released. For 2020 we used both 2018 and 2019 topics (around 100 topics), but in 2018 and 2019 only 50 topics were used. Even with this limited small data, fine-tuned models outperformed the models without fine-tuning. Therefore, we used the fine-tuned models in our submission. Similar to others, BERT_RR could not outperform the baseline but returned competitive results compared other models. An interesting observation is that all other models returned lower scores in 2020 compared to 2019. Only BERT_RR returned a much higher score in 2020 compared to 2019. This may be due to the almost twice the size of the training data used in fine-tuning for 2020 topics. Fine-tuning with more data may provide higher improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Wikification Task</head><p>This is the first time the wikification task has been introduced as part of TREC News. Previous years' TREC News tracks included entity ranking task in which case the entity lists were given together with the linked Wikipedia pages, and participants were asked to only rank them based on their relevance to the news article. The wikification task is a more end-toend version of this as it includes finding the useful short passages within the news article and their corresponding Wikipedia pages as well. This year we focused on entities only. Normally the task description states that passages can be linked to either Wikipedia pages or other news articles. But in this study we only used Wikipedia pages for entity linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Our Approach</head><p>Our approach for the Wikification task consists of two steps. The first step is the identification of possible entity candidates from the news article and using these mentions to retrieve relevant Wikipedia pages for entity linking. The second step is the ranking of these retrieved Wikipedia pages based on their relevance and importance to the news article. For the second step two different approaches were applied and compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Entity Extraction and Linking</head><p>In this first step, initially the entities were extracted from the news articles. Stanford CoreNLP's NER <ref type="bibr" coords="6,256.62,494.39,11.45,9.63" target="#b6">[7]</ref> tool was used to identify the entities. The tagged entities generated by this tool were processed further in order to create better entity lists. Numerical and temporal expressions were ignored. Similarly pronouns and any tagged entities which starts with a lowercase letter were removed from the entity list. Finally, in order to filter the nearduplicate entities such as "Donald Trump", "President Trump", "Trump" or "Candidate Trump"; the frequency of words, their positions and phrase match were used.</p><p>After the entity mention lists were constructed, these entities were searched over an index built over Wikipedia articles. ElasticSerch was used to built this Wikipedia index which consists of different Wikipedia fields such as title, content, categories etc. During the search, these different fields were explored in terms of their effects on entity page retrieval's quality. As expected using only the title field of the Wikipedia page outperformed other field combinations in terms of returning better Wikipedia page matches for entities. During these experiments only the top returned Wikipedia page was considered for each entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Entity Ranking</head><p>As the next step, similar to last year's task, entity linked wiki-pages should be ranked based on their relevance to the news article. Two approaches were investigated in this step. The first approach is based on the position of entities within the news article. Ranking entities based on the order they are mentioned in the article seems to return good results based on previous years' TREC entity ranking task results. We also applied the same idea in our proposed system SUNLP_textorder.</p><p>As our second approach, we used the approach from last year's topranking system at the entity ranking task. In that approach, entities were ranked according to the cosine similarity between their linked wiki-pages' vector and news articles' vector. Doc2Vec model <ref type="bibr" coords="7,362.32,372.32,11.45,9.63" target="#b7">[8]</ref> was used to create the vector representations for both Wiki-pages and news articles. This approach is referred as SUNLP_doc2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>Since this is the first time this task is organized, we did not have any data from previous years to analyze our proposed approaches during the development phase. For the TREC 2020 topics, we received the NDCG@5 scores for both of our runs together with some min/median/max scores of all runs. Our first system, SUNLP_textorder scored 0.3168 on average while last year's top-performing approach, SUNLP_doc2vec, scored 0.2378.</p><p>Performances of systems with respect to individual topics are displayed at Figure <ref type="figure" coords="7,172.19,544.06,5.42,9.63" target="#fig_0">1</ref> together with the max and median scores. Overall using the text order for ranking achieved maximum score in 21 out of 45 topics (excluding the topics with maximum score 0). Similarly in 35 of the topics, this approach got higher scores than the median. Last year's top-performing system performed worse than the text ordering. This may be due to the errors propagated from prior steps and text ordering being a more robust system here. Since this year's task is not exactly the same as last year's, a more detailed analysis is required and will be performed as future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented our work and experiments for the background linking and wikification tasks. In background linking task we mainly explored the effects of different and recent neural network architectures. For the wikification task, we mainly approached to the problem as an entity linking and ranking task. The experiments provided some useful insights but there are still some research questions left for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,143.73,506.83,322.79,9.63;8,143.73,520.38,230.66,9.63"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Topic-based NDCG@5 Scores of SUNLP_textorder, SUNLP_doc2vec, and reported max and median</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,158.37,145.30,293.52,88.14"><head>Table 1 :</head><label>1</label><figDesc>NDCG@5 Scores of Background Linking Approaches</figDesc><table coords="5,183.48,145.30,239.67,64.23"><row><cell></cell><cell>2018</cell><cell>2019</cell><cell>2020</cell></row><row><cell>SUNLP_FullText</cell><cell cols="3">0.4049 0.5623 0.5343</cell></row><row><cell>SUNLP_USE</cell><cell cols="3">0.3308 0.4145 0.3820</cell></row><row><cell cols="4">SUNLP_BERT_Summ 0.3351 0.5288 0.4447</cell></row><row><cell>SUNLP_BERT_RR</cell><cell cols="3">0.3480 0.3670 0.4559</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,142.38,621.90,188.25,7.47"><p>https://github.com/elastic/elasticsearch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,142.38,632.86,250.43,7.47"><p>https://tfhub.dev/google/universal-sentence-encoder/2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,142.38,643.82,245.72,7.47"><p>https://pypi.org/project/bert-extractive-summarizer/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,142.38,660.70,151.59,7.47"><p>huggingface.co/bert-base-uncased</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.71,157.58,340.84,9.63;9,142.71,171.12,74.71,9.63" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,332.09,157.58,137.66,9.63">Trec 2019 news track overview</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,155.36,171.12,26.99,9.63">TREC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.71,193.64,341.74,9.63;9,142.71,207.19,164.87,9.63" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Ictnet at trec 2019 news track.,&quot; in TREC</note>
</biblStruct>

<biblStruct coords="9,142.71,229.70,341.75,9.63;9,142.71,243.25,341.75,9.63;9,142.71,256.80,233.62,9.63" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,398.94,243.25,85.52,9.63;9,142.71,256.80,33.51,9.63">Universal sentence encoder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>-Y. Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.71,279.32,341.75,9.63;9,142.71,292.87,220.72,9.63" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,201.15,279.32,283.31,9.63;9,142.71,292.87,21.57,9.63">Leveraging bert for extractive text summarization on lectures</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04165</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.71,315.38,341.74,9.63;9,142.71,328.93,341.75,9.63;9,142.71,342.48,229.69,9.63" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,411.80,315.38,18.94,9.63">Cedr</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,445.44,315.38,39.01,9.63;9,142.71,328.93,341.75,9.63;9,142.71,342.48,178.62,9.63">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.71,365.00,341.75,9.63;9,142.71,378.55,341.74,9.63;9,142.71,392.10,341.75,9.63;9,142.71,405.65,91.63,9.63" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,260.03,365.00,224.43,9.63;9,142.71,378.55,144.18,9.63">Deeper text understanding for ir with contextual neural language modeling</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,302.92,378.55,181.53,9.63;9,142.71,392.10,341.75,9.63;9,142.71,405.65,40.57,9.63">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.71,428.16,341.75,9.63;9,142.71,441.71,340.85,9.63;9,142.71,455.26,341.74,9.63;9,142.71,468.81,276.71,9.63" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,214.21,441.71,260.91,9.63">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,156.38,455.26,328.07,9.63;9,142.71,468.81,190.99,9.63">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.71,491.32,341.75,9.63;9,142.71,504.87,341.74,9.63;9,142.71,518.42,24.71,9.63" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,254.72,491.32,229.74,9.63;9,142.71,504.87,30.55,9.63">Distributed representations of sentences and documents</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,198.61,504.87,209.19,9.63">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
