<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,109.02,71.58,381.38,12.90;1,104.84,87.52,387.86,12.90;1,190.64,103.46,216.27,12.90">Aliababa DAMO Academy at TREC Precision Medicine 2020: State-of-the-art Evidence Retriever for Precision Medicine with Expert-in-the-loop Active Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.37,132.89,43.52,10.75"><forename type="first">Qiao</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.77,132.89,67.01,10.75"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.49,132.89,64.76,10.75"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.96,132.89,50.46,10.75"><forename type="first">Ming</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.09,146.83,85.37,10.75"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.17,146.83,73.42,10.75"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.30,146.83,75.40,10.75"><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Indiana University Bloomington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,109.02,71.58,381.38,12.90;1,104.84,87.52,387.86,12.90;1,190.64,103.46,216.27,12.90">Aliababa DAMO Academy at TREC Precision Medicine 2020: State-of-the-art Evidence Retriever for Precision Medicine with Expert-in-the-loop Active Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4979080E90FC255789D3C3FCEFB28D2E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the submissions of Alibaba DAMO Academy to the TREC Precision Medicine (PM) Track in 2020, which achieve state-of-the-art performance in the evidence quality assessment. The focus of the TREC PM Track is to retrieve academic papers that report critical clinical evidence for or against a given treatment in a population specified by its disease and gene mutation. We use a two-step approach that includes: 1) a baseline retriever using query expansion with Elasticsearch (ES) and 2) an automatic or expert-in-the-loop reranker: the automatic re-ranker uses features of the ES scores, pre-trained BioBERT scores, publication type scores and citation count scores; the expert-in-the-loop re-ranker uses expert annotations, fine-tuned BioBERT as well as features used in the automatic re-ranker. For the expert-in-the-loop re-ranker, we use a novel active learning annotation strategy that is sample-efficient: at each iteration of the annotation, 1) we fine-tune the BioBERT using all expert annotations of query-document relevance; 2) we let human experts annotate the actual relevance of the most relevant unannotated query-document pairs predicted by the fine-tuned BioBERT. Our submissions outperform the median topic-wise scores in the phase 1 assessment for general relevance and achieve state-of-the-art performance in the phase 2 assessment for evidence quality. Our analyses show that evidence quality is a distinct aspect than the general relevance, and thus additional modeling of it is necessary to assist IR for Evidence-based Precision Medicine</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Precision Medicine (PM) takes into account individual differences in people's genes, environments, and lifestyles when tailoring their treatment and prevention strategies <ref type="bibr" coords="1,184.07,738.25,72.14,9.46">(Initiative, 2016)</ref>. PM is * Work done during internship at Alibaba. widely applied in oncology, where patients with different gene mutations receive different therapies though they have the same type of tumor. To facilitate data-driven approaches of PM, the Text REtrieval Conference (TREC) holds the PM Track annually since 2017. From 2017 to 2019, the TREC PM focuses on finding relevant academic papers or clinical trials of patient topics specified by their demographics, diseases and gene mutations <ref type="bibr" coords="1,488.27,334.79,37.27,9.46;1,307.28,348.34,46.30,9.46" target="#b3">(Roberts et al., 2017</ref><ref type="bibr" coords="1,361.21,348.34,19.64,9.46" target="#b2">(Roberts et al., , 2018</ref><ref type="bibr" coords="1,388.48,348.34,23.48,9.46" target="#b4">(Roberts et al., , 2019))</ref>.</p><p>In 2020, the TREC PM focus has been changed to retrieve academic papers that report critical clinical evidence for or against a given treatment in a population specified by its disease and gene mutation. Clinical evidence denotes the patient-centered clinical research that studies the efficacy/safety of a treatment, diagnostic power of a test, prognostic value of a marker etc. In the scheme of Evidencebased Medicine (EBM, Sackett 1997), high-quality clinical evidence, e.g.: systematic reviews and randomized controlled trials, should be used to guide clinical practice, including PM.</p><p>Traditional Information Retrieval (IR) systems are based on BM25 or TF-IDF that only rank documents by their bag-of-word similarity to the input query. Such systems do not consider evidence quality of the documents, so they are unideal to assist Evidence-based Precision Medicine. This issue can be potentially solved by a re-ranker that models evidence quality based on supervised learning. However, training such re-rankers might require a large number of expert annotations, which can be prohibitively expensive. In this work, we propose a novel active learning scheme where biomedical experts iteratively label the most confident predictions of the re-ranker based on a pre-trained language model, BioBERT <ref type="bibr" coords="1,385.72,714.87,72.86,9.46" target="#b1">(Lee et al., 2020)</ref>.</p><p>This paper describes the submissions of Alibaba DAMO Academy to the TREC PM 2020, which achieve state-of-the-art performance in the evidence quality assessment. Section 2 gives an overview of our two-step approach. Section 3 and Section 4 introduce the retriever and re-ranker in our two-step approach, respectively. Section 5 shows the competition results and analyses. We conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology Overview</head><p>We use a two-step approach to retrieve relevant PubMed documents for each given PM topic<ref type="foot" coords="2,270.65,194.75,3.99,6.91" target="#foot_0">1</ref> : 1) A baseline retrieval strategy that is fast and can generate a relatively small number (e.g. thousands) of candidates out of millions of PubMed articles; 2) A re-ranker that finely re-ranks the retrieved documents based on their evidence quality. Ideally, the former step should guarantee high recall and the latter should have high precision.</p><p>Baseline retrieval strategy It is based on Elas-ticSearch<ref type="foot" coords="2,111.86,326.47,3.99,6.91" target="#foot_1">2</ref> where the original queries are expanded by a list of weighted synonyms. The same baseline retrieval strategy is used by all submissions. We will introduce it in Section 3.</p><p>The re-ranker We use two types of re-rankers for the challenge: 1) The automatic re-ranker which unsupervisedly ranks the candidates by features of a pre-trained BioBERT and several article-level features; 2) The expert-in-the-loop re-ranker that predicts the relevance by active learning from interactive annotations of a biomedical expert. Each submission uses either the automatic re-ranker or the expert-in-the-loop re-ranker. We will discuss the re-rankers in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Retrieval Strategy</head><p>We index the titles and abstracts of all PubMed articles using ElasticSearch.</p><p>For each topic, we denote its &lt;disease&gt; as d, the gene &lt;variant&gt; as g and the &lt;treatment&gt; as t.</p><p>We find the synonyms of the d and g via the National Library of Medicine's web API: https://ghr.nlm.nih. gov/condition/{d}?report=json and https:// ghr.nlm.nih.gov/gene/{g}?report=json, respectively. We denote the retrieved synonyms of d and g as {d 1 , d 2 , ..., d m } and {g 1 , g 2 , ..., g n }, where d 1 = d and g 1 = g. We do not expand the &lt;treatment&gt; because the provided term either has no synonym or is used in almost all articles.</p><p>For each synonym d i and g j , we count their document frequency df (d i ) and df (g j ), and calculate the weights of each synonym:</p><formula xml:id="formula_0" coords="2,306.88,147.59,151.66,125.49">w(d i ) = df (d i )/Z d w(g i ) = df (g i )/Z g where Z d = i df (d i ) Z g = i df (g i )</formula><p>The weights are used in the ElasticSearch queries to lower the ranks of rare synonyms. At the highest level, we query the ElasticSearch PubMed indices using a boolean query which must match the disease and treatment query, and should match the gene query and a list of keywords, where: 1) The disease, treatment and gene queries are all dis max queries composed of their synonyms with the weights as boost factors. The tie breaker is set to 0.8 and the title field has a 3.0 boost factor while that of the abstract field is 1.0; 2) The keyword list includes words like "trial" to serve as a weak classifier for evidence-based precision medicine papers.</p><p>We set the maximum number of retrieved documents of each topic as 10, 000. On average, we retrieve 1, 589 documents for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Re-ranker</head><p>We submitted 5 systems, namely damoespb1, damoespb2, damoespcbh1, damoespcbh2, damoe-spcbh3. They use different re-rankers to rank the same set of documents retrieved by the baseline retrieval strategy described in Section 3. Damoespb1 and damoespb2 use the automatic re-ranker (Section 4.1) and are 'automatic' runs that don't rely on expert involvement. Damoespcbh1, damoespcbh2 and damoespcbh3 use the expert-in-the-loop reranker (Section 4.2). They are 'manual' runs that utilize expert annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Automatic Re-ranker</head><p>The automatic re-ranker uses features of {a, b, c, d}, corresponding to: a) the scores returned by the ElasticSearch; b) the relevance scores predicted by a pre-trained BioBERT. BioBERT <ref type="bibr" coords="3,169.73,337.62,74.84,9.46" target="#b1">(Lee et al., 2020</ref>) is a pretrained biomedical language model that can be finetuned to perform downstream tasks. We use the annotations from the previous TREC PM challenges to fine-tune the BioBERT: Specifically, we collect 54.5k topic-document relevance annotations from the qrel files of TREC PM 2017-2019. We only use the &lt;disease&gt; and &lt;variant&gt; fields of the topics as input and fine-tune the BioBERT to predict their normalized relevance in the annotations. c) the publication type score. PubMed also indexes each article with a publication type, such as journal article, review, clinical trials, etc. We manually rate the score of each publication type based on the judgments of evidence quality. Our publication type and score mapping is shown in Table <ref type="table" coords="3,98.36,555.29,4.09,9.46" target="#tab_0">1</ref>.</p><p>d) the citation count score. We rank the citation count of all PubMed articles and use the quantile of a specific article's citation count as a feature. Similar to but simpler than PageRank, this feature is designed to reflect the community-level importance of each article.</p><p>Finally, the re-ranking score of document i is calculated by:</p><formula xml:id="formula_1" coords="3,72.00,689.65,216.57,25.67">r i = w a * a i a max +w b * b i b max +w b * c i c max +w d * d i d max</formula><p>where a i , b i , c i , d i are the above features of document i, a max , b max , c max , d max are the maximum features among all documents, w a , w b , w c and w d are the weights associated with different features and are determined empirically (shown in Table <ref type="table" coords="3,515.80,80.22,3.88,9.46" target="#tab_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Expert-in-the-loop Re-ranker</head><p>We show the expert-in-the-loop annotation strategy in Figure <ref type="figure" coords="3,349.68,133.44,4.09,9.46" target="#fig_0">1</ref>. A senior M.D. candidate (first author) is employed to annotate the evidence quality of a document for a given topic based on the criteria shown in Algorithm 1.  We utilize the expert annotations to train a simple linear regression model using the abovementioned features (a, b, c and d, in Section 4.1). The linear regressor then predicts the scores for all retrieved q-d pairs.</p><p>Besides, the manual re-ranker also uses features of: e) the relevance scores predicted by a fine-tuned BioBERT. The BioBERT is fine-tuned by 1, 000 query-document pairs we annotate.</p><p>For the annotated query-document pairs, we directly use the expert annotation. For the unannotated ones, the re-ranking score is a weighted sum of the relevance scores predicted by the fine-tuned BioBERT and the linear regression model: </p><formula xml:id="formula_2" coords="3,335.60,755.52,150.73,10.85">r i = w lr * LR(a i , b i , c i , d i ) + w e *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ExpertAnnotation</head><p>Input: A document a (i.e. title and abstract of a PubMed article) and a topic q. q d , qg and qt denote the topic's &lt;disease&gt;, &lt;variant&gt; and &lt;treatment&gt; fields, respectively.</p><formula xml:id="formula_3" coords="4,80.97,344.64,136.04,8.12">Output: A relevance score r ∈ [0, 1].</formula><p>Let the expert annotate whether q d , qg and qt are mentioned in a, getting r d ∈ {0, 1}, rg ∈ {0, 1}, rt ∈ {0, 1}. if r d = 1 and rt = 1 then Let the expert annotate whether treatment qt for the disease q d is the focus of document a, getting f ∈ {0, 1} if f = 0 then Let the expert annotate whether the document a discusses mono-treatment of qt, getting m ∈ {0, 1}.</p><p>Let the expert annotate the evidence quality e of document a for the topic q, e ∈ [- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>The assessments of this year's TREC PM are divided into 2 phases, where phase 1 is "Relevance Assessment" and phase 2 is "Evidence Assessment". Phase 1 judgment follows the settings of previous years of the track, while phase 2 focuses on finding high-quality evidence (e.g.: systematic reviews and randomized controlled trials) for the given cancer treatment. The results are shown in Table <ref type="table" coords="4,98.36,659.96,4.09,9.46" target="#tab_4">3</ref>.</p><p>In the phase 1 assessment, most of our submissions score higher than the topic-wise median submission, but the topic-wise best submission outperforms our submissions by large margins. Though utilizing expert annotations for learning, our manual runs (damoespcbh1-3) show no significant improvements over our automatic runs (damoespb1-2) in phase 1 assessment. We mainly model the evidence quality in our systems, which might give low scores to the highly related but of low evidence quality documents (e.g.: narrative reviews) and lead to the unideal performance in the phase 1 assessment.</p><p>In the phase 2 assessment, our submission damoespcbh3 and damoespcbh1&amp;2 achieve the highest scores for NDCG@30 (0.4519) and NDCG@5 (0.4543), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analyses</head><p>In Table <ref type="table" coords="4,347.53,677.08,4.17,9.46" target="#tab_2">2</ref>, we show the Pearson correlations between the used features and the official relevance scores in both phases. phase 1 scores, which is not surprising since such annotations are also about general relevance. The ES scores achieve the second highest correlation.</p><p>The expert annotations for the evidence quality, however, have only 0.2157 Pearson correlation with the general relevance scores. This indicates that relevant papers might not have high evidence quality. Surprisingly, the features of publication types and the citation counts, which are designed for the evidence quality ranking and are positively correlated with the evidence quality, are negatively correlated with the general relevance scores. It shows that the two assessment phases might have some opposite considerations.</p><p>Evidence Quality (Phase 2): we show the correlations between different features and the evidence quality scores. The trends are similar in both the original and exponential scores. BioBERT finetuned by the expert annotations achieves comparable performance to the expert annotations, and they are the most correlated features. Besides, the fine-tuned BioBERT outperforms the expert annotations by a large margin <ref type="bibr" coords="5,184.48,576.90,78.14,9.46">(37.33 v.s. 21.57)</ref> in the phase 1 assessment, indicating that it can re-rank the documents by evidence quality while remaining the original general relevance ranks.</p><p>The most correlated features of phase 1, i.e.: the pre-trained BioBERT and the ES score, have the lowest correlations with the phase 2 scores, which further confirms that the evidence quality assessment is distinct from the general relevance assessment. Interestingly, the simple citation count feature has high correlations with the evidence quality scores, probably because the highly-cited papers usually provide critical clinical evidence for precision medicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we present the winning solution in the phase 2 of TREC Precision Medicine 2020. It uses 1) an ElasticSearch-based retriever with query expansion and keyword matching and 2) a re-ranker based on article features and the BioBERT finetuned by annotations from expert-in-the-loop active learning. Analyses show that the evidence quality is a distinct aspect than the general relevance, and thus additional modeling of it is necessary to assist IR for Evidence-based Precision Medicine.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,307.28,518.68,219.92,8.64;3,307.28,530.64,68.38,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of our expert-in-the-loop annotation strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,71.69,68.69,220.23,228.40"><head>Table 1 :</head><label>1</label><figDesc>Mappings between publication types and clinical evidence quality scores.</figDesc><table coords="3,91.36,68.69,179.54,191.50"><row><cell>Publication Type</cell><cell>Score</cell></row><row><cell>Published Erratum</cell><cell>-2</cell></row><row><cell>Retraction of Publication</cell><cell>-2</cell></row><row><cell>Editorial</cell><cell>-1</cell></row><row><cell>Comment</cell><cell>-1</cell></row><row><cell>Journal Article</cell><cell>0</cell></row><row><cell>Review</cell><cell>0</cell></row><row><cell>English Abstract</cell><cell>0</cell></row><row><cell>Letter</cell><cell>0</cell></row><row><cell>Observational Study</cell><cell>1</cell></row><row><cell>Case Reports</cell><cell>1</cell></row><row><cell>Clinical Trial</cell><cell>2</cell></row><row><cell>Systematic Review</cell><cell>2</cell></row><row><cell>Meta-Analysis</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,488.76,755.52,7.96,10.63"><head>Table 2 :</head><label>2</label><figDesc>e i Feature weights of our 5 submissions and their correlations to the official scores. damoespb1 and damoe-spb2 are 'automatic' submissions, while damoespcbh1, damoespcbh2, damoespcbh3 are 'manual' submissions. ori / exp: the original and exponential scores of the phase 2. PT: pre-trained. FT: fine-tuned. Pub.: publication. Cit.: citation. Reg.: regressor. *Used when available.</figDesc><table coords="4,80.30,67.52,436.94,165.02"><row><cell>Features</cell><cell>ES score (wa)</cell><cell>PT BioBERT (w b )</cell><cell>Pub. Type (wc)</cell><cell>Cit. Count (w d )</cell><cell>Linear Reg. (wlr)</cell><cell>FT BioBERT (we)</cell><cell>Expert</cell></row><row><cell cols="2">Weights in Submitted Systems</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>damoespb1</cell><cell>1.0</cell><cell>0.5</cell><cell>1.5</cell><cell>0.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>damoespb2</cell><cell>1.0</cell><cell>0.5</cell><cell>1.0</cell><cell>0.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>damoespcbh1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>1.0</cell><cell>*</cell></row><row><cell>damoespcbh2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>2.0</cell><cell>*</cell></row><row><cell>damoespcbh3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>5.0</cell><cell>*</cell></row><row><cell cols="2">Pearson r (%) with Qrels</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>General Relevance (Phase 1)</cell><cell>38.92</cell><cell>57.71</cell><cell>-4.35</cell><cell>-6.21</cell><cell>13.41</cell><cell>37.33</cell><cell>21.57</cell></row><row><cell>Evidence Quality ori / exp (Phase 2)</cell><cell>7.52 4.74</cell><cell>6.21 3.38</cell><cell>6.96 8.06</cell><cell>25.64 27.72</cell><cell>27.28 28.16</cell><cell>33.09 28.47</cell><cell>29.37 30.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,306.94,728.34,218.61,36.98"><head>Table 3 :</head><label>3</label><figDesc>Topic-wise averaged performance of different submissions in the evaluation.</figDesc><table coords="4,306.94,728.34,218.61,36.98"><row><cell>General Relevance (Phase 1): BioBERT that is</cell></row><row><cell>further pre-trained by the annotations of previous</cell></row><row><cell>TREC PM has the highest correlation with the</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.14,736.29,202.13,7.77;2,72.00,746.25,201.47,7.77"><p>In TREC PM, each topic is a patient query that includes his/her disease, gene mutation and a potential treatment.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,88.14,757.97,123.74,6.31"><p>https://www.elastic.co/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,307.28,453.73,218.27,8.64;5,318.19,464.69,76.53,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,307.28,453.73,218.27,8.64;5,318.19,464.69,73.12,8.64">The Precision Medicine Initiative. 2016. The precision medicine initiative</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,482.67,219.52,8.64;5,318.19,493.63,207.36,8.64;5,318.00,504.59,209.20,8.64;5,318.19,515.55,207.36,8.64;5,318.19,526.34,187.47,8.81" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,404.02,504.59,123.18,8.64;5,318.19,515.55,207.36,8.64;5,318.19,526.51,43.62,8.64">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,369.78,526.34,57.60,8.58">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,544.49,218.27,8.64;5,317.83,555.45,207.72,8.64;5,317.83,566.41,207.72,8.64;5,318.19,577.37,100.43,8.64" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,421.28,566.41,104.27,8.64;5,318.19,577.37,96.70,8.64">Overview of the trec 2018 precision medicine track</title>
		<author>
			<persName coords=""><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>William R Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lazar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,595.35,218.27,8.64;5,317.83,606.31,209.37,8.64;5,318.19,617.27,207.36,8.64;5,318.19,628.06,207.36,8.81;5,317.88,639.02,209.32,8.58;5,318.19,649.97,149.21,8.81" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,475.55,617.27,49.99,8.64;5,318.19,628.23,150.55,8.64">Overview of the trec 2017 precision medicine track</title>
		<author>
			<persName coords=""><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>William R Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shubham</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,486.73,628.06,38.81,8.58;5,317.88,639.02,209.32,8.58;5,318.19,649.97,16.59,8.58">The... text REtrieval conference: TREC. Text REtrieval Conference</title>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,668.13,218.27,8.64;5,317.83,679.08,209.37,8.64;5,318.19,690.04,209.02,8.64;5,318.19,701.00,209.01,8.64;5,318.19,711.96,80.52,8.64" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,391.89,701.00,135.31,8.64;5,318.19,711.96,76.78,8.64">Overview of the trec 2019 precision medicine track</title>
		<author>
			<persName coords=""><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>William R Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shubham</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Funda</forename><surname>Pant</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Meric-Bernstam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,729.94,218.27,8.64;5,317.94,740.73,209.27,8.81;5,318.19,751.86,25.48,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,406.50,729.94,100.86,8.64">Evidence-based medicine</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sackett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,317.94,740.73,98.72,8.58">Seminars in perinatology</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
