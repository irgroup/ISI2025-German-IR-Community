<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.62,115.90,276.12,12.68;1,250.71,133.83,113.93,12.68">Longformer for MS MARCO Document Re-ranking Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.24,172.04,52.85,8.80"><forename type="first">Ivan</forename><surname>Sekulić</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics Università della Svizzera italiana</orgName>
								<address>
									<settlement>Lugano</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.64,172.04,67.82,8.80"><forename type="first">Amir</forename><surname>Soleimani</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.01,172.04,104.86,8.80"><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.80,172.04,63.86,8.80"><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics Università della Svizzera italiana</orgName>
								<address>
									<settlement>Lugano</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.62,115.90,276.12,12.68;1,250.71,133.83,113.93,12.68">Longformer for MS MARCO Document Re-ranking Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">71D3ECB85DCEA31270FAE99060AB806B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Document ranking</term>
					<term>Longformer</term>
					<term>Neural ranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This technical report describes the approach of the Università della Svizzera italiana and the University of Amsterdam for MS MARCO document ranking task and TREC Deep Learning track 2020. Two step document ranking, where the initial retrieval is done by a classical information retrieval method, followed by neural re-ranking model, is the new standard. The best performance is achieved by using transformerbased models as re-rankers, e.g., BERT. We employ Longformer, a BERTlike model for long documents, on the MS MARCO document re-ranking task. The complete code used for training the model can be found on: https://github.com/isekulic/longformer-marco</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document ranking is central problems in information retrieval (IR). Given a query, the task is to rank the documents of some collection so that the most relevant ones appear on top of the list. Recently, neural ranking models have shown superior performance compare to the traditional IR methods. Given the much higher computational need of neural models, the two step retrieval process is widely adapted. First, a traditional IR method, like BM25 or query likelihood, retrieves top k documents from a given collection. Then, a computationally expensive neural model re-ranks the top k documents from the initial retrieval step.</p><p>A number of neural models for ranking has been proposed in recent years. Some of them are: DRMM <ref type="bibr" coords="1,261.52,608.24,9.96,8.80" target="#b3">[4]</ref>, KNRM <ref type="bibr" coords="1,316.11,608.24,14.61,8.80" target="#b9">[10]</ref>, Co-PACRR <ref type="bibr" coords="1,394.48,608.24,9.96,8.80" target="#b4">[5]</ref>, DUET <ref type="bibr" coords="1,446.45,608.24,9.96,8.80" target="#b6">[7]</ref>, and Conformer-Kernel with QTI <ref type="bibr" coords="1,259.16,620.19,9.96,8.80" target="#b7">[8]</ref>. With a combination of new generation of neural models, namely the transformer architecture, and large-scale datasets, neural rankers arose as superior to traditional methods, which was not possible before <ref type="bibr" coords="1,134.77,656.06,9.96,8.80" target="#b5">[6]</ref>. Most notable model from the transformers family is probably BERT <ref type="bibr" coords="1,467.31,656.06,9.96,8.80" target="#b1">[2]</ref>, which has been very successfully applied to passage ranking <ref type="bibr" coords="2,401.68,118.93,9.96,8.80" target="#b8">[9]</ref>, outperforming state-of-the-art by a large margin.</p><p>The largest dataset for the document ranking task is MS MARCO (Microsoft Machine Reading Comprehension). Transformer architecture, namely BERT, has already proven effective on the MS MARCO passage ranking task. However, the documents are much longer than the passages, making the task of document ranking more challenging.</p><p>To address the issue of increased length of the documents, we employ Longformer <ref type="bibr" coords="2,166.08,214.58,10.51,8.80" target="#b0">[1]</ref> -a BERT-like model for long documents. Longformer has an adjusted attention mechanism that combines local, BERT-like windowed attention, with a global attention, allowing the model to attend over much longer sequences than standard self-attention. Compared to BERT that typically processes up to 512 tokens at a time, Longformer is pre-trained on documents with length of 4096 tokens. We reach MRR@100 of 0.329 and 0.305 on the official dev and the test sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>We train and evaluate Longformer on MS MARCO document ranking dataset. It consists of more than 370k queries, 3.2 million documents, and the corresponding relevance labels for query-document pairs. Relevance labels are transferred from the MS MARCO passage ranking task, by mapping a positive passage to a document containing the passage, for each query. This is done under an assumption that the document that contains a relevant passage is a relevant document. Additional information about the dataset is present in Table <ref type="table" coords="2,402.25,410.03,3.87,8.80" target="#tab_0">1</ref>. The document ranking task features two tasks: Document Re-Ranking Given a candidate top 100 documents for each query, as retrieved by BM25, re-rank the documents by relevance. Document Full Ranking Given a corpus of 3.2m documents generate a candidate top 100 documents for each query, sorted by relevance.</p><p>We participate in the document re-ranking task, where we use Longformer to assign relevance to the 100 documents retrieved by BM25, which are provided by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We train Longformer <ref type="bibr" coords="3,232.12,147.10,10.51,8.80" target="#b0">[1]</ref> to estimate relevance of a query-document pair. The training setting is formulated as in <ref type="bibr" coords="3,287.79,159.05,9.96,8.80" target="#b8">[9]</ref>. We feed the query as sentence A and the document as sentence B to the tokenizer, which yields the following input to the Longformer<ref type="foot" coords="3,185.04,181.41,3.97,6.16" target="#foot_0">3</ref> : &lt;s&gt; query &lt;/s&gt; document &lt;/s&gt;</p><p>We truncate the document such that the sequence of the concatenated query, document, and the separator tokens does not exceed 4096 tokens. After passing the sequence through the Longformer model, the &lt;s&gt; vector is given as input to a classifier head, consisting of two linear layers with dropout and a nonlinear function. The classifier outputs a two-dimensional vector, where the first dimension indicates probability of a document not being relevant to the query, while the second indicates relevance probability. For a given query, we rank the candidate documents based on the relevance probability, which is computed independently for each document.</p><p>We fine-tune the pre-trained Longformer with a cross-entropy loss, using the following hyperparameters: batch size of 128, Adam optimiser with the initial learning rate of 3 × 10 -5 , a linear scheduler with warmup for 2500 steps, trained for 150k iterations. Further hyperparameter tuning might yield better results. For training, we also reduce the positive to negative document ratio to 1:10, from the given 1:100 (as each query is given top 100 documents extracted by BM25 by the organisers). We use PytorchLightning <ref type="bibr" coords="3,365.22,398.98,10.51,8.80" target="#b2">[3]</ref> for our training setting implementation and HuggingFace's Transformers package [?] for Longformer implementation.</p><p>4 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TREC Deep Learning 2020</head><p>The results of our TREC Deep Learning submission are presented in Table <ref type="table" coords="3,472.84,511.36,3.87,8.80" target="#tab_1">2</ref>. We participate in document re-ranking task with one run only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MS MARCO document re-ranking</head><p>The results on the MS MARCO document re-ranking task on the dev and the test set are presented in Table <ref type="table" coords="4,276.71,152.17,3.87,8.80" target="#tab_2">3</ref>. The official metric is mean reciprocal rank (MRR@100). Other submissions and approaches can be found on the official leaderboard<ref type="foot" coords="4,186.00,174.52,3.97,6.16" target="#foot_1">4</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We employed Longformer for MS MARCO document re-ranking task. We submitted the model to the official MS MARCO leaderboard, as well as to the TREC Deep Learning track 2020. The results suggest that further work is required to match the performance of other transformer-based models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,440.56,345.83,80.72"><head>Table 1 .</head><label>1</label><figDesc>Number of documents in MS MARCO corpus and the number of queries in train, dev, and test set.</figDesc><table coords="2,251.62,475.48,109.05,45.80"><row><cell># documents</cell><cell></cell><cell>3.2M</cell></row><row><cell></cell><cell cols="2">train 367,013</cell></row><row><cell># queries</cell><cell>dev</cell><cell>5,193</cell></row><row><cell></cell><cell>test</cell><cell>5,793</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,216.73,556.49,181.90,47.84"><head>Table 2 .</head><label>2</label><figDesc>TREC Deep Learning 2020 results.</figDesc><table coords="3,220.20,580.46,171.89,23.88"><row><cell>Run name</cell><cell cols="2">MRR MAP MAP@10</cell></row><row><cell cols="2">longformer_1 0.8889 0.3503</cell><cell>0.2028</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,134.77,206.16,345.83,92.18"><head>Table 3 .</head><label>3</label><figDesc>MRR@100 of the Longformer and the official baselines provided by the organisers.<ref type="bibr" coords="4,180.46,217.13,9.72,7.92" target="#b7">[8]</ref> </figDesc><table coords="4,369.64,241.59,44.27,7.92"><row><cell>dev</cell><cell>test</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,144.73,645.79,335.86,7.92;3,144.73,656.74,90.96,7.92"><p>Tokens &lt;s&gt; and &lt;/s&gt; are equivalent to the [CLS] and [SEP] tokens in BERT tokenizer, respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,144.73,656.74,205.21,7.92"><p>https://microsoft.github.io/msmarco/#docranking</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,142.95,482.00,337.64,7.92;4,151.52,492.96,159.01,7.92" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,298.94,482.00,177.52,7.92">Longformer: The long-document transformer</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,142.95,504.19,337.64,7.92;4,151.52,515.15,329.07,7.92;4,151.52,526.11,25.59,7.92" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,346.99,504.19,133.60,7.92;4,151.52,515.15,189.90,7.92">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,142.95,537.34,37.36,7.92;4,212.48,537.34,14.59,7.92;4,259.24,537.34,31.88,7.92;4,323.29,537.34,38.14,7.92;4,393.59,537.34,33.08,7.92;4,458.83,537.34,21.76,7.92;4,151.52,548.29,265.80,7.93" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,259.24,537.34,31.88,7.92;4,323.29,537.34,34.32,7.92">Pytorch lightning</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning3" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.95,559.53,337.63,7.92;4,151.52,570.49,329.08,7.92;4,151.52,581.45,240.34,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,311.47,559.53,169.12,7.92;4,151.52,570.49,50.17,7.92">A deep relevance matching model for adhoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,224.34,570.49,256.26,7.92;4,151.52,581.45,164.77,7.92">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.95,592.68,337.64,7.92;4,151.52,603.63,329.07,7.92;4,151.52,614.59,253.21,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,345.38,592.68,135.21,7.92;4,151.52,603.63,114.82,7.92">Co-pacrr: A context-aware neural ir model for ad-hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,288.09,603.63,192.50,7.92;4,151.52,614.59,169.05,7.92">Proceedings of the eleventh ACM international conference on web search and data mining</title>
		<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.95,625.82,337.64,7.92;4,151.52,636.78,246.10,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,182.99,625.82,226.83,7.92">The neural hype and comparisons against weak baselines</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,430.25,625.82,50.34,7.92;4,151.52,636.78,24.01,7.92">ACM SIGIR Forum</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.95,119.62,337.64,7.92;5,151.52,130.58,329.07,7.92;5,151.52,141.54,221.68,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,291.32,119.62,189.27,7.92;5,151.52,130.58,153.25,7.92">Learning to match using local and distributed representations of text for web search</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,327.18,130.58,153.40,7.92;5,151.52,141.54,127.50,7.92">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1291" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.95,152.50,337.64,7.92;5,151.52,163.46,329.08,7.92" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="5,363.32,152.50,117.27,7.92;5,151.52,163.46,165.59,7.92">Conformer-kernel with query term independence for document retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hofstatter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10434</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,142.95,174.42,337.63,7.92;5,151.52,185.37,97.78,7.92" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="5,270.03,174.42,133.27,7.92">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,142.61,196.33,337.98,7.92;5,151.52,207.29,329.08,7.92;5,151.52,218.25,329.07,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,352.76,196.33,127.84,7.92;5,151.52,207.29,94.93,7.92">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,269.77,207.29,210.83,7.92;5,151.52,218.25,255.64,7.92">Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
