<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,57.37,84.23,497.27,15.44;1,195.69,104.15,221.11,15.44">HBKU at TREC 2020: Conversational Multi-Stage Retrieval with Pseudo-Relevance Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,106.11,130.29,72.47,10.59"><forename type="first">Haya</forename><surname>Al-Thani</surname></persName>
							<email>hayaalthani@hbku.edu.qa</email>
							<affiliation key="aff0">
								<orgName type="department">Hamad Bin Khalifa University Doha</orgName>
								<address>
									<country>Qatar Bernard J. Jansen</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Qatar Computing Research Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Hamad Bin Khalifa University Doha</orgName>
								<address>
									<country>Qatar Tamer Elsayed</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Qatar University Doha</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,57.37,84.23,497.27,15.44;1,195.69,104.15,221.11,15.44">HBKU at TREC 2020: Conversational Multi-Stage Retrieval with Pseudo-Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">86A68AF0CA63D45D1B2F305272FE9C9D</idno>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conversational Information Seeking</term>
					<term>Conversational Search Systems</term>
					<term>Multi-Stage Retrieval Systems</term>
					<term>Open-Domain</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Passage retrieval in a conversational context is extremely challenging due to limited data resources. Information seeking in a conversational setting may contain omissions, implied context, and topic shifts. TREC CAsT promotes research in this field by aiming to create a reusable dataset for open-domain conversational information seeking (CIS). The track achieves this goal by defining a passage retrieval task in a multi-turn conversation setting. Understanding conversation context and history is a key factor in this challenge. This solution addresses this challenge by implementing a multi-stage retrieval pipeline inspired by last year's winning algorithm. The first stage in this retrieval process is a historical query expansion step from last year's winning algorithm where context is extracted from historical queries in the conversation. The second stage is the addition of a pseudo-relevance feedback step where the query is expanded using top-k retrieved passages. Finally, a pre-trained BERT passage re-ranker is used. The solution performed better than the median results of other submitted runs with an NDCG@3 of 0.3127 for the best performing run.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Conversational Search Systems have been getting a lot of attention from the IR and NLP communities. The advancement of machine learning techniques has led to the rise of conversational agents such as digital personal assistants and smart speakers like Amazon's Alexa, Apple's Siri, and many others. Digital Assistants are often used to help people increase their productivity.Conversational information seeking (CIS) is an important emerging research area. As opposed to traditional information seeking, in CIS a user searches for information regarding single or multiple topics in a series of sequential questions or turns. Previous turns usually have an impact on subsequent turns. As a result, later turns in a conversation are often not well-formed or ambiguous. One of the field's key challenges is how to ensure context-awareness throughout the whole conversation. Another challenge is limited labeled data appropriate for training and evaluating CIS models. The Conversational Assistance Track (CAsT) was organized starting in TREC 2019 to address this challenge.</p><p>The goal of CAsT is to create a reusable benchmark for opendomain conversational search where answers are retrieved passages from a large text corpus. TREC CAsT aims to address some of the first major challenges in building conversational search systems (CSS). Their goal is to create a large-scale reusable test collection for open-domain CSS. This year's task is to retrieve a ranked list of passages over a large collection to satisfy the information need of a multi-turn conversation. Conversational search queries are often ambiguous due to coreference and omission problems. A user may often ask questions referring to previous turns in the dialogue. This year, the CAsT track's primary focus is to understand the information need behind different turns in the conversation context and find relevant responses. Table <ref type="table" coords="1,424.88,364.20,4.09,7.94" target="#tab_0">1</ref> shows an example of a conversation in the CAsT training set. It can be noted that subsequent turns in the conversation can only be understood by looking at the conversation's turn history. For example, the second utterance in this conversation references the first turn, and in later utterances, such as at turn 5, context is completely omitted. Incorporating context history accurately in such CSS problems is essential to improve performance.</p><p>To solve this task while still addressing these challenges, a multistage retrieval solution is proposed. The solution is inspired by last year's winning algorithm which applies a query expansion technique to add context to ambiguous conversation turns <ref type="bibr" coords="1,524.30,484.74,13.49,7.94" target="#b17">[18]</ref>. The historical query expansion algorithm (HQE) is a non-parametric algorithm that extracts topic and subtopic keywords from the previous conversation turns and uses these keywords to expand the turn query. After performing this expansion, passages are retrieved using a BM25 retrieval model. After that, a BERT passage re-ranker trained on the MS MARCO passage dataset is used to re-rank retrieved passages <ref type="bibr" coords="1,378.57,561.46,13.22,7.94" target="#b12">[13]</ref>. In this solution, a pseudo-relevance feedback stage is added to this pipeline. Pseudo-relevance feedback (PRF or blind feedback) uses top retrieved documents to extract terms to use in the query expansion stage. The user is not involved in the selection of relevant documents. PRF techniques can improve performance of many retrieval models <ref type="bibr" coords="1,462.96,616.25,13.48,7.94" target="#b18">[19]</ref>. For this year's CAsT task, this additional stage is added to the retrieval pipeline. After the HQE stage, a passage query expansion (PQE) step is added. PQE uses PRF to further expand the turn query by adding terms from the top-k retrieved passages based on TF-IDF. The terms of the top-k passages retrieved from the HQE stage are ranked by the TF-IDF scoring scheme. Then the top terms from these passages are used to expand the turn query further. After this additional Is there a relation between age and sugar levels?</p><p>stage, the resulting list of retrieved passages are re-ranked using the pre-trained BERT re-ranker.</p><p>Conversations are made up of a series of related or unrelated questions. In a regular conversation, turns can heavily depend on previous questions or answers, or shift to a completely new topic. In order to handle these omissions or shifts in conversation, turn queries are syntactically analyzed and categorized. The first query category proposed are 'explicit' queries. Explicit queries are considered complete and contain enough context. The second category would be 'implicit' queries that contain omissions or coreferences. Different methods were tested to categorize queries but, in the end, a simple syntactical method was implemented. A turn that contains no pronouns is assumed to be 'explicit', while a turn that contains at least one pronoun is 'implicit'.</p><p>For the submitted runs, the parameters of the HQE stage were kept constant and set to the same parameters that achieved best performance on last year's task. The runs experimented with different parameters for both the stage 2: PQE and stage 3: BERT re-ranker. Some runs implement the proposed query categorization scheme while others don't perform any categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LITERATURE REVIEW</head><p>Question answering is the process of finding answers to a given question given some context. This area has seen a lot of progress due to the successful application of deep learning architectures and the availability of large scale datasets such as MS MARCO, SQUAD, and HotpotQA <ref type="bibr" coords="2,108.57,559.00,13.40,7.94" target="#b11">[12,</ref><ref type="bibr" coords="2,124.01,559.00,10.27,7.94" target="#b13">14,</ref><ref type="bibr" coords="2,136.32,559.00,10.05,7.94" target="#b19">20]</ref>. A large focus of the field in recent years has been targeted towards neural QA <ref type="bibr" coords="2,191.86,569.96,13.33,7.94" target="#b14">[15]</ref>. Conventionally, neural QA is a two-stage process: first, relevant passages are retrieved and then a neural network model extracts the likeliest answer <ref type="bibr" coords="2,278.88,591.88,13.24,7.94" target="#b10">[11]</ref>. Devlin et. al. introduced BERT or bidirectional Encoder Representations from Transformers <ref type="bibr" coords="2,155.13,613.79,9.52,7.94" target="#b5">[6]</ref>. BERT is a language model that is pre-trained to learn deep bidirectional representations from text. A pre-trained BERT model can be fine-tuned on a specific task by adding an output layer. BERT has made a massive impact in many NLP tasks, including QA. In the work of Nogueira et. al., BERT is re-implemented as a passage re-ranker and achieves state-of-the-art results on the MS MARCO passage re-ranking task <ref type="bibr" coords="2,242.29,679.55,13.36,7.94" target="#b12">[13]</ref>.</p><p>In open domain question answering, the system returns answers to user questions from a wide range of domains. The pipeline of an open-domain system involves a retriever for selecting relevant documents from a large corpus of text such as Wikipedia and a machine reading comprehension model for inferring the answer from the retrieved documents <ref type="bibr" coords="2,407.72,304.68,9.29,7.94" target="#b4">[5]</ref>. Open-domain QA was popularized by the TREC-8 task <ref type="bibr" coords="2,378.99,315.64,13.23,7.94" target="#b15">[16]</ref>. It has recently gained a lot of traction due to the emergence of multiple datasets such as SearchQA, TriviaQA, and Quasar <ref type="bibr" coords="2,362.02,337.55,9.33,7.94" target="#b6">[7,</ref><ref type="bibr" coords="2,373.59,337.55,6.14,7.94" target="#b7">8,</ref><ref type="bibr" coords="2,381.98,337.55,10.13,7.94" target="#b9">10]</ref>.</p><p>Multi-stage retrieval systems can be taken as a two-step process. First, a list of candidate documents are generated, and then the list goes through one or more re-ranking stage. The number of stages have to be considered with efficiency and effectiveness in mind <ref type="bibr" coords="2,333.55,392.35,11.70,7.94" target="#b2">[3]</ref>. The baseline of this system is a cascade pipeline of BM25 candidate generation followed by a BERT re-ranker. The effectiveness of this has been proven in multiple IR datasets such as TREC CAR and MS MARCO.</p><p>Conversational Search is a major research problem in the IR community. Conversational search has been applied in many domains such as conversational recommendation systems, e-health systems, and personality recognition <ref type="bibr" coords="2,451.72,469.06,9.27,7.94" target="#b0">[1]</ref>. In the past few years rulebased conversational IR has given way to methods based on deep learning <ref type="bibr" coords="2,351.50,490.98,9.52,7.94" target="#b8">[9]</ref>. A significant topic of research in this field involves identifying user need while searching for information. One work that focuses on this problem has been to include query suggestion to clarify user's intent. By asking clarifying questions, user's intent can be better understood and the search can be redirected to achieve better results <ref type="bibr" coords="2,368.86,545.77,9.49,7.94" target="#b1">[2]</ref>. One major factor to consider when designing a conversational agent is how to maintain the conversation context <ref type="bibr" coords="2,317.96,567.69,13.37,7.94" target="#b16">[17]</ref>. Maintaining context is essential to user experience since formulating long questions and sentences is not natural in a normal conversation setting. Addressing this context problem is the focus of last year and this year's CAsT track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>In the CAsT track, conversational search is defined as an information retrieval task in a conversational setting. The goal of the task is to fulfill user's information need which is expressed through turns in a conversation. The response is a list of top-k relevant short passages retrieved from a large collection of passages. The task in year 1 focused on candidate response retrieval for conversational information seeking. Year 2 is similar to year 1 and focuses on candidate response ranking in context. The difference between the 2019 and 2020 CAsT task is the conversations will not include topic titles and descriptions. Topics will unfold through the conversation turns. A canonical system response will also be included with each previous turn.</p><p>The goal is to keep the task simple in order to create a reusable collection. Formally, a conversation S is made up of a series of n turn utterances u such that S= { u 1 , u 2 , u 3 ,. . . , u n } . The task is to retrieve a list of top-k passages P i for each turn u i to satisfy the information need of turn i.</p><p>Submission categories for this year will be 'automatic' runs that use the raw turns. 'Manual' runs that use manually rewritten turns that are rewritten to remove ambiguity and to add context. The third and last submission category is the 'canonical' run that use both the raw turns and the included canonical responses for previous turns.</p><p>Passage collections used this year are made up of passages from MS MARCO and the TREC Complex Answer Retrieval Paragraph Collection:</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>The solution is implemented as a three-stage retrieval pipeline. The first stage is the historical query expansion stage (HQE). HQE is a BM25 retrieval model that first extract context from previous turns and uses these extracted keywords to expand the turn query. The HQE algorithm along with a BERT re-ranker achieved best performance in 2019 CAsT challenge. The second stage is the passage query expansion stage (PQE) and is another BM25 retrieval phase but it adds context using pseudo-relevance feedback. The third and final stage is a pre-trained BERT re-ranker pre-trained on the MS MARCO dataset. The Anserini toolkit was used for collection indexing and retrieval 1 . The SpaCy library was used to syntactically analyze the conversation turns 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Design</head><p>Figure <ref type="figure" coords="3,78.84,566.11,4.09,7.94" target="#fig_0">1</ref> shows the overall system design. The multi-stage retrieval system consists of three stages. The system first starts with the raw turn query as the input. The query is then expanded using the historical query expansion algorithm. Then the expanded query is used to retrieve the first ranked list of passages. The passages are retrieved from an indexed collection of the combination of the MS MARCO and TREC CAR datasets. After that, the turn query is expanded again using the top terms from the top passages retrieved from the first stage HQE. The terms are selected based on TF-IDF. The turn query is expanded with the passage terms and a new ranked list of passages is retrieved from the collection. Finally, a 1 Anserini, https://github.com/castorini/anserini 2 SpaCy, https://spacy.io/ pre-trained BERT re-ranker is used to re-rank the list to get the final ranked list of retrieved passages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stage 1: Historical Query Expansion</head><p>The first stage of this multi-phase solution is to extract context from conversation turns using the historical query expansion algorithm that achieved best performance on the 2019 CAsT data (HQE) <ref type="bibr" coords="3,543.00,356.05,13.27,7.94" target="#b17">[18]</ref>. This method consists of mainly three main steps:</p><p>(1) Extraction of general topic and sub-topic keywords from historical turns within a conversation. (2) The measurement of turn ambiguity.</p><p>(3) Query expansion using topic and subtopic keywords extracted from historical turns. Main topic and subtopic keywords are extracted using what the authors call the keyword extractor (KE). The aim of KE is to compute the score of each token within an utterance. The score of that token indicates the importance of the token in the utterance. The authors use the BM25 retrieval score of the token's most relevant document as that indicator. The theory behind this design is that the importance of a token can be judged based on those documents that are most relevant to it. If the token's importance score is higher than a certain threshold it is considered a keyword.</p><p>The other main component to this solution is the query performance predictor (QPP). This component measures the turn utterance's ambiguity. They establish that ambiguity is closely related to its ambiguity with respect to the collection being searched. Thus, it is measured by analyzing retrieval score. If a turn's ambiguity score is higher than a certain threshold, the query is expanded using the HQE method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Stage 2: Passage Query Expansion</head><p>The second stage in this multi-stage retrieval process would be passage query expansion (PQE). In this stage, the queries are expanded through pseudo-relevance feedback where query expansion terms are extracted from top-ranked documents retrieved from the initial query <ref type="bibr" coords="3,355.45,679.55,9.37,7.94" target="#b3">[4]</ref>. PQE utilizes tf-idf based pseudo-relevance feedback where the expansion terms are obtained from the top retrieved passages ranked by BM25.</p><p>In order to avoid over loading the query with extra terms, the query is first categorized into 'explicit' and 'implicit' queries. Explicit queries are queries that are assumed to be complete and not in need of further expansion. Implicit queries are queries that are assumed to contain ambiguity like coreferences and omissions.</p><p>To categorize the turn utterances, the turns were syntactically analyzed using the SpaCy library. After tokenizing the turn, SpaCy parses and tags the given query which enables SpaCy to predict which tag or label most likely applies to the token in context. using this method, the turn was broken down into its composite objects such as verbs, nouns and pronouns. After analyzing different categorization methods using the syntactical structure of the query, the two categories were defined by focusing on the pronouns in a query. A turn was considered 'explicit' if it contained no pronouns, while 'implicit' queries contain at least one pronoun. If the turn utterance belongs to the 'implicit' query category, then it is further expanded using pseudo relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Stage 3: BERT Passage Re-Ranker</head><p>The final stage is to re-rank passages using a BERT re-ranking model. In order to compensate for the sparsity of CAsT training data, a pre-trained BERT model which is trained for passage reranking on another larger dataset is used. The pre-trained BERT reranker is publicly available, and is pre-trained on both MS MARCO and TREC CAR <ref type="bibr" coords="4,110.79,355.20,13.23,7.94" target="#b12">[13]</ref>. The MS MARCO dataset was used for training in this stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>To evaluate the performance of this retrieval pipeline on the 2020 data, the passage collection was first pre-processed and indexed. The evaluation measures and metrics are presented as well as an explanation of the four submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Pre-processing and Indexing</head><p>This year's CAsT track used two collections: MS MARCO and TREC CAR. The first step in data pre-processing is to remove duplicate passages from the collections using the provided duplicate passage list provided by the organizers. This mostly affects MS MARCO passages since CAR has already been deduped.</p><p>After that, the two collections are combined as a single collection with the schema of 'docid' and 'content'. This combined collection is later indexed using the Anserini toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Measures</head><p>The submitted runs are evaluated at NIST using the standard TREC style pooling and relevance assessment. Response pooling of the top results for the system was performed across participants. Response is assessed using a five-point relevance scale where:</p><p>• 0-Not relevant and fails to meet requirement.</p><p>• 1-Slightly meets and the answer can be inferred from the passage with some effort. • 2-Moderately meets requirement and the passage answers the turn but is focused on something related. • 3-Highly meets requirement and the passage answers the turn and is focused on the answer.</p><p>• 4-Fully meets requirement and the passage is the perfect response to the turn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Metrics</head><p>The evaluation metrics used are NDCG@1, NDCG@3, NDCG@5 and MAP@1000. The turn depth evaluates the system performance at the n-th turn in the conversation. Better performance at deeper turns in a conversation (larger n) indicates that the system is better at interpreting context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Official Runs</head><p>The four submitted runs were for the 'automatic' category which focuses on retrieving passages using the raw utterances exclusively. The parameters at the HQE stage are constant and set to the parameters suggested by the algorithm authors. The submitted runs are as follows:</p><p>HBKU_t2_1v1: This run expands queries in the PQE stage using top 3 terms from the top 3 retrieved passages. No query categorization was applied. Both HQE and PQE expanded queries were fed to the BERT re-ranker as input.</p><p>HBKU_t2_1v2: This run again expands queries in the PQE stage using top 3 terms from the top 3 retrieved passages without applying query categorization. only HQE expanded query was used at the BERT re-ranking phase.</p><p>HBKU_t5_1v1: This run is similar to the first one where the query was expanded using top 3 terms from the top 3 retrieved passages, however only queries of the 'implicit' category was expanded. Both HQE and PQE expanded queries were fed to the BERT re-ranker as input.</p><p>HBKU_t5_1v2: This run is similar to the previous run, however, in the BERT re-ranking phase only HQE expanded terms were used</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head><p>Table <ref type="table" coords="4,338.74,460.37,4.09,7.94" target="#tab_2">2</ref> shows a summary of the overall performance of this solution for the CAsT 2020 dataset. The proposed pipeline's performance is compared against the average median performance of all the submitted 'automatic' runs. From the table, it can be observed that all runs surpass the median performance on almost all the metrics. The best performing run is the HBKU_t5_1v2 where query categorization was applied to only supplement 'implicit' queries and where BERT was fed queries with HQE expansion. This shows that using the pseudo-relevant feedback method of PQE based on the TF-IDF scoring scheme might be best used for queries that are more ambiguous.</p><p>It is also interesting to see how performance varies across different turn depths. Table <ref type="table" coords="4,416.64,591.88,4.25,7.94" target="#tab_3">3</ref> shows the performance of the best submitted run (HBKU_t5_1v2) across different turns in a single topic. The topic being investigated is topic 81 which consists of 8 turns.</p><p>Table <ref type="table" coords="4,350.90,635.71,4.25,7.94" target="#tab_3">3</ref> illustrates that performance can fluctuate at different turn depths. The NDCG@3 and NDCG@5 score of turn IDs 81_4 and 81_7 was zero. The values for the different metrics fluctuate greatly from one turn to the other. This emphasizes how important it is to tailor retrieval for the conversation scenario. Context can be lost from one turn to the other when working with heavily interrelated turns, while topic shifts in a conversation can indicate a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>Passage Retrieval in a conversational setting is a very challenging field that introduces new problems for the IR and NLP communities. In a conversation, turns are often related and can contain coreferences and omissions. Context and history of a conversation is a very important factor in understanding user's information need. TREC CAsT aims to create a reusable benchmark by introducing a conversational passage retrieval task.</p><p>This submission to the 2020 TREC CAsT challenge tries to introduce context to conversation turns through a multi-stage retrieval pipeline inspired by last years winning algorithm. A pseudorelevance feedback step is introduced to the pipeline to try to enrich queries with terms from retrieved passages. In a conversation both the questions and answers usually direct the flow of the conversation. Using this theory, the passage query expansion stage is added to the pipeline. It is also very important to understand the type of turns in a conversation. Often times, a conversation turn can contain missing information or topic shifts. In this solution, queries are categorized into 'explicit' or 'implicit' queries based on whether the query contains pronouns. 'implicit' queries contain at least one pronoun and are assumed to need further clarification.</p><p>The solution performed better than the median across all submitted runs. However, for the future a more advanced method can be used to select terms in the PQE phase. This solution uses TF-IDF, but more advanced machine reading comprehension models can be used to select these terms. Using these sophisticated models, an 'answer' from the top retrieved answers can be used to add context to the turn. Turn categorization can be a valuable tool to understand the turns across different depths in the conversation. Better query categorization can help direct how to best add context to different turns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,341.48,289.28,193.21,7.70;3,317.96,115.92,240.25,159.36"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multi-Stage Retrieval System Pipeline.</figDesc><graphic coords="3,317.96,115.92,240.25,159.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,58.78,85.73,373.06,163.54"><head>Table 1 :</head><label>1</label><figDesc>TREC CAsT sample topic from the 2019 Train Dataset Title: Blood sugar Description: Blood sugar levels and possible complications that may arise due to it.</figDesc><table coords="2,58.78,142.95,190.87,106.32"><row><cell cols="2">Turn Conversation Utterances</cell></row><row><cell>1</cell><cell>What is a normal blood sugar level?</cell></row><row><cell>2</cell><cell>What does it mean if it's higher than this?</cell></row><row><cell>3</cell><cell>What is a dangerous level?</cell></row><row><cell>4</cell><cell>How do you bring it down quickly?</cell></row><row><cell>5</cell><cell>How fast can it rise?</cell></row><row><cell>6</cell><cell>And what if it's lower than normal?</cell></row><row><cell>7</cell><cell>How does this make you feel?</cell></row><row><cell>8</cell><cell>Do different activities lead to an imbalance?</cell></row><row><cell>9</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,69.77,298.30,224.28,84.66"><head></head><label></label><figDesc>MS MARCO has 1 million real search queries each with 10 passages from top ranked results. This results in a pool of approximately 8 million passages. The MARCO collection does contain near duplicates. • TREC CAR paragraph Corpus V2.03 is used. It's made up of paragraphs from Wikipedia '16. This corpus has been deduplicated. It contains approximately 30 million unique paragraphs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,116.02,85.73,379.66,97.69"><head>Table 2 :</head><label>2</label><figDesc>Submitted Run Performance Compared to Median of the Submitted 'Automatic' Runs NDCG@3 NDCG@5 NDCG@1000 AP@1000</figDesc><table coords="5,171.55,127.07,259.30,56.34"><row><cell>Median</cell><cell>0.279578</cell><cell>0.273523</cell><cell>0.374911</cell><cell>0.180143</cell></row><row><cell>HBKU_t2_1v1</cell><cell>0.2958</cell><cell>0.29</cell><cell>0.3692</cell><cell>0.2038</cell></row><row><cell>HBKU_t2_1v2</cell><cell>0.3089</cell><cell>0.2994</cell><cell>0.377</cell><cell>0.2077</cell></row><row><cell>HBKU_t5_1v1</cell><cell>0.3066</cell><cell>0.2964</cell><cell>0.3736</cell><cell>0.2061</cell></row><row><cell cols="2">HBKU_t5_1v2 0.3127</cell><cell>0.3026</cell><cell>0.379</cell><cell>0.2083</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,53.80,200.91,379.57,178.43"><head>Table 3 :</head><label>3</label><figDesc>Performance of Topic 81 Across Different Turn Depths</figDesc><table coords="5,53.80,227.07,372.75,152.27"><row><cell cols="5">Turn ID NDCG@3 NDCG@5 NDCG@1000 AP@1000</cell></row><row><cell>81_1</cell><cell>0.3348</cell><cell>0.4364</cell><cell>0.4218</cell><cell>0.1936</cell></row><row><cell>81_2</cell><cell>0.3616</cell><cell>0.4489</cell><cell>0.4768</cell><cell>0.2952</cell></row><row><cell>81_3</cell><cell>0.5312</cell><cell>0.6778</cell><cell>0.8407</cell><cell>0.7543</cell></row><row><cell>81_4</cell><cell>0</cell><cell>0</cell><cell>0.3263</cell><cell>0.0412</cell></row><row><cell>81_5</cell><cell>0.0848</cell><cell>0.0664</cell><cell>0.4247</cell><cell>0.1258</cell></row><row><cell>81_6</cell><cell>0.4693</cell><cell>0.3392</cell><cell>0.3352</cell><cell>0.0683</cell></row><row><cell>81_7</cell><cell>0</cell><cell>0</cell><cell>0.3345</cell><cell>0.0786</cell></row><row><cell>81_8</cell><cell>0.2346</cell><cell>0.1969</cell><cell>0.2422</cell><cell>0.0637</cell></row><row><cell cols="3">need to reset conversation history. Better methodologies of query</cell><cell></cell><cell></cell></row><row><cell cols="3">categorization might help understanding a conversation's context</cell><cell></cell><cell></cell></row><row><cell cols="2">flow and where to add context and where not to.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,333.39,407.88,224.81,6.18;5,333.39,415.85,224.81,6.18;5,333.39,423.79,224.81,6.23;5,333.39,431.76,89.90,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,464.18,407.88,94.02,6.18;5,333.39,415.85,23.05,6.18;5,378.35,415.85,179.85,6.18;5,333.39,423.82,46.28,6.18">Harnessing evolution of multi-turn conversations for effective answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manajit</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,392.15,423.79,166.05,6.23;5,333.39,431.76,67.22,6.23">Proceedings of the 2020 Conference on Human Information Interaction and Retrieval</title>
		<meeting>the 2020 Conference on Human Information Interaction and Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
	<note>Esteban Andrés Ríssola, and Fabio Crestani</note>
</biblStruct>

<biblStruct coords="5,333.39,439.76,225.88,6.18;5,333.15,447.73,226.13,6.18;5,333.39,455.67,224.81,6.23;5,333.39,463.64,132.19,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,333.15,447.73,223.23,6.18">Asking clarifying questions in open-domain information-seeking conversations</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,341.07,455.67,217.14,6.23;5,333.39,463.64,103.04,6.23">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,471.64,225.99,6.18;5,333.39,479.58,224.81,6.23;5,333.39,487.55,225.50,6.23;5,333.39,495.52,225.63,6.23;5,333.17,503.52,97.00,6.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,437.93,471.64,121.45,6.18;5,333.39,479.61,159.96,6.18">Effectiveness/efficiency tradeoffs for candidate generation in multi-stage retrieval architectures</title>
		<author>
			<persName coords=""><forename type="first">Nima</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2484028.2484132</idno>
		<ptr target="https://doi.org/10.1145/2484028.2484132" />
	</analytic>
	<monogr>
		<title level="m" coord="5,506.66,479.58,51.54,6.23;5,333.39,487.55,225.50,6.23;5,333.39,495.52,80.23,6.23">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval -SIGIR &apos;13</title>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval -SIGIR &apos;13<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="997" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,511.49,225.99,6.18;5,333.39,519.46,225.63,6.18;5,333.17,527.43,98.30,6.18" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,492.93,511.49,66.45,6.18;5,333.39,519.46,120.42,6.18">Query expansion techniques for information retrieval: A survey</title>
		<author>
			<persName coords=""><forename type="first">Hiteshwar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azad</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akshay</forename><surname>Deepak</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2019.05.009</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2019.05.009" />
		<imprint>
			<date type="published" when="2019-09">2019-09. 2019-09</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1698" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,535.40,225.88,6.18;5,333.39,543.37,224.81,6.18;5,333.39,551.34,225.88,6.18;5,333.39,559.31,16.21,6.18" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,368.42,543.37,189.78,6.18;5,333.39,551.34,57.39,6.18">Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05733</idno>
		<ptr target="http://arxiv.org/abs/1905.05733" />
		<imprint>
			<date type="published" when="2019-05-14">2019-05-14. 2019-05-14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,567.28,225.89,6.18;5,333.39,575.25,225.99,6.18;5,333.39,583.22,206.91,6.18" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,333.39,575.25,225.99,6.18;5,333.39,583.22,23.21,6.18">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2019-05-24">2019-05-24. 2019-05-24</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,591.19,225.64,6.18;5,333.39,599.16,225.88,6.18;5,333.39,607.13,139.43,6.18" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,537.49,591.19,21.54,6.18;5,333.39,599.16,176.22,6.18">Quasar: Datasets for Question Answering by Search and Reading</title>
		<author>
			<persName coords=""><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<ptr target="http://arxiv.org/abs/1707.03904" />
		<imprint>
			<date type="published" when="2017-08-08">2017-08-08. 2017-08-08</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,615.10,224.81,6.18;5,333.39,623.07,224.81,6.18;5,333.39,631.04,225.26,6.18;5,333.39,639.01,42.32,6.18" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">Ugur</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<ptr target="http://arxiv.org/abs/1704.05179" />
		<title level="m" coord="5,417.90,623.07,140.30,6.18;5,333.39,631.04,83.82,6.18">SearchQA: A New Q&amp;A Dataset Augmented with Context from a Search Engine</title>
		<imprint>
			<date type="published" when="2017-06-11">2017-06-11. 2017-06-11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,646.98,224.81,6.18;5,333.39,654.95,225.88,6.18;5,333.39,662.92,16.21,6.18" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08267</idno>
		<ptr target="http://arxiv.org/abs/1809.08267" />
		<title level="m" coord="5,495.12,646.98,63.08,6.18;5,333.39,654.95,51.96,6.18">Neural Approaches to Conversational AI</title>
		<imprint>
			<date type="published" when="2019-09-10">2019-09-10. 2019-09-10</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,670.89,225.88,6.18;5,333.18,678.86,225.02,6.18;5,333.39,686.83,224.97,6.18" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="5,333.18,678.86,225.02,6.18;5,333.39,686.83,43.02,6.18">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551</idno>
		<ptr target="http://arxiv.org/abs/1705.03551" />
		<imprint>
			<date type="published" when="2017-05-13">2017-05-13. 2017-05-13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,694.80,225.63,6.18;5,333.39,702.74,224.81,6.23;6,69.23,89.05,67.05,6.23" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Kratzwald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Eigenmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Feuerriegel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03008</idno>
		<title level="m" coord="5,535.59,694.80,23.43,6.18;5,333.39,702.77,163.90,6.18">Rankqa: Neural question answering with answer re-ranking</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,69.23,97.04,224.81,6.18;6,69.23,105.01,224.81,6.18;6,69.23,112.98,150.66,6.18" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="6,153.91,105.01,140.13,6.18;6,69.23,112.98,115.88,6.18">MS MARCO: A HUMAN GENERATED MACHINE READING COMPREHENSION DATASET</title>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,69.23,120.95,224.81,6.18;6,69.23,128.92,198.58,6.18" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="6,222.03,120.95,72.01,6.18;6,69.23,128.92,14.22,6.18">Passage Re-ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<ptr target="http://arxiv.org/abs/1901.04085" />
		<imprint>
			<date type="published" when="2020-04-14">2020-04-14</date>
			<biblScope unit="page" from="2020" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,69.23,136.90,224.81,6.18;6,69.23,144.87,224.81,6.18;6,69.23,152.84,87.84,6.18" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,197.09,136.90,96.95,6.18;6,69.23,144.87,126.37,6.18">2018-06-11. Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<ptr target="http://arxiv.org/abs/1806.03822" />
		<imprint>
			<date type="published" when="2018-06-11">2018-06-11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,69.23,160.81,224.81,6.18;6,69.23,168.78,224.81,6.18;6,69.23,176.72,224.81,6.23;6,69.23,184.69,225.63,6.23;6,69.01,192.69,97.00,6.18" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,150.01,168.78,144.03,6.18;6,69.23,176.75,87.37,6.18">Conversations with Documents: An Exploration of Document-Centered Assistance</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Maartje Ter Hoeve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elnaz</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343413.3377971</idno>
		<ptr target="https://doi.org/10.1145/3343413.3377971" />
	</analytic>
	<monogr>
		<title level="m" coord="6,168.71,176.72,125.34,6.23;6,69.23,184.69,99.88,6.23">Proceedings of the 2020 Conference on Human Information Interaction and Retrieval</title>
		<meeting>the 2020 Conference on Human Information Interaction and Retrieval<address><addrLine>Vancouver BC Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,69.23,200.63,225.58,6.23;6,69.00,208.63,40.71,6.18" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,139.35,200.66,126.39,6.18">The TREC-8 question answering track report</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,270.20,200.63,21.20,6.23">In TREC</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="77" to="82" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.39,89.07,225.88,6.18;6,333.39,97.04,225.89,6.18;6,333.39,104.99,224.81,6.23;6,333.39,112.96,225.88,6.23;6,333.39,120.95,113.14,6.18" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="6,350.46,97.04,205.68,6.18">Exploring Conversational Search With Humans, Assistants, and Wizards</title>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Vtyurina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denis</forename><surname>Savenkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3027063.3053175</idno>
		<ptr target="https://doi.org/10.1145/3027063.3053175" />
	</analytic>
	<monogr>
		<title level="m" coord="6,340.78,104.99,217.42,6.23;6,333.39,112.96,88.20,6.23">Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems -CHI EA &apos;17</title>
		<meeting>the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems -CHI EA &apos;17<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2187" to="2193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.39,128.92,224.81,6.18;6,333.18,136.87,223.19,6.23" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="6,364.76,136.90,160.04,6.18">Query and Answer Expansion from Conversation History</title>
		<author>
			<persName coords=""><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,539.41,136.87,13.57,6.23">TREC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.39,144.87,224.81,6.18;6,333.39,152.84,224.81,6.18;6,333.39,160.81,225.99,6.18;6,333.39,168.78,224.97,6.18;6,333.39,176.75,48.11,6.18" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="6,474.97,152.84,83.23,6.18;6,333.39,160.81,225.99,6.18;6,333.39,168.78,35.48,6.18">Response Ranking with Deep Matching Networks and External Knowledge in Information-seeking Conversation Systems</title>
		<author>
			<persName coords=""><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210011</idno>
		<idno type="arXiv">arXiv:1805.00188</idno>
		<ptr target="https://doi.org/10.1145/3209978.3210011" />
		<imprint>
			<date type="published" when="2018-06-27">2018-06-27. 2018-06-27</date>
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,333.39,184.72,225.99,6.18;6,333.39,192.69,225.06,6.18;6,333.39,200.66,225.88,6.18;6,333.39,208.63,139.43,6.18" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<ptr target="http://arxiv.org/abs/1809.09600" />
		<title level="m" coord="6,518.92,192.69,39.53,6.18;6,333.39,200.66,183.19,6.18">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</title>
		<imprint>
			<date type="published" when="2018-09-25">2018-09-25. 2018-09-25</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
