<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,230.03,69.92,139.87,12.90;1,145.97,85.86,305.60,12.90">Spotify at TREC 2020: Genre-Aware Abstractive Podcast Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.44,113.95,102.48,10.75;1,251.92,112.42,1.41,6.99"><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
							<email>rezapou2@illinois.edu</email>
						</author>
						<author>
							<persName coords="1,359.22,113.95,78.29,10.75"><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
							<email>sravana@spotify.com</email>
						</author>
						<author>
							<persName coords="1,168.79,186.86,60.78,10.75"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
							<email>aclifton@spotify.com</email>
						</author>
						<author>
							<persName coords="1,368.73,186.86,59.26,10.75"><forename type="first">Rosie</forename><surname>Jones</surname></persName>
							<email>rjones@spotify.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,230.03,69.92,139.87,12.90;1,145.97,85.86,305.60,12.90">Spotify at TREC 2020: Genre-Aware Abstractive Podcast Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">593861CBB29931C5C492CC076F9C05D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper contains the description of our submissions to the summarization task of the Podcast Track in TREC (the Text REtrieval Conference) 2020. The goal of this challenge was to generate short, informative summaries that contain the key information present in a podcast episode using automatically generated transcripts of the podcast audio. Since podcasts vary with respect to their genre, topic, and granularity of information, we propose two summarization models that explicitly take genre and named entities into consideration in order to generate summaries appropriate to the style of the podcasts. Our models are abstractive, and supervised using creator-provided descriptions as ground truth summaries. The results of the submitted summaries show that our best model achieves an aggregate quality score of 1.58 in comparison to the creator descriptions and a baseline abstractive system which both score 1.49 (an improvement of 9%) as assessed by human evaluators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advancements in state-of-the-art sequence to sequence models and transformer architectures <ref type="bibr" coords="1,71.64,616.06,98.23,9.46" target="#b15">(Vaswani et al., 2017;</ref><ref type="bibr" coords="1,173.19,616.06,72.97,9.46" target="#b1">Dai et al., 2019;</ref><ref type="bibr" coords="1,249.48,616.06,40.79,9.46;1,72.00,629.61,53.81,9.46" target="#b8">Nallapati et al., 2016;</ref><ref type="bibr" coords="1,129.23,629.61,71.45,9.46" target="#b11">Ott et al., 2019;</ref><ref type="bibr" coords="1,204.09,629.61,82.75,9.46" target="#b5">Lewis et al., 2020)</ref>, and the availability of large-scale datasets have led to rapid progress in generating near-humanquality coherent summaries with abstractive summarization. The majority of effort in this domain has been focused on summarizing news datasets such as CNN/Daily Mail <ref type="bibr" coords="1,187.88,710.91,103.12,9.46" target="#b3">(Hermann et al., 2015)</ref> and XSum <ref type="bibr" coords="1,124.12,724.45,98.40,9.46" target="#b9">(Narayan et al., 2018)</ref>. Podcasts are much more diverse than news articles in the level of information as well as their structure, which varies by theme, genre, level of formality, and target audience, motivating the TREC 2020 podcast summarization task <ref type="bibr" coords="1,394.96,305.74,80.56,9.46" target="#b4">(Jones et al., 2020)</ref>, where the objective is to design models to produce coherent text summaries of the main information in podcast episodes.</p><p>While no ground truth summaries were provided for the TREC 2020 task, the episode descriptions written by the podcast creators serve as proxies for summaries, and are used for training our supervised models. We make some observations about the creator descriptions towards understanding what goes into a 'good' podcast summary. Our first observation is that the descriptions vary stylistically by genre. For instance, the descriptions of sports podcasts tend to list a series of small events and interviews, usually anchored on the names of players, coaches, and teams. In contrast, descriptions of true crime podcasts tend to contain suspenseful hooks, and avoid giving away much of the plot (Table 1). Some podcasts include the names of hosts or guests in their descriptions, while others only talk about the main theme of the episode.</p><p>By definition, a good summary should be concise, preserve the most salient information, and be factually correct and faithful to the given document <ref type="bibr" coords="1,339.22,632.92,146.35,9.46" target="#b10">(Nenkova and McKeown, 2011;</ref><ref type="bibr" coords="1,489.71,632.92,35.84,9.46;1,307.28,646.47,53.51,9.46" target="#b6">Maynez et al., 2020)</ref>. In addition, we believe that since users' expectations of summaries are shaped by the creator descriptions and the style and tone of the content, it is important to generate summaries that are appropriate to the style of the podcast.</p><p>In this work, we present two types of summarization models that take these observations into consideration. The goal of the first model, which we shorthand as 'category-aware', is to generate summaries whose style is appropriate for the genre or category of the specific podcast. The second model aims to maximize the presence of named entities in the summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Filtering</head><p>The Spotify Podcast Dataset consists of 105,360 episodes with transcripts and creator descriptions <ref type="bibr" coords="2,71.64,498.94,87.67,9.46" target="#b0">(Clifton et al., 2020)</ref>, and is provided as a training dataset for the summarization task. We applied a set of heuristic filters on this data with respect to the creator descriptions.</p><p>• Removed episodes with unusually short (fewer than 10 characters) or long (more than 1300 characters) creator descriptions.</p><p>• Removed episodes whose descriptions were highly similar to the descriptions of other episodes in the same show or to the show description. Similarity was measured by taking the TF-IDF representation of a description and comparing it to others using cosine similarity.</p><p>• Removed ads and promotional sentences in the descriptions. We annotated a set of 1000 episodes for such material, and trained a classifier to detect these sentences; more details are described in <ref type="bibr" coords="2,165.61,755.86,82.41,9.46" target="#b13">Reddy et al. (2021)</ref>.</p><p>The dataset after these filters consists of 90055 episodes. We held out 1000 random episodes for test and validation sets for development, and used 88055 episodes for training our models. Our final submissions were made on the task's test set of 1027 episodes disjoint from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Abstractive Model Framework</head><p>Our summarization models are built using BART <ref type="bibr" coords="2,306.92,194.81,84.91,9.46" target="#b5">(Lewis et al., 2020)</ref>, which is a denoising autoencoder for pretraining sequence to sequence models. To generate summaries, we started from a model pretrained on the CNN/Daily Mail news summarization dataset<ref type="foot" coords="2,407.50,246.96,3.99,6.91" target="#foot_0">1</ref> and then fine-tuned it on our podcast transcript dataset with respect to the two proposed models (described below in §4).</p><p>We used the implementation of BART in the Huggingface Transformers library <ref type="bibr" coords="2,469.29,303.57,57.62,9.46;2,307.28,317.12,24.94,9.46" target="#b16">(Wolf et al., 2020)</ref> with the default hyperparameters. We set the batch size to 1, and made use of 4 GPUs for training. The best model with the highest ROUGE-L score on the validation set was saved. The maximum sequence limit we set for the input podcast transcript was 1024 tokens (that is, any material beyond the first 1024 tokens was ignored). We constrained the model to generate summaries with at least 50 and at most 250 tokens.</p><p>This setup is the same as the bartpodcasts model described in the task overview <ref type="bibr" coords="2,441.71,452.98,80.51,9.46" target="#b4">(Jones et al., 2020)</ref>, and is one of our baselines for comparison. We also compare our models to the bartcnn model baseline, which is the out of the box BART model trained on the CNN/Daily Mail corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Category-Aware Summarization</head><p>The idea of building a category-aware summarization model is motivated by the hypothesis that selecting what is important to a summary depends on the topical category of the podcast. This hypothesis is based upon observations of the creator descriptions in the dataset. For example, descriptions of 'Sports' podcast episodes tend to contain names of players and matches, 'True Crime' podcasts incorporate a hook and suspense, and 'Comedy' podcasts are stylistically informal (Table <ref type="table" coords="2,487.29,694.17,3.94,9.46" target="#tab_0">1</ref>).</p><p>While we expect some of these patterns to be naturally learned by a supervised model trained on the large corpus, we nudged the model to generate stylistically appropriate summaries by explicitly conditioning the summaries on the podcast genre. Some previous work approaches this problem by concatenating a vector corresponding to an embedding of the conditioning parameters to the inputs in an RNN <ref type="bibr" coords="3,110.48,331.43,119.21,9.46" target="#b2">(Ficler and Goldberg, 2017)</ref>. More recent work simply concatenates a token corresponding to the parameter to the text input in sequence-tosequence transformer models <ref type="bibr" coords="3,203.43,372.07,84.02,9.46" target="#b12">(Raffel et al., 2020)</ref>. We experimented with a few ways of encoding the category labels in the summarization model, and settled upon prepending the labels as special tokens to the transcript as the input during training and inference, as shown in Figure <ref type="figure" coords="3,201.50,439.82,4.17,9.46" target="#fig_0">1</ref>. For the episodes with multiple category labels, we concatenated the labels and included them all as distinct special tokens, concatenated in a fixed (lexicographic) order.</p><p>The podcast categories are not given in the metadata files distributed with the podcast dataset. However, they can be derived from the RSS headers associated with each podcast under the itunes:category field. In our case, we leveraged the labels assigned in the Spotify catalog. The category labels in the catalog are mostly the same as the labels in the RSS header, with some minor changes. The taxonomy of iTunes categories changes over time, leading to different labels for semantically similar categories. We chose to heuristically collapse similar categories together, such as 'Sports' with 'Sports &amp; Recreation', giving a set of 22 categories (Table <ref type="table" coords="3,131.72,672.36,3.94,9.46">2</ref>).</p><p>After prepending the labels as special tokens to the transcript, we fine-tuned the BART baseline. Since training BART was computationally expensive, we only trained the model for a maximum of 2 epochs and generated abstractive summaries using 1 (category-aware-1epoch) and 2 (category- Table <ref type="table" coords="3,331.73,502.01,3.88,8.64">2</ref>: Set of collapsed category labels and their distribution on the task test set. Some episodes are associated with multiple labels.</p><p>aware-2epochs) epochs, separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Named Entity Biased Model</head><p>Named entities are information-dense, and many podcast topics tend to center around people and places, making them appropriate for inclusion in summaries. We furthermore (1) observed that named entity mentions are prevalent in creator descriptions, and (2) surveyed a group of podcast listeners about what they look for in a summary, where the presence of names and locations was highlighted. Therefore, our second summarization algorithm aims to bias the model to maximize named entities in summaries.</p><p>Our model takes a two-step approach by extract-ing a portion of the transcript that is both highly relevant to the episode and tends to contain salient named entities. The extracted portion of the transcripts is the input for both training and inference. This approach is similar to the previous approaches that implement an extractive summarizer followed by an abstractive model <ref type="bibr" coords="4,178.21,147.96,72.59,9.46" target="#b14">(Tan et al., 2017)</ref>.</p><p>Our proposed extractive summarizer is similar to TextRank <ref type="bibr" coords="4,115.89,175.47,114.30,9.46" target="#b7">(Mihalcea and Tarau, 2004</ref>) and consists of the following steps:</p><p>• We first identified named entities for the entire transcript of each episode by using a BERT token classification model trained 2 on the CoNLL-03 NER data.</p><p>• We divided each episode into segments corresponding to about 60 seconds of audio.</p><p>• Every segment was represented in its original form s, and as a list of its named entity tokens t. All pairwise similarities between segments were computed as the proportion of word overlap between them. We computed both Sim s (i, j), the similarity between the original forms of the segments i and j, and Sim t (i, j), the similarity between the list of named entity tokens in the segments.</p><p>• Degree centralities for each segment C s (i) = j Sim s (i, j) and C t (i) = j Sim t (i, j) were computed. The top 7 segments by C s (i) and the top 3 non-overlapping segments by C t (i) were extracted and presented in position order as the extractive summary. We chose this heuristic to encourage the extracted segments to be semantically relevant and contain named entities.</p><p>The BART baseline was then fine-tuned for 3 epochs on extracted segments as input (rather than the first 1024 tokens of the transcript). We name this the coarse2fine model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The TREC task uses two evaluation methods to analyze performance: (1) ROUGE-L scores to compare the generated summaries against the creator descriptions as the reference, and (2) human evaluations facilitated by the National Institute of 2 https://huggingface.co/dbmdz/bert-large-casedfinetuned-conll03-english Standards and Technology (NIST) to asses the quality of the generated summaries with respect to the episode content.</p><p>Table <ref type="table" coords="4,345.06,352.03,5.49,9.46" target="#tab_2">3</ref> shows the results of our models against the bartcnn and bartpodcasts baselines on the whole test set. Compared to the baselines, both category-aware models achieved higher ROUGE precision and F1-scores. While bartcnn has the highest recall (27.19%), our category-aware-2epochs summarizer outperformed other models achieving 27.75% precision and 18.42% F1-score.</p><p>Since creator descriptions are of varying quality, we performed another evaluation, selecting only those episodes whose creator descriptions were labeled as 'Good' or 'Excellent' by the evaluators (Table <ref type="table" coords="4,336.82,514.96,3.88,9.46" target="#tab_3">4</ref>). The category-aware-2epochs model still achieves the highest ROUGE precision, and has comparable F1 with the bartpodcasts baseline.</p><p>High precision across our models shows that prepending the categories to the transcripts resulted in more relevant information in the generated summaries, with tradeoffs against recall. Tables <ref type="table" coords="4,501.44,596.59,5.50,9.46" target="#tab_2">3</ref> and<ref type="table" coords="4,306.89,610.14,5.55,9.46" target="#tab_3">4</ref> show that the coarse2fine model only improved  over bartcnn. We observe that the summaries do consist of more named entities compared to the other models (Table <ref type="table" coords="5,165.59,93.76,4.63,9.46" target="#tab_7">6</ref>) as intended. We observe that the discontinuities in the extracted segments result in more incoherent abstractive summaries than the models that consume a contiguous block of text, and sometimes contain 'hallucinated' content.</p><p>Looking into the manual assessment of the summaries (Table <ref type="table" coords="5,135.37,177.99,4.02,9.46" target="#tab_5">5</ref>), we see that the category-aware-2epochs model generated more summaries labeled as 'Excellent' and fewer summaries labeled as 'Bad' compared to the other models and is comparable in quality to the creator descriptions.</p><p>As an illustrative example, we present the summaries of the models as well as the creator description of one selected episode of a sports podcast in Table <ref type="table" coords="5,112.37,289.32,4.17,9.46" target="#tab_7">6</ref>. The creator description contains different named entities (in bold) as well as social media information and links (in blue). The bartcnn summary is verbose and long and captured the majority of the information from the beginning of the transcript, following the structure of the news summarizer. bartpodcasts and category-aware-1epoch, on the other hand, are similar to the creator description. The category-aware-2epochs is the shortest compared to the other models, however, it includes the main theme and the majority of the necessary information. Finally, the coarse2fine summary does not include the names of the host or guest but it is the only one that contains additional named entities, similar to the creator description.</p><p>NIST judges also assessed the quality of the summaries by answering eight yes/no questions about podcast summary attributes. A good summary will receive 'yes' for all questions except Q6 ("Does the summary contain redundant information?"). With respect to Q6, Q7, and Q8, our category-aware-2epoch model generates the most coherent summaries with the least amount of redundant information compared to the baselines (Table <ref type="table" coords="5,233.17,603.89,4.45,9.46" target="#tab_8">7</ref>) suggesting that the model may be learning aspects of style.</p><p>Overall, the automatic and manual evaluations of our models show that our category-aware models outperformed the baseline resulting in high quality summaries. The results of the submitted summaries show that our best model achieves an aggregate quality score (as defined by the task) of 1.58 in comparison to the creator descriptions and a baseline abstractive system which both score 1.49 (an improvement of 9%) as assessed by human evaluators (Table <ref type="table" coords="5,121.42,755.86,3.94,9.46" target="#tab_5">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we showcased two abstractive summarization models, one that is informed by the categories of the podcasts, and a hybrid model that uses an extractive step biased towards named entities. Experimental results show that our category-aware models generate better summaries than the baselines, but the model intended to bias summaries to named entity mentions underperforms. In future, we hope to investigate new and robust ways of biasing the summarizer to generate named entities in the summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Creator Description</head><p>Mike Rankin welcomes Sox in the Basement host Chris Lanuti to talk optimism on the South Side. The guys react to Yoan Moncada's recent extension and later explore the potential for Dylan Cease, Aaron Bummer and well as Andrew Vaughn in 2020. You can follow FutureSox (@FutureSox), Chris Lanuti (@ChrisLanuti) and Sox In The Basement (@SoxInTheBasement) on Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Summaries bartcnn</head><p>Hi. Hello and welcome to another episode of the Future Socks podcast. My name is Mike Rankin. I will be your host this time around and boy. What's going on? I appreciate you jumping in with us to talk some White Sox baseball on the future Sox podcast. You talked about socks in the basement for a couple of years now take me back to the Art, and what was the inspiration and then the growth across those two seasons? Well, the funny thing is that I had been doing podcasting for like a decade just as a hobby I used to do morning radio, but I never did anything in Chicago. I was in Champaign, Illinois and Reno, Nevada and Wheeling, West Virginia Pittsburgh and Southern California and Bakersfield, which is the armpit of America. It's like a hundred twenty two degrees in the shade and I'm from Chicago. And so when I came back here and you know started doing Thing that was out of radio. I still had the bug and I did this show called the broadcast basement and it was just a bunch of my friends used to be in radian. We'd mess around and somewhere along the line a couple years ago. Chris Ludacris of Socks in the Basement joins the show to talk about the White Sox offseason and the future of the team as well as his podcast Socks In The Basement and how it has grown to where it is today. coarse2fine Socks in the Basement"" is a weekly podcast covering the Chicago White Sox baseball team through the eyes of Sox fans. Hosts are @BBWhiteSox, @SoxOnTheBus, and @BBBlackSox. This week we discuss the White Sox's bullpen, Jose Abreu, Jose Ramirez, Dylan Cease, and much more. You can follow us on Twitter @SocksInTheBasementPod and check out the extra content about the show at SoxOnTheBaset.com. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,173.07,229.83,251.41,8.64;3,281.82,116.54,130.44,56.49"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sketch of the Category-Aware Summarization Model</figDesc><graphic coords="3,281.82,116.54,130.44,56.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,71.69,64.01,220.23,298.11"><head>Table 1 :</head><label>1</label><figDesc>Jon Rothstein is joined by Arizona State Head Coach Bobby Hurley, Creighton Guard Mitchell Ballock, and Merrimack Head Coach Joey Gallo checks in on the Hustle Mania Hotline. This is March and Jon has a special message for all of you heading into the NCAA tournament.</figDesc><table coords="2,80.70,64.01,204.65,240.29"><row><cell cols="2">Sports In EP 4 True Crime Sometimes, it takes years to connect a</cell></row><row><cell></cell><cell>killers crimes. On March 6th 1959 a man</cell></row><row><cell></cell><cell>was born who would, eventually, be tried</cell></row><row><cell></cell><cell>for the murder of a single woman. It</cell></row><row><cell></cell><cell>would take years for police to connect</cell></row><row><cell></cell><cell>him to 4 other murders. Years and a</cell></row><row><cell></cell><cell>clever investigator who got the DNA he</cell></row><row><cell></cell><cell>needed.</cell></row><row><cell>Comedy</cell><cell>This weeks episode is a little bit of a</cell></row><row><cell></cell><cell>throwback.. We have one of our close</cell></row><row><cell></cell><cell>high school friends Zak on the show, and</cell></row><row><cell></cell><cell>he is just a blast! We discuss relation-</cell></row><row><cell></cell><cell>ships, our favourite movies and our expe-</cell></row><row><cell></cell><cell>rience's with high school! This episode</cell></row><row><cell></cell><cell>is pretty chill, so you better have a drink</cell></row><row><cell></cell><cell>for this one!</cell></row></table><note coords="2,108.79,317.61,183.13,8.64;2,72.00,329.57,219.92,8.64;2,72.00,341.52,218.27,8.64;2,72.00,353.48,168.97,8.64"><p>Examples of creator descriptions for a different genres of podcast episodes, highlighting the prevalence of named entities in sports, suspense in true crime, and colloquial language in comedy.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,306.97,63.98,220.23,187.29"><head>Table 3 :</head><label>3</label><figDesc>ROUGE scores against all of the creator descriptions in the test set.</figDesc><table coords="4,315.79,63.98,203.45,187.29"><row><cell></cell><cell></cell><cell>ROUGE-L</cell><cell></cell></row><row><cell></cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>bartcnn</cell><cell>8.49</cell><cell>27.19</cell><cell>11.3</cell></row><row><cell>bartpodcasts</cell><cell>20.78</cell><cell>21.01</cell><cell>16.64</cell></row><row><cell>category-aware-1epoch</cell><cell>22.70</cell><cell>20.82</cell><cell>17.62</cell></row><row><cell>category-aware-2epochs</cell><cell>25.75</cell><cell>19.86</cell><cell>18.42</cell></row><row><cell>coarse2fine</cell><cell>15.76</cell><cell>18.75</cell><cell>13.59</cell></row><row><cell></cell><cell></cell><cell>ROUGE-L</cell><cell></cell></row><row><cell></cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>bartcnn</cell><cell>10.87</cell><cell>29.85</cell><cell>14.64</cell></row><row><cell>bartpodcasts</cell><cell>27.89</cell><cell>25.51</cell><cell>22.15</cell></row><row><cell>category-aware-1epoch</cell><cell>27.49</cell><cell>22.45</cell><cell>20.54</cell></row><row><cell>category-aware-2epochs</cell><cell>30.63</cell><cell>23.23</cell><cell>22.02</cell></row><row><cell>coarse2fine</cell><cell>22.85</cell><cell>20.57</cell><cell>17.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,306.97,264.24,219.57,20.91"><head>Table 4 :</head><label>4</label><figDesc>ROUGE scores aggregated over only those 71 creator descriptions that were rated good or excellent.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,306.97,717.03,220.23,44.50"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note coords="4,346.27,717.03,179.28,8.64;4,307.28,728.98,219.92,8.64;4,307.28,740.94,219.51,8.64;4,307.28,752.89,80.51,8.64"><p>Distribution of EGFB annotations over 179 judged episodes (percentages), and the aggregate quality score, computed as a weighted average with E=4, G=2, F=1, and B=0.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,81.21,217.66,437.24,141.45"><head></head><label></label><figDesc>bartpodcasts Socks in the Basement" host Chris Ludacris joins Mike Rankin of the Future Sox Podcast to talk all things White Sox baseball, including the recent success of the team and what it means for the future of the franchise. Chris is the host of the Socks In The Basement podcast, which is a weekly podcast covering the Chicago White Sox. You can follow them on Twitter @socksinthebasepod and check out the show at socksinthesound.com. You can also follow the show on Twitter at @SocksInTheBasetPod and on Instagram at SocksinTheBasepod.comsocks in the baset. The show is brought to you by the Chicago Sun-Times and Wrigley Field. category-aware-1epoch On this episode of the Future Sox Podcast, Chris Ludacris from Socks in the Basement joins Mike Rankin to talk about the White Sox's 2019 season and what we expect from the Sox in 2020. We also talk about how the Sox are getting better on the field and off the field, and how we expect the Sox to get better in 2020 as well. We hope you enjoy this episode, and if you do, please let us know what you thought of the show and what you would like to see in 2020 by leaving us a review on your favorite podcast platform. We love to hear your thoughts and feedback on the show, so feel free to reach out to us on Twitter at @SocksInTheBasementPod and we'd love to have you on the podcast in the future. category-aware-2epochs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,71.69,439.53,457.62,277.33"><head>Table 6 :</head><label>6</label><figDesc>Example summaries generated by various models for a sports podcast episode (https://open.spotify.</figDesc><table coords="6,80.82,474.76,442.35,242.10"><row><cell>Attribute</cell><cell>bartcnn</cell><cell cols="2">bartpodcasts category-</cell><cell>category-</cell><cell>coarse2fine</cell></row><row><cell></cell><cell></cell><cell></cell><cell>aware-</cell><cell>aware-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1epoch</cell><cell>2epochs</cell><cell></cell></row><row><cell>Q1: Does the summary include names of</cell><cell>72.6</cell><cell>51.4</cell><cell>64.2</cell><cell>55.3</cell><cell>35.2</cell></row><row><cell>the main people (hosts, guests, characters) in-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>volved or mentioned in the podcast?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q2: Does the summary give any additional in-</cell><cell>48.0</cell><cell>34.6</cell><cell>40.2</cell><cell>40.8</cell><cell>22.9</cell></row><row><cell>formation about the people mentioned (such</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>as their job titles, biographies, personal back-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ground, etc)?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q3: Does the summary include the main</cell><cell>69.3</cell><cell>78.2</cell><cell>76.5</cell><cell>77.7</cell><cell>60.3</cell></row><row><cell>topic(s) of the podcast?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q4: Does the summary tell you anything about</cell><cell>58.1</cell><cell>49.2</cell><cell>59.2</cell><cell>57.0</cell><cell>41.3</cell></row><row><cell>the format of the podcast; e.g. whether it's an</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>interview, whether it's a chat between friends,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a monologue, etc?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q5: Does the summary give you more context</cell><cell>67.6</cell><cell>65.4</cell><cell>65.4</cell><cell>65.9</cell><cell>57</cell></row><row><cell>on the title of the podcast?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q6: Does the summary contain redundant in-</cell><cell>25.7</cell><cell>14.0</cell><cell>12.8</cell><cell>6.7</cell><cell>10.1</cell></row><row><cell>formation?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q7: Is the summary written in good English?</cell><cell>34.6</cell><cell>86.6</cell><cell>86</cell><cell>88.0</cell><cell>79.9</cell></row><row><cell>Q8: Are the start and end of the summary good</cell><cell>22.3</cell><cell>62</cell><cell>69.8</cell><cell>70.4</cell><cell>55.3</cell></row><row><cell>sentence and paragraph start and end points?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="6,72.00,452.98,188.72,6.31"><p>com/episode/5KmWK8Qh5lsWIb2sUuJp4r).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,89.19,730.23,418.87,8.64"><head>Table 7 :</head><label>7</label><figDesc>Average percentage of 'Yes' judgments by human evaluators for each of the summary attributes.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,323.42,757.13,168.24,7.77"><p>https://huggingface.co/facebook/bart-large-cnn</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,307.28,282.83,219.52,8.64;5,318.19,293.79,209.02,8.64;5,317.94,304.75,208.86,8.64;5,318.19,315.71,209.01,8.64;5,318.19,326.50,207.36,8.81;5,318.19,337.46,209.01,8.58;5,318.19,348.42,208.61,8.81;5,318.19,359.55,209.01,8.64;5,318.19,370.50,79.15,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,439.13,315.71,88.07,8.64;5,318.19,326.67,126.01,8.64">000 podcasts: A spoken English document corpus</title>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Bonab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.519</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,476.19,326.50,49.35,8.58;5,318.19,337.46,209.01,8.58;5,318.19,348.42,77.76,8.58">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="5903" to="5917" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct coords="5,307.28,392.90,219.92,8.64;5,318.19,403.86,209.10,8.64;5,317.88,414.82,207.67,8.64;5,318.19,425.61,207.36,8.81;5,317.58,436.57,209.62,8.58;5,318.19,447.53,146.96,8.81" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,317.88,414.82,207.67,8.64;5,318.19,425.78,87.17,8.64">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,428.00,425.61,97.54,8.58;5,317.58,436.57,209.62,8.58;5,318.19,447.53,68.12,8.58">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,470.10,218.27,8.64;5,318.19,481.06,209.01,8.64;5,318.19,491.85,209.02,8.81;5,318.19,502.81,209.10,8.81;5,317.83,513.94,172.13,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,479.60,470.10,45.95,8.64;5,318.19,481.06,209.01,8.64;5,318.19,492.02,14.39,8.64">Controlling linguistic style aspects in neural language generation</title>
		<author>
			<persName coords=""><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4912</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,359.94,491.85,167.26,8.58;5,318.19,502.81,48.11,8.58">Proceedings of the Workshop on Stylistic Variation</title>
		<meeting>the Workshop on Stylistic Variation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,536.34,219.92,8.64;5,318.19,547.30,208.61,8.64;5,318.19,558.25,207.36,8.64;5,318.19,569.04,207.36,8.81;5,318.19,580.00,153.12,8.81" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,419.89,558.25,105.65,8.64;5,318.19,569.21,63.51,8.64">Teaching machines to read and comprehend</title>
		<author>
			<persName coords=""><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,400.44,569.04,125.11,8.58;5,318.19,580.00,74.02,8.58">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,602.57,219.92,8.64;5,318.19,613.53,208.61,8.64;5,318.19,624.49,207.36,8.64;5,318.19,635.28,207.36,8.81;5,317.88,646.24,208.21,8.81" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,475.67,624.49,49.87,8.64;5,318.19,635.45,127.86,8.64">Overview of the TREC 2020 Podcasts Track</title>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,468.94,635.28,56.61,8.58;5,317.88,646.24,84.42,8.58">The 29th Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>TREC 2020) notebook. NIST</note>
</biblStruct>

<biblStruct coords="5,307.28,668.81,219.92,8.64;5,318.19,679.77,207.53,8.64;5,318.19,690.73,209.10,8.64;5,318.19,701.69,209.01,8.64;5,318.19,712.64,208.61,8.64;5,318.19,723.43,209.01,8.81;5,318.19,734.39,207.36,8.58;5,317.91,745.35,207.64,8.81;5,318.19,756.48,122.60,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,345.60,701.69,181.60,8.64;5,318.19,712.64,208.61,8.64;5,318.19,723.60,76.04,8.64">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,414.53,723.43,112.67,8.58;5,318.19,734.39,207.36,8.58;5,317.91,745.35,42.37,8.58">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,67.28,218.27,8.64;7,82.91,78.24,209.02,8.64;7,82.91,89.03,207.36,8.81;7,82.91,99.99,207.36,8.58;7,82.58,110.95,209.34,8.81;7,82.91,122.08,192.06,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,185.31,78.24,106.61,8.64;7,82.91,89.20,135.59,8.64">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.173</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,240.91,89.03,49.35,8.58;7,82.91,99.99,207.36,8.58;7,82.58,110.95,106.40,8.58">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1906" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,143.86,219.92,8.64;7,82.91,154.65,209.02,8.81;7,82.91,165.61,207.36,8.58;7,82.60,176.57,112.61,8.81" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,224.93,143.86,67.00,8.64;7,82.91,154.82,71.56,8.64;7,173.19,176.74,16.51,8.64">Textrank: Bringing order into text</title>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,173.71,154.65,118.22,8.58;7,82.91,165.61,207.36,8.58;7,82.60,176.57,82.88,8.58">Proceedings of the 2004 conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2004 conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct coords="7,72.00,198.52,219.52,8.64;7,82.91,209.48,209.01,8.64;7,82.91,220.44,207.36,8.64;7,82.91,231.23,207.36,8.81;7,82.66,242.19,209.26,8.58;7,82.91,253.14,130.69,8.81" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,213.23,198.52,78.29,8.64;7,82.91,209.48,136.73,8.64;7,257.62,209.48,34.30,8.64;7,82.91,220.44,207.36,8.64;7,82.91,231.40,69.66,8.64">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,184.05,231.23,106.22,8.58;7,82.66,242.19,209.26,8.58;7,82.91,253.14,61.33,8.58">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
	<note>Cicero dos Santos, C ¸aglar Gulc ¸ehre, and Bing Xiang</note>
</biblStruct>

<biblStruct coords="7,72.00,275.10,220.01,8.64;7,82.91,286.06,207.69,8.64;7,82.91,297.01,209.01,8.64;7,82.91,307.80,207.36,8.81;7,82.58,318.76,209.34,8.58;7,82.91,329.72,209.01,8.81;7,82.91,340.85,197.60,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,110.27,286.06,180.33,8.64;7,82.91,297.01,209.01,8.64;7,82.91,307.97,83.62,8.64">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName coords=""><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,190.14,307.80,100.12,8.58;7,82.58,318.76,209.34,8.58;7,82.91,329.72,69.21,8.58">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,362.46,219.92,8.81;7,82.91,373.42,170.76,8.81" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,269.98,362.46,21.94,8.58;7,82.91,373.42,81.94,8.58">Automatic summarization</title>
		<author>
			<persName coords=""><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,395.38,218.27,8.64;7,82.91,406.33,207.36,8.64;7,82.91,417.29,207.36,8.64;7,82.91,428.08,207.35,8.81;7,82.91,439.04,209.01,8.58;7,82.91,450.00,207.36,8.58;7,82.24,460.96,127.69,8.81" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,178.55,417.29,111.72,8.64;7,82.91,428.25,121.55,8.64">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,229.25,428.08,61.01,8.58;7,82.91,439.04,209.01,8.58;7,82.91,450.00,207.36,8.58;7,82.24,460.96,68.35,8.58">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,482.91,219.92,8.64;7,82.91,493.87,207.36,8.64;7,82.91,504.83,207.36,8.64;7,82.91,515.79,209.01,8.64;7,82.91,526.58,209.01,8.81;7,82.91,537.54,87.75,8.81" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,250.42,504.83,39.85,8.64;7,82.91,515.79,209.01,8.64;7,82.91,526.75,62.86,8.64">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,155.64,526.58,136.28,8.58;7,82.91,537.54,24.46,8.58">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,559.49,218.27,8.64;7,82.91,570.45,209.10,8.64;7,82.91,581.41,207.36,8.64;7,82.60,592.20,207.67,8.58;7,82.58,603.16,209.34,8.58;7,82.91,614.11,132.53,8.58" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,110.11,581.41,162.99,8.64">Detecting extraneous content in podcasts</title>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aswin</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,82.60,592.20,207.67,8.58;7,82.58,603.16,32.66,8.58">Proceedings of the 16th Conference of the European Chapter</title>
		<title level="s" coord="7,130.40,603.16,161.52,8.58;7,82.91,614.11,30.01,8.58">the Association for Computational Linguistics</title>
		<meeting>the 16th Conference of the European Chapter<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,636.07,220.01,8.64;7,82.91,647.03,207.36,8.64;7,82.91,657.82,209.01,8.81;7,82.91,668.77,209.02,8.58;7,82.91,679.73,207.36,8.81;7,82.56,690.86,47.32,8.64" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,82.91,647.03,207.36,8.64;7,82.91,657.98,151.83,8.64">From neural sentence summarization to headline generation: A coarse-to-fine approach</title>
		<author>
			<persName coords=""><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/574</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,255.86,657.82,36.06,8.58;7,82.91,668.77,209.02,8.58;7,82.91,679.73,175.74,8.58">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4109" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,712.64,218.27,8.64;7,82.91,723.60,207.36,8.64;7,82.91,734.56,207.36,8.64;7,82.66,745.35,209.26,8.81;7,82.91,756.31,139.73,8.81" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,229.71,734.56,60.56,8.64;7,82.66,745.52,35.63,8.64">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,141.78,745.35,150.14,8.58;7,82.91,756.31,60.63,8.58">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,307.28,67.28,218.27,8.64;7,318.19,78.24,209.02,8.64;7,318.19,89.20,209.02,8.64;7,318.19,100.16,208.61,8.64;7,318.19,111.12,208.61,8.64;7,317.88,122.08,208.91,8.64;7,318.19,133.04,209.01,8.64;7,318.19,143.99,209.02,8.64;7,318.19,154.78,209.01,8.81;7,318.19,165.74,209.02,8.58;7,317.94,176.70,209.26,8.81;7,318.19,187.83,152.21,8.64" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,501.55,133.04,25.65,8.64;7,318.19,143.99,209.02,8.64;7,318.19,154.95,11.42,8.64">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,349.08,154.78,178.12,8.58;7,318.19,165.74,209.02,8.58;7,317.94,176.70,91.79,8.58">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
