<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.48,135.10,389.06,12.58">The University of Maryland at the TREC 2020 Podcasts Track</title>
				<funder ref="#_gGQeKqa">
					<orgName type="full">Intelligence Advanced Research Projects Activity</orgName>
					<orgName type="abbreviated">IARPA</orgName>
				</funder>
				<funder>
					<orgName type="full">Office of the Director of National Intelligence</orgName>
					<orgName type="abbreviated">ODNI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,154.02,165.62,95.58,10.48"><forename type="first">Petra</forename><surname>Galuščáková</surname></persName>
							<email>galuscakova@gmail.com</email>
						</author>
						<author>
							<persName coords="1,261.48,165.62,53.98,10.48"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.70,165.62,89.96,10.48"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Information Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.48,135.10,389.06,12.58">The University of Maryland at the TREC 2020 Podcasts Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F16AFA55FCA81C69413546425F9C51FD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Maryland submitted five runs to Task 1 of the TREC 2020 podcasts track. The best results, achieved by combining seven system variants and then re-ranking with using combinations of two neural models, achieved a 27% improvement in NDCG over a simple Indri baseline in the official evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Podcasts track <ref type="bibr" coords="1,190.09,395.77,10.51,8.74" target="#b4">[5]</ref> continues the tradition of speech retrieval tracks that started with the Spoken Document Retrieval track and continued with the TREC Video Track (which now continues as TRECVID). We submitted five runs for the Ad Hoc Segment Retrieval Task, focusing on three research questions:</p><p>• Q1 ) Can neural re-ranking improve over ranking based on lexical evidence?</p><p>• Q2 ) Would the combination of multiple re-rankers be more effective than a single re-rankers?</p><p>• Q3 ) When combining re-rankers, is it more effective to re-rank before or after system combination? All of our experiments were based on Automatic Speech Recognition (ASR) transcripts that were provided by the organizers; we made no direct use of the audio. We are making the code used to generate our runs freely available. <ref type="foot" coords="1,139.55,525.71,3.97,6.12" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Test Collection</head><p>The main focus of the TREC 2020 Podcasts Track<ref type="foot" coords="1,287.90,580.47,3.97,6.12" target="#foot_1">2</ref> is being able to effectively find information in a relatively large collection of podcasts. The access to the information was studied in two tasks: Ad-hoc Segment Retrieval (Task 1 ) and Summarization (Task 2 ). We participated only in Task 1, results for which we describe in this paper. The focus of this task was on finding the relevant 2 minutes long segments of the podcasts, which can be sometimes several hours long. The attention thus needs to be paid to both -the quality of the returned information and the starting point provided to the user.</p><p>The topics, documents, and relevance judgments used in the track were all provided by Spotify <ref type="bibr" coords="2,505.14,75.16,9.96,8.74" target="#b2">[3]</ref>. The Train set contained 8 training topics for which a total of 609 segments had been judged for relevance to some topic, 217 of which were judged as relevant to their associated topic to some degree. The test set contained 50 topics, with no relevance judgments. There were three types of topics: topical, refinding and known item. An example of each topic type is shown in the Table <ref type="table" coords="2,314.65,122.98,3.87,8.74" target="#tab_0">1</ref>. The majority of the topics in the Test set (35) were topical, 8 were refinding topics, 7 were known-item topics. The Train set contained 6 topical topics, one refinding topic and one known-item topic. Each topic consists of a title that can be interpreted as a Web-like query and a more expressive description of the information need, similar to what might be said in conversation. Graded relevance judgments, ranging from 0 to 4, were provided for every allowable replay start point (which for Task 1 was defined as each full minute from the beginning of the podcast). The highest relevance grade (4) could be only assigned to a known-item or refinding topic; it was defined to indicate the 'segment that is the earliest entry point into the one episode that the user is seeking'. The highest relevance grade for a topical topic (3) was defined to indicate that 'the segment conveys highly relevant information, is an ideal entry point for a human listener, and is fully on topic'. Judged segments could start at any full minute, and were two minutes long (or possibly shorter, for the final segment of a podcast).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>The same set of podcasts was searched for Train and Test. It consisted of 105,360 podcast recordings totalling over 47,000 hours of audio, and it is thus one of the largest available speech retrieval test collections. One-best transcripts were provided for the full collection by Spotify. These transcripts include timing information for each word, automatically detected speaker turns, and an automated estimate of the transcription quality for each utterance. Of these, we used only the transcribed words in our experiments. Additionally, metadata recording the title and description of the podcast and the title and description of the episode was also provided. We did use this additional metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We submitted five runs for the Ad-hoc Segment Retrieval Task (Task 1). For all the runs, the podcasts were first split into 2 minute segments<ref type="foot" coords="2,217.02,600.69,3.97,6.12" target="#foot_2">3</ref> , starting a new segment each minute. The resulting index includes only these pre-processed segments. Standard Indri normalization, which for example lowercases all characters, was used, but no other text normalization was done.</p><p>Sequential Dependence:<ref type="foot" coords="2,191.61,642.53,3.97,6.12" target="#foot_3">4</ref> As our baseline system, we used Sequential Dependence (SD) model <ref type="bibr" coords="2,503.77,644.10,9.96,8.74" target="#b1">[2]</ref>, with stemming, using the concatenation of the topic's title and description fields as the query. The Indri <ref type="bibr" coords="2,524.50,656.06,15.50,8.74" target="#b11">[12]</ref> query is created as follows:</p><p>#weight(0.8 q 0.1 #2(q) 0.1 #uw8(q)) (</p><p>where q represents the concatenation of topic title and description fields. This query represents a weighted combination of</p><p>• individual query terms with weight 0.8</p><p>• query terms within an ordered window of size 2 (i.e. bigrams) (#2 ) with weight 0.1</p><p>• query terms within an unordered window of size 8 (#uw8 ) with weight 0.1</p><p>As Table <ref type="table" coords="3,115.78,192.66,4.98,8.74" target="#tab_1">2</ref> shows, this model, marked as stemmed LM + SD, achieves the best NDCG on the Train set among the lexical models that we tried.</p><p>SD Re-Ranked by T5: <ref type="foot" coords="3,184.09,220.97,3.97,6.12" target="#foot_4">5</ref> In order to answer Q1, we apply a transformer-based model to re-rank the top 1000 results from the SD model. For this purpose, we use the base version of T5 model <ref type="bibr" coords="3,476.42,234.50,15.50,8.74" target="#b10">[11]</ref> fine-tuned on the MS MARCO <ref type="bibr" coords="3,165.40,246.46,10.52,8.74" target="#b0">[1]</ref> passage retrieval collection. We selected this model because we believed that the passage retrieval task to be similar to the podcast retrieval task in which also the relevant passage needs to be retrieved. We used the publicly released fine-tuned model provided by the University of Waterloo<ref type="foot" coords="3,511.18,268.79,3.97,6.12" target="#foot_5">6</ref> . The query used for the SD stage before re-ranking used the concatenated title and description fields. Following the setup of Nogueira et al. <ref type="bibr" coords="3,194.96,294.28,9.96,8.74" target="#b8">[9]</ref>, we construct the input sequence to the T5 model as follows:</p><p>Query: q Document: d Relevant:</p><p>Here, q stands for the description field of the topic and d represents the 2 minute text segment of the podcast. Given this input sequence, the fine-tuned model on MS MARCO is trained to produce token 'true' following the token 'Relevant:' if the document d is relevant to query q's topic otherwise it produces token 'false'. We use the softmaxed score of the token 'true' as the score for the document d for query q. We follow the same setup for T5 model in the subsequent runs.</p><p>Combine Re-Rank Combine: <ref type="foot" coords="3,219.15,399.60,3.97,6.12" target="#foot_6">7</ref> The small size of the Train set results in weak estimates of relative preference among systems, so we elected to try system combination in an effort to create a run that would work well on Test. We combined the following systems:</p><p>• Retrieval of the unstemmed content using Indri language model (unstemmed LM ). The query was created by placing all words from the topic title and description fields in the Indri <ref type="bibr" coords="3,476.68,457.60,15.50,8.74" target="#b11">[12]</ref> #combine operator.</p><p>• Retrieval of the unstemmed content using Indri language model with Word2vec <ref type="bibr" coords="3,449.34,488.95,10.52,8.74" target="#b6">[7]</ref> expansion applied to the topic title (unstemmed LM + word2vec QE ). The centroid was created from the words used in the topic title and 10 terms closest to this centroid in the embeddings space were concatenated with the topic. The query was created by placing all words from the topic title and the 10 expansion terms in the Indri #combine operator.</p><p>• Retrieval of the unstemmed content using TFIDF relevance model (unstemmed TFIDF ). Terms from topic title and description fields were used and no Indri operator was used in this case.</p><p>• Retrieval of the unstemmed content using Indri language model, but with segments of 5 minutes used as the indexed passages (unstemmed LM 5 min). As these segments are longer than those required in the task, we first locate the middle point in the segment and then find the closest possible starting point of the segment occurring after it in the recording. This means that the returned starting point is typically exactly in the third minute after the beginning of the 5-minute long passage. The query was created by placing all words from the topic title and description fields in the Indri #combine operator.</p><p>• Retrieval of the stemmed content using Indri language model with Sequential Dependence Model applied to all words from title and description (stemmed LM + SD). The Indri query is created as described in Eqn. 1 using all the words from the topic title and description fields.</p><p>• Retrieval of the stemmed content using Indri language model with stopwords removal applied (stemmed weighted LM + stopwords). Indri #weight operator is used with the words from the title having a weight of 1 and the words from the description having a weight of 0.5 .</p><p>• Retrieval of the stemmed content using Indri language model (stemmed LM + metadata). All passages in the collections are expanded with the metadata description of the podcast. The transcribed passage in the text form is concatenated with the the additional information consisting of the show name, show description, publisher, episode name and episode description. The query was created by placing all words from the topic title, and description fields in the Indri #combine operator.</p><p>Results for these individual systems on the Train set are shown in Table <ref type="table" coords="4,412.14,232.57,3.87,8.74" target="#tab_1">2</ref>. Different systems leverage different evidence, so we expect the combination of these systems to outperform any single system. We balanced the number of stemmed and unstemmed systems with an eye toward improving robustness. We first combine these systems using the reciprocal rank combination from TREC tools <ref type="bibr" coords="4,441.25,268.43,14.61,8.74" target="#b9">[10]</ref>. This combination is then re-ranked using two types of transformer models: BERT-Large <ref type="bibr" coords="4,375.45,280.39,10.52,8.74" target="#b3">[4]</ref> and T5-Base. Both of these models are publicly released and are fine-tuned on the MS MARCO passage retrieval task. 89 To further improve robustness, the T5 model is re-ranked separately using only the description or only the title field of the topic, while the BERT model is re-ranked using the concatenation of title and description topic fields, which was the best performing setup on the Train set. Finally, all the three re-ranked outputs are combined together with the original (unre-ranked) system combination, again using reciprocal rank combination. This process is illustrated in Figure <ref type="figure" coords="4,172.76,352.12,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical Run</head><p>No Re-Rank Combine: <ref type="foot" coords="4,168.04,542.85,7.94,6.12" target="#foot_9">10</ref> In this run, we used the same seven lexical systems as those used in the previous run. However, each of these systems was first re-ranked using one of the two re-ranking models (T5-Base or BERT-Large). Again to increase the diversity, some of the models we re-ranked using only the topic title, some using only the description and some using both. The precise setups were selected based on the performance of different setups with the respect to diversity of the used systems:</p><p>• unstemmed LM re-ranked with T5-Base using only description field. The input sequence is constructed as described in Eqn 2 with q as the description field and d as the 2 minute podcast text segment. • unstemmed LM + word2vec QE re-ranked with BERT-Large using only title field. The input sequence to the BERT-Large model is as follows:</p><formula xml:id="formula_2" coords="5,269.54,350.50,270.46,8.74">[CLS] q [SEP] d [SEP]<label>(3)</label></formula><p>where q stands for the title field and d represents the 2 minute podcast text segment. We follow the same setup to construct the input sequence for BERT-Large model in the subsequent systems.</p><p>• unstemmed TFIDF model re-ranked with BERT-Large using only description field. The input sequence is constructed as described in Eqn 3 with q as the description field and d as the 2 minute podcast text segment.</p><p>• unstemmed LM 5 min re-ranked with BERT-Large using concatenation of title and description fields. The input sequence is constructed as described in Eqn 3 with q as the concatenation of title and description field and d as the 5 minute podcast text segment.</p><p>• stemmed LM + SD re-ranked with T5-Base using only description field. The input sequence is constructed as described in Eqn 2 with q as the description field and d as the 2 minute podcast text segment.</p><p>• stemmed weighted LM + stopwords re-ranked with BERT-Large using concatenation of title and description fields. The input sequence is constructed as described in Eqn 3 with q as the concatenation of the title and description fields and d as the 2 minute podcast text segment.</p><p>• stemmed LM + metadata re-ranked with T5-Base using only title field. The input sequence is constructed as described in Eqn 2 with q as the title field and d as the 2 minute podcast text segment.</p><p>Performance of these systems on the training set is displayed in Table <ref type="table" coords="5,404.92,613.51,4.98,8.74" target="#tab_1">2</ref> and it is possible to directly compare these system with their non-ranked variants. The re-ranked variants are then combined together using the reciprocal ranks-based combination. This system is thus complimentary to the previous Run 3. While in Run 3 we first created a single strong baseline by combining diverse lexical approaches and then we re-ranked that, in this run we reversed the procedure, first applied the re-ranking and then combined the re-ranked runs. Comparison of the Run 3 and Run 4 thus should help us to answer the Q3. Three-Stage Combination: <ref type="foot" coords="6,203.75,323.89,7.94,6.12" target="#foot_10">11</ref> Finally, we tried combining all four of our other submitted runs to produce a combination of combinations, one of which itself was a combination of combinations. If these comblex combinations were sufficiently diverse, then this third level of system combination might yield further benefits. Rather than using RR combination, in this case we first normalized the scores produced by each of the four systems using sum-to-one normalization and then combined the results using CombMNZ <ref type="bibr" coords="6,462.98,373.28,14.61,8.74" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The official Test results provided by the track organizers are shown in Table <ref type="table" coords="6,408.81,428.05,4.98,8.74" target="#tab_2">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The results in Table <ref type="table" coords="7,163.19,413.76,4.98,8.74" target="#tab_2">3</ref> help to answer the research questions stated in the introduction:</p><p>• Q1 ) The re-ranking of the results of the lexical retrieval using transformer model had been recently shown to be helpful on a range of tasks <ref type="bibr" coords="7,272.00,444.18,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="7,285.88,444.18,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="7,296.97,444.18,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="7,313.06,444.18,11.62,8.74" target="#b14">15]</ref>. We also see that a neural re-ranking model (reranking by T5) runs outperforms the strong lexical SD model by both NDCG and P10. The biggest difference between these runs is for the 'thrift store smell' topic (topic 49) in which the re-ranked run also performs better than the SD run. However, the differences between the runs are not statistically significant. The performance of the re-ranked system is also consistently better on the small Train set, as shown in Table <ref type="table" coords="7,178.38,503.96,3.87,8.74" target="#tab_1">2</ref>.</p><p>• Q2 ) All combination models which also use re-ranking (Combine Re-Rank Combine, Re-Rank Combine and Three-Stage Combination) are better than the single re-ranking model (SD Re-Ranked by T5). Differences in the NDCG and MAP scores are statistically significant.</p><p>• Q3 ) Our setup in which we first create a strong baseline using a combination of the lexical approaches performs better than the setup in which we first apply the re-ranking on each run and only then combine them. Though these setups are not directly comparable, our results might support the belief that the strong baseline is needed for the re-ranking to perform well.</p><p>Surprisingly, the results on the training set and test set exhibit the same trends, even though there are a very small number of topics available in the training set. Also in contrast to our initial expectations, the Three-Stage Combination run did not yield any further improvement over these combinations.</p><p>Comparing the NDCG of our best performing Combine Re-Rank Combine run (Run 3) with the median NDCG of all the runs submitted to the task 12 , our system performs worse than the median values only for 3 topics. We achieved the best reported NDGC results (i.e., the NDCG score of our run is the maximum achieved score over all rund) for 11 topics. As there are no relevance judgements for two topics, this is almost a quarter of the available topics. This trend is visible in the Figure <ref type="figure" coords="8,372.72,99.07,3.87,8.74" target="#fig_2">3</ref>, which compares the performance of the Combine Re-Rank Combine and the median and maximal achieved per-topic scores. The scores in the figure are sorted according to the median scores, with the 'easier' (higher-median) topics on the left and the 'harder' (lower-median) topics on the right. The improvement of our best run over the median seems to increase somewhat with topic difficulty, which makes sense since these are absolute improvements over the median, and larger absolute improvements are possible for topics with lower median NDCG values. Where our run achieved the maximum reported NDCG, that happened more often on easier and mid-range topics than on hard topics. The biggest improvement over the median was achieved on the 'fyre festival' topic (topic 42). The biggest shortfall of our system when compared to the maximum occurred for the 'horoscope reading cancer' topic (topic 31). Our largest underperformance of the median was for the 'missouri quilt mom' topic (topic 46). For each of our runs, the NDCG on topical topics is much better than our NDCG for the other two categories (refinding and known item topics); those differences are smallest, however, for our Combine Re-Rank Combine run. In the case of the refinding and known item topics we also see larger differences between our best tun and the maximum NDCG for that topic. We thus plan to further explore specifically refinding and known item topics in our following work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this first year of the TREC Podcasts Track our focus was on achieving robust results using system combination and neural re-ranking. Our results are promising, achieving a statistically significant 27% relative improvement in NDCG and a 16% relative improvement in precision at 10 documents over a strong lexical matching baseline built using Indri language model and Sequential Dependence model. We look forward to the discussions at TREC, and to the second year of the track!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,167.42,284.25,277.17,8.74;5,72.00,72.00,468.01,197.14"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Combine Re-Rank Combine system diagram (Run 3).</figDesc><graphic coords="5,72.00,72.00,468.01,197.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,188.29,293.08,235.41,8.74;6,72.00,72.00,468.01,205.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Re-Rank Combine system diagram (Run 4).</figDesc><graphic coords="6,72.00,72.00,468.01,205.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,72.00,350.36,468.00,8.74;7,72.00,362.32,430.27,8.74;7,79.05,75.22,453.90,256.82"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Improvement of Combine Re-Rank Combine run (Run 3) over Median (outline above that shows further improvement of Max over Run 3). Topics are sorted in order of decreasing Median NDCG.</figDesc><graphic coords="7,79.05,75.22,453.90,256.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,103.54,194.33,425.02,169.22"><head>Table 1 :</head><label>1</label><figDesc>Examples of different topic types from the Train set.</figDesc><table coords="2,103.54,194.33,425.02,145.25"><row><cell></cell><cell>Title</cell><cell>Description</cell></row><row><cell>Topical</cell><cell>black hole image</cell><cell>In May 2019 astronomers released the first-ever picture</cell></row><row><cell></cell><cell></cell><cell>of a black hole. I would like to hear some conversa-</cell></row><row><cell></cell><cell></cell><cell>tions and educational discussion about the science of</cell></row><row><cell></cell><cell></cell><cell>astronomy, black holes, and of the picture itself.</cell></row><row><cell>Refinding</cell><cell>story about riding a</cell><cell>I remember hearing a podcast that had a story about</cell></row><row><cell></cell><cell>bird</cell><cell>a kid riding some kind of bird. I want to find it again.</cell></row><row><cell>Known-item</cell><cell>daniel ek interview</cell><cell>Someone told me about a podcast interview with</cell></row><row><cell></cell><cell></cell><cell>Daniel Ek, CEO of Spotify, about the founding and</cell></row><row><cell></cell><cell></cell><cell>early days of Spotify. I would like to find the show</cell></row><row><cell></cell><cell></cell><cell>and episode that contains that interview. Other inter-</cell></row><row><cell></cell><cell></cell><cell>views with Ek are relevant as well.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,377.97,468.00,145.31"><head>Table 2 :</head><label>2</label><figDesc>NDCG scores of the lexical approaches used in the Run3 and Run4. The re-ranked scores correspond to the re-ranking approach and setup used in the Run4. The highest scores with and without re-ranking are highlighted.</figDesc><table coords="4,347.90,377.97,101.02,8.74"><row><cell>re-ranking Re-ranked</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,428.05,468.00,183.11"><head>Table 3 :</head><label>3</label><figDesc>together with corresponding results computed locally on the Train set. Results for submitted runs. Test results are the official Podcast Track results, Train results are the corresponding scores obtained locally on the Train set. The highest score for each collection and measure are highlighted. The significance test results are calculated using paired t-test at p &lt; 0.05, means that the run is better than SD, * than SD Re-Ranked by T5, • than Re-Rank Combine or • than Three Stage Combination.</figDesc><table coords="6,87.34,465.85,437.32,85.47"><row><cell>Run</cell><cell></cell><cell>Test</cell><cell></cell><cell></cell><cell>Train</cell></row><row><cell></cell><cell>NDCG</cell><cell>MAP</cell><cell>P10</cell><cell>NDCG</cell><cell>MAP</cell><cell>P10</cell></row><row><cell cols="2">Sequential Dependence (SD) 0.5271</cell><cell>0.3665</cell><cell>0.5125</cell><cell>0.4657</cell><cell>0.2087</cell><cell>0.3125</cell></row><row><cell>SD Re-Ranked by T5 Combine Re-Rank Combine Re-Rank Combine Three-Stage Combination</cell><cell cols="6">0.6182 0.6682  *  • 0.4624  *  •• 0.5958 • 0.6282 0.3363 0.4500 0.3524 0.5271 0.5797 0.2617 0.3875 0.6563  *  0.4283  *  0.5583 0.5954 0.2932 0.4375 0.6467  *  0.4342  *  0.5833 0.5864 0.3030 0.4250</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,661.33,165.13,6.64"><p>https://github.com/galuscakova/podcasts</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,87.24,670.83,156.66,6.64"><p>https://podcastsdataset.byspotify.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,87.24,675.16,355.58,6.99"><p>Except the unstemmed LM 5 min setup which is a part of the Combine Re-Rank Combine run.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,87.24,684.67,216.84,6.99"><p>This run is referred to as Run 2 in the official task results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,87.24,665.66,216.84,6.99"><p>This run is referred to as Run 1 in the official task results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,87.24,675.74,221.18,6.64"><p>https://huggingface.co/castorini/monot5-base-msmarco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="3,87.24,684.67,216.84,6.99"><p>This run is referred to as Run 3 in the official task results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="4,87.24,645.63,221.18,6.64"><p>https://huggingface.co/castorini/monot5-base-msmarco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="4,87.24,655.13,233.88,6.64"><p>https://huggingface.co/castorini/monobert-large-msmarco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="4,87.24,664.06,216.84,6.99"><p>This run is referred to as Run 4 in the official task results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="6,87.24,643.27,216.84,6.99"><p>This run is referred to as Run 5 in the official task results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11" coords="7,87.24,684.67,353.60,6.99"><p>Minimum, median and maximum NDCG values for each topic were provided by the organizers.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research has been supported in part by the <rs type="funder">Office of the Director of National Intelligence (ODNI)</rs>, <rs type="funder">Intelligence Advanced Research Projects Activity (IARPA)</rs>, via contract <rs type="grantNumber">FA8650-17-C-9117</rs>. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of <rs type="affiliation">ODNI</rs>, <rs type="institution">IARPA</rs>, or the <rs type="institution">U.S. Government</rs>. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gGQeKqa">
					<idno type="grant-number">FA8650-17-C-9117</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,92.48,538.34,447.52,8.74;8,92.48,550.29,447.52,8.74;8,92.48,562.25,152.92,8.74" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268v3</idno>
		<title level="m" coord="8,230.46,550.29,277.13,8.74">A Human Generated MAchine Reading COmprehension Dataset</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,92.48,582.17,447.52,8.74;8,92.48,594.13,447.52,8.74;8,92.48,606.08,174.23,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,286.26,582.17,253.73,8.74;8,92.48,594.13,24.22,8.74">Learning concept importance using a weighted dependence model</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,140.40,594.13,399.61,8.74;8,92.48,606.08,47.46,8.74">Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM &apos;10</title>
		<meeting>the Third ACM International Conference on Web Search and Data Mining, WSDM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,626.01,447.52,8.74;8,92.48,637.96,447.52,8.74;8,92.48,649.92,439.51,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,237.90,637.96,219.90,8.74">000 Podcasts: A Spoken English Document Corpus</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bonab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,478.86,637.96,61.14,8.74;8,92.48,649.92,328.25,8.74">Proceedings of the 28th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 28th International Conference on Computational Linguistics (COLING)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.48,669.84,447.52,8.74;8,92.48,681.80,297.16,8.74" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="8,341.30,669.84,198.71,8.74;8,92.48,681.80,116.96,8.74">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,92.48,75.16,447.52,8.74;9,92.48,87.11,447.52,8.74;9,92.48,99.07,208.26,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,148.31,87.11,167.40,8.74">TREC 2020 Podcasts Track Overview</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,340.24,87.11,199.76,8.74;9,92.48,99.07,64.26,8.74">The 29th Text Retrieval Conference (TREC) notebook. NIST</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,118.99,398.27,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,125.40,118.99,177.88,8.74">The neural hype, justified! A recantation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,324.48,118.99,84.68,8.74">ACM SIGIR Forum</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,138.92,447.52,8.74;9,92.48,150.87,447.52,8.74;9,92.48,162.83,388.03,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,380.00,138.92,160.00,8.74;9,92.48,150.87,165.91,8.74">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,211.06,162.83,239.12,8.74">Advances in Neural Information Processing Systems 26</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,182.75,427.44,8.74" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="9,206.37,182.75,131.93,8.74">Passage re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,92.48,202.68,447.52,8.74;9,92.48,214.64,171.14,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,249.70,202.68,285.46,8.74">Document ranking with a pretrained sequence-to-sequence model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06713</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,92.48,234.56,447.52,8.74;9,92.48,246.52,447.52,8.74;9,92.48,258.47,321.24,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,249.47,234.56,290.53,8.74;9,92.48,246.52,202.95,8.74">TrecTools: an open-source Python library for Information Retrieval practitioners involved in TREC-like campaigns</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,317.72,246.52,222.28,8.74;9,92.48,258.47,291.08,8.74">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,278.40,447.52,8.74;9,92.48,290.35,447.52,8.74;9,92.48,302.31,146.98,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,92.48,290.35,350.17,8.74">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,453.47,290.35,86.52,8.74;9,92.48,302.31,77.70,8.74">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,322.23,447.52,8.74;9,92.48,334.19,447.52,8.74;9,92.48,346.14,110.01,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,340.34,322.23,199.66,8.74;9,92.48,334.19,83.62,8.74">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,198.01,334.19,291.74,8.74">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis<address><addrLine>McLean, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,366.07,425.44,8.74" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,126.23,366.07,162.25,8.74">Data Fusion in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct coords="9,92.48,385.99,447.52,8.74;9,92.48,397.95,447.53,8.74;9,92.48,409.90,419.12,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,392.65,385.99,147.35,8.74;9,92.48,397.95,118.04,8.74">End-to-end open-domain question answering with BERTserini</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,232.84,397.95,295.31,8.74">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.48,429.83,447.52,8.74;9,92.48,441.78,143.24,8.74" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10972</idno>
		<title level="m" coord="9,242.03,429.83,263.51,8.74">Simple applications of BERT for ad hoc document retrieval</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
