<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,166.31,71.79,264.92,12.90">RMIT at TREC Deep Learning Track 2020</title>
				<funder ref="#_TQHdhsx">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.48,121.13,100.40,10.75"><forename type="first">J</forename><forename type="middle">Shane</forename><surname>Culpepper</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.31,121.13,68.11,10.75"><forename type="first">Binsheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RMIT University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,166.31,71.79,264.92,12.90">RMIT at TREC Deep Learning Track 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5130C78D864AF4408049619639D553CE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RMIT University submitted multiple baseline runs to improve the diversity of system types in the judgment pool for the TREC Deep Learning Track in 2020. All of these runs used the publicly available Terrier and Indri search engines and no machine learning. In addition, we submitted a single non-baseline run which applied a custom pairwise ranker to a Bart transformer to rerank passages for the passage ranking task. The RMIT Bart run as well as several of the more basic baseline runs performed well overall in the document and passage ranking tasks based on the preliminary assessment provided by NIST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Deep Learning Track in in its second year in 2020. The track used the MS-MARCO <ref type="bibr" coords="1,273.08,457.78,14.25,9.46;1,72.00,471.33,68.30,9.46" target="#b0">[Bajaj et al., 2016]</ref> document and passage reranking collection created by Microsoft just as it did in 2019. NIST provided 100 new queries to participants, who were asked to provide the top-100 documents and passages from their respective MS-MARCO collections. This year, the track organizers provided an additional training collection created at Microsoft, called ORCAS <ref type="bibr" coords="1,221.69,566.18,69.94,9.46;1,72.00,579.72,23.95,9.46" target="#b1">[Craswell et al., 2020]</ref>, which provides an additional 18 million training queries.</p><p>One of the key goals of the track is to explore the performance of Deep Learning models for information retrieval and to compare these new approaches with more traditional IR ranking algorithms. The results were promising in 2019, but relatively few submissions were received which did not use Deep Learning. So, in 2020, the organizers reached out to several IR research groups and asked them to submit additional baseline runs that used more traditional IR techniques such as query expansion, sequential dependency models, and feature-based learning-to-rank.</p><p>In response to the request, RMIT submitted 12 baseline runs to the document ranking track and 8 baseline runs to the passage ranking task. None of the runs use LTR as this remains difficult to do with the current collection as only plain text is available for passages and documents in the collection, and many of the most effective features require semistructured data such as HTML. We did however submit a single Deep Learning run using BART for the passage ranking task for comparison.</p><p>In the following sections we will describe the collection curation and processing, baseline configurations, and a few initial observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Collection Preparation</head><p>While the TREC organizers provide a version of the document collection in TRECTEXT format, we opted to perform our own pre-cleaning of the collection to remove various errors in the text provided by Microsoft. All document text was processed using python along with the clean-text[gpl], spacy, and lemminflect packages. The same transforms were applied to all documents, passages, and queries before constructing TRECTEXT files which were indexed using Indri or Terrier as described in the next section.</p><p>3 System Descriptions Document Ranking Runs. The 12 runs submitted for the document ranking task are described in Table <ref type="table" coords="1,334.17,647.47,4.17,9.46" target="#tab_1">1</ref>. We used two different publicly available search engines -Indri and Terrier. Note that we have a publicly available fork of Terrier which includes implementations of several stemmers which are not available in the official version. We also have made modifications to a few different ranking algorithms such as BM25 in order to allow us to set both k 1 and b parameters as we have found that the default values for these two parameters Terrier v5.1, Krovetz stemming, LemurTF IDF ranker with default parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>terrier-jskls</head><p>Terrier v5.1, Krovetz stemming, Js KLs ranker, DRF sequential dependency type SD with µ = 4000 and n-gram length = 5, KL query expansion with 1 documents and 10 terms. sures used in the final TREC assessments such as AP@100 and NDCG@10. Therefore, the best parameter configurations were drawn from tuning runs with the 2019 queries, or the default parameters were used as described in the table.</p><p>Passage Ranking Runs. The 8 runs submitted for the document ranking task are summarized in Table <ref type="table" coords="2,97.87,715.22,4.01,9.46" target="#tab_2">2</ref>. All of the baseline runs followed the same process as described for the document ranking runs. The one major exception was the RMIT-Bart which is the only non-standard baseline produced by RMIT in 2020. The details of this run a along with a basic analysis is provided in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RMIT Bart Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ranking Model</head><p>BART <ref type="bibr" coords="2,339.71,525.53,84.44,9.46" target="#b5">[Lewis et al., 2019</ref>] is a pretrained transformer model with an encoder-decoder architecture. We convert the BART model into a neural ranking model by adding a fine-tuning layer on top of the decoder layer. Figure <ref type="figure" coords="2,451.31,579.72,5.56,9.46" target="#fig_1">1</ref> shows how the model works. Below the last feed-forward layer is the pretrained BART model which takes the concatenation of a query and a document as input for both the encoders and the decoders and produces contextualized embeddings for every token. To get a relevance prediction, we project the embeddings of the last token into a scalar score as only the last token can attend the whole sentence. This is in contrast with typical BERT ranking models <ref type="bibr" coords="2,481.80,701.67,43.74,9.46;2,307.28,715.22,15.44,9.46">[Nogueira and</ref><ref type="bibr" coords="2,324.80,715.22,140.49,9.46">Cho, 2019, Nogueira et al., 2019]</ref> where the embeddings of the first token is used for fine-tuning.</p><p>The BartForSequenceClassification API from Huggingface is used to implement our ranker, but any   of the XXXForSequenceClassification APIs could be used to create a ranking model with few modifications, making it a very compelling tool for IR researchers to use in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We construct our own training data instead of using the tuples released by Microsoft. We first index the DeepCT <ref type="bibr" coords="3,113.06,742.32,103.72,9.46" target="#b2">[Dai and Callan, 2019]</ref> enriched collection and tune BM25 for recall using 1000 randomly selected training queries. Then we retrieve 1000 passages for each training query to use as our training data. The re-construction of the training data ensures that the passages in the training set, the validation set, and the testing set are consistent with our first-stage run.</p><p>Each training instance contains a query Q with one positive document D + and one pseudonegative document D -(there are no true negative judgments provided by the organizers). The loss for a training instance is calculated as</p><formula xml:id="formula_0" coords="3,312.12,495.58,208.57,12.06">Loss = max(0, 1 -(S(Q, D + ) -S(Q, D -)))</formula><p>(1) where S(•) is the score produced by the ranking model.</p><p>We train the model for 10 epochs and sample new negative documents in each epoch. We set the learning rate to 5e -6 with a 1 epoch warmup, a training batch size of 32, gradient accumulation is set to 4 batches, and the maximum sequence length is set to 256 tokens. We use the AdamW <ref type="bibr" coords="3,346.95,633.92,133.82,9.46" target="#b6">[Loshchilov and Hutter, 2019]</ref> optimizer with β 1 = 0.9, β 2 = 0.999, and weight decay = 5e -5 .</p><p>Using strong negative documents -which the model fails to rank lower than positive documents during training -to reinforce the model results when the effectiveness is low for the instance. We visually inspected a few strong negative documents and found that they could easily be labeled as positive if assessed by a human. This is not surprising as the MS-MARCO judgments are not guaranteed to be complete. So care should be taken when researchers use this data augmentation technique as it may simply be an over-optimization that is highly collection-specific. Nevertheless, if the goal is to improve effectiveness on this collection it appears to work quite well.   Table <ref type="table" coords="4,109.47,674.57,5.42,9.46" target="#tab_3">3</ref> summarizes the effectiveness results for the RMIT-BART run in 2020. Figure <ref type="figure" coords="4,236.17,688.12,5.56,9.46" target="#fig_3">2</ref> shows the RMIT-BART breakdown at the query level w.r.t. the worst, the median and the best results in terms of NDCG@10 and NDCG@1000 as provided by the organizers. It is clear RMIT-BART was more effectiveness than the median for the majority of  <ref type="bibr" coords="4,388.24,212.82,132.39,7.77;4,362.94,222.79,67.25,7.77;4,347.17,232.75,173.46,7.77;4,362.94,242.71,157.70,7.77;4,362.94,252.68,67.25,7.77">, 390360, 330975, 135802, 1131069, 1110678, 1103153 1 730539, 174463, 141630, 1136047, 1115210, 1113256, 1106979, 1064670, 1049519, 1043135</ref>  assessed topics this round, but there is still room for improvement in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>RMIT-BART performed the worst for topic 673670 "what is a alm" and 405163 "is caffeine an narcotic" (leftmost of Figure <ref type="figure" coords="4,510.47,418.28,8.50,9.46" target="#fig_3">2a</ref>). The rank position 1 documents for theses two topics appear to be relevant in our opinion, but were labeled as non-relevant by the assessors. For example, document 8632360 is "ALM is a set of pre-defined processes that start somewhere in the business as an idea, a need ...". The relevant documents actually have different definitions of the acronym "ALM" which is ambiguous.</p><p>So it is quite possible one might expect some level of assessor disagreement on the current judgments, but this has not been studied for the collection to our knowledge. Since the topics do not originate with the NIST assessors which is often the case in TREC exercises, the assessors would not be classified as "gold", which can lead to lower assessor agreement <ref type="bibr" coords="4,394.99,661.02,96.21,9.46" target="#b3">[Damessie et al., 2017</ref><ref type="bibr" coords="4,498.91,661.02,23.79,9.46" target="#b4">[Damessie et al., , 2018]]</ref>. This is always a possibility when assessing results with only a query and not a full description of the true information need. Relevance labels become more likely to interpreted differently when assessed independently by multiple assessors. This is a classic and lingering problem is IR evaluation worth exploring further in future work.</p><p>One interesting observation related to the TREC DL 2020 Task -there appear to be a significant number of duplicate and near duplicate queries in the training data available. Table <ref type="table" coords="5,227.72,127.24,5.56,9.46" target="#tab_5">4</ref> summarizes the duplicate or near duplicate queries contained in one of the training set. Of the 60 assessed topics, roughly half of them had one or more duplicate in the training sets. We intend to do a more detailed analysis of this phenomenon in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have described all of the runs submitted for the TREC DL 2020 reranking tasks in this report. Our baseline runs used basic Terrier and Indri configurations. In addition, we have performed a preliminary analysis of the RMIT-Bart run which applied a more common Deep Learning approach similar to many of the other participating teams. The RMIT Bart run as several of the more basic baseline runs performed well overall in the document and passage ranking tasks based on the preliminary assessment provided by NIST, which we will analyze further once the TREC conference concludes in November of 2020.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,104.03,69.12,46.96,9.46;3,181.71,69.12,50.89,9.46;3,104.03,88.86,35.00,8.29;3,181.71,88.14,265.44,9.46;3,104.03,102.41,37.00,8.29;3,181.71,101.69,293.91,9.46;3,104.03,115.96,38.99,8.29;3,181.71,114.89,225.24,9.81;3,104.03,129.51,49.95,8.29;3,181.71,128.79,311.81,9.46;3,181.71,141.99,80.24,10.63;3,104.03,156.61,41.49,8.29;3,181.71,155.89,311.81,9.46;3,181.71,169.44,32.91,9.46;3,104.03,183.70,43.98,8.29;3,181.71,182.99,311.81,9.46;3,181.71,196.54,32.91,9.46;3,104.25,210.09,389.26,9.46;3,181.31,223.29,139.38,9.81;3,104.25,237.90,53.40,8.32;3,181.71,236.84,312.08,9.81;3,181.71,250.38,109.08,9.81;3,104.25,265.55,43.85,7.77;3,181.71,264.28,313.62,9.46;3,181.71,277.83,31.52,9.46"><head></head><label></label><figDesc>Indri v5.17, Full Dependency Model with default parameters indri-sdm Indri v5.17, Sequential Dependency Model with default parameters indri-lmds Indri v5.17, Query Likelihood Model with µ = 460 terrier-BM25 Terrier v5.1, Krovetz stemming, Okapi BM25 ranker modified to use k 1 = 1.6, b = 0.7. terrier-dph Terrier v5.1, Krovetz stemming, DPH ranker. All parameters were the default. terrier-InL2 Terrier v5.1, Krovetz stemming, InL2 ranker. All parameters were the default. TF IDF d 2 t 50 Terrier v5.1, Krovetz stemming, TF IDF ranker, Bo1 query expansion with 2 documents and 50 terms. DLF d 5 t 25 Terrier v5.1, Krovetz stemming, DLF ranker, Bo1 query expansion with 5 documents and 25 terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,72.00,565.45,218.27,8.64;3,72.00,577.41,218.27,8.64;3,72.00,589.36,218.27,8.64;3,72.00,601.32,50.64,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RMIT-BART ranking model takes as input the concatenation of a query and a document and produces a scalar score based on the last token's contextualized embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,128.36,644.52,105.54,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Per-query result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,108.13,68.69,383.11,272.35"><head></head><label></label><figDesc>Terrier v5.1, Krovetz stemming, DPH ranker with default parameters. rterrier-dph sd Terrier v5.1, Krovetz stemming, DPH ranker, DRF sequential dependency type SD with µ = 4000 and n-gram length = 5. All other parameters were the default.</figDesc><table coords="2,108.13,68.69,383.11,191.05"><row><cell>Run Name</cell><cell>Description</cell></row><row><cell cols="2">RMIT DFRee Terrier v5.1, Krovetz stemming, DFRee ranker, DRF sequential depen-</cell></row><row><cell></cell><cell>dency type SD with µ = 4000 and n-gram length = 5, Bo1 query expan-</cell></row><row><cell></cell><cell>sion with 5 documents and 50 terms.</cell></row><row><cell>RMIT DPH</cell><cell>Terrier v5.1, Krovetz stemming, DPH ranker, DRF sequential dependency</cell></row><row><cell></cell><cell>type SD with µ = 4000 and n-gram length = 5, Bo1 query expansion</cell></row><row><cell></cell><cell>with 2 documents and 50 terms.</cell></row><row><cell>indri-sdmf</cell><cell>Indri v5.17, Sequential Dependencies with fields, title w = 0.35, url</cell></row><row><cell></cell><cell>w = 0.35, body unigram w = 0.99, coocurrence w = 0.1, unordered</cell></row><row><cell></cell><cell>window of size 8 and w = 0.2</cell></row><row><cell>rindri-bm25</cell><cell>Indri v5.17, Krovetz stemming, Okapi ranker, k 1 = 1.6, b = 0.7</cell></row><row><cell>rmit indri-fdm</cell><cell>Indri v5.17, Full Dependency Model with default parameters</cell></row><row><cell>rmit indri-sdm</cell><cell>Indri v5.17, Sequential Dependency Model with default parameters</cell></row><row><cell>rterrier-dph</cell><cell></cell></row></table><note coords="2,108.13,304.58,337.51,9.81;2,108.13,319.20,44.33,8.29;2,177.61,318.48,311.81,9.46;2,108.13,332.75,49.32,8.29"><p><p><p><p><p>rterrier-expC2</p>Terrier v5.1, Krovetz stemming, In expC2 ranker with c = 2.</p>rterrier-tfidf</p>Terrier v5.1, Krovetz stemming, TF IDF ranker with default parameters.</p>rterrier-tfidf2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,71.49,416.97,339.77,178.81"><head>Table 1 :</head><label>1</label><figDesc>System Descriptions for document ranking runs.</figDesc><table /><note coords="2,72.00,450.83,220.07,9.46;2,72.00,464.37,218.27,9.46;2,72.00,477.92,220.18,9.46;2,71.49,491.47,220.60,9.46;2,72.00,505.02,220.07,9.46;2,72.00,518.57,218.27,9.46;2,72.00,532.12,218.27,9.46;2,72.00,545.67,218.27,9.46;2,72.00,559.22,218.27,9.46;2,72.00,572.77,218.27,9.46;2,72.00,586.32,220.07,9.46"><p>are rarely the best choice. All of the modifications can be found in a GitHub repository located at https://github.com/jsc/terrier-5.1.1.1. We performed a very basic parameter sweep of various system configurations using the QREL judgments from the 2019 track queries as the judgments are too shallow to perform parameter tuning using the MS MARCO test set, which contain a single judgment per query. The original MS MARCO test queries work reasonably well if you only wish to tune for reciprocal rank, but not for deeper mea-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,86.14,308.58,320.96,236.05"><head>Table 2 :</head><label>2</label><figDesc>System Descriptions for passage ranking runs.</figDesc><table coords="3,86.14,343.80,193.30,200.83"><row><cell></cell><cell></cell><cell>Score</cell><cell></cell></row><row><cell></cell><cell cols="2">Feed Foward</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Decoders</cell></row><row><cell></cell><cell>…</cell><cell></cell><cell>…</cell></row><row><cell></cell><cell></cell><cell cols="2">Feed Forward</cell></row><row><cell></cell><cell>Forward</cell><cell cols="2">Cross-aention</cell></row><row><cell cols="2">Self-aention</cell><cell cols="2">Self-aention</cell></row><row><cell>query</cell><cell>document</cell><cell>query</cell><cell>document</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,77.94,200.92,199.12,282.59"><head>Table 3 :</head><label>3</label><figDesc>RMIT-BART effectiveness.</figDesc><table coords="4,77.94,227.26,199.12,256.24"><row><cell></cell><cell>Metric</cell><cell>Value</cell></row><row><cell></cell><cell>NDCG@10</cell><cell>0.754</cell></row><row><cell></cell><cell>NDCG@20</cell><cell>0.721</cell></row><row><cell></cell><cell cols="2">NDCG@1000 0.719</cell></row><row><cell></cell><cell>MAP</cell><cell>0.512</cell></row><row><cell></cell><cell>Recall</cell><cell>0.809</cell></row><row><cell></cell><cell>RBP(0.50)</cell><cell>0.722 +0.000</cell></row><row><cell></cell><cell>RBP(0.80)</cell><cell>0.621 +0.018</cell></row><row><cell></cell><cell>RBP(0.95)</cell><cell>0.375 +0.186</cell></row><row><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>ndcg@10</cell><cell>0.4 0.6</cell><cell>RMIT-BART Worst Median Best</cell></row><row><cell></cell><cell>0.2</cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Qid</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,306.97,275.78,218.58,20.91"><head>Table 4 :</head><label>4</label><figDesc>The 33 assessed topics which had one or more duplicate / near-duplicate query in the training sets.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by <rs type="funder">Australian Research Council</rs> Grant <rs type="grantNumber">DP190101113</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TQHdhsx">
					<idno type="grant-number">DP190101113</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,72.00,509.53,219.52,8.64;5,82.55,520.49,208.97,8.64;5,317.88,67.28,209.32,8.64;5,317.83,78.24,209.37,8.64;5,318.19,89.02,207.36,8.82;5,318.19,99.98,134.95,8.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,401.79,78.24,125.41,8.64;5,318.19,89.20,177.68,8.64">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,307.28,120.08,218.27,8.64;5,318.19,131.04,209.02,8.64;5,318.19,141.82,207.36,8.82;5,318.19,152.78,100.17,8.82" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,382.73,131.04,144.47,8.64;5,318.19,142.00,142.88,8.64">ORCAS: 18 million clicked querydocument pairs for analyzing search</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Billerbeck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05324</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,307.28,172.89,218.27,8.64;5,317.88,183.84,209.32,8.64;5,318.19,194.62,171.09,8.82" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10687</idno>
		<title level="m" coord="5,392.75,172.89,132.80,8.64;5,317.88,183.84,209.32,8.64;5,318.19,194.80,24.02,8.64">Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval</title>
		<imprint>
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,214.73,220.01,8.64;5,318.19,225.69,209.02,8.64;5,318.19,236.47,208.60,8.82;5,318.19,247.61,97.40,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,365.90,225.69,161.31,8.64;5,318.19,236.65,134.44,8.64">Gauging the quality of relevance assessments using inter-rater agreement</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T</forename><surname>Damessie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">P</forename><surname>Nghiem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,474.71,236.47,47.51,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1089" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,267.53,220.01,8.64;5,318.19,278.49,209.10,8.64;5,318.19,289.27,152.02,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,318.19,278.49,204.71,8.64">Presentation ordering effects on assessor agreement</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T</forename><surname>Damessie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,328.97,289.27,45.82,8.59">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,307.22,219.52,8.64;5,317.83,318.18,209.37,8.64;5,318.19,329.13,207.36,8.64;5,318.19,340.09,208.61,8.64;5,317.88,350.87,208.17,8.82;5,318.19,361.83,82.19,8.82" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<title level="m" coord="5,389.17,329.13,136.38,8.64;5,318.19,340.09,208.61,8.64;5,317.88,351.05,127.05,8.64">Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct coords="5,307.28,381.94,218.62,8.64;5,318.19,392.72,209.10,8.82;5,318.19,403.85,22.42,8.64" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coord="5,423.03,381.94,102.86,8.64;5,318.19,392.90,56.93,8.64">Decoupled Weight Decay Regularization</title>
		<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct coords="5,307.28,423.78,218.27,8.64;5,318.19,434.56,164.32,8.82" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="5,423.06,423.78,102.49,8.64;5,318.19,434.74,21.89,8.64">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019-01">jan 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,307.28,454.66,218.27,8.64;5,318.19,465.44,207.36,8.82;5,318.19,476.40,58.11,8.82" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14424</idno>
		<title level="m" coord="5,477.95,454.66,47.60,8.64;5,318.19,465.62,123.88,8.64">Multi-Stage Document Ranking with BERT</title>
		<imprint>
			<date type="published" when="2019-10">oct 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
