<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,109.93,71.79,377.18,12.90">Automatic Summarization of Open-Domain Podcast Episodes</title>
				<funder ref="#_CSQ8ykU">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,131.02,106.18,72.75,10.75"><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
							<email>kqsong@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Central Florida ♦ Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.49,106.18,39.75,10.75"><forename type="first">Chen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Central Florida ♦ Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.07,106.18,77.69,10.75"><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Central Florida ♦ Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.40,106.18,41.67,10.75"><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Central Florida ♦ Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.86,106.18,36.57,10.75"><forename type="first">Fei</forename><surname>Liu</surname></persName>
							<email>feiliu@cs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Central Florida ♦ Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,109.93,71.79,377.18,12.90">Automatic Summarization of Open-Domain Podcast Episodes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6B9C91D009D18993A22B873A388C3F80</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present implementation details of our abstractive summarizers that achieve competitive results on the Podcast Summarization task of TREC 2020. A concise textual summary that captures important information is crucial for users to decide whether to listen to the podcast. Prior work focuses primarily on learning contextualized representations. Instead, we investigate several less-studied aspects of neural abstractive summarization, including (i) the importance of selecting important segments from transcripts to serve as input to the summarizer; (ii) striking a balance between the amount and quality of training instances; (iii) the appropriate summary length and start/end points. We highlight the design considerations behind our system and offer key insights into the strengths and weaknesses of neural abstractive systems. Our results suggest that identifying important segments from transcripts to use as input to an abstractive summarizer is advantageous for summarizing long documents. Our best system achieves a quality rating of 1.559 judged by NIST evaluators-an absolute increase of 0.268 (+21%) over the creator descriptions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Podcast is a promising new medium for reaching a broad audience. A podcast series usually feature one or more recurring hosts engaged in a discussion about a particular topic. 1 New platforms developed by Spotify, Apple, Google and Pandora encompass a wide variety of topics, ranging from talk shows to true crime and investigative journalism. Our data are provided by Spotify <ref type="bibr" coords="1,181.15,672.10,83.22,9.46" target="#b0">(Jones et al., 2020)</ref>, containing 100,000 podcast episodes comprised of raw audio files, their transcripts and metadata. The transcription is provided by Google Cloud Platform's Speech-to-Text API. 2 1 https://en.wikipedia.org/wiki/Podcast 2 https://cloud.google.com/speech-to-text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segments of a Podcast Transcript</head><p>What's good? Everybody is of all trades here with the game Illuminati. Hope you guys are having a great day. So far. If you guys didn't get a chance to check out our last video. Be sure to check out the link down in the description box below. We are back for another Triple Threat Sports podcast. Now before we get into the Super Bowl edition of Triple Threat Sports podcast. I got to introduce you guys to my co-host first co-host. Say what up C ewan's what up next you guys know him as UT X JG to dine, but he's also known as the ... The LA Rams supporter and he's going to the Superbowl. Say what up, GG don't it? Feel so good though. It feels so good. The lone person the triple threat Sports podcast by team is going to the shelf and people are mad and we gonna talk about it man. We got we definitely going to talk about it for sure. So at the time of this recording we've already gone through the Pro Bowl which was yesterday. I'm sure some of you guys watched it and you know whoopty whoopty Doo I've ... Interest in the Pro Bowl after like I turned I 15 but AFC 126 to 7, but we're going to talk about these NFL Conference Championship games. You got to between the Rams and the Saints which whoo, boy, there's a lot of controversy behind that one and then the Chiefs and the Patriots before we get to the smoke and everything like that because jg's been handing them out all this week. Let's go ahead and start going into the NFL conference championships for the Patriots and the Chiefs now, I'll go with you ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Creator Description</head><p>The Guys are back for another Triple Threat Sports Podcast! This time UTXJGTHEDON is giving out all the smoke as his Los Angeles Rams is heading to the Super Bowl to face the New England Patriots.</p><p>-Support this podcast: https://anchor.fm/triplethreatsportspodcast</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Summary</head><p>In this episode of the Triple Threat Sports Podcast, JG and UTX discuss the NFL Conference Championship games between the Patriots and Chiefs and the Rams and Saints. They also discuss the controversy between the Chiefs and Patriots and what they had to say about it. The guys also give their predictions for the Super Bowl and what teams they think are going to win the game.</p><p>Table <ref type="table" coords="1,331.36,590.52,3.76,8.64">1</ref>: A snippet of the podcast transcript, its original creator description and an abstract produced by our summarizer.</p><p>We seek to generate a concise textual summary for any podcast episode, which a user might read when deciding whether to listen to the podcast. An ideal summary is required to accurately convey all the most important attributes of the episode, such as topical content, genre and participants. It is best for the summary to contain no redundant material that is not needed when deciding whether to listen. In Table <ref type="table" coords="1,332.81,728.77,4.01,9.46">1</ref>, we show a snippet of the podcast transcript and its creator description. The snippet contains 3 segments, each corresponds to 30 seconds of audio. The major challenge in performing podcast summarization includes (a) the unique characteristics of spoken text. Disfluencies and redundancies are abundant in spoken text; its information density is often low when compared to written text. The podcasts are of various genres: monologue, interview, conversation, debate, documentary, etc. and transcription is more challenging and noisier; (b) the excessive length of transcripts. It exceeds the limit imposed by many neural abstractive models. A podcast transcript contains on average 80 segments and 5,743 tokens. It serves as the input to a podcast summarization system to produce an abstract. The creator description is part of the metadata. It contains 81 tokens on average and is used as the reference summary.</p><p>This work draws on our rich experience in summarizing meeting conversations <ref type="bibr" coords="2,216.51,498.43,74.66,9.46">(Liu et al., 2009;</ref><ref type="bibr" coords="2,72.00,511.98,34.94,9.46">Liu and</ref><ref type="bibr" coords="2,110.36,511.98,75.45,9.46">Liu, 2009, 2013;</ref><ref type="bibr" coords="2,189.23,511.98,81.56,9.46" target="#b1">Koay et al., 2020)</ref> and building neural abstractive systems <ref type="bibr" coords="2,223.54,525.53,68.09,9.46;2,72.00,539.08,19.32,9.46">(Lebanoff et al., 2019</ref><ref type="bibr" coords="2,98.87,539.08,24.45,9.46">(Lebanoff et al., , 2020;;</ref><ref type="bibr" coords="2,126.05,539.08,74.20,9.46" target="#b9">Song et al., 2020)</ref>. We have chosen an abstractive system over its extractive counterpart for this task, as neural abstractive systems have seen significant progress <ref type="bibr" coords="2,179.26,579.72,82.65,9.46" target="#b7">(Raffel et al., 2019;</ref><ref type="bibr" coords="2,264.42,579.72,25.85,9.46;2,72.00,593.27,53.10,9.46">Lewis et al., 2020;</ref><ref type="bibr" coords="2,128.16,593.27,65.88,9.46" target="#b6">Qi et al., 2020)</ref>. Not only can an abstract accurately convey the content of the podcast, but it is in a succinct form that is easy to read on a smartphone. Our system seeks to fine-tune a neural abstractive summarizer with an encoder-decoder architecture <ref type="bibr" coords="2,117.02,661.02,83.58,9.46">(Lewis et al., 2020)</ref> on podcast data. We especially emphasize content selection, where an extractive module is developed to select salient segments from the beginning and end of a transcript, which serve as the input to an abstractive summarizer. In this work, we systematically investigate three crucial questions concerning abstractive summarization of podcast transcripts.</p><p>• Is it sufficient to feed the leading sentences of a transcript to the summarizer to produce an abstract, or are there advantages to be gained from selecting salient segments with an extractive module to use as input to the summarizer? (Content Selection)</p><p>• Should we remove training pairs with noisy, low-quality reference summaries in entirety, or can we improve their quality, and thus strike a balance between the amount and quality of training examples? (The Quality of Reference)</p><p>• What summary length would be most appropriate for podcast episodes to serve our goal of assisting users in deciding whether to listen to the podcast? (Summary Postprocessing)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Method</head><p>We aim to produce a concise textual summary from a podcast transcript that captures the most important information of the episode to help users decide whether to listen to that episode. Our method makes use of podcast transcripts only but not raw audio. It utilizes the BART model <ref type="bibr" coords="2,467.52,579.37,59.39,9.46;2,307.28,592.92,25.37,9.46">(Lewis et al., 2020)</ref> to condense a source text into an an abstractive summary, which employs an encoder-decoder architecture. The model is pretrained using a denoising objective. The source text is corrupted by replacing spans of text with mask symbols. The encoder encodes the corrupted text using a bidirectional model, and the decoder learns to reconstruct the original source text by attending to hidden states of the final layer of the encoder using a cross-attention mechanism.</p><p>Our implementation is based on BART-LARGE. The encoder and decoder each contain 12 layers of Transformer blocks. The hidden state and embed-ding size is 1,024. Byte-Pair Encoding (BPE; <ref type="bibr" coords="3,272.16,66.67,14.94,9.46;3,72.00,80.22,74.31,9.46" target="#b8">Sennrich et al., 2016)</ref> is used to tokenize the source text. It has a vocabulary of 50,265 subword units. The BART model is fine-tuned on CNN (bart-large-cnn) then on podcast data; the latter contain 79,262/500 examples for training and validation. Our system is evaluated on a test set with 1,027 examples. Given a transcript, we compare two methods to generate the source text that serves as the input to BART, corresponding to two runs we submitted to the Podcast Challenge. Our system architecture is illustrated in Figure <ref type="figure" coords="3,103.21,215.71,5.45,9.46" target="#fig_0">1</ref> and details are described as follows.</p><p>• UCF_NLP1 The transcript is truncated to a length of L=1,024 tokens. The method takes the lead sentences of a transcript, feeds them to the BART model to produce a succinct abstractive summary that captures important information about the podcast episode.</p><p>• UCF_NLP2 It produces an abstractive summary from a podcast transcript in a similar fashion. Crucially, the method enhances content selection by identifying summary-worthy segments from the transcript to serve as input to BART. The selected segments are limited to a length of L=1,024 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Content Selection</head><p>We seek to empirically answer the question: "Is it sufficient to feed the lead sentences of the transcript to an abstractive summarizer, or are there advantages to be gained from selecting salient segments with an extraction module to use as input to the summarizer?" We consider segments produced by the Google Speech-to-Text API as basic units of extraction, each corresponds to 30 seconds of audio. We opt for segment-rather than sentence-based extraction for two reasons. First, the information density of single utterances is often low. In contrast, the segments are lengthier and more detailed. They tend to have similar lengths and are less likely to be misclassified due to length variation. Second, comparing to sentences, concatenating segments to form a source text that serves as the input to BART can help preserve the context of the utterances extracted from the transcript. We introduce a hybrid representation for the i-th candidate segment that combines deep contextualized representations and surface features. Particularly, each segment is encoded by RoBERTa <ref type="bibr" coords="3,271.13,728.77,19.14,9.46;3,72.00,742.32,50.92,9.46" target="#b5">(Liu et al., 2019)</ref>. It contains 24 layers of Transformer blocks, has a hidden size of 1024 and 16 attention heads. A special token <ref type="bibr" coords="3,411.19,68.80,17.44,6.45">[CLS]</ref> is added to the beginning of the segment and [SEP] is added to the end. We use the output vector corresponding to the <ref type="bibr" coords="3,508.10,95.90,17.44,6.45">[CLS]</ref> token as the contextualized representation for the segment, denoted by h c i ∈ R D . A segment containing salient words is deemed to be important. We measure word salience by its duration (in seconds) and TF-IDF score, which are orthogonal to contextualized representations and aim to capture the topical salience. To characterize a segment using its containing words, we compute 12 feature scores for a candidate segment, including (a) the sum and average of word TF-IDF scores; (b) the sum and average of word durations; (3) the average of word TF-IDF scores (and durations), limiting to 5/10/15/20 words per segment that yield the highest scores. Each feature score is discretized into a binary vector using a number of bins whose sizes are {2, 3, 5, • • • 31, 37} (12 prime numbers). E.g., a feature score is mapped to a 2-dimensional vector [0,1] (bin size=2) if its value is in the upper half of all values; otherwise it is <ref type="bibr" coords="3,460.14,355.11,12.05,9.46">[1,</ref><ref type="bibr" coords="3,472.19,355.11,8.03,9.46">0]</ref>. By concatenating binary vectors of different bin sizes, and vectors corresponding to different feature scores, we obtain a 2,364-dimentional vector for each segment. The vector is passed through a feedforward layer to generate a surface feature vector of size D, denoted by h s i ∈ R D . We take 33 segments from the beginning and 7 segments from the end of each transcript to be the candidate segments. This amounts to a total of 40 segments per episode. The selection is bounded by the GPU memory, but allows us to cover 81% of the ground-truth summary segments. Each segment is characterized by its contextualized representations h c i , surface features h s i , and a position embedding h p i , all of which are added up in an element-wise manner to serve as input to a 2-layer Transformer encoder, with a hidden size of D=1,024, 16 attention heads and no pretraining, to produce a vector for each candidate segment of the transcript. Each vector is fed to a feedforward and a softmax layer to predict if the segment is salient.</p><p>The ground-truth segment labels are derived by comparing segments with creator descriptions. We calculate the ROUGE-2 Recall score for a segment against any sentence of the creator description. A segment is labelled as positive if the score is greater than a threshold (τ =0.2), otherwise negative. The positive-to-negative ratio is 1:18 among candidate segments, and no downsampling was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Creator Description</head><p>The Guys are back for another Triple Threat Sports Podcast! This time UTXJGTHEDON is giving out all the smoke as his Los Angeles Rams is heading to the Super Bowl to face the New England Patriots.</p><p>-Support this podcast: https://anchor.fm/triplethreatsportspodcast</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clean Reference Summary</head><p>The Guys are back for another Triple Threat Sports Podcast! This time UTXJGTHEDON is giving out all the smoke as his Los Angeles Rams is heading to the Super Bowl to face the New England Patriots.</p><p>Table 2: Our data cleansing method focuses on improving the quality of reference summaries using a number of heuristics, rather than eliminating noisy reference summaries in entirety. It thus strikes a good balance between the quality and amount of training examples.</p><p>Our preliminary results suggest that using a hybrid representation that combines surface features with contextualized representations for the segments can lead to an improvement in extraction performance (+0.53% F-score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Quality of Reference</head><p>One of the significant challenges in abstractive summarization is the scarcity of labelled data. While it is common practice to remove training examples containing noisy, low-quality reference summaries, it is not obvious whether this is the best path to take for data curation, as a significant amount of examples may be eliminated from the training set. Thus, we raise the question: "Should we remove training examples with noisy low-quality reference summaries in entirety, or can we improve their quality, and thus strike a balance between the amount and quality of training examples?"</p><p>The training data provided by the Podcast Challenge contain over 100,000 episodes and short descriptions written by their respective creators. The organizers find that about a third are less useful descriptions, and have since filtered out descriptions that are (a) too long (greater than 750 characters) or short (less than 20 characters); (b) too similar to other descriptions (cut-and-paste or template); (c) too similar to its show description (no new info). This practice results in 66,245 training examples, corresponding to a 34% reduction of training data, which has a visible effect on performance.</p><p>Instead of eliminating noisy examples in entirety, we strive to enhance the quality of creator descriptions using heuristics. Our goal is to identify sentences that contain improper content and remove them from the descriptions. We compute a salience score for each sentence of the description by summing over word IDF scores. A low IDF score indicates the word frequently appears in other episodes, and thus is uninformative. <ref type="foot" coords="4,422.38,64.62,3.99,6.91" target="#foot_0">3</ref> We remove sentences if their salience scores are lower than a threshold (σ=10). The remaining sentences of a creator description are concatenated to form a clean reference summary. Our method results in 79,912 training examples. It reduces the average length of the reference summary from 81 to 76 words. In Table 2, we show an example containing reference summaries before and after data cleansing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Summary Postprocessing</head><p>We next describe our efforts at postprocessing the summaries generated by BART. The length_penalty of BART penalizes the log probability of a summary (a negative value) by its length, with an exponent p, setting p=2.0 promotes the generation of longer summaries. We set no_repeat_ngram_size to be 3, which stipulates that a trigram cannot occur more than once in the summary. After a grid search in the range of <ref type="bibr" coords="4,373.86,343.39,17.44,9.46">[35,</ref><ref type="bibr" coords="4,391.30,343.39,13.08,9.46">42]</ref>, we set the min_length of a summary to be 35 subwords for UCF_NLP1 and 39 for UCF_NLP2. The max_length of a summary is set to 250 subwords. We use a beam size of K=4 for summary decoding.</p><p>BART may optimize well with the inductive bias, but it remains necessary to apply a series of heuristics to the output summaries to alleviate any overfitting that has led to the generation of template language and improve the summary presentability. Among others, the heuristics include (a) removing the content after "-" (e.g., "-This episode is sponsored by," "-Send in a voice message"); (b) removing URLs; (c) removing brackets and the content inside it; (d) removing any trailing incomplete sentence if the summary is excessively long (≥128 tokens); (e) removing duplicate sentences that occur three times or more across different episodes. We observe that a handful of sentences appeared in summaries of different episodes. They are unrelated to the transcripts, but are generated possibly due to overfitting (e.g., The Oops podcast examines the mistakes that change the trajectory of people's lives: the bad decisions, the aftermath, the path to redemption and all things in between.)</p><p>(3) Excellent The summary accurately conveys all the most important attributes of the episode, which could include topical content, genre, and participants. In addition to giving an accurate representation of the content, it contains almost no redundant material which is not needed when deciding whether to listen. It is also coherent, comprehensible, and has no grammatical errors. (2) Good</p><p>The summary conveys most of the most important attributes and gives the reader a reasonable sense of what the episode contains with little redundant material which is not needed when deciding whether to listen. Occasional grammatical or coherence errors are acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(1) Fair</head><p>The summary conveys some attributes of the content but gives the reader an imperfect or incomplete sense of what the episode contains. It may contain redundant material which is not needed when deciding whether to listen and may contain repetitions or broken sentences. (0) Bad The summary does not convey any of the most important content items of the episode or gives the reader an incorrect or incomprehensible sense of what the episode contains. It may contain a large amount of redundant information that is not needed when deciding whether to listen to the episode.</p><p>Table <ref type="table" coords="5,95.73,186.80,3.71,8.64">3</ref>: The qualitative judgments performed by NIST. The rating is a number from 0-3, with 0 being Bad and 3 being Excellent.</p><p>Q1 Does the summary include names of the main people (hosts, guests, characters) involved or mentioned in the podcast?</p><p>Q2 Does the summary give any additional information about the people mentioned (such as their job titles, biographies, personal background, etc)?</p><p>Q3 Does the summary include the main topic(s) of the podcast?</p><p>Q4 Does the summary tell you anything about the format of the podcast; e.g. whether it's an interview, whether it's a chat between friends, a monologue, etc?</p><p>Q5 Does the summary give you more context on the title of the podcast?</p><p>Q6 Does the summary contain redundant information?</p><p>Q7 Is the summary written in good English?</p><p>Q8 Are the start and end of the summary good sentence and paragraph start and end points?</p><p>Table <ref type="table" coords="5,96.08,376.27,3.73,8.64">4</ref>: There are eight yes-or-no questions asked about the summary. The judgments are performed by NIST. An ideal summary should receive a "yes" (1) for all questions but Q6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Conclusion</head><p>There were 29 submitted runs for the podcast summarization task of TREC 2020. Each run contains a set of summaries generated for the full test set with 1,027 episodes. Among these, 179 episodes were selected by NIST evaluators in a random fashion to perform qualitative judgments on the summary quality. An evaluator quickly skimmed the episode, and made judgments for each summary for that episode, in a random order. Intermixed in the submitted summaries were the creator description ("DESC") for an episode, and a "filtered" summary from Spotify ("FILT"). Our system runs are denoted by "UCF_NLP1" and "UCF_NLP2". The latter used an additional extraction module to identify salient segments from the transcripts. The evaluation criteria used by NIST evaluators are shown in Table <ref type="table" coords="5,157.00,674.57,4.10,9.46">3</ref>. The summary quality rating is a number from 0-3, corresponding to four levels: Bad/Fair/Good/Excellent. Additionally, there were eight yes-or-no questions asked about the summary (Table <ref type="table" coords="5,101.74,728.77,3.91,9.46">4</ref>). They were designed to evaluate the various aspects of summaries. An ideal summary will receive a "yes" for all questions but Q6.</p><p>We present our results in Tables 5-9. Our system "UCF_NLP2" has achieved a quality rating of 1.559. This is an absolute increase of 0.268 (+21%) over episode descriptions. We raise awareness of possible inconsistencies between ROUGE and human judgments of summary quality. The inconsistencies could stem from the deficiencies of ROUGE in capturing semantic meanings, or it could be due to episode descriptions are not the most appropriate reference summaries. We find content selection to remain important for podcast summarization. A summary containing only partial information about an episode may hamper the reader's understanding. Further, we caution that it is challenging to generate abstractive summaries that are fully accurate to the content of the podcast. Not only are there transcription errors, but subtle change of meaning can happen in system abstracts. Even humans may not spot some subtle errors without a careful comparison of system abstracts and transcripts.  An assessor quickly skimmed the episode, and made judgments for each summary of the episode. "DESC" represents the episode description. "FILT" is a "filtered" summary provided by Spotify. "UCF_NLP " are our system outputs. Our best system "UCF_NLP2" uses an additional extraction component to identify summary-worthy segments from the transcripts. It achieves a quality rating of 1.559. This is an absolute increase of 0.268 (+21%) over the episode description. Table <ref type="table" coords="6,95.98,264.02,3.71,8.64">6</ref>: Percentages of testing episodes (out of a total of 179) on which our system performs equal to or better than the majority baseline. The majority rating was obtained by choosing the most frequent rating across a total of 29 submitted summaries, for each episode and each question. Our systems "UCF_NLP " achieve the most gains compared to episode descriptions "DESC" on questions Q1 (people names), Q3 (main topics), Q5 (title context) and Q7 (good English). on which a system performs equal to or better/worse than the majority baseline ("Majority", top), and episode descriptions ("DESC", bottom). "±1/2/3" represents the gap between the numerical ratings, measured in terms of summary quality. The majority rating was obtained by choosing the most frequent quality rating across a total of 29 submitted summaries, for each episode. Our system "UCF_NLP2" frequently outperforms or performs comparably to the majority baseline (96.65%) and episode descriptions (79.33%). We observe that UCF_NLP1 outperforms UCF_NLP2 in terms of R-1, R-2 and R-L F-scores. We note that ROUGE scores are not necessarily a measure of summary quality, as UCF_NLP2 was rated higher according to human judgments.</p><p>Logan <ref type="bibr" coords="6,335.49,504.50,191.30,8.64;6,317.72,515.46,137.13,8.64">Lebanoff, Franck Dernoncourt, Doo Soon Kim, Walter Chang, and Fei Liu. 2020</ref>. A cascade approach to neural abstractive summarization with content selection and fusion. In Proceedings of the Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (AACL).</p><p>Logan </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,201.66,453.55,8.64;2,72.00,212.27,455.11,7.77;2,72.00,222.23,453.55,7.77;2,72.00,231.91,185.44,8.06;2,112.91,60.56,369.01,140.25"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of our system architecture. UCF_NLP1 truncates the transcript to a length of L=1,024 tokens, before feeding it to the BART model to produce a concise abstract. UCF_NLP2 produces an abstractive summary in a similar fashion. It enhances content selection by identifying important segments from the transcripts to serve as input to BART. The selected segments are limited to a length of L=1,024 tokens.</figDesc><graphic coords="2,112.91,60.56,369.01,140.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,71.69,132.84,455.42,8.64"><head>Table 5 :</head><label>5</label><figDesc>The average results of human judgments for 179 testing summaries. The evaluation was performed by NIST assessors.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,71.69,326.16,219.18,150.21"><head>Table 7 :</head><label>7</label><figDesc>Percentages of testing episodes (out of a total of 179)</figDesc><table coords="6,79.32,326.16,204.48,129.46"><row><cell></cell><cell cols="4">Majority Wins (%) Equal</cell><cell cols="3">System Wins (%)</cell></row><row><cell>System</cell><cell>-3</cell><cell>-2</cell><cell>-1</cell><cell>0</cell><cell>+1</cell><cell>+2</cell><cell>+3</cell></row><row><cell>DESC</cell><cell cols="2">0.0 2.2</cell><cell>8.4</cell><cell>48.0</cell><cell cols="2">22.9 12.8</cell><cell>7.3</cell></row><row><cell>FILT</cell><cell cols="2">0.0 2.2</cell><cell>8.4</cell><cell>44.7</cell><cell cols="2">25.1 10.1</cell><cell>9.5</cell></row><row><cell cols="3">UCF_NLP1 0.6 1.1</cell><cell>5.0</cell><cell>43.6</cell><cell cols="2">22.9 19.0</cell><cell>7.8</cell></row><row><cell>UCF_NLP2</cell><cell cols="2">0 1.1</cell><cell>2.2</cell><cell>43.6</cell><cell cols="3">26.3 16.2 10.6</cell></row><row><cell></cell><cell cols="3">DESC Wins (%)</cell><cell cols="4">Equal System Wins (%)</cell></row><row><cell>System</cell><cell>-3</cell><cell>-2</cell><cell>-1</cell><cell>0</cell><cell>+1</cell><cell>+2</cell><cell>+3</cell></row><row><cell>FILT</cell><cell cols="2">1.1 1.1</cell><cell>14.0</cell><cell>65.9</cell><cell>15.1</cell><cell>2.2</cell><cell>0.6</cell></row><row><cell cols="3">UCF_NLP1 0.6 6.7</cell><cell>17.9</cell><cell>39.7</cell><cell>23.5</cell><cell>9.5</cell><cell>2.2</cell></row><row><cell cols="3">UCF_NLP2 0.6 5.0</cell><cell>15.1</cell><cell>41.9</cell><cell cols="2">24.0 10.6</cell><cell>2.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,71.69,325.37,435.73,435.47"><head>Table 8 :</head><label>8</label><figDesc>ROUGE scores on the full test set containing 1,027 episodes. The episode descriptions are used as gold-standard summaries; they are called "model" summaries. ‡ represents ROUGE scores computed by NIST. The other scores are calculated by us. There is a minor discrepancy between these scores when given the same summaries (±0.18% for ROUGE-L). UCF_NLP1 marginally outperforms UCF_NLP2 according to ROUGE-2 and ROUGE-L.</figDesc><table coords="6,86.41,579.05,187.70,91.40"><row><cell>Test-1027</cell><cell>System</cell><cell>P (%)</cell><cell>R (%)</cell><cell>F (%)</cell></row><row><cell>ROUGE-1</cell><cell>UCF_NLP1 UCF_NLP2</cell><cell>37.14 36.29</cell><cell>30.48 31.39</cell><cell>29.62 29.64</cell></row><row><cell>ROUGE-2</cell><cell>UCF_NLP1 UCF_NLP2</cell><cell>15.82 14.89</cell><cell>12.43 12.52</cell><cell>12.42 11.96</cell></row><row><cell></cell><cell>UCF_NLP1</cell><cell>26.63</cell><cell>22.23</cell><cell>21.40</cell></row><row><cell>ROUGE-L</cell><cell>UCF_NLP1  ‡ UCF_NLP2</cell><cell>26.67 25.53</cell><cell>22.05 22.59</cell><cell>21.32 20.99</cell></row><row><cell></cell><cell>UCF_NLP2  ‡</cell><cell>25.53</cell><cell>22.42</cell><cell>20.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,306.97,409.88,220.14,28.35"><head>Table 9 :</head><label>9</label><figDesc>ROUGE scores on the test set of 179 episodes. The scores are calculated by us. Episode descriptions are used as gold-standard summaries; they are called "model" summaries.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,307.28,581.18,220.01,183.94"><head></head><label></label><figDesc>Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon Kim, Seokhwan Kim, Walter Chang, and Fei Liu. 2019. Scoring sentence singletons and pairs for abstractive summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175-2189, Florence, Italy. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="4,323.42,707.32,202.13,7.77;4,307.28,717.28,218.27,7.77;4,307.28,727.24,218.27,7.77;4,307.28,737.20,218.26,7.77;4,307.28,747.17,218.26,7.77;4,306.60,757.13,218.87,7.77"><p>We perform data normalization by replacing URLs, Email addresses, @usernames, #hashtags, digits and tokens that are excessively long (greater than 25 characters) with placeholders before computing word IDF scores. Only words occurring 5 times or more in the corpus and with IDF scores greater than 1.5 are considered when computing sentence salience scores.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by the <rs type="funder">National Science Foundation</rs> <rs type="grantNumber">IIS-1909603</rs>. We are grateful to <rs type="institution">Amazon</rs> for partially sponsoring the research and computation in this study through the <rs type="institution">Amazon AWS Machine Learning Research Award</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CSQ8ykU">
					<idno type="grant-number">IIS-1909603</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,307.28,636.18,219.92,8.64;5,318.19,647.13,209.01,8.64;5,318.19,658.09,209.10,8.64;5,317.88,668.88,207.66,8.81;5,317.63,679.84,205.24,8.81" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,317.88,669.05,151.38,8.64">TREC 2020 Podcasts Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In The 29th Text Retrieval Conference (TREC) notebook. NIST</note>
</biblStruct>

<biblStruct coords="5,307.28,701.69,218.27,8.64;5,318.19,712.64,207.72,8.64;5,318.19,723.60,207.36,8.64;5,318.19,734.39,209.01,8.81;5,318.19,745.35,207.36,8.58;5,317.52,756.31,45.66,8.58" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,506.79,712.64,19.12,8.64;5,318.19,723.60,207.36,8.64;5,318.19,734.56,48.43,8.64">How domain terminology affects meeting summarization performance</title>
		<author>
			<persName coords=""><forename type="first">Jin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Koay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaojin</forename><surname>Roustai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dillon</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Kerrigan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,396.85,734.39,130.35,8.58;5,318.19,745.35,207.36,8.58;5,317.52,756.31,40.59,8.58">Proceedings of the 28th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 28th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,67.28,219.92,8.64;7,82.91,78.24,207.36,8.64;7,82.91,89.03,207.36,8.81;7,82.66,99.99,209.26,8.81;7,82.91,111.12,209.01,8.64;7,82.91,122.08,32.94,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,186.18,67.28,105.74,8.64;7,82.91,78.24,207.36,8.64;7,82.91,89.20,54.78,8.64">From extractive to abstractive meeting summaries: Can it be done by sentence compression?</title>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,157.32,89.03,132.95,8.58;7,82.66,99.99,119.33,8.58">Proceedings of the ACL-IJCNLP 2009 Conference Short Papers</title>
		<meeting>the ACL-IJCNLP 2009 Conference Short Papers<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="261" to="264" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="7,72.00,142.00,218.27,8.64;7,82.91,152.96,207.36,8.64;7,82.91,163.92,209.02,8.64;7,82.91,174.71,207.36,8.81;7,82.63,185.67,162.62,8.81" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,208.81,142.00,81.46,8.64;7,82.91,152.96,207.36,8.64;7,82.91,163.92,209.02,8.64;7,82.91,174.88,31.73,8.64">Towards abstractive speech summarization: Exploring unsupervised and supervised approaches for spoken utterance compression</title>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,123.57,174.71,166.69,8.58;7,82.63,185.67,84.25,8.58">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1469" to="1480" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,205.76,220.01,8.64;7,82.91,216.72,209.01,8.64;7,82.91,227.51,207.36,8.81;7,82.55,238.47,207.72,8.58;7,82.58,249.43,209.34,8.58;7,82.91,260.39,208.86,8.81;7,82.91,271.52,209.01,8.64;7,82.91,282.47,71.96,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,82.91,216.72,209.01,8.64;7,82.91,227.68,130.56,8.64">Unsupervised approaches for automatic keyword extraction using meeting transcripts</title>
		<author>
			<persName coords=""><forename type="first">Feifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,231.13,227.51,59.14,8.58;7,82.55,238.47,207.72,8.58;7,82.58,249.43,209.34,8.58;7,82.91,260.39,157.55,8.58">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="620" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,302.40,219.92,8.64;7,82.91,313.36,208.61,8.64;7,82.91,324.32,209.10,8.64;7,82.91,335.28,209.02,8.64;7,82.91,346.07,108.45,8.81" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="7,82.91,335.28,209.02,8.64;7,82.91,346.24,25.37,8.64">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,366.16,219.52,8.64;7,82.91,377.12,207.36,8.64;7,82.91,388.08,209.02,8.64;7,82.91,399.04,209.10,8.64;7,82.91,409.83,75.27,8.58" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,177.28,388.08,114.64,8.64;7,82.91,399.04,205.29,8.64">Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training</title>
		<author>
			<persName coords=""><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04063</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,429.92,218.27,8.64;7,82.91,440.88,208.61,8.64;7,82.44,451.84,207.83,8.64;7,82.91,462.80,209.01,8.64;7,82.91,473.59,107.90,8.81" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,210.12,451.84,80.15,8.64;7,82.91,462.80,209.01,8.64;7,82.91,473.76,24.90,8.64">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,493.68,220.01,8.64;7,82.91,504.64,207.36,8.64;7,82.55,515.43,209.37,8.81;7,82.91,526.39,207.36,8.58;7,82.63,537.35,209.13,8.81;7,82.16,548.48,209.76,8.64;7,82.91,559.44,71.96,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,115.92,504.64,174.36,8.64;7,82.55,515.60,76.09,8.64">Neural machine translation of rare words with subword units</title>
		<author>
			<persName coords=""><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,178.67,515.43,113.25,8.58;7,82.91,526.39,207.36,8.58;7,82.63,537.35,43.74,8.58;7,179.63,537.35,53.03,8.58">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct coords="7,72.00,579.36,219.52,8.64;7,82.91,590.32,209.02,8.64;7,82.91,601.11,209.01,8.81;7,82.91,612.07,207.36,8.58;7,82.30,623.03,118.05,8.58" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,165.99,590.32,125.94,8.64;7,82.91,601.28,172.43,8.64">Controlling the amount of verbatim copying in abstractive summarization</title>
		<author>
			<persName coords=""><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bingqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Liu Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,274.11,601.11,17.81,8.58;7,82.91,612.07,207.36,8.58;7,82.30,623.03,113.66,8.58">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
