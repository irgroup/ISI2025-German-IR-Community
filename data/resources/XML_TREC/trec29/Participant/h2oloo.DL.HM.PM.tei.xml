<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.99,101.17,13.39,15.48;1,157.38,107.46,5.85,10.48;1,163.73,101.17,304.28,15.48;1,122.00,121.09,370.17,15.48;1,220.08,141.02,171.84,15.48">H 2 oloo at TREC 2020: When all you got is a hammer... Deep Learning, Health Misinformation, and Precision Medicine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,157.37,198.50,63.89,8.96"><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.49,198.50,56.92,8.96"><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.53,198.50,53.66,8.96"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.71,198.50,39.24,8.96"><forename type="first">Hang</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.93,198.50,48.62,8.96"><forename type="first">Ruizhou</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.52,209.40,74.50,8.96"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.70,209.40,46.77,8.96"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.05,223.54,73.14,8.64"><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.99,101.17,13.39,15.48;1,157.38,107.46,5.85,10.48;1,163.73,101.17,304.28,15.48;1,122.00,121.09,370.17,15.48;1,220.08,141.02,171.84,15.48">H 2 oloo at TREC 2020: When all you got is a hammer... Deep Learning, Health Misinformation, and Precision Medicine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3BDA76E7632F6846F1907A9B12BA287</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The h 2 oloo team from the University of Waterloo participated in the TREC 2020 Deep Learning, Health Misinformation, and Precision Medicine Tracks. Our primary goal was to validate sequence-to-sequence based retrieval techniques that we have been working on in the context of multi-stage retrieval dubbed "Expando-Mono-Duo" [6, 10] comprising a candidate document generation stage (driven by "bag of words" techniques) followed by a pointwise and then a pairwise reranking stage built around T5 <ref type="bibr" coords="1,229.92,365.15,15.16,8.64" target="#b10">[11]</ref>, a powerful sequence-to-sequence transformer language model. For the Health Misinformation task, we also employ learnings from our fact verification system, VerT5erini [9]. All of our experiments employed the open-source Anserini IR toolkit <ref type="bibr" coords="1,434.35,400.17,15.89,8.64" target="#b13">[14,</ref><ref type="bibr" coords="1,453.49,400.17,11.91,8.64" target="#b15">16]</ref>, which is based on the popular open-source Lucene search library, for initial retrieval that feeds the T5-based rerankers. Besides being the state of the art in various other collections (e.g., Robust04 and TREC-COVID), we found our models achieved much better effectiveness compared to the BM25 baselines as well as the median scores in all three tracks, demonstrating the versatility and the zero-shot transfer capabilities of our multi-stage ranking system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The h 2 oloo team from the University of Waterloo participated in multiple tracks at TREC 2020. This notebook paper describes our approach in the Deep Learning, Health Misinformation, and Precision Medicine Tracks.</p><p>We use a two-stage ranking architecture coupled with pre-indexing document expansion, all of which use the T5 sequence-to-sequence transformer <ref type="bibr" coords="1,295.63,572.77,15.42,8.64" target="#b10">[11]</ref>. This earns it the name, "Expando-Mono-Duo T5" <ref type="bibr" coords="1,125.66,583.68,15.26,8.64" target="#b9">[10]</ref>. The general strategy involves an initial BM25-based keyword retrieval that is refined by a pointwise and then pairwise ranking. Document expansion is used when feasible to enrich keyword representations in the inverted index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multi-Stage Ranking with T5</head><p>In our formulation, a multi-stage ranking architecture comprises a number of stages, denoted H 0 to H N . Except for H 0 , which retrieves k 0 candidates from an inverted index, each stage H n receives a ranked list R n-1 comprising k n-1 candidates from the previous stage. Each stage, in turn, provides a ranked list R n comprising k n candidates to the subsequent stage, with the obvious requirement that k n ≤ k n-1 . The ranked list generated by the final stage H N is designated for consumption by the (human) searcher. However, prior to building the inverted index that feeds H 0 , we first perform  In stage H 0 , given a query q, the top-k 0 (= 5 in the figure) candidate documents R 0 are retrieved using BM25. In stage H 1 , monoT5 produces a relevance score s i for each pair of query q and candidate d i ∈ R 0 . The top-k 1 (= 3 in the figure) candidates with respect to these relevance scores are passed to stage H 2 , in which duoT5 computes a relevance score p i,j for each triple (q, d i , d j ). The final list of candidates R 2 is formed by reranking the candidates according to these scores.</p><formula xml:id="formula_0" coords="2,131.98,81.94,338.18,130.24">d i q d 2 d 3 d i q d j R 0 R 1 R 2 d 5 q H 0 H 1 H 2 d 2</formula><p>document expansion on the input corpus to enrich its representation (we denote this as the H -1 stage).</p><p>We describe each component of the overall architecture (see Figure <ref type="figure" coords="2,378.11,336.96,4.15,8.64">1</ref>) in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">H -1 : Doc Expansion with docT5query</head><p>The idea behind document expansion is to enrich each document with additional terms that are representative of its content. In our particular implementation, we take a corpus of (question, relevant document) pairs and train a sequence-to-sequence model to predict, given a document, questions that it can potentially answer. These questions are then directly appended to the document; once this expansion has been performed for every document, the collection is indexed, as before.</p><p>As the first stage of our pipeline, we expand all documents in the MS MARCO corpus with queries predicted with docT5query <ref type="bibr" coords="2,218.99,457.75,10.62,8.64" target="#b6">[7]</ref>. The model was trained with a constant learning rate of 10 -3 for 4k iterations with batches of 256, which corresponds to 2 epochs with the MS MARCO passage ranking training set. We use a maximum of 512 input tokens and 64 output tokens. In the MS MARCO passage ranking dataset, none of the inputs or outputs have to be truncated when using these lengths. Similar to Nogueira et al. <ref type="bibr" coords="2,212.81,501.39,10.68,8.64" target="#b7">[8]</ref>, we find that the top-k sampling decoder <ref type="bibr" coords="2,392.84,501.39,11.71,8.64" target="#b1">[2]</ref> produces more effective queries than beam search. We use k = 10 and sample 40 queries per document.</p><p>We use T5-base as we did not notice any improvement in retrieval effectiveness with the large model. We did not experiment with T5-3B and T5-11B due to their computational cost. We use Google's TPU v3s to train and run inference. Training takes less than 1.5 hours on a single TPU. For inference, sampling 5 queries per document for 8.8M documents requires approximately 40 hours on a single TPU, costing $96 USD (40 hours × $2.40 USD/hour) using preemptible TPUs. Note that inference is trivially parallelizable and linear with respect to the number of samples. All expanded documents are then indexed with the Anserini IR toolkit <ref type="bibr" coords="2,288.76,594.14,16.61,8.64" target="#b14">[15]</ref> (post-v0.9.2); the expanded queries are appended to the original documents, but not specially delimited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">H 0 : "Bag of Words" BM25</head><p>The stage H 0 receives as input the user query q and produces top-k 0 candidates R 0 . In our implementation, the query is treated as a "bag of words" for ranking documents from the corpus using a standard inverted index based on the BM25 scoring function <ref type="bibr" coords="2,318.23,676.73,15.40,8.64" target="#b11">[12]</ref>. We use the Anserini IR toolkit <ref type="bibr" coords="2,466.30,676.73,15.87,8.64" target="#b13">[14,</ref><ref type="bibr" coords="2,484.66,676.73,11.91,8.64" target="#b15">16]</ref>, <ref type="foot" coords="2,500.54,675.06,3.49,6.05" target="#foot_0">1</ref>which is built on the popular open-source Lucene search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">H 1 : Pointwise Reranking with monoT5</head><p>In general, the task of a reranking stage H n is to estimate a score s i quantifying how relevant a candidate d i ∈ R n-1 is to a query q. Naturally, we expect that the ranking induced by these scores yields a higher metric (e.g., MAP or MRR) than the scores from the previous stage.</p><p>In stage H 1 , the documents retrieved in H 0 are reranked by a pointwise reranker, which we call monoT5. Our reranking method is based on Nogueira et al. <ref type="bibr" coords="3,355.73,144.57,10.72,8.64" target="#b5">[6]</ref>, which uses T5 <ref type="bibr" coords="3,435.34,144.57,15.42,8.64" target="#b10">[11]</ref>, a sequenceto-sequence model that uses a similar masked language modeling objective as BERT to pretrain its encoder-decoder architecture. In this model, all target tasks are cast as sequence-to-sequence tasks. We adapt the approach to document ranking by using the following input sequence:</p><p>Query: q Document: d Relevant:</p><p>where q and d are the query and document texts, respectively. The model is fine-tuned to produce the words "true" or "false" depending on whether the document is relevant or not to the query. That is, "true" and "false" are the "target words" (i.e., ground truth predictions in the sequence-to-sequence transformation).</p><p>At inference time, to compute probabilities for each query-document pair (in a reranking setting), we apply a softmax only on the logits of the "true" and "false" tokens. Hence, we rerank the documents according to the probabilities assigned to the "true" token. We arrived at this particular approach after some trial and error. Other approaches, for example, reranking documents according to the logit of the "true" token or using logits of all tokens to compute the softmax, were not effective, i.e., the retrieval metrics were close to zero.</p><p>We note that while H 0 uses a corpus enriched by document expansion, documents in R 0 consist of original texts that are not expanded; this is due to the input length restrictions of T5.</p><p>We train our models on MS MARCO passage <ref type="bibr" coords="3,299.44,360.44,10.72,8.64" target="#b0">[1]</ref>, which is a passage ranking dataset with 8.8M passages obtained from the top 10 results retrieved from the Bing search engine. The training set contains approximately 500K pairs of query and relevant documents. Each query has one relevant passage, on average. Non-relevant documents for training are also provided as part of the training dataset.</p><p>We fine-tuned our monoT5-3B model on the MS MARCO passage ranking training set with a constant learning rate of 10 -3 for 100K iterations with class-balanced batches of size 128. We use a maximum of 512 input tokens and one output token. In the MS MARCO passage ranking dataset, none of the inputs have to be truncated when using this length. Training monoT5-3B take approximately 160 hours overall on a single Google's TPU v3-8.</p><p>At inference time in all but the Deep Learning passage ranking task, the document length is usually much longer than the length restrictions of the model. Hence, it is not possible to feed the entire text of the document into our model at once. To address this issue, we first segmented each document into passages by applying a sliding window of n length sentences with a stride of n stride . We obtained a probability of relevance for each segment by performing inference on it independently, and then selecting the highest probability among the segments as the relevance score of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">H 2 : Pairwise Reranking with duoT5</head><p>The output R 1 from the previous stage is used as input to the pairwise reranker we call duoT5. Within the framework of "learning to rank", duoT5 can be characterized as a "pairwise" approach, while monoT5 can be characterized as a "pointwise" approach <ref type="bibr" coords="3,334.25,601.31,10.56,8.64" target="#b3">[4]</ref>. In this pairwise approach, the reranker estimates the probability p i,j of the candidate d i being more relevant than d j for query q, where i = j.</p><p>This reranker, also using T5, instead takes as input the sequence:</p><p>Query: q Document0: d i Document1: d j Relevant:</p><p>The pairwise sequence-to-sequence model is fine-tuned to produce the words "true" or "false" depending on whether the document d i has higher or lower relevance than d j to the question q.</p><p>At inference time, we aggregate the pairwise scores p i,j so that each document receives a single score s i . We ablate over the number of candidates k 1 that is reranked by the pairwise ranker. The number of inference calls made per query is given by the number of candidate pairs i.e. k 1 (k 1 -1). We only run experiments where k 1 = 50. We use the following aggregation technique: SYM-SUM :</p><formula xml:id="formula_1" coords="4,275.55,104.86,228.45,20.06">s i = j∈Ji (p i,j + (1 -p j,i ))<label>(1)</label></formula><p>where</p><formula xml:id="formula_2" coords="4,134.47,132.76,106.27,9.65">J i = {0 ≤ j &lt; k 1 , j = i}.</formula><p>The candidates in R 1 are reranked according to their scores s i to get the final list of candidates R 2 .</p><p>The output R 2 , in our current framework, is provided to the end-user, and serves as the input to computing the final evaluation metrics.</p><p>We fine-tuned duoT5 from the monoT5 model trained on MS MARCO passage ranking dataset as it serves as a good initial point having learnt the task of pointwise ranking. We use the same hyperparameters as those used for training monoT5. We initially experiment with duoT5-base and find that model performance converges at about 50K iterations. Hence, we train duoT5-3B for 50K iterations which corresponds to 80 hours overall on a single Google TPU v3-8.</p><p>At inference time, in all but the Deep Learning Passage Ranking task, we run duoT5 using the highest monoT5 scoring segment as the representative of the document. We increased the maximum input tokens for duoT5 from 512 to 1024 to account for pairs of passages that were longer than the default limit of 512 tokens. We were able to do so in T5 since the models were trained with relative positional encodings <ref type="bibr" coords="4,190.48,291.34,16.46,8.64" target="#b12">[13]</ref> and thus can (hopefully) generalize to contexts larger than those seen during training. This modification, however, imposed additional computational costs that come from the model needing to attend to twice the number of tokens; transformers exhibit quadratic complexity in both time and space with respect to input length <ref type="bibr" coords="4,301.42,324.06,10.58,8.64" target="#b2">[3]</ref>. <ref type="foot" coords="4,315.53,322.39,3.49,6.05" target="#foot_1">2</ref>Note that if more than k 1 hits are requested as the output of duoT5, we simply take additional ranked output from monoT5. For example, if we requires 1000 hits, then the first 50 will come from duoT5 (assuming k 1 = 50), while the remaining results (rank positions 51-1000) will be the unaltered rankings from monoT5. Hence, the goal of the pairwise reranker is to improve the quality of results high in the ranked list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Learning Track</head><p>All inference experiments in the document ranking task are run with n length = 10 and n stride = 5.</p><p>Based on the multi-stage pipeline described in Section 2, we submitted a total of four bag-of-words baseline runs and three neural runs to the Deep Learning Track for both passage and document ranking tasks. We first describe each of the baseline runs below.</p><p>BM25 (bm25): This is our baseline bag-of-words retrieval, i.e., H 0 is the only stage of ranking. We adopt all the default settings in Anserini's BM25 implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BM25 + RM3 (bm25rm3):</head><p>To examine the effects of query expansion, we employ the strong BM25 + RM3 baseline described in <ref type="bibr" coords="4,253.09,535.07,15.27,8.64" target="#b16">[17]</ref>.</p><p>docT5query + BM25 (d2q_bm25): To examine the effects of document expansion, we apply the document expansion stage H -1 , followed by BM25 retrieval stage, H 0 .</p><p>docT5query + BM25 + rm3 (d2q_bm25rm3): To examine the effects of both document expansion and query expansion, we apply the document expansion stage H -1 , followed by the BM25 + RM3 query expansion baseline in H 0 .</p><p>We now describe each of the multi-stage neural reranking runs (which we call "Expando-Mono-Duo T5" runs) below.</p><p>BM25 + RM3 + monoT5 + duoT5 (bm25rm3_duo): We first rerank the top-k 0 (=1000) documents retrieved by BM25 + RM3 using our pointwise reranker, monoT5-3B. Then, we rerank the top-k 1 (=50) documents retreived by H 1 using our pairwise reranker, duoT5-3B. docT5query + BM25 + monoT5 + duoT5 (d2q_bm25_duo): Same as BM25 + RM3 + monoT5 + duoT5 except we use H -1 and H 0 as in docT5query + BM25.</p><p>docT5query + BM25 + RM3 + monoT5 + duoT5 (d2q_bm25_duo): Same as BM25 + RM3 + monoT5 + duoT5 except we use H -1 and H 0 as in docT5query + BM25 + RM3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>Results from the Passage Ranking and Document Ranking Tracks are show in Tables <ref type="table" coords="5,471.69,489.80,5.08,8.64" target="#tab_0">1</ref> and<ref type="table" coords="5,498.92,489.80,5.08,8.64" target="#tab_1">2</ref> respectively. In both tables, the first row shows the mean of the median per-topic scores, representing the score of a run that received the median score on all topics. The next four rows (rows 1 -4) show the scores of the Anserini baseline runs and the final three rows (row 5 -7) show the scores of our neural runs.</p><p>In Table <ref type="table" coords="5,142.49,549.83,3.66,8.64" target="#tab_0">1</ref>, we generally find that all our baseline "bag of words" runs fall below the "median" scores. However, whenever we use document expansion (rows 3 and 4), we find our systems exceed the median in terms of nDCG@1K which demonstrates the strength of docT5query. We note similar results in Table <ref type="table" coords="5,171.12,582.56,3.74,8.64" target="#tab_1">2</ref>. except some more scores exceed the median.</p><p>In both Tracks, it is clear that document expansion helps both the BM25 baseline (rows 1 and 3) and the BM25 + RM3 baseline (rows 2 and 4). However, query expansion generally only seems to help in the BM25 baseline (rows 1 and 2). While the improvements when it is used along with document expansion is not clear in terms of the official metrics, we still note improvements in terms of R@1K, the metric H 0 aims to maximize, as the resulting candidate set is passed onto neural rerankers.</p><p>The runs that employ multi-stage neural reranking (rows 5 -7) show large improvements in effectiveness over their respective Anserini baselines (rows 2 -4) as expected. Upon looking at the impact of document expansion and query expansion in the multi-stage ranking pipeline, we find that document expansion clearly helps our neural rerankers (rows 5 and 7) in terms of MAP, nDCG@10 and nDCG@1K. However, it is not clear if query expansion helps on top of document expansion (rows 6 and 7) as both sets of scores are very similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Health Misinformation Track</head><p>Macavaney et al. <ref type="bibr" coords="6,176.92,98.17,11.54,8.64" target="#b4">[5]</ref> demonstrates that fine-tuning the classifiers on Med-MARCO, a medical subset of MS MARCO, helps with biomedical-domain relevance ranking. We note similar results in the TREC-COVID task in <ref type="bibr" coords="6,199.45,119.99,15.33,8.64" target="#b17">[18]</ref>. Hence, we choose to use monoT5-3B and duoT5-3B models that were fine-tuned on MS MARCO passage ranking dataset then fine-tuned (again) on Med-MARCO. All inference experiments are run with n length = 6 and n stride = 3.</p><p>For this Track only, we added to our pipeline the label prediction model from VerT5erini <ref type="bibr" coords="6,457.61,158.19,10.45,8.64" target="#b8">[9]</ref>. We call this model LabelT5 and describe it next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Label Prediction-based Reranking with LabelT5</head><p>Given the topic q and the highest monoT5 scoring segment s i from a document d i , the model is tasked to predict a label ŷ(q, s i ) ∈ {true, weak, f alse}. Here, we use the following input sequence:</p><p>Query: q Document: s i Relevant:</p><p>We train the label prediction model using the effective judgements from the 2019 Decision (Medical Misinformation) Track. We map effective and ineffective judgements to "true" and "false" respectively. The documents judged as inconclusive, no info or not relevant are all labelled "weak" label.</p><p>We fine-tuned our LabelT5-3B model with a constant learning rate of 10 -3 for 500 iterations with batches of size 128. We use a maximum of 512 inputs tokens and one output token. Training LabelT5-3B takes approximately 40 minutes on a single Google's TPU v3-8.</p><p>At inference time, to compute probabilities for each query-document pair (in a reranking setting), we apply a softmax only on the logits of the "true", "weak", and "false" tokens. Then, we rerank the documents according to the probabilities assigned to the "true" token if the answer field is "yes" and the probabilities assigned to the "false" token if the answer field is "no".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Runs</head><p>We submitted a total of nine runs to the Health Misinformation Track Ad-hoc Retrieval Task, which are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BM25 (m1):</head><p>This is our baseline bag-of-words retrieval, i.e., H 0 is the only stage of ranking. We adopt all the default settings. We index all the news articles found in the CommonCrawl News crawl from January 1st, 2020 to April 30th, 2020. At inference time, we retrieve the top-k 0 (=1000) documents per query. We do not use doc2query expansions (stage H -1 ) in this track due to the high costs of expanding the very large corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BM25 + monoT5 (m2):</head><p>We rerank the top-k 0 (=1000) documents retrieved by BM25 using our pointwise reranker, monoT5-3B.</p><p>BM25 + monoT5 answer (m3): We employ BM25 + monoT5 but we modify the query based on the answer field in the topic prior to neural reranking. We rephrase the question "Can X Y COVID-19?" (where X is a treatment and Y is one of five effect terms) to "X can Y COVID-19" if the answer field is "yes" and "X can not Y COVID-19" if the answer field is "no". The goal of this submission was to see if there are any improvements brought by aligning the query with the answer field.</p><p>BM25 + monoT5 + duoT5 (m4): We rerank the top-k 1 (=50) documents returned by BM25 + monoT5 using our pairwise reranker, duoT5-3B.</p><p>BM25 + monoT5 answer + duoT5 answer (m5): We rerank the top-k 1 (=50) documents returned by BM25 + monoT5 answer using our pairwise reranker, duoT5-3B. duoT5-3B uses the rephrased query as that in BM25 + monoT5 answer .</p><p>BM25 + monoT5 credible (m6): Same as BM25 + monoT5 answer except that while rephrasing we also prefix the query with the text "Clinical Studies, FDA, CDC, Health Officials, WHO or researchers say". Note that in our submission, we called this run m10 but from here on we refer to it as m6.</p><p>BM25 + monoT5 answer + LabelT5 (m7): We rerank the top-k 1 (=1000) document segments returned by BM25 + monoT5 answer using our label prediction model, LabelT5.</p><p>BM25 + Average of monoT5 answer and LabelT5 (m8): We take the mean of the scores of top-k 1 (=1000) documents returned by BM25 + monoT5 answer and the scores of the same documents by LabelT5.</p><p>BM25 + Linear Combination of monoT5 credible and monoT5 answer (m9): We take a linear combination of the score s answer from BM25 + monoT5 answer and the score s credible from BM25 + monoT5 credible . Then, we re-weight the top-1000 documents as s = 2s credible + s answer , where the weights 2 and 1 are arbitrarily selected so that we weigh credibility scores higher, while trying to retain some notion of standard relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Harmful and helpful compatibility scores are provided in Table <ref type="table" coords="7,355.74,244.55,3.66,8.64" target="#tab_2">3</ref>. First, we notice that all our systems score higher than the "median" helpful compatibility score. Pointwise reranking helps on top of the BM25 baseline (rows 1 and 2) and pairwise reranking helps on top of pointwise reranking (rows 2 and 4) as expected. The helpful compatibility score is also improved when the query is rephrased based on the alignment (rows 2-5). Interestingly, using LabelT5 results in a drop in the helpful compatibility scores (rows 3 and 7). This is perhaps because the amount of training data we have from the TREC 2019 Health Misinformation Track is limited. Averaging scores with the relevance classifier helps mitigate some of this (rows 3, 7 and 8). Using query prefix in m6 seems to have a detrimental effect compared to the standard alignment-based rephrasing (rows 3 and 6). Again in this case, the linear combination with the relevance classifiers scores helps bridge the gap (rows 3, 6 and 9). Yet, our zero-shot neural reranking pipeline m5 shines in terms of helpful compatibility scores.</p><p>Looking at harmful compatibility scores alone, a quantity we aim to minimize, we find the scores of m7 and m8 (rows 7 and 8) are much lower than those of the other systems. While having a low harmful compatibility score doesn't mean anything in isolation since a system that just outputs irrelevant information will have a score of 0, we note that both these runs have much higher than "median" helpful compatibility scores (the system that output irrelevant information would have a helpful compatibility score of 0 too, which isn't desirable at all). The improvement in harmful compatibility is perhaps due to the fact that the label prediction model is trained to specifically look out for misinformation and hence documents with incorrect information would rarely show up higher on the list. Averaging scores with the relevance classifier results in a very small loss in terms of harmful compatibility scores while largely bridging the gap in terms of helpful compatibility scores (rows 3, 7 and 8). Hence, m8 is a pretty notable run as it succeeds in finding helpful information while minimizing the exposure of incorrect information. Another trend we notice with harmful compatibility scores is that using the alignment information to rephrase queries helps a lot (rows 2 and 3 as well as rows 4 and 5).</p><p>In Table <ref type="table" coords="7,142.37,528.24,3.66,8.64" target="#tab_3">4</ref>, other metrics based on usefulness, correctness and credibility are provided. Here we again note that all our runs have better scores than the "median" run. Surprisingly, our zero-shot neural reranking pipeline m5 outperforms all other systems in terms of the official metrics in this set of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Precision Medicine Track</head><p>Like the Health Misinformation Track, we use the monoT5-3B and duoT5-3B models that were fine-tuned on MS MARCO passage ranking dataset then fine-tuned (again) on Med-MARCO since the task is in the biomedical domain. All inference experiments are run with n length = 10 and n stride = 5. We submitted a total of six runs to the Precision Medicine Track, which are described below.</p><p>BM25: This is our baseline bag-of-words retrieval, i.e., H 0 is the only stage of ranking. We index all abstracts in PubMed. We do not use their full texts. At inference time, we retrieve the top-k 0 (=1000) documents per query. We do not use doc2query expansions (stage H -1 ) in this track due to the high costs of expanding the PubMed corpus. BM25 + monoT5: We rerank the top-k 0 (=1000) documents retrieved by BM25 using our pointwise reranker, monoT5-3B. We transform queries into the natural language questions with the following template: "is treatment treatment effective for disease disease in patients with gene gene mutation?", where the treatment, disease and gene are the given keyword queries.</p><p>BM25 + monoT5 + duoT5: We rerank the top-k 1 (=50) documents returned by BM25+monoT5 using our pairwise reranker, duoT5-3B. The queries are transformed into natural language questions in the same way as BM25 + monoT5.</p><p>BM25 + monoT5 rct : Same as BM25 + monoT5, except prior to reranking we prepend the keywords "meta-analysis" and "randomized controlled trial (RCT)" to the query. The natural language queries are prepared in the same way as BM25 + monoT5. The primary goal here is to see if monoT5 can leverage these additional keywords to prioritize abstracts that belong to the top critical evidence tier according to the TREC 2020 Precision Medicine Track guidelines.</p><p>BM25 + monoT5 rct +duoT5 rct : Same as BM25 + monoT5 + duoT5, except prior to reranking we prepend the keywords "meta-analysis" and "randomized controlled trial (RCT)" to the query. The natural language queries are prepared in the same way as BM25 + monoT5. The goal of this run is to yet again see if duoT5 can leverage the same additional keywords described in the prior run.</p><p>BM25 + monoT5 e1 : We rerank the top-k 1 (=100) documents returned by BM25+monoT5 rct by penalizing the top documents from the BM25 + monoT5. Specifically, we re-weight the top 100 documents as s = 5s rct -s, where s is the score from BM25 + monoT5, s rct is the score from BM25 + monoT5 rct , and the weights 5 and 1 are arbitrarily chosen to downweight by a small amount the documents that are "relevant" yet show little or no critical evidence.</p><p>All of our submissions are zero-shot, i.e., our models are trained solely on MS MARCO passage ranking dataset and we do not adjust their hyperparameters based on any of the prior years' TREC Precision Medicine Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have described our submissions to Deep Learning, Health Misinformation, and Precision Medicine Tracks of TREC 2020. Our standard pipeline (docT5query + BM25 + monoT5 + duoT5) has previously demonstrated strong zero-shot transfer capabilities on various domains such as news articles (Robust04) <ref type="bibr" coords="10,188.29,131.09,11.75,8.64" target="#b5">[6]</ref> and COVID-related scientific articles (TREC-COVID) <ref type="bibr" coords="10,427.49,131.09,15.42,8.64" target="#b17">[18]</ref>. These results were once again confirmed on all three tracks, in which our pipeline achieved much better effectiveness than BM25 baselines as well as the median system performance, while being minimally adapted to these tasks.</p><p>In the Health Misinformation Track, we note that a label prediction model makes the system much less prone to retrieving harmful documents while minimally decreasing the number of helpful documents retrieved. Future work improving on robust systems of this sort is critical to better misinformation-free retrieval.</p><p>In the Precision Medicine Track, we see that a simple query expansion model that prepends words from the top evidence tier, helps improve the quality of the run returned by our pointwise neural reranker. Further exploration involving aggregating scores across queries designed for multiple evidence tiers as well as other fields like citation counts and publication type might be important in building better retrieval systems for the Precision Medicine Track.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,108.00,232.63,396.67,8.64;2,108.00,243.22,396.00,9.65;2,108.00,254.13,396.00,9.65;2,108.00,265.04,396.67,9.65;2,108.00,275.95,396.00,9.65;2,108.00,286.86,396.00,9.65;2,108.00,298.09,161.57,8.64"><head>1 Figure 1 :</head><label>11</label><figDesc>Figure1: Illustration of our multi-stage ranking architecture. Prior to indexing, we (optionally) perform document expansion, denoted H -1 . In stage H 0 , given a query q, the top-k 0 (= 5 in the figure) candidate documents R 0 are retrieved using BM25. In stage H 1 , monoT5 produces a relevance score s i for each pair of query q and candidate d i ∈ R 0 . The top-k 1 (= 3 in the figure) candidates with respect to these relevance scores are passed to stage H 2 , in which duoT5 computes a relevance score p i,j for each triple (q, d i , d j ). The final list of candidates R 2 is formed by reranking the candidates according to these scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,151.79,77.20,308.41,233.65"><head>Table 1 :</head><label>1</label><figDesc>Results on TREC 2020 Deep Learning Track Passage Ranking Task.</figDesc><table coords="5,173.07,77.20,263.36,233.65"><row><cell>Run</cell><cell>MAP</cell><cell cols="3">nDCG@10 nDCG@1K RR</cell><cell>R@1K</cell></row><row><cell>(0) Median</cell><cell cols="2">0.4413 0.6810</cell><cell>0.6631</cell><cell>0.8443</cell><cell>-</cell></row><row><cell>(1) p_bm25</cell><cell cols="2">0.2856 0.4796</cell><cell>0.5830</cell><cell cols="2">0.6585 0.7863</cell></row><row><cell>(2) p_bm25rm3</cell><cell cols="2">0.3019 0.4821</cell><cell>0.6046</cell><cell cols="2">0.6360 0.8217</cell></row><row><cell>(3) p_d2q_bm25</cell><cell cols="2">0.4074 0.6187</cell><cell>0.6840</cell><cell cols="2">0.7326 0.8452</cell></row><row><cell>(4) p_d2q_bm25rm3</cell><cell cols="2">0.4295 0.6172</cell><cell>0.7041</cell><cell cols="2">0.7424 0.8699</cell></row><row><cell>(5) p_bm25rm3_duo</cell><cell cols="2">0.5355 0.7583</cell><cell>0.7387</cell><cell>0.8759</cell><cell>-</cell></row><row><cell cols="3">(6) p_d2q_bm25_duo 0.5609 0.7837</cell><cell>0.7539</cell><cell>0.8798</cell><cell>-</cell></row><row><cell>(7) p_d2q_rm3_duo</cell><cell cols="2">0.5643 0.7821</cell><cell>0.7732</cell><cell>0.8798</cell><cell>-</cell></row><row><cell>Run</cell><cell>MAP</cell><cell cols="3">nDCG@10 nDCG@1K RR</cell><cell>R@1K</cell></row><row><cell>(0) Median</cell><cell cols="2">0.3902 0.5733</cell><cell>0.5859</cell><cell>0.9444</cell><cell>-</cell></row><row><cell>(1) d_bm25</cell><cell cols="2">0.3791 0.5271</cell><cell>0.5647</cell><cell cols="2">0.8521 0.8085</cell></row><row><cell>(2) d_bm25rm3</cell><cell cols="2">0.4006 0.5248</cell><cell>0.5726</cell><cell cols="2">0.8541 0.8260</cell></row><row><cell>(3) d_d2q_bm25</cell><cell cols="2">0.4230 0.5885</cell><cell>0.6115</cell><cell cols="2">0.9369 0.8403</cell></row><row><cell>(4) d_d2q_bm25rm3</cell><cell cols="2">0.4228 0.5407</cell><cell>0.5902</cell><cell cols="2">0.8147 0.8596</cell></row><row><cell>(5) d_bm25rm3_duo</cell><cell cols="2">0.5270 0.6794</cell><cell>0.6929</cell><cell>0.9476</cell><cell>-</cell></row><row><cell cols="3">(6) d_d2q_bm25_duo 0.5422 0.6934</cell><cell>0.7089</cell><cell>0.9476</cell><cell>-</cell></row><row><cell>(7) d_d2q_rm3_duo</cell><cell cols="2">0.5427 0.6900</cell><cell>0.7122</cell><cell>0.9476</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,146.74,327.79,318.52,8.64"><head>Table 2 :</head><label>2</label><figDesc>Results on TREC 2020 Deep Learning Track Document Ranking Task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,113.82,77.60,384.36,263.73"><head>Table 3 :</head><label>3</label><figDesc>Harmful and helpful compatibility scores on TREC 2020 Health Misinformation Track. Model CMAP co,cr CMAP us,cr CMAP co,us,cr nDCG us nDCG co nDCG cr nDCG all</figDesc><table coords="8,115.72,77.60,372.79,263.73"><row><cell></cell><cell></cell><cell>Model</cell><cell cols="3">COMP harmf ul COMP helpf ul</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(0) Median 0.0747</cell><cell>0.3337</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(1) m1</cell><cell>0.1197</cell><cell>0.3679</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(2) m2</cell><cell>0.1132</cell><cell>0.4404</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(3) m3</cell><cell>0.0750</cell><cell>0.5107</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(4) m4</cell><cell>0.1200</cell><cell>0.4658</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(5) m5</cell><cell>0.0800</cell><cell>0.5487</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(6) m6</cell><cell>0.0653</cell><cell>0.4832</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(7) m7</cell><cell>0.0150</cell><cell>0.4486</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(8) m8</cell><cell>0.0163</cell><cell>0.4900</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(9) m9</cell><cell>0.0747</cell><cell>0.5017</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(0) Median 0.1003</cell><cell>0.1717</cell><cell>0.1389</cell><cell>0.4699</cell><cell>0.3380</cell><cell>0.3308</cell><cell>0.4471</cell></row><row><cell>(1) m1</cell><cell>0.1911</cell><cell>0.2765</cell><cell>0.2369</cell><cell>0.6077</cell><cell>0.4997</cell><cell>0.5774</cell><cell>0.4853</cell></row><row><cell>(2) m2</cell><cell>0.2211</cell><cell>0.3149</cell><cell>0.2730</cell><cell>0.6387</cell><cell>0.5341</cell><cell>0.5945</cell><cell>0.5014</cell></row><row><cell>(3) m3</cell><cell>0.2534</cell><cell>0.3287</cell><cell>0.2915</cell><cell>0.6443</cell><cell>0.5620</cell><cell>0.6072</cell><cell>0.5306</cell></row><row><cell>(4) m4</cell><cell>0.2483</cell><cell>0.3402</cell><cell>0.2971</cell><cell>0.6596</cell><cell>0.5549</cell><cell>0.6185</cell><cell>0.5221</cell></row><row><cell>(5) m5</cell><cell>0.2898</cell><cell>0.3560</cell><cell>0.3187</cell><cell>0.6661</cell><cell>0.5901</cell><cell>0.6309</cell><cell>0.5609</cell></row><row><cell>(6) m6</cell><cell>0.2800</cell><cell>0.3105</cell><cell>0.2856</cell><cell>0.6281</cell><cell>0.5833</cell><cell>0.6065</cell><cell>0.5596</cell></row><row><cell>(7) m7</cell><cell>0.2423</cell><cell>0.2265</cell><cell>0.2216</cell><cell>0.5758</cell><cell>0.5638</cell><cell>0.5486</cell><cell>0.5212</cell></row><row><cell>(8) m8</cell><cell>0.2703</cell><cell>0.2633</cell><cell>0.2531</cell><cell>0.6018</cell><cell>0.5805</cell><cell>0.5725</cell><cell>0.5437</cell></row><row><cell>(9) m9</cell><cell>0.2733</cell><cell>0.3308</cell><cell>0.2974</cell><cell>0.6439</cell><cell>0.5774</cell><cell>0.6174</cell><cell>0.5553</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,107.69,362.78,397.85,8.64"><head>Table 4 :</head><label>4</label><figDesc>Usefulness, correctness and credibility results on TREC 2020 Health Misinformation Track.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,124.14,714.52,89.41,7.47"><p>http://anserini.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,124.14,704.20,379.86,7.77;4,108.00,714.16,325.09,7.77"><p>Note that increasing the length to 1024 tokens was sufficient in this case. However, for monoT5, such an increase would still not have been sufficient to perform inference on a complete document.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>Table <ref type="table" coords="9,133.51,440.69,5.08,8.64">5</ref> shows the results from Phase 1 (Relevance Assessment) Evaluation of the TREC 2020 Precision Medicine Track. The first row shows the median score of all submitted runs. Our BM25 baseline (row 1) is only slightly worse than monoT5 (row 2) and duoT5 (row 3) with respect to infNDCG, but it is better than both neural models in terms of P@10. Results in terms of RPrec are mixed.</p><p>The variants monoT5 rct (row 4), duoT5 rct (row 5), and monoT5 e1 (row 6) performed worse than their respective base models (rows 2 and 3). These results however are not surprising as Relevance Assessment relies heavily on matching exact keywords from the topics.</p><p>Table <ref type="table" coords="9,131.12,538.92,4.88,8.64">6</ref> shows the results from Phase 2 (Evidence Assessment) Evaluation of the TREC 2020 Precision Medicine Track. Here, we note that most of our submissions score way higher than the median submission. In this case, we see the same trend from the other tracks where pointwise reranking helps on top of the BM25 baseline (rows 1 and 2) and pairwise reranking helps on top of pointwise reranking (rows 2 and 3).</p><p>Our experimental submission (row 6) performed poorly compared to all our other systems. This is perhaps because we penalized too much based on our monoT5 scores (row 2).</p><p>Leveraging the additional keywords belonging to the top evidence tier in the TREC 2020 Precision Medicine Track guidelines clearly helps in the case of the pointwise reranker as we see an 8.5 point improvement in effectiveness (rows 2 and 4). While we still notice a gain in performance in the pairwise reranker case (rows 3 and 5), we see the unexpected result that duoT5 rct reranking the top segments from monoT5 rct results in a drop in effectiveness (rows 4 and 5). We believe that this is because the additional keywords confuse the pairwise reranker more than it would a pointwise reranker, since it requires comparison between the two documents on two dimensions: which document is more relevant for the topic while also comparing based on the type of study. This is something a pairwise model trained only on a relevance ranking dataset might not capture well.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,129.58,318.53,375.66,8.64;10,129.58,329.44,375.80,8.64;10,129.22,340.17,358.03,8.82" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Wang</forename><surname>Marco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m" coord="10,129.22,340.34,250.51,8.64">A human generated MAchine Reading COmprehension dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,358.54,375.67,8.82;10,129.58,369.62,22.42,8.64" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04833</idno>
		<title level="m" coord="10,273.18,358.71,148.58,8.64">Hierarchical Neural Story Generation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,387.81,374.42,8.82;10,129.58,398.72,339.08,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,302.10,387.99,137.19,8.64">Reformer: The efficient transformer</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,455.64,387.81,48.36,8.59;10,129.58,398.72,258.56,8.59">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ICLR 2020</note>
</biblStruct>

<biblStruct coords="10,129.58,417.09,374.42,8.82;10,129.27,428.00,122.99,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,171.29,417.27,169.97,8.64">Learning to Rank for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,348.68,417.09,155.32,8.59;10,129.27,428.00,34.98,8.59">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,446.55,374.59,8.64;10,129.58,457.28,268.47,8.82" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02365</idno>
		<title level="m" coord="10,312.82,446.55,191.35,8.64;10,129.58,457.46,160.48,8.64">SLEDGE: A Simple Yet Effective Baseline for COVID-19 Scientific Knowledge Search</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,475.83,376.07,8.64;10,129.58,486.56,188.54,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,311.52,475.83,194.13,8.64;10,129.58,486.74,61.83,8.64">Document ranking with a pretrained sequence-tosequence model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,210.36,486.56,77.00,8.59">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,505.11,271.70,8.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,226.42,505.11,143.86,8.64">From doc2query to docTTTTTquery</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,523.48,376.16,8.64;10,129.58,534.21,100.17,8.82" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08375</idno>
		<title level="m" coord="10,324.16,523.48,177.55,8.64">Document Expansion by Query Prediction</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,552.76,376.16,8.64;10,129.58,563.49,100.17,8.82" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11930</idno>
		<title level="m" coord="10,309.00,552.76,191.30,8.64">Scientific Claim Verification with VERT5ERINI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,582.04,374.41,8.64;10,129.22,592.77,293.01,8.82" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,275.35,582.04,228.64,8.64;10,129.22,592.95,184.62,8.64">The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05667</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,611.32,376.16,8.64;10,129.58,622.05,374.42,8.82;10,129.16,632.96,177.81,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,149.36,622.23,306.41,8.64">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,463.39,622.05,40.61,8.59;10,129.16,632.96,110.93,8.59">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,651.51,376.17,8.64;10,129.58,662.24,342.32,8.82" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,434.14,651.51,66.52,8.64">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,140.37,662.24,236.96,8.59">Proceedings of the 3rd Text REtrieval Conference (TREC-3)</title>
		<meeting>the 3rd Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.58,680.79,376.16,8.64;10,129.58,691.52,374.42,8.82;10,129.25,702.43,374.74,8.82;10,129.23,713.51,83.02,8.64" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,287.78,680.79,213.80,8.64">Self-Attention with Relative Position Representations</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,140.46,691.52,363.54,8.59;10,129.25,702.43,240.88,8.59">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s" coord="10,421.01,702.43,48.95,8.59">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,129.58,75.48,374.42,8.64;11,129.58,86.21,374.42,8.82;11,129.58,97.12,375.66,8.82;11,129.58,108.20,22.42,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,250.34,75.48,253.66,8.64;11,129.58,86.39,31.11,8.64">Anserini: Enabling the use of Lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,178.82,86.21,325.18,8.59;11,129.58,97.12,232.47,8.59">Proceedings of the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017)</title>
		<meeting>the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,129.58,127.08,374.42,8.64;11,129.58,137.81,374.42,8.82;11,129.22,148.72,253.99,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,250.34,127.08,253.66,8.64;11,129.58,137.99,32.35,8.64">Anserini: Enabling the use of Lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,180.65,137.81,323.35,8.59;11,129.22,148.72,150.21,8.59">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,129.58,167.60,374.42,8.82;11,129.58,178.51,226.92,8.82" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,246.67,167.78,218.62,8.64">Anserini: Reproducible ranking baselines using Lucene</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,473.41,167.60,30.59,8.59;11,129.58,178.51,128.51,8.59">Journal of Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,129.58,197.39,374.42,8.82;11,129.58,208.30,374.42,8.59;11,129.27,219.21,77.89,8.82" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,277.44,197.57,153.91,8.64">Critically Examining the &quot;Neural Hype</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,444.45,197.39,59.55,8.59;11,129.58,208.30,374.42,8.59;11,129.27,219.21,34.98,8.59">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,129.58,238.27,375.66,8.64;11,129.58,249.18,374.42,8.64;11,129.58,259.91,246.02,8.82" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07846</idno>
		<title level="m" coord="11,231.42,249.18,272.58,8.64;11,129.58,260.08,138.21,8.64">Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
