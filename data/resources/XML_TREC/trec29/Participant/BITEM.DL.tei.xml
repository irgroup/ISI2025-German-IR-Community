<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.08,164.85,329.10,15.12;1,286.11,186.77,39.03,15.12">SIB Text Mining at TREC 2020 Deep Learning Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-15">February 15, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.25,219.25,72.02,10.48"><forename type="first">Julien</forename><surname>Knafou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sciences and Arts of Western</orgName>
								<orgName type="institution" key="instit1">HES-SO</orgName>
								<orgName type="institution" key="instit2">University of Applied</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SIB</orgName>
								<orgName type="institution" key="instit2">Swiss Institute of Bioinformatics</orgName>
								<address>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.32,219.25,88.02,10.48"><forename type="first">Matthew</forename><surname>Jeffryes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sciences and Arts of Western</orgName>
								<orgName type="institution" key="instit1">HES-SO</orgName>
								<orgName type="institution" key="instit2">University of Applied</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SIB</orgName>
								<orgName type="institution" key="instit2">Swiss Institute of Bioinformatics</orgName>
								<address>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.81,219.25,84.45,10.48"><forename type="first">Sohrab</forename><surname>Ferdowsi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sciences and Arts of Western</orgName>
								<orgName type="institution" key="instit1">HES-SO</orgName>
								<orgName type="institution" key="instit2">University of Applied</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.31,233.20,67.31,10.48"><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sciences and Arts of Western</orgName>
								<orgName type="institution" key="instit1">HES-SO</orgName>
								<orgName type="institution" key="instit2">University of Applied</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SIB</orgName>
								<orgName type="institution" key="instit2">Swiss Institute of Bioinformatics</orgName>
								<address>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.08,164.85,329.10,15.12;1,286.11,186.77,39.03,15.12">SIB Text Mining at TREC 2020 Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-15">February 15, 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">4ABCA8E5BE86CC472ED3ECD57E345D14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This second campaign of the TREC Deep Learning Track was an opportunity for us to experiment with deep neural language models reranking techniques in a realistic use case. This year's tasks were the same as the previous edition: (1) building a reranking system and ( <ref type="formula" coords="1,408.89,417.23,3.93,7.86">2</ref>) building an end-to-end retrieval system. Both tasks could be completed on both a document and a passage collection. In this paper, we describe how we coupled Anserini's information retrieval toolkit with a BERT-based classifier to build a state-of-the-art end-to-end retrieval system. Our only submission which is based on a RoBERTa large pretrained model achieves for (1) a ncdg@10 of .6558 and .6295 for passages and documents respectively and for (2) a ndcg@10 of .6614 and .6404 for passages and documents respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Last year's TREC Deep Learning task was the first edition of the track where both traditional information retrieval tools and newer techniques involving machine learning performances were compared in a real-world scenario. This was the most prominent effort to benchmark information retrieval methods in very large scales. In particular, the aim was to see whether large data could make certain methods, perhaps those relying on deep learning, excel other methods used for other data regimes.</p><p>This year, the second edition is essentially the same task on the same collections (MS MARCO) <ref type="bibr" coords="1,236.32,654.63,78.68,8.74" target="#b0">[Bajaj et al., 2018]</ref>. It brought us a validation set which is last year's test set and the opportunity to look back at what has been done over the last year in and out of the competition.</p><p>For this campaign, we tried new information retrieval systems along with BERT-based models <ref type="bibr" coords="2,224.18,163.83,81.50,8.74" target="#b1">[Devlin et al., 2018]</ref>. This paper describes our methodology for our submission that achieves a ndcg of 0.7186 and 0.6389 for passage and document reranking, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>A seminal effort in language modeling is BERT, showing significant success in a wide range of NLP tasks. This is based on bi-directional transformers, and is pretrained with the tasks of masked language modeling, as well as next sentence prediction. Ever since BERT was introduced, several works tried to augment its performance. A successful work in this direction is RoBERTa <ref type="bibr" coords="2,403.65,290.32,69.58,8.74" target="#b4">[Liu et al., 2019]</ref>, using larger and more diverse corpora for pretraining, as well as removing the next sentence prediction task, but using a novel masked language modeling. While it needs larger compute power, it noticeably improves the performance of BERT across different tasks.</p><p>The information retrieval community has also been using BERT for the learning-to-rank task. In particular, on the MS MARCO database, the work of <ref type="bibr" coords="2,133.77,374.01,113.31,8.74" target="#b4">[Nogueira and Cho, 2019]</ref> uses BERT for passage reranking by concatenating the query and the passage, similar to the next sentence prediction setup, but by obtaining the probability of relevance of a passage to a query. Another recent effort is <ref type="bibr" coords="2,170.48,409.88,75.59,8.74" target="#b2">[Han et al., 2020]</ref> that uses more specialized learning-to-rank losses to fine-tune BERT. While this work also uses RoBERTa, when we started training our model, this was absent within the information retrieval communities. Our motivation was to see whether similar performance improvements for NLP tasks using RoBERTa can be observed also for reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks</head><p>The TREC 2020 Deep Learning Track is divided in two tasks; a document and a passage reranking tasks. Both of them are divided in two sub-tasks. The first one is the full ranking task where, for a given query, a ranked list of candidates needs to be retrieved in a corpus. For this, one needs to develop an end-to-end solution. The maximum returned list length is either 1000 passages or a 100 documents. The second sub-task is the top-k reranking task where a list of candidates is provided for each query by the track organizer and only a reranking is required. In this sub-task too, the number of candidates differ depending on the task; for the passages, a list of 1000 candidates is provided, while for documents, a list of 100 candidates is provided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>We can divide the provided data into four categories; first, the corpus where a huge collection of either passages or documents are listed, second, a list of queries mapped to either passages or documents in the collections, third, a dataset that not only map query to passages, but also tells if a passage is irrelevant to a given query, finally, the ORCAS click data that has been released at the end of the campaign (which we did not have the opportunity to exploit, due to the timing of its release). We used data from tables 1 and 2 only for the information retrieval part, the first step of our end-to-end solution. Table <ref type="table" coords="3,339.63,470.76,4.98,8.74" target="#tab_0">1</ref> shows the number of passages and documents in the corpus for a size of about 2.9GB and 22GB, respectively. Those corpora are the collections from which we had to retrieve candidates in the first sub-tasks.</p><p>Table <ref type="table" coords="3,176.44,518.58,4.98,8.74" target="#tab_1">2</ref> shows the number of queries for both documents and passages per split. The document queries map to a single document. Most passage queries map to a single passage, but in the development set, 12% map to more than one passage. The test (2020) split was released later during the campaign. We currently have only the queries and top-k candidates for this one, the mapping will come later.</p><p>Table <ref type="table" coords="3,176.31,590.31,4.98,8.74" target="#tab_2">3</ref> shows the number of both positive and negative passages linked to a set of queries. In order to get comparable results with <ref type="bibr" coords="3,382.65,602.26,75.35,8.74" target="#b2">[Han et al., 2020]</ref> and <ref type="bibr" coords="3,133.77,614.22,112.60,8.74" target="#b4">[Nogueira and Cho, 2019]</ref>, and also because the dataset size was already big enough, we only used the small version of it.</p><p>In table 4, we see the number of query-candidate pairs for passages and documents per split. We only used the validation set in order to assess our models and the test (2020) set for the submission of our run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Methodology</head><p>Our end-to-end method is based on a two-fold strategy. First, we use information retrieval methods in order to narrow down a given query to a limited amount of candidates. Then, we rerank the query-candidate pairs using a BERT-based classifier. The following subsection will describe in detail both steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Information retrieval</head><p>Initial document retrieval used conventional information retrieval methods to produce a smaller set of candidates to which more computationally intensive methods could be applied. We used the Anserini toolkit to insert the document and passage datasets into Lucene indexes, which were then queried to produce the candidate sets <ref type="bibr" coords="4,215.06,517.03,76.99,8.74" target="#b5">[Yang et al., 2018]</ref>. For the document retrieval task, we used BM25 with k = 3.44 and b = 0.87. On the development set, this gave recall at 1000 of 0.933 and on the validation set, this gave recall at 1000 of 0.681. For the passage retrieval task we used BM25 with k = 0.9 and b = 0.4. On the development set, this gave recall at 1000 of 0.857 and on the validation set, this gave recall at 1000 of 0.738.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reranking</head><p>As discussed in the data section, for the reranking part, we used the small version of query-positive and negative passages pairs training dataset. Table <ref type="table" coords="5,235.33,175.73,3.87,8.74">5</ref>: Fine-tuning setting for both models relevant to a query. In order to do that, we concatenate each query-passage pair with a separator token in-between and assigned a positive or a negative label to it whether the passage is relevant to a query or not. We focused our fine-tuning on two BERT-based pretrained models <ref type="bibr" coords="5,357.36,242.87,85.49,8.74" target="#b1">[Devlin et al., 2018]</ref> namely BERT base and RoBERTa large <ref type="bibr" coords="5,264.78,254.82,68.55,8.74" target="#b4">[Liu et al., 2019]</ref>. We fixed the maximum amount of tokens at 64 and 512 for the query length and the sequence concatenation length, respectively. We tried to maintained a certain balance of positive and negative passages for each batch. We used an Adam optimizer <ref type="bibr" coords="5,420.15,290.69,57.33,8.74;5,133.77,302.64,40.82,8.74" target="#b3">[Kingma and Ba, 2017]</ref> for both models.</p><p>Table <ref type="table" coords="5,176.73,314.60,4.98,8.74">5</ref> shows the main hyperparameters we set for this experiment. Due to the limited amount of resources during the first period of this experiment, we had to make decisions about the size of our training strategy. As this kind of model training is computationally intense, we first tried to train BERT base on only 640k of query-passage pairs. Once we were sure that the setup was working, we decided to train the RoBERTa large model on 12M pairs. In order to increase the batch size and decrease training time, we used mixed precision floating points (FP), this had an impact on the learning rate we decided to use as using a bigger rate could result in an unstable optimization.</p><p>As another strategy to reduce the memory requirements for fine-tuning the BERT models, we freezed the weights of the network and used the latent codes as fixed inputs. Instead of the usual one fully connected layer, this input was fed to a fully convolutional network and was trained for reranking using the binary cross-entropy loss. We noticed a very small drop in performance and did not report it in our submission. However, this can be an interesting future direction to economise on resource consumption by decoupling the BERT weights from those of the task-specific network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Passage reranking</head><p>Once the model is trained, each query-passage pair (candidate) is evaluated and each passage is assigned a score according to its probability of being a relevant passage with respect to a query. We then return a list of query-passage pairs ranked according to their scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Document reranking</head><p>For the document reranking, we first divide each document to a list of passages. We then evaluate each passage the same way we did with the passage reranking. The document score will be equal to the biggest passage score of a given querydocument pair. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and discussion</head><p>Table <ref type="table" coords="6,160.95,363.61,4.98,8.74">6</ref> shows our results for both BERT base and RoBERTa large models on the validation split using top-k candidates of the second sub-task. Looking at the ndcg@10, which was the main metric of last year's leaderboard, we can see that for both tasks, RoBERTa large obtains the best results by about 5 points in the passage task. Looking at the document results, it is for the moment unclear if the small difference between the models is due to the model or the way we handled the transition from passage to document (see previous section).</p><p>Table <ref type="table" coords="6,175.93,447.30,4.98,8.74" target="#tab_5">7</ref> shows the official results for each sub-task. Our end-to-end solution outperformed the reranking results which means that our retrieval solution was better than the baseline provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Looking at the results, we're unable to draw any concrete conclusion, other than that a BERT-based reranker is an easy to implement tool that improves top-k metrics when compared to traditional information retrieval systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,165.98,126.76,279.29,181.58"><head>Table 1 :</head><label>1</label><figDesc>Number of documents and passages in the both corpus</figDesc><table coords="3,216.31,126.76,178.63,181.58"><row><cell></cell><cell></cell><cell cols="2"># of items</cell></row><row><cell cols="2">Passages</cell><cell cols="2">8,841,823</cell></row><row><cell cols="2">Documents</cell><cell cols="2">3,213,835</cell></row><row><cell></cell><cell>Split</cell><cell></cell><cell># of Queries</cell></row><row><cell></cell><cell>Train</cell><cell></cell><cell>808,731</cell></row><row><cell>Passages</cell><cell cols="2">Dev Validation</cell><cell>101,093 101,092</cell></row><row><cell></cell><cell cols="2">Test (2020)</cell><cell>200</cell></row><row><cell></cell><cell>Train</cell><cell></cell><cell>367,013</cell></row><row><cell>Documents</cell><cell cols="2">Dev Validation</cell><cell>5,193 200</cell></row><row><cell></cell><cell cols="2">Test (2020)</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,188.40,321.47,234.45,8.74"><head>Table 2 :</head><label>2</label><figDesc>Number of passages' and documents' queries</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,135.51,126.76,340.23,193.53"><head>Table 3 :</head><label>3</label><figDesc>Number of positive and a negative passages linked to a set of queries</figDesc><table coords="4,191.25,126.76,228.75,193.53"><row><cell>Dataset</cell><cell cols="2"># query-positive &amp; negative passage pairs</cell></row><row><cell cols="2">Train small</cell><cell>39,780,811</cell></row><row><cell cols="2">Train large</cell><cell>397,756,691</cell></row><row><cell></cell><cell>Split</cell><cell># query-candidate pairs</cell></row><row><cell></cell><cell>Train</cell><cell>478,002,393</cell></row><row><cell>Passages</cell><cell>Dev Validation</cell><cell>6,668,967 189,877</cell></row><row><cell></cell><cell>Test (2020)</cell><cell>190,699</cell></row><row><cell></cell><cell>Train</cell><cell>36,701,116</cell></row><row><cell>Documents</cell><cell>Dev Validation</cell><cell>519,300 20,000</cell></row><row><cell></cell><cell>Test (2020)</cell><cell>20,000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,204.13,333.42,202.99,8.74"><head>Table 4 :</head><label>4</label><figDesc>Number of query and candidate pairs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,133.77,126.76,352.05,193.88"><head>Table 7 :</head><label>7</label><figDesc>Official results</figDesc><table coords="6,133.77,126.76,352.05,172.01"><row><cell cols="2">Model name</cell><cell>RR</cell><cell cols="2">ndcg@10</cell><cell>MAP</cell><cell>R@100</cell></row><row><cell>Passages</cell><cell cols="2">BERT base RoBERTa large 0.9291 0.9360</cell><cell cols="2">0.6677 0.7107</cell><cell>0.4312 0.4644 0.5148 0.4852</cell></row><row><cell>Documents</cell><cell cols="2">BERT base RoBERTa large 0.9244 0.9070</cell><cell cols="2">0.6456 0.6566</cell><cell>0.2846 0.3871 0.2863 0.3871</cell></row><row><cell cols="6">Table 6: Results on the validation set using provided IR; 1000 candidates per</cell></row><row><cell cols="6">query and 100 candidates for passage and document respectively</cell></row><row><cell>Run type</cell><cell>RR</cell><cell cols="4">ndcg@100 ndcg@10</cell><cell>MAP</cell><cell>R@100</cell></row><row><cell>Passages</cell><cell>reranking end-to-end 0.8769 0.8635</cell><cell cols="2">0.6587 0.6816</cell><cell cols="2">0.7169 0.7192</cell><cell>0.4823 0.4990 0.7438 0.6986</cell></row><row><cell>Documents</cell><cell>reranking end-to-end 0.9365 0.9185</cell><cell cols="2">0.6060 0.6390</cell><cell cols="2">0.6295 0.6404</cell><cell>0.4199 0.4423 0.6410 0.5985</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.26,604.65,339.22,8.74;6,144.28,616.61,333.19,8.74;6,144.28,628.56,333.20,8.74;6,144.28,640.52,221.87,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,386.68,628.56,90.80,8.74;6,144.28,640.52,217.57,8.74">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><surname>Bajaj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.77,127.96,343.71,8.74;7,144.28,139.92,333.20,8.74;7,144.28,151.87,146.96,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,144.28,139.92,333.20,8.74;7,144.28,151.87,35.47,8.74">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><surname>Devlin</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.77,171.80,343.72,8.74;7,144.28,183.75,178.47,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,144.28,183.75,73.53,8.74">Learning-to-rank</title>
		<author>
			<persName coords=""><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>with bert in tf-ranking</note>
</biblStruct>

<biblStruct coords="7,133.77,203.68,343.71,8.74;7,144.28,215.63,103.61,8.74" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ba ;</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<title level="m" coord="7,386.05,203.68,91.43,8.74;7,144.28,215.63,99.18,8.74">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.77,235.56,343.71,8.74;7,144.28,247.51,333.20,8.74;7,144.28,259.47,277.75,8.74;7,133.77,279.39,343.71,8.74;7,144.28,291.35,160.05,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,389.53,247.51,87.95,8.74;7,144.28,259.47,165.99,8.74">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName coords=""><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019. 2019</date>
			<publisher>Nogueira and Cho</publisher>
		</imprint>
	</monogr>
	<note>Passage re-ranking with BERT. CoRR, abs/1901.04085</note>
</biblStruct>

<biblStruct coords="7,133.77,311.27,343.71,8.74;7,144.28,323.23,333.19,8.74;7,144.28,335.18,25.46,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,402.35,311.27,75.13,8.74;7,144.28,323.23,169.37,8.74">Anserini: Reproducible ranking baselines using lucene</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,326.42,323.23,146.73,8.74">J. Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
