<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.85,83.94,295.40,12.90">DCU-ADAPT at the TREC 2020 Podcasts Track</title>
				<funder ref="#_6hNpY7H">
					<orgName type="full">Science Foundation Ireland as part of the ADAPT Centre</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,211.76,113.05,79.13,10.29"><forename type="first">Yasufumi</forename><surname>Moriya</surname></persName>
							<email>yasufumi.moriya@adaptcentre.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.33,113.05,85.99,10.29"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gareth.jones@adaptcentre.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.85,83.94,295.40,12.90">DCU-ADAPT at the TREC 2020 Podcasts Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32909EC70058C4168E6D71C7E42E5E85</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe DCU-ADAPT's participation in the TREC 2020 Podcasts Track. We participated in the ad-hoc segment retrieval task. The goal of the task was to search for fixedlength segments from a large archive of podcasts which contain good jump-in points to relevant content for a given query topic. The challenge of retrieving relevant segments with good jump-in points at high rank is made more difficult by the presence of transcription errors in transcripts created using automatic speech recognition. We investigated three query expansion techniques designed overcome this issue. Our first approach was to extract nouns and named entities from the query description provided with each query and to add them to the corresponding query. Our second approach was to retrieve documents for the query using a commercial online web search engine and add selected words from the web documents to the query. Our final approach was to select words to expand the query using a pseudo-relevance feedback method and Word-Net. Combining the above approaches for query expansion, we achieved a normalised discounted cumulative gain (nDCG) value of 0.586.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The growing multimedia archives available on the Internet are increasing the demand for effective tools to enable efficient engagement with this content, including tools for search and summarisation. While tools for search of online text document archives are well advanced, the functionality of tools for spoken content remains very limited. For example, enabling search only using titles and metadata, users may though be interested in a particular relevant region of a spoken document. More sophisticated functionality of the search system, to enable such regions to be identified requires what is actually said to be determined and for this transcript to be used within an information access system. The ad-hoc segment retrieval task of the TREC 2020 Podcasts Track provided a challenge to promote work into facilitating effective search of errorful automatic speech recognition (ASR) created podcast transcripts. Participants were asked to find good "jump-in" points relevant for a given query rather than to retrieve whole podcasts relevant to a query.</p><p>Standard spoken content retrieval systems use speech transcripts created by an ASR system with a conventional text retrieval algorithm operating on the speech transcripts <ref type="bibr" coords="1,244.56,640.60,9.52,7.77" target="#b1">[1]</ref>. One of the issues of this approach is caused by the errors contained in the speech transcripts. Even when query words are spoken in a document, a search system may be unable to identify a relevant document due to the absence of the query word from the ASR created textual representation of the document. An example from the data provided for the Podcasts is for the query "Daniel Ek interview". The provided ASR transcript system mis-transcribed the family name "Ek" to "eck" or "ech", and there was no instance of "Ek" in the provided collection of speech transcripts. Another issue not only limited to spoken search but also to textual search is the possibility of semantic mismatch between query words and documents. When a given search query is broader than the corresponding expressions used in relevant documents, the retrieved documents may not satisfy user information needs. An example found in the podcasts data is the query "story about riding a bird". Despite the query word "bird", target documents relevant to this query contain a story about riding a "seagull".</p><p>In order to overcome these issues, we investigated query expansion for ad-hoc podcast retrieval using three different resources. The goal of query expansion is to add terms to the original query which are useful for discovering relevant documents. The first resource we found to be useful was the query descriptions provided with the task queries. These are longer descriptions of the information need. Some terms found in the description can provide more detail to the original queries increasing the likelihood that they will match with relevant content. The second resource we investigated was an external web search engine. We retrieved web pages for each of the provided queries, and extracted potentially useful query expansion terms from the web pages. These terms were then added to the original queries. The third resource was WordNet which captures semantic relationships between words. This approach was used to mitigate the semantic mismatch between query words and words contained in relevant documents. Overall, by combining all of the above approaches for query expansion, our best run achieved an nDCG value of 0.586.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Description</head><p>In this section, we outline the TREC 2020 Podcasts Track <ref type="bibr" coords="1,529.03,504.88,10.45,7.77" target="#b2">[2]</ref> and the podcasts dataset provided for the track by Spotify <ref type="bibr" coords="1,526.79,515.29,9.52,7.77" target="#b3">[3]</ref>. The TREC 2020 Podcasts Track offered two tasks: ad-hoc segment retrieval and summarisation. The goal of the ad-hoc segment retrieval task was to find good starting points of podcast episodes given a user search query. The goal of the summarisation task was to produce a text snippet of given audio and a speech transcript. The DCU-ADAPT team participated in the ad-hoc segment retrieval task.</p><p>The podcasts dataset contains approximately 100,000 episodes of podcasts. Participants were provided with automatic transcripts of all of the episodes of podcasts. Along with the automatic transcripts, raw audio files of all of the episodes were available to participants. For the retrieval task, participants were provided with 8 training queries accompanied by a set of relevance judgements for development and assessment of retrieval systems. The provided relevance judgements evaluate relevance of documents on a 0-4 value scale. The submitted runs were evaluated on 50 test queries. The types of queries vary from queries looking for a specific named entity "Daniel Ek interview" to more general queries "gaslighting".</p><p>In the ad-hoc segment retrieval task, participants were asked to return a maximum of 1,000 podcast segments for each query. The relevance of these segments was rated based on their relevance to the information need expressed in the query and whether they provided a good "jump-in" point for the given query. The length of each segment was requested by the task organisers to be 2 minutes (120 seconds). The offset of each segment needed to be at multiples of 60 (i.e., 0, 60, 120). The submitted systems were evaluated according to normalised discounted cumulative gain (nDCG). While mean average precision (MAP) is a popular metric for information retrieval systems, nDCG can take account of the order of retrieved documents (i.e., some documents are more relevant and should be ranked higher than less relevant documents).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System Description</head><p>In this section, we describe our system architecture and the query expansion approaches that we investigated to address for this task, focusing particularly on methods which seek to address transcription errors and semantic mismatches between queries and transcripts as discussed in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data pre-processing</head><p>As the task requires participants to submit 120 seconds segments of podcasts whose offset should be multiples of 60, the provided ASR transcripts were segmented into 2 minute chunks. This means that there was an overlap between one transcript segment and the next one (e.g., segment offset 0 and segment offset 60). Words spanning across two segments (e.g., start time is 59.5 and end time is 60.5) were assigned to the first segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Indexing and weight model</head><p>We used PyTerrier to index the data and perform search over the indexed data <ref type="bibr" coords="2,106.02,435.62,9.52,7.77" target="#b4">[4]</ref>. We applied the divergence from randomness model using Popper's normalisation (DPH) to the collection to rank podcast segments <ref type="bibr" coords="2,143.06,456.44,9.52,7.77" target="#b5">[5]</ref>. The advantage of the DPH model over a popular retrieval model such as BM25 <ref type="bibr" coords="2,218.17,466.85,10.45,7.77" target="#b6">[6]</ref> is that the DPH model is parameter-free whereas hyper-parameters need to be tuned to operate alternative models such as BM25. Empirically, we found a small difference between DPH and BM25 models when these models were evaluated on the training queries and judgements provided for the task. The DPH model was applied to the collection with the Bo1 query expansion model <ref type="bibr" coords="2,253.45,529.32,9.52,7.77" target="#b5">[5]</ref>. Bo1 is a query expansion model which uses pseudo-relevance feedback to assign weights to each query term. The DPH model ranks podcast segments and the Bo1 model re-assigns a weight to query terms using pseudo-relevance feedback. This was our basic retrieval model, and was also applied with the extended queries which contain both original query terms and additional query terms found by the expansion approaches introduced in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Query Expansion using Additional Resources</head><p>As discussed in Section 1, transcription errors contained in podcasts and semantic mismatches between query words and documents can be an obstacle to effective search. To address these issues, we investigated query expansion approaches using query descriptions, a web search engine and WordNet. Query expansion can increase the possibility of documents being found compared to the original queries. The three approaches we employed are illustrated in Figure <ref type="figure" coords="2,171.75,723.89,3.36,7.77" target="#fig_0">1</ref>. Each of these has the potential to discover important query terms and can complement each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Query Description</head><p>The provided training and test queries are accompanied by a description field giving more details of each information need. We noticed that some of the words included in these descriptions might be useful to match with relevant documents. We applied a part-of-speech tagger and a named entity recogniser to the query descriptions using SpaCy<ref type="foot" coords="2,403.59,349.43,2.99,5.18" target="#foot_0">1</ref> . Identified nouns or/and named entities were added to the original query as additional search terms. While nouns from the query descriptions alone could be useful information for queries (e.g., "Wuhan" for the query "Coronavirus spread"), the named entity recogniser can extract dates and years (e.g., "2019" for the "Coronavirus spread"). It should be noted that all of the nouns and named entities identified by the SpaCy model are added to the original queries. This approach is referred in our experiments to as "nouns" and "entities".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Use of a Web Search Engine</head><p>While query descriptions may contain some useful terms for search operations, the number of terms available is limited. Using a public web search engine allows a query expansion approach to find important terms from topically related documents on the Internet. We retrieved 10 web pages for each query using the Google search API. 10 pages were used since this is the stand setting ot the Google search API. Adopting a pseudo relevance approach, we assumed that all of the retrieved 10 pages were relevant to the original query, and calculated Robertson's offer weight for each term t(i) in the 10 web pages. To obtain the offer weight of a term, a relevance weight rw(i) needs to be computed first:</p><formula xml:id="formula_0" coords="2,318.57,613.36,220.92,19.75">rw(i) = log (r(i) + 0.5)(N -n(i) -R + r(i) + 0.5) (n(i) -r(i) + 0.5)(R -r(i) + 0.5)<label>(1)</label></formula><p>where n(i) was the number of podcast segments term t(i) occurs in, N was the total number of podcast segments, r(i) was the number of known relevant documents term t(i) occurs in and R was the number of known relevant documents, in our case the 10 assumed relevant documents retrieved usng the Google search API. The offer weight of a term t(i) was then computed by:</p><formula xml:id="formula_1" coords="2,385.36,716.13,154.13,8.06">ow(i) = r(i) × rw(i)<label>(2)</label></formula><p>The potential expansion terms were ranked according to the offer weight. For each query, the top 5 terms according to the offer weight were added to the original query. The number of expansion terms was determined by nDCG values on the training queries. This approach is referred in our investigation to as "web search".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Use of WordNet</head><p>The query extension approach using a web search engine is effective when the original query contains specific terms in particular named entities, while it is hard to obtain informative terms from a generic query (e.g., "story about riding a bird"). Our third approach aimed to obtain hypernyms and hyponyms of a query term using pseudo-relevance feedback. A collection of podcast segments was first ranked by the DPH model using Bo1 query expansion. The top 50 ranked documents for each query were assumed to be relevant. Then, a part-of-speech tagger was applied to the queries and to the pseudo relevant documents to retain only nouns. To compute relevance of query nouns and nouns in the pseudo relevant document collection, Wu-Palmer similarity <ref type="bibr" coords="3,95.25,292.87,10.45,7.77">[7]</ref> was applied to the two nouns. The Wu-Palmer similarity can measure semantic closeness of two words exploiting the graph structure of WordNet. When the similarity score of a noun exceeds 0.8, the word was assumed to be relevant to a query noun. Finally, the terms semantically relevant to the queries were ranked according to the Robertson offer weight (Section 3.3.2). Terms were added to queries when their weight was higher than 5.0. All of the parameters mentioned for this approach were determined by examining nDCG values on the training queries. This approach is referred to as "WordNet".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Run descriptions</head><p>We submitted 5 runs combining the query expansion approaches introduced in the previous sections. Combination of two or more query expansion methods was performed by adding all of the expansion terms to the original queries and applying the DPH model with the Bo1 query expansion to the extended queries.</p><p>• Run1: nouns and WordNet </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and analysis</head><p>Table <ref type="table" coords="3,79.53,598.96,4.48,7.77" target="#tab_1">1</ref> shows the results of our submitted 5 runs. Overall, Run 5 was the best submission achieving and nDCG value of 0.586, followed by Run 4 with 0.581. The results also show that query expansion using web search (Run 4 and Run 5) boosted nDCG compared to the other 3 runs. The result overall show the positive impact of using WordNet expansion. This is an interesting result, since query expansion using WordNet is generally not found to be effective for IR. However, out ow(i) term selection method appears to work well in combination with the potential expansion terms found in WordNet, and we will investigate this in more detail in further work. Examining the results for Run1, Run 2 and Run 3 in comparison, it is not clear whether the noun expansion method makes a positive contribution, and further experimentation is needed to better understand these results. Participants were provided with the summary table of submission runs from all participants. This shows minimum, median and maximum nDCG values of each query. Comparing our best Run 5 to the median values, it turned out that our Run 5 produced an nDCG value higher than the median for 36 out of 50 test queries. Furthermore, Run 5 achieved the highest nDCG values of all the submitted systems for query IDs 11, 12, 18, 52 and 55. These queries are "how to cook turkey", "Imran Khan career", "Women in STEM", "Fauci interview" and "Malcolm X biography". As can be seen in Table <ref type="table" coords="3,456.22,486.97,3.36,7.77" target="#tab_2">2</ref>, higher nDCG values from Run 4 and Run 5 show that query expansion using the web search engine was beneficial for queries 11, 18, 52 and 55. The query expansion method found terms, for example, "oven" and "gravy" for the topic "how to cook turkey", "coronavirus" and "Anthony (the first name of Fauci)" for the topic "Fauci interview" and "Mecca" and "Muhammad" for the topic "Malcolm X biography". Query 12 is an example where Run 1, Run 3 and Run 5 were better than Run 2 and Run 4 indicating that terms found from the WordNet were useful. For the topic "Imran Khan career", terms such as "job" and "assistance" were discovered as important terms to be added to the query.</p><p>The query expansion approach using the web search engine, however, did not always bring benefit to the search system. There are 4 queries whose nDCG value for Run 5 was 0.1 or lower than that of Run 3, indicating that adding terms from the web pages had a negative impact on ranked documents in these cases. These queries are "causes and prevention of wild fires (query 16)", "Cryptocurrency risks (query 36)", "Missouri quilt mom (query 46)" and "gaslighting (query 56)". Despite the lower nDCG score, the terms obtained from web search seem to be valid. For example, the terms such as "bitcoin" and "currencies" were extracted for the query "cryptocurrency risks" and "victim" and "sanity" for the query "gaslighting".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we described the DCU-ADAPT team participation in the TREC 2020 Podcasts Track. The Podcasts Track offered an ad-hoc segment retrieval task and a summarisation task. We participated in the ad-hoc segment retrieval task and explored the effects of using external sources for query expansion. The first query expansion method extracted nouns and named entities from query descriptions and adds them to the original queries. The second approach exploited an external web search engine to retrieve relevant documents, and extracted important terms from the web documents. The third approach used pseudo-relevance feedback and extracts relevant terms from WordNet as potential expsnsion terms. Our best run combining all of the above query expansion methods achieved 0.586 nDCG. Our analysis revealed that certain terms discovered by query expansion using the web search engine were particularly effective on ranking podcast segments, although this approach had a negative impact on more generic queries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,312.72,227.73,226.78,7.93;2,312.72,238.14,29.30,7.72;2,312.72,73.99,226.77,142.48"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Query expansion approaches using external resources.</figDesc><graphic coords="2,312.72,73.99,226.77,142.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,342.67,75.29,166.86,81.15"><head>Table 1 :</head><label>1</label><figDesc>nDCG results of our 5 submitted runs</figDesc><table coords="3,393.98,96.21,62.01,60.23"><row><cell cols="2">Run ID nDCG</cell></row><row><cell>Run 1</cell><cell>0.570</cell></row><row><cell>Run 2</cell><cell>0.551</cell></row><row><cell>Run 3</cell><cell>0.569</cell></row><row><cell>Run 4</cell><cell>0.581</cell></row><row><cell>Run 5</cell><cell>0.586</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,312.72,187.04,226.77,164.84"><head>Table 2 :</head><label>2</label><figDesc>nDCG results of 9 query IDs with interesting differences between the submitted runs. Our Run 5 scored the highest of all submitted systems on the first 5 queries. On the bottom 4 queries, adding terms found in the web documents decreased nDCG values by more than 0.1.</figDesc><table coords="3,323.86,249.61,202.25,102.27"><row><cell cols="6">Query ID Run 1 Run 2 Run 3 Run 4 Run 5</cell></row><row><cell>11</cell><cell>0.605</cell><cell cols="3">0.605 0.605 0.74</cell><cell>0.74</cell></row><row><cell>12</cell><cell>0.765</cell><cell cols="3">0.618 0.765 0.638</cell><cell>0.772</cell></row><row><cell>18</cell><cell>0.55</cell><cell>0.55</cell><cell cols="2">0.554 0.623</cell><cell>0.638</cell></row><row><cell>52</cell><cell>0.302</cell><cell cols="3">0.302 0.302 0.861</cell><cell>0.861</cell></row><row><cell>55</cell><cell>0.45</cell><cell cols="2">0.576 0.45</cell><cell>0.845</cell><cell>0.856</cell></row><row><cell>16</cell><cell>0.697</cell><cell cols="3">0.706 0.711 0.596</cell><cell>0.569</cell></row><row><cell>36</cell><cell>0.686</cell><cell cols="3">0.686 0.686 0.569</cell><cell>0.569</cell></row><row><cell>46</cell><cell>0.903</cell><cell cols="3">0.903 0.903 0.778</cell><cell>0.778</cell></row><row><cell>56</cell><cell>0.728</cell><cell cols="3">0.728 0.728 0.284</cell><cell>0.284</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,327.07,734.95,47.85,6.91"><p>https://spacy.io</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>This work was supported by <rs type="funder">Science Foundation Ireland as part of the ADAPT Centre</rs> (Grant <rs type="grantNumber">13/RC/2106</rs>) at <rs type="institution">Dublin City University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6hNpY7H">
					<idno type="grant-number">13/RC/2106</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,139.63,341.66,67.20,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,71.87,358.92,212.50,6.91;4,71.88,367.77,212.49,7.05;4,71.88,376.76,167.53,7.05" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,169.89,358.92,114.49,6.91;4,71.88,367.91,97.59,6.91">Spoken Content Retrieval: A survey of techniques and technologies</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,178.17,367.77,106.20,6.86;4,71.88,376.76,52.11,6.86">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="235" to="422" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,71.87,389.87,212.50,6.91;4,71.88,398.87,212.50,6.91;4,71.88,407.72,212.50,7.05;4,71.88,416.71,78.22,7.05" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,217.41,398.87,66.96,6.91;4,71.88,407.86,49.78,6.91">TREC 2020 Podcasts Track Overview</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in The 29th Text Retrieval Conference (TREC) notebook. NIST, 2020</note>
</biblStruct>

<biblStruct coords="4,71.87,429.82,212.50,6.91;4,71.88,438.82,212.50,6.91;4,71.88,447.67,212.50,7.05;4,71.88,456.66,212.50,6.86;4,71.88,465.65,183.77,7.05" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bonab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<title level="m" coord="4,75.56,447.81,182.91,6.91;4,71.88,456.66,212.50,6.86;4,71.88,465.65,105.17,6.86">Proceedings of the 28th International Conference on Computational Linguistics (COLING 2020</title>
		<meeting>the 28th International Conference on Computational Linguistics (COLING 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5903" to="5917" />
		</imprint>
	</monogr>
	<note>100,000 Podcasts: A Spoken English Document Corpus</note>
</biblStruct>

<biblStruct coords="4,71.87,478.77,212.50,6.91;4,71.88,487.62,212.50,7.05;4,71.88,496.61,37.86,7.05" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,184.86,478.77,99.52,6.91;4,71.88,487.76,119.95,6.91">Declarative experimentation in information retrieval using PyTerrier</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,211.73,487.62,72.64,6.86;4,71.88,496.61,14.35,6.86">Proceedings of ICTIR 2020</title>
		<meeting>ICTIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,71.87,509.72,212.50,6.91;4,71.88,518.72,212.50,6.91;4,71.88,527.57,212.50,7.05;4,71.88,536.70,81.69,6.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,189.82,509.72,94.55,6.91;4,71.88,518.72,212.50,6.91;4,71.88,527.71,12.47,6.91">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,93.01,527.57,161.25,6.86">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,71.87,549.67,212.50,6.91;4,71.88,558.53,212.50,7.05;4,71.88,567.66,89.66,6.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,118.82,558.67,55.82,6.91">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,186.44,558.53,80.97,6.86">NIST special publication</title>
		<imprint>
			<biblScope unit="issue">500225</biblScope>
			<biblScope unit="page" from="109" to="123" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,71.87,580.63,212.50,6.91;4,71.88,589.49,212.50,6.86;4,71.88,598.48,131.52,7.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,150.45,580.63,119.12,6.91">Verbs semantics and lexical selection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,71.88,589.49,212.50,6.86;4,71.88,598.48,68.66,6.86">Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 32nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
