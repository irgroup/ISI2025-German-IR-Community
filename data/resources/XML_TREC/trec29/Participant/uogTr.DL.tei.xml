<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.42,84.23,327.16,15.44;1,179.52,104.15,252.95,15.44">University of Glasgow Terrier Team at the TREC 2020 Deep Learning Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,156.10,134.27,54.18,10.59"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
							<email>x.wang.8@research.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.63,134.27,59.62,10.59"><forename type="first">Yaxiong</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.66,180.70,80.66,10.59"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,446.96,180.70,53.61,10.59"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.42,84.23,327.16,15.44;1,179.52,104.15,252.95,15.44">University of Glasgow Terrier Team at the TREC 2020 Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A88169A4B390066584E1E44F91233DF0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our submission to the document ranking task of the TREC 2020 Deep Learning Track. We followed a three-stage architecture: candidate set retrieval, feature calculation and reranking using a learning-to-rank technique. In particular, in the feature calculation stage, we leverage the traditional information retrieval document weighting models and the deep contextualised language models to provide the features for the learning-to-rank technique in the final stage. We submitted three runs for the document ranking task: uogTr31oR, uogTrQCBMP and uogTrT20 and six baseline runs with no neural re-ranking techniques applied. Among our submitted runs, run uogTrQCBMP, which combines query expansion, ColBERT neural ranking and MaxPassage, was the most effective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The University of Glasgow Terrier team participated in the TREC 2020 Deep Learning track, in order to improve the effectiveness and flexibility of our new PyTerrier <ref type="bibr" coords="1,181.11,423.76,10.42,7.94" target="#b8">[9]</ref> information retrieval toolkit for an adhoc ranking task on a large corpus of Web documents. PyTerrier leverages Python to allow the easy expression of complex retrieval pipelines. In our participation, we addressed the document ranking task of the Deep Learning Track, but without using the provided initial rankings. We followed a three-stage framework: candidate set retrieval, feature calculation and re-ranking using a learning-to-rank technique. In the first stage, we performed the candidate set retrieval using DPH, DPH with Bo1 query expansion and a T5 <ref type="bibr" coords="1,86.84,522.39,15.11,7.94" target="#b13">[13]</ref>-based query expansion technique individually on the index created using Terrier. During the feature calculation, we incorporated traditional retrieval models as well as deep contextualised language models, such as BERT ùê∂ùêøùëÜ <ref type="bibr" coords="1,188.67,555.26,13.49,7.94" target="#b11">[11]</ref>, ColBERT <ref type="bibr" coords="1,244.74,555.26,10.68,7.94" target="#b5">[6]</ref> as well as SciBERT variants, to calculate the feature values. Finally, the Lamb-daMART <ref type="bibr" coords="1,90.25,577.18,10.68,7.94" target="#b0">[1]</ref> learning-to-rank technique was used to obtain the final score of each ranked document based on its various features.</p><p>The structure of the remainder of this paper is structured as follows: Section 2 discusses our indexing setup; Section 3 introduces the notions of retrieval pipelines in PyTerrier. Section 4 describes the three-stage architecture of our approach, including the candidate set retrieval followed by the feature calculation and learning-to-rank stage. Both the baseline runs and the submitted runs are detailed in Section 5. Section 6 highlights our results. Concluding remarks follow in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INDEXING</head><p>We used the same indices for MSMARCO as we created for TREC 2019 <ref type="bibr" coords="1,72.97,723.38,13.45,7.94" target="#b15">[15]</ref>. In particular, for these indices, we chose not to use the TREC-formatted version of the MSMARCO corpus, but instead, reformatted the CSV files into TREC files such that the URL &amp; title are clearly delineated. Next, we used several indexing configurations:</p><p>‚Ä¢ Positions: We recorded positional information.</p><p>‚Ä¢ Fields: We separately recorded the frequencies of terms occurring in different parts of the document. In particular, we recorded the 'TITLE', 'BODY' and 'URL' fields, following our past participations in the TREC Web track <ref type="bibr" coords="1,498.94,305.68,13.36,7.94" target="#b9">[10]</ref>. ‚Ä¢ Stemming &amp; Stopwords: We did not apply stemming nor remove stopwords. In all cases, we used the standard Terrier indexing configuration to create an inverted index, and a direct index to support query expansion and other retrieval techniques, as well as recording the raw text of the URLs, titles and contents of the documents as metadata, to allow deep learning upon these textual representations, as discussed further in Section 4.2 below. For the document contents, we saved only 4KB of plain text. Later experiments show that this limit had no marked impact on effectiveness.</p><p>Finally, we added an additional index for the ORCAS queries associated to each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PYTERRIER RETRIEVAL PIPELINES</head><p>All of our experiments and submitted runs for the TREC 2020 Deep Learning track built upon PyTerrier, our new expressive Python bindings for Terrier <ref type="bibr" coords="1,390.72,496.89,9.27,7.94" target="#b8">[9]</ref>. In particular, PyTerrier defines all retrieval components (rankers or re-rankers) to take the form of transformer objects, which transform one dataframe to another. To create flexible pipelines composed of multiple transformers, PyTerrier overloads standard Python operators for transformer objects as follows:</p><p>‚Ä¢ &gt;&gt; (then): Pass the output of one transformer into another.</p><p>‚Ä¢ + (linear combination): Combine the retrieval scores of two transformers, ala. CombSUM. ‚Ä¢ ‚àº (cache): Cache (aka. memoize) the outputs of the retrieval transformer to disk, such that subsequent retrieval operations occur faster. ‚Ä¢ * (feature union): Consider the outputs of two transformers as separate features for learning-to-rank. All ranking features described in the rest of this paper were expressed as pipelines of transformers using these operators. We refer the reader to <ref type="bibr" coords="1,389.24,666.09,10.68,7.94" target="#b8">[9]</ref> for more information about the PyTerrier data model and the flexibility of the operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ARCHITECTURE</head><p>Conceptually, our ranking framework mirrored that of our previous participation, in that it consists of 3 stages:</p><p>(1) Candidate retrieval (2) Feature calculation (3) Re-ranking using a learning-to-rank technique In the following, we describe each of these stages in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Candidate Retrieval</head><p>In line with past experience from our participation to the TREC 2019 Deep Learning track, we used the DPH hypergeometric model <ref type="bibr" coords="2,279.45,170.92,14.60,7.94" target="#b14">[14]</ref> from the Divergence From Randomness (DFR) framework for the first retrieval pass. Indeed, we found DPH to be at least as effective as BM25 without the need for parameter tuning. Similarly, we used the Bo1 query expansion model. Figure <ref type="figure" coords="2,197.48,214.75,4.11,7.94" target="#fig_0">1</ref> presents the heatmaps of retrieval effectiveness including MAP, NDCG and NDCG@10 when varying the number of feedback documents and expansion terms for Bo1 on the TREC 2019 document ranking topics. We found that Bo1 was very effective with its default settings of 3 feedback documents and 10 expansion terms -this is the setting applied in our TREC 2020 participation. Finally, we experimented with a query expansion variant that makes use of T5 <ref type="bibr" coords="2,224.42,291.47,14.77,7.94" target="#b13">[13]</ref> for generating an expanded query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature calculation</head><p>We then focused on re-ranking the candidate documents. Each feature was expressed as a PyTerrier retrieval pipeline. Our feature groups encapsulated both classical ranking features, such as statistical retrieval models, and query independent features. We also had a number of BERT-based features.</p><p>Retrieval models. We calculated DPH and the number of matching query term features on each field (title, URL, contents). We also calculated a sequential dependence feature <ref type="bibr" coords="2,208.62,422.06,13.22,7.94" target="#b12">[12]</ref>. Finally, we applied DPH on smaller passages of the document text, and then took the maximum score.</p><p>Query independent features. Field lengths for title, URL and document contents, in terms of the number of indexed tokens.</p><p>Query Expansion. Rescoring of the candidate documents by application of the Bo1 query expansion mechanism; similarly, rescoring of the top-documents by application of collection enrichment, i.e. query expansion with the reformulated query obtained from the top-ranked Wikipedia documents.</p><p>GLoVe. Representation of document and query in the word2vec space, using the GloVe embeddings. Query document similarity was computed using Word Movers Distance <ref type="bibr" coords="2,220.10,573.87,9.52,7.94" target="#b6">[7]</ref>, as implemented by GenSim.</p><p>BERT-based features. We calculated BERT ùê∂ùêøùëÜ <ref type="bibr" coords="2,225.45,602.56,14.60,7.94" target="#b11">[11]</ref> and ColBERT <ref type="bibr" coords="2,290.64,602.56,10.43,7.94" target="#b5">[6]</ref> features 1 . In both cases, we also varied the underlying neural language model between bert-based-uncased and scibert. For BERT ùê∂ùêøùëÜ , we used the CEDR implementation <ref type="bibr" coords="2,223.38,635.43,9.39,7.94" target="#b7">[8]</ref>.</p><p>Following <ref type="bibr" coords="2,102.64,646.39,9.42,7.94" target="#b2">[3]</ref>, for long documents, we added features applying MaxPassage on both the BERT ùê∂ùêøùëÜ and ColBERT neural re-rankers, by composing passaging and maximum score transformers around the BERT transformers. Listing 1 provides salient PyTerrier code for creating a retrieval pipeline encompassing DPH with query expansion and ColBERT MaxPassage, composed using the &gt;&gt; (then) 1 As estimated through the special CLS token, explaining the BERT ùê∂ùêøùëÜ name.  # d e p l o y DPH + QE , r e t r i e v e t h e doc metadata 2 f i r s t p a s s _ d p h _ q e = p t . B a t c h R e t r i e v e ( i n d e x , 3 wmodel= "DPH" , 4 metadata = [ " docno " , " t i t l e " , " body " ] , 5 c o n t r o l s = { " qe " : " on " , " qemodel " : " Bo1 " } ,</p><formula xml:id="formula_0" coords="2,320.16,144.12,2.78,5.30">6</formula><p>n u m _ r e s u l t s = 1 0 0 )  operator. Finally, we devised linear combinations of BERT features, using PyTerrier's + operator.</p><p>Entity BERT Feature. We also calculated a variant of BERT ùê∂ùêøùëÜ using entities.</p><p>Table <ref type="table" coords="2,350.28,322.90,4.24,7.94" target="#tab_1">1</ref> summarises all feature groups and the total number of features in each group. Not all features were applied for all runs. For this reason, the right-hand side of Table <ref type="table" coords="2,489.87,344.82,4.25,7.94" target="#tab_1">1</ref> has columns for four feature sets, which define the features groups used within each feature set. Of note, the "16" and "17o" feature sets do not use deep learned neural ranking features; "17o" and "31o" feature sets both deploy a feature encapsulating the ORCAS query click as a field; finally, for the "20" feature set, we eliminated a subset of the BERTbased features based on feature performance and learner feature importances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning-to-rank</head><p>Following previous years, we made use of a learning-to-rank technique to combine the various retrieval approaches. To do so, all selected features were combined using the * (feature union) operator of PyTerrier.</p><p>While we again used LambaMART <ref type="bibr" coords="2,461.31,501.22,9.52,7.94" target="#b0">[1]</ref>, we switched from the Jforests <ref type="bibr" coords="2,347.97,512.18,10.68,7.94" target="#b3">[4]</ref> implementations we have used in recent years to the Python integrations provided by xgBoost <ref type="bibr" coords="2,475.59,523.14,10.68,7.94" target="#b1">[2]</ref> and LightGBM <ref type="bibr" coords="2,546.89,523.14,9.52,7.94" target="#b4">[5]</ref>. After some experimentation, we settled on LightGBM as the most effective of the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Setup Details</head><p>We used 1000 queries from the MSMARCO training document ranking task for training the BERT ùê∂ùêøùëÜ deep learned models and the learning-to-rank configurations. The ColBERT model was trained using the file triples.train.small.tsv.gz from the MSMARCO passage ranking task. Validation and run selection were conducted using the TREC 2019 Deep Learning track document ranking task queries and relevance assessments.</p><p>When applying MaxPassage, we applied a sliding window of 128 tokens which advanced by 64 tokens each time. The title of the document is prepended to each passage.</p><p>All our runs were created using PyTerrier in Jupyter notebooks. PyTerrier is available from https://github.com/terrier-org/pyterrier. Example notebooks for runs submitted to the MSMARCO leaderboard are available at https://github.com/cmacdonald/pyterrier- feedback terms 0.31 0.31 0.31 0.31 0.3 0.3 0.31 0.31 0.3 0.3 0.33 0.33 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.31 0.34 0.34 0.33 0.33 0.32 0.32 0.32 0.32 0.32 0.32 0.34 0.35 0.33 0.33 0.32 0.32 0.33 0.33 0.32 0.32 0.35 0.35 0.33 0.33 0.32 0.33 0.33 0.33 0.32 0.32 0.35 0.35 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.32 0.35 0.35 0.34 0.33 0.32 0.33 0.33 0.33 0.33 0.33 0.35 0.35 0.34 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.35 0.35 0.34 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.35 0.35 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.32  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RUNS</head><p>We submitted 3 runs to the Document Ranking task. We were also invited by the track organisers to submit baselines runs, which applied no neural re-ranking techniques. We chose to submit 6 baseline runs, which also acted as baselines for our group's main submission runs. In addition, we describe three additional runs on our stemmed index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Runs</head><p>Our submitted baseline runs to the document ranking task of the TREC 2019 Deep Learning track contained no neural features, and constituted runs with and without learning-to-rank and query expansion. In particular, our 6 baseline runs are as follows:</p><p>‚Ä¢ uogTrBaseDPH: Applying DPH on our unstemmed index.</p><p>‚Ä¢ uogTrBaseDPHQ: Applying DPH and Bo1 query expansion on our unstemmed index. This corresponds to the retrieval transformer expressed in line 2 of Listing 1. ‚Ä¢ uogTrBaseL16: Re-ranking the results identified by uogTrBase-DPH using the 16 feature set. Re-ranking is performed by LightGBM. ‚Ä¢ uogTrBaseL17o: As uogTrBaseL16, but with an additional feature in the form of a DPH score on the ORCAS field. ‚Ä¢ uogTrBaseQL16: As uogTrBaseL16, but re-ranking the candidate set obtained from uogTrBaseDPHQ. ‚Ä¢ uogTrBaseQL17o: As uogTrBaseL17o, but re-ranking the candidate set obtained from uogTrBaseDPHQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Submitted Group Runs</head><p>We submitted the following 3 runs:</p><p>‚Ä¢ uogTr31oR: Applying all 31 features, re-ranked using Light-GBM. This run re-ranks the candidate sets obtained from uogTrBaseDPH. Compared to uogTrBaseL17o, it adds neural re-ranking features. ‚Ä¢ uogTrQCBMP: Applies ColBERT MaxPassage on the candidate sets obtained from uogTrBaseDPHQ. This run can be obtained using the code in Listing 1. ‚Ä¢ uogTrT20: Applies the 20 features set on the candidate set identified by our T5-based query expansion model. In addition, we describe 3 additional runs that using a stemmed index (but do not apply LTR):</p><p>‚Ä¢ uogTrBaseDPHSS: Applying DPH on our stemmed index.</p><p>‚Ä¢ uogTrBaseDPHQSS: Applying DPH and Bo1 query expansion on our stemmed index. ‚Ä¢ uogTrQCBMPSS: Applies ColBERT MaxPassage on the candidate sets obtained from uogTrBaseDPHQSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS &amp; ANALYSIS</head><p>Table <ref type="table" coords="3,340.08,629.52,4.24,7.94" target="#tab_2">2</ref> lists the obtained effectiveness results for all our runs, including our baseline, submitted group runs and additional runs, as well as the TREC per-topic best and median scores across all participating systems, in terms of MAP, P@10 2 , NDCG@10, NDCG@100 and MRR. Firstly, we analyse the performance of the baseline runs. Comparing uogTrBaseDPH with uogTrBaseDPHQ, it is clear that query expansion benefited all metrics except MRR. For the application of learning-to-rank, the picture is mixed: there are improvements between uogTrBaseDPH and uogTrBaseL16 for all metrics except MRR; between uogTrBaseDPHQ and uogTrBaseQL16, only a small improvement for NDCG@100 is observed. Finally, we see that adding an ORCAS feature made small improvements: comparing uogTrBaseL16 with uogTrBaseL17o, P@10, NDCG@10, NDCG@100 are slightly improved; comparing uogTrBaseQL16 with uogTrBaseQL17o, MAP, P@10, NDCG@10, NDCG@100 and MRR are also improved.</p><p>Next, we turn our attention to our submitted group runs. Among these three runs, run uogTrQCBMP -which did not deploy learningto-rank -was the most effective on all metrics except MRR; this run deployed ColBERT MaxPassage on top of DPH + QE. Compared to the corresponding baseline uogTrBaseDPHQ run, this achieved an improvement of 8.4% in MAP and 14.6% in NDCG@10.</p><p>Among the two learning-to-rank runs, uogTrT20 was the most effective for MAP and P@10; on closer examination of the results, we conclude that the promising effectiveness of this approach is due to the reduced learning-to-rank feature set, rather than the new query expansion approach. Comparing uogTr31oR with baseline uogTrBaseDPH, we find that the BERT features result in an uplift of 22.2% in MAP (0.3070 ‚Üí 0.3722) and 18.8% in NDCG@10 (0.4871 ‚Üí 0.5476). To illustrate the effectiveness of the deployed features, Figure <ref type="figure" coords="4,79.33,328.86,4.17,7.94">2</ref> plots the performances of the features for the TREC 2019 and TREC 2020 topics (numbers above each bar are the rank of that feature among all others). We observe a strong correlation between the performances of the features in these rankings, with the strongest feature (ColBERT MaxPassage, as deployed in run uogTrQCBMP) consistently the most effective. In both figures, the linear combination of SciBERT and BERT results in the 2nd most effective feature.</p><p>Finally, we look at the overall reported performances. We note that the TREC Median pseudo-system appears to be very strong this year, and we were disappointed not to exceed it on average (we only exceed it for NDCG@10, 0.5733 vs. 0.5791); we have since investigated and eliminated possible confounding variables such as the stemming configuration, and the 4KB size of the document recorded in the Terrier index used for the BERT models. We continue to investigate ways to improve our performance compared to the TREC median.</p><p>Analysing the overall trends, while it is clear that learning-torank models are flexible and can be used to successfully combine effective ranking features, for this task, it may not result in models that are more effective than the most effective constituent features. The high effectiveness of run uogTrQCBMP is testament to this observation. After the TREC submissions, we further conducted additional runs on a stemmed index, namely uogTrBaseDPHSS and uogTrBaseDPHQSS, as well as uogTrQCBMPSS. In general, compared to the unstemmed index, runs using the stemmed index provide higher performance. Among the three additional runs, uogTrQCBMPSS exceeds the TREC median performance on all metrics except MRR demonstrating the effectiveness of the established model and the importance of the proper experimental setting for stemming on this test collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>Overall, our participation in the TREC Deep Learning track was a useful activity to refine methods of integration of deep learning techniques as retrieval pipelines in PyTerrier. In terms of effectiveness, our most effective run uogTrQCBMP outperformed the TREC median in terms of NDCG@10 and uogTrQCBMPSS exceeds the TREC median on all metrics except MRR. Moreover, the learningto-rank runs were not among the most effective, which emphasises the difficulty in learning effective models for adhoc retrieval tasks using training datasets with very few judgements, echoing our findings from TREC 2019.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,335.73,91.48,204.69,7.70"><head>Listing 1 :</head><label>1</label><figDesc>DPH with QE and ColBERT MaxPassage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,320.16,104.27,2.78,5.30"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="2,320.16,152.09,2.78,5.30;2,320.16,159.50,166.51,5.98;2,320.16,167.47,230.63,5.98;2,317.38,176.00,5.56,5.30;2,317.38,183.41,140.56,5.98;2,317.38,191.38,124.23,5.98"><head>7 8 #</head><label>8</label><figDesc>l o a d an e x i s t i n g f i n e -tun ed ColBERT model 9 c o l b e r t = C o l B E R T P i p e l i n e ( " d i r / c o l b e r t . dnn " , d o c _ a t t r = " body " ) 10 11 # compose c o l b e r t , u s i n g MaxPassage 12 q e _ c o l b e r t _ 1 0 0 _ m a x p a s s a g e = f i r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="3,102.98,209.51,406.05,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Query expansion performances with Bo1 for given distinct feedback terms and documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,124.41,83.68,363.18,247.38"><head></head><label></label><figDesc></figDesc><graphic coords="5,124.41,83.68,363.18,247.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,124.41,348.00,363.17,247.75"><head></head><label></label><figDesc></figDesc><graphic coords="5,124.41,348.00,363.17,247.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.50,232.42,504.70,195.86"><head>Table 1 :</head><label>1</label><figDesc>Feature group descriptions and alignment with feature sets. ‚úì denotes that all features in that group (row) were used in that feature set (column), while # denotes the number of features selected from that group.</figDesc><table coords="3,53.80,266.32,458.97,161.96"><row><cell>Feature Group</cell><cell>Description</cell><cell>#</cell><cell cols="2">Feature Sets</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">16 17o 20 31o</cell></row><row><cell>Retrieval Models</cell><cell>DPH, DPH on individual fields, PL2F, number of matching query</cell><cell>10 ‚úì</cell><cell>‚úì</cell><cell>‚úì</cell><cell>‚úì</cell></row><row><cell></cell><cell>terms per field, DFR sequential dependence, DPH MaxPassage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>QI Features</cell><cell>Field lengths</cell><cell>3 ‚úì</cell><cell>‚úì</cell><cell>‚úì</cell><cell>‚úì</cell></row><row><cell>QE Features</cell><cell>Query expansion and collection enrichment scores</cell><cell>2 ‚úì</cell><cell>‚úì</cell><cell>‚úì</cell><cell>‚úì</cell></row><row><cell>GloVe</cell><cell>Word Mover Distance</cell><cell>1 ‚úì</cell><cell>‚úì</cell><cell>‚úì</cell><cell>‚úì</cell></row><row><cell>BERT-based Features</cell><cell>BERT ùê∂ùêøùëÜ , ColBERT as well as SciBERT variants, with and with-</cell><cell>12</cell><cell></cell><cell>‚úì</cell><cell>‚úì</cell></row><row><cell></cell><cell>out MaxPassage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Entity-based BERT Feature A variant of BERT ùê∂ùêøùëÜ encapsulating entity knowledge</cell><cell>1</cell><cell></cell><cell></cell><cell>‚úì</cell></row><row><cell>ORCAS</cell><cell>DPH calculated on the ORCAS field</cell><cell>1</cell><cell>‚úì</cell><cell></cell><cell>‚úì</cell></row><row><cell cols="2">msmarco-document-leaderboard-runs. Our integrations of CEDR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">and ColBERT are available as PyTerrier plugins from https://github.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>com/cmacdonald/pyterrier_bert.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,323.19,85.73,229.77,190.33"><head>Table 2 :</head><label>2</label><figDesc>Performance of submitted and baseline runs.</figDesc><table coords="4,323.19,108.00,229.77,168.05"><row><cell></cell><cell>MAP</cell><cell>P@10</cell><cell cols="2">NDCG@10 NDCG@100</cell><cell>MRR</cell></row><row><cell>TREC Best</cell><cell>0.6109</cell><cell>-</cell><cell>0.8019</cell><cell>0.7663</cell><cell>0.9822</cell></row><row><cell>TREC Median</cell><cell>0.3902</cell><cell>-</cell><cell>0.5733</cell><cell>0.5859</cell><cell>0.9444</cell></row><row><cell></cell><cell></cell><cell cols="2">baseline runs</cell><cell></cell><cell></cell></row><row><cell>uogTrBaseDPH</cell><cell>0.3070</cell><cell>0.5089</cell><cell>0.4871</cell><cell>0.4972</cell><cell>0.8415</cell></row><row><cell>uogTrBaseDPHQ</cell><cell>0.3461</cell><cell>0.5444</cell><cell>0.5052</cell><cell>0.5345</cell><cell>0.8052</cell></row><row><cell>uogTrBaseL16</cell><cell>0.3248</cell><cell>0.5289</cell><cell>0.4964</cell><cell>0.5052</cell><cell>0.8219</cell></row><row><cell>uogTrBaseL17o</cell><cell>0.3248</cell><cell>0.5333</cell><cell>0.5120</cell><cell>0.5090</cell><cell>0.7980</cell></row><row><cell>uogTrBaseQL16</cell><cell>0.3436</cell><cell>0.5311</cell><cell>0.4998</cell><cell>0.5357</cell><cell>0.7930</cell></row><row><cell>uogTrBaseQL17o</cell><cell>0.3530</cell><cell>0.5511</cell><cell>0.5203</cell><cell>0.5399</cell><cell>0.8276</cell></row><row><cell></cell><cell></cell><cell cols="2">submitted runs</cell><cell></cell><cell></cell></row><row><cell>uogTr31oR</cell><cell>0.3468</cell><cell>0.5622</cell><cell>0.5476</cell><cell>0.5284</cell><cell>0.8926</cell></row><row><cell>uogTrQCBMP</cell><cell cols="2">0.3752 0.6000</cell><cell>0.5791</cell><cell>0.5673</cell><cell>0.8722</cell></row><row><cell>uogTrT20</cell><cell>0.3692</cell><cell>0.5667</cell><cell>0.5453</cell><cell>0.5296</cell><cell>0.8711</cell></row><row><cell></cell><cell></cell><cell cols="2">additional runs</cell><cell></cell><cell></cell></row><row><cell>uogTrBaseDPHSS</cell><cell>0.4023</cell><cell>0.5689</cell><cell>0.5145</cell><cell>0.5626</cell><cell>0.8313</cell></row><row><cell>uogTrBaseDPHQSS</cell><cell>0.4213</cell><cell>0.5778</cell><cell>0.5173</cell><cell>0.5734</cell><cell>0.8343</cell></row><row><cell>uogTrQCBMPSS</cell><cell>0.4066</cell><cell>0.6022</cell><cell>0.5936</cell><cell>0.6050</cell><cell>0.8819</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,324.49,724.69,179.20,6.18"><p>We were not provided with Best or Median numbers for P@10.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>The co-authors acknowledge the assistance of <rs type="person">Siwei Liu</rs>, <rs type="person">Sarawoot Kongyoung</rs> and <rs type="person">Ting Su</rs> in supporting our participation.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="4,333.39,487.55,225.27,6.23;4,333.39,495.52,139.55,6.23" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,421.45,487.55,137.21,6.23;4,333.39,495.52,34.24,6.23">From RankNet to LambdaRank to LambdaMART: An Overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2010-82</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="4,333.39,503.52,225.88,6.18;4,333.39,511.46,224.81,6.23;4,333.39,519.43,101.64,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,444.39,503.52,111.89,6.18">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,341.93,511.46,216.28,6.23;4,333.39,519.43,72.02,6.23">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,527.43,224.81,6.18;4,333.39,535.37,201.41,6.23" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,444.60,527.43,113.60,6.18;4,333.39,535.40,104.26,6.18">Deeper text understanding for IR with contextual neural language modeling</title>
		<author>
			<persName coords=""><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,450.13,535.37,55.27,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,543.37,225.99,6.18;4,333.39,551.31,224.81,6.23;4,333.39,559.28,44.23,6.23" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,505.82,543.37,53.56,6.18;4,333.39,551.34,180.58,6.18">Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models</title>
		<author>
			<persName coords=""><forename type="first">Yasser</forename><surname>Ganjisaffar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,526.37,551.31,31.83,6.23;4,333.39,559.28,21.31,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,567.28,225.58,6.18;4,333.39,575.25,224.81,6.18;4,333.39,583.19,222.47,6.23" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,426.67,575.25,131.53,6.18;4,333.39,583.22,35.21,6.18">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName coords=""><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weidong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,380.44,583.19,139.53,6.23">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,591.19,225.99,6.18;4,333.39,599.13,224.81,6.23;4,333.39,607.10,67.09,6.23" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12832</idno>
		<title level="m" coord="4,451.25,591.19,108.13,6.18;4,333.39,599.16,174.53,6.18">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,615.10,224.81,6.18;4,333.39,623.04,225.51,6.23;4,333.39,631.01,36.95,6.23" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,526.95,615.10,31.25,6.18;4,333.39,623.07,96.91,6.18">From word embeddings to document distances</title>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,442.27,623.04,116.63,6.23;4,333.39,631.01,7.80,6.23">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,639.01,225.64,6.18;4,333.39,646.95,225.57,6.23;4,333.23,654.95,14.51,6.18" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,540.39,639.01,18.64,6.18;4,333.39,646.98,138.64,6.18">CEDR: Contextualized embeddings for document ranking</title>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,484.00,646.95,54.06,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1101" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,662.92,224.81,6.18;4,333.39,670.86,224.81,6.23;4,333.39,678.83,193.98,6.23" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,467.53,662.92,90.67,6.18;4,333.39,670.89,106.34,6.18">Declarative Experimentation in Information Retrieval using PyTerrier</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Tonellotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,451.92,670.86,106.29,6.23;4,333.39,678.83,164.82,6.23">Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</title>
		<meeting>the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,686.83,225.88,6.18;4,333.39,694.80,224.81,6.18;4,333.39,702.74,98.86,6.23" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="4,372.01,694.80,176.07,6.18">University of Glasgow at TREC 2009: Experiments with Terrier</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,333.39,702.74,69.52,6.23">Proceedings of TREC 2009</title>
		<meeting>TREC 2009</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,58.22,625.68,495.56,7.71" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="5,58.22,625.68,491.31,7.71">Figure 2: NDCG@10 feature performances for features in the 31o feature set -these features are constituents of uogTr31oR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,69.23,654.94,225.89,6.18;5,69.23,662.88,108.33,6.23" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="5,203.43,654.94,88.07,6.18">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,69.23,670.88,225.88,6.18;5,69.23,678.82,225.88,6.23;5,69.23,686.82,24.81,6.18" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="5,173.98,670.88,101.42,6.18;5,69.23,678.85,154.81,6.18">Incorporating term dependency in the DFR framework</title>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,236.72,678.82,55.53,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="843" to="844" />
		</imprint>
	</monogr>
	<note>Vassilis Plachouras, and Iadh Ounis</note>
</biblStruct>

<biblStruct coords="5,69.23,694.79,225.58,6.18;5,69.23,702.76,224.81,6.18;5,333.39,654.91,224.81,6.23;5,333.39,662.88,109.13,6.23" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="5,237.66,702.76,56.38,6.18;5,333.39,654.94,165.57,6.18">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,504.42,654.91,53.79,6.23;5,333.39,662.88,50.59,6.23">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,670.88,225.99,6.18;5,333.39,678.82,224.81,6.23;5,333.39,686.79,50.71,6.23" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="5,463.57,670.88,95.81,6.18;5,333.39,678.85,180.66,6.18">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,525.95,678.82,32.25,6.23;5,333.39,686.79,21.31,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,694.79,225.07,6.18;5,333.18,702.73,218.91,6.23" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="5,496.61,694.79,61.85,6.18;5,333.18,702.76,148.42,6.18">University of Glasgow Terrier Team at the TREC 2019 Deep Learning Track</title>
		<author>
			<persName coords=""><forename type="first">Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,493.92,702.73,54.74,6.23">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
