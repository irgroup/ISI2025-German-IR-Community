<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.97,154.89,433.32,15.12">A Multiple Models Ensembling Method in Trec Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,93.41,187.37,47.31,10.48"><forename type="first">Liyu</forename><surname>Cao</surname></persName>
							<email>caoliyu922@pingan.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Technology (Shenzhen) Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.19,187.37,65.04,10.48"><forename type="first">Yixuan</forename><surname>Qiao</surname></persName>
							<email>qiaoyixuan528@pingan.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Technology (Shenzhen) Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,242.69,187.37,51.06,10.48"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<email>chenhao305@pingan.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Technology (Shenzhen) Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.62,187.37,49.99,10.48"><forename type="first">Peng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Technology (Shenzhen) Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.50,187.37,42.59,10.48"><forename type="first">Yuan</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Technology (Shenzhen) Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,444.75,187.37,68.35,10.48"><forename type="first">Guotong</forename><surname>Xie</surname></persName>
							<email>xieguotong@pingan.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Technology (Shenzhen) Co., Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.97,154.89,433.32,15.12">A Multiple Models Ensembling Method in Trec Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C86E59D086270ECB345B7F0BFCEEFAFF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is to describe our participation in the passage ranking tasks of TREC 2020 Depp Learning Track. We take leverage of several pre-trained models, such as BERT, ELECTRA and XLNET, to re-rank passages. Firstly, we use corpus in this track and enhanced next sentence prediction task to pretreain a new BERT large model. Secondly, we finetune the new BERT model as well as ELECTRA, XL-NET and ALBERT with selected triplet data. Then, we ensemble several different kind of models to score and rank top-1000 passage. Finally, we select and rerank top-10 passages in the former step according to the similarity to the top-1 passage to reduce noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Deep Learning Track continues to have the same tasks and goals as TREC 2019. Similar to the previous year, one of the main goals is to study what methods work best when a large amount of training data is available. It consists of two tasks: passage ranking and document ranking; and two subtasks in each case: full ranking and re-ranking. Each task is based on human-generated data, which comes from MS-MARCO dataset.</p><p>Our approach is mainly based on pre-trained models, such as BERT, ALBERT, ELECTRA and XL-NET, which are state-of-art models in different natural language understanding tasks. Besides directly fine-tuning BERT with given training data, we try to retrain a new BERT model with part of whole train-ing data. Based on the new models, then we fine-tune it with pairwise ranking strategy on the labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>This section presents approaches we used in passage re-ranking task. About 1000 passages are selected by BM25 for each query, and we need to re-rank passages to ensure that top-10 passages are more relevant to the query. We use several pre-trained models to compare their performances in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">BERT-based Re-ranker</head><p>Pre-trained models, especially BERT <ref type="bibr" coords="1,469.33,450.52,15.31,8.74" target="#b0">[1]</ref>, have been shown powerful in wide range of language understanding tasks. There are two steps: pre-training and fine-tuning. Firstly, we pre-train a new BERT model with a more difficult pre-training task, which is inspired from StructBERT <ref type="bibr" coords="1,414.33,510.29,15.05,8.74" target="#b1">[2]</ref> and IDST <ref type="bibr" coords="1,480.23,510.29,9.96,8.74" target="#b2">[3]</ref>. Secondly, we fine-tune the model parameters on training data with a pairwise ranking strategy to receive a score to represent the relevence between query and passage.</p><p>In the pre-training part, instead of using next sentence prediction as one of the pre-training objectives, we reinforce the task to previous sentence prediction and next sentence prediction. The original NSP task is too weak to train a powerful model, because it is easy to distinguish whether it is next sentence since they come from the same passage and have same topic. However, previous and next sentence prediction can take leverage of contextual information since it have to judge which sentence is former. Specifically, given a sentence S1 from a passage, there is a onethird probability to choose the previous sentence, a one-third probability to choose the next sentence and a one-third probability to choose a random sentence from the other passage. Besides the new NSP task, we kept the MLM task as original and then train a new BERT model with them.</p><p>In the fine-tuning part, we follow the method <ref type="bibr" coords="2,290.13,212.58,10.52,8.74" target="#b3">[4]</ref> and treat the model as a classifier to get a relevent score. We pack one query and one passage with [SEP], then put the whole sentence to BERT model and use <ref type="bibr" coords="2,107.39,260.40,24.49,8.74">[CLS]</ref> vector in the last layer as a representation to compute a score. To take advantage of triplet training dataset, we use pairwise strategy to train BERT model. The triplet loss is: L(q, p + , p -, θ) = max(0, θ -S(q, p + ) + S(q, p -)))</p><p>(1) where q represents query, p + represents positive passage and p -represents negative passage, S(q, p) represents score given by BERT model, S = sigmoid(w • h L CLS ), where w is a trainable parameter and h CLS is the hiddent state of [CLS].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ALBERT-based Re-ranker</head><p>ALBERT is a lite BERT model which incorporates two parameter reduction techniques. <ref type="bibr" coords="2,240.83,456.66,10.52,8.74" target="#b4">[5]</ref> By factorized embedding parameterization, which means decomposing the large vocabulary embedding matrix into two small matrices, and cross-layer parameter sharing An ALBERT has 18x fewer parameters than BERT-large model and can be trained about 1.7x faster.</p><p>As we do in section 2.1, we use hidden vector of [CLS] in the last layer to re-rank all candidate passages. The loss function is the same as equation(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ELECTRA-based Re-ranker</head><p>ELECTRA proposes a more sample-efficient pretraining task called replaced token detection. <ref type="bibr" coords="2,290.13,630.08,10.52,8.74" target="#b5">[6]</ref> Firstly, we train a small generator network to replace some tokens with plausible alternatives. Then we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not.</p><p>we organize input as [CLS] query [SEP] passage [SEP] and feed them to ELECTRA and use the final hidden vector of [CLS] to get a relevant score. The loss function is also the same as equation(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">XLNET-based Re-ranker</head><p>Different with BERT, XLNET is a generalized autoreggressive pre-training model <ref type="bibr" coords="2,444.86,295.33,13.19,8.74" target="#b6">[7]</ref>. In this re-ranker, we use XLNET-base model to re-rank top 1000 passages. We join query with positive passage or negative passage and sent them to XLNET model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The input format is [query][SEP][passage][SEP]</head><p>[CLS] and we use the final hidden vector of [CLS] to get a relevant score. The loss function is also the same as equation(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Re-rank top-10</head><p>After re-ranking, We find that top-10 passages sometimes contains irrelevant passages with query. In order to eliminate the influence of such negative passages, we assume that top-1 passage is a positive passage and other relevant passage should be similar to it.</p><p>We use glove-300 <ref type="bibr" coords="2,394.70,546.39,12.66,8.74" target="#b7">[8]</ref> to compute vectors to represent words in a passage and then use the average of all these vectors as the passage's representation. By compute cosine similarity of the vectors, we can receive similarity of the passages with top-1 passage. We set a threshold to decide whether a passage's score, ranked from 2 to 10, should be modified. At last, we set the threshold as 0.1, which means if the max similarity score minus the minimum score is larger than 0.1, the minimum score should minus 0.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Result</head><p>Table <ref type="table" coords="3,98.59,438.35,4.98,8.74" target="#tab_0">1</ref> presents results from different models and ensembled models. As we can see an ensemble of BERT, ELECTRA and XLNET, followed with a post processing method can reach the best ndcg@10, while an ensemble of only BERT large models performs best on mAP.</p><p>In this paper, we describe our methods based on pre-trained models for passage re-ranking subtask of Trec 2020 Deep learning track. We find that there is no difference between different kind of models. Also, an ensemble of different pre-trained models can reach higher score than each single model. However, we can not achieve big improvement than the best team in last year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,134.88,456.63,260.47"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="3,72.00,134.88,456.63,260.47"><row><cell></cell><cell></cell><cell cols="2">overall performance of submitted models on test 2020</cell></row><row><cell>Run Tag</cell><cell>Group</cell><cell>Run Description</cell><cell cols="2">NDCG@10 mAP</cell></row><row><cell cols="2">pinganNLP-1 pinganNLP</cell><cell>BERT+ELETRA+XLNET ensemble (6 models)</cell><cell>0.7343</cell><cell>0.4896</cell></row><row><cell cols="2">pinganNLP-2 pinganNLP</cell><cell>pinganNLP-1 + re-rank top 10</cell><cell>0.7368</cell><cell>0.4881</cell></row><row><cell cols="3">pinganNLP-3 pinganNLP only BERT ensemble (6 models) + re-rank top 10</cell><cell>0.7352</cell><cell>0.4918</cell></row><row><cell cols="3">3 Discussion and Conclusion</cell><cell></cell></row><row><cell cols="2">4 Experiment</cell><cell></cell><cell></cell></row><row><cell>4.1 Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">The corpus contains 1,010,916 queries on a collection</cell><cell></cell></row><row><cell cols="3">of 8,841,823 passages. Sentences used in section 2.1</cell><cell></cell></row><row><cell cols="3">pre-training part are sampled from these passages.</cell><cell></cell></row><row><cell cols="3">Triplet train dataset contains 397,756,691 pairs of</cell><cell></cell></row><row><cell cols="3">query, positive and negative passages. However, it</cell><cell></cell></row><row><cell cols="3">only consists of about 500,000 different queries. We</cell><cell></cell></row><row><cell cols="3">randomly select one piece of data for each query, so</cell><cell></cell></row><row><cell cols="3">the final triplet data we used contains 461,594 triplet</cell><cell></cell></row><row><cell>pairs.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,87.50,653.99,213.15,8.74;3,87.50,665.94,213.15,8.74;3,326.10,216.72,213.14,8.74;3,326.10,228.67,144.48,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,182.99,665.94,117.66,8.74;3,326.10,216.72,213.14,8.74;3,326.10,228.67,35.47,8.74">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Devlin</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Ming-Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lee</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Toutanova</forename><surname>Kristina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,382.88,228.67,55.52,8.74">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.10,248.60,213.15,8.74;3,326.10,260.55,213.15,8.74;3,326.10,272.51,213.14,8.74;3,326.10,284.46,213.14,8.74;3,326.10,296.42,80.41,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,326.10,272.51,213.14,8.74;3,326.10,284.46,213.14,8.74;3,326.10,296.42,12.04,8.74">Structbert: Incorporating language structures into pre-training for deep language understanding</title>
		<author>
			<persName coords=""><forename type="first">Wang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bao</forename><surname>Zuyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Jiangnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Liwei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Si</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,359.02,296.42,17.18,8.74">Trec</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.10,316.34,213.14,8.74;3,326.10,328.30,213.14,8.74;3,326.10,340.25,213.15,8.74;3,326.10,352.21,213.15,8.74;3,326.10,364.16,169.27,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,467.21,328.30,72.03,8.74;3,326.10,340.25,213.15,8.74;3,326.10,352.21,213.15,8.74;3,326.10,364.16,100.07,8.74">Idst at trec 2019 deep learning track: Deep cascade ranking with generation-based document expansion and pretrained language model</title>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Chenliang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Jiangnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Si</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,447.89,364.16,17.18,8.74">Trec</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.10,384.09,213.15,8.74;3,326.10,396.04,118.52,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="3,505.51,384.09,33.74,8.74;3,326.10,396.04,87.25,8.74">Passage re-ranking with bert</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.10,415.97,213.14,8.74;3,326.10,427.92,213.14,8.74;3,326.10,439.88,213.14,8.74;3,326.10,451.83,213.14,8.74;3,326.10,463.79,22.69,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,401.38,439.88,137.86,8.74;3,326.10,451.83,159.37,8.74">A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName coords=""><forename type="first">Lan</forename><surname>Zhenzhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Mingda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Goodman</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gimpel</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharma</forename><surname>Piyush</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soricut</forename><surname>Radu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,512.00,451.83,21.80,8.74">ICLR</title>
		<meeting><address><addrLine>Albert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.10,483.72,213.15,8.74;3,326.10,495.67,213.14,8.74;3,326.10,507.63,213.15,8.74;3,326.10,519.58,141.09,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,476.77,495.67,62.48,8.74;3,326.10,507.63,213.15,8.74;3,326.10,519.58,66.62,8.74">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName coords=""><forename type="first">Clark</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luong</forename><surname>Minh-Thang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Manning</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,413.93,519.58,21.80,8.74">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.10,539.51,213.14,8.74;3,326.10,551.46,213.14,8.74;3,326.10,563.42,213.14,8.74;3,326.10,575.37,164.22,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="3,346.97,563.42,192.28,8.74;3,326.10,575.37,133.58,8.74">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.10,595.30,213.15,8.74;3,326.10,607.25,213.14,8.74;3,326.10,619.21,132.48,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="3,384.70,607.25,154.55,8.74;3,326.10,619.21,46.50,8.74">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">Pennington</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Socher</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manning</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,393.70,619.21,32.39,8.74">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
