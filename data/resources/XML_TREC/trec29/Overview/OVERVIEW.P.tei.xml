<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.42,131.64,311.16,18.37">TREC 2020 Podcasts Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,127.19,171.63,66.95,12.75"><forename type="first">Rosie</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.25,171.63,71.22,12.75"><forename type="first">Ben</forename><surname>Cartere</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.70,171.63,45.13,12.75"><forename type="first">Ann</forename><surname>Cli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.97,171.63,90.32,12.75"><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.99,189.56,101.64,12.75"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.73,189.56,81.76,12.75"><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,176.44,207.49,79.94,12.75"><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.48,207.49,88.67,12.75"><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.26,207.49,60.94,12.75"><forename type="first">Yongze</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.93,261.29,83.09,12.75"><forename type="first">Eric</forename><surname>Clarin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.42,131.64,311.16,18.37">TREC 2020 Podcasts Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8CF8182A7E7DC2E983B1FB7A401A65C2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Podcast Track is new at the Text Retrieval Conference (TREC) in 2020. The podcast track was designed to encourage research into podcasts in the information retrieval and NLP research communities. The track consisted of two shared tasks: segment retrieval and summarization, both based on a dataset of over 100,000 podcast episodes (metadata, audio, and automatic transcripts) which was released concurrently with the track. The track generated considerable interest, a racted hundreds of new registrations to TREC and fi een teams, mostly disjoint between search and summarization, made final submissions for assessment. Deep learning was the dominant experimental approach for both search experiments and summarization. This paper gives an overview of the tasks and the results of the participants' experiments. The track will return to TREC 2021 with the same two tasks, incorporating slight modifications in response to participant feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Podcasts are a growing medium of recorded spoken audio. They are more diverse in style, content, format, and production type than previously studied speech formats, such as broadcast news <ref type="bibr" coords="1,262.72,566.61,24.25,9.70;1,65.06,580.16,78.21,9.70" target="#b0">(Garofolo et al., 2000)</ref> or meeting transcripts <ref type="bibr" coords="1,258.87,580.16,32.96,9.70;1,65.06,593.71,46.86,9.70" target="#b1">(Renals et al., 2008)</ref>, and they encompass many more genres than typically studied in video research <ref type="bibr" coords="1,248.99,607.26,42.83,9.70;1,65.06,620.81,49.26,9.70" target="#b2">(Smeaton et al., 2006)</ref>. They come in many di erent formats and levels of formality -news journalism or conversational chat, fiction or non-fiction. Podcasts have a sharply growing share of listening consumption <ref type="bibr" coords="1,85.49,675.01,100.21,9.70">(Edison Research, 2020</ref>) and yet have been relatively understudied. The medium shows great potential to become a rich domain for research in information access and speech and language technologies (among other fields), with many poten-tial opportunities to improve user engagement and consumption of podcast content. The TREC Podcast Track which was launched in 2020 is intended to facilitate research in language technologies applied to podcasts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Data</head><p>The data distributed by the track organisers consisted of just over 100,000 episodes of Englishlanguage podcasts. Each episode comes with full audio, a transcript which was automatically generated using Google's Speech-to-Text API as of early 2020, and a description and metadata provided by the podcast creator, along with the RSS feed content for the show. The data set is described in greater detail by <ref type="bibr" coords="1,399.69,715.65,86.63,9.70" target="#b3">Cli on et al. (2020)</ref>; an example is given in Figure <ref type="figure" coords="1,399.37,729.20,3.74,9.70">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Participation</head><p>The Podcast Track a racted a great deal of a ention with more than 200 registrations to participate. Most registrants did not submit experiments for assessment. A er the submission deadline had passed, registrants were sent a questionnaire to establish what they found to be the biggest challenge when working on their experiment and their submission, and if they did not submit a result, what the most important challenge they found to stand in the way of submission. Participants were also asked to suggest how participation might be made easier for the coming year. The response rate was on the low side (10 responses) and the collated results indicate that the size of the data overwhelmed some participants. Suggestions for the coming year included organising a task for a subset of the data to enable new entrants to familiarise themselves with the problem space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Tasks</head><p>In 2020 the Podcast Track o ered two tasks: (1) retrieval of fixed two-minute segments and (2) summarization of episodes. Both tasks were possible to complete on the automatic transcripts of episodes, rather than the audio data. The full audio data was provided, and teams were free to use it for their tasks (though only one team did do so, using the audio to improve the automatic transcription quality). The segment retrieval and summarization submissions were entirely based on textual input for all submi ed experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>While there has been relatively li le published work exploring information access technologies for podcasts, there is longstanding interest in spoken content retrieval in a range of other se ings involving spoken content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spoken Document Retrieval</head><p>The best known of work in spoken document retrieval is the TREC Spoken Document Retrieval Track which ran at TREC from <ref type="bibr" coords="2,469.34,254.41,19.65,9.70">1997</ref><ref type="bibr" coords="2,493.91,254.41,19.65,9.70" target="#b0">-2000</ref><ref type="bibr" coords="2,369.13,267.95,21.64,9.70" target="#b0">(Garofolo et al., 2000))</ref>. The track focused on examining spoken document retrieval for broadcast news from radio and television sources of increasing size and complexity with each edition of the task. Task participants were provided with baseline transcripts of the spoken created using a then state-of-the-art automatic speech recognition (ASR) system and accurate or near-accurate manual transcripts of the content. The track began by using documents created by manually segmenting the news broadcasts into stories, but la erly began to explore automated identification of start points within unsegmented news broadcasts. The key findings were that for broadcasts, similar retrieval e ectiveness could be achieved for errorful automatic speech recognition transcripts as for manual transcripts, through the appropriate use of external resources such as large contemporaneous news text archives.</p><p>A very di erent spoken retrieval task ran at the CLEF conference in the years 2005-2007 as the Cross-Language Speech Retrieval (CL-SR) task <ref type="bibr" coords="2,320.17,553.06,81.63,9.70" target="#b4">(Pecina et al., 2008)</ref>. This focused on retrieval from a large archive of oral history -spontaneous conversations in the form of personal testimonies. Participants were provided with automatic speech recognition (ASR) transcripts of the spoken content, with a diverse set of associated metadata, manually and automatically assigned controlled vocabulary descriptors for concepts presented in each oral testimony, dates and locations associated with the content discussed, manually assigned person names, and expert hand-wri en segment summaries of the events discussed, together with a set of carefully designed search topics. The main task was to identify starting points for cohesive stories within the each conversational testimony interview where the ground-truth story boundaries were manually assigned by domain experts. The main findings of this task were that accurate automated location of topic start points is challenging, and that, importantly, conversations of this type frequently fail to include mention of important entities within the dialogue. This means that search queries which include these entities o en fail to match well with relevant content. This contrasts with search of broadcast news where such entities are mentioned very frequently to enable listeners to news updates can easily understand the events being described. Retrieval effectiveness was greatly improved by judicious use of the provided manual metadata, but it was recognised that such metadata will not be available for many spoken content archives.</p><p>Another spoken content retrieval task was offered at NTCIR from 2010-2016. This focused on search of Japanese language lectures and technical presentations. The first phase of the task focused only the retrieval of spoken content <ref type="bibr" coords="3,233.92,379.31,57.91,9.70;3,65.06,392.86,23.56,9.70" target="#b5">(Akiba et al., 2016)</ref> while the second phase included the additional complexity of spoken queries <ref type="bibr" coords="3,233.44,406.41,58.39,9.70;3,65.06,419.96,21.64,9.70" target="#b6">(Akiba et al., 2011)</ref>. As well as issues for automated transcription relating of the unstructured informal nature of the spoken delivery of this content, transcription of this content introduced challenges of transcription of specialised domain specific vocabulary items. Participants were provided with a set of search topics with a requirement to locate relevant content within the transcripts. A unique feature of this dataset was the very detailed fine-granularity labelling of relevant content for each search query within the transcripts. This meant that it was possible to do very detailed analysis of the ability of search methods to identify relevant content, including the relationship between search behaviour and the accuracy of the transcription of the query search terms within the transcripts.</p><p>A further study of spoken content search was the Rich Speech Retrieval and Search and Hyperlinking tasks at Mediaeval from <ref type="bibr" coords="3,221.00,669.43,19.65,9.70">2011</ref><ref type="bibr" coords="3,245.56,669.43,19.65,9.70" target="#b9">-2015</ref><ref type="bibr" coords="3,114.69,682.98,22.69,9.70" target="#b7">(Larson et al., 2011;;</ref><ref type="bibr" coords="3,141.56,682.98,95.76,9.70" target="#b8">Eskevich et al., 2012;</ref><ref type="bibr" coords="3,241.51,682.98,21.64,9.70" target="#b9">2015)</ref>. The primary search focus of this task was the identification of "jump-in" points in multimedia con-tent based on the spoken soundtrack. In di erent years the task focused on di erent multimedia content collections. Initially the Blip10000 collection of crawled content from the blip.tv<ref type="foot" coords="3,488.46,127.80,3.71,7.09" target="#foot_0">1</ref> online platform of semi-professional user generated (SPUG) content <ref type="bibr" coords="3,358.79,156.94,114.51,9.70" target="#b10">(Schmiedeke et al., 2013;</ref><ref type="bibr" coords="3,477.68,156.94,69.26,9.70;3,320.17,170.49,23.56,9.70" target="#b8">Eskevich et al., 2012)</ref> and later a collection of diverse broadcast television content provided by the BBC <ref type="bibr" coords="3,504.58,184.03,42.36,9.70;3,320.17,197.58,48.20,9.70" target="#b9">(Eskevich et al., 2015)</ref>. Participants were provided with stateof-the-art ASR transcripts of the content archives and carefully developed search queries. Tasks included known-item and ad hoc search, with relevance assessment using crowdsourcing methods. As well as confirming earlier findings in terms of automated location of useful jump-in points, there was significant focus in these tasks on how submissions should be comparatively evaluated. In particular, the trade-o between ranking of retrieved items containing relevant content and the accuracy of the identification jump-in points in retrieved items.</p><p>As well as these benchmark tasks, another relevant study in spoken content retrieval using the AMI corpus <ref type="bibr" coords="3,376.71,402.15,88.35,9.70" target="#b1">(Renals et al., 2008)</ref> is reported in Eskevich and Jones (2014) which gives a detailed examination of the di erences in the ranking of retrieved items between manual and automated transcripts arising from ASR errors. A more complete overview of research in spoken content retrieval from its beginnings in the early 1990s to today can be found in Jones <ref type="bibr" coords="3,406.13,496.99,25.06,9.70">(2019)</ref>. While none of this existing work focuses on podcast search, the various content archives used raise many of the same issues that can be observed in podcasts in terms of content diversity, use of domain specific vocabularies, and probable issues relating to absence of entity mentions in conversational podcasts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Summarization</head><p>While there is a great deal of work on summarizing text in the news domain (eg <ref type="bibr" coords="3,455.12,655.88,91.82,9.70;3,320.17,669.43,24.38,9.70" target="#b13">Mihalcea and Tarau (2004)</ref>), there is much less existing work on summarization of spoken content. One study relevant to the Podcast Track is that of <ref type="bibr" coords="3,463.13,696.53,79.64,9.70" target="#b14">Spina et al. (2017)</ref>. This work focuses on the creation of query biased audio summaries of podcasts. A crowdsourced experiment demonstrated that highly noisy automatically generated transcripts of spoken documents are e ective sources of document summaries to support users in making relevance judgements for a query. Particularly notable was the finding that summaries generated using ASR transcripts were comparable in terms of usability to summaries generated using error-free manual transcripts. <ref type="bibr" coords="4,65.06,255.89,81.31,9.70" target="#b15">Besser et al. (2008)</ref> argues that the underlying goals of podcast search may be similar to those for blog search, as podcast can be viewed as audio blogs. In <ref type="bibr" coords="4,65.06,296.53,89.42,9.70" target="#b16">Tsagkias et al. (2010)</ref>, the general appeal of podcast feeds/shows is predicted from various features. The authors identify as important factors of whether a user subscribes to a podcast feed: whether the feed has a logo, length of the description, keyword count, episode length, author count, and feed period. <ref type="bibr" coords="4,82.61,391.96,74.85,9.70" target="#b17">Yang et al. (2019)</ref> showed they could use acoustic features to predict seriousness and energy of podcasts, as well as popularity. Acoustic features take advantage of a unique aspect of podcasts, and can be used as part of a multimodal approach to podcast information access, which we hope to see more of in the track in future years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Podcast Information Access</head><p>3 Segment Retrieval Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition</head><p>The retrieval task was defined as the problem of finding relevant segments from the episodes for a set of search queries which were provided in traditional TREC topic format. The provided transcripts have word-level time-stamps on a granularity of 0.1s which allows retrieval systems to index the contents by time o sets. A segment was defined to be a two-minute chunk starting on the minute; e.g. [0.0-119.9] seconds, <ref type="bibr" coords="4,154.94,675.01,39.91,9.70">[60-199.9</ref>] seconds, [120-139.9] seconds, etc. Segments overlap each other by one minute -any segment except for the first and last segment is covered by the preceding and following segments. The rationale for creating overlapping segments is to account for the case where a phrase or sentence is split across segment boundaries. This creates 3.4M segments in total from the document collection with an average word count of 340 ± 70 per segment. Topics consist of a topic number, keyword query, a type label, and a description of the user's information need. Eight topics were given at the outset for the participants to practice on, and 50 topics were released as the test task. Topics were formulated in three types: topical, re-finding, and known item. Example topics are given in Figure <ref type="figure" coords="4,535.74,224.68,3.74,9.70" target="#fig_3">2</ref>. For training data, many participants used pretrained transfer learning models, some used language technologies and knowledge-based models, and some used only data from the set as shown in table <ref type="table" coords="4,344.38,400.44,3.74,9.70" target="#tab_1">2</ref>. Only one experiment made use of the audio data to produce and use a di erent transcript than the provided one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submissions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>Two-minute length segments were judged by NIST assessors for their relevance to the topic description. NIST assessors had access to both the ASR transcript (including text before and a er the text of the two-minute segment, which can be used as context) as well as the corresponding audio segment. Assessments were made on the PEGFB graded scale (Perfect, Excellent, Good, Fair, Bad) as approximately follows:</p><p>Perfect (4): this grade is used only for "known item" and "refinding" topic types. It reflects the segment that is the earliest entry point into the one episode that the user is seeking.</p><p>Excellent (3): the segment conveys highly relevant information, is an ideal entry point for a human listener, and is fully on topic. An example would be a segment that begins at or very close to the start of a discussion on the Good (2): the segment conveys highly-tosomewhat relevant information, is a good entry point for a human listener, and is fully to mostly on topic. An example would be a segment that is a few minutes "o " in terms of position, so that while it is relevant to the user's information need, they might have preferred to start two minutes earlier or later.</p><p>Fair (1): the segment conveys somewhat relevant information, but is a sub-par entry point for a human listener and may not be fully on topic. Examples would be segments that switch from non-relevant to relevant (so that the listener is not able to immediately understand the relevance of the segment), segments that start well into a discussion without providing enough context for understanding, etc.</p><p>Bad (0): the segment is not relevant.</p><p>Figure <ref type="figure" coords="6,115.95,616.83,5.07,9.70" target="#fig_1">3</ref> shows the number of relevant segments of di erent type per topic. The results are ranged into three groups based on the topic types: topical (15-43), refinding (45-49), known items <ref type="bibr" coords="6,274.77,657.47,12.79,9.70">(53)</ref><ref type="bibr" coords="6,287.56,657.47,4.26,9.70">(54)</ref><ref type="bibr" coords="6,287.56,657.47,4.26,9.70">(55)</ref><ref type="bibr" coords="6,65.06,671.02,11.86,9.70">(56)</ref>. This demonstrates that all topics had some relevant segments retrieved by participants and assessed by assessors. The primary metric for evaluation is mean nDCG, with normalization based on an ideal ranking of all relevant segments. Note that a single episode may contribute one or more relevant segments, some of which may be overlapping, but these are treated as independent items for the purpose of nDCG computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Search Baselines</head><p>Podcast search could be implemented without the full episode transcripts if the titles and creatorprovided descriptions provide enough information for search and indexing. As a first baseline, we compared document level retrieval of transcripts to document level retrieval based on titles and creatorprovided descriptions. Table <ref type="table" coords="6,457.14,671.02,5.07,9.70">3</ref> shows how using transcripts yields vastly higher scores, compared to using titles or descriptions, episode-level or episode   Four baseline segment retrieval runs on transcripts are included, using both standard information retrieval methods as well as re-ranking models using BERT <ref type="bibr" coords="9,122.04,200.67,88.83,9.70" target="#b18">(Devlin et al., 2019)</ref> to represent segment content.</p><p>1. BM25: Standard information retrieval algorithm developed for the Okapi system 2 ; the query field of the topic was used for search terms, and up to 1000 segments are returned for each topic.</p><p>2. QL ( ery Likelihood): Standard information retrieval algorithm 2 3. RERANK-QUERY: A BERT re-ranking model pre-trained on MS MARCO passage retrieval data <ref type="bibr" coords="9,117.45,413.59,115.88,9.70" target="#b20">(Nogueira and Cho, 2019)</ref> without further parameter tuning; the query of the topic was used as the input to the re-ranking model; the re-ranking scores of top-50 segments from BM25 were calculated and submi ed per topic.</p><p>4. RERANK-DESC: Same as RERANK-QUERY except that the description of the topic was used as the input in re-ranking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Search Results</head><p>Table <ref type="table" coords="9,93.95,620.81,5.07,9.70" target="#tab_3">4</ref> gives an overview of the scores for the submi ed experiments. Scoring only the top 30 items or the top 10 items of the list promotes some reranking approaches to the top of the list, illustrating the e ect of use case-motivated evaluation metrics on system comparison. One participant resubmi ed results a er the assessment, to redress the e ects of a processing mishap, and those results are marked in the table with an asterisk.</p><p>4 Summarization Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Definition</head><p>Given a podcast episode, its audio, and transcription, the task is to return a short text snippet which accurately conveys the content of the podcast. Returned summaries should be grammatical, standalone u erances of significantly shorter length than the input episode description, short enough to be quickly read on a smartphone screen.</p><p>No ground truth summaries are provided; the closest proxies are the show and episode descriptions provided by the podcast creators. We observe that these descriptions vary widely in scope, and are not always intended to act as summaries of the episode content, reflecting the di erent genres represented in the sample and the di erent intentions of the creators for the descriptions. We filtered the descriptions to establish a subset that is more appropriate as a ground truth set compared to full set of descriptions. The filtering was done with three heuristics shown in Table <ref type="table" coords="9,435.18,382.58,3.74,9.70" target="#tab_4">5</ref>. These filters overlap to some extent, and remove about a third of the entire set; the remaining 66,245 descriptions we call the Brass Set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submissions</head><p>8 participants submi ed 22 experiments for the summarization task (Table <ref type="table" coords="9,440.30,490.25,3.58,9.70">6</ref>). All experiments used some form of deep learning model, and while some used extractive filtering of material from the transcripts as a step in their processes, all were based on abstractive techniques. No participant used the audio data for summary generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>The summary labels and scores for the participating systems are created for evaluation sets in two ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual Assessments and Scoring</head><p>Summaries are judged on a four-step scale intended to model how well a listener is able to make a decision whether to listen to a podcast or not, conveying a gist of what the user should expect to  Excellent: the summary accurately conveys all the most important a ributes of the episode, which could include topical content, genre, and participants. In addition to giving an accurate representation of the content, it contains almost no redundant material which is not needed when deciding whether to listen. It is also coherent, comprehensible, and has no grammatical</p><p>Good: the summary conveys most of the most important a ributes and gives the reader a reasonable sense of what the episode contains with li le redundant material which is not needed when deciding whether to listen. Occasional grammatical or coherence errors are acceptable.</p><p>Fair: the summary conveys some a ributes of the content but gives the reader an imperfect or incomplete sense of what the episode contains. It may contain redundant material which is not needed when deciding whether to listen and may contain repetitions or broken sentences.</p><p>Bad: the summary does not convey any of the most important content items of the episode or gives the reader an incorrect or incomprehensible sense of what the episode contains. It may contain a large amount of redundant information that is not needed when deciding whether to listen to the episode.</p><p>NIST assessors evaluated 180 of the automatically-generated summaries produced by participants using the EGFB scale. These assessments are converted into a numerical score by a weighting scheme tested for being able separate the baseline systems applied to the Brass set. Weights of 4-2-1-0 for EGFB turned out to be simple and e ective in this respect.</p><p>In addition to the EGFB assessments, we created a set of boolean a ributes that a desirable podcast summary might contain. The primary evaluation metric is the EGFB score; the answers to these a ributes are merely an informative signal for participants, and may be useful in devising automated summarization metrics in the future. The a ributes are defined from a small-scale survey of podcast listeners, and are listed below.</p><p>1. names: Does the summary include names of the main people (hosts, guests, characters) involved or mentioned in the podcast?</p><p>2. bio: Does the summary give any additional information about the people mentioned (such as their job titles, biographies, personal background, etc)?</p><p>3. topics: Does the summary include the main topic(s) of the podcast?</p><p>4. format: Does the summary tell you anything about the format of the podcast; e.g. whether it's an interview, whether it's a chat between friends, a monologue, etc? 5. title-context: Does the summary give you more context on the title of the podcast?</p><p>6. redundant: Does the summary contain redundant information?</p><p>7. english: Is the summary wri en in good English?</p><p>8. sentence: Are the start and end of the summary good sentence and paragraph start and end points?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROUGE against Creator Descriptions</head><p>Each of the test set episodes has a creator-provided description. This description is used as a reference (in the absence of ground truth summaries), and a ROUGE-L <ref type="bibr" coords="12,378.81,634.36,45.32,9.70" target="#b21">Lin (2004)</ref> score against the description is computed. ROUGE-L computes overlap of substrings up to the length of the longest common subsequence. Note that these creator-provided descriptions are of varying quality: of the 179-sized subset of descriptions assessed by NIST, we find that only 71, between a third and half, are of Good or Excellent quality.</p><p>We provided a version of the episode descriptions processed by a BERT-based sentence classifier that was trained from a small set of manually annotated examples to identify and remove extraneous content such as boilerplate, ads, promotions, and show notes that do not directly summarize or describe the episode <ref type="bibr" coords="13,204.18,170.49,83.32,9.70">(Reddy et al., 2021)</ref>. 'Cleaned' descriptions with extraneous content removed were produced, and NIST asessors judged the cleaned descriptions as well as the original descriptions for summary quality.</p><p>ROUGE scores were computed against the original episode descriptions, but may be computed against the 'cleaned' descriptions as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summarization Baselines</head><p>Five baseline summarization runs are included. We aimed to include a representation of abstractive as well as extractive models. 3. bartpodcasts: The bartcnn model above, fine-tuned on the full 100k episodes in the dataset, excluding episodes with very short (fewer than 10 characters) or very long descriptions (over 1300 characters). Episodes with descriptions that were highly similar<ref type="foot" coords="13,287.62,527.03,3.71,7.09" target="#foot_3">4</ref> to other descriptions in the same show, or to the show description itself, were also ignored. The descriptions were also processed through a model to detect and remove ads, promotions, and show notes such as links to transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">textranksegments:</head><p>We chunked the transcript into one-minute chunks, and applied the TextRank algorithm <ref type="bibr" coords="13,207.62,660.66,79.46,9.70;13,94.32,674.21,43.44,9.70" target="#b13">(Mihalcea and Tarau, 2004)</ref>, with word overlap as the simi-larity metric, to find the most 'central' oneminute segment.</p><p>5. textranksentences: The same process as above, except that we chunked the transcript into sentences using SpaCy<ref type="foot" coords="13,476.25,150.38,3.71,7.09" target="#foot_4">5</ref> and extracted the two most central sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Summarization Results</head><p>179 episodes were scored for EGFB quality and the boolean a ributes by NIST assessors for the 22 submi ed experiments and the 5 baselines. The submi ed experiments were also scored automatically using ROUGE-L against the creator-provided descriptions for all the 1024 test episodes. Table <ref type="table" coords="13,541.87,287.72,5.07,9.70" target="#tab_5">7</ref> shows both the manual assessment scores as well as the automatic evaluations. All a ributes were found to be significantly correlated with the aggregate quality score (Figure <ref type="figure" coords="13,536.20,341.93,3.58,9.70" target="#fig_6">4</ref>  The ROUGE-L F-score is found to be weakly but significantly correlated with the aggregate EGFB quality score (Pearson correlation 0.28). As Figure <ref type="figure" coords="14,65.06,102.74,5.07,9.70" target="#fig_7">5</ref> shows, while summaries rated E and G do have higher median ROUGE scores than those rated F and B, the variation is tremendously large, especially for summaries rated F, raising the question of whether ROUGE against creator descriptions is a su iciently reliable metric for this task. The episodes in the test set were variously challenging. Some very topical podcasts with a clear statement of purpose or a concise topical heading are comparatively easy to summarise, if that statement was identified in the episode transcript or even in the episode description: "Welcome to my podcast! Let us talk and learn about God's word, lifepurpose, values, and faith!", or "On this day in 1826, 15-year-old Ellen Turner was abducted in a forced marriage plot intended to swindle her family out of their fortune. " Episodes with a broad range of covered topics (such as the hosts' opinions on various books, movies and video games and their experiences from working in comedy), and episodes that are not about topics but rather, are sleep aids or avant-garde performance pieces, proved challenging for most systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Task evolution for Year 2</head><p>For 2021, we have the ambition to encourage participants to make use of the audio data in addition to the transcripts, but we do not wish to change the overall task formulation.</p><p>We intend to continue the segment retrieval task with some modifications. In 2020, the segment retrieval output was restricted to two-minute segments at fixed starting points over the episode: in 2021, we will consider freely selected jump-in points in the episode, to allow for more precise segment results. We will add new topic types to the three used this year, including types that are likely to be be er addressed if the audio signal is taken into consideration.</p><p>We will specify the use case which the summarization task is intended to address in greater detail, with the target notion being an Audio Trailer, i.e. the output of the task should be a short highlight clip from the podcast episode in question. In practice, this means that the clip is not required to provide a representation of the entire content but an indicative segment which will inspire the listener to listen to the entire episode. The details of this specification will be formulated to make assessment transparent and reproducible. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,320.17,278.49,226.77,9.70;4,320.17,292.04,226.77,9.70;4,320.17,305.59,226.77,9.70;4,320.17,319.14,14.64,9.70;4,351.81,319.14,195.13,9.70;4,320.17,332.69,226.77,9.70;4,320.17,346.24,26.09,9.70"><head></head><label></label><figDesc>7 participants submi ed 24 experiments for the retrieval task. All runs were 'automatic', i.e, without human intervention; almost all runs were based on the ery Description field, i.e. the more verbose exposition of information need as shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,320.17,362.08,226.78,10.63;6,320.17,376.52,226.78,10.63;6,320.17,390.97,226.77,10.63;6,320.17,405.42,226.78,10.63;6,320.17,419.86,36.02,10.63;6,325.56,84.36,216.00,265.45"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of relevant segments of different type per topic, ranged by the number of relevant episodes per three topics categories (topical (15-43), refinding (45-49), known items (53-56).</figDesc><graphic coords="6,325.56,84.36,216.00,265.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,162.90,347.03,286.21,10.63"><head></head><label></label><figDesc>Figure 1: Sample from an episode transcript and metadata</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,228.66,637.22,154.69,10.63"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example search topics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,80.99,374.29,210.83,9.70;13,94.32,387.84,100.08,9.70;13,80.99,411.03,210.83,9.70;13,94.32,424.58,197.51,9.70;13,94.32,438.13,141.17,9.70;13,238.22,436.09,3.71,7.09"><head></head><label></label><figDesc>1. onemin: Transcript text for the first one minute of the episode. 2. bartcnn: A BART (Lewis et al., 2020) seq2seq model pre-trained on the CNN/Daily Mail corpus for news summarization 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="13,539.78,341.93,7.16,9.70;13,320.17,355.48,226.77,9.70;13,320.17,369.03,226.77,9.70;13,320.17,382.58,226.77,9.70;13,320.17,396.13,226.77,9.70;13,320.17,409.68,226.77,9.70;13,320.17,423.23,153.70,9.70"><head></head><label></label><figDesc>), to di erent degrees with 'Does the summary include the main topic(s) of the podcast?' being the most correlated. Future work might investigate these a ribute values across all submi ed systems towards gaining a concrete understanding of what makes a good podcast summaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="13,320.17,599.17,226.77,10.63;13,320.17,613.61,226.77,10.63;13,320.17,628.06,119.32,10.63;13,325.56,447.94,216.00,138.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pearson correlation of a ributes with the aggregate EGFB quality score across all submi ed baseline runs.</figDesc><graphic coords="13,325.56,447.94,216.00,138.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="14,65.06,345.62,226.77,10.63;14,65.06,360.06,226.77,10.63;14,65.06,374.51,86.26,10.63;14,70.44,195.74,216.00,137.61"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of ROUGE-L F-Score for each manually assessed label across all submitted baseline runs.</figDesc><graphic coords="14,70.44,195.74,216.00,137.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,74.42,86.72,208.03,158.82"><head>Table 1 :</head><label>1</label><figDesc>Participation statistics</figDesc><table coords="2,74.42,86.72,208.03,132.04"><row><cell>Statistic Name</cell><cell>Value</cell></row><row><cell>Email list sign-ups</cell><cell>285</cell></row><row><cell>In TREC slack channel #podcasts 2020</cell><cell>194</cell></row><row><cell>TREC podcasts registrations</cell><cell>213</cell></row><row><cell>Signed data sharing agreement</cell><cell>77</cell></row><row><cell>Downloaded transcripts</cell><cell>64</cell></row><row><cell>Downloaded audio</cell><cell>18</cell></row><row><cell>Participated in Search</cell><cell>7</cell></row><row><cell>Participated in Summarization</cell><cell>8</cell></row><row><cell>Participated in Both</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,71.03,145.22,515.93,627.95"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="5,71.03,145.22,515.93,506.80"><row><cell>Participant</cell><cell>run id</cell><cell cols="2">field transfer data</cell><cell>IR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>learning processing</cell><cell></cell></row><row><cell cols="2">Dublin City U dcu1</cell><cell>D</cell><cell>SpaCy</cell><cell>QE from WordNet</cell></row><row><cell></cell><cell>dcu2</cell><cell>D</cell><cell>SpaCy</cell><cell>QE from Descriptions</cell></row><row><cell></cell><cell>dcu3</cell><cell>D</cell><cell>Spacy</cell><cell>QE, auto RF</cell></row><row><cell></cell><cell>dcu4</cell><cell>D</cell><cell>Spacy</cell><cell>QE from web text</cell></row><row><cell></cell><cell>dcu5</cell><cell>D</cell><cell>Spacy</cell><cell>Combination 1-4</cell></row><row><cell>LRG</cell><cell>LRGREtvrs-r 1</cell><cell>D</cell><cell></cell><cell>XLNet;Regression</cell></row><row><cell></cell><cell>LRGREtvrs-r 2</cell><cell>D</cell><cell></cell><cell>XLNet;Regression+Concat</cell></row><row><cell></cell><cell>LRGREtvrs-r 3</cell><cell>D</cell><cell></cell><cell>XLNet;Similarity</cell></row><row><cell>U Maryland</cell><cell>UMD IR run1</cell><cell>D</cell><cell></cell><cell>Indri</cell></row><row><cell></cell><cell>UMD IR run2</cell><cell>D</cell><cell></cell><cell>Indri</cell></row><row><cell></cell><cell>UMD IR run3</cell><cell>D</cell><cell>stemming</cell><cell>Combination + Rerank</cell></row><row><cell></cell><cell></cell><cell></cell><cell>word2vec</cell><cell></cell></row><row><cell></cell><cell>UMD ID run4</cell><cell>D</cell><cell>stemming</cell><cell>rerank + Combination</cell></row><row><cell></cell><cell></cell><cell></cell><cell>word2vec</cell><cell></cell></row><row><cell></cell><cell>UMD IR run5</cell><cell>D</cell><cell>stemming</cell><cell>Combination of 1-4</cell></row><row><cell cols="2">U Texas Dallas UTDThesis Run1</cell><cell>D</cell><cell cols="2">fuzzy match Lucene</cell></row><row><cell cols="2">Johns Hopkins hltcoe1</cell><cell>Q</cell><cell>5-gram</cell><cell>Rocchio RF</cell></row><row><cell>HLT COE</cell><cell>hltcoe2</cell><cell>Q</cell><cell></cell><cell>Rocchio RF</cell></row><row><cell></cell><cell>hltcoe3</cell><cell>Q</cell><cell></cell><cell>no RF</cell></row><row><cell></cell><cell>hltcoe4</cell><cell>D</cell><cell></cell><cell>Rocchio RF</cell></row><row><cell></cell><cell>hltcoe5</cell><cell>Q</cell><cell>transcript</cell><cell>Rocchio RF</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4-gram</cell><cell></cell></row><row><cell>U Oklahoma</cell><cell>oudalab1</cell><cell>D</cell><cell>SpaCy</cell><cell>BM25; Faiss; finetuned on S AD</cell></row><row><cell>Spotify</cell><cell>BERT-DESC-S</cell><cell>D</cell><cell></cell><cell>rerank 50;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>finetuned on other topics</cell></row><row><cell></cell><cell>BERT-DESC-Q</cell><cell>D</cell><cell></cell><cell>rerank 50;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>finetuned on automatic topics</cell></row><row><cell></cell><cell>BERT-DESC-TD</cell><cell>D</cell><cell></cell><cell>rerank 50;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>finetuned on synthetic data</cell></row><row><cell>baseline</cell><cell>BM25</cell><cell>Q</cell><cell></cell><cell>BM25</cell></row><row><cell></cell><cell>QL</cell><cell>Q</cell><cell></cell><cell>query likelihood</cell></row><row><cell></cell><cell>RERANK-QUERY</cell><cell>Q</cell><cell></cell><cell>rerank 50</cell></row><row><cell></cell><cell>RERANK-DESC</cell><cell>D</cell><cell></cell><cell>rerank 50</cell></row></table><note coords="5,217.54,665.80,217.67,10.63;5,187.88,762.54,222.35,10.63;6,65.06,43.87,123.66,10.63;6,454.08,43.87,78.98,10.63;6,94.32,89.19,197.51,9.70;6,94.32,102.74,86.00,9.70"><p>Technologies employed for the retrieval task TREC 2021 Podcasts Track Overview -Page 5 Podcasts Track Overview Jones et al. 2021 topic, immediately signaling relevance and context to the user.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,65.06,105.42,481.89,586.25"><head>Table 4 :</head><label>4</label><figDesc>Overview of results from submi ed segment retrieval experiments. Post-assessment rerun submissions are marked with an asterisk.</figDesc><table coords="10,142.35,105.42,327.31,586.25"><row><cell></cell><cell></cell><cell cols="3">nDCG nDCG at 30 precision at 10</cell></row><row><cell>UMD IR run3</cell><cell></cell><cell>0.67</cell><cell>0.52</cell><cell>0.60</cell></row><row><cell>UMD ID run4</cell><cell></cell><cell>0.66</cell><cell>0.49</cell><cell>0.56</cell></row><row><cell>UMD IR run1</cell><cell></cell><cell>0.62</cell><cell>0.45</cell><cell>0.53</cell></row><row><cell>UMD IR run5</cell><cell></cell><cell>0.65</cell><cell>0.50</cell><cell>0.58</cell></row><row><cell>UMD IR run2</cell><cell></cell><cell>0.59</cell><cell>0.42</cell><cell>0.51</cell></row><row><cell>run dcu5</cell><cell></cell><cell>0.59</cell><cell>0.43</cell><cell>0.54</cell></row><row><cell>run dcu4</cell><cell></cell><cell>0.58</cell><cell>0.42</cell><cell>0.54</cell></row><row><cell>run dcu1</cell><cell></cell><cell>0.57</cell><cell>0.42</cell><cell>0.50</cell></row><row><cell>run dcu3</cell><cell></cell><cell>0.57</cell><cell>0.42</cell><cell>0.50</cell></row><row><cell>run dcu2</cell><cell></cell><cell>0.55</cell><cell>0.40</cell><cell>0.48</cell></row><row><cell>LRGREtvs-r 2 *</cell><cell></cell><cell>0.54</cell><cell>0.40</cell><cell>0.48</cell></row><row><cell>LRGREtvs-r 1 *</cell><cell></cell><cell>0.54</cell><cell>0.40</cell><cell>0.47</cell></row><row><cell>hltcoe4</cell><cell></cell><cell>0.51</cell><cell>0.43</cell><cell>0.54</cell></row><row><cell>LRGREtvs-r 3 *</cell><cell></cell><cell>0.50</cell><cell>0.32</cell><cell>0.41</cell></row><row><cell>hltcoe3</cell><cell></cell><cell>0.50</cell><cell>0.35</cell><cell>0.43</cell></row><row><cell>hltcoe2</cell><cell></cell><cell>0.47</cell><cell>0.38</cell><cell>0.45</cell></row><row><cell>hltcoe1</cell><cell></cell><cell>0.45</cell><cell>0.33</cell><cell>0.38</cell></row><row><cell>BERT-DESC-S</cell><cell></cell><cell>0.43</cell><cell>0.47</cell><cell>0.57</cell></row><row><cell>BERT-DESC-TD</cell><cell></cell><cell>0.43</cell><cell>0.47</cell><cell>0.56</cell></row><row><cell>BERT-DESC-Q</cell><cell></cell><cell>0.41</cell><cell>0.45</cell><cell>0.53</cell></row><row><cell>hltcoe5</cell><cell></cell><cell>0.38</cell><cell>0.30</cell><cell>0.37</cell></row><row><cell>UTDThesis Run1</cell><cell></cell><cell>0.34</cell><cell>0.34</cell><cell>0.43</cell></row><row><cell>oudalab1</cell><cell></cell><cell>0.00</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Baseline BM25</cell><cell></cell><cell>0.52</cell><cell>0.40</cell><cell>0.49</cell></row><row><cell>Baseline QL</cell><cell></cell><cell>0.52</cell><cell>0.40</cell><cell>0.48</cell></row><row><cell cols="2">Baseline RERANK-DESC</cell><cell>0.43</cell><cell>0.48</cell><cell>0.57</cell></row><row><cell cols="2">Baseline RERANK-QUERY</cell><cell>0.43</cell><cell>0.47</cell><cell>0.56</cell></row><row><cell>filter</cell><cell>criteria</cell><cell></cell><cell></cell><cell>items a ected</cell></row><row><cell>Length</cell><cell cols="3">very long (&gt; 750 characters) or</cell><cell>24, 033 (23%)</cell></row><row><cell></cell><cell cols="3">very short (&lt; 20 characters)</cell></row><row><cell>Similarity to</cell><cell cols="2">&gt; 50% lexical overlap</cell><cell></cell><cell>15, 375 (15%)</cell></row><row><cell cols="4">other descriptions with other episode descriptions</cell></row><row><cell>Similarity to</cell><cell cols="2">&gt; 40% lexical overlap</cell><cell></cell><cell>9, 444 (9%)</cell></row><row><cell cols="4">show description with own show description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,80.22,705.42,451.55,67.75"><head>Table 5 :</head><label>5</label><figDesc>Filters to remove 'less descriptive' episode descriptions, to form the brass subcorpus.</figDesc><table coords="10,185.10,762.54,227.91,10.63"><row><cell>TREC 2021 Podcasts Track Overview -Page 10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="15,65.06,160.30,481.89,612.87"><head>Table 7 :</head><label>7</label><figDesc>Overview of manual assessment results from submi ed summarization experiments. The aggregate EGFB score is computed by assigning E=4, G=2, F=1, B=0. ROUGE scores are computed against the original creator provided descriptions of each episode.</figDesc><table coords="15,113.99,160.30,384.02,447.19"><row><cell>experiment</cell><cell cols="5">aggregate #E #E,G ROUGE-L ROUGE-L</cell></row><row><cell></cell><cell>EGFB score</cell><cell></cell><cell></cell><cell>recall</cell><cell>precision</cell></row><row><cell>cued speechUniv2</cell><cell>2.04</cell><cell cols="2">47 105</cell><cell>0.224</cell><cell>0.235</cell></row><row><cell>cued speechUniv1</cell><cell>1.98</cell><cell cols="2">49 104</cell><cell>0.226</cell><cell>0.232</cell></row><row><cell>cued speechUniv4</cell><cell>1.94</cell><cell cols="2">46 101</cell><cell>0.204</cell><cell>0.231</cell></row><row><cell>UCF NLP2</cell><cell>1.81</cell><cell>45</cell><cell>92</cell><cell>0.224</cell><cell>0.256</cell></row><row><cell>cued speechUniv3</cell><cell>1.78</cell><cell>39</cell><cell>90</cell><cell>0.205</cell><cell>0.220</cell></row><row><cell>hk uu podcast1</cell><cell>1.74</cell><cell>35</cell><cell>89</cell><cell>0.190</cell><cell>0.265</cell></row><row><cell>UCF NLP1</cell><cell>1.64</cell><cell>34</cell><cell>79</cell><cell>0.220</cell><cell>0.267</cell></row><row><cell>categoryaware2</cell><cell>1.58</cell><cell>32</cell><cell>71</cell><cell>0.199</cell><cell>0.257</cell></row><row><cell>categoryaware1</cell><cell>1.51</cell><cell>26</cell><cell>75</cell><cell>0.208</cell><cell>0.227</cell></row><row><cell>coarse2fine</cell><cell>1.3</cell><cell>18</cell><cell>57</cell><cell>0.187</cell><cell>0.158</cell></row><row><cell>udel wang zheng1</cell><cell>1.19</cell><cell>13</cell><cell>52</cell><cell>0.161</cell><cell>0.239</cell></row><row><cell>udel wang zheng4</cell><cell>1.16</cell><cell>14</cell><cell>53</cell><cell>0.168</cell><cell>0.202</cell></row><row><cell>udel wang zheng3</cell><cell>1.08</cell><cell>10</cell><cell>44</cell><cell>0.160</cell><cell>0.208</cell></row><row><cell>2306987O abs run1</cell><cell>1.00</cell><cell>12</cell><cell>39</cell><cell>0.156</cell><cell>0.208</cell></row><row><cell>2306987O extabs run2</cell><cell>0.99</cell><cell>13</cell><cell>42</cell><cell>0.167</cell><cell>0.237</cell></row><row><cell>2306987O extabs run3</cell><cell>0.80</cell><cell>8</cell><cell>22</cell><cell>0.147</cell><cell>0.220</cell></row><row><cell>udel wang zheng2</cell><cell>0.76</cell><cell>7</cell><cell>28</cell><cell>0.139</cell><cell>0.184</cell></row><row><cell>UTDThesis1</cell><cell>0.43</cell><cell>1</cell><cell>11</cell><cell>0.129</cell><cell>0.172</cell></row><row><cell>unhtrema4</cell><cell>0.04</cell><cell>1</cell><cell>1</cell><cell>0.180</cell><cell>0.069</cell></row><row><cell>unhtrema3</cell><cell>0.03</cell><cell>0</cell><cell>0</cell><cell>0.134</cell><cell>0.089</cell></row><row><cell>unhtrema2</cell><cell>0.01</cell><cell>0</cell><cell>0</cell><cell>0.090</cell><cell>0.131</cell></row><row><cell>unhtrema1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.061</cell><cell>0.156</cell></row><row><cell>Human description</cell><cell>1.45</cell><cell>28</cell><cell>71</cell><cell></cell><cell></cell></row><row><cell>Baseline filtered</cell><cell>1.49</cell><cell>33</cell><cell>71</cell><cell></cell><cell></cell></row><row><cell>Baseline bartpodcasts</cell><cell>1.49</cell><cell>25</cell><cell>75</cell><cell>0.210</cell><cell>0.208</cell></row><row><cell>Baseline bartcnn</cell><cell>0.99</cell><cell>10</cell><cell>35</cell><cell>0.272</cell><cell>0.085</cell></row><row><cell>Baseline onemin</cell><cell>0.93</cell><cell>5</cell><cell>30</cell><cell>0.282</cell><cell>0.087</cell></row><row><cell>Baseline textranksegments</cell><cell>0.38</cell><cell>3</cell><cell>9</cell><cell>0.165</cell><cell>0.083</cell></row><row><cell>Baseline textranksentences</cell><cell>0.23</cell><cell>1</cell><cell>4</cell><cell>0.162</cell><cell>0.065</cell></row></table><note coords="15,185.10,762.54,227.91,10.63"><p>TREC 2021 Podcasts Track Overview -Page 15</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,82.99,730.24,230.14,8.30"><p>https://en.wikipedia.org/wiki/Blip_(website)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,82.99,717.87,463.96,8.86;6,65.06,729.82,297.68,8.86"><p>Implemented using the Pyserini package, https://github.com/castorini/pyserini -a Python front end to the Anserini open-source information retrieval toolkit<ref type="bibr" coords="6,287.05,729.82,75.69,8.86" target="#b19">(Yang et al. (2017))</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="13,82.99,693.96,193.02,8.86"><p>h ps://huggingface.co/facebook/bart-large-cnn</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="13,82.99,705.91,463.96,8.86;13,65.06,717.87,187.86,8.86"><p>We represented each description by a normalized vector of TF-IDF values, and computed similarity as the cosine similarity between the vector representations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="13,82.99,729.82,61.69,8.86"><p>h ps://spacy.io</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Building and distributing the dataset and task definitions for the TREC 2020 Podcast Track was a highly collaborative e ort, and we have many people to thank for their valuable contributions: <rs type="person">Mounia Lalmas-Roelleke</rs>, <rs type="person">Claire Burke</rs>, <rs type="person">Zachary Piccolomini</rs>, <rs type="person">Derek Tang</rs>, <rs type="person">David Riordan</rs>, <rs type="person">Fallon Chen</rs>, <rs type="person">Lex Bea</rs> ie, <rs type="person">Daren Gill</rs>, <rs type="person">Laura Pezzini</rs>, <rs type="person">Jenni Lee</rs>, <rs type="person">Alexandra Wei</rs>, <rs type="person">Johannes Vuorensola</rs>, <rs type="person">Nir Zicherman</rs>, <rs type="person">Max Cutler</rs>, <rs type="person">Julia Kaplan</rs>, <rs type="person">Ching-Wei Chen</rs>, <rs type="person">Brian Brost</rs>, <rs type="person">Till Ho mann</rs>, <rs type="person">Nagarjuna Kumarappan</rs>, <rs type="person">Maria Dominguez</rs>, <rs type="person">Laurence Pascall</rs>, <rs type="person">Md I ekhar Tanveer</rs>, <rs type="person">Rezvaneh Rezapour</rs>, <rs type="person">Hamed Bonab</rs> and <rs type="person">Jen McFadden</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="16,65.06,113.70,226.77,9.70;16,76.76,127.25,215.06,9.70;16,76.76,140.80,215.06,9.70;16,76.76,154.34,215.07,9.71;16,76.76,167.89,215.06,9.71;16,76.76,179.04,215.06,12.11;16,76.76,195.00,136.61,9.70" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,176.90,127.25,114.93,9.70;16,76.76,140.80,210.31,9.70">The TREC spoken document retrieval track: A success story (RIAO)</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cedric</forename><forename type="middle">G P</forename><surname>Auzanne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,88.84,154.34,202.99,9.71">Content-Based Multimedia Information Access</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Le Centre de Hautes Études Internationales d&apos;Informatique Documentaire</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,65.06,218.54,226.78,9.70;16,76.76,232.09,215.06,9.70;16,76.76,245.63,215.07,9.71;16,76.76,259.18,215.06,9.71;16,76.76,272.73,152.85,9.71" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,76.76,232.09,215.06,9.70;16,76.76,245.64,95.90,9.70">Interpretation of multiparty meetings: The AMI and AMIDA projects</title>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,205.83,245.63,86.00,9.71;16,76.76,259.18,215.06,9.71;16,76.76,272.73,123.25,9.71">IEEE Workshop on Hands-Free Speech Communication and Microphone Arrays (HSCMA 2008)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,65.06,296.29,226.77,9.70;16,76.76,309.83,215.07,9.71;16,76.76,323.38,215.06,9.71;16,76.76,336.92,173.28,9.71;16,65.06,360.48,82.33,9.70;16,175.36,360.48,116.47,9.70;16,76.76,374.49,177.54,9.09;16,76.76,387.58,215.07,9.70;16,76.76,401.13,67.36,9.70" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,269.28,296.29,22.54,9.70;16,76.76,309.84,141.21,9.70">Evaluation campaigns and TRECVid</title>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
		<idno>02/09/2021</idno>
		<ptr target="https://www.edisonresearch.com/the-infinite-dial-2020/" />
	</analytic>
	<monogr>
		<title level="m" coord="16,240.43,309.83,51.39,9.71;16,76.76,323.38,215.06,9.71;16,76.76,336.92,143.81,9.71;16,175.36,360.48,111.93,9.70">Proceedings of the 8th ACM SIGMM International workshop on Multimedia Information Retrieval</title>
		<meeting>the 8th ACM SIGMM International workshop on Multimedia Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2020</date>
		</imprint>
	</monogr>
	<note>The infinite dial 2020</note>
</biblStruct>

<biblStruct coords="16,65.06,424.68,226.77,9.70;16,76.76,438.23,215.06,9.70;16,76.76,451.77,215.06,9.70;16,76.76,465.32,215.06,9.70;16,76.76,478.87,215.06,9.70;16,76.76,492.41,215.06,9.71;16,76.76,505.96,202.44,9.71" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,252.76,465.32,39.07,9.70;16,76.76,478.87,193.98,9.70">000 Podcasts: A Spoken English Document Corpus</title>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Cli On</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Bonab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Cartere</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,76.76,492.41,215.06,9.71;16,76.76,505.96,171.83,9.71">Proceedings of the 28th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 28th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,65.06,529.52,226.77,9.70;16,76.76,543.07,215.06,9.70;16,76.76,556.62,215.06,9.70;16,76.76,570.15,215.06,9.71;16,76.76,583.70,215.06,9.71;16,76.76,597.25,215.06,9.71;16,76.76,610.80,215.07,9.71;16,76.76,624.36,22.69,9.70" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,76.76,556.62,215.06,9.70;16,76.76,570.17,104.54,9.70">Overview of the CLEF-2007 Cross-Language Speech Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petra</forename><surname>Ho Mannová</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,211.99,570.15,79.83,9.71;16,76.76,583.70,215.06,9.71;16,76.76,597.25,215.06,9.71;16,76.76,610.80,48.39,9.71">Advances in Multilingual and Multimodal Information Retrieval: Eighth Workshop of the Cross-Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2007">2007. 2008</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="16,65.06,647.91,226.77,9.70;16,76.76,661.46,215.06,9.70;16,76.76,675.01,215.06,9.70;16,76.76,688.56,215.06,9.70;16,76.76,702.09,215.06,9.71;16,76.76,715.64,215.06,9.71;16,76.76,729.19,114.04,9.71" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,181.63,688.56,110.19,9.70;16,76.76,702.11,94.03,9.70">Overview of the NTCIR-10 SpokenDoc-2 Task</title>
		<author>
			<persName coords=""><forename type="first">Tomoyosi</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiromitsu</forename><surname>Nishizaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kiyoaki</forename><surname>Aikawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinhui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshiaki</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seiichi</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroaki</forename><surname>Nanjo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoichi</forename><surname>Yamashita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,191.25,702.09,100.57,9.71;16,76.76,715.64,215.06,9.71;16,76.76,729.19,84.17,9.71">Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies</title>
		<meeting>the 12th NTCIR Conference on Evaluation of Information Access Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,320.17,89.19,226.77,9.70;16,331.88,102.74,215.06,9.70;16,331.88,116.29,215.06,9.70;16,331.88,129.83,215.06,9.71;16,331.88,143.38,215.06,9.71;16,331.88,156.93,122.16,9.71;16,471.95,156.93,74.99,9.71;16,331.88,170.47,192.14,9.71" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,490.06,102.74,56.88,9.70;16,331.88,116.29,193.22,9.70">Overview of the NTCIR-12 Spoken ery &amp; Doc-2 Task</title>
		<author>
			<persName coords=""><forename type="first">Tomoyosi</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiromitsu</forename><surname>Nishizaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroaki</forename><surname>Nanjo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,331.88,129.83,215.06,9.71;16,331.88,143.38,215.06,9.71;16,331.88,156.93,122.16,9.71;16,471.95,156.93,74.99,9.71;16,331.88,170.47,162.37,9.71">Proceedings of the 9th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, estion Answering and Cross-Lingual Information Access</title>
		<meeting>the 9th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, estion Answering and Cross-Lingual Information Access</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,320.17,192.23,226.77,9.70;16,331.88,205.78,215.06,9.70;16,331.88,219.33,215.06,9.70;16,331.88,232.88,215.06,9.70;16,331.88,246.42,215.07,9.71;16,331.88,259.97,213.27,9.71" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,442.38,219.33,104.56,9.70;16,331.88,232.88,215.06,9.70;16,331.88,246.43,16.93,9.70">Overview of mediaeval 2011 rich speech retrieval task and genre tagging task</title>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roeland</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Schmiedeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,366.27,246.42,180.68,9.71;16,331.88,259.97,182.73,9.71">Working Notes Proceedings of the MediaEval 2011 Multimedia Benchmark Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,320.17,281.73,226.77,9.70;16,331.88,295.28,215.06,9.70;16,331.88,308.83,215.06,9.70;16,331.88,322.37,215.07,9.71;16,331.88,335.92,198.01,9.71" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,331.88,308.83,210.52,9.70">Search and hyperlinking task at mediaeval 2012</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shu</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roeland</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,345.24,322.37,201.71,9.71;16,331.88,335.92,167.47,9.71">Working Notes Proceedings of the MediaEval 2012 Multimedia Benchmark Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,320.17,357.68,226.77,9.70;16,331.88,371.22,215.06,9.70;16,331.88,384.77,215.06,9.70;16,331.88,398.31,215.06,9.71;16,331.88,411.86,215.06,9.71;16,331.88,425.41,71.54,9.71" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,331.88,384.77,215.06,9.70;16,331.88,398.32,77.21,9.70">SAVA at Mediaeval 2015: Search and anchoring in video archives</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roeland</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">N</forename><surname>Racca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,438.71,398.31,108.23,9.71;16,331.88,411.86,215.06,9.71;16,331.88,425.41,41.00,9.71">Working Notes Proceedings of the MediaEval 2015 Multimedia Benchmark Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,320.17,447.17,226.77,9.70;16,331.88,460.72,215.06,9.70;16,331.88,474.27,215.07,9.70;16,331.88,487.82,215.06,9.70;16,331.88,501.37,215.06,9.70;16,331.88,514.90,215.06,9.71;16,331.88,528.45,215.07,9.71;16,331.88,542.01,129.77,9.70" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,460.27,487.82,86.67,9.70;16,331.88,501.37,215.06,9.70;16,331.88,514.92,76.74,9.70">Blip10000: A social video dataset containing spug content for tagging and retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Schmiedeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><surname>Ferrané</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><forename type="middle">A</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lori</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,427.27,514.90,119.67,9.71;16,331.88,528.45,138.82,9.71">Proceedings of the 4th ACM Multimedia Systems Conference</title>
		<meeting>the 4th ACM Multimedia Systems Conference</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,320.17,563.76,226.77,9.70;16,331.88,577.31,215.06,9.70;16,331.88,590.85,215.07,9.71;16,331.88,604.41,22.69,9.70" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,504.17,563.76,42.77,9.70;16,331.88,577.31,215.06,9.70;16,331.88,590.86,27.68,9.70">Exploring speech retrieval from meetings using the AMI corpus</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,376.53,590.85,135.73,9.71">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,320.17,626.16,226.77,9.70;16,331.88,639.70,215.06,9.71;16,331.88,653.25,215.06,9.71;16,331.88,666.80,204.77,9.71" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="16,407.76,626.16,139.18,9.70;16,331.88,639.70,215.06,9.71;16,331.88,653.25,215.06,9.71;16,331.88,666.80,131.62,9.71">About sound and vision: CLEF beyond text retrieval tasks. In Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,320.17,688.56,226.77,9.70;16,331.88,702.09,215.07,9.71;16,331.88,715.64,215.06,9.71;16,331.88,729.19,139.98,9.71" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,463.29,688.56,83.65,9.70;16,331.88,702.11,62.38,9.70">Textrank: Bringing order into text</title>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,413.21,702.09,133.73,9.71;16,331.88,715.64,215.06,9.71;16,331.88,729.19,109.09,9.71">Proceedings of the 2004 conference on Empirical Methods in Natural Language Processing (EMNLP). ACL</title>
		<meeting>the 2004 conference on Empirical Methods in Natural Language Processing (EMNLP). ACL</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,65.06,89.19,226.77,9.70;17,76.76,102.74,215.06,9.70;17,76.76,116.29,215.06,9.70;17,76.76,129.83,215.06,9.71;17,76.76,143.38,215.06,9.71;17,76.76,156.94,22.69,9.70" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,227.31,102.74,64.52,9.70;17,76.76,116.29,215.06,9.70;17,76.76,129.84,59.38,9.70">Extracting audio summaries to support e ective spoken document search</title>
		<author>
			<persName coords=""><forename type="first">Damiano</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanne</forename><forename type="middle">R</forename><surname>Trippas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Cavedon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,146.68,129.83,145.15,9.71;17,76.76,143.38,184.24,9.71">Journal of the Association for Information Science and Technology (JASIST)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,65.06,179.55,226.77,9.70;17,76.76,193.10,215.06,9.70;17,76.76,206.65,215.06,9.70;17,76.76,220.19,215.06,9.71;17,76.76,233.74,215.07,9.71;17,76.76,247.30,215.06,9.70;17,76.76,260.85,119.71,9.70" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,76.76,193.10,215.06,9.70;17,76.76,206.65,78.10,9.70">An exploratory study of user goals and strategies in podcast search</title>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Besser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><forename type="middle">A</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,198.77,220.19,93.05,9.71;17,76.76,233.74,74.04,9.71">Proceedings from the workshop Lernen</title>
		<editor>
			<persName><forename type="first">Joachim</forename><surname>Baumeister</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Atzmüller</surname></persName>
		</editor>
		<meeting>from the workshop Lernen<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Wissen &amp; Adaptivität</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>LWA). Department of Computer Science, University of Würzburg</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="17,65.06,283.46,226.77,9.70;17,76.76,297.01,215.06,9.70;17,76.76,310.55,215.06,9.71;17,76.76,324.10,215.06,9.71;17,76.76,337.65,110.39,9.71" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,117.06,297.01,174.76,9.70;17,76.76,310.56,149.88,9.70">Predicting podcast preference: An analysis framework and its application</title>
		<author>
			<persName coords=""><forename type="first">Manos</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><forename type="middle">A</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,233.70,310.55,58.12,9.71;17,76.76,324.10,215.06,9.71;17,76.76,337.65,53.67,9.71">Journal of the Association for Information Science and Technology (JASIST)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,65.06,360.27,226.77,9.70;17,76.76,373.82,215.06,9.70;17,76.76,387.37,215.06,9.70;17,76.76,400.91,215.06,9.71;17,76.76,414.46,215.06,9.71;17,76.76,428.01,173.74,9.71" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,76.76,387.37,215.06,9.70;17,76.76,400.92,120.42,9.70">More than just words: Modeling non-textual characteristics of podcasts</title>
		<author>
			<persName coords=""><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Drew</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Sobolev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mor</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,226.88,400.91,64.94,9.71;17,76.76,414.46,215.06,9.71;17,76.76,428.01,142.52,9.71">Proceedings of the Twel h ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the Twel h ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,65.06,450.64,226.77,9.70;17,76.76,464.18,215.06,9.70;17,76.76,477.73,215.06,9.70;17,76.76,491.27,215.07,9.71;17,76.76,504.82,215.06,9.71;17,76.76,518.37,215.06,9.71;17,331.88,89.18,215.07,9.71;17,331.88,102.74,113.72,9.70" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,170.42,464.18,121.41,9.70;17,76.76,477.73,215.06,9.70;17,76.76,491.28,36.89,9.70">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,136.63,491.27,155.20,9.71;17,76.76,504.82,134.68,9.71">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,320.17,126.25,226.77,9.70;17,331.88,139.80,215.06,9.70;17,331.88,153.34,215.07,9.71;17,331.88,166.89,215.06,9.71;17,331.88,180.44,155.90,9.71" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="17,488.94,126.25,58.00,9.70;17,331.88,139.80,215.06,9.70;17,331.88,153.35,35.64,9.70">Anserini: Enabling the use of lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,387.62,153.34,159.33,9.71;17,331.88,166.89,215.06,9.71;17,331.88,180.44,126.43,9.71">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,320.17,203.96,191.43,9.70;17,527.64,203.96,19.30,9.70;17,331.88,217.51,129.75,9.70;17,482.14,217.50,64.81,9.71;17,331.88,231.05,98.84,9.71" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="17,527.64,203.96,19.30,9.70;17,331.88,217.51,125.65,9.70">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,320.17,254.57,226.77,9.70;17,331.88,268.11,215.07,9.71;17,331.88,281.66,215.06,9.71;17,331.88,295.22,205.62,9.70" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="17,394.75,254.57,152.19,9.70;17,331.88,268.12,109.22,9.70">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,465.99,268.11,80.96,9.71;17,331.88,281.66,189.55,9.71">Proceedings of the workshop &quot;Text summarization branches out</title>
		<meeting>the workshop &quot;Text summarization branches out</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,320.17,318.73,226.77,9.70;17,331.88,332.28,215.06,9.70;17,331.88,345.83,215.06,9.70;17,331.88,359.37,215.06,9.71;17,331.88,372.92,215.06,9.71;17,331.88,386.46,135.72,9.71" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="17,331.88,345.83,191.10,9.70">Detecting extraneous content in podcasts</title>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aswin</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,331.88,359.37,215.06,9.71;17,331.88,372.92,215.06,9.71;17,331.88,386.46,105.48,9.71">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,320.17,409.99,226.77,9.70;17,331.88,423.54,215.06,9.70;17,331.88,437.09,215.06,9.70;17,331.88,450.64,215.06,9.70;17,331.88,464.18,215.06,9.70;17,331.88,477.72,215.07,9.71;17,331.88,491.27,215.06,9.71;17,331.88,504.82,215.07,9.71;17,331.88,518.38,146.92,9.70" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="17,399.31,450.64,147.63,9.70;17,331.88,464.18,215.06,9.70;17,331.88,477.73,140.70,9.70">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Ze Lemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,495.55,477.72,51.39,9.71;17,331.88,491.27,215.06,9.71;17,331.88,504.82,140.31,9.71">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
